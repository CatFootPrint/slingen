/*
 * kf_kernel.h
 *
Decl { {u'B': Matrix[B, (28, 12), GenMatAccess], u'F': SquaredMatrix[F, (28, 28), GenMatAccess], u'H': Matrix[H, (12, 28), GenMatAccess], u'Q': Symmetric[Q, (28, 28), USMatAccess], u'P': Symmetric[P, (28, 28), USMatAccess], u'R': Symmetric[R, (12, 12), USMatAccess], u'M1': Matrix[M1, (12, 28), GenMatAccess], u'M0': SquaredMatrix[M0, (28, 28), GenMatAccess], u'M3': Symmetric[M3, (12, 12), USMatAccess], u'M2': Matrix[M2, (28, 12), GenMatAccess], u'Y': Symmetric[Y, (28, 28), USMatAccess], u'v0': Matrix[v0, (12, 1), GenMatAccess], u'u': Matrix[u, (12, 1), GenMatAccess], u'y': Matrix[y, (28, 1), GenMatAccess], u'x': Matrix[x, (28, 1), GenMatAccess], u'z': Matrix[z, (12, 1), GenMatAccess], 'T614': Matrix[T614, (1, 12), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'ldiv_ut_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'chol_u_ow_opt': {'m': 'm3.ll'}, 'ldiv_un_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'chol_u_ow_opt_m3_ldiv_un_ow_opt_m4_n1_ldiv_ut_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[28,12] ) ) * Tile( (1, 1), Tile( (4, 4), u[12,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[28,28] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[12,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[12,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[12,28] ) ) * Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[12,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[12,28] ) ) * Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[28,12] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[12,28] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[12,12] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[12,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[12,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[12,12] ) ) )
Eq.ann: {}
Entry 7:
For_{fi39;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi39 + 1), M3[12,12],h(3, 12, fi39 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi39 + 1), M3[12,12],h(3, 12, fi39 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi39 + 2), M3[12,12],h(2, 12, fi39 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi39 + 2), M3[12,12],h(2, 12, fi39 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) = ( Tile( (1, 1), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Div Tile( (1, 1), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ) )
Eq.ann: {}
Entry 14:
For_{fi100;0;-fi39 + 4;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
 )Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi39 + 8, 12, fi39 + 4), M3[12,12],h(-fi39 + 8, 12, fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi39 + 8, 12, fi39 + 4), M3[12,12],h(-fi39 + 8, 12, fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi39), M3[12,12],h(-fi39 + 8, 12, fi39 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi39), M3[12,12],h(-fi39 + 8, 12, fi39 + 4)) ) ) ) )
Eq.ann: {}
 )Entry 8:
Eq: Tile( (1, 1), G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 8)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 8)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), M3[12,12],h(3, 12, 9)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), M3[12,12],h(3, 12, 9)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 9)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), M3[12,12],h(2, 12, 10)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), M3[12,12],h(2, 12, 10)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) = ( Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) Div Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ) )
Eq.ann: {}
Entry 20:
For_{fi186;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 12, fi186), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi186), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi186), M3[12,12],h(1, 12, fi186)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi186 + 1), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi186 + 1), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186), M3[12,12],h(3, 12, fi186 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 12, fi186 + 1), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi186 + 1), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi186 + 1), M3[12,12],h(1, 12, fi186 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi186 + 2), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi186 + 2), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186 + 1), M3[12,12],h(2, 12, fi186 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186 + 1), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 12, fi186 + 2), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi186 + 2), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi186 + 2), M3[12,12],h(1, 12, fi186 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186 + 3), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186 + 3), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186 + 2), M3[12,12],h(1, 12, fi186 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi186 + 2), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 12, fi186 + 3), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi186 + 3), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi186 + 3), M3[12,12],h(1, 12, fi186 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi186 + 8, 12, fi186 + 4), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi186 + 8, 12, fi186 + 4), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi186), M3[12,12],h(-fi186 + 8, 12, fi186 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi186), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 21:
Eq: Tile( (1, 1), G(h(1, 12, 8), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 8), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 12, 9), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 9), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 12, 10), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 10), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), v0[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 12, 11), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 11), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ) )
Eq.ann: {}
Entry 28:
For_{fi263;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 12, -fi263 + 11), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi263 + 11), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi263 + 11), M3[12,12],h(1, 12, -fi263 + 11)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi263 + 8), M3[12,12],h(1, 12, -fi263 + 11)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi263 + 11), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 12, -fi263 + 10), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi263 + 10), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi263 + 10), M3[12,12],h(1, 12, -fi263 + 10)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi263 + 8), M3[12,12],h(1, 12, -fi263 + 10)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi263 + 10), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 12, -fi263 + 9), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi263 + 9), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi263 + 9), M3[12,12],h(1, 12, -fi263 + 9)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi263 + 8), M3[12,12],h(1, 12, -fi263 + 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi263 + 9), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi263 + 8), M3[12,12],h(1, 12, -fi263 + 8)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi263 + 8, 12, 0), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi263 + 8, 12, 0), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi263 + 8, 12, 0), M3[12,12],h(4, 12, -fi263 + 8)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, -fi263 + 8), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 29:
Eq: Tile( (1, 1), G(h(1, 12, 3), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 3), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 3), M3[12,12],h(1, 12, 3)) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), M3[12,12],h(1, 12, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 12, 2), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 2), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 2), M3[12,12],h(1, 12, 2)) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), M3[12,12],h(1, 12, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 12, 1), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 1), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 1), M3[12,12],h(1, 12, 1)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), v0[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), v0[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M3[12,12],h(1, 12, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), v0[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 12, 0), v0[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 0), v0[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 0), M3[12,12],h(1, 12, 0)) ) )
Eq.ann: {}
Entry 36:
For_{fi340;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi340), M3[12,12],h(1, 12, fi340)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi340 + 1), M3[12,12],h(1, 12, fi340 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi340 + 2), M3[12,12],h(1, 12, fi340 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi340 + 3), M3[12,12],h(1, 12, fi340 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi359;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340), M1[12,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi340 + 1), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi340 + 1), M1[12,28],h(4, 28, fi359)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340), M3[12,12],h(3, 12, fi340 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340), M1[12,28],h(4, 28, fi359)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 1), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 1), M1[12,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi340 + 2), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi340 + 2), M1[12,28],h(4, 28, fi359)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 1), M3[12,12],h(2, 12, fi340 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 1), M1[12,28],h(4, 28, fi359)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 2), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 2), M1[12,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 3), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 3), M1[12,28],h(4, 28, fi359)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 2), M3[12,12],h(1, 12, fi340 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 2), M1[12,28],h(4, 28, fi359)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 3), M1[12,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, fi340 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi340 + 3), M1[12,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi340 + 8, 12, fi340 + 4), M1[12,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi340 + 8, 12, fi340 + 4), M1[12,28],h(28, 28, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi340), M3[12,12],h(-fi340 + 8, 12, fi340 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi340), M1[12,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 37:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 10)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 11)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ) )
Eq.ann: {}
Entry 39:
For_{fi406;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 8)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M1[12,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), M1[12,28],h(4, 28, fi406)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M1[12,28],h(4, 28, fi406)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M1[12,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), M1[12,28],h(4, 28, fi406)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M1[12,28],h(4, 28, fi406)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 10)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M1[12,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M1[12,28],h(4, 28, fi406)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M1[12,28],h(4, 28, fi406)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M1[12,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 11)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M1[12,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
 )Entry 40:
For_{fi453;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 11)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi453 + 11), M3[12,12],h(1, 12, -fi453 + 11)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 10)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi453 + 10), M3[12,12],h(1, 12, -fi453 + 10)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 9)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi453 + 9), M3[12,12],h(1, 12, -fi453 + 9)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 8)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi453 + 8), M3[12,12],h(1, 12, -fi453 + 8)) ) )
Eq.ann: {}
Entry 4:
For_{fi472;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 11), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 11)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 11), M1[12,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi453 + 8), M3[12,12],h(1, 12, -fi453 + 11)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 11), M1[12,28],h(4, 28, fi472)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 10), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 10)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 10), M1[12,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi453 + 8), M3[12,12],h(1, 12, -fi453 + 10)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 10), M1[12,28],h(4, 28, fi472)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 9), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 9), M1[12,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 8), M3[12,12],h(1, 12, -fi453 + 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 9), M1[12,28],h(4, 28, fi472)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, -fi453 + 8)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi453 + 8), M1[12,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi453 + 8, 12, 0), M1[12,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi453 + 8, 12, 0), M1[12,28],h(28, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi453 + 8, 12, 0), M3[12,12],h(4, 12, -fi453 + 8)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, -fi453 + 8), M1[12,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 41:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 3), M3[12,12],h(1, 12, 3)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 2), M3[12,12],h(1, 12, 2)) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 1), M3[12,12],h(1, 12, 1)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,12],h(1, 12, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 0), M3[12,12],h(1, 12, 0)) ) )
Eq.ann: {}
Entry 45:
For_{fi519;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), M1[12,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), M3[12,12],h(1, 12, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), M1[12,28],h(4, 28, fi519)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), M1[12,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), M3[12,12],h(1, 12, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), M1[12,28],h(4, 28, fi519)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), M1[12,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M3[12,12],h(1, 12, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), M1[12,28],h(4, 28, fi519)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,12],h(1, 12, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M1[12,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
 )Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[28,12] ) ) * Tile( (1, 1), Tile( (4, 4), v0[12,1] ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[28,12] ) ) * Tile( (1, 1), Tile( (4, 4), M1[12,28] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-02-06
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


#define PARAM0 12
#define PARAM1 12
#define PARAM2 28

#define ERRTHRESH 1e-5

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31,
	_t19_32, _t19_33, _t19_34, _t19_35, _t19_36, _t19_37, _t19_38, _t19_39,
	_t19_40, _t19_41, _t19_42, _t19_43;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11, _t27_12, _t27_13, _t27_14, _t27_15,
	_t27_16, _t27_17, _t27_18, _t27_19, _t27_20, _t27_21, _t27_22, _t27_23,
	_t27_24, _t27_25, _t27_26, _t27_27;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23,
	_t30_24, _t30_25, _t30_26, _t30_27;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23,
	_t31_24, _t31_25, _t31_26, _t31_27;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11, _t34_12, _t34_13, _t34_14, _t34_15,
	_t34_16, _t34_17, _t34_18, _t34_19;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7,
	_t36_8, _t36_9, _t36_10, _t36_11, _t36_12, _t36_13, _t36_14, _t36_15,
	_t36_16, _t36_17, _t36_18, _t36_19, _t36_20, _t36_21, _t36_22, _t36_23,
	_t36_24, _t36_25, _t36_26, _t36_27;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19, _t37_20, _t37_21, _t37_22, _t37_23,
	_t37_24, _t37_25, _t37_26, _t37_27;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31,
	_t39_32, _t39_33, _t39_34, _t39_35, _t39_36, _t39_37, _t39_38, _t39_39,
	_t39_40, _t39_41, _t39_42, _t39_43, _t39_44, _t39_45, _t39_46, _t39_47,
	_t39_48, _t39_49, _t39_50, _t39_51, _t39_52, _t39_53, _t39_54, _t39_55;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3, _t40_4, _t40_5, _t40_6, _t40_7,
	_t40_8, _t40_9, _t40_10, _t40_11, _t40_12, _t40_13, _t40_14, _t40_15,
	_t40_16, _t40_17, _t40_18, _t40_19, _t40_20, _t40_21, _t40_22, _t40_23,
	_t40_24, _t40_25, _t40_26, _t40_27;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19, _t43_20, _t43_21, _t43_22, _t43_23,
	_t43_24, _t43_25, _t43_26, _t43_27, _t43_28, _t43_29, _t43_30, _t43_31,
	_t43_32, _t43_33, _t43_34, _t43_35;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26, _t49_27;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23,
	_t50_24, _t50_25, _t50_26, _t50_27, _t50_28, _t50_29, _t50_30, _t50_31,
	_t50_32, _t50_33, _t50_34, _t50_35, _t50_36, _t50_37, _t50_38, _t50_39,
	_t50_40, _t50_41, _t50_42, _t50_43;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15,
	_t51_16, _t51_17, _t51_18, _t51_19, _t51_20, _t51_21, _t51_22, _t51_23,
	_t51_24, _t51_25, _t51_26, _t51_27, _t51_28, _t51_29, _t51_30, _t51_31;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26, _t53_27;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39,
	_t54_40, _t54_41, _t54_42, _t54_43;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11, _t55_12, _t55_13, _t55_14, _t55_15,
	_t55_16, _t55_17, _t55_18, _t55_19, _t55_20, _t55_21, _t55_22, _t55_23,
	_t55_24, _t55_25, _t55_26, _t55_27, _t55_28, _t55_29, _t55_30, _t55_31;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3, _t56_4, _t56_5, _t56_6, _t56_7,
	_t56_8, _t56_9, _t56_10, _t56_11, _t56_12, _t56_13, _t56_14, _t56_15,
	_t56_16, _t56_17, _t56_18, _t56_19, _t56_20, _t56_21, _t56_22, _t56_23,
	_t56_24, _t56_25, _t56_26, _t56_27, _t56_28, _t56_29, _t56_30, _t56_31,
	_t56_32, _t56_33, _t56_34, _t56_35, _t56_36, _t56_37, _t56_38, _t56_39,
	_t56_40, _t56_41, _t56_42, _t56_43, _t56_44, _t56_45, _t56_46, _t56_47,
	_t56_48, _t56_49, _t56_50, _t56_51, _t56_52, _t56_53, _t56_54, _t56_55,
	_t56_56, _t56_57, _t56_58, _t56_59, _t56_60, _t56_61, _t56_62, _t56_63,
	_t56_64, _t56_65, _t56_66, _t56_67, _t56_68, _t56_69, _t56_70, _t56_71,
	_t56_72, _t56_73, _t56_74, _t56_75, _t56_76, _t56_77, _t56_78, _t56_79,
	_t56_80, _t56_81, _t56_82, _t56_83, _t56_84, _t56_85, _t56_86, _t56_87,
	_t56_88, _t56_89, _t56_90, _t56_91, _t56_92, _t56_93, _t56_94, _t56_95,
	_t56_96, _t56_97, _t56_98, _t56_99, _t56_100, _t56_101, _t56_102, _t56_103,
	_t56_104, _t56_105, _t56_106, _t56_107, _t56_108, _t56_109, _t56_110, _t56_111,
	_t56_112, _t56_113, _t56_114, _t56_115, _t56_116, _t56_117, _t56_118, _t56_119,
	_t56_120, _t56_121, _t56_122, _t56_123, _t56_124, _t56_125, _t56_126, _t56_127,
	_t56_128, _t56_129, _t56_130, _t56_131, _t56_132, _t56_133, _t56_134, _t56_135,
	_t56_136, _t56_137, _t56_138, _t56_139, _t56_140, _t56_141, _t56_142, _t56_143,
	_t56_144, _t56_145, _t56_146, _t56_147, _t56_148, _t56_149, _t56_150, _t56_151,
	_t56_152, _t56_153, _t56_154, _t56_155, _t56_156, _t56_157, _t56_158, _t56_159,
	_t56_160, _t56_161, _t56_162, _t56_163, _t56_164, _t56_165, _t56_166, _t56_167;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11, _t57_12, _t57_13, _t57_14, _t57_15,
	_t57_16, _t57_17, _t57_18, _t57_19, _t57_20, _t57_21, _t57_22, _t57_23,
	_t57_24, _t57_25, _t57_26, _t57_27, _t57_28, _t57_29, _t57_30, _t57_31,
	_t57_32, _t57_33, _t57_34, _t57_35, _t57_36, _t57_37, _t57_38, _t57_39;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9, _t58_10, _t58_11, _t58_12, _t58_13, _t58_14, _t58_15,
	_t58_16, _t58_17, _t58_18, _t58_19, _t58_20, _t58_21, _t58_22, _t58_23,
	_t58_24, _t58_25, _t58_26, _t58_27, _t58_28, _t58_29, _t58_30, _t58_31,
	_t58_32, _t58_33, _t58_34, _t58_35, _t58_36, _t58_37, _t58_38, _t58_39,
	_t58_40, _t58_41, _t58_42, _t58_43, _t58_44, _t58_45, _t58_46, _t58_47,
	_t58_48, _t58_49, _t58_50, _t58_51, _t58_52, _t58_53, _t58_54, _t58_55,
	_t58_56, _t58_57, _t58_58, _t58_59, _t58_60, _t58_61, _t58_62, _t58_63,
	_t58_64, _t58_65, _t58_66, _t58_67, _t58_68, _t58_69, _t58_70, _t58_71,
	_t58_72, _t58_73, _t58_74, _t58_75, _t58_76, _t58_77, _t58_78, _t58_79,
	_t58_80, _t58_81, _t58_82, _t58_83, _t58_84, _t58_85, _t58_86, _t58_87,
	_t58_88, _t58_89, _t58_90, _t58_91, _t58_92, _t58_93, _t58_94, _t58_95,
	_t58_96, _t58_97, _t58_98, _t58_99, _t58_100, _t58_101, _t58_102, _t58_103,
	_t58_104, _t58_105, _t58_106, _t58_107, _t58_108, _t58_109, _t58_110, _t58_111,
	_t58_112, _t58_113, _t58_114, _t58_115, _t58_116, _t58_117, _t58_118, _t58_119,
	_t58_120, _t58_121, _t58_122, _t58_123, _t58_124, _t58_125, _t58_126, _t58_127,
	_t58_128, _t58_129, _t58_130, _t58_131, _t58_132, _t58_133, _t58_134, _t58_135,
	_t58_136, _t58_137, _t58_138, _t58_139, _t58_140, _t58_141, _t58_142, _t58_143,
	_t58_144, _t58_145, _t58_146, _t58_147, _t58_148, _t58_149, _t58_150, _t58_151,
	_t58_152, _t58_153, _t58_154, _t58_155, _t58_156, _t58_157, _t58_158, _t58_159,
	_t58_160, _t58_161, _t58_162, _t58_163, _t58_164, _t58_165, _t58_166, _t58_167,
	_t58_168, _t58_169, _t58_170, _t58_171, _t58_172, _t58_173, _t58_174, _t58_175,
	_t58_176, _t58_177, _t58_178, _t58_179, _t58_180, _t58_181, _t58_182, _t58_183,
	_t58_184, _t58_185, _t58_186, _t58_187, _t58_188, _t58_189, _t58_190, _t58_191,
	_t58_192, _t58_193, _t58_194, _t58_195, _t58_196, _t58_197, _t58_198, _t58_199,
	_t58_200, _t58_201, _t58_202, _t58_203, _t58_204, _t58_205, _t58_206, _t58_207,
	_t58_208, _t58_209, _t58_210, _t58_211, _t58_212, _t58_213, _t58_214, _t58_215;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26, _t59_27, _t59_28, _t59_29, _t59_30, _t59_31,
	_t59_32, _t59_33, _t59_34, _t59_35, _t59_36, _t59_37, _t59_38, _t59_39,
	_t59_40, _t59_41, _t59_42, _t59_43, _t59_44;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11, _t61_12, _t61_13, _t61_14, _t61_15,
	_t61_16, _t61_17, _t61_18, _t61_19, _t61_20, _t61_21, _t61_22, _t61_23,
	_t61_24, _t61_25, _t61_26, _t61_27, _t61_28, _t61_29, _t61_30, _t61_31,
	_t61_32, _t61_33, _t61_34, _t61_35, _t61_36, _t61_37;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19, _t62_20, _t62_21, _t62_22, _t62_23,
	_t62_24, _t62_25, _t62_26, _t62_27, _t62_28, _t62_29, _t62_30, _t62_31,
	_t62_32, _t62_33, _t62_34, _t62_35, _t62_36, _t62_37, _t62_38, _t62_39,
	_t62_40, _t62_41;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15,
	_t64_16, _t64_17, _t64_18, _t64_19, _t64_20, _t64_21, _t64_22, _t64_23,
	_t64_24, _t64_25, _t64_26, _t64_27, _t64_28, _t64_29, _t64_30, _t64_31,
	_t64_32, _t64_33, _t64_34, _t64_35;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15,
	_t65_16, _t65_17, _t65_18, _t65_19, _t65_20, _t65_21, _t65_22, _t65_23,
	_t65_24, _t65_25, _t65_26, _t65_27, _t65_28, _t65_29, _t65_30, _t65_31,
	_t65_32, _t65_33, _t65_34, _t65_35, _t65_36, _t65_37, _t65_38, _t65_39,
	_t65_40, _t65_41, _t65_42, _t65_43, _t65_44, _t65_45, _t65_46, _t65_47,
	_t65_48, _t65_49, _t65_50, _t65_51, _t65_52, _t65_53, _t65_54, _t65_55,
	_t65_56, _t65_57, _t65_58, _t65_59, _t65_60, _t65_61, _t65_62, _t65_63,
	_t65_64, _t65_65, _t65_66, _t65_67, _t65_68, _t65_69, _t65_70, _t65_71,
	_t65_72, _t65_73;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26, _t66_27, _t66_28;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15,
	_t67_16, _t67_17, _t67_18, _t67_19, _t67_20, _t67_21, _t67_22, _t67_23,
	_t67_24, _t67_25, _t67_26, _t67_27, _t67_28, _t67_29, _t67_30, _t67_31,
	_t67_32, _t67_33, _t67_34, _t67_35;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15,
	_t68_16, _t68_17, _t68_18, _t68_19, _t68_20, _t68_21, _t68_22, _t68_23,
	_t68_24, _t68_25, _t68_26, _t68_27, _t68_28, _t68_29, _t68_30, _t68_31,
	_t68_32, _t68_33, _t68_34, _t68_35, _t68_36, _t68_37, _t68_38, _t68_39,
	_t68_40, _t68_41, _t68_42, _t68_43, _t68_44, _t68_45, _t68_46, _t68_47,
	_t68_48, _t68_49, _t68_50, _t68_51, _t68_52, _t68_53, _t68_54, _t68_55,
	_t68_56, _t68_57;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23,
	_t69_24, _t69_25, _t69_26, _t69_27, _t69_28;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6, _t70_7,
	_t70_8, _t70_9, _t70_10, _t70_11, _t70_12, _t70_13, _t70_14, _t70_15,
	_t70_16, _t70_17, _t70_18, _t70_19, _t70_20, _t70_21, _t70_22, _t70_23,
	_t70_24, _t70_25, _t70_26, _t70_27, _t70_28, _t70_29, _t70_30, _t70_31,
	_t70_32, _t70_33, _t70_34, _t70_35, _t70_36, _t70_37, _t70_38, _t70_39,
	_t70_40, _t70_41, _t70_42, _t70_43, _t70_44, _t70_45, _t70_46, _t70_47,
	_t70_48, _t70_49, _t70_50, _t70_51, _t70_52, _t70_53, _t70_54, _t70_55,
	_t70_56, _t70_57, _t70_58, _t70_59, _t70_60, _t70_61, _t70_62, _t70_63,
	_t70_64, _t70_65, _t70_66, _t70_67, _t70_68, _t70_69, _t70_70;
  __m256d _t71_0, _t71_1, _t71_2, _t71_3, _t71_4, _t71_5, _t71_6, _t71_7,
	_t71_8, _t71_9, _t71_10, _t71_11, _t71_12, _t71_13, _t71_14, _t71_15,
	_t71_16, _t71_17, _t71_18, _t71_19, _t71_20, _t71_21, _t71_22, _t71_23,
	_t71_24, _t71_25, _t71_26, _t71_27, _t71_28, _t71_29, _t71_30, _t71_31,
	_t71_32, _t71_33, _t71_34;
  __m256d _t72_0, _t72_1, _t72_2, _t72_3, _t72_4, _t72_5, _t72_6, _t72_7,
	_t72_8, _t72_9, _t72_10, _t72_11, _t72_12, _t72_13, _t72_14, _t72_15,
	_t72_16, _t72_17, _t72_18, _t72_19, _t72_20, _t72_21, _t72_22, _t72_23,
	_t72_24, _t72_25, _t72_26, _t72_27;
  __m256d _t73_0, _t73_1, _t73_2, _t73_3, _t73_4, _t73_5, _t73_6, _t73_7,
	_t73_8, _t73_9, _t73_10, _t73_11, _t73_12, _t73_13, _t73_14, _t73_15,
	_t73_16, _t73_17, _t73_18, _t73_19, _t73_20, _t73_21, _t73_22, _t73_23,
	_t73_24, _t73_25, _t73_26, _t73_27, _t73_28, _t73_29, _t73_30, _t73_31,
	_t73_32, _t73_33, _t73_34, _t73_35, _t73_36, _t73_37, _t73_38, _t73_39,
	_t73_40, _t73_41, _t73_42, _t73_43, _t73_44, _t73_45, _t73_46, _t73_47,
	_t73_48, _t73_49, _t73_50, _t73_51, _t73_52, _t73_53, _t73_54, _t73_55,
	_t73_56;
  __m256d _t74_0, _t74_1, _t74_2, _t74_3, _t74_4, _t74_5, _t74_6, _t74_7,
	_t74_8, _t74_9, _t74_10, _t74_11, _t74_12, _t74_13, _t74_14, _t74_15,
	_t74_16, _t74_17, _t74_18, _t74_19, _t74_20, _t74_21, _t74_22, _t74_23,
	_t74_24, _t74_25, _t74_26, _t74_27, _t74_28, _t74_29, _t74_30, _t74_31,
	_t74_32, _t74_33, _t74_34, _t74_35;
  __m256d _t75_0, _t75_1, _t75_2, _t75_3, _t75_4, _t75_5, _t75_6;
  __m256d _t76_0, _t76_1, _t76_2, _t76_3, _t76_4, _t76_5;
  __m256d _t77_0, _t77_1, _t77_2, _t77_3, _t77_4, _t77_5, _t77_6, _t77_7,
	_t77_8, _t77_9, _t77_10, _t77_11, _t77_12, _t77_13, _t77_14, _t77_15,
	_t77_16, _t77_17, _t77_18, _t77_19, _t77_20, _t77_21, _t77_22, _t77_23,
	_t77_24, _t77_25, _t77_26, _t77_27, _t77_28, _t77_29, _t77_30, _t77_31,
	_t77_32, _t77_33, _t77_34, _t77_35, _t77_36, _t77_37, _t77_38, _t77_39;
  __m256d _t78_0, _t78_1, _t78_2, _t78_3, _t78_4, _t78_5, _t78_6, _t78_7,
	_t78_8, _t78_9, _t78_10, _t78_11, _t78_12, _t78_13, _t78_14, _t78_15,
	_t78_16, _t78_17, _t78_18, _t78_19, _t78_20, _t78_21, _t78_22, _t78_23,
	_t78_24, _t78_25, _t78_26, _t78_27;
  __m256d _t79_0, _t79_1, _t79_2, _t79_3, _t79_4, _t79_5, _t79_6, _t79_7,
	_t79_8, _t79_9, _t79_10, _t79_11, _t79_12, _t79_13, _t79_14, _t79_15;
  __m256d _t80_0, _t80_1, _t80_2, _t80_3, _t80_4, _t80_5, _t80_6, _t80_7,
	_t80_8, _t80_9, _t80_10, _t80_11, _t80_12, _t80_13, _t80_14, _t80_15,
	_t80_16, _t80_17, _t80_18, _t80_19, _t80_20, _t80_21, _t80_22, _t80_23;
  __m256d _t81_0, _t81_1, _t81_2, _t81_3, _t81_4, _t81_5, _t81_6, _t81_7,
	_t81_8, _t81_9, _t81_10, _t81_11, _t81_12, _t81_13, _t81_14, _t81_15,
	_t81_16, _t81_17, _t81_18, _t81_19, _t81_20, _t81_21, _t81_22, _t81_23,
	_t81_24, _t81_25, _t81_26, _t81_27, _t81_28, _t81_29, _t81_30, _t81_31;
  __m256d _t82_0, _t82_1, _t82_2, _t82_3, _t82_4, _t82_5, _t82_6, _t82_7,
	_t82_8, _t82_9, _t82_10, _t82_11, _t82_12, _t82_13, _t82_14, _t82_15,
	_t82_16, _t82_17, _t82_18, _t82_19, _t82_20, _t82_21, _t82_22, _t82_23,
	_t82_24, _t82_25, _t82_26, _t82_27;


  // Generating : y[28,1] = Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), x[28,1],h(1, 1, 0)) ) + ( G(h(4, 28, i0), B[28,12],h(4, 12, 0)) * G(h(4, 12, 0), u[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), x[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + Sum_{k3} ( $(h(4, 28, i0), ( G(h(4, 28, i0), B[28,12],h(4, 12, k3)) * G(h(4, 12, k3), u[12,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t0_9 = _mm256_loadu_pd(F + 28*i0);
    _t0_8 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t0_7 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t0_6 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t0_5 = _mm256_loadu_pd(x);
    _t0_4 = _mm256_loadu_pd(B + 12*i0);
    _t0_3 = _mm256_loadu_pd(B + 12*i0 + 12);
    _t0_2 = _mm256_loadu_pd(B + 12*i0 + 24);
    _t0_1 = _mm256_loadu_pd(B + 12*i0 + 36);
    _t0_0 = _mm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t1_4 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t1_3 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t1_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t1_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);
      _t1_0 = _mm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t0_10 = _mm256_add_pd(_t0_10, _t1_5);

      // AVX Storer:
    }

    for( int k3 = 4; k3 <= 11; k3+=4 ) {
      _t2_4 = _mm256_loadu_pd(B + 12*i0 + k3);
      _t2_3 = _mm256_loadu_pd(B + 12*i0 + k3 + 12);
      _t2_2 = _mm256_loadu_pd(B + 12*i0 + k3 + 24);
      _t2_1 = _mm256_loadu_pd(B + 12*i0 + k3 + 36);
      _t2_0 = _mm256_loadu_pd(u + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t0_10 = _mm256_add_pd(_t0_10, _t2_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(y + i0, _t0_10);
  }

  _t3_11 = _mm256_loadu_pd(P);
  _t3_10 = _mm256_maskload_pd(P + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_9 = _mm256_maskload_pd(P + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_8 = _mm256_maskload_pd(P + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t3_7 = _mm256_loadu_pd(P + 116);
  _t3_6 = _mm256_maskload_pd(P + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_5 = _mm256_maskload_pd(P + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_4 = _mm256_maskload_pd(P + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t3_3 = _mm256_loadu_pd(P + 696);
  _t3_2 = _mm256_maskload_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[28,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, 0), P[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 4)) * G(h(4, 28, 4), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, 4), P[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{k3} ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k3)) * G(h(4, 28, k3), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, k3), P[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 24)) * G(h(4, 28, 24), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_12 = _t3_11;
  _t3_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 3), _t3_10, 12);
  _t3_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 0), _t3_9, 49);
  _t3_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 12), _mm256_shuffle_pd(_t3_9, _t3_8, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_16 = _t3_7;
  _t3_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 3), _t3_6, 12);
  _t3_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 0), _t3_5, 49);
  _t3_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 12), _mm256_shuffle_pd(_t3_5, _t3_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_20 = _t3_3;
  _t3_21 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_22 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 28*i0);
    _t4_14 = _mm256_broadcast_sd(F + 28*i0 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 28*i0 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 28*i0 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 28*i0 + 28);
    _t4_10 = _mm256_broadcast_sd(F + 28*i0 + 29);
    _t4_9 = _mm256_broadcast_sd(F + 28*i0 + 30);
    _t4_8 = _mm256_broadcast_sd(F + 28*i0 + 31);
    _t4_7 = _mm256_broadcast_sd(F + 28*i0 + 56);
    _t4_6 = _mm256_broadcast_sd(F + 28*i0 + 57);
    _t4_5 = _mm256_broadcast_sd(F + 28*i0 + 58);
    _t4_4 = _mm256_broadcast_sd(F + 28*i0 + 59);
    _t4_3 = _mm256_broadcast_sd(F + 28*i0 + 84);
    _t4_2 = _mm256_broadcast_sd(F + 28*i0 + 85);
    _t4_1 = _mm256_broadcast_sd(F + 28*i0 + 86);
    _t4_0 = _mm256_broadcast_sd(F + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_12), _mm256_mul_pd(_t4_14, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_14), _mm256_mul_pd(_t4_12, _t3_15)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_12), _mm256_mul_pd(_t4_10, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_14), _mm256_mul_pd(_t4_8, _t3_15)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_12), _mm256_mul_pd(_t4_6, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_14), _mm256_mul_pd(_t4_4, _t3_15)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_12), _mm256_mul_pd(_t4_2, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_14), _mm256_mul_pd(_t4_0, _t3_15)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t5_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t5_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t5_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t5_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t5_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t5_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t5_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t5_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t5_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t5_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t5_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t5_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t5_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t5_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t5_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t5_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t5_3 = _mm256_loadu_pd(P + k2);
      _t5_2 = _mm256_loadu_pd(P + k2 + 28);
      _t5_1 = _mm256_loadu_pd(P + k2 + 56);
      _t5_0 = _mm256_loadu_pd(P + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t5_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 32);
      _t5_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 32);
      _t5_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 49);
      _t5_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t5_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_19, _t5_24), _mm256_mul_pd(_t5_18, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_17, _t5_26), _mm256_mul_pd(_t5_16, _t5_27)));
      _t5_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t5_24), _mm256_mul_pd(_t5_14, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t5_26), _mm256_mul_pd(_t5_12, _t5_27)));
      _t5_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t5_24), _mm256_mul_pd(_t5_10, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t5_26), _mm256_mul_pd(_t5_8, _t5_27)));
      _t5_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t5_24), _mm256_mul_pd(_t5_6, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t5_26), _mm256_mul_pd(_t5_4, _t5_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t4_16 = _mm256_add_pd(_t4_16, _t5_20);
      _t4_17 = _mm256_add_pd(_t4_17, _t5_21);
      _t4_18 = _mm256_add_pd(_t4_18, _t5_22);
      _t4_19 = _mm256_add_pd(_t4_19, _t5_23);

      // AVX Storer:
    }
    _t6_19 = _mm256_loadu_pd(P + 4);
    _t6_18 = _mm256_loadu_pd(P + 32);
    _t6_17 = _mm256_loadu_pd(P + 60);
    _t6_16 = _mm256_loadu_pd(P + 88);
    _t6_15 = _mm256_broadcast_sd(F + 28*i0 + 4);
    _t6_14 = _mm256_broadcast_sd(F + 28*i0 + 5);
    _t6_13 = _mm256_broadcast_sd(F + 28*i0 + 6);
    _t6_12 = _mm256_broadcast_sd(F + 28*i0 + 7);
    _t6_11 = _mm256_broadcast_sd(F + 28*i0 + 32);
    _t6_10 = _mm256_broadcast_sd(F + 28*i0 + 33);
    _t6_9 = _mm256_broadcast_sd(F + 28*i0 + 34);
    _t6_8 = _mm256_broadcast_sd(F + 28*i0 + 35);
    _t6_7 = _mm256_broadcast_sd(F + 28*i0 + 60);
    _t6_6 = _mm256_broadcast_sd(F + 28*i0 + 61);
    _t6_5 = _mm256_broadcast_sd(F + 28*i0 + 62);
    _t6_4 = _mm256_broadcast_sd(F + 28*i0 + 63);
    _t6_3 = _mm256_broadcast_sd(F + 28*i0 + 88);
    _t6_2 = _mm256_broadcast_sd(F + 28*i0 + 89);
    _t6_1 = _mm256_broadcast_sd(F + 28*i0 + 90);
    _t6_0 = _mm256_broadcast_sd(F + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t6_19), _mm256_mul_pd(_t4_14, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t6_17), _mm256_mul_pd(_t4_12, _t6_16)));
    _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t6_19), _mm256_mul_pd(_t4_10, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t6_17), _mm256_mul_pd(_t4_8, _t6_16)));
    _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t6_19), _mm256_mul_pd(_t4_6, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t6_17), _mm256_mul_pd(_t4_4, _t6_16)));
    _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t6_19), _mm256_mul_pd(_t4_2, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t6_17), _mm256_mul_pd(_t4_0, _t6_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t6_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t3_16), _mm256_mul_pd(_t6_14, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t3_18), _mm256_mul_pd(_t6_12, _t3_19)));
    _t6_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t3_16), _mm256_mul_pd(_t6_10, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t3_18), _mm256_mul_pd(_t6_8, _t3_19)));
    _t6_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t3_16), _mm256_mul_pd(_t6_6, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t3_18), _mm256_mul_pd(_t6_4, _t3_19)));
    _t6_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_3, _t3_16), _mm256_mul_pd(_t6_2, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_1, _t3_18), _mm256_mul_pd(_t6_0, _t3_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t6_20 = _mm256_add_pd(_t6_20, _t6_24);
    _t6_21 = _mm256_add_pd(_t6_21, _t6_25);
    _t6_22 = _mm256_add_pd(_t6_22, _t6_26);
    _t6_23 = _mm256_add_pd(_t6_23, _t6_27);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t7_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t7_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t7_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t7_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t7_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t7_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t7_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t7_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t7_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t7_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t7_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t7_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t7_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t7_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t7_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t7_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t7_3 = _mm256_loadu_pd(P + k2 + 112);
      _t7_2 = _mm256_loadu_pd(P + k2 + 140);
      _t7_1 = _mm256_loadu_pd(P + k2 + 168);
      _t7_0 = _mm256_loadu_pd(P + k2 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t7_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
      _t7_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
      _t7_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
      _t7_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t7_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_24), _mm256_mul_pd(_t7_18, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_26), _mm256_mul_pd(_t7_16, _t7_27)));
      _t7_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_24), _mm256_mul_pd(_t7_14, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_26), _mm256_mul_pd(_t7_12, _t7_27)));
      _t7_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_24), _mm256_mul_pd(_t7_10, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_26), _mm256_mul_pd(_t7_8, _t7_27)));
      _t7_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_24), _mm256_mul_pd(_t7_6, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_26), _mm256_mul_pd(_t7_4, _t7_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t6_20 = _mm256_add_pd(_t6_20, _t7_20);
      _t6_21 = _mm256_add_pd(_t6_21, _t7_21);
      _t6_22 = _mm256_add_pd(_t6_22, _t7_22);
      _t6_23 = _mm256_add_pd(_t6_23, _t7_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k3 = 8; k3 <= 23; k3+=4 ) {
      _t8_3 = _mm256_loadu_pd(P + k3);
      _t8_2 = _mm256_loadu_pd(P + k3 + 28);
      _t8_1 = _mm256_loadu_pd(P + k3 + 56);
      _t8_0 = _mm256_loadu_pd(P + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t8_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t8_3), _mm256_mul_pd(_t4_14, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t8_1), _mm256_mul_pd(_t4_12, _t8_0)));
      _t8_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t8_3), _mm256_mul_pd(_t4_10, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t8_1), _mm256_mul_pd(_t4_8, _t8_0)));
      _t8_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t8_3), _mm256_mul_pd(_t4_6, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t8_1), _mm256_mul_pd(_t4_4, _t8_0)));
      _t8_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t8_3), _mm256_mul_pd(_t4_2, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t8_1), _mm256_mul_pd(_t4_0, _t8_0)));

      // AVX Storer:

      for( int k2 = 4; k2 <= k3 - 1; k2+=4 ) {
        _t9_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
        _t9_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
        _t9_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
        _t9_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
        _t9_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
        _t9_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
        _t9_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
        _t9_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
        _t9_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
        _t9_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
        _t9_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
        _t9_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
        _t9_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
        _t9_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
        _t9_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
        _t9_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
        _t9_3 = _mm256_loadu_pd(P + 28*k2 + k3);
        _t9_2 = _mm256_loadu_pd(P + 28*k2 + k3 + 28);
        _t9_1 = _mm256_loadu_pd(P + 28*k2 + k3 + 56);
        _t9_0 = _mm256_loadu_pd(P + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_3), _mm256_mul_pd(_t9_18, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_1), _mm256_mul_pd(_t9_16, _t9_0)));
        _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_3), _mm256_mul_pd(_t9_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_1), _mm256_mul_pd(_t9_12, _t9_0)));
        _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_3), _mm256_mul_pd(_t9_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_1), _mm256_mul_pd(_t9_8, _t9_0)));
        _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_3), _mm256_mul_pd(_t9_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_1), _mm256_mul_pd(_t9_4, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t8_4 = _mm256_add_pd(_t8_4, _t9_20);
        _t8_5 = _mm256_add_pd(_t8_5, _t9_21);
        _t8_6 = _mm256_add_pd(_t8_6, _t9_22);
        _t8_7 = _mm256_add_pd(_t8_7, _t9_23);

        // AVX Storer:
      }
      _t10_19 = _mm256_broadcast_sd(F + 28*i0 + k3);
      _t10_18 = _mm256_broadcast_sd(F + 28*i0 + k3 + 1);
      _t10_17 = _mm256_broadcast_sd(F + 28*i0 + k3 + 2);
      _t10_16 = _mm256_broadcast_sd(F + 28*i0 + k3 + 3);
      _t10_15 = _mm256_broadcast_sd(F + 28*i0 + k3 + 28);
      _t10_14 = _mm256_broadcast_sd(F + 28*i0 + k3 + 29);
      _t10_13 = _mm256_broadcast_sd(F + 28*i0 + k3 + 30);
      _t10_12 = _mm256_broadcast_sd(F + 28*i0 + k3 + 31);
      _t10_11 = _mm256_broadcast_sd(F + 28*i0 + k3 + 56);
      _t10_10 = _mm256_broadcast_sd(F + 28*i0 + k3 + 57);
      _t10_9 = _mm256_broadcast_sd(F + 28*i0 + k3 + 58);
      _t10_8 = _mm256_broadcast_sd(F + 28*i0 + k3 + 59);
      _t10_7 = _mm256_broadcast_sd(F + 28*i0 + k3 + 84);
      _t10_6 = _mm256_broadcast_sd(F + 28*i0 + k3 + 85);
      _t10_5 = _mm256_broadcast_sd(F + 28*i0 + k3 + 86);
      _t10_4 = _mm256_broadcast_sd(F + 28*i0 + k3 + 87);
      _t10_3 = _mm256_loadu_pd(P + 29*k3);
      _t10_2 = _mm256_maskload_pd(P + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t10_1 = _mm256_maskload_pd(P + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t10_0 = _mm256_maskload_pd(P + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t10_24 = _t10_3;
      _t10_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
      _t10_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
      _t10_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t10_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_24), _mm256_mul_pd(_t10_18, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_26), _mm256_mul_pd(_t10_16, _t10_27)));
      _t10_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_24), _mm256_mul_pd(_t10_14, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_26), _mm256_mul_pd(_t10_12, _t10_27)));
      _t10_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_24), _mm256_mul_pd(_t10_10, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_26), _mm256_mul_pd(_t10_8, _t10_27)));
      _t10_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_24), _mm256_mul_pd(_t10_6, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t10_26), _mm256_mul_pd(_t10_4, _t10_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_4 = _mm256_add_pd(_t8_4, _t10_20);
      _t8_5 = _mm256_add_pd(_t8_5, _t10_21);
      _t8_6 = _mm256_add_pd(_t8_6, _t10_22);
      _t8_7 = _mm256_add_pd(_t8_7, _t10_23);

      // AVX Storer:

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t11_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
        _t11_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
        _t11_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
        _t11_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
        _t11_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
        _t11_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
        _t11_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
        _t11_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
        _t11_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
        _t11_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
        _t11_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
        _t11_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
        _t11_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
        _t11_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
        _t11_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
        _t11_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
        _t11_3 = _mm256_loadu_pd(P + k2 + 28*k3);
        _t11_2 = _mm256_loadu_pd(P + k2 + 28*k3 + 28);
        _t11_1 = _mm256_loadu_pd(P + k2 + 28*k3 + 56);
        _t11_0 = _mm256_loadu_pd(P + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t11_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
        _t11_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
        _t11_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
        _t11_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t11_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_24), _mm256_mul_pd(_t11_18, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_26), _mm256_mul_pd(_t11_16, _t11_27)));
        _t11_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_24), _mm256_mul_pd(_t11_14, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_26), _mm256_mul_pd(_t11_12, _t11_27)));
        _t11_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_24), _mm256_mul_pd(_t11_10, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_26), _mm256_mul_pd(_t11_8, _t11_27)));
        _t11_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_24), _mm256_mul_pd(_t11_6, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_26), _mm256_mul_pd(_t11_4, _t11_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t8_4 = _mm256_add_pd(_t8_4, _t11_20);
        _t8_5 = _mm256_add_pd(_t8_5, _t11_21);
        _t8_6 = _mm256_add_pd(_t8_6, _t11_22);
        _t8_7 = _mm256_add_pd(_t8_7, _t11_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M0 + 28*i0 + k3, _t8_4);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 28, _t8_5);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 56, _t8_6);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 84, _t8_7);
    }
    _t12_3 = _mm256_loadu_pd(P + 24);
    _t12_2 = _mm256_loadu_pd(P + 52);
    _t12_1 = _mm256_loadu_pd(P + 80);
    _t12_0 = _mm256_loadu_pd(P + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t12_3), _mm256_mul_pd(_t4_14, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t12_1), _mm256_mul_pd(_t4_12, _t12_0)));
    _t12_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t12_3), _mm256_mul_pd(_t4_10, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t12_1), _mm256_mul_pd(_t4_8, _t12_0)));
    _t12_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t12_3), _mm256_mul_pd(_t4_6, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t12_1), _mm256_mul_pd(_t4_4, _t12_0)));
    _t12_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t12_3), _mm256_mul_pd(_t4_2, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t12_1), _mm256_mul_pd(_t4_0, _t12_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t13_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t13_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t13_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t13_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t13_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t13_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t13_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t13_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t13_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t13_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t13_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t13_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t13_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t13_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t13_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t13_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t13_3 = _mm256_loadu_pd(P + 28*k2 + 24);
      _t13_2 = _mm256_loadu_pd(P + 28*k2 + 52);
      _t13_1 = _mm256_loadu_pd(P + 28*k2 + 80);
      _t13_0 = _mm256_loadu_pd(P + 28*k2 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t13_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_3), _mm256_mul_pd(_t13_18, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_1), _mm256_mul_pd(_t13_16, _t13_0)));
      _t13_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_3), _mm256_mul_pd(_t13_14, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_1), _mm256_mul_pd(_t13_12, _t13_0)));
      _t13_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_3), _mm256_mul_pd(_t13_10, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_1), _mm256_mul_pd(_t13_8, _t13_0)));
      _t13_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_3), _mm256_mul_pd(_t13_6, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_1), _mm256_mul_pd(_t13_4, _t13_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t12_4 = _mm256_add_pd(_t12_4, _t13_20);
      _t12_5 = _mm256_add_pd(_t12_5, _t13_21);
      _t12_6 = _mm256_add_pd(_t12_6, _t13_22);
      _t12_7 = _mm256_add_pd(_t12_7, _t13_23);

      // AVX Storer:
    }
    _t14_15 = _mm256_broadcast_sd(F + 28*i0 + 24);
    _t14_14 = _mm256_broadcast_sd(F + 28*i0 + 25);
    _t14_13 = _mm256_broadcast_sd(F + 28*i0 + 26);
    _t14_12 = _mm256_broadcast_sd(F + 28*i0 + 27);
    _t14_11 = _mm256_broadcast_sd(F + 28*i0 + 52);
    _t14_10 = _mm256_broadcast_sd(F + 28*i0 + 53);
    _t14_9 = _mm256_broadcast_sd(F + 28*i0 + 54);
    _t14_8 = _mm256_broadcast_sd(F + 28*i0 + 55);
    _t14_7 = _mm256_broadcast_sd(F + 28*i0 + 80);
    _t14_6 = _mm256_broadcast_sd(F + 28*i0 + 81);
    _t14_5 = _mm256_broadcast_sd(F + 28*i0 + 82);
    _t14_4 = _mm256_broadcast_sd(F + 28*i0 + 83);
    _t14_3 = _mm256_broadcast_sd(F + 28*i0 + 108);
    _t14_2 = _mm256_broadcast_sd(F + 28*i0 + 109);
    _t14_1 = _mm256_broadcast_sd(F + 28*i0 + 110);
    _t14_0 = _mm256_broadcast_sd(F + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t14_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t3_20), _mm256_mul_pd(_t14_14, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t3_22), _mm256_mul_pd(_t14_12, _t3_23)));
    _t14_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t3_20), _mm256_mul_pd(_t14_10, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t3_22), _mm256_mul_pd(_t14_8, _t3_23)));
    _t14_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_7, _t3_20), _mm256_mul_pd(_t14_6, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_5, _t3_22), _mm256_mul_pd(_t14_4, _t3_23)));
    _t14_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _t3_20), _mm256_mul_pd(_t14_2, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_1, _t3_22), _mm256_mul_pd(_t14_0, _t3_23)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_4 = _mm256_add_pd(_t12_4, _t14_16);
    _t12_5 = _mm256_add_pd(_t12_5, _t14_17);
    _t12_6 = _mm256_add_pd(_t12_6, _t14_18);
    _t12_7 = _mm256_add_pd(_t12_7, _t14_19);

    // AVX Storer:
    _mm256_storeu_pd(M0 + 28*i0, _t4_16);
    _mm256_storeu_pd(M0 + 28*i0 + 28, _t4_17);
    _mm256_storeu_pd(M0 + 28*i0 + 56, _t4_18);
    _mm256_storeu_pd(M0 + 28*i0 + 84, _t4_19);
    _mm256_storeu_pd(M0 + 28*i0 + 4, _t6_20);
    _mm256_storeu_pd(M0 + 28*i0 + 32, _t6_21);
    _mm256_storeu_pd(M0 + 28*i0 + 60, _t6_22);
    _mm256_storeu_pd(M0 + 28*i0 + 88, _t6_23);
    _mm256_storeu_pd(M0 + 28*i0 + 24, _t12_4);
    _mm256_storeu_pd(M0 + 28*i0 + 52, _t12_5);
    _mm256_storeu_pd(M0 + 28*i0 + 80, _t12_6);
    _mm256_storeu_pd(M0 + 28*i0 + 108, _t12_7);
  }


  // Generating : Y[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, i0)) ),h(4, 28, i0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) ) ),h(4, 28, i0)) ) ) + Sum_{k3} ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, k3), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, k3), F[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) ) + S(h(4, 28, 24), ( ( G(h(4, 28, 24), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, 24), Q[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 28, 24), ( G(h(4, 28, 24), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, k2)) ) ),h(4, 28, 24)) ) )


  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t15_23 = _mm256_broadcast_sd(M0 + 28*i0);
    _t15_22 = _mm256_broadcast_sd(M0 + 28*i0 + 1);
    _t15_21 = _mm256_broadcast_sd(M0 + 28*i0 + 2);
    _t15_20 = _mm256_broadcast_sd(M0 + 28*i0 + 3);
    _t15_19 = _mm256_broadcast_sd(M0 + 28*i0 + 28);
    _t15_18 = _mm256_broadcast_sd(M0 + 28*i0 + 29);
    _t15_17 = _mm256_broadcast_sd(M0 + 28*i0 + 30);
    _t15_16 = _mm256_broadcast_sd(M0 + 28*i0 + 31);
    _t15_15 = _mm256_broadcast_sd(M0 + 28*i0 + 56);
    _t15_14 = _mm256_broadcast_sd(M0 + 28*i0 + 57);
    _t15_13 = _mm256_broadcast_sd(M0 + 28*i0 + 58);
    _t15_12 = _mm256_broadcast_sd(M0 + 28*i0 + 59);
    _t15_11 = _mm256_broadcast_sd(M0 + 28*i0 + 84);
    _t15_10 = _mm256_broadcast_sd(M0 + 28*i0 + 85);
    _t15_9 = _mm256_broadcast_sd(M0 + 28*i0 + 86);
    _t15_8 = _mm256_broadcast_sd(M0 + 28*i0 + 87);
    _t15_7 = _mm256_loadu_pd(F + 28*i0);
    _t15_6 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t15_5 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t15_4 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t15_3 = _mm256_loadu_pd(Q + 29*i0);
    _t15_2 = _mm256_maskload_pd(Q + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t15_1 = _mm256_maskload_pd(Q + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t15_0 = _mm256_maskload_pd(Q + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
    _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
    _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
    _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
    _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
    _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
    _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t15_36 = _t15_3;
    _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
    _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
    _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
    _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
    _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
    _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t15_24;
    _t15_29 = _t15_25;
    _t15_30 = _t15_26;
    _t15_31 = _t15_27;

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 28*i0 + k2);
      _t16_18 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 28);
      _t16_14 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 29);
      _t16_13 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 30);
      _t16_12 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 31);
      _t16_11 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 56);
      _t16_10 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 57);
      _t16_9 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 58);
      _t16_8 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 59);
      _t16_7 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 84);
      _t16_6 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 85);
      _t16_5 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 86);
      _t16_4 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 87);
      _t16_3 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t16_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t16_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t16_0 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_28), _mm256_mul_pd(_t16_18, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_30), _mm256_mul_pd(_t16_16, _t16_31)));
      _t16_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_28), _mm256_mul_pd(_t16_14, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_30), _mm256_mul_pd(_t16_12, _t16_31)));
      _t16_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_28), _mm256_mul_pd(_t16_10, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_30), _mm256_mul_pd(_t16_8, _t16_31)));
      _t16_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_28), _mm256_mul_pd(_t16_6, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_30), _mm256_mul_pd(_t16_4, _t16_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_24 = _t15_28;
      _t16_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
      _t16_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
      _t16_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_24 = _mm256_add_pd(_t16_24, _t16_20);
      _t16_25 = _mm256_add_pd(_t16_25, _t16_21);
      _t16_26 = _mm256_add_pd(_t16_26, _t16_22);
      _t16_27 = _mm256_add_pd(_t16_27, _t16_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t15_28 = _t16_24;
      _t15_29 = _t16_25;
      _t15_30 = _t16_26;
      _t15_31 = _t16_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 27; k3+=4 ) {
      _t17_7 = _mm256_loadu_pd(F + 28*k3);
      _t17_6 = _mm256_loadu_pd(F + 28*k3 + 28);
      _t17_5 = _mm256_loadu_pd(F + 28*k3 + 56);
      _t17_4 = _mm256_loadu_pd(F + 28*k3 + 84);
      _t17_3 = _mm256_loadu_pd(Q + 28*i0 + k3);
      _t17_2 = _mm256_loadu_pd(Q + 28*i0 + k3 + 28);
      _t17_1 = _mm256_loadu_pd(Q + 28*i0 + k3 + 56);
      _t17_0 = _mm256_loadu_pd(Q + 28*i0 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t17_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_6), _mm256_unpacklo_pd(_t17_5, _t17_4), 32);
      _t17_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_7, _t17_6), _mm256_unpackhi_pd(_t17_5, _t17_4), 32);
      _t17_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_6), _mm256_unpacklo_pd(_t17_5, _t17_4), 49);
      _t17_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_7, _t17_6), _mm256_unpackhi_pd(_t17_5, _t17_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t17_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t17_16), _mm256_mul_pd(_t15_22, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t17_18), _mm256_mul_pd(_t15_20, _t17_19)));
      _t17_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t17_16), _mm256_mul_pd(_t15_18, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t17_18), _mm256_mul_pd(_t15_16, _t17_19)));
      _t17_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t17_16), _mm256_mul_pd(_t15_14, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t17_18), _mm256_mul_pd(_t15_12, _t17_19)));
      _t17_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t17_16), _mm256_mul_pd(_t15_10, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t17_18), _mm256_mul_pd(_t15_8, _t17_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t17_8 = _mm256_add_pd(_t17_12, _t17_3);
      _t17_9 = _mm256_add_pd(_t17_13, _t17_2);
      _t17_10 = _mm256_add_pd(_t17_14, _t17_1);
      _t17_11 = _mm256_add_pd(_t17_15, _t17_0);

      // AVX Storer:

      for( int k2 = 4; k2 <= 27; k2+=4 ) {
        _t18_19 = _mm256_broadcast_sd(M0 + 28*i0 + k2);
        _t18_18 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 1);
        _t18_17 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 2);
        _t18_16 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 3);
        _t18_15 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 28);
        _t18_14 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 29);
        _t18_13 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 30);
        _t18_12 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 31);
        _t18_11 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 56);
        _t18_10 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 57);
        _t18_9 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 58);
        _t18_8 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 59);
        _t18_7 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 84);
        _t18_6 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 85);
        _t18_5 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 86);
        _t18_4 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 87);
        _t18_3 = _mm256_loadu_pd(F + k2 + 28*k3);
        _t18_2 = _mm256_loadu_pd(F + k2 + 28*k3 + 28);
        _t18_1 = _mm256_loadu_pd(F + k2 + 28*k3 + 56);
        _t18_0 = _mm256_loadu_pd(F + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t18_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
        _t18_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
        _t18_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
        _t18_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_24), _mm256_mul_pd(_t18_18, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_26), _mm256_mul_pd(_t18_16, _t18_27)));
        _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_24), _mm256_mul_pd(_t18_14, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_26), _mm256_mul_pd(_t18_12, _t18_27)));
        _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_24), _mm256_mul_pd(_t18_10, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_26), _mm256_mul_pd(_t18_8, _t18_27)));
        _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_24), _mm256_mul_pd(_t18_6, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_26), _mm256_mul_pd(_t18_4, _t18_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_8 = _mm256_add_pd(_t17_8, _t18_20);
        _t17_9 = _mm256_add_pd(_t17_9, _t18_21);
        _t17_10 = _mm256_add_pd(_t17_10, _t18_22);
        _t17_11 = _mm256_add_pd(_t17_11, _t18_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(Y + 28*i0 + k3, _t17_8);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 28, _t17_9);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 56, _t17_10);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 84, _t17_11);
    }
    _mm256_storeu_pd(Y + 29*i0, _t15_28);
    _mm256_maskstore_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
    _mm256_maskstore_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
    _mm256_maskstore_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  }

  _t19_23 = _mm256_broadcast_sd(M0 + 672);
  _t19_22 = _mm256_broadcast_sd(M0 + 673);
  _t19_21 = _mm256_broadcast_sd(M0 + 674);
  _t19_20 = _mm256_broadcast_sd(M0 + 675);
  _t19_19 = _mm256_broadcast_sd(M0 + 700);
  _t19_18 = _mm256_broadcast_sd(M0 + 701);
  _t19_17 = _mm256_broadcast_sd(M0 + 702);
  _t19_16 = _mm256_broadcast_sd(M0 + 703);
  _t19_15 = _mm256_broadcast_sd(M0 + 728);
  _t19_14 = _mm256_broadcast_sd(M0 + 729);
  _t19_13 = _mm256_broadcast_sd(M0 + 730);
  _t19_12 = _mm256_broadcast_sd(M0 + 731);
  _t19_11 = _mm256_broadcast_sd(M0 + 756);
  _t19_10 = _mm256_broadcast_sd(M0 + 757);
  _t19_9 = _mm256_broadcast_sd(M0 + 758);
  _t19_8 = _mm256_broadcast_sd(M0 + 759);
  _t19_7 = _mm256_loadu_pd(F + 672);
  _t19_6 = _mm256_loadu_pd(F + 700);
  _t19_5 = _mm256_loadu_pd(F + 728);
  _t19_4 = _mm256_loadu_pd(F + 756);
  _t19_3 = _mm256_loadu_pd(Q + 696);
  _t19_2 = _mm256_maskload_pd(Q + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t19_1 = _mm256_maskload_pd(Q + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t19_0 = _mm256_maskload_pd(Q + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t19_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_6), _mm256_unpacklo_pd(_t19_5, _t19_4), 32);
  _t19_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_7, _t19_6), _mm256_unpackhi_pd(_t19_5, _t19_4), 32);
  _t19_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_6), _mm256_unpacklo_pd(_t19_5, _t19_4), 49);
  _t19_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_7, _t19_6), _mm256_unpackhi_pd(_t19_5, _t19_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t19_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_23, _t19_40), _mm256_mul_pd(_t19_22, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_21, _t19_42), _mm256_mul_pd(_t19_20, _t19_43)));
  _t19_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_40), _mm256_mul_pd(_t19_18, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_42), _mm256_mul_pd(_t19_16, _t19_43)));
  _t19_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_40), _mm256_mul_pd(_t19_14, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_42), _mm256_mul_pd(_t19_12, _t19_43)));
  _t19_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_40), _mm256_mul_pd(_t19_10, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_42), _mm256_mul_pd(_t19_8, _t19_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t19_36 = _t19_3;
  _t19_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 3), _t19_2, 12);
  _t19_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 0), _t19_1, 49);
  _t19_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 12), _mm256_shuffle_pd(_t19_1, _t19_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t19_24 = _mm256_add_pd(_t19_32, _t19_36);
  _t19_25 = _mm256_add_pd(_t19_33, _t19_37);
  _t19_26 = _mm256_add_pd(_t19_34, _t19_38);
  _t19_27 = _mm256_add_pd(_t19_35, _t19_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t19_28 = _t19_24;
  _t19_29 = _t19_25;
  _t19_30 = _t19_26;
  _t19_31 = _t19_27;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t20_19 = _mm256_broadcast_sd(M0 + k2 + 672);
    _t20_18 = _mm256_broadcast_sd(M0 + k2 + 673);
    _t20_17 = _mm256_broadcast_sd(M0 + k2 + 674);
    _t20_16 = _mm256_broadcast_sd(M0 + k2 + 675);
    _t20_15 = _mm256_broadcast_sd(M0 + k2 + 700);
    _t20_14 = _mm256_broadcast_sd(M0 + k2 + 701);
    _t20_13 = _mm256_broadcast_sd(M0 + k2 + 702);
    _t20_12 = _mm256_broadcast_sd(M0 + k2 + 703);
    _t20_11 = _mm256_broadcast_sd(M0 + k2 + 728);
    _t20_10 = _mm256_broadcast_sd(M0 + k2 + 729);
    _t20_9 = _mm256_broadcast_sd(M0 + k2 + 730);
    _t20_8 = _mm256_broadcast_sd(M0 + k2 + 731);
    _t20_7 = _mm256_broadcast_sd(M0 + k2 + 756);
    _t20_6 = _mm256_broadcast_sd(M0 + k2 + 757);
    _t20_5 = _mm256_broadcast_sd(M0 + k2 + 758);
    _t20_4 = _mm256_broadcast_sd(M0 + k2 + 759);
    _t20_3 = _mm256_loadu_pd(F + k2 + 672);
    _t20_2 = _mm256_loadu_pd(F + k2 + 700);
    _t20_1 = _mm256_loadu_pd(F + k2 + 728);
    _t20_0 = _mm256_loadu_pd(F + k2 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t20_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 32);
    _t20_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 32);
    _t20_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 49);
    _t20_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t20_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_19, _t20_28), _mm256_mul_pd(_t20_18, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_17, _t20_30), _mm256_mul_pd(_t20_16, _t20_31)));
    _t20_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t20_28), _mm256_mul_pd(_t20_14, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t20_30), _mm256_mul_pd(_t20_12, _t20_31)));
    _t20_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t20_28), _mm256_mul_pd(_t20_10, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t20_30), _mm256_mul_pd(_t20_8, _t20_31)));
    _t20_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t20_28), _mm256_mul_pd(_t20_6, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t20_30), _mm256_mul_pd(_t20_4, _t20_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t20_24 = _t19_28;
    _t20_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
    _t20_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
    _t20_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t20_24 = _mm256_add_pd(_t20_24, _t20_20);
    _t20_25 = _mm256_add_pd(_t20_25, _t20_21);
    _t20_26 = _mm256_add_pd(_t20_26, _t20_22);
    _t20_27 = _mm256_add_pd(_t20_27, _t20_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t19_28 = _t20_24;
    _t19_29 = _t20_25;
    _t19_30 = _t20_26;
    _t19_31 = _t20_27;
  }


  // Generating : v0[12,1] = Sum_{i0} ( ( S(h(4, 12, i0), ( G(h(4, 12, i0), z[12,1],h(1, 1, 0)) - ( G(h(4, 12, i0), H[12,28],h(4, 28, 0)) * G(h(4, 28, 0), y[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k3} ( -$(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k3)) * G(h(4, 28, k3), y[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t21_5 = _mm256_loadu_pd(z + i0);
    _t21_4 = _mm256_loadu_pd(H + 28*i0);
    _t21_3 = _mm256_loadu_pd(H + 28*i0 + 28);
    _t21_2 = _mm256_loadu_pd(H + 28*i0 + 56);
    _t21_1 = _mm256_loadu_pd(H + 28*i0 + 84);
    _t21_0 = _mm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t21_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t21_7 = _mm256_sub_pd(_t21_5, _t21_6);

    // AVX Storer:

    for( int k3 = 4; k3 <= 27; k3+=4 ) {
      _t22_4 = _mm256_loadu_pd(H + 28*i0 + k3);
      _t22_3 = _mm256_loadu_pd(H + 28*i0 + k3 + 28);
      _t22_2 = _mm256_loadu_pd(H + 28*i0 + k3 + 56);
      _t22_1 = _mm256_loadu_pd(H + 28*i0 + k3 + 84);
      _t22_0 = _mm256_loadu_pd(y + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t22_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_4, _t22_0), _mm256_mul_pd(_t22_3, _t22_0)), _mm256_hadd_pd(_mm256_mul_pd(_t22_2, _t22_0), _mm256_mul_pd(_t22_1, _t22_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_4, _t22_0), _mm256_mul_pd(_t22_3, _t22_0)), _mm256_hadd_pd(_mm256_mul_pd(_t22_2, _t22_0), _mm256_mul_pd(_t22_1, _t22_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t21_7 = _mm256_sub_pd(_t21_7, _t22_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(v0 + i0, _t21_7);
  }

  _t23_7 = _mm256_loadu_pd(Y);
  _t23_6 = _mm256_maskload_pd(Y + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t23_5 = _mm256_maskload_pd(Y + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t23_4 = _mm256_maskload_pd(Y + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t23_3 = _mm256_loadu_pd(Y + 116);
  _t23_2 = _mm256_maskload_pd(Y + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t23_1 = _mm256_maskload_pd(Y + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t23_0 = _mm256_maskload_pd(Y + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[12,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k2)) * T( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, 4)) * G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k2)) * T( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{k3} ( ( ( ( S(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) + $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k3)) * G(h(4, 28, k3), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k2)) * T( G(h(4, 28, k3), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 12, i0), ( G(h(4, 12, i0), H[12,28],h(4, 28, 24)) * G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_8 = _t23_7;
  _t23_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 3), _t23_6, 12);
  _t23_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 0), _t23_5, 49);
  _t23_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 12), _mm256_shuffle_pd(_t23_5, _t23_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_12 = _t23_3;
  _t23_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 3), _t23_2, 12);
  _t23_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 0), _t23_1, 49);
  _t23_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 12), _mm256_shuffle_pd(_t23_1, _t23_0, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_16 = _t19_28;
  _t23_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t23_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t23_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t24_15 = _mm256_broadcast_sd(H + 28*i0);
    _t24_14 = _mm256_broadcast_sd(H + 28*i0 + 1);
    _t24_13 = _mm256_broadcast_sd(H + 28*i0 + 2);
    _t24_12 = _mm256_broadcast_sd(H + 28*i0 + 3);
    _t24_11 = _mm256_broadcast_sd(H + 28*i0 + 28);
    _t24_10 = _mm256_broadcast_sd(H + 28*i0 + 29);
    _t24_9 = _mm256_broadcast_sd(H + 28*i0 + 30);
    _t24_8 = _mm256_broadcast_sd(H + 28*i0 + 31);
    _t24_7 = _mm256_broadcast_sd(H + 28*i0 + 56);
    _t24_6 = _mm256_broadcast_sd(H + 28*i0 + 57);
    _t24_5 = _mm256_broadcast_sd(H + 28*i0 + 58);
    _t24_4 = _mm256_broadcast_sd(H + 28*i0 + 59);
    _t24_3 = _mm256_broadcast_sd(H + 28*i0 + 84);
    _t24_2 = _mm256_broadcast_sd(H + 28*i0 + 85);
    _t24_1 = _mm256_broadcast_sd(H + 28*i0 + 86);
    _t24_0 = _mm256_broadcast_sd(H + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t24_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t23_8), _mm256_mul_pd(_t24_14, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t23_10), _mm256_mul_pd(_t24_12, _t23_11)));
    _t24_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t23_8), _mm256_mul_pd(_t24_10, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t23_10), _mm256_mul_pd(_t24_8, _t23_11)));
    _t24_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t23_8), _mm256_mul_pd(_t24_6, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t23_10), _mm256_mul_pd(_t24_4, _t23_11)));
    _t24_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t23_8), _mm256_mul_pd(_t24_2, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t23_10), _mm256_mul_pd(_t24_0, _t23_11)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t25_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t25_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t25_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t25_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t25_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t25_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t25_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t25_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t25_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t25_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t25_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t25_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t25_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t25_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t25_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t25_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t25_3 = _mm256_loadu_pd(Y + k2);
      _t25_2 = _mm256_loadu_pd(Y + k2 + 28);
      _t25_1 = _mm256_loadu_pd(Y + k2 + 56);
      _t25_0 = _mm256_loadu_pd(Y + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t25_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
      _t25_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
      _t25_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
      _t25_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t25_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_24), _mm256_mul_pd(_t25_18, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_26), _mm256_mul_pd(_t25_16, _t25_27)));
      _t25_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_24), _mm256_mul_pd(_t25_14, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_26), _mm256_mul_pd(_t25_12, _t25_27)));
      _t25_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_24), _mm256_mul_pd(_t25_10, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_26), _mm256_mul_pd(_t25_8, _t25_27)));
      _t25_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_24), _mm256_mul_pd(_t25_6, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_26), _mm256_mul_pd(_t25_4, _t25_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t24_16 = _mm256_add_pd(_t24_16, _t25_20);
      _t24_17 = _mm256_add_pd(_t24_17, _t25_21);
      _t24_18 = _mm256_add_pd(_t24_18, _t25_22);
      _t24_19 = _mm256_add_pd(_t24_19, _t25_23);

      // AVX Storer:
    }
    _t26_19 = _mm256_loadu_pd(Y + 4);
    _t26_18 = _mm256_loadu_pd(Y + 32);
    _t26_17 = _mm256_loadu_pd(Y + 60);
    _t26_16 = _mm256_loadu_pd(Y + 88);
    _t26_15 = _mm256_broadcast_sd(H + 28*i0 + 4);
    _t26_14 = _mm256_broadcast_sd(H + 28*i0 + 5);
    _t26_13 = _mm256_broadcast_sd(H + 28*i0 + 6);
    _t26_12 = _mm256_broadcast_sd(H + 28*i0 + 7);
    _t26_11 = _mm256_broadcast_sd(H + 28*i0 + 32);
    _t26_10 = _mm256_broadcast_sd(H + 28*i0 + 33);
    _t26_9 = _mm256_broadcast_sd(H + 28*i0 + 34);
    _t26_8 = _mm256_broadcast_sd(H + 28*i0 + 35);
    _t26_7 = _mm256_broadcast_sd(H + 28*i0 + 60);
    _t26_6 = _mm256_broadcast_sd(H + 28*i0 + 61);
    _t26_5 = _mm256_broadcast_sd(H + 28*i0 + 62);
    _t26_4 = _mm256_broadcast_sd(H + 28*i0 + 63);
    _t26_3 = _mm256_broadcast_sd(H + 28*i0 + 88);
    _t26_2 = _mm256_broadcast_sd(H + 28*i0 + 89);
    _t26_1 = _mm256_broadcast_sd(H + 28*i0 + 90);
    _t26_0 = _mm256_broadcast_sd(H + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t26_19), _mm256_mul_pd(_t24_14, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t26_17), _mm256_mul_pd(_t24_12, _t26_16)));
    _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t26_19), _mm256_mul_pd(_t24_10, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t26_17), _mm256_mul_pd(_t24_8, _t26_16)));
    _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t26_19), _mm256_mul_pd(_t24_6, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t26_17), _mm256_mul_pd(_t24_4, _t26_16)));
    _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t26_19), _mm256_mul_pd(_t24_2, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t26_17), _mm256_mul_pd(_t24_0, _t26_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t23_12), _mm256_mul_pd(_t26_14, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t23_14), _mm256_mul_pd(_t26_12, _t23_15)));
    _t26_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t23_12), _mm256_mul_pd(_t26_10, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t23_14), _mm256_mul_pd(_t26_8, _t23_15)));
    _t26_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t23_12), _mm256_mul_pd(_t26_6, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t23_14), _mm256_mul_pd(_t26_4, _t23_15)));
    _t26_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t23_12), _mm256_mul_pd(_t26_2, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t23_14), _mm256_mul_pd(_t26_0, _t23_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t26_20 = _mm256_add_pd(_t26_20, _t26_24);
    _t26_21 = _mm256_add_pd(_t26_21, _t26_25);
    _t26_22 = _mm256_add_pd(_t26_22, _t26_26);
    _t26_23 = _mm256_add_pd(_t26_23, _t26_27);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t27_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t27_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t27_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t27_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t27_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t27_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t27_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t27_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t27_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t27_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t27_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t27_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t27_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t27_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t27_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t27_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t27_3 = _mm256_loadu_pd(Y + k2 + 112);
      _t27_2 = _mm256_loadu_pd(Y + k2 + 140);
      _t27_1 = _mm256_loadu_pd(Y + k2 + 168);
      _t27_0 = _mm256_loadu_pd(Y + k2 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t27_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 32);
      _t27_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 32);
      _t27_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 49);
      _t27_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t27_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_19, _t27_24), _mm256_mul_pd(_t27_18, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_17, _t27_26), _mm256_mul_pd(_t27_16, _t27_27)));
      _t27_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_15, _t27_24), _mm256_mul_pd(_t27_14, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_13, _t27_26), _mm256_mul_pd(_t27_12, _t27_27)));
      _t27_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_11, _t27_24), _mm256_mul_pd(_t27_10, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_9, _t27_26), _mm256_mul_pd(_t27_8, _t27_27)));
      _t27_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_7, _t27_24), _mm256_mul_pd(_t27_6, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_5, _t27_26), _mm256_mul_pd(_t27_4, _t27_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_20 = _mm256_add_pd(_t26_20, _t27_20);
      _t26_21 = _mm256_add_pd(_t26_21, _t27_21);
      _t26_22 = _mm256_add_pd(_t26_22, _t27_22);
      _t26_23 = _mm256_add_pd(_t26_23, _t27_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k3 = 8; k3 <= 23; k3+=4 ) {
      _t28_3 = _mm256_loadu_pd(Y + k3);
      _t28_2 = _mm256_loadu_pd(Y + k3 + 28);
      _t28_1 = _mm256_loadu_pd(Y + k3 + 56);
      _t28_0 = _mm256_loadu_pd(Y + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t28_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t28_3), _mm256_mul_pd(_t24_14, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t28_1), _mm256_mul_pd(_t24_12, _t28_0)));
      _t28_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t28_3), _mm256_mul_pd(_t24_10, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t28_1), _mm256_mul_pd(_t24_8, _t28_0)));
      _t28_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t28_3), _mm256_mul_pd(_t24_6, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t28_1), _mm256_mul_pd(_t24_4, _t28_0)));
      _t28_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t28_3), _mm256_mul_pd(_t24_2, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t28_1), _mm256_mul_pd(_t24_0, _t28_0)));

      // AVX Storer:

      for( int k2 = 4; k2 <= k3 - 1; k2+=4 ) {
        _t29_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
        _t29_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
        _t29_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
        _t29_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
        _t29_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
        _t29_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
        _t29_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
        _t29_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
        _t29_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
        _t29_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
        _t29_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
        _t29_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
        _t29_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
        _t29_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
        _t29_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
        _t29_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
        _t29_3 = _mm256_loadu_pd(Y + 28*k2 + k3);
        _t29_2 = _mm256_loadu_pd(Y + 28*k2 + k3 + 28);
        _t29_1 = _mm256_loadu_pd(Y + 28*k2 + k3 + 56);
        _t29_0 = _mm256_loadu_pd(Y + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t29_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_3), _mm256_mul_pd(_t29_18, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_1), _mm256_mul_pd(_t29_16, _t29_0)));
        _t29_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_3), _mm256_mul_pd(_t29_14, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_1), _mm256_mul_pd(_t29_12, _t29_0)));
        _t29_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_3), _mm256_mul_pd(_t29_10, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_1), _mm256_mul_pd(_t29_8, _t29_0)));
        _t29_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_3), _mm256_mul_pd(_t29_6, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_1), _mm256_mul_pd(_t29_4, _t29_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t28_4 = _mm256_add_pd(_t28_4, _t29_20);
        _t28_5 = _mm256_add_pd(_t28_5, _t29_21);
        _t28_6 = _mm256_add_pd(_t28_6, _t29_22);
        _t28_7 = _mm256_add_pd(_t28_7, _t29_23);

        // AVX Storer:
      }
      _t30_19 = _mm256_broadcast_sd(H + 28*i0 + k3);
      _t30_18 = _mm256_broadcast_sd(H + 28*i0 + k3 + 1);
      _t30_17 = _mm256_broadcast_sd(H + 28*i0 + k3 + 2);
      _t30_16 = _mm256_broadcast_sd(H + 28*i0 + k3 + 3);
      _t30_15 = _mm256_broadcast_sd(H + 28*i0 + k3 + 28);
      _t30_14 = _mm256_broadcast_sd(H + 28*i0 + k3 + 29);
      _t30_13 = _mm256_broadcast_sd(H + 28*i0 + k3 + 30);
      _t30_12 = _mm256_broadcast_sd(H + 28*i0 + k3 + 31);
      _t30_11 = _mm256_broadcast_sd(H + 28*i0 + k3 + 56);
      _t30_10 = _mm256_broadcast_sd(H + 28*i0 + k3 + 57);
      _t30_9 = _mm256_broadcast_sd(H + 28*i0 + k3 + 58);
      _t30_8 = _mm256_broadcast_sd(H + 28*i0 + k3 + 59);
      _t30_7 = _mm256_broadcast_sd(H + 28*i0 + k3 + 84);
      _t30_6 = _mm256_broadcast_sd(H + 28*i0 + k3 + 85);
      _t30_5 = _mm256_broadcast_sd(H + 28*i0 + k3 + 86);
      _t30_4 = _mm256_broadcast_sd(H + 28*i0 + k3 + 87);
      _t30_3 = _mm256_loadu_pd(Y + 29*k3);
      _t30_2 = _mm256_maskload_pd(Y + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t30_1 = _mm256_maskload_pd(Y + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t30_0 = _mm256_maskload_pd(Y + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t30_24 = _t30_3;
      _t30_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 3), _t30_2, 12);
      _t30_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 0), _t30_1, 49);
      _t30_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 12), _mm256_shuffle_pd(_t30_1, _t30_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_19, _t30_24), _mm256_mul_pd(_t30_18, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_17, _t30_26), _mm256_mul_pd(_t30_16, _t30_27)));
      _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t30_24), _mm256_mul_pd(_t30_14, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t30_26), _mm256_mul_pd(_t30_12, _t30_27)));
      _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t30_24), _mm256_mul_pd(_t30_10, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t30_26), _mm256_mul_pd(_t30_8, _t30_27)));
      _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t30_24), _mm256_mul_pd(_t30_6, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t30_26), _mm256_mul_pd(_t30_4, _t30_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t28_4 = _mm256_add_pd(_t28_4, _t30_20);
      _t28_5 = _mm256_add_pd(_t28_5, _t30_21);
      _t28_6 = _mm256_add_pd(_t28_6, _t30_22);
      _t28_7 = _mm256_add_pd(_t28_7, _t30_23);

      // AVX Storer:

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t31_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
        _t31_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
        _t31_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
        _t31_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
        _t31_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
        _t31_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
        _t31_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
        _t31_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
        _t31_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
        _t31_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
        _t31_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
        _t31_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
        _t31_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
        _t31_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
        _t31_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
        _t31_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
        _t31_3 = _mm256_loadu_pd(Y + k2 + 28*k3);
        _t31_2 = _mm256_loadu_pd(Y + k2 + 28*k3 + 28);
        _t31_1 = _mm256_loadu_pd(Y + k2 + 28*k3 + 56);
        _t31_0 = _mm256_loadu_pd(Y + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t31_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 32);
        _t31_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 32);
        _t31_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 49);
        _t31_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t31_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t31_24), _mm256_mul_pd(_t31_18, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t31_26), _mm256_mul_pd(_t31_16, _t31_27)));
        _t31_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t31_24), _mm256_mul_pd(_t31_14, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t31_26), _mm256_mul_pd(_t31_12, _t31_27)));
        _t31_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t31_24), _mm256_mul_pd(_t31_10, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t31_26), _mm256_mul_pd(_t31_8, _t31_27)));
        _t31_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t31_24), _mm256_mul_pd(_t31_6, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t31_26), _mm256_mul_pd(_t31_4, _t31_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t28_4 = _mm256_add_pd(_t28_4, _t31_20);
        _t28_5 = _mm256_add_pd(_t28_5, _t31_21);
        _t28_6 = _mm256_add_pd(_t28_6, _t31_22);
        _t28_7 = _mm256_add_pd(_t28_7, _t31_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M1 + 28*i0 + k3, _t28_4);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 28, _t28_5);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 56, _t28_6);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 84, _t28_7);
    }
    _t32_3 = _mm256_loadu_pd(Y + 24);
    _t32_2 = _mm256_loadu_pd(Y + 52);
    _t32_1 = _mm256_loadu_pd(Y + 80);
    _t32_0 = _mm256_loadu_pd(Y + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t32_3), _mm256_mul_pd(_t24_14, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t32_1), _mm256_mul_pd(_t24_12, _t32_0)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t32_3), _mm256_mul_pd(_t24_10, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t32_1), _mm256_mul_pd(_t24_8, _t32_0)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t32_3), _mm256_mul_pd(_t24_6, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t32_1), _mm256_mul_pd(_t24_4, _t32_0)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t32_3), _mm256_mul_pd(_t24_2, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t32_1), _mm256_mul_pd(_t24_0, _t32_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t33_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t33_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t33_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t33_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t33_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t33_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t33_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t33_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t33_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t33_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t33_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t33_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t33_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t33_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t33_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t33_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t33_3 = _mm256_loadu_pd(Y + 28*k2 + 24);
      _t33_2 = _mm256_loadu_pd(Y + 28*k2 + 52);
      _t33_1 = _mm256_loadu_pd(Y + 28*k2 + 80);
      _t33_0 = _mm256_loadu_pd(Y + 28*k2 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t33_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_19, _t33_3), _mm256_mul_pd(_t33_18, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_17, _t33_1), _mm256_mul_pd(_t33_16, _t33_0)));
      _t33_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_15, _t33_3), _mm256_mul_pd(_t33_14, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_13, _t33_1), _mm256_mul_pd(_t33_12, _t33_0)));
      _t33_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_11, _t33_3), _mm256_mul_pd(_t33_10, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_9, _t33_1), _mm256_mul_pd(_t33_8, _t33_0)));
      _t33_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_7, _t33_3), _mm256_mul_pd(_t33_6, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_5, _t33_1), _mm256_mul_pd(_t33_4, _t33_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t32_4 = _mm256_add_pd(_t32_4, _t33_20);
      _t32_5 = _mm256_add_pd(_t32_5, _t33_21);
      _t32_6 = _mm256_add_pd(_t32_6, _t33_22);
      _t32_7 = _mm256_add_pd(_t32_7, _t33_23);

      // AVX Storer:
    }
    _t34_15 = _mm256_broadcast_sd(H + 28*i0 + 24);
    _t34_14 = _mm256_broadcast_sd(H + 28*i0 + 25);
    _t34_13 = _mm256_broadcast_sd(H + 28*i0 + 26);
    _t34_12 = _mm256_broadcast_sd(H + 28*i0 + 27);
    _t34_11 = _mm256_broadcast_sd(H + 28*i0 + 52);
    _t34_10 = _mm256_broadcast_sd(H + 28*i0 + 53);
    _t34_9 = _mm256_broadcast_sd(H + 28*i0 + 54);
    _t34_8 = _mm256_broadcast_sd(H + 28*i0 + 55);
    _t34_7 = _mm256_broadcast_sd(H + 28*i0 + 80);
    _t34_6 = _mm256_broadcast_sd(H + 28*i0 + 81);
    _t34_5 = _mm256_broadcast_sd(H + 28*i0 + 82);
    _t34_4 = _mm256_broadcast_sd(H + 28*i0 + 83);
    _t34_3 = _mm256_broadcast_sd(H + 28*i0 + 108);
    _t34_2 = _mm256_broadcast_sd(H + 28*i0 + 109);
    _t34_1 = _mm256_broadcast_sd(H + 28*i0 + 110);
    _t34_0 = _mm256_broadcast_sd(H + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t34_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_15, _t23_16), _mm256_mul_pd(_t34_14, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_13, _t23_18), _mm256_mul_pd(_t34_12, _t23_19)));
    _t34_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_11, _t23_16), _mm256_mul_pd(_t34_10, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_9, _t23_18), _mm256_mul_pd(_t34_8, _t23_19)));
    _t34_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_7, _t23_16), _mm256_mul_pd(_t34_6, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_5, _t23_18), _mm256_mul_pd(_t34_4, _t23_19)));
    _t34_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_3, _t23_16), _mm256_mul_pd(_t34_2, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_1, _t23_18), _mm256_mul_pd(_t34_0, _t23_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t32_4 = _mm256_add_pd(_t32_4, _t34_16);
    _t32_5 = _mm256_add_pd(_t32_5, _t34_17);
    _t32_6 = _mm256_add_pd(_t32_6, _t34_18);
    _t32_7 = _mm256_add_pd(_t32_7, _t34_19);

    // AVX Storer:
    _mm256_storeu_pd(M1 + 28*i0, _t24_16);
    _mm256_storeu_pd(M1 + 28*i0 + 28, _t24_17);
    _mm256_storeu_pd(M1 + 28*i0 + 56, _t24_18);
    _mm256_storeu_pd(M1 + 28*i0 + 84, _t24_19);
    _mm256_storeu_pd(M1 + 28*i0 + 4, _t26_20);
    _mm256_storeu_pd(M1 + 28*i0 + 32, _t26_21);
    _mm256_storeu_pd(M1 + 28*i0 + 60, _t26_22);
    _mm256_storeu_pd(M1 + 28*i0 + 88, _t26_23);
    _mm256_storeu_pd(M1 + 28*i0 + 24, _t32_4);
    _mm256_storeu_pd(M1 + 28*i0 + 52, _t32_5);
    _mm256_storeu_pd(M1 + 28*i0 + 80, _t32_6);
    _mm256_storeu_pd(M1 + 28*i0 + 108, _t32_7);
  }


  // Generating : M2[28,12] = ( ( ( Sum_{k3} ( ( S(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 0)) ) ),h(4, 12, k3)) + Sum_{k2} ( $(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, k2)) ) ),h(4, 12, k3)) ) ) ) + Sum_{k3} ( ( ( S(h(4, 28, 4), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 0)) ) ),h(4, 12, k3)) + $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 4)) ) ),h(4, 12, k3)) ) + Sum_{k2} ( $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, k2)) ) ),h(4, 12, k3)) ) ) ) ) + Sum_{i0} ( Sum_{k3} ( ( ( ( S(h(4, 28, i0), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 0)) ) ),h(4, 12, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 12, k3), H[12,28],h(4, 28, k2)) ) ),h(4, 12, k3)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, i0)) ) ),h(4, 12, k3)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, k2)) ) ),h(4, 12, k3)) ) ) ) ) ) + Sum_{k3} ( ( ( S(h(4, 28, 24), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 0)) ) ),h(4, 12, k3)) + Sum_{k2} ( $(h(4, 28, 24), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 12, k3), H[12,28],h(4, 28, k2)) ) ),h(4, 12, k3)) ) ) + $(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 24)) ) ),h(4, 12, k3)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t35_0 = _t23_7;
  _t35_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 3), _t23_6, 12);
  _t35_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 0), _t23_5, 49);
  _t35_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 12), _mm256_shuffle_pd(_t23_5, _t23_4, 12), 49);


  for( int k3 = 0; k3 <= 11; k3+=4 ) {
    _t36_19 = _mm256_loadu_pd(H + 28*k3);
    _t36_18 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t36_17 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t36_16 = _mm256_loadu_pd(H + 28*k3 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t36_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_19, _t36_18), _mm256_unpacklo_pd(_t36_17, _t36_16), 32);
    _t36_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_19, _t36_18), _mm256_unpackhi_pd(_t36_17, _t36_16), 32);
    _t36_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_19, _t36_18), _mm256_unpacklo_pd(_t36_17, _t36_16), 49);
    _t36_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_19, _t36_18), _mm256_unpackhi_pd(_t36_17, _t36_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t36_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_15, _t36_24), _mm256_mul_pd(_t36_14, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_13, _t36_26), _mm256_mul_pd(_t36_12, _t36_27)));
    _t36_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_11, _t36_24), _mm256_mul_pd(_t36_10, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_9, _t36_26), _mm256_mul_pd(_t36_8, _t36_27)));
    _t36_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_7, _t36_24), _mm256_mul_pd(_t36_6, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_5, _t36_26), _mm256_mul_pd(_t36_4, _t36_27)));
    _t36_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_3, _t36_24), _mm256_mul_pd(_t36_2, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_1, _t36_26), _mm256_mul_pd(_t36_0, _t36_27)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t37_19 = _mm256_broadcast_sd(Y + k2);
      _t37_18 = _mm256_broadcast_sd(Y + k2 + 1);
      _t37_17 = _mm256_broadcast_sd(Y + k2 + 2);
      _t37_16 = _mm256_broadcast_sd(Y + k2 + 3);
      _t37_15 = _mm256_broadcast_sd(Y + k2 + 28);
      _t37_14 = _mm256_broadcast_sd(Y + k2 + 29);
      _t37_13 = _mm256_broadcast_sd(Y + k2 + 30);
      _t37_12 = _mm256_broadcast_sd(Y + k2 + 31);
      _t37_11 = _mm256_broadcast_sd(Y + k2 + 56);
      _t37_10 = _mm256_broadcast_sd(Y + k2 + 57);
      _t37_9 = _mm256_broadcast_sd(Y + k2 + 58);
      _t37_8 = _mm256_broadcast_sd(Y + k2 + 59);
      _t37_7 = _mm256_broadcast_sd(Y + k2 + 84);
      _t37_6 = _mm256_broadcast_sd(Y + k2 + 85);
      _t37_5 = _mm256_broadcast_sd(Y + k2 + 86);
      _t37_4 = _mm256_broadcast_sd(Y + k2 + 87);
      _t37_3 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t37_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t37_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t37_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_19, _t37_24), _mm256_mul_pd(_t37_18, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_17, _t37_26), _mm256_mul_pd(_t37_16, _t37_27)));
      _t37_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_15, _t37_24), _mm256_mul_pd(_t37_14, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_13, _t37_26), _mm256_mul_pd(_t37_12, _t37_27)));
      _t37_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_11, _t37_24), _mm256_mul_pd(_t37_10, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_9, _t37_26), _mm256_mul_pd(_t37_8, _t37_27)));
      _t37_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_7, _t37_24), _mm256_mul_pd(_t37_6, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_5, _t37_26), _mm256_mul_pd(_t37_4, _t37_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t36_20 = _mm256_add_pd(_t36_20, _t37_20);
      _t36_21 = _mm256_add_pd(_t36_21, _t37_21);
      _t36_22 = _mm256_add_pd(_t36_22, _t37_22);
      _t36_23 = _mm256_add_pd(_t36_23, _t37_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + k3, _t36_20);
    _mm256_storeu_pd(M2 + k3 + 12, _t36_21);
    _mm256_storeu_pd(M2 + k3 + 24, _t36_22);
    _mm256_storeu_pd(M2 + k3 + 36, _t36_23);
  }

  _t38_3 = _mm256_loadu_pd(Y + 4);
  _t38_2 = _mm256_loadu_pd(Y + 32);
  _t38_1 = _mm256_loadu_pd(Y + 60);
  _t38_0 = _mm256_loadu_pd(Y + 88);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t38_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
  _t38_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
  _t38_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
  _t38_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t38_4 = _t23_3;
  _t38_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 3), _t23_2, 12);
  _t38_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 0), _t23_1, 49);
  _t38_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 12), _mm256_shuffle_pd(_t23_1, _t23_0, 12), 49);


  for( int k3 = 0; k3 <= 11; k3+=4 ) {
    _t39_39 = _mm256_loadu_pd(H + 28*k3);
    _t39_38 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t39_37 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t39_36 = _mm256_loadu_pd(H + 28*k3 + 84);
    _t39_35 = _mm256_loadu_pd(H + 28*k3 + 4);
    _t39_34 = _mm256_loadu_pd(H + 28*k3 + 32);
    _t39_33 = _mm256_loadu_pd(H + 28*k3 + 60);
    _t39_32 = _mm256_loadu_pd(H + 28*k3 + 88);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_39, _t39_38), _mm256_unpacklo_pd(_t39_37, _t39_36), 32);
    _t39_49 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_39, _t39_38), _mm256_unpackhi_pd(_t39_37, _t39_36), 32);
    _t39_50 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_39, _t39_38), _mm256_unpacklo_pd(_t39_37, _t39_36), 49);
    _t39_51 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_39, _t39_38), _mm256_unpackhi_pd(_t39_37, _t39_36), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_48), _mm256_mul_pd(_t39_14, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_50), _mm256_mul_pd(_t39_12, _t39_51)));
    _t39_41 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_48), _mm256_mul_pd(_t39_10, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_50), _mm256_mul_pd(_t39_8, _t39_51)));
    _t39_42 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_48), _mm256_mul_pd(_t39_6, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_50), _mm256_mul_pd(_t39_4, _t39_51)));
    _t39_43 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_3, _t39_48), _mm256_mul_pd(_t39_2, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_1, _t39_50), _mm256_mul_pd(_t39_0, _t39_51)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_52 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_35, _t39_34), _mm256_unpacklo_pd(_t39_33, _t39_32), 32);
    _t39_53 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_35, _t39_34), _mm256_unpackhi_pd(_t39_33, _t39_32), 32);
    _t39_54 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_35, _t39_34), _mm256_unpacklo_pd(_t39_33, _t39_32), 49);
    _t39_55 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_35, _t39_34), _mm256_unpackhi_pd(_t39_33, _t39_32), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_31, _t39_52), _mm256_mul_pd(_t39_30, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_29, _t39_54), _mm256_mul_pd(_t39_28, _t39_55)));
    _t39_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_27, _t39_52), _mm256_mul_pd(_t39_26, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_25, _t39_54), _mm256_mul_pd(_t39_24, _t39_55)));
    _t39_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_23, _t39_52), _mm256_mul_pd(_t39_22, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_21, _t39_54), _mm256_mul_pd(_t39_20, _t39_55)));
    _t39_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_52), _mm256_mul_pd(_t39_18, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_54), _mm256_mul_pd(_t39_16, _t39_55)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t39_40 = _mm256_add_pd(_t39_40, _t39_44);
    _t39_41 = _mm256_add_pd(_t39_41, _t39_45);
    _t39_42 = _mm256_add_pd(_t39_42, _t39_46);
    _t39_43 = _mm256_add_pd(_t39_43, _t39_47);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t40_19 = _mm256_broadcast_sd(Y + k2 + 112);
      _t40_18 = _mm256_broadcast_sd(Y + k2 + 113);
      _t40_17 = _mm256_broadcast_sd(Y + k2 + 114);
      _t40_16 = _mm256_broadcast_sd(Y + k2 + 115);
      _t40_15 = _mm256_broadcast_sd(Y + k2 + 140);
      _t40_14 = _mm256_broadcast_sd(Y + k2 + 141);
      _t40_13 = _mm256_broadcast_sd(Y + k2 + 142);
      _t40_12 = _mm256_broadcast_sd(Y + k2 + 143);
      _t40_11 = _mm256_broadcast_sd(Y + k2 + 168);
      _t40_10 = _mm256_broadcast_sd(Y + k2 + 169);
      _t40_9 = _mm256_broadcast_sd(Y + k2 + 170);
      _t40_8 = _mm256_broadcast_sd(Y + k2 + 171);
      _t40_7 = _mm256_broadcast_sd(Y + k2 + 196);
      _t40_6 = _mm256_broadcast_sd(Y + k2 + 197);
      _t40_5 = _mm256_broadcast_sd(Y + k2 + 198);
      _t40_4 = _mm256_broadcast_sd(Y + k2 + 199);
      _t40_3 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t40_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t40_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t40_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t40_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 32);
      _t40_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 32);
      _t40_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 49);
      _t40_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t40_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_19, _t40_24), _mm256_mul_pd(_t40_18, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_17, _t40_26), _mm256_mul_pd(_t40_16, _t40_27)));
      _t40_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_15, _t40_24), _mm256_mul_pd(_t40_14, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_13, _t40_26), _mm256_mul_pd(_t40_12, _t40_27)));
      _t40_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_11, _t40_24), _mm256_mul_pd(_t40_10, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_9, _t40_26), _mm256_mul_pd(_t40_8, _t40_27)));
      _t40_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_7, _t40_24), _mm256_mul_pd(_t40_6, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_5, _t40_26), _mm256_mul_pd(_t40_4, _t40_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_40 = _mm256_add_pd(_t39_40, _t40_20);
      _t39_41 = _mm256_add_pd(_t39_41, _t40_21);
      _t39_42 = _mm256_add_pd(_t39_42, _t40_22);
      _t39_43 = _mm256_add_pd(_t39_43, _t40_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + k3 + 48, _t39_40);
    _mm256_storeu_pd(M2 + k3 + 60, _t39_41);
    _mm256_storeu_pd(M2 + k3 + 72, _t39_42);
    _mm256_storeu_pd(M2 + k3 + 84, _t39_43);
  }


  for( int i0 = 8; i0 <= 23; i0+=4 ) {
    _t41_7 = _mm256_loadu_pd(Y + i0);
    _t41_6 = _mm256_loadu_pd(Y + i0 + 28);
    _t41_5 = _mm256_loadu_pd(Y + i0 + 56);
    _t41_4 = _mm256_loadu_pd(Y + i0 + 84);
    _t41_3 = _mm256_loadu_pd(Y + 29*i0);
    _t41_2 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t41_1 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t41_0 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t41_8 = _t41_3;
    _t41_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
    _t41_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
    _t41_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

    for( int k3 = 0; k3 <= 11; k3+=4 ) {
      _t42_19 = _mm256_loadu_pd(H + 28*k3);
      _t42_18 = _mm256_loadu_pd(H + 28*k3 + 28);
      _t42_17 = _mm256_loadu_pd(H + 28*k3 + 56);
      _t42_16 = _mm256_loadu_pd(H + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
      _t42_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
      _t42_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
      _t42_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_19, _t42_18), _mm256_unpacklo_pd(_t42_17, _t42_16), 32);
      _t42_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_19, _t42_18), _mm256_unpackhi_pd(_t42_17, _t42_16), 32);
      _t42_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_19, _t42_18), _mm256_unpacklo_pd(_t42_17, _t42_16), 49);
      _t42_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_19, _t42_18), _mm256_unpackhi_pd(_t42_17, _t42_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t42_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_28), _mm256_mul_pd(_t42_14, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_30), _mm256_mul_pd(_t42_12, _t42_31)));
      _t42_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_28), _mm256_mul_pd(_t42_10, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_30), _mm256_mul_pd(_t42_8, _t42_31)));
      _t42_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_7, _t42_28), _mm256_mul_pd(_t42_6, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_5, _t42_30), _mm256_mul_pd(_t42_4, _t42_31)));
      _t42_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_3, _t42_28), _mm256_mul_pd(_t42_2, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_1, _t42_30), _mm256_mul_pd(_t42_0, _t42_31)));

      // AVX Storer:

      for( int k2 = 4; k2 <= i0 - 1; k2+=4 ) {
        _t43_23 = _mm256_loadu_pd(Y + i0 + 28*k2);
        _t43_22 = _mm256_loadu_pd(Y + i0 + 28*k2 + 28);
        _t43_21 = _mm256_loadu_pd(Y + i0 + 28*k2 + 56);
        _t43_20 = _mm256_loadu_pd(Y + i0 + 28*k2 + 84);
        _t43_19 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t43_18 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t43_17 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t43_16 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_23, _t43_22), _mm256_unpacklo_pd(_t43_21, _t43_20), 32);
        _t43_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_23, _t43_22), _mm256_unpackhi_pd(_t43_21, _t43_20), 32);
        _t43_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_23, _t43_22), _mm256_unpacklo_pd(_t43_21, _t43_20), 49);
        _t43_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_23, _t43_22), _mm256_unpackhi_pd(_t43_21, _t43_20), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_19, _t43_18), _mm256_unpacklo_pd(_t43_17, _t43_16), 32);
        _t43_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_19, _t43_18), _mm256_unpackhi_pd(_t43_17, _t43_16), 32);
        _t43_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_19, _t43_18), _mm256_unpacklo_pd(_t43_17, _t43_16), 49);
        _t43_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_19, _t43_18), _mm256_unpackhi_pd(_t43_17, _t43_16), 49);

        // 4-BLAC: 4x4 * 4x4
        _t43_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_15, _t43_32), _mm256_mul_pd(_t43_14, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_13, _t43_34), _mm256_mul_pd(_t43_12, _t43_35)));
        _t43_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_11, _t43_32), _mm256_mul_pd(_t43_10, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_9, _t43_34), _mm256_mul_pd(_t43_8, _t43_35)));
        _t43_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_7, _t43_32), _mm256_mul_pd(_t43_6, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_5, _t43_34), _mm256_mul_pd(_t43_4, _t43_35)));
        _t43_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_3, _t43_32), _mm256_mul_pd(_t43_2, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_1, _t43_34), _mm256_mul_pd(_t43_0, _t43_35)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t42_20 = _mm256_add_pd(_t42_20, _t43_24);
        _t42_21 = _mm256_add_pd(_t42_21, _t43_25);
        _t42_22 = _mm256_add_pd(_t42_22, _t43_26);
        _t42_23 = _mm256_add_pd(_t42_23, _t43_27);

        // AVX Storer:
      }
      _t44_19 = _mm256_loadu_pd(H + i0 + 28*k3);
      _t44_18 = _mm256_loadu_pd(H + i0 + 28*k3 + 28);
      _t44_17 = _mm256_loadu_pd(H + i0 + 28*k3 + 56);
      _t44_16 = _mm256_loadu_pd(H + i0 + 28*k3 + 84);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t44_24 = _t41_3;
      _t44_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
      _t44_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
      _t44_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t44_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_19, _t44_18), _mm256_unpacklo_pd(_t44_17, _t44_16), 32);
      _t44_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_19, _t44_18), _mm256_unpackhi_pd(_t44_17, _t44_16), 32);
      _t44_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_19, _t44_18), _mm256_unpacklo_pd(_t44_17, _t44_16), 49);
      _t44_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_19, _t44_18), _mm256_unpackhi_pd(_t44_17, _t44_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t44_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_28), _mm256_mul_pd(_t44_14, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_30), _mm256_mul_pd(_t44_12, _t44_31)));
      _t44_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_28), _mm256_mul_pd(_t44_10, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_30), _mm256_mul_pd(_t44_8, _t44_31)));
      _t44_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_7, _t44_28), _mm256_mul_pd(_t44_6, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_5, _t44_30), _mm256_mul_pd(_t44_4, _t44_31)));
      _t44_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_3, _t44_28), _mm256_mul_pd(_t44_2, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_1, _t44_30), _mm256_mul_pd(_t44_0, _t44_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t42_20 = _mm256_add_pd(_t42_20, _t44_20);
      _t42_21 = _mm256_add_pd(_t42_21, _t44_21);
      _t42_22 = _mm256_add_pd(_t42_22, _t44_22);
      _t42_23 = _mm256_add_pd(_t42_23, _t44_23);

      // AVX Storer:

      for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t45_19 = _mm256_broadcast_sd(Y + 28*i0 + k2);
        _t45_18 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 1);
        _t45_17 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 2);
        _t45_16 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 3);
        _t45_15 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 28);
        _t45_14 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 29);
        _t45_13 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 30);
        _t45_12 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 31);
        _t45_11 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 56);
        _t45_10 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 57);
        _t45_9 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 58);
        _t45_8 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 59);
        _t45_7 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 84);
        _t45_6 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 85);
        _t45_5 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 86);
        _t45_4 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 87);
        _t45_3 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t45_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t45_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t45_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t45_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
        _t45_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
        _t45_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
        _t45_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t45_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_24), _mm256_mul_pd(_t45_18, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_26), _mm256_mul_pd(_t45_16, _t45_27)));
        _t45_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_24), _mm256_mul_pd(_t45_14, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_26), _mm256_mul_pd(_t45_12, _t45_27)));
        _t45_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_24), _mm256_mul_pd(_t45_10, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_26), _mm256_mul_pd(_t45_8, _t45_27)));
        _t45_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_24), _mm256_mul_pd(_t45_6, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_26), _mm256_mul_pd(_t45_4, _t45_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t42_20 = _mm256_add_pd(_t42_20, _t45_20);
        _t42_21 = _mm256_add_pd(_t42_21, _t45_21);
        _t42_22 = _mm256_add_pd(_t42_22, _t45_22);
        _t42_23 = _mm256_add_pd(_t42_23, _t45_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M2 + 12*i0 + k3, _t42_20);
      _mm256_storeu_pd(M2 + 12*i0 + k3 + 12, _t42_21);
      _mm256_storeu_pd(M2 + 12*i0 + k3 + 24, _t42_22);
      _mm256_storeu_pd(M2 + 12*i0 + k3 + 36, _t42_23);
    }
  }

  _t46_3 = _mm256_loadu_pd(Y + 24);
  _t46_2 = _mm256_loadu_pd(Y + 52);
  _t46_1 = _mm256_loadu_pd(Y + 80);
  _t46_0 = _mm256_loadu_pd(Y + 108);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t46_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
  _t46_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
  _t46_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
  _t46_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t46_4 = _t19_28;
  _t46_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t46_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t46_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);


  for( int k3 = 0; k3 <= 11; k3+=4 ) {
    _t47_19 = _mm256_loadu_pd(H + 28*k3);
    _t47_18 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t47_17 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t47_16 = _mm256_loadu_pd(H + 28*k3 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_19, _t47_18), _mm256_unpacklo_pd(_t47_17, _t47_16), 32);
    _t47_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_19, _t47_18), _mm256_unpackhi_pd(_t47_17, _t47_16), 32);
    _t47_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_19, _t47_18), _mm256_unpacklo_pd(_t47_17, _t47_16), 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_19, _t47_18), _mm256_unpackhi_pd(_t47_17, _t47_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_24), _mm256_mul_pd(_t47_14, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_26), _mm256_mul_pd(_t47_12, _t47_27)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_24), _mm256_mul_pd(_t47_10, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_26), _mm256_mul_pd(_t47_8, _t47_27)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_24), _mm256_mul_pd(_t47_6, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_26), _mm256_mul_pd(_t47_4, _t47_27)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_3, _t47_24), _mm256_mul_pd(_t47_2, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_1, _t47_26), _mm256_mul_pd(_t47_0, _t47_27)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t48_23 = _mm256_loadu_pd(Y + 28*k2 + 24);
      _t48_22 = _mm256_loadu_pd(Y + 28*k2 + 52);
      _t48_21 = _mm256_loadu_pd(Y + 28*k2 + 80);
      _t48_20 = _mm256_loadu_pd(Y + 28*k2 + 108);
      _t48_19 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t48_18 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t48_17 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t48_16 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_23, _t48_22), _mm256_unpacklo_pd(_t48_21, _t48_20), 32);
      _t48_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_23, _t48_22), _mm256_unpackhi_pd(_t48_21, _t48_20), 32);
      _t48_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_23, _t48_22), _mm256_unpacklo_pd(_t48_21, _t48_20), 49);
      _t48_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_23, _t48_22), _mm256_unpackhi_pd(_t48_21, _t48_20), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_19, _t48_18), _mm256_unpacklo_pd(_t48_17, _t48_16), 32);
      _t48_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_19, _t48_18), _mm256_unpackhi_pd(_t48_17, _t48_16), 32);
      _t48_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_19, _t48_18), _mm256_unpacklo_pd(_t48_17, _t48_16), 49);
      _t48_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_19, _t48_18), _mm256_unpackhi_pd(_t48_17, _t48_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t48_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_15, _t48_32), _mm256_mul_pd(_t48_14, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_13, _t48_34), _mm256_mul_pd(_t48_12, _t48_35)));
      _t48_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_11, _t48_32), _mm256_mul_pd(_t48_10, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_9, _t48_34), _mm256_mul_pd(_t48_8, _t48_35)));
      _t48_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_7, _t48_32), _mm256_mul_pd(_t48_6, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_5, _t48_34), _mm256_mul_pd(_t48_4, _t48_35)));
      _t48_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_3, _t48_32), _mm256_mul_pd(_t48_2, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_1, _t48_34), _mm256_mul_pd(_t48_0, _t48_35)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t47_20 = _mm256_add_pd(_t47_20, _t48_24);
      _t47_21 = _mm256_add_pd(_t47_21, _t48_25);
      _t47_22 = _mm256_add_pd(_t47_22, _t48_26);
      _t47_23 = _mm256_add_pd(_t47_23, _t48_27);

      // AVX Storer:
    }
    _t49_19 = _mm256_loadu_pd(H + 28*k3 + 24);
    _t49_18 = _mm256_loadu_pd(H + 28*k3 + 52);
    _t49_17 = _mm256_loadu_pd(H + 28*k3 + 80);
    _t49_16 = _mm256_loadu_pd(H + 28*k3 + 108);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t49_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t49_19, _t49_18), _mm256_unpacklo_pd(_t49_17, _t49_16), 32);
    _t49_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t49_19, _t49_18), _mm256_unpackhi_pd(_t49_17, _t49_16), 32);
    _t49_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t49_19, _t49_18), _mm256_unpacklo_pd(_t49_17, _t49_16), 49);
    _t49_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t49_19, _t49_18), _mm256_unpackhi_pd(_t49_17, _t49_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t49_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_15, _t49_24), _mm256_mul_pd(_t49_14, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_13, _t49_26), _mm256_mul_pd(_t49_12, _t49_27)));
    _t49_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_11, _t49_24), _mm256_mul_pd(_t49_10, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_9, _t49_26), _mm256_mul_pd(_t49_8, _t49_27)));
    _t49_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_7, _t49_24), _mm256_mul_pd(_t49_6, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_5, _t49_26), _mm256_mul_pd(_t49_4, _t49_27)));
    _t49_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_3, _t49_24), _mm256_mul_pd(_t49_2, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_1, _t49_26), _mm256_mul_pd(_t49_0, _t49_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t47_20 = _mm256_add_pd(_t47_20, _t49_20);
    _t47_21 = _mm256_add_pd(_t47_21, _t49_21);
    _t47_22 = _mm256_add_pd(_t47_22, _t49_22);
    _t47_23 = _mm256_add_pd(_t47_23, _t49_23);

    // AVX Storer:
    _mm256_storeu_pd(M2 + k3 + 288, _t47_20);
    _mm256_storeu_pd(M2 + k3 + 300, _t47_21);
    _mm256_storeu_pd(M2 + k3 + 312, _t47_22);
    _mm256_storeu_pd(M2 + k3 + 324, _t47_23);
  }


  // Generating : M3[12,12] = ( ( Sum_{i0} ( ( ( S(h(4, 12, i0), ( ( G(h(4, 12, i0), M1[12,28],h(4, 28, 0)) * T( G(h(4, 12, i0), H[12,28],h(4, 28, 0)) ) ) + G(h(4, 12, i0), R[12,12],h(4, 12, i0)) ),h(4, 12, i0)) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), M1[12,28],h(4, 28, k2)) * T( G(h(4, 12, i0), H[12,28],h(4, 28, k2)) ) ),h(4, 12, i0)) ) ) + Sum_{k3} ( ( S(h(4, 12, i0), ( ( G(h(4, 12, i0), M1[12,28],h(4, 28, 0)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, 0)) ) ) + G(h(4, 12, i0), R[12,12],h(4, 12, k3)) ),h(4, 12, k3)) + Sum_{k2} ( $(h(4, 12, i0), ( G(h(4, 12, i0), M1[12,28],h(4, 28, k2)) * T( G(h(4, 12, k3), H[12,28],h(4, 28, k2)) ) ),h(4, 12, k3)) ) ) ) ) ) + S(h(4, 12, 8), ( ( G(h(4, 12, 8), M1[12,28],h(4, 28, 0)) * T( G(h(4, 12, 8), H[12,28],h(4, 28, 0)) ) ) + G(h(4, 12, 8), R[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) + Sum_{k2} ( $(h(4, 12, 8), ( G(h(4, 12, 8), M1[12,28],h(4, 28, k2)) * T( G(h(4, 12, 8), H[12,28],h(4, 28, k2)) ) ),h(4, 12, 8)) ) )


  for( int i0 = 0; i0 <= 7; i0+=4 ) {
    _t50_23 = _mm256_broadcast_sd(M1 + 28*i0);
    _t50_22 = _mm256_broadcast_sd(M1 + 28*i0 + 1);
    _t50_21 = _mm256_broadcast_sd(M1 + 28*i0 + 2);
    _t50_20 = _mm256_broadcast_sd(M1 + 28*i0 + 3);
    _t50_19 = _mm256_broadcast_sd(M1 + 28*i0 + 28);
    _t50_18 = _mm256_broadcast_sd(M1 + 28*i0 + 29);
    _t50_17 = _mm256_broadcast_sd(M1 + 28*i0 + 30);
    _t50_16 = _mm256_broadcast_sd(M1 + 28*i0 + 31);
    _t50_15 = _mm256_broadcast_sd(M1 + 28*i0 + 56);
    _t50_14 = _mm256_broadcast_sd(M1 + 28*i0 + 57);
    _t50_13 = _mm256_broadcast_sd(M1 + 28*i0 + 58);
    _t50_12 = _mm256_broadcast_sd(M1 + 28*i0 + 59);
    _t50_11 = _mm256_broadcast_sd(M1 + 28*i0 + 84);
    _t50_10 = _mm256_broadcast_sd(M1 + 28*i0 + 85);
    _t50_9 = _mm256_broadcast_sd(M1 + 28*i0 + 86);
    _t50_8 = _mm256_broadcast_sd(M1 + 28*i0 + 87);
    _t50_7 = _mm256_loadu_pd(H + 28*i0);
    _t50_6 = _mm256_loadu_pd(H + 28*i0 + 28);
    _t50_5 = _mm256_loadu_pd(H + 28*i0 + 56);
    _t50_4 = _mm256_loadu_pd(H + 28*i0 + 84);
    _t50_3 = _mm256_loadu_pd(R + 13*i0);
    _t50_2 = _mm256_maskload_pd(R + 13*i0 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t50_1 = _mm256_maskload_pd(R + 13*i0 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t50_0 = _mm256_maskload_pd(R + 13*i0 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t50_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 32);
    _t50_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 32);
    _t50_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 49);
    _t50_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t50_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_23, _t50_40), _mm256_mul_pd(_t50_22, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_21, _t50_42), _mm256_mul_pd(_t50_20, _t50_43)));
    _t50_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t50_40), _mm256_mul_pd(_t50_18, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t50_42), _mm256_mul_pd(_t50_16, _t50_43)));
    _t50_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t50_40), _mm256_mul_pd(_t50_14, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t50_42), _mm256_mul_pd(_t50_12, _t50_43)));
    _t50_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t50_40), _mm256_mul_pd(_t50_10, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t50_42), _mm256_mul_pd(_t50_8, _t50_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t50_36 = _t50_3;
    _t50_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 3), _t50_2, 12);
    _t50_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 0), _t50_1, 49);
    _t50_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 12), _mm256_shuffle_pd(_t50_1, _t50_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t50_24 = _mm256_add_pd(_t50_32, _t50_36);
    _t50_25 = _mm256_add_pd(_t50_33, _t50_37);
    _t50_26 = _mm256_add_pd(_t50_34, _t50_38);
    _t50_27 = _mm256_add_pd(_t50_35, _t50_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t50_28 = _t50_24;
    _t50_29 = _t50_25;
    _t50_30 = _t50_26;
    _t50_31 = _t50_27;

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t51_19 = _mm256_broadcast_sd(M1 + 28*i0 + k2);
      _t51_18 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 1);
      _t51_17 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 2);
      _t51_16 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 3);
      _t51_15 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 28);
      _t51_14 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 29);
      _t51_13 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 30);
      _t51_12 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 31);
      _t51_11 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 56);
      _t51_10 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 57);
      _t51_9 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 58);
      _t51_8 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 59);
      _t51_7 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 84);
      _t51_6 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 85);
      _t51_5 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 86);
      _t51_4 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 87);
      _t51_3 = _mm256_loadu_pd(H + 28*i0 + k2);
      _t51_2 = _mm256_loadu_pd(H + 28*i0 + k2 + 28);
      _t51_1 = _mm256_loadu_pd(H + 28*i0 + k2 + 56);
      _t51_0 = _mm256_loadu_pd(H + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t51_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 32);
      _t51_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 32);
      _t51_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 49);
      _t51_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t51_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_19, _t51_28), _mm256_mul_pd(_t51_18, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_17, _t51_30), _mm256_mul_pd(_t51_16, _t51_31)));
      _t51_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_15, _t51_28), _mm256_mul_pd(_t51_14, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_13, _t51_30), _mm256_mul_pd(_t51_12, _t51_31)));
      _t51_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_11, _t51_28), _mm256_mul_pd(_t51_10, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_9, _t51_30), _mm256_mul_pd(_t51_8, _t51_31)));
      _t51_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_7, _t51_28), _mm256_mul_pd(_t51_6, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_5, _t51_30), _mm256_mul_pd(_t51_4, _t51_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t51_24 = _t50_28;
      _t51_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 3), _t50_29, 12);
      _t51_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 0), _t50_30, 49);
      _t51_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 12), _mm256_shuffle_pd(_t50_30, _t50_31, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t51_24 = _mm256_add_pd(_t51_24, _t51_20);
      _t51_25 = _mm256_add_pd(_t51_25, _t51_21);
      _t51_26 = _mm256_add_pd(_t51_26, _t51_22);
      _t51_27 = _mm256_add_pd(_t51_27, _t51_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t50_28 = _t51_24;
      _t50_29 = _t51_25;
      _t50_30 = _t51_26;
      _t50_31 = _t51_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 11; k3+=4 ) {
      _t52_7 = _mm256_loadu_pd(H + 28*k3);
      _t52_6 = _mm256_loadu_pd(H + 28*k3 + 28);
      _t52_5 = _mm256_loadu_pd(H + 28*k3 + 56);
      _t52_4 = _mm256_loadu_pd(H + 28*k3 + 84);
      _t52_3 = _mm256_loadu_pd(R + 12*i0 + k3);
      _t52_2 = _mm256_loadu_pd(R + 12*i0 + k3 + 12);
      _t52_1 = _mm256_loadu_pd(R + 12*i0 + k3 + 24);
      _t52_0 = _mm256_loadu_pd(R + 12*i0 + k3 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t52_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 32);
      _t52_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 32);
      _t52_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 49);
      _t52_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t52_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_23, _t52_16), _mm256_mul_pd(_t50_22, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_21, _t52_18), _mm256_mul_pd(_t50_20, _t52_19)));
      _t52_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t52_16), _mm256_mul_pd(_t50_18, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t52_18), _mm256_mul_pd(_t50_16, _t52_19)));
      _t52_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t52_16), _mm256_mul_pd(_t50_14, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t52_18), _mm256_mul_pd(_t50_12, _t52_19)));
      _t52_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t52_16), _mm256_mul_pd(_t50_10, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t52_18), _mm256_mul_pd(_t50_8, _t52_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t52_8 = _mm256_add_pd(_t52_12, _t52_3);
      _t52_9 = _mm256_add_pd(_t52_13, _t52_2);
      _t52_10 = _mm256_add_pd(_t52_14, _t52_1);
      _t52_11 = _mm256_add_pd(_t52_15, _t52_0);

      // AVX Storer:

      for( int k2 = 4; k2 <= 27; k2+=4 ) {
        _t53_19 = _mm256_broadcast_sd(M1 + 28*i0 + k2);
        _t53_18 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 1);
        _t53_17 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 2);
        _t53_16 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 3);
        _t53_15 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 28);
        _t53_14 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 29);
        _t53_13 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 30);
        _t53_12 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 31);
        _t53_11 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 56);
        _t53_10 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 57);
        _t53_9 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 58);
        _t53_8 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 59);
        _t53_7 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 84);
        _t53_6 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 85);
        _t53_5 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 86);
        _t53_4 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 87);
        _t53_3 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t53_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t53_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t53_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t53_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_3, _t53_2), _mm256_unpacklo_pd(_t53_1, _t53_0), 32);
        _t53_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_3, _t53_2), _mm256_unpackhi_pd(_t53_1, _t53_0), 32);
        _t53_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_3, _t53_2), _mm256_unpacklo_pd(_t53_1, _t53_0), 49);
        _t53_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_3, _t53_2), _mm256_unpackhi_pd(_t53_1, _t53_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t53_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_19, _t53_24), _mm256_mul_pd(_t53_18, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_17, _t53_26), _mm256_mul_pd(_t53_16, _t53_27)));
        _t53_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_15, _t53_24), _mm256_mul_pd(_t53_14, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_13, _t53_26), _mm256_mul_pd(_t53_12, _t53_27)));
        _t53_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_11, _t53_24), _mm256_mul_pd(_t53_10, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_9, _t53_26), _mm256_mul_pd(_t53_8, _t53_27)));
        _t53_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_7, _t53_24), _mm256_mul_pd(_t53_6, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_5, _t53_26), _mm256_mul_pd(_t53_4, _t53_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t52_8 = _mm256_add_pd(_t52_8, _t53_20);
        _t52_9 = _mm256_add_pd(_t52_9, _t53_21);
        _t52_10 = _mm256_add_pd(_t52_10, _t53_22);
        _t52_11 = _mm256_add_pd(_t52_11, _t53_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M3 + 12*i0 + k3, _t52_8);
      _mm256_storeu_pd(M3 + 12*i0 + k3 + 12, _t52_9);
      _mm256_storeu_pd(M3 + 12*i0 + k3 + 24, _t52_10);
      _mm256_storeu_pd(M3 + 12*i0 + k3 + 36, _t52_11);
    }
    _mm256_storeu_pd(M3 + 13*i0, _t50_28);
    _mm256_maskstore_pd(M3 + 13*i0 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t50_29);
    _mm256_maskstore_pd(M3 + 13*i0 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t50_30);
    _mm256_maskstore_pd(M3 + 13*i0 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t50_31);
  }

  _t54_23 = _mm256_broadcast_sd(M1 + 224);
  _t54_22 = _mm256_broadcast_sd(M1 + 225);
  _t54_21 = _mm256_broadcast_sd(M1 + 226);
  _t54_20 = _mm256_broadcast_sd(M1 + 227);
  _t54_19 = _mm256_broadcast_sd(M1 + 252);
  _t54_18 = _mm256_broadcast_sd(M1 + 253);
  _t54_17 = _mm256_broadcast_sd(M1 + 254);
  _t54_16 = _mm256_broadcast_sd(M1 + 255);
  _t54_15 = _mm256_broadcast_sd(M1 + 280);
  _t54_14 = _mm256_broadcast_sd(M1 + 281);
  _t54_13 = _mm256_broadcast_sd(M1 + 282);
  _t54_12 = _mm256_broadcast_sd(M1 + 283);
  _t54_11 = _mm256_broadcast_sd(M1 + 308);
  _t54_10 = _mm256_broadcast_sd(M1 + 309);
  _t54_9 = _mm256_broadcast_sd(M1 + 310);
  _t54_8 = _mm256_broadcast_sd(M1 + 311);
  _t54_7 = _mm256_loadu_pd(H + 224);
  _t54_6 = _mm256_loadu_pd(H + 252);
  _t54_5 = _mm256_loadu_pd(H + 280);
  _t54_4 = _mm256_loadu_pd(H + 308);
  _t54_3 = _mm256_loadu_pd(R + 104);
  _t54_2 = _mm256_maskload_pd(R + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t54_1 = _mm256_maskload_pd(R + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t54_0 = _mm256_maskload_pd(R + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_7, _t54_6), _mm256_unpacklo_pd(_t54_5, _t54_4), 32);
  _t54_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_7, _t54_6), _mm256_unpackhi_pd(_t54_5, _t54_4), 32);
  _t54_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_7, _t54_6), _mm256_unpacklo_pd(_t54_5, _t54_4), 49);
  _t54_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_7, _t54_6), _mm256_unpackhi_pd(_t54_5, _t54_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t54_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_23, _t54_40), _mm256_mul_pd(_t54_22, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_21, _t54_42), _mm256_mul_pd(_t54_20, _t54_43)));
  _t54_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_19, _t54_40), _mm256_mul_pd(_t54_18, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_17, _t54_42), _mm256_mul_pd(_t54_16, _t54_43)));
  _t54_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_15, _t54_40), _mm256_mul_pd(_t54_14, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_13, _t54_42), _mm256_mul_pd(_t54_12, _t54_43)));
  _t54_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_11, _t54_40), _mm256_mul_pd(_t54_10, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_9, _t54_42), _mm256_mul_pd(_t54_8, _t54_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t54_36 = _t54_3;
  _t54_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 3), _t54_2, 12);
  _t54_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 0), _t54_1, 49);
  _t54_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 12), _mm256_shuffle_pd(_t54_1, _t54_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t54_24 = _mm256_add_pd(_t54_32, _t54_36);
  _t54_25 = _mm256_add_pd(_t54_33, _t54_37);
  _t54_26 = _mm256_add_pd(_t54_34, _t54_38);
  _t54_27 = _mm256_add_pd(_t54_35, _t54_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t54_28 = _t54_24;
  _t54_29 = _t54_25;
  _t54_30 = _t54_26;
  _t54_31 = _t54_27;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t55_19 = _mm256_broadcast_sd(M1 + k2 + 224);
    _t55_18 = _mm256_broadcast_sd(M1 + k2 + 225);
    _t55_17 = _mm256_broadcast_sd(M1 + k2 + 226);
    _t55_16 = _mm256_broadcast_sd(M1 + k2 + 227);
    _t55_15 = _mm256_broadcast_sd(M1 + k2 + 252);
    _t55_14 = _mm256_broadcast_sd(M1 + k2 + 253);
    _t55_13 = _mm256_broadcast_sd(M1 + k2 + 254);
    _t55_12 = _mm256_broadcast_sd(M1 + k2 + 255);
    _t55_11 = _mm256_broadcast_sd(M1 + k2 + 280);
    _t55_10 = _mm256_broadcast_sd(M1 + k2 + 281);
    _t55_9 = _mm256_broadcast_sd(M1 + k2 + 282);
    _t55_8 = _mm256_broadcast_sd(M1 + k2 + 283);
    _t55_7 = _mm256_broadcast_sd(M1 + k2 + 308);
    _t55_6 = _mm256_broadcast_sd(M1 + k2 + 309);
    _t55_5 = _mm256_broadcast_sd(M1 + k2 + 310);
    _t55_4 = _mm256_broadcast_sd(M1 + k2 + 311);
    _t55_3 = _mm256_loadu_pd(H + k2 + 224);
    _t55_2 = _mm256_loadu_pd(H + k2 + 252);
    _t55_1 = _mm256_loadu_pd(H + k2 + 280);
    _t55_0 = _mm256_loadu_pd(H + k2 + 308);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t55_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32);
    _t55_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_3, _t55_2), _mm256_unpackhi_pd(_t55_1, _t55_0), 32);
    _t55_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 49);
    _t55_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_3, _t55_2), _mm256_unpackhi_pd(_t55_1, _t55_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t55_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_19, _t55_28), _mm256_mul_pd(_t55_18, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_17, _t55_30), _mm256_mul_pd(_t55_16, _t55_31)));
    _t55_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_15, _t55_28), _mm256_mul_pd(_t55_14, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_13, _t55_30), _mm256_mul_pd(_t55_12, _t55_31)));
    _t55_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_11, _t55_28), _mm256_mul_pd(_t55_10, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_9, _t55_30), _mm256_mul_pd(_t55_8, _t55_31)));
    _t55_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_7, _t55_28), _mm256_mul_pd(_t55_6, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_5, _t55_30), _mm256_mul_pd(_t55_4, _t55_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t55_24 = _t54_28;
    _t55_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 3), _t54_29, 12);
    _t55_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 0), _t54_30, 49);
    _t55_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 12), _mm256_shuffle_pd(_t54_30, _t54_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t55_24 = _mm256_add_pd(_t55_24, _t55_20);
    _t55_25 = _mm256_add_pd(_t55_25, _t55_21);
    _t55_26 = _mm256_add_pd(_t55_26, _t55_22);
    _t55_27 = _mm256_add_pd(_t55_27, _t55_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t54_28 = _t55_24;
    _t54_29 = _t55_25;
    _t54_30 = _t55_26;
    _t54_31 = _t55_27;
  }

  _t56_21 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_23 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_24 = _mm256_maskload_pd(M3 + 13, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_25 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_26 = _mm256_maskload_pd(M3 + 37, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t56_27 = _mm256_maskload_pd(M3 + 13, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_29 = _mm256_maskload_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_30 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_31 = _mm256_maskload_pd(M3 + 38, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t56_32 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_33 = _mm256_maskload_pd(M3 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_34 = _mm256_maskload_pd(M3 + 39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_43 = _mm256_loadu_pd(M3 + 4);
  _t56_44 = _mm256_loadu_pd(M3 + 8);
  _t56_37 = _mm256_loadu_pd(M3 + 16);
  _t56_40 = _mm256_loadu_pd(M3 + 20);
  _t56_38 = _mm256_loadu_pd(M3 + 28);
  _t56_41 = _mm256_loadu_pd(M3 + 32);
  _t56_39 = _mm256_loadu_pd(M3 + 40);
  _t56_42 = _mm256_loadu_pd(M3 + 44);
  _t56_17 = _mm256_broadcast_sd(&(M3[27]));

  // Generating : M3[12,12] = S(h(1, 12, fi39), Sqrt( G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ),h(1, 12, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_46 = _t56_21;

  // 4-BLAC: sqrt(1x4)
  _t56_47 = _mm256_sqrt_pd(_t56_46);

  // AVX Storer:
  _t56_21 = _t56_47;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ),h(1, 12, fi39))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_48 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_49 = _t56_21;

  // 4-BLAC: 1x4 / 1x4
  _t56_50 = _mm256_div_pd(_t56_48, _t56_49);

  // AVX Storer:
  _t56_22 = _t56_50;

  // Generating : M3[12,12] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ),h(3, 12, fi39 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_51 = _t56_20;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_52 = _t56_23;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_53 = _mm256_mul_pd(_t56_51, _t56_52);

  // AVX Storer:
  _t56_23 = _t56_53;

  // Generating : M3[12,12] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M3[12,12],h(3, 12, fi39 + 1)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ),h(3, 12, fi39 + 1))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t56_54 = _t56_24;
  _t56_55 = _mm256_blend_pd(_mm256_shuffle_pd(_t56_24, _t56_25, 3), _t56_25, 12);
  _t56_56 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t56_24, _t56_25, 0), _t56_26, 49);
  _t56_57 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_58 = _t56_23;

  // 4-BLAC: (1x4)^T
  _t56_59 = _t56_58;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_60 = _t56_23;

  // 4-BLAC: 4x1 * 1x4
  _t56_61 = _mm256_mul_pd(_t56_15, _t56_60);
  _t56_62 = _mm256_mul_pd(_t56_14, _t56_60);
  _t56_63 = _mm256_mul_pd(_t56_13, _t56_60);
  _t56_64 = _mm256_mul_pd(_t56_12, _t56_60);

  // 4-BLAC: 4x4 - 4x4
  _t56_65 = _mm256_sub_pd(_t56_54, _t56_61);
  _t56_66 = _mm256_sub_pd(_t56_55, _t56_62);
  _t56_67 = _mm256_sub_pd(_t56_56, _t56_63);
  _t56_68 = _mm256_sub_pd(_t56_57, _t56_64);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t56_24 = _t56_65;
  _t56_25 = _t56_66;
  _t56_26 = _t56_67;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), Sqrt( G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ),h(1, 12, fi39 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_69 = _t56_27;

  // 4-BLAC: sqrt(1x4)
  _t56_70 = _mm256_sqrt_pd(_t56_69);

  // AVX Storer:
  _t56_27 = _t56_70;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ),h(1, 12, fi39 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_71 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_72 = _t56_27;

  // 4-BLAC: 1x4 / 1x4
  _t56_73 = _mm256_div_pd(_t56_71, _t56_72);

  // AVX Storer:
  _t56_28 = _t56_73;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ),h(2, 12, fi39 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_74 = _t56_19;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_75 = _t56_29;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_76 = _mm256_mul_pd(_t56_74, _t56_75);

  // AVX Storer:
  _t56_29 = _t56_76;

  // Generating : M3[12,12] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M3[12,12],h(2, 12, fi39 + 2)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ),h(2, 12, fi39 + 2))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t56_77 = _t56_30;
  _t56_78 = _mm256_shuffle_pd(_t56_30, _t56_31, 3);
  _t56_79 = _mm256_setzero_pd();
  _t56_80 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_81 = _t56_29;

  // 4-BLAC: (1x4)^T
  _t56_82 = _t56_81;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_83 = _t56_29;

  // 4-BLAC: 4x1 * 1x4
  _t56_84 = _mm256_mul_pd(_t56_11, _t56_83);
  _t56_85 = _mm256_mul_pd(_t56_10, _t56_83);
  _t56_86 = _mm256_mul_pd(_t56_9, _t56_83);
  _t56_87 = _mm256_mul_pd(_t56_8, _t56_83);

  // 4-BLAC: 4x4 - 4x4
  _t56_88 = _mm256_sub_pd(_t56_77, _t56_84);
  _t56_89 = _mm256_sub_pd(_t56_78, _t56_85);
  _t56_90 = _mm256_sub_pd(_t56_79, _t56_86);
  _t56_91 = _mm256_sub_pd(_t56_80, _t56_87);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t56_30 = _t56_88;
  _t56_31 = _t56_89;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), Sqrt( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_92 = _t56_32;

  // 4-BLAC: sqrt(1x4)
  _t56_93 = _mm256_sqrt_pd(_t56_92);

  // AVX Storer:
  _t56_32 = _t56_93;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), ( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) Div G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_94 = _t56_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_95 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t56_96 = _mm256_div_pd(_t56_94, _t56_95);

  // AVX Storer:
  _t56_33 = _t56_96;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_97 = _t56_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_98 = _t56_33;

  // 4-BLAC: (4x1)^T
  _t56_99 = _t56_98;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_100 = _t56_33;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_101 = _mm256_mul_pd(_t56_99, _t56_100);

  // 4-BLAC: 1x4 - 1x4
  _t56_102 = _mm256_sub_pd(_t56_97, _t56_101);

  // AVX Storer:
  _t56_34 = _t56_102;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), Sqrt( G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_103 = _t56_34;

  // 4-BLAC: sqrt(1x4)
  _t56_104 = _mm256_sqrt_pd(_t56_103);

  // AVX Storer:
  _t56_34 = _t56_104;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_105 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_106 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t56_107 = _mm256_div_pd(_t56_105, _t56_106);

  // AVX Storer:
  _t56_35 = _t56_107;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_108 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_109 = _t56_34;

  // 4-BLAC: 1x4 / 1x4
  _t56_110 = _mm256_div_pd(_t56_108, _t56_109);

  // AVX Storer:
  _t56_36 = _t56_110;

  // Generating : M3[12,12] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_111 = _t56_20;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_43 = _mm256_mul_pd(_t56_111, _t56_43);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t56_112 = _t56_37;
  _t56_113 = _t56_38;
  _t56_114 = _t56_39;
  _t56_115 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_116 = _t56_23;

  // 4-BLAC: (1x4)^T
  _t56_117 = _t56_116;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_118 = _mm256_mul_pd(_t56_7, _t56_43);
  _t56_119 = _mm256_mul_pd(_t56_6, _t56_43);
  _t56_120 = _mm256_mul_pd(_t56_5, _t56_43);
  _t56_121 = _mm256_mul_pd(_t56_4, _t56_43);

  // 4-BLAC: 4x4 - 4x4
  _t56_122 = _mm256_sub_pd(_t56_112, _t56_118);
  _t56_123 = _mm256_sub_pd(_t56_113, _t56_119);
  _t56_124 = _mm256_sub_pd(_t56_114, _t56_120);
  _t56_125 = _mm256_sub_pd(_t56_115, _t56_121);

  // AVX Storer:
  _t56_37 = _t56_122;
  _t56_38 = _t56_123;
  _t56_39 = _t56_124;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_126 = _t56_19;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_37 = _mm256_mul_pd(_t56_126, _t56_37);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t56_127 = _t56_38;
  _t56_128 = _t56_39;
  _t56_129 = _mm256_setzero_pd();
  _t56_130 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_131 = _t56_29;

  // 4-BLAC: (1x4)^T
  _t56_132 = _t56_131;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_133 = _mm256_mul_pd(_t56_3, _t56_37);
  _t56_134 = _mm256_mul_pd(_t56_2, _t56_37);
  _t56_135 = _mm256_mul_pd(_t56_1, _t56_37);
  _t56_136 = _mm256_mul_pd(_t56_0, _t56_37);

  // 4-BLAC: 4x4 - 4x4
  _t56_137 = _mm256_sub_pd(_t56_127, _t56_133);
  _t56_138 = _mm256_sub_pd(_t56_128, _t56_134);
  _t56_139 = _mm256_sub_pd(_t56_129, _t56_135);
  _t56_140 = _mm256_sub_pd(_t56_130, _t56_136);

  // AVX Storer:
  _t56_38 = _t56_137;
  _t56_39 = _t56_138;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_141 = _t56_18;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_38 = _mm256_mul_pd(_t56_141, _t56_38);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_142 = _t56_17;

  // 4-BLAC: (4x1)^T
  _t56_143 = _t56_142;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_45 = _mm256_mul_pd(_t56_143, _t56_38);

  // 4-BLAC: 1x4 - 1x4
  _t56_39 = _mm256_sub_pd(_t56_39, _t56_45);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) Kro G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_144 = _t56_16;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_39 = _mm256_mul_pd(_t56_144, _t56_39);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_145 = _t56_20;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_44 = _mm256_mul_pd(_t56_145, _t56_44);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t56_146 = _t56_40;
  _t56_147 = _t56_41;
  _t56_148 = _t56_42;
  _t56_149 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_150 = _t56_23;

  // 4-BLAC: (1x4)^T
  _t56_117 = _t56_150;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_118 = _mm256_mul_pd(_t56_7, _t56_44);
  _t56_119 = _mm256_mul_pd(_t56_6, _t56_44);
  _t56_120 = _mm256_mul_pd(_t56_5, _t56_44);
  _t56_121 = _mm256_mul_pd(_t56_4, _t56_44);

  // 4-BLAC: 4x4 - 4x4
  _t56_151 = _mm256_sub_pd(_t56_146, _t56_118);
  _t56_152 = _mm256_sub_pd(_t56_147, _t56_119);
  _t56_153 = _mm256_sub_pd(_t56_148, _t56_120);
  _t56_154 = _mm256_sub_pd(_t56_149, _t56_121);

  // AVX Storer:
  _t56_40 = _t56_151;
  _t56_41 = _t56_152;
  _t56_42 = _t56_153;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_155 = _t56_19;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_40 = _mm256_mul_pd(_t56_155, _t56_40);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t56_156 = _t56_41;
  _t56_157 = _t56_42;
  _t56_158 = _mm256_setzero_pd();
  _t56_159 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_160 = _t56_29;

  // 4-BLAC: (1x4)^T
  _t56_132 = _t56_160;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_133 = _mm256_mul_pd(_t56_3, _t56_40);
  _t56_134 = _mm256_mul_pd(_t56_2, _t56_40);
  _t56_135 = _mm256_mul_pd(_t56_1, _t56_40);
  _t56_136 = _mm256_mul_pd(_t56_0, _t56_40);

  // 4-BLAC: 4x4 - 4x4
  _t56_161 = _mm256_sub_pd(_t56_156, _t56_133);
  _t56_162 = _mm256_sub_pd(_t56_157, _t56_134);
  _t56_163 = _mm256_sub_pd(_t56_158, _t56_135);
  _t56_164 = _mm256_sub_pd(_t56_159, _t56_136);

  // AVX Storer:
  _t56_41 = _t56_161;
  _t56_42 = _t56_162;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_165 = _t56_18;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_41 = _mm256_mul_pd(_t56_165, _t56_41);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_166 = _t56_17;

  // 4-BLAC: (4x1)^T
  _t56_143 = _t56_166;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_45 = _mm256_mul_pd(_t56_143, _t56_41);

  // 4-BLAC: 1x4 - 1x4
  _t56_42 = _mm256_sub_pd(_t56_42, _t56_45);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) Kro G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_167 = _t56_16;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_42 = _mm256_mul_pd(_t56_167, _t56_42);

  // AVX Storer:

  // Generating : M3[12,12] = Sum_{i0} ( S(h(4, 12, fi39 + i0 + 4), ( G(h(4, 12, fi39 + i0 + 4), M3[12,12],h(4, 12, fi39 + i0 + 4)) - ( T( G(h(4, 12, fi39), M3[12,12],h(4, 12, fi39 + i0 + 4)) ) * G(h(4, 12, fi39), M3[12,12],h(4, 12, fi39 + i0 + 4)) ) ),h(4, 12, fi39 + i0 + 4)) )

  _mm256_storeu_pd(M3 + 104, _t54_28);
  _mm256_maskstore_pd(M3 + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t54_29);
  _mm256_maskstore_pd(M3 + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t54_30);
  _mm256_maskstore_pd(M3 + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t54_31);
  _mm256_storeu_pd(M3 + 4, _t56_43);
  _mm256_storeu_pd(M3 + 16, _t56_37);
  _mm256_storeu_pd(M3 + 28, _t56_38);
  _mm256_storeu_pd(M3 + 40, _t56_39);
  _mm256_storeu_pd(M3 + 8, _t56_44);
  _mm256_storeu_pd(M3 + 20, _t56_40);
  _mm256_storeu_pd(M3 + 32, _t56_41);
  _mm256_storeu_pd(M3 + 44, _t56_42);

  for( int i0 = 0; i0 <= 7; i0+=4 ) {
    _t57_20 = _mm256_loadu_pd(M3 + 13*i0 + 52);
    _t57_21 = _mm256_maskload_pd(M3 + 13*i0 + 64, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t57_22 = _mm256_maskload_pd(M3 + 13*i0 + 76, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t57_23 = _mm256_maskload_pd(M3 + 13*i0 + 88, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t57_19 = _mm256_loadu_pd(M3 + i0 + 4);
    _t57_18 = _mm256_loadu_pd(M3 + i0 + 16);
    _t57_17 = _mm256_loadu_pd(M3 + i0 + 28);
    _t57_16 = _mm256_loadu_pd(M3 + i0 + 40);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t57_32 = _t57_20;
    _t57_33 = _mm256_blend_pd(_mm256_shuffle_pd(_t57_20, _t57_21, 3), _t57_21, 12);
    _t57_34 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t57_20, _t57_21, 0), _t57_22, 49);
    _t57_35 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t57_20, _t57_21, 12), _mm256_shuffle_pd(_t57_22, _t57_23, 12), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t57_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t57_19, _t57_18), _mm256_unpacklo_pd(_t57_17, _t57_16), 32);
    _t57_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t57_19, _t57_18), _mm256_unpackhi_pd(_t57_17, _t57_16), 32);
    _t57_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t57_19, _t57_18), _mm256_unpacklo_pd(_t57_17, _t57_16), 49);
    _t57_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t57_19, _t57_18), _mm256_unpackhi_pd(_t57_17, _t57_16), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t57_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t57_15, _t57_19), _mm256_mul_pd(_t57_14, _t57_18)), _mm256_add_pd(_mm256_mul_pd(_t57_13, _t57_17), _mm256_mul_pd(_t57_12, _t57_16)));
    _t57_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t57_11, _t57_19), _mm256_mul_pd(_t57_10, _t57_18)), _mm256_add_pd(_mm256_mul_pd(_t57_9, _t57_17), _mm256_mul_pd(_t57_8, _t57_16)));
    _t57_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t57_7, _t57_19), _mm256_mul_pd(_t57_6, _t57_18)), _mm256_add_pd(_mm256_mul_pd(_t57_5, _t57_17), _mm256_mul_pd(_t57_4, _t57_16)));
    _t57_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t57_3, _t57_19), _mm256_mul_pd(_t57_2, _t57_18)), _mm256_add_pd(_mm256_mul_pd(_t57_1, _t57_17), _mm256_mul_pd(_t57_0, _t57_16)));

    // 4-BLAC: 4x4 - 4x4
    _t57_28 = _mm256_sub_pd(_t57_32, _t57_24);
    _t57_29 = _mm256_sub_pd(_t57_33, _t57_25);
    _t57_30 = _mm256_sub_pd(_t57_34, _t57_26);
    _t57_31 = _mm256_sub_pd(_t57_35, _t57_27);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t57_20 = _t57_28;
    _t57_21 = _t57_29;
    _t57_22 = _t57_30;
    _t57_23 = _t57_31;
    _mm256_storeu_pd(M3 + 13*i0 + 52, _t57_20);
    _mm256_maskstore_pd(M3 + 13*i0 + 64, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t57_21);
    _mm256_maskstore_pd(M3 + 13*i0 + 76, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t57_22);
    _mm256_maskstore_pd(M3 + 13*i0 + 88, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t57_23);
  }

  _t54_30 = _mm256_maskload_pd(M3 + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t54_31 = _mm256_maskload_pd(M3 + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t54_29 = _mm256_maskload_pd(M3 + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t54_28 = _mm256_loadu_pd(M3 + 104);
  _t58_31 = _mm256_maskload_pd(M3 + 52, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_33 = _mm256_maskload_pd(M3 + 53, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_34 = _mm256_maskload_pd(M3 + 65, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_35 = _mm256_maskload_pd(M3 + 77, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_36 = _mm256_maskload_pd(M3 + 89, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t58_37 = _mm256_maskload_pd(M3 + 65, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_39 = _mm256_maskload_pd(M3 + 66, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t58_40 = _mm256_maskload_pd(M3 + 78, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t58_41 = _mm256_maskload_pd(M3 + 90, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t58_42 = _mm256_maskload_pd(M3 + 78, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_43 = _mm256_maskload_pd(M3 + 79, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_44 = _mm256_maskload_pd(M3 + 91, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_64 = _mm256_loadu_pd(M3 + 56);
  _t58_47 = _mm256_loadu_pd(M3 + 68);
  _t58_48 = _mm256_loadu_pd(M3 + 80);
  _t58_49 = _mm256_loadu_pd(M3 + 92);
  _t58_27 = _mm256_broadcast_sd(&(M3[79]));
  _t58_50 = _mm256_maskload_pd(M3 + 104, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_52 = _mm256_maskload_pd(M3 + 105, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_53 = _mm256_maskload_pd(M3 + 117, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_54 = _mm256_maskload_pd(M3 + 129, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_55 = _mm256_maskload_pd(M3 + 141, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t58_56 = _mm256_maskload_pd(M3 + 117, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_58 = _mm256_maskload_pd(M3 + 118, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t58_59 = _mm256_maskload_pd(M3 + 130, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t58_60 = _mm256_maskload_pd(M3 + 142, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t58_61 = _mm256_maskload_pd(M3 + 130, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_62 = _mm256_maskload_pd(M3 + 131, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_63 = _mm256_maskload_pd(M3 + 143, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

  // Generating : M3[12,12] = S(h(1, 12, fi39), Sqrt( G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ),h(1, 12, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_73 = _t58_31;

  // 4-BLAC: sqrt(1x4)
  _t58_74 = _mm256_sqrt_pd(_t58_73);

  // AVX Storer:
  _t58_31 = _t58_74;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ),h(1, 12, fi39))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_75 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_76 = _t58_31;

  // 4-BLAC: 1x4 / 1x4
  _t58_77 = _mm256_div_pd(_t58_75, _t58_76);

  // AVX Storer:
  _t58_32 = _t58_77;

  // Generating : M3[12,12] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ),h(3, 12, fi39 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_78 = _t58_30;

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_79 = _t58_33;

  // 4-BLAC: 1x4 Kro 1x4
  _t58_80 = _mm256_mul_pd(_t58_78, _t58_79);

  // AVX Storer:
  _t58_33 = _t58_80;

  // Generating : M3[12,12] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M3[12,12],h(3, 12, fi39 + 1)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) ),h(3, 12, fi39 + 1))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t58_81 = _t58_34;
  _t58_82 = _mm256_blend_pd(_mm256_shuffle_pd(_t58_34, _t58_35, 3), _t58_35, 12);
  _t58_83 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t58_34, _t58_35, 0), _t58_36, 49);
  _t58_84 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_85 = _t58_33;

  // 4-BLAC: (1x4)^T
  _t56_59 = _t58_85;

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_86 = _t58_33;

  // 4-BLAC: 4x1 * 1x4
  _t56_61 = _mm256_mul_pd(_t56_15, _t58_86);
  _t56_62 = _mm256_mul_pd(_t56_14, _t58_86);
  _t56_63 = _mm256_mul_pd(_t56_13, _t58_86);
  _t56_64 = _mm256_mul_pd(_t56_12, _t58_86);

  // 4-BLAC: 4x4 - 4x4
  _t58_87 = _mm256_sub_pd(_t58_81, _t56_61);
  _t58_88 = _mm256_sub_pd(_t58_82, _t56_62);
  _t58_89 = _mm256_sub_pd(_t58_83, _t56_63);
  _t58_90 = _mm256_sub_pd(_t58_84, _t56_64);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t58_34 = _t58_87;
  _t58_35 = _t58_88;
  _t58_36 = _t58_89;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), Sqrt( G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ),h(1, 12, fi39 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_91 = _t58_37;

  // 4-BLAC: sqrt(1x4)
  _t58_92 = _mm256_sqrt_pd(_t58_91);

  // AVX Storer:
  _t58_37 = _t58_92;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ),h(1, 12, fi39 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_93 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_94 = _t58_37;

  // 4-BLAC: 1x4 / 1x4
  _t58_95 = _mm256_div_pd(_t58_93, _t58_94);

  // AVX Storer:
  _t58_38 = _t58_95;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ),h(2, 12, fi39 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_96 = _t58_29;

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_97 = _t58_39;

  // 4-BLAC: 1x4 Kro 1x4
  _t58_98 = _mm256_mul_pd(_t58_96, _t58_97);

  // AVX Storer:
  _t58_39 = _t58_98;

  // Generating : M3[12,12] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M3[12,12],h(2, 12, fi39 + 2)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) ),h(2, 12, fi39 + 2))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t58_99 = _t58_40;
  _t58_100 = _mm256_shuffle_pd(_t58_40, _t58_41, 3);
  _t58_101 = _mm256_setzero_pd();
  _t58_102 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_103 = _t58_39;

  // 4-BLAC: (1x4)^T
  _t56_82 = _t58_103;

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_104 = _t58_39;

  // 4-BLAC: 4x1 * 1x4
  _t56_84 = _mm256_mul_pd(_t56_11, _t58_104);
  _t56_85 = _mm256_mul_pd(_t56_10, _t58_104);
  _t56_86 = _mm256_mul_pd(_t56_9, _t58_104);
  _t56_87 = _mm256_mul_pd(_t56_8, _t58_104);

  // 4-BLAC: 4x4 - 4x4
  _t58_105 = _mm256_sub_pd(_t58_99, _t56_84);
  _t58_106 = _mm256_sub_pd(_t58_100, _t56_85);
  _t58_107 = _mm256_sub_pd(_t58_101, _t56_86);
  _t58_108 = _mm256_sub_pd(_t58_102, _t56_87);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t58_40 = _t58_105;
  _t58_41 = _t58_106;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), Sqrt( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_109 = _t58_42;

  // 4-BLAC: sqrt(1x4)
  _t58_110 = _mm256_sqrt_pd(_t58_109);

  // AVX Storer:
  _t58_42 = _t58_110;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), ( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) Div G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_111 = _t58_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_112 = _t58_42;

  // 4-BLAC: 1x4 / 1x4
  _t58_113 = _mm256_div_pd(_t58_111, _t58_112);

  // AVX Storer:
  _t58_43 = _t58_113;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_114 = _t58_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_115 = _t58_43;

  // 4-BLAC: (4x1)^T
  _t56_99 = _t58_115;

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_116 = _t58_43;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_101 = _mm256_mul_pd(_t56_99, _t58_116);

  // 4-BLAC: 1x4 - 1x4
  _t58_117 = _mm256_sub_pd(_t58_114, _t56_101);

  // AVX Storer:
  _t58_44 = _t58_117;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), Sqrt( G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_118 = _t58_44;

  // 4-BLAC: sqrt(1x4)
  _t58_119 = _mm256_sqrt_pd(_t58_118);

  // AVX Storer:
  _t58_44 = _t58_119;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_120 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_121 = _t58_42;

  // 4-BLAC: 1x4 / 1x4
  _t58_122 = _mm256_div_pd(_t58_120, _t58_121);

  // AVX Storer:
  _t58_45 = _t58_122;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ),h(1, 12, fi39 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_123 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_124 = _t58_44;

  // 4-BLAC: 1x4 / 1x4
  _t58_125 = _mm256_div_pd(_t58_123, _t58_124);

  // AVX Storer:
  _t58_46 = _t58_125;

  // Generating : M3[12,12] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_126 = _t58_30;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_64 = _mm256_mul_pd(_t58_126, _t58_64);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t58_127 = _t58_47;
  _t58_128 = _t58_48;
  _t58_129 = _t58_49;
  _t58_130 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_131 = _t58_33;

  // 4-BLAC: (1x4)^T
  _t56_117 = _t58_131;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_118 = _mm256_mul_pd(_t56_7, _t58_64);
  _t56_119 = _mm256_mul_pd(_t56_6, _t58_64);
  _t56_120 = _mm256_mul_pd(_t56_5, _t58_64);
  _t56_121 = _mm256_mul_pd(_t56_4, _t58_64);

  // 4-BLAC: 4x4 - 4x4
  _t58_132 = _mm256_sub_pd(_t58_127, _t56_118);
  _t58_133 = _mm256_sub_pd(_t58_128, _t56_119);
  _t58_134 = _mm256_sub_pd(_t58_129, _t56_120);
  _t58_135 = _mm256_sub_pd(_t58_130, _t56_121);

  // AVX Storer:
  _t58_47 = _t58_132;
  _t58_48 = _t58_133;
  _t58_49 = _t58_134;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_136 = _t58_29;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_47 = _mm256_mul_pd(_t58_136, _t58_47);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t58_137 = _t58_48;
  _t58_138 = _t58_49;
  _t58_139 = _mm256_setzero_pd();
  _t58_140 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_141 = _t58_39;

  // 4-BLAC: (1x4)^T
  _t56_132 = _t58_141;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_133 = _mm256_mul_pd(_t56_3, _t58_47);
  _t56_134 = _mm256_mul_pd(_t56_2, _t58_47);
  _t56_135 = _mm256_mul_pd(_t56_1, _t58_47);
  _t56_136 = _mm256_mul_pd(_t56_0, _t58_47);

  // 4-BLAC: 4x4 - 4x4
  _t58_142 = _mm256_sub_pd(_t58_137, _t56_133);
  _t58_143 = _mm256_sub_pd(_t58_138, _t56_134);
  _t58_144 = _mm256_sub_pd(_t58_139, _t56_135);
  _t58_145 = _mm256_sub_pd(_t58_140, _t56_136);

  // AVX Storer:
  _t58_48 = _t58_142;
  _t58_49 = _t58_143;

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_146 = _t58_28;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_48 = _mm256_mul_pd(_t58_146, _t58_48);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_147 = _t58_27;

  // 4-BLAC: (4x1)^T
  _t56_143 = _t58_147;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_45 = _mm256_mul_pd(_t56_143, _t58_48);

  // 4-BLAC: 1x4 - 1x4
  _t58_49 = _mm256_sub_pd(_t58_49, _t56_45);

  // AVX Storer:

  // Generating : M3[12,12] = S(h(1, 12, fi39 + 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) Kro G(h(1, 12, fi39 + 3), M3[12,12],h(4, 12, fi100 + fi39 + 4)) ),h(4, 12, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_148 = _t58_26;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_49 = _mm256_mul_pd(_t58_148, _t58_49);

  // AVX Storer:

  // Generating : M3[12,12] = Sum_{i0} ( S(h(4, 12, fi39 + i0 + 4), ( G(h(4, 12, fi39 + i0 + 4), M3[12,12],h(4, 12, fi39 + i0 + 4)) - ( T( G(h(4, 12, fi39), M3[12,12],h(4, 12, fi39 + i0 + 4)) ) * G(h(4, 12, fi39), M3[12,12],h(4, 12, fi39 + i0 + 4)) ) ),h(4, 12, fi39 + i0 + 4)) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t58_149 = _t54_28;
  _t58_150 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 3), _t54_29, 12);
  _t58_151 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 0), _t54_30, 49);
  _t58_152 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 12), _mm256_shuffle_pd(_t54_30, _t54_31, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t58_212 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_64, _t58_47), _mm256_unpacklo_pd(_t58_48, _t58_49), 32);
  _t58_213 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_64, _t58_47), _mm256_unpackhi_pd(_t58_48, _t58_49), 32);
  _t58_214 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_64, _t58_47), _mm256_unpacklo_pd(_t58_48, _t58_49), 49);
  _t58_215 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_64, _t58_47), _mm256_unpackhi_pd(_t58_48, _t58_49), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t58_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_15, _t58_64), _mm256_mul_pd(_t58_14, _t58_47)), _mm256_add_pd(_mm256_mul_pd(_t58_13, _t58_48), _mm256_mul_pd(_t58_12, _t58_49)));
  _t58_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_11, _t58_64), _mm256_mul_pd(_t58_10, _t58_47)), _mm256_add_pd(_mm256_mul_pd(_t58_9, _t58_48), _mm256_mul_pd(_t58_8, _t58_49)));
  _t58_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_7, _t58_64), _mm256_mul_pd(_t58_6, _t58_47)), _mm256_add_pd(_mm256_mul_pd(_t58_5, _t58_48), _mm256_mul_pd(_t58_4, _t58_49)));
  _t58_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_3, _t58_64), _mm256_mul_pd(_t58_2, _t58_47)), _mm256_add_pd(_mm256_mul_pd(_t58_1, _t58_48), _mm256_mul_pd(_t58_0, _t58_49)));

  // 4-BLAC: 4x4 - 4x4
  _t58_69 = _mm256_sub_pd(_t58_149, _t58_65);
  _t58_70 = _mm256_sub_pd(_t58_150, _t58_66);
  _t58_71 = _mm256_sub_pd(_t58_151, _t58_67);
  _t58_72 = _mm256_sub_pd(_t58_152, _t58_68);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t54_28 = _t58_69;
  _t54_29 = _t58_70;
  _t54_30 = _t58_71;
  _t54_31 = _t58_72;

  // Generating : M3[12,12] = S(h(1, 12, 8), Sqrt( G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ),h(1, 12, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_153 = _t58_50;

  // 4-BLAC: sqrt(1x4)
  _t58_154 = _mm256_sqrt_pd(_t58_153);

  // AVX Storer:
  _t58_50 = _t58_154;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ),h(1, 12, 8))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_155 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_156 = _t58_50;

  // 4-BLAC: 1x4 / 1x4
  _t58_157 = _mm256_div_pd(_t58_155, _t58_156);

  // AVX Storer:
  _t58_51 = _t58_157;

  // Generating : M3[12,12] = S(h(1, 12, 8), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 8)) Kro G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ),h(3, 12, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_158 = _t58_25;

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_159 = _t58_52;

  // 4-BLAC: 1x4 Kro 1x4
  _t58_160 = _mm256_mul_pd(_t58_158, _t58_159);

  // AVX Storer:
  _t58_52 = _t58_160;

  // Generating : M3[12,12] = S(h(3, 12, 9), ( G(h(3, 12, 9), M3[12,12],h(3, 12, 9)) - ( T( G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) * G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) ),h(3, 12, 9))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t58_161 = _t58_53;
  _t58_162 = _mm256_blend_pd(_mm256_shuffle_pd(_t58_53, _t58_54, 3), _t58_54, 12);
  _t58_163 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t58_53, _t58_54, 0), _t58_55, 49);
  _t58_164 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_165 = _t58_52;

  // 4-BLAC: (1x4)^T
  _t58_166 = _t58_165;

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_167 = _t58_52;

  // 4-BLAC: 4x1 * 1x4
  _t58_168 = _mm256_mul_pd(_t58_23, _t58_167);
  _t58_169 = _mm256_mul_pd(_t58_22, _t58_167);
  _t58_170 = _mm256_mul_pd(_t58_21, _t58_167);
  _t58_171 = _mm256_mul_pd(_t58_20, _t58_167);

  // 4-BLAC: 4x4 - 4x4
  _t58_172 = _mm256_sub_pd(_t58_161, _t58_168);
  _t58_173 = _mm256_sub_pd(_t58_162, _t58_169);
  _t58_174 = _mm256_sub_pd(_t58_163, _t58_170);
  _t58_175 = _mm256_sub_pd(_t58_164, _t58_171);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t58_53 = _t58_172;
  _t58_54 = _t58_173;
  _t58_55 = _t58_174;

  // Generating : M3[12,12] = S(h(1, 12, 9), Sqrt( G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ),h(1, 12, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_176 = _t58_56;

  // 4-BLAC: sqrt(1x4)
  _t58_177 = _mm256_sqrt_pd(_t58_176);

  // AVX Storer:
  _t58_56 = _t58_177;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ),h(1, 12, 9))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_178 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_179 = _t58_56;

  // 4-BLAC: 1x4 / 1x4
  _t58_180 = _mm256_div_pd(_t58_178, _t58_179);

  // AVX Storer:
  _t58_57 = _t58_180;

  // Generating : M3[12,12] = S(h(1, 12, 9), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 9)) Kro G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ),h(2, 12, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_181 = _t58_24;

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_182 = _t58_58;

  // 4-BLAC: 1x4 Kro 1x4
  _t58_183 = _mm256_mul_pd(_t58_181, _t58_182);

  // AVX Storer:
  _t58_58 = _t58_183;

  // Generating : M3[12,12] = S(h(2, 12, 10), ( G(h(2, 12, 10), M3[12,12],h(2, 12, 10)) - ( T( G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) * G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) ),h(2, 12, 10))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t58_184 = _t58_59;
  _t58_185 = _mm256_shuffle_pd(_t58_59, _t58_60, 3);
  _t58_186 = _mm256_setzero_pd();
  _t58_187 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_188 = _t58_58;

  // 4-BLAC: (1x4)^T
  _t58_189 = _t58_188;

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_190 = _t58_58;

  // 4-BLAC: 4x1 * 1x4
  _t58_191 = _mm256_mul_pd(_t58_19, _t58_190);
  _t58_192 = _mm256_mul_pd(_t58_18, _t58_190);
  _t58_193 = _mm256_mul_pd(_t58_17, _t58_190);
  _t58_194 = _mm256_mul_pd(_t58_16, _t58_190);

  // 4-BLAC: 4x4 - 4x4
  _t58_195 = _mm256_sub_pd(_t58_184, _t58_191);
  _t58_196 = _mm256_sub_pd(_t58_185, _t58_192);
  _t58_197 = _mm256_sub_pd(_t58_186, _t58_193);
  _t58_198 = _mm256_sub_pd(_t58_187, _t58_194);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t58_59 = _t58_195;
  _t58_60 = _t58_196;

  // Generating : M3[12,12] = S(h(1, 12, 10), Sqrt( G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ),h(1, 12, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_199 = _t58_61;

  // 4-BLAC: sqrt(1x4)
  _t58_200 = _mm256_sqrt_pd(_t58_199);

  // AVX Storer:
  _t58_61 = _t58_200;

  // Generating : M3[12,12] = S(h(1, 12, 10), ( G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) Div G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ),h(1, 12, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_201 = _t58_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_202 = _t58_61;

  // 4-BLAC: 1x4 / 1x4
  _t58_203 = _mm256_div_pd(_t58_201, _t58_202);

  // AVX Storer:
  _t58_62 = _t58_203;

  // Generating : M3[12,12] = S(h(1, 12, 11), ( G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) - ( T( G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) ),h(1, 12, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_204 = _t58_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_205 = _t58_62;

  // 4-BLAC: (4x1)^T
  _t58_206 = _t58_205;

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_207 = _t58_62;

  // 4-BLAC: 1x4 Kro 1x4
  _t58_208 = _mm256_mul_pd(_t58_206, _t58_207);

  // 4-BLAC: 1x4 - 1x4
  _t58_209 = _mm256_sub_pd(_t58_204, _t58_208);

  // AVX Storer:
  _t58_63 = _t58_209;

  // Generating : M3[12,12] = S(h(1, 12, 11), Sqrt( G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ),h(1, 12, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_210 = _t58_63;

  // 4-BLAC: sqrt(1x4)
  _t58_211 = _mm256_sqrt_pd(_t58_210);

  // AVX Storer:
  _t58_63 = _t58_211;

  _mm256_maskstore_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_21);
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_23);
  _mm256_maskstore_pd(M3 + 13, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_27);
  _mm256_maskstore_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_29);
  _mm256_maskstore_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_32);
  _mm256_maskstore_pd(M3 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_33);
  _mm256_maskstore_pd(M3 + 39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_34);
  _mm256_maskstore_pd(M3 + 52, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_31);
  _mm256_maskstore_pd(M3 + 53, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t58_33);
  _mm256_maskstore_pd(M3 + 65, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_37);
  _mm256_maskstore_pd(M3 + 66, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t58_39);
  _mm256_maskstore_pd(M3 + 78, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_42);
  _mm256_maskstore_pd(M3 + 79, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_43);
  _mm256_maskstore_pd(M3 + 91, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_44);
  _mm256_storeu_pd(M3 + 56, _t58_64);
  _mm256_storeu_pd(M3 + 68, _t58_47);
  _mm256_storeu_pd(M3 + 80, _t58_48);
  _mm256_storeu_pd(M3 + 92, _t58_49);

  for( int fi39 = 0; fi39 <= 7; fi39+=4 ) {
    _t59_9 = _mm256_maskload_pd(v0 + fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_8 = _mm256_maskload_pd(M3 + 13*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_10 = _mm256_maskload_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t59_7 = _mm256_maskload_pd(M3 + 13*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t59_6 = _mm256_broadcast_sd(&(v0[fi39]));
    _t59_11 = _mm256_maskload_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_5 = _mm256_maskload_pd(M3 + 13*fi39 + 13, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_12 = _mm256_maskload_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t59_4 = _mm256_maskload_pd(M3 + 13*fi39 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t59_3 = _mm256_broadcast_sd(&(v0[fi39 + 1]));
    _t59_13 = _mm256_maskload_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_2 = _mm256_maskload_pd(M3 + 13*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_14 = _mm256_maskload_pd(v0 + fi39 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_1 = _mm256_maskload_pd(M3 + 13*fi39 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_0 = _mm256_maskload_pd(M3 + 13*fi39 + 39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

    // Generating : v0[12,1] = S(h(1, 12, fi39), ( G(h(1, 12, fi39), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_15 = _t59_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_16 = _t59_8;

    // 4-BLAC: 1x4 / 1x4
    _t59_17 = _mm256_div_pd(_t59_15, _t59_16);

    // AVX Storer:
    _t59_9 = _t59_17;

    // Generating : v0[12,1] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), v0[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) Kro G(h(1, 12, fi39), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t59_18 = _t59_10;

    // AVX Loader:

    // 1x3 -> 1x4
    _t59_19 = _t59_7;

    // 4-BLAC: (1x4)^T
    _t59_20 = _t59_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_21 = _t59_6;

    // 4-BLAC: 4x1 Kro 1x4
    _t59_22 = _mm256_mul_pd(_t59_20, _t59_21);

    // 4-BLAC: 4x1 - 4x1
    _t59_23 = _mm256_sub_pd(_t59_18, _t59_22);

    // AVX Storer:
    _t59_10 = _t59_23;

    // Generating : v0[12,1] = S(h(1, 12, fi39 + 1), ( G(h(1, 12, fi39 + 1), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_24 = _t59_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_25 = _t59_5;

    // 4-BLAC: 1x4 / 1x4
    _t59_26 = _mm256_div_pd(_t59_24, _t59_25);

    // AVX Storer:
    _t59_11 = _t59_26;

    // Generating : v0[12,1] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), v0[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) Kro G(h(1, 12, fi39 + 1), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t59_27 = _t59_12;

    // AVX Loader:

    // 1x2 -> 1x4
    _t59_28 = _t59_4;

    // 4-BLAC: (1x4)^T
    _t59_29 = _t59_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_30 = _t59_3;

    // 4-BLAC: 4x1 Kro 1x4
    _t59_31 = _mm256_mul_pd(_t59_29, _t59_30);

    // 4-BLAC: 4x1 - 4x1
    _t59_32 = _mm256_sub_pd(_t59_27, _t59_31);

    // AVX Storer:
    _t59_12 = _t59_32;

    // Generating : v0[12,1] = S(h(1, 12, fi39 + 2), ( G(h(1, 12, fi39 + 2), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_33 = _t59_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_34 = _t59_2;

    // 4-BLAC: 1x4 / 1x4
    _t59_35 = _mm256_div_pd(_t59_33, _t59_34);

    // AVX Storer:
    _t59_13 = _t59_35;

    // Generating : v0[12,1] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), v0[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_36 = _t59_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_37 = _t59_1;

    // 4-BLAC: (4x1)^T
    _t59_38 = _t59_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_39 = _t59_13;

    // 4-BLAC: 1x4 Kro 1x4
    _t59_40 = _mm256_mul_pd(_t59_38, _t59_39);

    // 4-BLAC: 1x4 - 1x4
    _t59_41 = _mm256_sub_pd(_t59_36, _t59_40);

    // AVX Storer:
    _t59_14 = _t59_41;

    // Generating : v0[12,1] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_42 = _t59_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_43 = _t59_0;

    // 4-BLAC: 1x4 / 1x4
    _t59_44 = _mm256_div_pd(_t59_42, _t59_43);

    // AVX Storer:
    _t59_14 = _t59_44;

    // Generating : v0[12,1] = Sum_{i0} ( S(h(4, 12, fi39 + i0 + 4), ( G(h(4, 12, fi39 + i0 + 4), v0[12,1],h(1, 1, 0)) - ( T( G(h(4, 12, fi39), M3[12,12],h(4, 12, fi39 + i0 + 4)) ) * G(h(4, 12, fi39), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm256_maskstore_pd(v0 + fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_9);
    _mm256_maskstore_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t59_10);
    _mm256_maskstore_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t59_12);

    for( int i0 = 0; i0 <= -fi39 + 7; i0+=4 ) {
      _t60_6 = _mm256_loadu_pd(v0 + fi39 + i0 + 4);
      _t60_4 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 4);
      _t60_3 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 16);
      _t60_2 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 28);
      _t60_1 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 40);
      _t60_0 = _mm256_maskload_pd(v0 + fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t60_7 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_4, _t60_3), _mm256_unpacklo_pd(_t60_2, _t60_1), 32);
      _t60_8 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_4, _t60_3), _mm256_unpackhi_pd(_t60_2, _t60_1), 32);
      _t60_9 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_4, _t60_3), _mm256_unpacklo_pd(_t60_2, _t60_1), 49);
      _t60_10 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_4, _t60_3), _mm256_unpackhi_pd(_t60_2, _t60_1), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t60_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t60_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32)), _mm256_mul_pd(_t60_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t60_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32)), _mm256_mul_pd(_t60_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t60_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32)), _mm256_mul_pd(_t60_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t60_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32)), _mm256_mul_pd(_t60_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_0, _t59_11), _mm256_unpacklo_pd(_t59_13, _t59_14), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t60_6 = _mm256_sub_pd(_t60_6, _t60_5);

      // AVX Storer:
      _mm256_storeu_pd(v0 + fi39 + i0 + 4, _t60_6);
    }
    _mm256_maskstore_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_11);
    _mm256_maskstore_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_13);
    _mm256_maskstore_pd(v0 + fi39 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_14);
  }

  _t61_2 = _mm256_maskload_pd(v0 + 8, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t61_3 = _mm256_maskload_pd(v0 + 9, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t61_1 = _mm256_broadcast_sd(&(v0[8]));
  _t61_4 = _mm256_maskload_pd(v0 + 9, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t61_5 = _mm256_maskload_pd(v0 + 10, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t61_0 = _mm256_broadcast_sd(&(v0[9]));
  _t61_6 = _mm256_maskload_pd(v0 + 10, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t61_7 = _mm256_maskload_pd(v0 + 11, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

  // Generating : v0[12,1] = S(h(1, 12, 8), ( G(h(1, 12, 8), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 8), M3[12,12],h(1, 12, 8)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_8 = _t61_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_9 = _t58_50;

  // 4-BLAC: 1x4 / 1x4
  _t61_10 = _mm256_div_pd(_t61_8, _t61_9);

  // AVX Storer:
  _t61_2 = _t61_10;

  // Generating : v0[12,1] = S(h(3, 12, 9), ( G(h(3, 12, 9), v0[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) Kro G(h(1, 12, 8), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t61_11 = _t61_3;

  // AVX Loader:

  // 1x3 -> 1x4
  _t61_12 = _t58_52;

  // 4-BLAC: (1x4)^T
  _t61_13 = _t61_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_14 = _t61_1;

  // 4-BLAC: 4x1 Kro 1x4
  _t61_15 = _mm256_mul_pd(_t61_13, _t61_14);

  // 4-BLAC: 4x1 - 4x1
  _t61_16 = _mm256_sub_pd(_t61_11, _t61_15);

  // AVX Storer:
  _t61_3 = _t61_16;

  // Generating : v0[12,1] = S(h(1, 12, 9), ( G(h(1, 12, 9), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 9), M3[12,12],h(1, 12, 9)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_17 = _t61_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_18 = _t58_56;

  // 4-BLAC: 1x4 / 1x4
  _t61_19 = _mm256_div_pd(_t61_17, _t61_18);

  // AVX Storer:
  _t61_4 = _t61_19;

  // Generating : v0[12,1] = S(h(2, 12, 10), ( G(h(2, 12, 10), v0[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) Kro G(h(1, 12, 9), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t61_20 = _t61_5;

  // AVX Loader:

  // 1x2 -> 1x4
  _t61_21 = _t58_58;

  // 4-BLAC: (1x4)^T
  _t61_22 = _t61_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_23 = _t61_0;

  // 4-BLAC: 4x1 Kro 1x4
  _t61_24 = _mm256_mul_pd(_t61_22, _t61_23);

  // 4-BLAC: 4x1 - 4x1
  _t61_25 = _mm256_sub_pd(_t61_20, _t61_24);

  // AVX Storer:
  _t61_5 = _t61_25;

  // Generating : v0[12,1] = S(h(1, 12, 10), ( G(h(1, 12, 10), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_26 = _t61_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_27 = _t58_61;

  // 4-BLAC: 1x4 / 1x4
  _t61_28 = _mm256_div_pd(_t61_26, _t61_27);

  // AVX Storer:
  _t61_6 = _t61_28;

  // Generating : v0[12,1] = S(h(1, 12, 11), ( G(h(1, 12, 11), v0[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_29 = _t61_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_30 = _t58_62;

  // 4-BLAC: (4x1)^T
  _t61_31 = _t61_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_32 = _t61_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t61_33 = _mm256_mul_pd(_t61_31, _t61_32);

  // 4-BLAC: 1x4 - 1x4
  _t61_34 = _mm256_sub_pd(_t61_29, _t61_33);

  // AVX Storer:
  _t61_7 = _t61_34;

  // Generating : v0[12,1] = S(h(1, 12, 11), ( G(h(1, 12, 11), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_35 = _t61_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t61_36 = _t58_63;

  // 4-BLAC: 1x4 / 1x4
  _t61_37 = _mm256_div_pd(_t61_35, _t61_36);

  // AVX Storer:
  _t61_7 = _t61_37;

  _mm256_maskstore_pd(M3 + 104, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_50);
  _mm256_maskstore_pd(M3 + 105, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t58_52);
  _mm256_maskstore_pd(M3 + 117, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_56);
  _mm256_maskstore_pd(M3 + 118, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t58_58);
  _mm256_maskstore_pd(M3 + 130, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_61);
  _mm256_maskstore_pd(M3 + 131, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_62);
  _mm256_maskstore_pd(M3 + 143, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t58_63);
  _mm256_maskstore_pd(v0 + 8, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t61_2);
  _mm256_maskstore_pd(v0 + 9, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t61_4);
  _mm256_maskstore_pd(v0 + 10, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t61_6);
  _mm256_maskstore_pd(v0 + 11, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t61_7);

  for( int fi39 = 0; fi39 <= 7; fi39+=4 ) {
    _t62_9 = _mm256_maskload_pd(v0 + -fi39 + 11, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_8 = _mm256_maskload_pd(M3 + -13*fi39 + 143, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_10 = _mm256_maskload_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t62_7 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(M3 + -13*fi39 + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -13*fi39 + 119, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(M3 + -13*fi39 + 131, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t62_6 = _mm256_broadcast_sd(&(v0[-fi39 + 11]));
    _t62_11 = _mm256_maskload_pd(v0 + -fi39 + 10, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_5 = _mm256_maskload_pd(M3 + -13*fi39 + 130, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_12 = _mm256_maskload_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t62_4 = _mm256_shuffle_pd(_mm256_maskload_pd(M3 + -13*fi39 + 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -13*fi39 + 118, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);
    _t62_3 = _mm256_broadcast_sd(&(v0[-fi39 + 10]));
    _t62_13 = _mm256_maskload_pd(v0 + -fi39 + 9, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_2 = _mm256_maskload_pd(M3 + -13*fi39 + 117, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_14 = _mm256_maskload_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_1 = _mm256_maskload_pd(M3 + -13*fi39 + 105, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t62_0 = _mm256_maskload_pd(M3 + -13*fi39 + 104, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

    // Generating : v0[12,1] = S(h(1, 12, -fi39 + 11), ( G(h(1, 12, -fi39 + 11), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 11), M3[12,12],h(1, 12, -fi39 + 11)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_15 = _t62_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_16 = _t62_8;

    // 4-BLAC: 1x4 / 1x4
    _t62_17 = _mm256_div_pd(_t62_15, _t62_16);

    // AVX Storer:
    _t62_9 = _t62_17;

    // Generating : v0[12,1] = S(h(3, 12, -fi39 + 8), ( G(h(3, 12, -fi39 + 8), v0[12,1],h(1, 1, 0)) - ( G(h(3, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 11)) Kro G(h(1, 12, -fi39 + 11), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t62_18 = _t62_10;

    // AVX Loader:

    // 3x1 -> 4x1
    _t62_19 = _t62_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_20 = _t62_6;

    // 4-BLAC: 4x1 Kro 1x4
    _t62_21 = _mm256_mul_pd(_t62_19, _t62_20);

    // 4-BLAC: 4x1 - 4x1
    _t62_22 = _mm256_sub_pd(_t62_18, _t62_21);

    // AVX Storer:
    _t62_10 = _t62_22;

    // Generating : v0[12,1] = S(h(1, 12, -fi39 + 10), ( G(h(1, 12, -fi39 + 10), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 10), M3[12,12],h(1, 12, -fi39 + 10)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_23 = _t62_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_24 = _t62_5;

    // 4-BLAC: 1x4 / 1x4
    _t62_25 = _mm256_div_pd(_t62_23, _t62_24);

    // AVX Storer:
    _t62_11 = _t62_25;

    // Generating : v0[12,1] = S(h(2, 12, -fi39 + 8), ( G(h(2, 12, -fi39 + 8), v0[12,1],h(1, 1, 0)) - ( G(h(2, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 10)) Kro G(h(1, 12, -fi39 + 10), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t62_26 = _t62_12;

    // AVX Loader:

    // 2x1 -> 4x1
    _t62_27 = _t62_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_28 = _t62_3;

    // 4-BLAC: 4x1 Kro 1x4
    _t62_29 = _mm256_mul_pd(_t62_27, _t62_28);

    // 4-BLAC: 4x1 - 4x1
    _t62_30 = _mm256_sub_pd(_t62_26, _t62_29);

    // AVX Storer:
    _t62_12 = _t62_30;

    // Generating : v0[12,1] = S(h(1, 12, -fi39 + 9), ( G(h(1, 12, -fi39 + 9), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 9), M3[12,12],h(1, 12, -fi39 + 9)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_31 = _t62_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_32 = _t62_2;

    // 4-BLAC: 1x4 / 1x4
    _t62_33 = _mm256_div_pd(_t62_31, _t62_32);

    // AVX Storer:
    _t62_13 = _t62_33;

    // Generating : v0[12,1] = S(h(1, 12, -fi39 + 8), ( G(h(1, 12, -fi39 + 8), v0[12,1],h(1, 1, 0)) - ( G(h(1, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 9)) Kro G(h(1, 12, -fi39 + 9), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_34 = _t62_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_35 = _t62_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_36 = _t62_13;

    // 4-BLAC: 1x4 Kro 1x4
    _t62_37 = _mm256_mul_pd(_t62_35, _t62_36);

    // 4-BLAC: 1x4 - 1x4
    _t62_38 = _mm256_sub_pd(_t62_34, _t62_37);

    // AVX Storer:
    _t62_14 = _t62_38;

    // Generating : v0[12,1] = S(h(1, 12, -fi39 + 8), ( G(h(1, 12, -fi39 + 8), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 8)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_39 = _t62_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_40 = _t62_0;

    // 4-BLAC: 1x4 / 1x4
    _t62_41 = _mm256_div_pd(_t62_39, _t62_40);

    // AVX Storer:
    _t62_14 = _t62_41;

    // Generating : v0[12,1] = Sum_{i0} ( S(h(4, 12, i0), ( G(h(4, 12, i0), v0[12,1],h(1, 1, 0)) - ( G(h(4, 12, i0), M3[12,12],h(4, 12, -fi39 + 8)) * G(h(4, 12, -fi39 + 8), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm256_maskstore_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t62_10);
    _mm256_maskstore_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t62_12);
    _mm256_maskstore_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_14);

    for( int i0 = 0; i0 <= -fi39 + 7; i0+=4 ) {
      _t63_6 = _mm256_loadu_pd(v0 + i0);
      _t63_4 = _mm256_loadu_pd(M3 + -fi39 + 12*i0 + 8);
      _t63_3 = _mm256_loadu_pd(M3 + -fi39 + 12*i0 + 20);
      _t63_2 = _mm256_loadu_pd(M3 + -fi39 + 12*i0 + 32);
      _t63_1 = _mm256_loadu_pd(M3 + -fi39 + 12*i0 + 44);
      _t63_0 = _mm256_maskload_pd(v0 + -fi39 + 8, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t63_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t63_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32)), _mm256_mul_pd(_t63_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t63_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32)), _mm256_mul_pd(_t63_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t63_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32)), _mm256_mul_pd(_t63_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t63_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32)), _mm256_mul_pd(_t63_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_0, _t62_13), _mm256_unpacklo_pd(_t62_11, _t62_9), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t63_6 = _mm256_sub_pd(_t63_6, _t63_5);

      // AVX Storer:
      _mm256_storeu_pd(v0 + i0, _t63_6);
    }
    _mm256_maskstore_pd(v0 + -fi39 + 11, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_9);
    _mm256_maskstore_pd(v0 + -fi39 + 10, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_11);
    _mm256_maskstore_pd(v0 + -fi39 + 9, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_13);
  }

  _t56_32 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_33 = _mm256_maskload_pd(M3 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_27 = _mm256_maskload_pd(M3 + 13, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_23 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_21 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_34 = _mm256_maskload_pd(M3 + 39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_29 = _mm256_maskload_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t64_3 = _mm256_maskload_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t64_4 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t64_2 = _mm256_broadcast_sd(&(v0[3]));
  _t64_5 = _mm256_maskload_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t64_6 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t64_1 = _mm256_broadcast_sd(&(v0[2]));
  _t64_7 = _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t64_8 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t64_0 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

  // Generating : v0[12,1] = S(h(1, 12, 3), ( G(h(1, 12, 3), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 3), M3[12,12],h(1, 12, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_9 = _t64_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_10 = _t56_34;

  // 4-BLAC: 1x4 / 1x4
  _t64_11 = _mm256_div_pd(_t64_9, _t64_10);

  // AVX Storer:
  _t64_3 = _t64_11;

  // Generating : v0[12,1] = S(h(3, 12, 0), ( G(h(3, 12, 0), v0[12,1],h(1, 1, 0)) - ( G(h(3, 12, 0), M3[12,12],h(1, 12, 3)) Kro G(h(1, 12, 3), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t64_12 = _t64_4;

  // AVX Loader:

  // 3x1 -> 4x1
  _t64_13 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_23, _t56_33, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_29, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_14 = _t64_2;

  // 4-BLAC: 4x1 Kro 1x4
  _t64_15 = _mm256_mul_pd(_t64_13, _t64_14);

  // 4-BLAC: 4x1 - 4x1
  _t64_16 = _mm256_sub_pd(_t64_12, _t64_15);

  // AVX Storer:
  _t64_4 = _t64_16;

  // Generating : v0[12,1] = S(h(1, 12, 2), ( G(h(1, 12, 2), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 2), M3[12,12],h(1, 12, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_17 = _t64_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_18 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t64_19 = _mm256_div_pd(_t64_17, _t64_18);

  // AVX Storer:
  _t64_5 = _t64_19;

  // Generating : v0[12,1] = S(h(2, 12, 0), ( G(h(2, 12, 0), v0[12,1],h(1, 1, 0)) - ( G(h(2, 12, 0), M3[12,12],h(1, 12, 2)) Kro G(h(1, 12, 2), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t64_20 = _t64_6;

  // AVX Loader:

  // 2x1 -> 4x1
  _t64_21 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_29, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_22 = _t64_1;

  // 4-BLAC: 4x1 Kro 1x4
  _t64_23 = _mm256_mul_pd(_t64_21, _t64_22);

  // 4-BLAC: 4x1 - 4x1
  _t64_24 = _mm256_sub_pd(_t64_20, _t64_23);

  // AVX Storer:
  _t64_6 = _t64_24;

  // Generating : v0[12,1] = S(h(1, 12, 1), ( G(h(1, 12, 1), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 1), M3[12,12],h(1, 12, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_25 = _t64_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_26 = _t56_27;

  // 4-BLAC: 1x4 / 1x4
  _t64_27 = _mm256_div_pd(_t64_25, _t64_26);

  // AVX Storer:
  _t64_7 = _t64_27;

  // Generating : v0[12,1] = S(h(1, 12, 0), ( G(h(1, 12, 0), v0[12,1],h(1, 1, 0)) - ( G(h(1, 12, 0), M3[12,12],h(1, 12, 1)) Kro G(h(1, 12, 1), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_28 = _t64_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_29 = _t64_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_30 = _t64_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t64_31 = _mm256_mul_pd(_t64_29, _t64_30);

  // 4-BLAC: 1x4 - 1x4
  _t64_32 = _mm256_sub_pd(_t64_28, _t64_31);

  // AVX Storer:
  _t64_8 = _t64_32;

  // Generating : v0[12,1] = S(h(1, 12, 0), ( G(h(1, 12, 0), v0[12,1],h(1, 1, 0)) Div G(h(1, 12, 0), M3[12,12],h(1, 12, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_33 = _t64_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_34 = _t56_21;

  // 4-BLAC: 1x4 / 1x4
  _t64_35 = _mm256_div_pd(_t64_33, _t64_34);

  // AVX Storer:
  _t64_8 = _t64_35;


  for( int fi39 = 0; fi39 <= 7; fi39+=4 ) {
    _t65_18 = _mm256_maskload_pd(M3 + 13*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t65_17 = _mm256_maskload_pd(M3 + 13*fi39 + 13, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t65_16 = _mm256_maskload_pd(M3 + 13*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t65_15 = _mm256_maskload_pd(M3 + 13*fi39 + 39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t65_26 = _mm256_loadu_pd(M1 + 28*fi39);
    _t65_23 = _mm256_loadu_pd(M1 + 28*fi39 + 28);
    _t65_24 = _mm256_loadu_pd(M1 + 28*fi39 + 56);
    _t65_25 = _mm256_loadu_pd(M1 + 28*fi39 + 84);
    _t65_13 = _mm256_maskload_pd(M3 + 13*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t65_11 = _mm256_maskload_pd(M3 + 13*fi39 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t65_9 = _mm256_broadcast_sd(&(M3[13*fi39 + 27]));

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39), M3[12,12],h(1, 12, fi39)) ),h(1, 12, fi39))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_28 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_29 = _t65_18;

    // 4-BLAC: 1x4 / 1x4
    _t65_30 = _mm256_div_pd(_t65_28, _t65_29);

    // AVX Storer:
    _t65_19 = _t65_30;

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 1), M3[12,12],h(1, 12, fi39 + 1)) ),h(1, 12, fi39 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_31 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_32 = _t65_17;

    // 4-BLAC: 1x4 / 1x4
    _t65_33 = _mm256_div_pd(_t65_31, _t65_32);

    // AVX Storer:
    _t65_20 = _t65_33;

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 2)) ),h(1, 12, fi39 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_34 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_35 = _t65_16;

    // 4-BLAC: 1x4 / 1x4
    _t65_36 = _mm256_div_pd(_t65_34, _t65_35);

    // AVX Storer:
    _t65_21 = _t65_36;

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi39 + 3), M3[12,12],h(1, 12, fi39 + 3)) ),h(1, 12, fi39 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_37 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_38 = _t65_15;

    // 4-BLAC: 1x4 / 1x4
    _t65_39 = _mm256_div_pd(_t65_37, _t65_38);

    // AVX Storer:
    _t65_22 = _t65_39;

    // Generating : M1[12,28] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_40 = _t65_14;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_26 = _mm256_mul_pd(_t65_40, _t65_26);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M1[12,28],h(4, 28, fi100)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 3x4 -> 4x4
    _t65_41 = _t65_23;
    _t65_42 = _t65_24;
    _t65_43 = _t65_25;
    _t65_44 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t65_45 = _t65_13;

    // 4-BLAC: (1x4)^T
    _t65_46 = _t65_45;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_47 = _mm256_mul_pd(_t65_7, _t65_26);
    _t65_48 = _mm256_mul_pd(_t65_6, _t65_26);
    _t65_49 = _mm256_mul_pd(_t65_5, _t65_26);
    _t65_50 = _mm256_mul_pd(_t65_4, _t65_26);

    // 4-BLAC: 4x4 - 4x4
    _t65_51 = _mm256_sub_pd(_t65_41, _t65_47);
    _t65_52 = _mm256_sub_pd(_t65_42, _t65_48);
    _t65_53 = _mm256_sub_pd(_t65_43, _t65_49);
    _t65_54 = _mm256_sub_pd(_t65_44, _t65_50);

    // AVX Storer:
    _t65_23 = _t65_51;
    _t65_24 = _t65_52;
    _t65_25 = _t65_53;

    // Generating : M1[12,28] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_55 = _t65_12;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_23 = _mm256_mul_pd(_t65_55, _t65_23);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M1[12,28],h(4, 28, fi100)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 2x4 -> 4x4
    _t65_56 = _t65_24;
    _t65_57 = _t65_25;
    _t65_58 = _mm256_setzero_pd();
    _t65_59 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t65_60 = _t65_11;

    // 4-BLAC: (1x4)^T
    _t65_61 = _t65_60;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_62 = _mm256_mul_pd(_t65_3, _t65_23);
    _t65_63 = _mm256_mul_pd(_t65_2, _t65_23);
    _t65_64 = _mm256_mul_pd(_t65_1, _t65_23);
    _t65_65 = _mm256_mul_pd(_t65_0, _t65_23);

    // 4-BLAC: 4x4 - 4x4
    _t65_66 = _mm256_sub_pd(_t65_56, _t65_62);
    _t65_67 = _mm256_sub_pd(_t65_57, _t65_63);
    _t65_68 = _mm256_sub_pd(_t65_58, _t65_64);
    _t65_69 = _mm256_sub_pd(_t65_59, _t65_65);

    // AVX Storer:
    _t65_24 = _t65_66;
    _t65_25 = _t65_67;

    // Generating : M1[12,28] = S(h(1, 12, fi39 + 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) Kro G(h(1, 12, fi39 + 2), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_70 = _t65_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_24 = _mm256_mul_pd(_t65_70, _t65_24);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M1[12,28],h(4, 28, fi100)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_71 = _t65_9;

    // 4-BLAC: (4x1)^T
    _t65_72 = _t65_71;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_27 = _mm256_mul_pd(_t65_72, _t65_24);

    // 4-BLAC: 1x4 - 1x4
    _t65_25 = _mm256_sub_pd(_t65_25, _t65_27);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, fi39 + 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) Kro G(h(1, 12, fi39 + 3), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_73 = _t65_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_25 = _mm256_mul_pd(_t65_73, _t65_25);

    // AVX Storer:

    for( int fi100 = 4; fi100 <= 24; fi100+=4 ) {
      _t66_3 = _mm256_loadu_pd(M1 + fi100 + 28*fi39);
      _t66_0 = _mm256_loadu_pd(M1 + fi100 + 28*fi39 + 28);
      _t66_1 = _mm256_loadu_pd(M1 + fi100 + 28*fi39 + 56);
      _t66_2 = _mm256_loadu_pd(M1 + fi100 + 28*fi39 + 84);

      // Generating : M1[12,28] = S(h(1, 12, fi39), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39)) Kro G(h(1, 12, fi39), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_4 = _t65_14;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_3 = _mm256_mul_pd(_t66_4, _t66_3);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(3, 12, fi39 + 1), ( G(h(3, 12, fi39 + 1), M1[12,28],h(4, 28, fi100)) - ( T( G(h(1, 12, fi39), M3[12,12],h(3, 12, fi39 + 1)) ) * G(h(1, 12, fi39), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 3x4 -> 4x4
      _t66_5 = _t66_0;
      _t66_6 = _t66_1;
      _t66_7 = _t66_2;
      _t66_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t66_9 = _t65_13;

      // 4-BLAC: (1x4)^T
      _t66_10 = _t66_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t65_47 = _mm256_mul_pd(_t65_7, _t66_3);
      _t65_48 = _mm256_mul_pd(_t65_6, _t66_3);
      _t65_49 = _mm256_mul_pd(_t65_5, _t66_3);
      _t65_50 = _mm256_mul_pd(_t65_4, _t66_3);

      // 4-BLAC: 4x4 - 4x4
      _t66_11 = _mm256_sub_pd(_t66_5, _t65_47);
      _t66_12 = _mm256_sub_pd(_t66_6, _t65_48);
      _t66_13 = _mm256_sub_pd(_t66_7, _t65_49);
      _t66_14 = _mm256_sub_pd(_t66_8, _t65_50);

      // AVX Storer:
      _t66_0 = _t66_11;
      _t66_1 = _t66_12;
      _t66_2 = _t66_13;

      // Generating : M1[12,28] = S(h(1, 12, fi39 + 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 1)) Kro G(h(1, 12, fi39 + 1), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_15 = _t65_12;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_0 = _mm256_mul_pd(_t66_15, _t66_0);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(2, 12, fi39 + 2), ( G(h(2, 12, fi39 + 2), M1[12,28],h(4, 28, fi100)) - ( T( G(h(1, 12, fi39 + 1), M3[12,12],h(2, 12, fi39 + 2)) ) * G(h(1, 12, fi39 + 1), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 2x4 -> 4x4
      _t66_16 = _t66_1;
      _t66_17 = _t66_2;
      _t66_18 = _mm256_setzero_pd();
      _t66_19 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t66_20 = _t65_11;

      // 4-BLAC: (1x4)^T
      _t66_21 = _t66_20;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t65_62 = _mm256_mul_pd(_t65_3, _t66_0);
      _t65_63 = _mm256_mul_pd(_t65_2, _t66_0);
      _t65_64 = _mm256_mul_pd(_t65_1, _t66_0);
      _t65_65 = _mm256_mul_pd(_t65_0, _t66_0);

      // 4-BLAC: 4x4 - 4x4
      _t66_22 = _mm256_sub_pd(_t66_16, _t65_62);
      _t66_23 = _mm256_sub_pd(_t66_17, _t65_63);
      _t66_24 = _mm256_sub_pd(_t66_18, _t65_64);
      _t66_25 = _mm256_sub_pd(_t66_19, _t65_65);

      // AVX Storer:
      _t66_1 = _t66_22;
      _t66_2 = _t66_23;

      // Generating : M1[12,28] = S(h(1, 12, fi39 + 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 2)) Kro G(h(1, 12, fi39 + 2), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_26 = _t65_10;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_1 = _mm256_mul_pd(_t66_26, _t66_1);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(1, 12, fi39 + 3), ( G(h(1, 12, fi39 + 3), M1[12,28],h(4, 28, fi100)) - ( T( G(h(1, 12, fi39 + 2), M3[12,12],h(1, 12, fi39 + 3)) ) Kro G(h(1, 12, fi39 + 2), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_27 = _t65_9;

      // 4-BLAC: (4x1)^T
      _t65_72 = _t66_27;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t65_27 = _mm256_mul_pd(_t65_72, _t66_1);

      // 4-BLAC: 1x4 - 1x4
      _t66_2 = _mm256_sub_pd(_t66_2, _t65_27);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(1, 12, fi39 + 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, fi39 + 3)) Kro G(h(1, 12, fi39 + 3), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_28 = _t65_8;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_2 = _mm256_mul_pd(_t66_28, _t66_2);

      // AVX Storer:
      _mm256_storeu_pd(M1 + fi100 + 28*fi39, _t66_3);
      _mm256_storeu_pd(M1 + fi100 + 28*fi39 + 28, _t66_0);
      _mm256_storeu_pd(M1 + fi100 + 28*fi39 + 56, _t66_1);
      _mm256_storeu_pd(M1 + fi100 + 28*fi39 + 84, _t66_2);
    }

    // Generating : M1[12,28] = Sum_{i0} ( Sum_{k3} ( S(h(4, 12, fi39 + i0 + 4), ( G(h(4, 12, fi39 + i0 + 4), M1[12,28],h(4, 28, k3)) - ( T( G(h(4, 12, fi39), M3[12,12],h(4, 12, fi39 + i0 + 4)) ) * G(h(4, 12, fi39), M1[12,28],h(4, 28, k3)) ) ),h(4, 28, k3)) ) )
    _mm256_storeu_pd(M1 + 28*fi39, _t65_26);
    _mm256_storeu_pd(M1 + 28*fi39 + 28, _t65_23);
    _mm256_storeu_pd(M1 + 28*fi39 + 56, _t65_24);
    _mm256_storeu_pd(M1 + 28*fi39 + 84, _t65_25);

    for( int i0 = 0; i0 <= -fi39 + 7; i0+=4 ) {

      for( int k3 = 0; k3 <= 27; k3+=4 ) {
        _t67_28 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 112);
        _t67_29 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 140);
        _t67_30 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 168);
        _t67_31 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 196);
        _t67_23 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 4);
        _t67_22 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 16);
        _t67_21 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 28);
        _t67_20 = _mm256_loadu_pd(M3 + 13*fi39 + i0 + 40);
        _t67_19 = _mm256_loadu_pd(M1 + 28*fi39 + k3);
        _t67_18 = _mm256_loadu_pd(M1 + 28*fi39 + k3 + 28);
        _t67_17 = _mm256_loadu_pd(M1 + 28*fi39 + k3 + 56);
        _t67_16 = _mm256_loadu_pd(M1 + 28*fi39 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t67_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_23, _t67_22), _mm256_unpacklo_pd(_t67_21, _t67_20), 32);
        _t67_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t67_23, _t67_22), _mm256_unpackhi_pd(_t67_21, _t67_20), 32);
        _t67_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_23, _t67_22), _mm256_unpacklo_pd(_t67_21, _t67_20), 49);
        _t67_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t67_23, _t67_22), _mm256_unpackhi_pd(_t67_21, _t67_20), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t67_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_15, _t67_19), _mm256_mul_pd(_t67_14, _t67_18)), _mm256_add_pd(_mm256_mul_pd(_t67_13, _t67_17), _mm256_mul_pd(_t67_12, _t67_16)));
        _t67_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_11, _t67_19), _mm256_mul_pd(_t67_10, _t67_18)), _mm256_add_pd(_mm256_mul_pd(_t67_9, _t67_17), _mm256_mul_pd(_t67_8, _t67_16)));
        _t67_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_7, _t67_19), _mm256_mul_pd(_t67_6, _t67_18)), _mm256_add_pd(_mm256_mul_pd(_t67_5, _t67_17), _mm256_mul_pd(_t67_4, _t67_16)));
        _t67_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_3, _t67_19), _mm256_mul_pd(_t67_2, _t67_18)), _mm256_add_pd(_mm256_mul_pd(_t67_1, _t67_17), _mm256_mul_pd(_t67_0, _t67_16)));

        // 4-BLAC: 4x4 - 4x4
        _t67_28 = _mm256_sub_pd(_t67_28, _t67_24);
        _t67_29 = _mm256_sub_pd(_t67_29, _t67_25);
        _t67_30 = _mm256_sub_pd(_t67_30, _t67_26);
        _t67_31 = _mm256_sub_pd(_t67_31, _t67_27);

        // AVX Storer:
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 112, _t67_28);
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 140, _t67_29);
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 168, _t67_30);
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 196, _t67_31);
      }
    }
  }

  _t58_61 = _mm256_maskload_pd(M3 + 130, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t58_58 = _mm256_maskload_pd(M3 + 118, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t58_52 = _mm256_maskload_pd(M3 + 105, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t58_63 = _mm256_maskload_pd(M3 + 143, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t68_16 = _mm256_loadu_pd(M1 + 224);
  _t68_13 = _mm256_loadu_pd(M1 + 252);
  _t68_14 = _mm256_loadu_pd(M1 + 280);
  _t68_15 = _mm256_loadu_pd(M1 + 308);
  _t68_9 = _mm256_broadcast_sd(&(M3[131]));

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 10), M3[12,12],h(1, 12, 10)) ),h(1, 12, 10))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_19 = _t58_61;

  // 4-BLAC: 1x4 / 1x4
  _t68_20 = _mm256_div_pd(_t68_18, _t68_19);

  // AVX Storer:
  _t68_11 = _t68_20;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 11), M3[12,12],h(1, 12, 11)) ),h(1, 12, 11))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_21 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_22 = _t58_63;

  // 4-BLAC: 1x4 / 1x4
  _t68_23 = _mm256_div_pd(_t68_21, _t68_22);

  // AVX Storer:
  _t68_12 = _t68_23;

  // Generating : M1[12,28] = S(h(1, 12, 8), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 8)) Kro G(h(1, 12, 8), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_24 = _t58_25;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_16 = _mm256_mul_pd(_t68_24, _t68_16);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(3, 12, 9), ( G(h(3, 12, 9), M1[12,28],h(4, 28, fi39)) - ( T( G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) * G(h(1, 12, 8), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 3x4 -> 4x4
  _t68_25 = _t68_13;
  _t68_26 = _t68_14;
  _t68_27 = _t68_15;
  _t68_28 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t68_29 = _t58_52;

  // 4-BLAC: (1x4)^T
  _t68_30 = _t68_29;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t68_31 = _mm256_mul_pd(_t68_7, _t68_16);
  _t68_32 = _mm256_mul_pd(_t68_6, _t68_16);
  _t68_33 = _mm256_mul_pd(_t68_5, _t68_16);
  _t68_34 = _mm256_mul_pd(_t68_4, _t68_16);

  // 4-BLAC: 4x4 - 4x4
  _t68_35 = _mm256_sub_pd(_t68_25, _t68_31);
  _t68_36 = _mm256_sub_pd(_t68_26, _t68_32);
  _t68_37 = _mm256_sub_pd(_t68_27, _t68_33);
  _t68_38 = _mm256_sub_pd(_t68_28, _t68_34);

  // AVX Storer:
  _t68_13 = _t68_35;
  _t68_14 = _t68_36;
  _t68_15 = _t68_37;

  // Generating : M1[12,28] = S(h(1, 12, 9), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 9)) Kro G(h(1, 12, 9), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_39 = _t58_24;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_13 = _mm256_mul_pd(_t68_39, _t68_13);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(2, 12, 10), ( G(h(2, 12, 10), M1[12,28],h(4, 28, fi39)) - ( T( G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) * G(h(1, 12, 9), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 2x4 -> 4x4
  _t68_40 = _t68_14;
  _t68_41 = _t68_15;
  _t68_42 = _mm256_setzero_pd();
  _t68_43 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t68_44 = _t58_58;

  // 4-BLAC: (1x4)^T
  _t68_45 = _t68_44;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t68_46 = _mm256_mul_pd(_t68_3, _t68_13);
  _t68_47 = _mm256_mul_pd(_t68_2, _t68_13);
  _t68_48 = _mm256_mul_pd(_t68_1, _t68_13);
  _t68_49 = _mm256_mul_pd(_t68_0, _t68_13);

  // 4-BLAC: 4x4 - 4x4
  _t68_50 = _mm256_sub_pd(_t68_40, _t68_46);
  _t68_51 = _mm256_sub_pd(_t68_41, _t68_47);
  _t68_52 = _mm256_sub_pd(_t68_42, _t68_48);
  _t68_53 = _mm256_sub_pd(_t68_43, _t68_49);

  // AVX Storer:
  _t68_14 = _t68_50;
  _t68_15 = _t68_51;

  // Generating : M1[12,28] = S(h(1, 12, 10), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 10)) Kro G(h(1, 12, 10), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_54 = _t68_10;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_14 = _mm256_mul_pd(_t68_54, _t68_14);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(1, 12, 11), ( G(h(1, 12, 11), M1[12,28],h(4, 28, fi39)) - ( T( G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_55 = _t68_9;

  // 4-BLAC: (4x1)^T
  _t68_56 = _t68_55;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_17 = _mm256_mul_pd(_t68_56, _t68_14);

  // 4-BLAC: 1x4 - 1x4
  _t68_15 = _mm256_sub_pd(_t68_15, _t68_17);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(1, 12, 11), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 11)) Kro G(h(1, 12, 11), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_57 = _t68_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_15 = _mm256_mul_pd(_t68_57, _t68_15);

  // AVX Storer:


  for( int fi39 = 4; fi39 <= 24; fi39+=4 ) {
    _t69_3 = _mm256_loadu_pd(M1 + fi39 + 224);
    _t69_0 = _mm256_loadu_pd(M1 + fi39 + 252);
    _t69_1 = _mm256_loadu_pd(M1 + fi39 + 280);
    _t69_2 = _mm256_loadu_pd(M1 + fi39 + 308);

    // Generating : M1[12,28] = S(h(1, 12, 8), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 8)) Kro G(h(1, 12, 8), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_4 = _t58_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_3 = _mm256_mul_pd(_t69_4, _t69_3);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(3, 12, 9), ( G(h(3, 12, 9), M1[12,28],h(4, 28, fi39)) - ( T( G(h(1, 12, 8), M3[12,12],h(3, 12, 9)) ) * G(h(1, 12, 8), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 3x4 -> 4x4
    _t69_5 = _t69_0;
    _t69_6 = _t69_1;
    _t69_7 = _t69_2;
    _t69_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t69_9 = _t58_52;

    // 4-BLAC: (1x4)^T
    _t69_10 = _t69_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t68_31 = _mm256_mul_pd(_t68_7, _t69_3);
    _t68_32 = _mm256_mul_pd(_t68_6, _t69_3);
    _t68_33 = _mm256_mul_pd(_t68_5, _t69_3);
    _t68_34 = _mm256_mul_pd(_t68_4, _t69_3);

    // 4-BLAC: 4x4 - 4x4
    _t69_11 = _mm256_sub_pd(_t69_5, _t68_31);
    _t69_12 = _mm256_sub_pd(_t69_6, _t68_32);
    _t69_13 = _mm256_sub_pd(_t69_7, _t68_33);
    _t69_14 = _mm256_sub_pd(_t69_8, _t68_34);

    // AVX Storer:
    _t69_0 = _t69_11;
    _t69_1 = _t69_12;
    _t69_2 = _t69_13;

    // Generating : M1[12,28] = S(h(1, 12, 9), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 9)) Kro G(h(1, 12, 9), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_15 = _t58_24;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_0 = _mm256_mul_pd(_t69_15, _t69_0);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(2, 12, 10), ( G(h(2, 12, 10), M1[12,28],h(4, 28, fi39)) - ( T( G(h(1, 12, 9), M3[12,12],h(2, 12, 10)) ) * G(h(1, 12, 9), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 2x4 -> 4x4
    _t69_16 = _t69_1;
    _t69_17 = _t69_2;
    _t69_18 = _mm256_setzero_pd();
    _t69_19 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t69_20 = _t58_58;

    // 4-BLAC: (1x4)^T
    _t69_21 = _t69_20;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t68_46 = _mm256_mul_pd(_t68_3, _t69_0);
    _t68_47 = _mm256_mul_pd(_t68_2, _t69_0);
    _t68_48 = _mm256_mul_pd(_t68_1, _t69_0);
    _t68_49 = _mm256_mul_pd(_t68_0, _t69_0);

    // 4-BLAC: 4x4 - 4x4
    _t69_22 = _mm256_sub_pd(_t69_16, _t68_46);
    _t69_23 = _mm256_sub_pd(_t69_17, _t68_47);
    _t69_24 = _mm256_sub_pd(_t69_18, _t68_48);
    _t69_25 = _mm256_sub_pd(_t69_19, _t68_49);

    // AVX Storer:
    _t69_1 = _t69_22;
    _t69_2 = _t69_23;

    // Generating : M1[12,28] = S(h(1, 12, 10), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 10)) Kro G(h(1, 12, 10), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_26 = _t68_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_1 = _mm256_mul_pd(_t69_26, _t69_1);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, 11), ( G(h(1, 12, 11), M1[12,28],h(4, 28, fi39)) - ( T( G(h(1, 12, 10), M3[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_27 = _t68_9;

    // 4-BLAC: (4x1)^T
    _t68_56 = _t69_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t68_17 = _mm256_mul_pd(_t68_56, _t69_1);

    // 4-BLAC: 1x4 - 1x4
    _t69_2 = _mm256_sub_pd(_t69_2, _t68_17);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, 11), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 11)) Kro G(h(1, 12, 11), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_28 = _t68_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_2 = _mm256_mul_pd(_t69_28, _t69_2);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi39 + 224, _t69_3);
    _mm256_storeu_pd(M1 + fi39 + 252, _t69_0);
    _mm256_storeu_pd(M1 + fi39 + 280, _t69_1);
    _mm256_storeu_pd(M1 + fi39 + 308, _t69_2);
  }

  _mm256_storeu_pd(M1 + 224, _t68_16);
  _mm256_storeu_pd(M1 + 252, _t68_13);
  _mm256_storeu_pd(M1 + 280, _t68_14);
  _mm256_storeu_pd(M1 + 308, _t68_15);

  for( int fi39 = 0; fi39 <= 7; fi39+=4 ) {
    _t70_18 = _mm256_maskload_pd(M3 + -13*fi39 + 143, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t70_17 = _mm256_maskload_pd(M3 + -13*fi39 + 130, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t70_16 = _mm256_maskload_pd(M3 + -13*fi39 + 117, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t70_15 = _mm256_maskload_pd(M3 + -13*fi39 + 104, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t70_26 = _mm256_loadu_pd(M1 + -28*fi39 + 308);
    _t70_23 = _mm256_loadu_pd(M1 + -28*fi39 + 224);
    _t70_24 = _mm256_loadu_pd(M1 + -28*fi39 + 252);
    _t70_25 = _mm256_loadu_pd(M1 + -28*fi39 + 280);
    _t70_13 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(M3 + -13*fi39 + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -13*fi39 + 119, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(M3 + -13*fi39 + 131, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t70_11 = _mm256_shuffle_pd(_mm256_maskload_pd(M3 + -13*fi39 + 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -13*fi39 + 118, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);
    _t70_9 = _mm256_broadcast_sd(&(M3[-13*fi39 + 105]));

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 11), M3[12,12],h(1, 12, -fi39 + 11)) ),h(1, 12, -fi39 + 11))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t70_60 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_61 = _t70_18;

    // 4-BLAC: 1x4 / 1x4
    _t70_62 = _mm256_div_pd(_t70_60, _t70_61);

    // AVX Storer:
    _t70_19 = _t70_62;

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 10), M3[12,12],h(1, 12, -fi39 + 10)) ),h(1, 12, -fi39 + 10))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t70_63 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_64 = _t70_17;

    // 4-BLAC: 1x4 / 1x4
    _t70_65 = _mm256_div_pd(_t70_63, _t70_64);

    // AVX Storer:
    _t70_20 = _t70_65;

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 9), M3[12,12],h(1, 12, -fi39 + 9)) ),h(1, 12, -fi39 + 9))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t70_66 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_67 = _t70_16;

    // 4-BLAC: 1x4 / 1x4
    _t70_68 = _mm256_div_pd(_t70_66, _t70_67);

    // AVX Storer:
    _t70_21 = _t70_68;

    // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 8)) ),h(1, 12, -fi39 + 8))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t70_69 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_70 = _t70_15;

    // 4-BLAC: 1x4 / 1x4
    _t70_28 = _mm256_div_pd(_t70_69, _t70_70);

    // AVX Storer:
    _t70_22 = _t70_28;

    // Generating : M1[12,28] = S(h(1, 12, -fi39 + 11), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 11)) Kro G(h(1, 12, -fi39 + 11), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_29 = _t70_14;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t70_26 = _mm256_mul_pd(_t70_29, _t70_26);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(3, 12, -fi39 + 8), ( G(h(3, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) - ( G(h(3, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 11)) * G(h(1, 12, -fi39 + 11), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 3x4 -> 4x4
    _t70_30 = _t70_23;
    _t70_31 = _t70_24;
    _t70_32 = _t70_25;
    _t70_33 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t70_34 = _t70_13;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t70_35 = _mm256_mul_pd(_t70_7, _t70_26);
    _t70_36 = _mm256_mul_pd(_t70_6, _t70_26);
    _t70_37 = _mm256_mul_pd(_t70_5, _t70_26);
    _t70_38 = _mm256_mul_pd(_t70_4, _t70_26);

    // 4-BLAC: 4x4 - 4x4
    _t70_39 = _mm256_sub_pd(_t70_30, _t70_35);
    _t70_40 = _mm256_sub_pd(_t70_31, _t70_36);
    _t70_41 = _mm256_sub_pd(_t70_32, _t70_37);
    _t70_42 = _mm256_sub_pd(_t70_33, _t70_38);

    // AVX Storer:
    _t70_23 = _t70_39;
    _t70_24 = _t70_40;
    _t70_25 = _t70_41;

    // Generating : M1[12,28] = S(h(1, 12, -fi39 + 10), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 10)) Kro G(h(1, 12, -fi39 + 10), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_43 = _t70_12;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t70_25 = _mm256_mul_pd(_t70_43, _t70_25);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(2, 12, -fi39 + 8), ( G(h(2, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) - ( G(h(2, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 10)) * G(h(1, 12, -fi39 + 10), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 2x4 -> 4x4
    _t70_44 = _t70_23;
    _t70_45 = _t70_24;
    _t70_46 = _mm256_setzero_pd();
    _t70_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t70_48 = _t70_11;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t70_49 = _mm256_mul_pd(_t70_3, _t70_25);
    _t70_50 = _mm256_mul_pd(_t70_2, _t70_25);
    _t70_51 = _mm256_mul_pd(_t70_1, _t70_25);
    _t70_52 = _mm256_mul_pd(_t70_0, _t70_25);

    // 4-BLAC: 4x4 - 4x4
    _t70_53 = _mm256_sub_pd(_t70_44, _t70_49);
    _t70_54 = _mm256_sub_pd(_t70_45, _t70_50);
    _t70_55 = _mm256_sub_pd(_t70_46, _t70_51);
    _t70_56 = _mm256_sub_pd(_t70_47, _t70_52);

    // AVX Storer:
    _t70_23 = _t70_53;
    _t70_24 = _t70_54;

    // Generating : M1[12,28] = S(h(1, 12, -fi39 + 9), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 9)) Kro G(h(1, 12, -fi39 + 9), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_57 = _t70_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t70_24 = _mm256_mul_pd(_t70_57, _t70_24);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, -fi39 + 8), ( G(h(1, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) - ( G(h(1, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 9)) Kro G(h(1, 12, -fi39 + 9), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_58 = _t70_9;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t70_27 = _mm256_mul_pd(_t70_58, _t70_24);

    // 4-BLAC: 1x4 - 1x4
    _t70_23 = _mm256_sub_pd(_t70_23, _t70_27);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, -fi39 + 8), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 8)) Kro G(h(1, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_59 = _t70_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t70_23 = _mm256_mul_pd(_t70_59, _t70_23);

    // AVX Storer:

    for( int fi100 = 4; fi100 <= 24; fi100+=4 ) {
      _t71_11 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 308);
      _t71_8 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 224);
      _t71_9 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 252);
      _t71_10 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 280);

      // Generating : M1[12,28] = S(h(1, 12, -fi39 + 11), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 11)) Kro G(h(1, 12, -fi39 + 11), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t71_12 = _t70_14;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t71_11 = _mm256_mul_pd(_t71_12, _t71_11);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(3, 12, -fi39 + 8), ( G(h(3, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) - ( G(h(3, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 11)) * G(h(1, 12, -fi39 + 11), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 3x4 -> 4x4
      _t71_13 = _t71_8;
      _t71_14 = _t71_9;
      _t71_15 = _t71_10;
      _t71_16 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t71_17 = _t70_13;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t70_35 = _mm256_mul_pd(_t71_7, _t71_11);
      _t70_36 = _mm256_mul_pd(_t71_6, _t71_11);
      _t70_37 = _mm256_mul_pd(_t71_5, _t71_11);
      _t70_38 = _mm256_mul_pd(_t71_4, _t71_11);

      // 4-BLAC: 4x4 - 4x4
      _t71_18 = _mm256_sub_pd(_t71_13, _t70_35);
      _t71_19 = _mm256_sub_pd(_t71_14, _t70_36);
      _t71_20 = _mm256_sub_pd(_t71_15, _t70_37);
      _t71_21 = _mm256_sub_pd(_t71_16, _t70_38);

      // AVX Storer:
      _t71_8 = _t71_18;
      _t71_9 = _t71_19;
      _t71_10 = _t71_20;

      // Generating : M1[12,28] = S(h(1, 12, -fi39 + 10), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 10)) Kro G(h(1, 12, -fi39 + 10), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t71_22 = _t70_12;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t71_10 = _mm256_mul_pd(_t71_22, _t71_10);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(2, 12, -fi39 + 8), ( G(h(2, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) - ( G(h(2, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 10)) * G(h(1, 12, -fi39 + 10), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 2x4 -> 4x4
      _t71_23 = _t71_8;
      _t71_24 = _t71_9;
      _t71_25 = _mm256_setzero_pd();
      _t71_26 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t71_27 = _t70_11;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t70_49 = _mm256_mul_pd(_t71_3, _t71_10);
      _t70_50 = _mm256_mul_pd(_t71_2, _t71_10);
      _t70_51 = _mm256_mul_pd(_t71_1, _t71_10);
      _t70_52 = _mm256_mul_pd(_t71_0, _t71_10);

      // 4-BLAC: 4x4 - 4x4
      _t71_28 = _mm256_sub_pd(_t71_23, _t70_49);
      _t71_29 = _mm256_sub_pd(_t71_24, _t70_50);
      _t71_30 = _mm256_sub_pd(_t71_25, _t70_51);
      _t71_31 = _mm256_sub_pd(_t71_26, _t70_52);

      // AVX Storer:
      _t71_8 = _t71_28;
      _t71_9 = _t71_29;

      // Generating : M1[12,28] = S(h(1, 12, -fi39 + 9), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 9)) Kro G(h(1, 12, -fi39 + 9), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t71_32 = _t70_10;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t71_9 = _mm256_mul_pd(_t71_32, _t71_9);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(1, 12, -fi39 + 8), ( G(h(1, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) - ( G(h(1, 12, -fi39 + 8), M3[12,12],h(1, 12, -fi39 + 9)) Kro G(h(1, 12, -fi39 + 9), M1[12,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t71_33 = _t70_9;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t70_27 = _mm256_mul_pd(_t71_33, _t71_9);

      // 4-BLAC: 1x4 - 1x4
      _t71_8 = _mm256_sub_pd(_t71_8, _t70_27);

      // AVX Storer:

      // Generating : M1[12,28] = S(h(1, 12, -fi39 + 8), ( G(h(1, 1, 0), T614[1,12],h(1, 12, -fi39 + 8)) Kro G(h(1, 12, -fi39 + 8), M1[12,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t71_34 = _t70_8;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t71_8 = _mm256_mul_pd(_t71_34, _t71_8);

      // AVX Storer:
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 308, _t71_11);
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 224, _t71_8);
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 252, _t71_9);
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 280, _t71_10);
    }

    // Generating : M1[12,28] = Sum_{i0} ( Sum_{k3} ( S(h(4, 12, i0), ( G(h(4, 12, i0), M1[12,28],h(4, 28, k3)) - ( G(h(4, 12, i0), M3[12,12],h(4, 12, -fi39 + 8)) * G(h(4, 12, -fi39 + 8), M1[12,28],h(4, 28, k3)) ) ),h(4, 28, k3)) ) )
    _mm256_storeu_pd(M1 + -28*fi39 + 308, _t70_26);
    _mm256_storeu_pd(M1 + -28*fi39 + 280, _t70_25);
    _mm256_storeu_pd(M1 + -28*fi39 + 252, _t70_24);
    _mm256_storeu_pd(M1 + -28*fi39 + 224, _t70_23);

    for( int i0 = 0; i0 <= -fi39 + 7; i0+=4 ) {

      for( int k3 = 0; k3 <= 27; k3+=4 ) {
        _t72_24 = _mm256_loadu_pd(M1 + 28*i0 + k3);
        _t72_25 = _mm256_loadu_pd(M1 + 28*i0 + k3 + 28);
        _t72_26 = _mm256_loadu_pd(M1 + 28*i0 + k3 + 56);
        _t72_27 = _mm256_loadu_pd(M1 + 28*i0 + k3 + 84);
        _t72_19 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 8);
        _t72_18 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 9);
        _t72_17 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 10);
        _t72_16 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 11);
        _t72_15 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 20);
        _t72_14 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 21);
        _t72_13 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 22);
        _t72_12 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 23);
        _t72_11 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 32);
        _t72_10 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 33);
        _t72_9 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 34);
        _t72_8 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 35);
        _t72_7 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 44);
        _t72_6 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 45);
        _t72_5 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 46);
        _t72_4 = _mm256_broadcast_sd(M3 + -fi39 + 12*i0 + 47);
        _t72_3 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 224);
        _t72_2 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 252);
        _t72_1 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 280);
        _t72_0 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 308);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t72_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_19, _t72_3), _mm256_mul_pd(_t72_18, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_17, _t72_1), _mm256_mul_pd(_t72_16, _t72_0)));
        _t72_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_15, _t72_3), _mm256_mul_pd(_t72_14, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_13, _t72_1), _mm256_mul_pd(_t72_12, _t72_0)));
        _t72_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_11, _t72_3), _mm256_mul_pd(_t72_10, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_9, _t72_1), _mm256_mul_pd(_t72_8, _t72_0)));
        _t72_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_7, _t72_3), _mm256_mul_pd(_t72_6, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_5, _t72_1), _mm256_mul_pd(_t72_4, _t72_0)));

        // 4-BLAC: 4x4 - 4x4
        _t72_24 = _mm256_sub_pd(_t72_24, _t72_20);
        _t72_25 = _mm256_sub_pd(_t72_25, _t72_21);
        _t72_26 = _mm256_sub_pd(_t72_26, _t72_22);
        _t72_27 = _mm256_sub_pd(_t72_27, _t72_23);

        // AVX Storer:
        _mm256_storeu_pd(M1 + 28*i0 + k3, _t72_24);
        _mm256_storeu_pd(M1 + 28*i0 + k3 + 28, _t72_25);
        _mm256_storeu_pd(M1 + 28*i0 + k3 + 56, _t72_26);
        _mm256_storeu_pd(M1 + 28*i0 + k3 + 84, _t72_27);
      }
    }
  }

  _t56_32 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_33 = _mm256_maskload_pd(M3 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_27 = _mm256_maskload_pd(M3 + 13, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_23 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_21 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_34 = _mm256_maskload_pd(M3 + 39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_29 = _mm256_maskload_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t73_12 = _mm256_loadu_pd(M1 + 84);
  _t73_9 = _mm256_loadu_pd(M1);
  _t73_10 = _mm256_loadu_pd(M1 + 28);
  _t73_11 = _mm256_loadu_pd(M1 + 56);
  _t73_8 = _mm256_broadcast_sd(&(M3[1]));

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 3), M3[12,12],h(1, 12, 3)) ),h(1, 12, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t73_14 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_15 = _t56_34;

  // 4-BLAC: 1x4 / 1x4
  _t73_16 = _mm256_div_pd(_t73_14, _t73_15);

  // AVX Storer:
  _t56_36 = _t73_16;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 2), M3[12,12],h(1, 12, 2)) ),h(1, 12, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t73_17 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_18 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t73_19 = _mm256_div_pd(_t73_17, _t73_18);

  // AVX Storer:
  _t56_35 = _t73_19;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 1), M3[12,12],h(1, 12, 1)) ),h(1, 12, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t73_20 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_21 = _t56_27;

  // 4-BLAC: 1x4 / 1x4
  _t73_22 = _mm256_div_pd(_t73_20, _t73_21);

  // AVX Storer:
  _t56_28 = _t73_22;

  // Generating : T614[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 0), M3[12,12],h(1, 12, 0)) ),h(1, 12, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t73_23 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_24 = _t56_21;

  // 4-BLAC: 1x4 / 1x4
  _t73_25 = _mm256_div_pd(_t73_23, _t73_24);

  // AVX Storer:
  _t56_22 = _t73_25;

  // Generating : M1[12,28] = S(h(1, 12, 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 3)) Kro G(h(1, 12, 3), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_26 = _t56_16;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t73_12 = _mm256_mul_pd(_t73_26, _t73_12);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(3, 12, 0), ( G(h(3, 12, 0), M1[12,28],h(4, 28, fi39)) - ( G(h(3, 12, 0), M3[12,12],h(1, 12, 3)) * G(h(1, 12, 3), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 3x4 -> 4x4
  _t73_27 = _t73_9;
  _t73_28 = _t73_10;
  _t73_29 = _t73_11;
  _t73_30 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t73_31 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_23, _t56_33, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_29, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t73_32 = _mm256_mul_pd(_t73_7, _t73_12);
  _t73_33 = _mm256_mul_pd(_t73_6, _t73_12);
  _t73_34 = _mm256_mul_pd(_t73_5, _t73_12);
  _t73_35 = _mm256_mul_pd(_t73_4, _t73_12);

  // 4-BLAC: 4x4 - 4x4
  _t73_36 = _mm256_sub_pd(_t73_27, _t73_32);
  _t73_37 = _mm256_sub_pd(_t73_28, _t73_33);
  _t73_38 = _mm256_sub_pd(_t73_29, _t73_34);
  _t73_39 = _mm256_sub_pd(_t73_30, _t73_35);

  // AVX Storer:
  _t73_9 = _t73_36;
  _t73_10 = _t73_37;
  _t73_11 = _t73_38;

  // Generating : M1[12,28] = S(h(1, 12, 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 2)) Kro G(h(1, 12, 2), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_40 = _t56_18;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t73_11 = _mm256_mul_pd(_t73_40, _t73_11);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(2, 12, 0), ( G(h(2, 12, 0), M1[12,28],h(4, 28, fi39)) - ( G(h(2, 12, 0), M3[12,12],h(1, 12, 2)) * G(h(1, 12, 2), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 2x4 -> 4x4
  _t73_41 = _t73_9;
  _t73_42 = _t73_10;
  _t73_43 = _mm256_setzero_pd();
  _t73_44 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t73_45 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_29, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t73_46 = _mm256_mul_pd(_t73_3, _t73_11);
  _t73_47 = _mm256_mul_pd(_t73_2, _t73_11);
  _t73_48 = _mm256_mul_pd(_t73_1, _t73_11);
  _t73_49 = _mm256_mul_pd(_t73_0, _t73_11);

  // 4-BLAC: 4x4 - 4x4
  _t73_50 = _mm256_sub_pd(_t73_41, _t73_46);
  _t73_51 = _mm256_sub_pd(_t73_42, _t73_47);
  _t73_52 = _mm256_sub_pd(_t73_43, _t73_48);
  _t73_53 = _mm256_sub_pd(_t73_44, _t73_49);

  // AVX Storer:
  _t73_9 = _t73_50;
  _t73_10 = _t73_51;

  // Generating : M1[12,28] = S(h(1, 12, 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_54 = _t56_19;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t73_10 = _mm256_mul_pd(_t73_54, _t73_10);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(1, 12, 0), ( G(h(1, 12, 0), M1[12,28],h(4, 28, fi39)) - ( G(h(1, 12, 0), M3[12,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_55 = _t73_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t73_13 = _mm256_mul_pd(_t73_55, _t73_10);

  // 4-BLAC: 1x4 - 1x4
  _t73_9 = _mm256_sub_pd(_t73_9, _t73_13);

  // AVX Storer:

  // Generating : M1[12,28] = S(h(1, 12, 0), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t73_56 = _t56_20;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t73_9 = _mm256_mul_pd(_t73_56, _t73_9);

  // AVX Storer:


  for( int fi39 = 4; fi39 <= 24; fi39+=4 ) {
    _t74_12 = _mm256_loadu_pd(M1 + fi39 + 84);
    _t74_9 = _mm256_loadu_pd(M1 + fi39);
    _t74_10 = _mm256_loadu_pd(M1 + fi39 + 28);
    _t74_11 = _mm256_loadu_pd(M1 + fi39 + 56);
    _t74_8 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : M1[12,28] = S(h(1, 12, 3), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 3)) Kro G(h(1, 12, 3), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_13 = _t56_16;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_12 = _mm256_mul_pd(_t74_13, _t74_12);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(3, 12, 0), ( G(h(3, 12, 0), M1[12,28],h(4, 28, fi39)) - ( G(h(3, 12, 0), M3[12,12],h(1, 12, 3)) * G(h(1, 12, 3), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 3x4 -> 4x4
    _t74_14 = _t74_9;
    _t74_15 = _t74_10;
    _t74_16 = _t74_11;
    _t74_17 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t74_18 = _mm256_blend_pd(_mm256_permute2f128_pd(_t74_8, _t56_33, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_29, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t73_32 = _mm256_mul_pd(_t74_7, _t74_12);
    _t73_33 = _mm256_mul_pd(_t74_6, _t74_12);
    _t73_34 = _mm256_mul_pd(_t74_5, _t74_12);
    _t73_35 = _mm256_mul_pd(_t74_4, _t74_12);

    // 4-BLAC: 4x4 - 4x4
    _t74_19 = _mm256_sub_pd(_t74_14, _t73_32);
    _t74_20 = _mm256_sub_pd(_t74_15, _t73_33);
    _t74_21 = _mm256_sub_pd(_t74_16, _t73_34);
    _t74_22 = _mm256_sub_pd(_t74_17, _t73_35);

    // AVX Storer:
    _t74_9 = _t74_19;
    _t74_10 = _t74_20;
    _t74_11 = _t74_21;

    // Generating : M1[12,28] = S(h(1, 12, 2), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 2)) Kro G(h(1, 12, 2), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_23 = _t56_18;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_11 = _mm256_mul_pd(_t74_23, _t74_11);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(2, 12, 0), ( G(h(2, 12, 0), M1[12,28],h(4, 28, fi39)) - ( G(h(2, 12, 0), M3[12,12],h(1, 12, 2)) * G(h(1, 12, 2), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 2x4 -> 4x4
    _t74_24 = _t74_9;
    _t74_25 = _t74_10;
    _t74_26 = _mm256_setzero_pd();
    _t74_27 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t74_28 = _mm256_shuffle_pd(_mm256_blend_pd(_t74_8, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_29, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t73_46 = _mm256_mul_pd(_t74_3, _t74_11);
    _t73_47 = _mm256_mul_pd(_t74_2, _t74_11);
    _t73_48 = _mm256_mul_pd(_t74_1, _t74_11);
    _t73_49 = _mm256_mul_pd(_t74_0, _t74_11);

    // 4-BLAC: 4x4 - 4x4
    _t74_29 = _mm256_sub_pd(_t74_24, _t73_46);
    _t74_30 = _mm256_sub_pd(_t74_25, _t73_47);
    _t74_31 = _mm256_sub_pd(_t74_26, _t73_48);
    _t74_32 = _mm256_sub_pd(_t74_27, _t73_49);

    // AVX Storer:
    _t74_9 = _t74_29;
    _t74_10 = _t74_30;

    // Generating : M1[12,28] = S(h(1, 12, 1), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_33 = _t56_19;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_10 = _mm256_mul_pd(_t74_33, _t74_10);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, 0), ( G(h(1, 12, 0), M1[12,28],h(4, 28, fi39)) - ( G(h(1, 12, 0), M3[12,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M1[12,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_34 = _t73_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t73_13 = _mm256_mul_pd(_t74_34, _t74_10);

    // 4-BLAC: 1x4 - 1x4
    _t74_9 = _mm256_sub_pd(_t74_9, _t73_13);

    // AVX Storer:

    // Generating : M1[12,28] = S(h(1, 12, 0), ( G(h(1, 1, 0), T614[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), M1[12,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_35 = _t56_20;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_9 = _mm256_mul_pd(_t74_35, _t74_9);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi39 + 84, _t74_12);
    _mm256_storeu_pd(M1 + fi39, _t74_9);
    _mm256_storeu_pd(M1 + fi39 + 28, _t74_10);
    _mm256_storeu_pd(M1 + fi39 + 56, _t74_11);
  }


  // Generating : x[28,1] = Sum_{i0} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), y[28,1],h(1, 1, 0)) + ( G(h(4, 28, i0), M2[28,12],h(4, 12, 0)) * G(h(4, 12, 0), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k3} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,12],h(4, 12, k3)) * G(h(4, 12, k3), v0[12,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t64_4);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t64_6);

  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t75_4 = _mm256_loadu_pd(y + i0);
    _t75_3 = _mm256_loadu_pd(M2 + 12*i0);
    _t75_2 = _mm256_loadu_pd(M2 + 12*i0 + 12);
    _t75_1 = _mm256_loadu_pd(M2 + 12*i0 + 24);
    _t75_0 = _mm256_loadu_pd(M2 + 12*i0 + 36);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t75_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t75_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32)), _mm256_mul_pd(_t75_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t75_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32)), _mm256_mul_pd(_t75_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t75_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32)), _mm256_mul_pd(_t75_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t75_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32)), _mm256_mul_pd(_t75_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_8, _t64_7), _mm256_unpacklo_pd(_t64_5, _t64_3), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t75_5 = _mm256_add_pd(_t75_4, _t75_6);

    // AVX Storer:

    for( int k3 = 4; k3 <= 11; k3+=4 ) {
      _t76_4 = _mm256_loadu_pd(M2 + 12*i0 + k3);
      _t76_3 = _mm256_loadu_pd(M2 + 12*i0 + k3 + 12);
      _t76_2 = _mm256_loadu_pd(M2 + 12*i0 + k3 + 24);
      _t76_1 = _mm256_loadu_pd(M2 + 12*i0 + k3 + 36);
      _t76_0 = _mm256_loadu_pd(v0 + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t76_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t76_4, _t76_0), _mm256_mul_pd(_t76_3, _t76_0)), _mm256_hadd_pd(_mm256_mul_pd(_t76_2, _t76_0), _mm256_mul_pd(_t76_1, _t76_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t76_4, _t76_0), _mm256_mul_pd(_t76_3, _t76_0)), _mm256_hadd_pd(_mm256_mul_pd(_t76_2, _t76_0), _mm256_mul_pd(_t76_1, _t76_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t75_5 = _mm256_add_pd(_t75_5, _t76_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(x + i0, _t75_5);
  }


  // Generating : P[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) - ( G(h(4, 28, i0), M2[28,12],h(4, 12, 0)) * G(h(4, 12, 0), M1[12,28],h(4, 28, i0)) ) ),h(4, 28, i0)) + Sum_{k2} ( -$(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,12],h(4, 12, k2)) * G(h(4, 12, k2), M1[12,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) + Sum_{k3} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k3)) - ( G(h(4, 28, i0), M2[28,12],h(4, 12, 0)) * G(h(4, 12, 0), M1[12,28],h(4, 28, k3)) ) ),h(4, 28, k3)) + Sum_{k2} ( -$(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,12],h(4, 12, k2)) * G(h(4, 12, k2), M1[12,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) ) ) ) + S(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) - ( G(h(4, 28, 24), M2[28,12],h(4, 12, 0)) * G(h(4, 12, 0), M1[12,28],h(4, 28, 24)) ) ),h(4, 28, 24)) ) + Sum_{k2} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), M2[28,12],h(4, 12, k2)) * G(h(4, 12, k2), M1[12,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  _mm256_storeu_pd(M1 + 84, _t73_12);
  _mm256_storeu_pd(M1 + 56, _t73_11);
  _mm256_storeu_pd(M1 + 28, _t73_10);
  _mm256_storeu_pd(M1, _t73_9);

  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t77_23 = _mm256_loadu_pd(Y + 29*i0);
    _t77_22 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t77_21 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t77_20 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t77_19 = _mm256_broadcast_sd(M2 + 12*i0);
    _t77_18 = _mm256_broadcast_sd(M2 + 12*i0 + 1);
    _t77_17 = _mm256_broadcast_sd(M2 + 12*i0 + 2);
    _t77_16 = _mm256_broadcast_sd(M2 + 12*i0 + 3);
    _t77_15 = _mm256_broadcast_sd(M2 + 12*i0 + 12);
    _t77_14 = _mm256_broadcast_sd(M2 + 12*i0 + 13);
    _t77_13 = _mm256_broadcast_sd(M2 + 12*i0 + 14);
    _t77_12 = _mm256_broadcast_sd(M2 + 12*i0 + 15);
    _t77_11 = _mm256_broadcast_sd(M2 + 12*i0 + 24);
    _t77_10 = _mm256_broadcast_sd(M2 + 12*i0 + 25);
    _t77_9 = _mm256_broadcast_sd(M2 + 12*i0 + 26);
    _t77_8 = _mm256_broadcast_sd(M2 + 12*i0 + 27);
    _t77_7 = _mm256_broadcast_sd(M2 + 12*i0 + 36);
    _t77_6 = _mm256_broadcast_sd(M2 + 12*i0 + 37);
    _t77_5 = _mm256_broadcast_sd(M2 + 12*i0 + 38);
    _t77_4 = _mm256_broadcast_sd(M2 + 12*i0 + 39);
    _t77_3 = _mm256_loadu_pd(M1 + i0);
    _t77_2 = _mm256_loadu_pd(M1 + i0 + 28);
    _t77_1 = _mm256_loadu_pd(M1 + i0 + 56);
    _t77_0 = _mm256_loadu_pd(M1 + i0 + 84);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t77_36 = _t77_23;
    _t77_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t77_23, _t77_22, 3), _t77_22, 12);
    _t77_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t77_23, _t77_22, 0), _t77_21, 49);
    _t77_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t77_23, _t77_22, 12), _mm256_shuffle_pd(_t77_21, _t77_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t77_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_19, _t77_3), _mm256_mul_pd(_t77_18, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_17, _t77_1), _mm256_mul_pd(_t77_16, _t77_0)));
    _t77_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_15, _t77_3), _mm256_mul_pd(_t77_14, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_13, _t77_1), _mm256_mul_pd(_t77_12, _t77_0)));
    _t77_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_11, _t77_3), _mm256_mul_pd(_t77_10, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_9, _t77_1), _mm256_mul_pd(_t77_8, _t77_0)));
    _t77_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_7, _t77_3), _mm256_mul_pd(_t77_6, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_5, _t77_1), _mm256_mul_pd(_t77_4, _t77_0)));

    // 4-BLAC: 4x4 - 4x4
    _t77_32 = _mm256_sub_pd(_t77_36, _t77_28);
    _t77_33 = _mm256_sub_pd(_t77_37, _t77_29);
    _t77_34 = _mm256_sub_pd(_t77_38, _t77_30);
    _t77_35 = _mm256_sub_pd(_t77_39, _t77_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t77_24 = _t77_32;
    _t77_25 = _t77_33;
    _t77_26 = _t77_34;
    _t77_27 = _t77_35;

    for( int k2 = 4; k2 <= 11; k2+=4 ) {
      _t78_19 = _mm256_broadcast_sd(M2 + 12*i0 + k2);
      _t78_18 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 1);
      _t78_17 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 2);
      _t78_16 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 3);
      _t78_15 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 12);
      _t78_14 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 13);
      _t78_13 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 14);
      _t78_12 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 15);
      _t78_11 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 24);
      _t78_10 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 25);
      _t78_9 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 26);
      _t78_8 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 27);
      _t78_7 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 36);
      _t78_6 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 37);
      _t78_5 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 38);
      _t78_4 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 39);
      _t78_3 = _mm256_loadu_pd(M1 + i0 + 28*k2);
      _t78_2 = _mm256_loadu_pd(M1 + i0 + 28*k2 + 28);
      _t78_1 = _mm256_loadu_pd(M1 + i0 + 28*k2 + 56);
      _t78_0 = _mm256_loadu_pd(M1 + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t78_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_19, _t78_3), _mm256_mul_pd(_t78_18, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_17, _t78_1), _mm256_mul_pd(_t78_16, _t78_0)));
      _t78_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_15, _t78_3), _mm256_mul_pd(_t78_14, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_13, _t78_1), _mm256_mul_pd(_t78_12, _t78_0)));
      _t78_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_11, _t78_3), _mm256_mul_pd(_t78_10, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_9, _t78_1), _mm256_mul_pd(_t78_8, _t78_0)));
      _t78_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_7, _t78_3), _mm256_mul_pd(_t78_6, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_5, _t78_1), _mm256_mul_pd(_t78_4, _t78_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t78_24 = _t77_24;
      _t78_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t77_24, _t77_25, 3), _t77_25, 12);
      _t78_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t77_24, _t77_25, 0), _t77_26, 49);
      _t78_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t77_24, _t77_25, 12), _mm256_shuffle_pd(_t77_26, _t77_27, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t78_24 = _mm256_sub_pd(_t78_24, _t78_20);
      _t78_25 = _mm256_sub_pd(_t78_25, _t78_21);
      _t78_26 = _mm256_sub_pd(_t78_26, _t78_22);
      _t78_27 = _mm256_sub_pd(_t78_27, _t78_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t77_24 = _t78_24;
      _t77_25 = _t78_25;
      _t77_26 = _t78_26;
      _t77_27 = _t78_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 27; k3+=4 ) {
      _t79_7 = _mm256_loadu_pd(Y + 28*i0 + k3);
      _t79_6 = _mm256_loadu_pd(Y + 28*i0 + k3 + 28);
      _t79_5 = _mm256_loadu_pd(Y + 28*i0 + k3 + 56);
      _t79_4 = _mm256_loadu_pd(Y + 28*i0 + k3 + 84);
      _t79_3 = _mm256_loadu_pd(M1 + k3);
      _t79_2 = _mm256_loadu_pd(M1 + k3 + 28);
      _t79_1 = _mm256_loadu_pd(M1 + k3 + 56);
      _t79_0 = _mm256_loadu_pd(M1 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t79_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_19, _t79_3), _mm256_mul_pd(_t77_18, _t79_2)), _mm256_add_pd(_mm256_mul_pd(_t77_17, _t79_1), _mm256_mul_pd(_t77_16, _t79_0)));
      _t79_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_15, _t79_3), _mm256_mul_pd(_t77_14, _t79_2)), _mm256_add_pd(_mm256_mul_pd(_t77_13, _t79_1), _mm256_mul_pd(_t77_12, _t79_0)));
      _t79_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_11, _t79_3), _mm256_mul_pd(_t77_10, _t79_2)), _mm256_add_pd(_mm256_mul_pd(_t77_9, _t79_1), _mm256_mul_pd(_t77_8, _t79_0)));
      _t79_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_7, _t79_3), _mm256_mul_pd(_t77_6, _t79_2)), _mm256_add_pd(_mm256_mul_pd(_t77_5, _t79_1), _mm256_mul_pd(_t77_4, _t79_0)));

      // 4-BLAC: 4x4 - 4x4
      _t79_12 = _mm256_sub_pd(_t79_7, _t79_8);
      _t79_13 = _mm256_sub_pd(_t79_6, _t79_9);
      _t79_14 = _mm256_sub_pd(_t79_5, _t79_10);
      _t79_15 = _mm256_sub_pd(_t79_4, _t79_11);

      // AVX Storer:

      for( int k2 = 4; k2 <= 11; k2+=4 ) {
        _t80_19 = _mm256_broadcast_sd(M2 + 12*i0 + k2);
        _t80_18 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 1);
        _t80_17 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 2);
        _t80_16 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 3);
        _t80_15 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 12);
        _t80_14 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 13);
        _t80_13 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 14);
        _t80_12 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 15);
        _t80_11 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 24);
        _t80_10 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 25);
        _t80_9 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 26);
        _t80_8 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 27);
        _t80_7 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 36);
        _t80_6 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 37);
        _t80_5 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 38);
        _t80_4 = _mm256_broadcast_sd(M2 + 12*i0 + k2 + 39);
        _t80_3 = _mm256_loadu_pd(M1 + 28*k2 + k3);
        _t80_2 = _mm256_loadu_pd(M1 + 28*k2 + k3 + 28);
        _t80_1 = _mm256_loadu_pd(M1 + 28*k2 + k3 + 56);
        _t80_0 = _mm256_loadu_pd(M1 + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t80_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t80_19, _t80_3), _mm256_mul_pd(_t80_18, _t80_2)), _mm256_add_pd(_mm256_mul_pd(_t80_17, _t80_1), _mm256_mul_pd(_t80_16, _t80_0)));
        _t80_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t80_15, _t80_3), _mm256_mul_pd(_t80_14, _t80_2)), _mm256_add_pd(_mm256_mul_pd(_t80_13, _t80_1), _mm256_mul_pd(_t80_12, _t80_0)));
        _t80_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t80_11, _t80_3), _mm256_mul_pd(_t80_10, _t80_2)), _mm256_add_pd(_mm256_mul_pd(_t80_9, _t80_1), _mm256_mul_pd(_t80_8, _t80_0)));
        _t80_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t80_7, _t80_3), _mm256_mul_pd(_t80_6, _t80_2)), _mm256_add_pd(_mm256_mul_pd(_t80_5, _t80_1), _mm256_mul_pd(_t80_4, _t80_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t79_12 = _mm256_sub_pd(_t79_12, _t80_20);
        _t79_13 = _mm256_sub_pd(_t79_13, _t80_21);
        _t79_14 = _mm256_sub_pd(_t79_14, _t80_22);
        _t79_15 = _mm256_sub_pd(_t79_15, _t80_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(P + 28*i0 + k3, _t79_12);
      _mm256_storeu_pd(P + 28*i0 + k3 + 28, _t79_13);
      _mm256_storeu_pd(P + 28*i0 + k3 + 56, _t79_14);
      _mm256_storeu_pd(P + 28*i0 + k3 + 84, _t79_15);
    }
    _mm256_storeu_pd(P + 29*i0, _t77_24);
    _mm256_maskstore_pd(P + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t77_25);
    _mm256_maskstore_pd(P + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t77_26);
    _mm256_maskstore_pd(P + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t77_27);
  }

  _t81_19 = _mm256_broadcast_sd(M2 + 288);
  _t81_18 = _mm256_broadcast_sd(M2 + 289);
  _t81_17 = _mm256_broadcast_sd(M2 + 290);
  _t81_16 = _mm256_broadcast_sd(M2 + 291);
  _t81_15 = _mm256_broadcast_sd(M2 + 300);
  _t81_14 = _mm256_broadcast_sd(M2 + 301);
  _t81_13 = _mm256_broadcast_sd(M2 + 302);
  _t81_12 = _mm256_broadcast_sd(M2 + 303);
  _t81_11 = _mm256_broadcast_sd(M2 + 312);
  _t81_10 = _mm256_broadcast_sd(M2 + 313);
  _t81_9 = _mm256_broadcast_sd(M2 + 314);
  _t81_8 = _mm256_broadcast_sd(M2 + 315);
  _t81_7 = _mm256_broadcast_sd(M2 + 324);
  _t81_6 = _mm256_broadcast_sd(M2 + 325);
  _t81_5 = _mm256_broadcast_sd(M2 + 326);
  _t81_4 = _mm256_broadcast_sd(M2 + 327);
  _t81_3 = _mm256_loadu_pd(M1 + 24);
  _t81_2 = _mm256_loadu_pd(M1 + 52);
  _t81_1 = _mm256_loadu_pd(M1 + 80);
  _t81_0 = _mm256_loadu_pd(M1 + 108);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t81_28 = _t19_28;
  _t81_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t81_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t81_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t81_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_19, _t81_3), _mm256_mul_pd(_t81_18, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_17, _t81_1), _mm256_mul_pd(_t81_16, _t81_0)));
  _t81_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_15, _t81_3), _mm256_mul_pd(_t81_14, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_13, _t81_1), _mm256_mul_pd(_t81_12, _t81_0)));
  _t81_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_11, _t81_3), _mm256_mul_pd(_t81_10, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_9, _t81_1), _mm256_mul_pd(_t81_8, _t81_0)));
  _t81_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_7, _t81_3), _mm256_mul_pd(_t81_6, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_5, _t81_1), _mm256_mul_pd(_t81_4, _t81_0)));

  // 4-BLAC: 4x4 - 4x4
  _t81_24 = _mm256_sub_pd(_t81_28, _t81_20);
  _t81_25 = _mm256_sub_pd(_t81_29, _t81_21);
  _t81_26 = _mm256_sub_pd(_t81_30, _t81_22);
  _t81_27 = _mm256_sub_pd(_t81_31, _t81_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t3_3 = _t81_24;
  _t3_2 = _t81_25;
  _t3_1 = _t81_26;
  _t3_0 = _t81_27;


  for( int k2 = 4; k2 <= 11; k2+=4 ) {
    _t82_19 = _mm256_broadcast_sd(M2 + k2 + 288);
    _t82_18 = _mm256_broadcast_sd(M2 + k2 + 289);
    _t82_17 = _mm256_broadcast_sd(M2 + k2 + 290);
    _t82_16 = _mm256_broadcast_sd(M2 + k2 + 291);
    _t82_15 = _mm256_broadcast_sd(M2 + k2 + 300);
    _t82_14 = _mm256_broadcast_sd(M2 + k2 + 301);
    _t82_13 = _mm256_broadcast_sd(M2 + k2 + 302);
    _t82_12 = _mm256_broadcast_sd(M2 + k2 + 303);
    _t82_11 = _mm256_broadcast_sd(M2 + k2 + 312);
    _t82_10 = _mm256_broadcast_sd(M2 + k2 + 313);
    _t82_9 = _mm256_broadcast_sd(M2 + k2 + 314);
    _t82_8 = _mm256_broadcast_sd(M2 + k2 + 315);
    _t82_7 = _mm256_broadcast_sd(M2 + k2 + 324);
    _t82_6 = _mm256_broadcast_sd(M2 + k2 + 325);
    _t82_5 = _mm256_broadcast_sd(M2 + k2 + 326);
    _t82_4 = _mm256_broadcast_sd(M2 + k2 + 327);
    _t82_3 = _mm256_loadu_pd(M1 + 28*k2 + 24);
    _t82_2 = _mm256_loadu_pd(M1 + 28*k2 + 52);
    _t82_1 = _mm256_loadu_pd(M1 + 28*k2 + 80);
    _t82_0 = _mm256_loadu_pd(M1 + 28*k2 + 108);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t82_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_19, _t82_3), _mm256_mul_pd(_t82_18, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_17, _t82_1), _mm256_mul_pd(_t82_16, _t82_0)));
    _t82_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_15, _t82_3), _mm256_mul_pd(_t82_14, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_13, _t82_1), _mm256_mul_pd(_t82_12, _t82_0)));
    _t82_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_11, _t82_3), _mm256_mul_pd(_t82_10, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_9, _t82_1), _mm256_mul_pd(_t82_8, _t82_0)));
    _t82_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_7, _t82_3), _mm256_mul_pd(_t82_6, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_5, _t82_1), _mm256_mul_pd(_t82_4, _t82_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t82_24 = _t3_3;
    _t82_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
    _t82_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
    _t82_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t82_24 = _mm256_sub_pd(_t82_24, _t82_20);
    _t82_25 = _mm256_sub_pd(_t82_25, _t82_21);
    _t82_26 = _mm256_sub_pd(_t82_26, _t82_22);
    _t82_27 = _mm256_sub_pd(_t82_27, _t82_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t3_3 = _t82_24;
    _t3_2 = _t82_25;
    _t3_1 = _t82_26;
    _t3_0 = _t82_27;
    _mm256_storeu_pd(P + 696, _t3_3);
    _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t3_2);
    _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t3_1);
    _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t3_0);
  }

  _mm256_storeu_pd(Y + 696, _t19_28);
  _mm256_maskstore_pd(Y + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t19_29);
  _mm256_maskstore_pd(Y + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t19_30);
  _mm256_maskstore_pd(Y + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t19_31);
  _mm256_maskstore_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t64_3);
  _mm256_maskstore_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t64_5);
  _mm256_maskstore_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t64_7);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t64_8);
  _mm256_storeu_pd(P + 696, _t3_3);
  _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t3_2);
  _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t3_1);
  _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t3_0);

}
