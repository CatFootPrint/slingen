/*
 * kf_kernel.h
 *
Decl { {u'B': Matrix[B, (28, 24), GenMatAccess], u'F': SquaredMatrix[F, (28, 28), GenMatAccess], u'H': Matrix[H, (24, 28), GenMatAccess], u'Q': Symmetric[Q, (28, 28), USMatAccess], u'P': Symmetric[P, (28, 28), USMatAccess], u'R': Symmetric[R, (24, 24), USMatAccess], u'M1': Matrix[M1, (24, 28), GenMatAccess], u'M0': SquaredMatrix[M0, (28, 28), GenMatAccess], u'M3': Symmetric[M3, (24, 24), USMatAccess], u'M2': Matrix[M2, (28, 24), GenMatAccess], u'Y': Symmetric[Y, (28, 28), USMatAccess], u'v0': Matrix[v0, (24, 1), GenMatAccess], u'u': Matrix[u, (24, 1), GenMatAccess], u'y': Matrix[y, (28, 1), GenMatAccess], u'x': Matrix[x, (28, 1), GenMatAccess], u'z': Matrix[z, (24, 1), GenMatAccess], 'T614': Matrix[T614, (1, 24), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'ldiv_ut_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'chol_u_ow_opt': {'m': 'm3.ll'}, 'ldiv_un_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'chol_u_ow_opt_m3_ldiv_un_ow_opt_m4_n1_ldiv_ut_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[28,24] ) ) * Tile( (1, 1), Tile( (4, 4), u[24,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[28,28] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[24,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[24,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[24,28] ) ) * Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[24,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[24,28] ) ) * Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[28,24] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[24,28] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[24,24] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[24,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[24,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[24,24] ) ) )
Eq.ann: {}
Entry 7:
For_{fi39;0;19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi39 + 1), M3[24,24],h(3, 24, fi39 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi39 + 1), M3[24,24],h(3, 24, fi39 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi39 + 2), M3[24,24],h(2, 24, fi39 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi39 + 2), M3[24,24],h(2, 24, fi39 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) = ( Tile( (1, 1), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Div Tile( (1, 1), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ) )
Eq.ann: {}
Entry 14:
For_{fi100;0;-fi39 + 16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ) )
Eq.ann: {}
 )Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi39 + 20, 24, fi39 + 4), M3[24,24],h(-fi39 + 20, 24, fi39 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi39 + 20, 24, fi39 + 4), M3[24,24],h(-fi39 + 20, 24, fi39 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 24, fi39), M3[24,24],h(-fi39 + 20, 24, fi39 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 24, fi39), M3[24,24],h(-fi39 + 20, 24, fi39 + 4)) ) ) ) )
Eq.ann: {}
 )Entry 8:
Eq: Tile( (1, 1), G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 20)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 20)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 21), M3[24,24],h(3, 24, 21)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 21), M3[24,24],h(3, 24, 21)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 21)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 21)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 22), M3[24,24],h(2, 24, 22)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 22), M3[24,24],h(2, 24, 22)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) = ( Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) Div Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ) = Sqrt( Tile( (1, 1), G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ) )
Eq.ann: {}
Entry 20:
For_{fi186;0;19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 24, fi186), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, fi186), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, fi186), M3[24,24],h(1, 24, fi186)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi186 + 1), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi186 + 1), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186), M3[24,24],h(3, 24, fi186 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 24, fi186 + 1), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, fi186 + 1), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, fi186 + 1), M3[24,24],h(1, 24, fi186 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi186 + 2), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi186 + 2), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186 + 1), M3[24,24],h(2, 24, fi186 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186 + 1), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 24, fi186 + 2), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, fi186 + 2), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, fi186 + 2), M3[24,24],h(1, 24, fi186 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186 + 3), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186 + 3), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186 + 2), M3[24,24],h(1, 24, fi186 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi186 + 2), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 24, fi186 + 3), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, fi186 + 3), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, fi186 + 3), M3[24,24],h(1, 24, fi186 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi186 + 20, 24, fi186 + 4), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi186 + 20, 24, fi186 + 4), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 24, fi186), M3[24,24],h(-fi186 + 20, 24, fi186 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 24, fi186), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 21:
Eq: Tile( (1, 1), G(h(1, 24, 20), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 20), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 21), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 21), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 24, 21), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 21), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 22), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 22), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 24, 22), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 22), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), v0[24,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 24, 23), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 23), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ) )
Eq.ann: {}
Entry 28:
For_{fi263;0;19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 24, -fi263 + 23), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, -fi263 + 23), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, -fi263 + 23), M3[24,24],h(1, 24, -fi263 + 23)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, -fi263 + 20), M3[24,24],h(1, 24, -fi263 + 23)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi263 + 23), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 24, -fi263 + 22), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, -fi263 + 22), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, -fi263 + 22), M3[24,24],h(1, 24, -fi263 + 22)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, -fi263 + 20), M3[24,24],h(1, 24, -fi263 + 22)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi263 + 22), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 24, -fi263 + 21), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, -fi263 + 21), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, -fi263 + 21), M3[24,24],h(1, 24, -fi263 + 21)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi263 + 20), M3[24,24],h(1, 24, -fi263 + 21)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi263 + 21), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, -fi263 + 20), M3[24,24],h(1, 24, -fi263 + 20)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi263 + 20, 24, 0), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi263 + 20, 24, 0), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi263 + 20, 24, 0), M3[24,24],h(4, 24, -fi263 + 20)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 24, -fi263 + 20), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 29:
Eq: Tile( (1, 1), G(h(1, 24, 3), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 3), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 3), M3[24,24],h(1, 24, 3)) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 0), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 0), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 0), M3[24,24],h(1, 24, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 3), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 24, 2), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 2), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 2), M3[24,24],h(1, 24, 2)) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 0), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 0), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 0), M3[24,24],h(1, 24, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 2), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 24, 1), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 1), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 1), M3[24,24],h(1, 24, 1)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), v0[24,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), v0[24,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), M3[24,24],h(1, 24, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 1), v0[24,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 24, 0), v0[24,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 24, 0), v0[24,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 24, 0), M3[24,24],h(1, 24, 0)) ) )
Eq.ann: {}
Entry 36:
For_{fi340;0;19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi340), M3[24,24],h(1, 24, fi340)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi340 + 1), M3[24,24],h(1, 24, fi340 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi340 + 2), M3[24,24],h(1, 24, fi340 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, fi340 + 3), M3[24,24],h(1, 24, fi340 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi359;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340), M1[24,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi340 + 1), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, fi340 + 1), M1[24,28],h(4, 28, fi359)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340), M3[24,24],h(3, 24, fi340 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340), M1[24,28],h(4, 28, fi359)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 1), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 1), M1[24,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi340 + 2), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, fi340 + 2), M1[24,28],h(4, 28, fi359)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 1), M3[24,24],h(2, 24, fi340 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 1), M1[24,28],h(4, 28, fi359)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 2), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 2), M1[24,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 3), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 3), M1[24,28],h(4, 28, fi359)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 2), M3[24,24],h(1, 24, fi340 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 2), M1[24,28],h(4, 28, fi359)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 3), M1[24,28],h(4, 28, fi359)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, fi340 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, fi340 + 3), M1[24,28],h(4, 28, fi359)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi340 + 20, 24, fi340 + 4), M1[24,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi340 + 20, 24, fi340 + 4), M1[24,28],h(28, 28, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 24, fi340), M3[24,24],h(-fi340 + 20, 24, fi340 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 24, fi340), M1[24,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 37:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 22)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 23)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ) )
Eq.ann: {}
Entry 39:
For_{fi406;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 20)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M1[24,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 21), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 21), M1[24,28],h(4, 28, fi406)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 20), M1[24,28],h(4, 28, fi406)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 21)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M1[24,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 22), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 22), M1[24,28],h(4, 28, fi406)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 21), M1[24,28],h(4, 28, fi406)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 22)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M1[24,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), M1[24,28],h(4, 28, fi406)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 22), M1[24,28],h(4, 28, fi406)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), M1[24,28],h(4, 28, fi406)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 23)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 23), M1[24,28],h(4, 28, fi406)) ) ) )
Eq.ann: {}
 )Entry 40:
For_{fi453;0;19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 23)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, -fi453 + 23), M3[24,24],h(1, 24, -fi453 + 23)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 22)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, -fi453 + 22), M3[24,24],h(1, 24, -fi453 + 22)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 21)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, -fi453 + 21), M3[24,24],h(1, 24, -fi453 + 21)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 20)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, -fi453 + 20), M3[24,24],h(1, 24, -fi453 + 20)) ) )
Eq.ann: {}
Entry 4:
For_{fi472;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 23), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 23)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 23), M1[24,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, -fi453 + 20), M3[24,24],h(1, 24, -fi453 + 23)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 23), M1[24,28],h(4, 28, fi472)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 22), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 22)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 22), M1[24,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, -fi453 + 20), M3[24,24],h(1, 24, -fi453 + 22)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 22), M1[24,28],h(4, 28, fi472)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 21), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 21)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 21), M1[24,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 20), M3[24,24],h(1, 24, -fi453 + 21)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 21), M1[24,28],h(4, 28, fi472)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, -fi453 + 20)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, -fi453 + 20), M1[24,28],h(4, 28, fi472)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi453 + 20, 24, 0), M1[24,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi453 + 20, 24, 0), M1[24,28],h(28, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi453 + 20, 24, 0), M3[24,24],h(4, 24, -fi453 + 20)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 24, -fi453 + 20), M1[24,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 41:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 3), M3[24,24],h(1, 24, 3)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 2), M3[24,24],h(1, 24, 2)) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 1), M3[24,24],h(1, 24, 1)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 1, 0), T614[1,24],h(1, 24, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 24, 0), M3[24,24],h(1, 24, 0)) ) )
Eq.ann: {}
Entry 45:
For_{fi519;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 3), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 3), M1[24,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 24, 0), M3[24,24],h(1, 24, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 3), M1[24,28],h(4, 28, fi519)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 2), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 2), M1[24,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 24, 0), M3[24,24],h(1, 24, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 2), M1[24,28],h(4, 28, fi519)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 1), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 1), M1[24,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), M3[24,24],h(1, 24, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 1), M1[24,28],h(4, 28, fi519)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T614[1,24],h(1, 24, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 24, 0), M1[24,28],h(4, 28, fi519)) ) ) )
Eq.ann: {}
 )Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[28,24] ) ) * Tile( (1, 1), Tile( (4, 4), v0[24,1] ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[28,24] ) ) * Tile( (1, 1), Tile( (4, 4), M1[24,28] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-02-06
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


#define PARAM0 24
#define PARAM1 24
#define PARAM2 28

#define ERRTHRESH 1e-5

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31,
	_t19_32, _t19_33, _t19_34, _t19_35, _t19_36, _t19_37, _t19_38, _t19_39,
	_t19_40, _t19_41, _t19_42, _t19_43;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11, _t27_12, _t27_13, _t27_14, _t27_15,
	_t27_16, _t27_17, _t27_18, _t27_19, _t27_20, _t27_21, _t27_22, _t27_23,
	_t27_24, _t27_25, _t27_26, _t27_27;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23,
	_t30_24, _t30_25, _t30_26, _t30_27;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23,
	_t31_24, _t31_25, _t31_26, _t31_27;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11, _t34_12, _t34_13, _t34_14, _t34_15,
	_t34_16, _t34_17, _t34_18, _t34_19;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7,
	_t36_8, _t36_9, _t36_10, _t36_11, _t36_12, _t36_13, _t36_14, _t36_15,
	_t36_16, _t36_17, _t36_18, _t36_19, _t36_20, _t36_21, _t36_22, _t36_23,
	_t36_24, _t36_25, _t36_26, _t36_27;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19, _t37_20, _t37_21, _t37_22, _t37_23,
	_t37_24, _t37_25, _t37_26, _t37_27;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31,
	_t39_32, _t39_33, _t39_34, _t39_35, _t39_36, _t39_37, _t39_38, _t39_39,
	_t39_40, _t39_41, _t39_42, _t39_43, _t39_44, _t39_45, _t39_46, _t39_47,
	_t39_48, _t39_49, _t39_50, _t39_51, _t39_52, _t39_53, _t39_54, _t39_55;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3, _t40_4, _t40_5, _t40_6, _t40_7,
	_t40_8, _t40_9, _t40_10, _t40_11, _t40_12, _t40_13, _t40_14, _t40_15,
	_t40_16, _t40_17, _t40_18, _t40_19, _t40_20, _t40_21, _t40_22, _t40_23,
	_t40_24, _t40_25, _t40_26, _t40_27;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19, _t43_20, _t43_21, _t43_22, _t43_23,
	_t43_24, _t43_25, _t43_26, _t43_27, _t43_28, _t43_29, _t43_30, _t43_31,
	_t43_32, _t43_33, _t43_34, _t43_35;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26, _t49_27;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23,
	_t50_24, _t50_25, _t50_26, _t50_27, _t50_28, _t50_29, _t50_30, _t50_31,
	_t50_32, _t50_33, _t50_34, _t50_35, _t50_36, _t50_37, _t50_38, _t50_39,
	_t50_40, _t50_41, _t50_42, _t50_43;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15,
	_t51_16, _t51_17, _t51_18, _t51_19, _t51_20, _t51_21, _t51_22, _t51_23,
	_t51_24, _t51_25, _t51_26, _t51_27, _t51_28, _t51_29, _t51_30, _t51_31;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26, _t53_27;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39,
	_t54_40, _t54_41, _t54_42, _t54_43;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11, _t55_12, _t55_13, _t55_14, _t55_15,
	_t55_16, _t55_17, _t55_18, _t55_19, _t55_20, _t55_21, _t55_22, _t55_23,
	_t55_24, _t55_25, _t55_26, _t55_27, _t55_28, _t55_29, _t55_30, _t55_31;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3, _t56_4, _t56_5, _t56_6, _t56_7,
	_t56_8, _t56_9, _t56_10, _t56_11, _t56_12, _t56_13, _t56_14, _t56_15,
	_t56_16, _t56_17, _t56_18, _t56_19, _t56_20, _t56_21, _t56_22, _t56_23,
	_t56_24, _t56_25, _t56_26, _t56_27, _t56_28, _t56_29, _t56_30, _t56_31,
	_t56_32, _t56_33, _t56_34, _t56_35, _t56_36, _t56_37, _t56_38, _t56_39,
	_t56_40, _t56_41, _t56_42, _t56_43, _t56_44, _t56_45, _t56_46, _t56_47,
	_t56_48, _t56_49, _t56_50, _t56_51, _t56_52, _t56_53, _t56_54, _t56_55,
	_t56_56, _t56_57, _t56_58, _t56_59, _t56_60, _t56_61, _t56_62, _t56_63,
	_t56_64, _t56_65, _t56_66, _t56_67, _t56_68, _t56_69, _t56_70, _t56_71,
	_t56_72, _t56_73, _t56_74, _t56_75, _t56_76, _t56_77, _t56_78, _t56_79,
	_t56_80, _t56_81, _t56_82, _t56_83, _t56_84, _t56_85, _t56_86, _t56_87,
	_t56_88, _t56_89, _t56_90, _t56_91, _t56_92, _t56_93, _t56_94, _t56_95,
	_t56_96, _t56_97, _t56_98, _t56_99, _t56_100, _t56_101, _t56_102, _t56_103,
	_t56_104, _t56_105, _t56_106, _t56_107, _t56_108, _t56_109, _t56_110, _t56_111,
	_t56_112, _t56_113, _t56_114, _t56_115, _t56_116, _t56_117, _t56_118, _t56_119,
	_t56_120, _t56_121, _t56_122, _t56_123, _t56_124, _t56_125, _t56_126, _t56_127,
	_t56_128, _t56_129, _t56_130, _t56_131, _t56_132, _t56_133, _t56_134, _t56_135,
	_t56_136, _t56_137, _t56_138, _t56_139, _t56_140;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11, _t57_12, _t57_13, _t57_14, _t57_15,
	_t57_16, _t57_17, _t57_18, _t57_19, _t57_20, _t57_21, _t57_22, _t57_23,
	_t57_24, _t57_25, _t57_26, _t57_27, _t57_28;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9, _t58_10, _t58_11, _t58_12, _t58_13, _t58_14, _t58_15,
	_t58_16, _t58_17, _t58_18, _t58_19, _t58_20, _t58_21, _t58_22, _t58_23,
	_t58_24, _t58_25, _t58_26, _t58_27, _t58_28, _t58_29, _t58_30, _t58_31,
	_t58_32, _t58_33, _t58_34, _t58_35, _t58_36, _t58_37, _t58_38, _t58_39;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26, _t59_27, _t59_28, _t59_29, _t59_30, _t59_31,
	_t59_32, _t59_33, _t59_34, _t59_35, _t59_36, _t59_37, _t59_38, _t59_39,
	_t59_40, _t59_41, _t59_42, _t59_43, _t59_44, _t59_45, _t59_46, _t59_47,
	_t59_48, _t59_49, _t59_50, _t59_51, _t59_52, _t59_53, _t59_54, _t59_55,
	_t59_56, _t59_57, _t59_58, _t59_59, _t59_60, _t59_61, _t59_62, _t59_63,
	_t59_64, _t59_65, _t59_66, _t59_67, _t59_68, _t59_69, _t59_70, _t59_71,
	_t59_72;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11, _t60_12, _t60_13, _t60_14, _t60_15,
	_t60_16, _t60_17, _t60_18, _t60_19, _t60_20, _t60_21, _t60_22, _t60_23,
	_t60_24, _t60_25, _t60_26, _t60_27, _t60_28, _t60_29, _t60_30, _t60_31;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11, _t61_12, _t61_13, _t61_14, _t61_15,
	_t61_16, _t61_17, _t61_18, _t61_19, _t61_20, _t61_21, _t61_22, _t61_23,
	_t61_24, _t61_25, _t61_26, _t61_27, _t61_28, _t61_29, _t61_30, _t61_31,
	_t61_32, _t61_33, _t61_34, _t61_35, _t61_36, _t61_37, _t61_38, _t61_39;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19, _t62_20, _t62_21, _t62_22, _t62_23,
	_t62_24, _t62_25, _t62_26, _t62_27, _t62_28, _t62_29, _t62_30, _t62_31,
	_t62_32, _t62_33, _t62_34, _t62_35, _t62_36, _t62_37, _t62_38, _t62_39,
	_t62_40, _t62_41, _t62_42, _t62_43, _t62_44, _t62_45, _t62_46, _t62_47,
	_t62_48, _t62_49, _t62_50, _t62_51, _t62_52, _t62_53, _t62_54, _t62_55,
	_t62_56, _t62_57, _t62_58, _t62_59, _t62_60, _t62_61, _t62_62, _t62_63,
	_t62_64, _t62_65, _t62_66, _t62_67, _t62_68, _t62_69, _t62_70, _t62_71,
	_t62_72, _t62_73, _t62_74, _t62_75, _t62_76, _t62_77, _t62_78, _t62_79,
	_t62_80, _t62_81, _t62_82;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6, _t63_7,
	_t63_8, _t63_9, _t63_10, _t63_11, _t63_12, _t63_13, _t63_14, _t63_15,
	_t63_16, _t63_17, _t63_18, _t63_19, _t63_20, _t63_21, _t63_22, _t63_23,
	_t63_24, _t63_25, _t63_26, _t63_27, _t63_28, _t63_29, _t63_30, _t63_31,
	_t63_32, _t63_33, _t63_34, _t63_35, _t63_36, _t63_37, _t63_38, _t63_39,
	_t63_40, _t63_41, _t63_42, _t63_43, _t63_44;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15,
	_t65_16, _t65_17, _t65_18, _t65_19, _t65_20, _t65_21, _t65_22, _t65_23,
	_t65_24, _t65_25, _t65_26, _t65_27, _t65_28, _t65_29, _t65_30, _t65_31,
	_t65_32, _t65_33, _t65_34, _t65_35, _t65_36, _t65_37;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26, _t66_27, _t66_28, _t66_29, _t66_30, _t66_31,
	_t66_32, _t66_33, _t66_34, _t66_35, _t66_36, _t66_37, _t66_38, _t66_39,
	_t66_40, _t66_41;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15,
	_t68_16, _t68_17, _t68_18, _t68_19, _t68_20, _t68_21, _t68_22, _t68_23,
	_t68_24, _t68_25, _t68_26, _t68_27, _t68_28, _t68_29, _t68_30, _t68_31,
	_t68_32, _t68_33, _t68_34, _t68_35;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23,
	_t69_24, _t69_25, _t69_26, _t69_27, _t69_28, _t69_29, _t69_30, _t69_31,
	_t69_32, _t69_33, _t69_34, _t69_35, _t69_36, _t69_37, _t69_38, _t69_39,
	_t69_40, _t69_41, _t69_42, _t69_43, _t69_44, _t69_45, _t69_46, _t69_47,
	_t69_48, _t69_49, _t69_50, _t69_51, _t69_52, _t69_53, _t69_54, _t69_55,
	_t69_56, _t69_57, _t69_58, _t69_59, _t69_60, _t69_61, _t69_62, _t69_63,
	_t69_64, _t69_65, _t69_66, _t69_67, _t69_68, _t69_69, _t69_70, _t69_71,
	_t69_72, _t69_73;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6, _t70_7,
	_t70_8, _t70_9, _t70_10, _t70_11, _t70_12, _t70_13, _t70_14, _t70_15,
	_t70_16, _t70_17, _t70_18, _t70_19, _t70_20, _t70_21, _t70_22, _t70_23,
	_t70_24, _t70_25, _t70_26, _t70_27, _t70_28;
  __m256d _t71_0, _t71_1, _t71_2, _t71_3, _t71_4, _t71_5, _t71_6, _t71_7,
	_t71_8, _t71_9, _t71_10, _t71_11, _t71_12, _t71_13, _t71_14, _t71_15,
	_t71_16, _t71_17, _t71_18, _t71_19, _t71_20, _t71_21, _t71_22, _t71_23,
	_t71_24, _t71_25, _t71_26, _t71_27, _t71_28, _t71_29, _t71_30, _t71_31,
	_t71_32, _t71_33, _t71_34, _t71_35;
  __m256d _t72_0, _t72_1, _t72_2, _t72_3, _t72_4, _t72_5, _t72_6, _t72_7,
	_t72_8, _t72_9, _t72_10, _t72_11, _t72_12, _t72_13, _t72_14, _t72_15,
	_t72_16, _t72_17, _t72_18, _t72_19, _t72_20, _t72_21, _t72_22, _t72_23,
	_t72_24, _t72_25, _t72_26, _t72_27, _t72_28, _t72_29, _t72_30, _t72_31,
	_t72_32, _t72_33, _t72_34, _t72_35, _t72_36, _t72_37, _t72_38, _t72_39,
	_t72_40, _t72_41, _t72_42, _t72_43, _t72_44, _t72_45, _t72_46, _t72_47,
	_t72_48, _t72_49, _t72_50, _t72_51, _t72_52, _t72_53, _t72_54, _t72_55,
	_t72_56, _t72_57;
  __m256d _t73_0, _t73_1, _t73_2, _t73_3, _t73_4, _t73_5, _t73_6, _t73_7,
	_t73_8, _t73_9, _t73_10, _t73_11, _t73_12, _t73_13, _t73_14, _t73_15,
	_t73_16, _t73_17, _t73_18, _t73_19, _t73_20, _t73_21, _t73_22, _t73_23,
	_t73_24, _t73_25, _t73_26, _t73_27, _t73_28;
  __m256d _t74_0, _t74_1, _t74_2, _t74_3, _t74_4, _t74_5, _t74_6, _t74_7,
	_t74_8, _t74_9, _t74_10, _t74_11, _t74_12, _t74_13, _t74_14, _t74_15,
	_t74_16, _t74_17, _t74_18, _t74_19, _t74_20, _t74_21, _t74_22, _t74_23,
	_t74_24, _t74_25, _t74_26, _t74_27, _t74_28, _t74_29, _t74_30, _t74_31,
	_t74_32, _t74_33, _t74_34, _t74_35, _t74_36, _t74_37, _t74_38, _t74_39,
	_t74_40, _t74_41, _t74_42, _t74_43, _t74_44, _t74_45, _t74_46, _t74_47,
	_t74_48, _t74_49, _t74_50, _t74_51, _t74_52, _t74_53, _t74_54, _t74_55,
	_t74_56, _t74_57, _t74_58, _t74_59, _t74_60, _t74_61, _t74_62, _t74_63,
	_t74_64, _t74_65, _t74_66, _t74_67, _t74_68, _t74_69, _t74_70;
  __m256d _t75_0, _t75_1, _t75_2, _t75_3, _t75_4, _t75_5, _t75_6, _t75_7,
	_t75_8, _t75_9, _t75_10, _t75_11, _t75_12, _t75_13, _t75_14, _t75_15,
	_t75_16, _t75_17, _t75_18, _t75_19, _t75_20, _t75_21, _t75_22, _t75_23,
	_t75_24, _t75_25, _t75_26, _t75_27, _t75_28, _t75_29, _t75_30, _t75_31,
	_t75_32, _t75_33, _t75_34;
  __m256d _t76_0, _t76_1, _t76_2, _t76_3, _t76_4, _t76_5, _t76_6, _t76_7,
	_t76_8, _t76_9, _t76_10, _t76_11, _t76_12, _t76_13, _t76_14, _t76_15,
	_t76_16, _t76_17, _t76_18, _t76_19, _t76_20, _t76_21, _t76_22, _t76_23,
	_t76_24, _t76_25, _t76_26, _t76_27;
  __m256d _t77_0, _t77_1, _t77_2, _t77_3, _t77_4, _t77_5, _t77_6, _t77_7,
	_t77_8, _t77_9, _t77_10, _t77_11, _t77_12, _t77_13, _t77_14, _t77_15,
	_t77_16, _t77_17, _t77_18, _t77_19, _t77_20, _t77_21, _t77_22, _t77_23,
	_t77_24, _t77_25, _t77_26, _t77_27, _t77_28, _t77_29, _t77_30, _t77_31,
	_t77_32, _t77_33, _t77_34, _t77_35, _t77_36, _t77_37, _t77_38, _t77_39,
	_t77_40, _t77_41, _t77_42, _t77_43, _t77_44, _t77_45, _t77_46, _t77_47,
	_t77_48, _t77_49, _t77_50, _t77_51, _t77_52, _t77_53, _t77_54, _t77_55,
	_t77_56;
  __m256d _t78_0, _t78_1, _t78_2, _t78_3, _t78_4, _t78_5, _t78_6, _t78_7,
	_t78_8, _t78_9, _t78_10, _t78_11, _t78_12, _t78_13, _t78_14, _t78_15,
	_t78_16, _t78_17, _t78_18, _t78_19, _t78_20, _t78_21, _t78_22, _t78_23,
	_t78_24, _t78_25, _t78_26, _t78_27, _t78_28, _t78_29, _t78_30, _t78_31,
	_t78_32, _t78_33, _t78_34, _t78_35;
  __m256d _t79_0, _t79_1, _t79_2, _t79_3, _t79_4, _t79_5, _t79_6;
  __m256d _t80_0, _t80_1, _t80_2, _t80_3, _t80_4, _t80_5;
  __m256d _t81_0, _t81_1, _t81_2, _t81_3, _t81_4, _t81_5, _t81_6, _t81_7,
	_t81_8, _t81_9, _t81_10, _t81_11, _t81_12, _t81_13, _t81_14, _t81_15,
	_t81_16, _t81_17, _t81_18, _t81_19, _t81_20, _t81_21, _t81_22, _t81_23,
	_t81_24, _t81_25, _t81_26, _t81_27, _t81_28, _t81_29, _t81_30, _t81_31,
	_t81_32, _t81_33, _t81_34, _t81_35, _t81_36, _t81_37, _t81_38, _t81_39;
  __m256d _t82_0, _t82_1, _t82_2, _t82_3, _t82_4, _t82_5, _t82_6, _t82_7,
	_t82_8, _t82_9, _t82_10, _t82_11, _t82_12, _t82_13, _t82_14, _t82_15,
	_t82_16, _t82_17, _t82_18, _t82_19, _t82_20, _t82_21, _t82_22, _t82_23,
	_t82_24, _t82_25, _t82_26, _t82_27;
  __m256d _t83_0, _t83_1, _t83_2, _t83_3, _t83_4, _t83_5, _t83_6, _t83_7,
	_t83_8, _t83_9, _t83_10, _t83_11, _t83_12, _t83_13, _t83_14, _t83_15;
  __m256d _t84_0, _t84_1, _t84_2, _t84_3, _t84_4, _t84_5, _t84_6, _t84_7,
	_t84_8, _t84_9, _t84_10, _t84_11, _t84_12, _t84_13, _t84_14, _t84_15,
	_t84_16, _t84_17, _t84_18, _t84_19, _t84_20, _t84_21, _t84_22, _t84_23;
  __m256d _t85_0, _t85_1, _t85_2, _t85_3, _t85_4, _t85_5, _t85_6, _t85_7,
	_t85_8, _t85_9, _t85_10, _t85_11, _t85_12, _t85_13, _t85_14, _t85_15,
	_t85_16, _t85_17, _t85_18, _t85_19, _t85_20, _t85_21, _t85_22, _t85_23,
	_t85_24, _t85_25, _t85_26, _t85_27, _t85_28, _t85_29, _t85_30, _t85_31;
  __m256d _t86_0, _t86_1, _t86_2, _t86_3, _t86_4, _t86_5, _t86_6, _t86_7,
	_t86_8, _t86_9, _t86_10, _t86_11, _t86_12, _t86_13, _t86_14, _t86_15,
	_t86_16, _t86_17, _t86_18, _t86_19, _t86_20, _t86_21, _t86_22, _t86_23,
	_t86_24, _t86_25, _t86_26, _t86_27;


  // Generating : y[28,1] = Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), x[28,1],h(1, 1, 0)) ) + ( G(h(4, 28, i0), B[28,24],h(4, 24, 0)) * G(h(4, 24, 0), u[24,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), x[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + Sum_{k3} ( $(h(4, 28, i0), ( G(h(4, 28, i0), B[28,24],h(4, 24, k3)) * G(h(4, 24, k3), u[24,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t0_9 = _mm256_loadu_pd(F + 28*i0);
    _t0_8 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t0_7 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t0_6 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t0_5 = _mm256_loadu_pd(x);
    _t0_4 = _mm256_loadu_pd(B + 24*i0);
    _t0_3 = _mm256_loadu_pd(B + 24*i0 + 24);
    _t0_2 = _mm256_loadu_pd(B + 24*i0 + 48);
    _t0_1 = _mm256_loadu_pd(B + 24*i0 + 72);
    _t0_0 = _mm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t1_4 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t1_3 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t1_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t1_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);
      _t1_0 = _mm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t0_10 = _mm256_add_pd(_t0_10, _t1_5);

      // AVX Storer:
    }

    for( int k3 = 4; k3 <= 23; k3+=4 ) {
      _t2_4 = _mm256_loadu_pd(B + 24*i0 + k3);
      _t2_3 = _mm256_loadu_pd(B + 24*i0 + k3 + 24);
      _t2_2 = _mm256_loadu_pd(B + 24*i0 + k3 + 48);
      _t2_1 = _mm256_loadu_pd(B + 24*i0 + k3 + 72);
      _t2_0 = _mm256_loadu_pd(u + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t0_10 = _mm256_add_pd(_t0_10, _t2_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(y + i0, _t0_10);
  }

  _t3_11 = _mm256_loadu_pd(P);
  _t3_10 = _mm256_maskload_pd(P + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_9 = _mm256_maskload_pd(P + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_8 = _mm256_maskload_pd(P + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t3_7 = _mm256_loadu_pd(P + 116);
  _t3_6 = _mm256_maskload_pd(P + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_5 = _mm256_maskload_pd(P + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_4 = _mm256_maskload_pd(P + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t3_3 = _mm256_loadu_pd(P + 696);
  _t3_2 = _mm256_maskload_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[28,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, 0), P[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 4)) * G(h(4, 28, 4), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, 4), P[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{k3} ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k3)) * G(h(4, 28, k3), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, k3), P[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 24)) * G(h(4, 28, 24), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_12 = _t3_11;
  _t3_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 3), _t3_10, 12);
  _t3_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 0), _t3_9, 49);
  _t3_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 12), _mm256_shuffle_pd(_t3_9, _t3_8, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_16 = _t3_7;
  _t3_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 3), _t3_6, 12);
  _t3_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 0), _t3_5, 49);
  _t3_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 12), _mm256_shuffle_pd(_t3_5, _t3_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_20 = _t3_3;
  _t3_21 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_22 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 28*i0);
    _t4_14 = _mm256_broadcast_sd(F + 28*i0 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 28*i0 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 28*i0 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 28*i0 + 28);
    _t4_10 = _mm256_broadcast_sd(F + 28*i0 + 29);
    _t4_9 = _mm256_broadcast_sd(F + 28*i0 + 30);
    _t4_8 = _mm256_broadcast_sd(F + 28*i0 + 31);
    _t4_7 = _mm256_broadcast_sd(F + 28*i0 + 56);
    _t4_6 = _mm256_broadcast_sd(F + 28*i0 + 57);
    _t4_5 = _mm256_broadcast_sd(F + 28*i0 + 58);
    _t4_4 = _mm256_broadcast_sd(F + 28*i0 + 59);
    _t4_3 = _mm256_broadcast_sd(F + 28*i0 + 84);
    _t4_2 = _mm256_broadcast_sd(F + 28*i0 + 85);
    _t4_1 = _mm256_broadcast_sd(F + 28*i0 + 86);
    _t4_0 = _mm256_broadcast_sd(F + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_12), _mm256_mul_pd(_t4_14, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_14), _mm256_mul_pd(_t4_12, _t3_15)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_12), _mm256_mul_pd(_t4_10, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_14), _mm256_mul_pd(_t4_8, _t3_15)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_12), _mm256_mul_pd(_t4_6, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_14), _mm256_mul_pd(_t4_4, _t3_15)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_12), _mm256_mul_pd(_t4_2, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_14), _mm256_mul_pd(_t4_0, _t3_15)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t5_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t5_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t5_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t5_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t5_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t5_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t5_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t5_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t5_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t5_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t5_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t5_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t5_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t5_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t5_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t5_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t5_3 = _mm256_loadu_pd(P + k2);
      _t5_2 = _mm256_loadu_pd(P + k2 + 28);
      _t5_1 = _mm256_loadu_pd(P + k2 + 56);
      _t5_0 = _mm256_loadu_pd(P + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t5_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 32);
      _t5_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 32);
      _t5_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 49);
      _t5_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t5_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_19, _t5_24), _mm256_mul_pd(_t5_18, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_17, _t5_26), _mm256_mul_pd(_t5_16, _t5_27)));
      _t5_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t5_24), _mm256_mul_pd(_t5_14, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t5_26), _mm256_mul_pd(_t5_12, _t5_27)));
      _t5_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t5_24), _mm256_mul_pd(_t5_10, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t5_26), _mm256_mul_pd(_t5_8, _t5_27)));
      _t5_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t5_24), _mm256_mul_pd(_t5_6, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t5_26), _mm256_mul_pd(_t5_4, _t5_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t4_16 = _mm256_add_pd(_t4_16, _t5_20);
      _t4_17 = _mm256_add_pd(_t4_17, _t5_21);
      _t4_18 = _mm256_add_pd(_t4_18, _t5_22);
      _t4_19 = _mm256_add_pd(_t4_19, _t5_23);

      // AVX Storer:
    }
    _t6_19 = _mm256_loadu_pd(P + 4);
    _t6_18 = _mm256_loadu_pd(P + 32);
    _t6_17 = _mm256_loadu_pd(P + 60);
    _t6_16 = _mm256_loadu_pd(P + 88);
    _t6_15 = _mm256_broadcast_sd(F + 28*i0 + 4);
    _t6_14 = _mm256_broadcast_sd(F + 28*i0 + 5);
    _t6_13 = _mm256_broadcast_sd(F + 28*i0 + 6);
    _t6_12 = _mm256_broadcast_sd(F + 28*i0 + 7);
    _t6_11 = _mm256_broadcast_sd(F + 28*i0 + 32);
    _t6_10 = _mm256_broadcast_sd(F + 28*i0 + 33);
    _t6_9 = _mm256_broadcast_sd(F + 28*i0 + 34);
    _t6_8 = _mm256_broadcast_sd(F + 28*i0 + 35);
    _t6_7 = _mm256_broadcast_sd(F + 28*i0 + 60);
    _t6_6 = _mm256_broadcast_sd(F + 28*i0 + 61);
    _t6_5 = _mm256_broadcast_sd(F + 28*i0 + 62);
    _t6_4 = _mm256_broadcast_sd(F + 28*i0 + 63);
    _t6_3 = _mm256_broadcast_sd(F + 28*i0 + 88);
    _t6_2 = _mm256_broadcast_sd(F + 28*i0 + 89);
    _t6_1 = _mm256_broadcast_sd(F + 28*i0 + 90);
    _t6_0 = _mm256_broadcast_sd(F + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t6_19), _mm256_mul_pd(_t4_14, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t6_17), _mm256_mul_pd(_t4_12, _t6_16)));
    _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t6_19), _mm256_mul_pd(_t4_10, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t6_17), _mm256_mul_pd(_t4_8, _t6_16)));
    _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t6_19), _mm256_mul_pd(_t4_6, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t6_17), _mm256_mul_pd(_t4_4, _t6_16)));
    _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t6_19), _mm256_mul_pd(_t4_2, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t6_17), _mm256_mul_pd(_t4_0, _t6_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t6_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t3_16), _mm256_mul_pd(_t6_14, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t3_18), _mm256_mul_pd(_t6_12, _t3_19)));
    _t6_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t3_16), _mm256_mul_pd(_t6_10, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t3_18), _mm256_mul_pd(_t6_8, _t3_19)));
    _t6_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t3_16), _mm256_mul_pd(_t6_6, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t3_18), _mm256_mul_pd(_t6_4, _t3_19)));
    _t6_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_3, _t3_16), _mm256_mul_pd(_t6_2, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_1, _t3_18), _mm256_mul_pd(_t6_0, _t3_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t6_20 = _mm256_add_pd(_t6_20, _t6_24);
    _t6_21 = _mm256_add_pd(_t6_21, _t6_25);
    _t6_22 = _mm256_add_pd(_t6_22, _t6_26);
    _t6_23 = _mm256_add_pd(_t6_23, _t6_27);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t7_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t7_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t7_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t7_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t7_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t7_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t7_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t7_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t7_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t7_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t7_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t7_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t7_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t7_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t7_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t7_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t7_3 = _mm256_loadu_pd(P + k2 + 112);
      _t7_2 = _mm256_loadu_pd(P + k2 + 140);
      _t7_1 = _mm256_loadu_pd(P + k2 + 168);
      _t7_0 = _mm256_loadu_pd(P + k2 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t7_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
      _t7_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
      _t7_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
      _t7_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t7_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_24), _mm256_mul_pd(_t7_18, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_26), _mm256_mul_pd(_t7_16, _t7_27)));
      _t7_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_24), _mm256_mul_pd(_t7_14, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_26), _mm256_mul_pd(_t7_12, _t7_27)));
      _t7_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_24), _mm256_mul_pd(_t7_10, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_26), _mm256_mul_pd(_t7_8, _t7_27)));
      _t7_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_24), _mm256_mul_pd(_t7_6, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_26), _mm256_mul_pd(_t7_4, _t7_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t6_20 = _mm256_add_pd(_t6_20, _t7_20);
      _t6_21 = _mm256_add_pd(_t6_21, _t7_21);
      _t6_22 = _mm256_add_pd(_t6_22, _t7_22);
      _t6_23 = _mm256_add_pd(_t6_23, _t7_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k3 = 8; k3 <= 23; k3+=4 ) {
      _t8_3 = _mm256_loadu_pd(P + k3);
      _t8_2 = _mm256_loadu_pd(P + k3 + 28);
      _t8_1 = _mm256_loadu_pd(P + k3 + 56);
      _t8_0 = _mm256_loadu_pd(P + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t8_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t8_3), _mm256_mul_pd(_t4_14, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t8_1), _mm256_mul_pd(_t4_12, _t8_0)));
      _t8_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t8_3), _mm256_mul_pd(_t4_10, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t8_1), _mm256_mul_pd(_t4_8, _t8_0)));
      _t8_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t8_3), _mm256_mul_pd(_t4_6, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t8_1), _mm256_mul_pd(_t4_4, _t8_0)));
      _t8_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t8_3), _mm256_mul_pd(_t4_2, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t8_1), _mm256_mul_pd(_t4_0, _t8_0)));

      // AVX Storer:

      for( int k2 = 4; k2 <= k3 - 1; k2+=4 ) {
        _t9_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
        _t9_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
        _t9_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
        _t9_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
        _t9_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
        _t9_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
        _t9_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
        _t9_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
        _t9_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
        _t9_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
        _t9_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
        _t9_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
        _t9_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
        _t9_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
        _t9_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
        _t9_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
        _t9_3 = _mm256_loadu_pd(P + 28*k2 + k3);
        _t9_2 = _mm256_loadu_pd(P + 28*k2 + k3 + 28);
        _t9_1 = _mm256_loadu_pd(P + 28*k2 + k3 + 56);
        _t9_0 = _mm256_loadu_pd(P + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_3), _mm256_mul_pd(_t9_18, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_1), _mm256_mul_pd(_t9_16, _t9_0)));
        _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_3), _mm256_mul_pd(_t9_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_1), _mm256_mul_pd(_t9_12, _t9_0)));
        _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_3), _mm256_mul_pd(_t9_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_1), _mm256_mul_pd(_t9_8, _t9_0)));
        _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_3), _mm256_mul_pd(_t9_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_1), _mm256_mul_pd(_t9_4, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t8_4 = _mm256_add_pd(_t8_4, _t9_20);
        _t8_5 = _mm256_add_pd(_t8_5, _t9_21);
        _t8_6 = _mm256_add_pd(_t8_6, _t9_22);
        _t8_7 = _mm256_add_pd(_t8_7, _t9_23);

        // AVX Storer:
      }
      _t10_19 = _mm256_broadcast_sd(F + 28*i0 + k3);
      _t10_18 = _mm256_broadcast_sd(F + 28*i0 + k3 + 1);
      _t10_17 = _mm256_broadcast_sd(F + 28*i0 + k3 + 2);
      _t10_16 = _mm256_broadcast_sd(F + 28*i0 + k3 + 3);
      _t10_15 = _mm256_broadcast_sd(F + 28*i0 + k3 + 28);
      _t10_14 = _mm256_broadcast_sd(F + 28*i0 + k3 + 29);
      _t10_13 = _mm256_broadcast_sd(F + 28*i0 + k3 + 30);
      _t10_12 = _mm256_broadcast_sd(F + 28*i0 + k3 + 31);
      _t10_11 = _mm256_broadcast_sd(F + 28*i0 + k3 + 56);
      _t10_10 = _mm256_broadcast_sd(F + 28*i0 + k3 + 57);
      _t10_9 = _mm256_broadcast_sd(F + 28*i0 + k3 + 58);
      _t10_8 = _mm256_broadcast_sd(F + 28*i0 + k3 + 59);
      _t10_7 = _mm256_broadcast_sd(F + 28*i0 + k3 + 84);
      _t10_6 = _mm256_broadcast_sd(F + 28*i0 + k3 + 85);
      _t10_5 = _mm256_broadcast_sd(F + 28*i0 + k3 + 86);
      _t10_4 = _mm256_broadcast_sd(F + 28*i0 + k3 + 87);
      _t10_3 = _mm256_loadu_pd(P + 29*k3);
      _t10_2 = _mm256_maskload_pd(P + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t10_1 = _mm256_maskload_pd(P + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t10_0 = _mm256_maskload_pd(P + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t10_24 = _t10_3;
      _t10_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
      _t10_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
      _t10_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t10_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_24), _mm256_mul_pd(_t10_18, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_26), _mm256_mul_pd(_t10_16, _t10_27)));
      _t10_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_24), _mm256_mul_pd(_t10_14, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_26), _mm256_mul_pd(_t10_12, _t10_27)));
      _t10_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_24), _mm256_mul_pd(_t10_10, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_26), _mm256_mul_pd(_t10_8, _t10_27)));
      _t10_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_24), _mm256_mul_pd(_t10_6, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t10_26), _mm256_mul_pd(_t10_4, _t10_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_4 = _mm256_add_pd(_t8_4, _t10_20);
      _t8_5 = _mm256_add_pd(_t8_5, _t10_21);
      _t8_6 = _mm256_add_pd(_t8_6, _t10_22);
      _t8_7 = _mm256_add_pd(_t8_7, _t10_23);

      // AVX Storer:

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t11_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
        _t11_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
        _t11_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
        _t11_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
        _t11_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
        _t11_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
        _t11_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
        _t11_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
        _t11_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
        _t11_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
        _t11_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
        _t11_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
        _t11_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
        _t11_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
        _t11_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
        _t11_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
        _t11_3 = _mm256_loadu_pd(P + k2 + 28*k3);
        _t11_2 = _mm256_loadu_pd(P + k2 + 28*k3 + 28);
        _t11_1 = _mm256_loadu_pd(P + k2 + 28*k3 + 56);
        _t11_0 = _mm256_loadu_pd(P + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t11_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
        _t11_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
        _t11_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
        _t11_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t11_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_24), _mm256_mul_pd(_t11_18, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_26), _mm256_mul_pd(_t11_16, _t11_27)));
        _t11_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_24), _mm256_mul_pd(_t11_14, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_26), _mm256_mul_pd(_t11_12, _t11_27)));
        _t11_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_24), _mm256_mul_pd(_t11_10, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_26), _mm256_mul_pd(_t11_8, _t11_27)));
        _t11_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_24), _mm256_mul_pd(_t11_6, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_26), _mm256_mul_pd(_t11_4, _t11_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t8_4 = _mm256_add_pd(_t8_4, _t11_20);
        _t8_5 = _mm256_add_pd(_t8_5, _t11_21);
        _t8_6 = _mm256_add_pd(_t8_6, _t11_22);
        _t8_7 = _mm256_add_pd(_t8_7, _t11_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M0 + 28*i0 + k3, _t8_4);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 28, _t8_5);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 56, _t8_6);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 84, _t8_7);
    }
    _t12_3 = _mm256_loadu_pd(P + 24);
    _t12_2 = _mm256_loadu_pd(P + 52);
    _t12_1 = _mm256_loadu_pd(P + 80);
    _t12_0 = _mm256_loadu_pd(P + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t12_3), _mm256_mul_pd(_t4_14, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t12_1), _mm256_mul_pd(_t4_12, _t12_0)));
    _t12_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t12_3), _mm256_mul_pd(_t4_10, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t12_1), _mm256_mul_pd(_t4_8, _t12_0)));
    _t12_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t12_3), _mm256_mul_pd(_t4_6, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t12_1), _mm256_mul_pd(_t4_4, _t12_0)));
    _t12_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t12_3), _mm256_mul_pd(_t4_2, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t12_1), _mm256_mul_pd(_t4_0, _t12_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t13_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t13_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t13_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t13_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t13_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t13_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t13_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t13_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t13_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t13_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t13_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t13_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t13_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t13_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t13_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t13_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t13_3 = _mm256_loadu_pd(P + 28*k2 + 24);
      _t13_2 = _mm256_loadu_pd(P + 28*k2 + 52);
      _t13_1 = _mm256_loadu_pd(P + 28*k2 + 80);
      _t13_0 = _mm256_loadu_pd(P + 28*k2 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t13_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_3), _mm256_mul_pd(_t13_18, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_1), _mm256_mul_pd(_t13_16, _t13_0)));
      _t13_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_3), _mm256_mul_pd(_t13_14, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_1), _mm256_mul_pd(_t13_12, _t13_0)));
      _t13_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_3), _mm256_mul_pd(_t13_10, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_1), _mm256_mul_pd(_t13_8, _t13_0)));
      _t13_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_3), _mm256_mul_pd(_t13_6, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_1), _mm256_mul_pd(_t13_4, _t13_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t12_4 = _mm256_add_pd(_t12_4, _t13_20);
      _t12_5 = _mm256_add_pd(_t12_5, _t13_21);
      _t12_6 = _mm256_add_pd(_t12_6, _t13_22);
      _t12_7 = _mm256_add_pd(_t12_7, _t13_23);

      // AVX Storer:
    }
    _t14_15 = _mm256_broadcast_sd(F + 28*i0 + 24);
    _t14_14 = _mm256_broadcast_sd(F + 28*i0 + 25);
    _t14_13 = _mm256_broadcast_sd(F + 28*i0 + 26);
    _t14_12 = _mm256_broadcast_sd(F + 28*i0 + 27);
    _t14_11 = _mm256_broadcast_sd(F + 28*i0 + 52);
    _t14_10 = _mm256_broadcast_sd(F + 28*i0 + 53);
    _t14_9 = _mm256_broadcast_sd(F + 28*i0 + 54);
    _t14_8 = _mm256_broadcast_sd(F + 28*i0 + 55);
    _t14_7 = _mm256_broadcast_sd(F + 28*i0 + 80);
    _t14_6 = _mm256_broadcast_sd(F + 28*i0 + 81);
    _t14_5 = _mm256_broadcast_sd(F + 28*i0 + 82);
    _t14_4 = _mm256_broadcast_sd(F + 28*i0 + 83);
    _t14_3 = _mm256_broadcast_sd(F + 28*i0 + 108);
    _t14_2 = _mm256_broadcast_sd(F + 28*i0 + 109);
    _t14_1 = _mm256_broadcast_sd(F + 28*i0 + 110);
    _t14_0 = _mm256_broadcast_sd(F + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t14_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t3_20), _mm256_mul_pd(_t14_14, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t3_22), _mm256_mul_pd(_t14_12, _t3_23)));
    _t14_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t3_20), _mm256_mul_pd(_t14_10, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t3_22), _mm256_mul_pd(_t14_8, _t3_23)));
    _t14_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_7, _t3_20), _mm256_mul_pd(_t14_6, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_5, _t3_22), _mm256_mul_pd(_t14_4, _t3_23)));
    _t14_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _t3_20), _mm256_mul_pd(_t14_2, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_1, _t3_22), _mm256_mul_pd(_t14_0, _t3_23)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_4 = _mm256_add_pd(_t12_4, _t14_16);
    _t12_5 = _mm256_add_pd(_t12_5, _t14_17);
    _t12_6 = _mm256_add_pd(_t12_6, _t14_18);
    _t12_7 = _mm256_add_pd(_t12_7, _t14_19);

    // AVX Storer:
    _mm256_storeu_pd(M0 + 28*i0, _t4_16);
    _mm256_storeu_pd(M0 + 28*i0 + 28, _t4_17);
    _mm256_storeu_pd(M0 + 28*i0 + 56, _t4_18);
    _mm256_storeu_pd(M0 + 28*i0 + 84, _t4_19);
    _mm256_storeu_pd(M0 + 28*i0 + 4, _t6_20);
    _mm256_storeu_pd(M0 + 28*i0 + 32, _t6_21);
    _mm256_storeu_pd(M0 + 28*i0 + 60, _t6_22);
    _mm256_storeu_pd(M0 + 28*i0 + 88, _t6_23);
    _mm256_storeu_pd(M0 + 28*i0 + 24, _t12_4);
    _mm256_storeu_pd(M0 + 28*i0 + 52, _t12_5);
    _mm256_storeu_pd(M0 + 28*i0 + 80, _t12_6);
    _mm256_storeu_pd(M0 + 28*i0 + 108, _t12_7);
  }


  // Generating : Y[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, i0)) ),h(4, 28, i0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) ) ),h(4, 28, i0)) ) ) + Sum_{k3} ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, k3), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, k3), F[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) ) + S(h(4, 28, 24), ( ( G(h(4, 28, 24), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, 24), Q[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 28, 24), ( G(h(4, 28, 24), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, k2)) ) ),h(4, 28, 24)) ) )


  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t15_23 = _mm256_broadcast_sd(M0 + 28*i0);
    _t15_22 = _mm256_broadcast_sd(M0 + 28*i0 + 1);
    _t15_21 = _mm256_broadcast_sd(M0 + 28*i0 + 2);
    _t15_20 = _mm256_broadcast_sd(M0 + 28*i0 + 3);
    _t15_19 = _mm256_broadcast_sd(M0 + 28*i0 + 28);
    _t15_18 = _mm256_broadcast_sd(M0 + 28*i0 + 29);
    _t15_17 = _mm256_broadcast_sd(M0 + 28*i0 + 30);
    _t15_16 = _mm256_broadcast_sd(M0 + 28*i0 + 31);
    _t15_15 = _mm256_broadcast_sd(M0 + 28*i0 + 56);
    _t15_14 = _mm256_broadcast_sd(M0 + 28*i0 + 57);
    _t15_13 = _mm256_broadcast_sd(M0 + 28*i0 + 58);
    _t15_12 = _mm256_broadcast_sd(M0 + 28*i0 + 59);
    _t15_11 = _mm256_broadcast_sd(M0 + 28*i0 + 84);
    _t15_10 = _mm256_broadcast_sd(M0 + 28*i0 + 85);
    _t15_9 = _mm256_broadcast_sd(M0 + 28*i0 + 86);
    _t15_8 = _mm256_broadcast_sd(M0 + 28*i0 + 87);
    _t15_7 = _mm256_loadu_pd(F + 28*i0);
    _t15_6 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t15_5 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t15_4 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t15_3 = _mm256_loadu_pd(Q + 29*i0);
    _t15_2 = _mm256_maskload_pd(Q + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t15_1 = _mm256_maskload_pd(Q + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t15_0 = _mm256_maskload_pd(Q + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
    _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
    _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
    _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
    _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
    _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
    _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t15_36 = _t15_3;
    _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
    _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
    _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
    _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
    _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
    _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t15_24;
    _t15_29 = _t15_25;
    _t15_30 = _t15_26;
    _t15_31 = _t15_27;

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 28*i0 + k2);
      _t16_18 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 28);
      _t16_14 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 29);
      _t16_13 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 30);
      _t16_12 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 31);
      _t16_11 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 56);
      _t16_10 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 57);
      _t16_9 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 58);
      _t16_8 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 59);
      _t16_7 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 84);
      _t16_6 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 85);
      _t16_5 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 86);
      _t16_4 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 87);
      _t16_3 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t16_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t16_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t16_0 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_28), _mm256_mul_pd(_t16_18, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_30), _mm256_mul_pd(_t16_16, _t16_31)));
      _t16_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_28), _mm256_mul_pd(_t16_14, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_30), _mm256_mul_pd(_t16_12, _t16_31)));
      _t16_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_28), _mm256_mul_pd(_t16_10, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_30), _mm256_mul_pd(_t16_8, _t16_31)));
      _t16_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_28), _mm256_mul_pd(_t16_6, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_30), _mm256_mul_pd(_t16_4, _t16_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_24 = _t15_28;
      _t16_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
      _t16_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
      _t16_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_24 = _mm256_add_pd(_t16_24, _t16_20);
      _t16_25 = _mm256_add_pd(_t16_25, _t16_21);
      _t16_26 = _mm256_add_pd(_t16_26, _t16_22);
      _t16_27 = _mm256_add_pd(_t16_27, _t16_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t15_28 = _t16_24;
      _t15_29 = _t16_25;
      _t15_30 = _t16_26;
      _t15_31 = _t16_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 27; k3+=4 ) {
      _t17_7 = _mm256_loadu_pd(F + 28*k3);
      _t17_6 = _mm256_loadu_pd(F + 28*k3 + 28);
      _t17_5 = _mm256_loadu_pd(F + 28*k3 + 56);
      _t17_4 = _mm256_loadu_pd(F + 28*k3 + 84);
      _t17_3 = _mm256_loadu_pd(Q + 28*i0 + k3);
      _t17_2 = _mm256_loadu_pd(Q + 28*i0 + k3 + 28);
      _t17_1 = _mm256_loadu_pd(Q + 28*i0 + k3 + 56);
      _t17_0 = _mm256_loadu_pd(Q + 28*i0 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t17_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_6), _mm256_unpacklo_pd(_t17_5, _t17_4), 32);
      _t17_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_7, _t17_6), _mm256_unpackhi_pd(_t17_5, _t17_4), 32);
      _t17_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_6), _mm256_unpacklo_pd(_t17_5, _t17_4), 49);
      _t17_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_7, _t17_6), _mm256_unpackhi_pd(_t17_5, _t17_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t17_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t17_16), _mm256_mul_pd(_t15_22, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t17_18), _mm256_mul_pd(_t15_20, _t17_19)));
      _t17_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t17_16), _mm256_mul_pd(_t15_18, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t17_18), _mm256_mul_pd(_t15_16, _t17_19)));
      _t17_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t17_16), _mm256_mul_pd(_t15_14, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t17_18), _mm256_mul_pd(_t15_12, _t17_19)));
      _t17_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t17_16), _mm256_mul_pd(_t15_10, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t17_18), _mm256_mul_pd(_t15_8, _t17_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t17_8 = _mm256_add_pd(_t17_12, _t17_3);
      _t17_9 = _mm256_add_pd(_t17_13, _t17_2);
      _t17_10 = _mm256_add_pd(_t17_14, _t17_1);
      _t17_11 = _mm256_add_pd(_t17_15, _t17_0);

      // AVX Storer:

      for( int k2 = 4; k2 <= 27; k2+=4 ) {
        _t18_19 = _mm256_broadcast_sd(M0 + 28*i0 + k2);
        _t18_18 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 1);
        _t18_17 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 2);
        _t18_16 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 3);
        _t18_15 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 28);
        _t18_14 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 29);
        _t18_13 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 30);
        _t18_12 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 31);
        _t18_11 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 56);
        _t18_10 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 57);
        _t18_9 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 58);
        _t18_8 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 59);
        _t18_7 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 84);
        _t18_6 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 85);
        _t18_5 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 86);
        _t18_4 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 87);
        _t18_3 = _mm256_loadu_pd(F + k2 + 28*k3);
        _t18_2 = _mm256_loadu_pd(F + k2 + 28*k3 + 28);
        _t18_1 = _mm256_loadu_pd(F + k2 + 28*k3 + 56);
        _t18_0 = _mm256_loadu_pd(F + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t18_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
        _t18_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
        _t18_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
        _t18_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_24), _mm256_mul_pd(_t18_18, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_26), _mm256_mul_pd(_t18_16, _t18_27)));
        _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_24), _mm256_mul_pd(_t18_14, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_26), _mm256_mul_pd(_t18_12, _t18_27)));
        _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_24), _mm256_mul_pd(_t18_10, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_26), _mm256_mul_pd(_t18_8, _t18_27)));
        _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_24), _mm256_mul_pd(_t18_6, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_26), _mm256_mul_pd(_t18_4, _t18_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_8 = _mm256_add_pd(_t17_8, _t18_20);
        _t17_9 = _mm256_add_pd(_t17_9, _t18_21);
        _t17_10 = _mm256_add_pd(_t17_10, _t18_22);
        _t17_11 = _mm256_add_pd(_t17_11, _t18_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(Y + 28*i0 + k3, _t17_8);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 28, _t17_9);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 56, _t17_10);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 84, _t17_11);
    }
    _mm256_storeu_pd(Y + 29*i0, _t15_28);
    _mm256_maskstore_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
    _mm256_maskstore_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
    _mm256_maskstore_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  }

  _t19_23 = _mm256_broadcast_sd(M0 + 672);
  _t19_22 = _mm256_broadcast_sd(M0 + 673);
  _t19_21 = _mm256_broadcast_sd(M0 + 674);
  _t19_20 = _mm256_broadcast_sd(M0 + 675);
  _t19_19 = _mm256_broadcast_sd(M0 + 700);
  _t19_18 = _mm256_broadcast_sd(M0 + 701);
  _t19_17 = _mm256_broadcast_sd(M0 + 702);
  _t19_16 = _mm256_broadcast_sd(M0 + 703);
  _t19_15 = _mm256_broadcast_sd(M0 + 728);
  _t19_14 = _mm256_broadcast_sd(M0 + 729);
  _t19_13 = _mm256_broadcast_sd(M0 + 730);
  _t19_12 = _mm256_broadcast_sd(M0 + 731);
  _t19_11 = _mm256_broadcast_sd(M0 + 756);
  _t19_10 = _mm256_broadcast_sd(M0 + 757);
  _t19_9 = _mm256_broadcast_sd(M0 + 758);
  _t19_8 = _mm256_broadcast_sd(M0 + 759);
  _t19_7 = _mm256_loadu_pd(F + 672);
  _t19_6 = _mm256_loadu_pd(F + 700);
  _t19_5 = _mm256_loadu_pd(F + 728);
  _t19_4 = _mm256_loadu_pd(F + 756);
  _t19_3 = _mm256_loadu_pd(Q + 696);
  _t19_2 = _mm256_maskload_pd(Q + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t19_1 = _mm256_maskload_pd(Q + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t19_0 = _mm256_maskload_pd(Q + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t19_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_6), _mm256_unpacklo_pd(_t19_5, _t19_4), 32);
  _t19_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_7, _t19_6), _mm256_unpackhi_pd(_t19_5, _t19_4), 32);
  _t19_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_6), _mm256_unpacklo_pd(_t19_5, _t19_4), 49);
  _t19_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_7, _t19_6), _mm256_unpackhi_pd(_t19_5, _t19_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t19_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_23, _t19_40), _mm256_mul_pd(_t19_22, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_21, _t19_42), _mm256_mul_pd(_t19_20, _t19_43)));
  _t19_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_40), _mm256_mul_pd(_t19_18, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_42), _mm256_mul_pd(_t19_16, _t19_43)));
  _t19_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_40), _mm256_mul_pd(_t19_14, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_42), _mm256_mul_pd(_t19_12, _t19_43)));
  _t19_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_40), _mm256_mul_pd(_t19_10, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_42), _mm256_mul_pd(_t19_8, _t19_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t19_36 = _t19_3;
  _t19_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 3), _t19_2, 12);
  _t19_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 0), _t19_1, 49);
  _t19_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 12), _mm256_shuffle_pd(_t19_1, _t19_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t19_24 = _mm256_add_pd(_t19_32, _t19_36);
  _t19_25 = _mm256_add_pd(_t19_33, _t19_37);
  _t19_26 = _mm256_add_pd(_t19_34, _t19_38);
  _t19_27 = _mm256_add_pd(_t19_35, _t19_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t19_28 = _t19_24;
  _t19_29 = _t19_25;
  _t19_30 = _t19_26;
  _t19_31 = _t19_27;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t20_19 = _mm256_broadcast_sd(M0 + k2 + 672);
    _t20_18 = _mm256_broadcast_sd(M0 + k2 + 673);
    _t20_17 = _mm256_broadcast_sd(M0 + k2 + 674);
    _t20_16 = _mm256_broadcast_sd(M0 + k2 + 675);
    _t20_15 = _mm256_broadcast_sd(M0 + k2 + 700);
    _t20_14 = _mm256_broadcast_sd(M0 + k2 + 701);
    _t20_13 = _mm256_broadcast_sd(M0 + k2 + 702);
    _t20_12 = _mm256_broadcast_sd(M0 + k2 + 703);
    _t20_11 = _mm256_broadcast_sd(M0 + k2 + 728);
    _t20_10 = _mm256_broadcast_sd(M0 + k2 + 729);
    _t20_9 = _mm256_broadcast_sd(M0 + k2 + 730);
    _t20_8 = _mm256_broadcast_sd(M0 + k2 + 731);
    _t20_7 = _mm256_broadcast_sd(M0 + k2 + 756);
    _t20_6 = _mm256_broadcast_sd(M0 + k2 + 757);
    _t20_5 = _mm256_broadcast_sd(M0 + k2 + 758);
    _t20_4 = _mm256_broadcast_sd(M0 + k2 + 759);
    _t20_3 = _mm256_loadu_pd(F + k2 + 672);
    _t20_2 = _mm256_loadu_pd(F + k2 + 700);
    _t20_1 = _mm256_loadu_pd(F + k2 + 728);
    _t20_0 = _mm256_loadu_pd(F + k2 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t20_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 32);
    _t20_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 32);
    _t20_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 49);
    _t20_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t20_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_19, _t20_28), _mm256_mul_pd(_t20_18, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_17, _t20_30), _mm256_mul_pd(_t20_16, _t20_31)));
    _t20_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t20_28), _mm256_mul_pd(_t20_14, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t20_30), _mm256_mul_pd(_t20_12, _t20_31)));
    _t20_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t20_28), _mm256_mul_pd(_t20_10, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t20_30), _mm256_mul_pd(_t20_8, _t20_31)));
    _t20_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t20_28), _mm256_mul_pd(_t20_6, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t20_30), _mm256_mul_pd(_t20_4, _t20_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t20_24 = _t19_28;
    _t20_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
    _t20_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
    _t20_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t20_24 = _mm256_add_pd(_t20_24, _t20_20);
    _t20_25 = _mm256_add_pd(_t20_25, _t20_21);
    _t20_26 = _mm256_add_pd(_t20_26, _t20_22);
    _t20_27 = _mm256_add_pd(_t20_27, _t20_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t19_28 = _t20_24;
    _t19_29 = _t20_25;
    _t19_30 = _t20_26;
    _t19_31 = _t20_27;
  }


  // Generating : v0[24,1] = Sum_{i0} ( ( S(h(4, 24, i0), ( G(h(4, 24, i0), z[24,1],h(1, 1, 0)) - ( G(h(4, 24, i0), H[24,28],h(4, 28, 0)) * G(h(4, 28, 0), y[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k3} ( -$(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k3)) * G(h(4, 28, k3), y[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t21_5 = _mm256_loadu_pd(z + i0);
    _t21_4 = _mm256_loadu_pd(H + 28*i0);
    _t21_3 = _mm256_loadu_pd(H + 28*i0 + 28);
    _t21_2 = _mm256_loadu_pd(H + 28*i0 + 56);
    _t21_1 = _mm256_loadu_pd(H + 28*i0 + 84);
    _t21_0 = _mm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t21_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t21_7 = _mm256_sub_pd(_t21_5, _t21_6);

    // AVX Storer:

    for( int k3 = 4; k3 <= 27; k3+=4 ) {
      _t22_4 = _mm256_loadu_pd(H + 28*i0 + k3);
      _t22_3 = _mm256_loadu_pd(H + 28*i0 + k3 + 28);
      _t22_2 = _mm256_loadu_pd(H + 28*i0 + k3 + 56);
      _t22_1 = _mm256_loadu_pd(H + 28*i0 + k3 + 84);
      _t22_0 = _mm256_loadu_pd(y + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t22_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_4, _t22_0), _mm256_mul_pd(_t22_3, _t22_0)), _mm256_hadd_pd(_mm256_mul_pd(_t22_2, _t22_0), _mm256_mul_pd(_t22_1, _t22_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_4, _t22_0), _mm256_mul_pd(_t22_3, _t22_0)), _mm256_hadd_pd(_mm256_mul_pd(_t22_2, _t22_0), _mm256_mul_pd(_t22_1, _t22_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t21_7 = _mm256_sub_pd(_t21_7, _t22_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(v0 + i0, _t21_7);
  }

  _t23_7 = _mm256_loadu_pd(Y);
  _t23_6 = _mm256_maskload_pd(Y + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t23_5 = _mm256_maskload_pd(Y + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t23_4 = _mm256_maskload_pd(Y + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t23_3 = _mm256_loadu_pd(Y + 116);
  _t23_2 = _mm256_maskload_pd(Y + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t23_1 = _mm256_maskload_pd(Y + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t23_0 = _mm256_maskload_pd(Y + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[24,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k2)) * T( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, 4)) * G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k2)) * T( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{k3} ( ( ( ( S(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) + $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k3)) * G(h(4, 28, k3), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k2)) * T( G(h(4, 28, k3), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 24, i0), ( G(h(4, 24, i0), H[24,28],h(4, 28, 24)) * G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_8 = _t23_7;
  _t23_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 3), _t23_6, 12);
  _t23_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 0), _t23_5, 49);
  _t23_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 12), _mm256_shuffle_pd(_t23_5, _t23_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_12 = _t23_3;
  _t23_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 3), _t23_2, 12);
  _t23_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 0), _t23_1, 49);
  _t23_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 12), _mm256_shuffle_pd(_t23_1, _t23_0, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_16 = _t19_28;
  _t23_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t23_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t23_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);


  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t24_15 = _mm256_broadcast_sd(H + 28*i0);
    _t24_14 = _mm256_broadcast_sd(H + 28*i0 + 1);
    _t24_13 = _mm256_broadcast_sd(H + 28*i0 + 2);
    _t24_12 = _mm256_broadcast_sd(H + 28*i0 + 3);
    _t24_11 = _mm256_broadcast_sd(H + 28*i0 + 28);
    _t24_10 = _mm256_broadcast_sd(H + 28*i0 + 29);
    _t24_9 = _mm256_broadcast_sd(H + 28*i0 + 30);
    _t24_8 = _mm256_broadcast_sd(H + 28*i0 + 31);
    _t24_7 = _mm256_broadcast_sd(H + 28*i0 + 56);
    _t24_6 = _mm256_broadcast_sd(H + 28*i0 + 57);
    _t24_5 = _mm256_broadcast_sd(H + 28*i0 + 58);
    _t24_4 = _mm256_broadcast_sd(H + 28*i0 + 59);
    _t24_3 = _mm256_broadcast_sd(H + 28*i0 + 84);
    _t24_2 = _mm256_broadcast_sd(H + 28*i0 + 85);
    _t24_1 = _mm256_broadcast_sd(H + 28*i0 + 86);
    _t24_0 = _mm256_broadcast_sd(H + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t24_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t23_8), _mm256_mul_pd(_t24_14, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t23_10), _mm256_mul_pd(_t24_12, _t23_11)));
    _t24_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t23_8), _mm256_mul_pd(_t24_10, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t23_10), _mm256_mul_pd(_t24_8, _t23_11)));
    _t24_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t23_8), _mm256_mul_pd(_t24_6, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t23_10), _mm256_mul_pd(_t24_4, _t23_11)));
    _t24_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t23_8), _mm256_mul_pd(_t24_2, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t23_10), _mm256_mul_pd(_t24_0, _t23_11)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t25_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t25_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t25_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t25_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t25_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t25_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t25_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t25_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t25_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t25_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t25_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t25_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t25_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t25_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t25_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t25_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t25_3 = _mm256_loadu_pd(Y + k2);
      _t25_2 = _mm256_loadu_pd(Y + k2 + 28);
      _t25_1 = _mm256_loadu_pd(Y + k2 + 56);
      _t25_0 = _mm256_loadu_pd(Y + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t25_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
      _t25_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
      _t25_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
      _t25_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t25_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_24), _mm256_mul_pd(_t25_18, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_26), _mm256_mul_pd(_t25_16, _t25_27)));
      _t25_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_24), _mm256_mul_pd(_t25_14, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_26), _mm256_mul_pd(_t25_12, _t25_27)));
      _t25_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_24), _mm256_mul_pd(_t25_10, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_26), _mm256_mul_pd(_t25_8, _t25_27)));
      _t25_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_24), _mm256_mul_pd(_t25_6, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_26), _mm256_mul_pd(_t25_4, _t25_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t24_16 = _mm256_add_pd(_t24_16, _t25_20);
      _t24_17 = _mm256_add_pd(_t24_17, _t25_21);
      _t24_18 = _mm256_add_pd(_t24_18, _t25_22);
      _t24_19 = _mm256_add_pd(_t24_19, _t25_23);

      // AVX Storer:
    }
    _t26_19 = _mm256_loadu_pd(Y + 4);
    _t26_18 = _mm256_loadu_pd(Y + 32);
    _t26_17 = _mm256_loadu_pd(Y + 60);
    _t26_16 = _mm256_loadu_pd(Y + 88);
    _t26_15 = _mm256_broadcast_sd(H + 28*i0 + 4);
    _t26_14 = _mm256_broadcast_sd(H + 28*i0 + 5);
    _t26_13 = _mm256_broadcast_sd(H + 28*i0 + 6);
    _t26_12 = _mm256_broadcast_sd(H + 28*i0 + 7);
    _t26_11 = _mm256_broadcast_sd(H + 28*i0 + 32);
    _t26_10 = _mm256_broadcast_sd(H + 28*i0 + 33);
    _t26_9 = _mm256_broadcast_sd(H + 28*i0 + 34);
    _t26_8 = _mm256_broadcast_sd(H + 28*i0 + 35);
    _t26_7 = _mm256_broadcast_sd(H + 28*i0 + 60);
    _t26_6 = _mm256_broadcast_sd(H + 28*i0 + 61);
    _t26_5 = _mm256_broadcast_sd(H + 28*i0 + 62);
    _t26_4 = _mm256_broadcast_sd(H + 28*i0 + 63);
    _t26_3 = _mm256_broadcast_sd(H + 28*i0 + 88);
    _t26_2 = _mm256_broadcast_sd(H + 28*i0 + 89);
    _t26_1 = _mm256_broadcast_sd(H + 28*i0 + 90);
    _t26_0 = _mm256_broadcast_sd(H + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t26_19), _mm256_mul_pd(_t24_14, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t26_17), _mm256_mul_pd(_t24_12, _t26_16)));
    _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t26_19), _mm256_mul_pd(_t24_10, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t26_17), _mm256_mul_pd(_t24_8, _t26_16)));
    _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t26_19), _mm256_mul_pd(_t24_6, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t26_17), _mm256_mul_pd(_t24_4, _t26_16)));
    _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t26_19), _mm256_mul_pd(_t24_2, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t26_17), _mm256_mul_pd(_t24_0, _t26_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t23_12), _mm256_mul_pd(_t26_14, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t23_14), _mm256_mul_pd(_t26_12, _t23_15)));
    _t26_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t23_12), _mm256_mul_pd(_t26_10, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t23_14), _mm256_mul_pd(_t26_8, _t23_15)));
    _t26_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t23_12), _mm256_mul_pd(_t26_6, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t23_14), _mm256_mul_pd(_t26_4, _t23_15)));
    _t26_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t23_12), _mm256_mul_pd(_t26_2, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t23_14), _mm256_mul_pd(_t26_0, _t23_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t26_20 = _mm256_add_pd(_t26_20, _t26_24);
    _t26_21 = _mm256_add_pd(_t26_21, _t26_25);
    _t26_22 = _mm256_add_pd(_t26_22, _t26_26);
    _t26_23 = _mm256_add_pd(_t26_23, _t26_27);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t27_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t27_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t27_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t27_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t27_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t27_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t27_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t27_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t27_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t27_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t27_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t27_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t27_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t27_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t27_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t27_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t27_3 = _mm256_loadu_pd(Y + k2 + 112);
      _t27_2 = _mm256_loadu_pd(Y + k2 + 140);
      _t27_1 = _mm256_loadu_pd(Y + k2 + 168);
      _t27_0 = _mm256_loadu_pd(Y + k2 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t27_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 32);
      _t27_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 32);
      _t27_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 49);
      _t27_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t27_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_19, _t27_24), _mm256_mul_pd(_t27_18, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_17, _t27_26), _mm256_mul_pd(_t27_16, _t27_27)));
      _t27_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_15, _t27_24), _mm256_mul_pd(_t27_14, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_13, _t27_26), _mm256_mul_pd(_t27_12, _t27_27)));
      _t27_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_11, _t27_24), _mm256_mul_pd(_t27_10, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_9, _t27_26), _mm256_mul_pd(_t27_8, _t27_27)));
      _t27_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_7, _t27_24), _mm256_mul_pd(_t27_6, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_5, _t27_26), _mm256_mul_pd(_t27_4, _t27_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_20 = _mm256_add_pd(_t26_20, _t27_20);
      _t26_21 = _mm256_add_pd(_t26_21, _t27_21);
      _t26_22 = _mm256_add_pd(_t26_22, _t27_22);
      _t26_23 = _mm256_add_pd(_t26_23, _t27_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k3 = 8; k3 <= 23; k3+=4 ) {
      _t28_3 = _mm256_loadu_pd(Y + k3);
      _t28_2 = _mm256_loadu_pd(Y + k3 + 28);
      _t28_1 = _mm256_loadu_pd(Y + k3 + 56);
      _t28_0 = _mm256_loadu_pd(Y + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t28_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t28_3), _mm256_mul_pd(_t24_14, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t28_1), _mm256_mul_pd(_t24_12, _t28_0)));
      _t28_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t28_3), _mm256_mul_pd(_t24_10, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t28_1), _mm256_mul_pd(_t24_8, _t28_0)));
      _t28_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t28_3), _mm256_mul_pd(_t24_6, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t28_1), _mm256_mul_pd(_t24_4, _t28_0)));
      _t28_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t28_3), _mm256_mul_pd(_t24_2, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t28_1), _mm256_mul_pd(_t24_0, _t28_0)));

      // AVX Storer:

      for( int k2 = 4; k2 <= k3 - 1; k2+=4 ) {
        _t29_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
        _t29_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
        _t29_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
        _t29_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
        _t29_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
        _t29_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
        _t29_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
        _t29_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
        _t29_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
        _t29_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
        _t29_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
        _t29_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
        _t29_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
        _t29_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
        _t29_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
        _t29_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
        _t29_3 = _mm256_loadu_pd(Y + 28*k2 + k3);
        _t29_2 = _mm256_loadu_pd(Y + 28*k2 + k3 + 28);
        _t29_1 = _mm256_loadu_pd(Y + 28*k2 + k3 + 56);
        _t29_0 = _mm256_loadu_pd(Y + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t29_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_3), _mm256_mul_pd(_t29_18, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_1), _mm256_mul_pd(_t29_16, _t29_0)));
        _t29_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_3), _mm256_mul_pd(_t29_14, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_1), _mm256_mul_pd(_t29_12, _t29_0)));
        _t29_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_3), _mm256_mul_pd(_t29_10, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_1), _mm256_mul_pd(_t29_8, _t29_0)));
        _t29_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_3), _mm256_mul_pd(_t29_6, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_1), _mm256_mul_pd(_t29_4, _t29_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t28_4 = _mm256_add_pd(_t28_4, _t29_20);
        _t28_5 = _mm256_add_pd(_t28_5, _t29_21);
        _t28_6 = _mm256_add_pd(_t28_6, _t29_22);
        _t28_7 = _mm256_add_pd(_t28_7, _t29_23);

        // AVX Storer:
      }
      _t30_19 = _mm256_broadcast_sd(H + 28*i0 + k3);
      _t30_18 = _mm256_broadcast_sd(H + 28*i0 + k3 + 1);
      _t30_17 = _mm256_broadcast_sd(H + 28*i0 + k3 + 2);
      _t30_16 = _mm256_broadcast_sd(H + 28*i0 + k3 + 3);
      _t30_15 = _mm256_broadcast_sd(H + 28*i0 + k3 + 28);
      _t30_14 = _mm256_broadcast_sd(H + 28*i0 + k3 + 29);
      _t30_13 = _mm256_broadcast_sd(H + 28*i0 + k3 + 30);
      _t30_12 = _mm256_broadcast_sd(H + 28*i0 + k3 + 31);
      _t30_11 = _mm256_broadcast_sd(H + 28*i0 + k3 + 56);
      _t30_10 = _mm256_broadcast_sd(H + 28*i0 + k3 + 57);
      _t30_9 = _mm256_broadcast_sd(H + 28*i0 + k3 + 58);
      _t30_8 = _mm256_broadcast_sd(H + 28*i0 + k3 + 59);
      _t30_7 = _mm256_broadcast_sd(H + 28*i0 + k3 + 84);
      _t30_6 = _mm256_broadcast_sd(H + 28*i0 + k3 + 85);
      _t30_5 = _mm256_broadcast_sd(H + 28*i0 + k3 + 86);
      _t30_4 = _mm256_broadcast_sd(H + 28*i0 + k3 + 87);
      _t30_3 = _mm256_loadu_pd(Y + 29*k3);
      _t30_2 = _mm256_maskload_pd(Y + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t30_1 = _mm256_maskload_pd(Y + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t30_0 = _mm256_maskload_pd(Y + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t30_24 = _t30_3;
      _t30_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 3), _t30_2, 12);
      _t30_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 0), _t30_1, 49);
      _t30_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 12), _mm256_shuffle_pd(_t30_1, _t30_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_19, _t30_24), _mm256_mul_pd(_t30_18, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_17, _t30_26), _mm256_mul_pd(_t30_16, _t30_27)));
      _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t30_24), _mm256_mul_pd(_t30_14, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t30_26), _mm256_mul_pd(_t30_12, _t30_27)));
      _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t30_24), _mm256_mul_pd(_t30_10, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t30_26), _mm256_mul_pd(_t30_8, _t30_27)));
      _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t30_24), _mm256_mul_pd(_t30_6, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t30_26), _mm256_mul_pd(_t30_4, _t30_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t28_4 = _mm256_add_pd(_t28_4, _t30_20);
      _t28_5 = _mm256_add_pd(_t28_5, _t30_21);
      _t28_6 = _mm256_add_pd(_t28_6, _t30_22);
      _t28_7 = _mm256_add_pd(_t28_7, _t30_23);

      // AVX Storer:

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t31_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
        _t31_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
        _t31_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
        _t31_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
        _t31_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
        _t31_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
        _t31_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
        _t31_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
        _t31_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
        _t31_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
        _t31_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
        _t31_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
        _t31_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
        _t31_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
        _t31_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
        _t31_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
        _t31_3 = _mm256_loadu_pd(Y + k2 + 28*k3);
        _t31_2 = _mm256_loadu_pd(Y + k2 + 28*k3 + 28);
        _t31_1 = _mm256_loadu_pd(Y + k2 + 28*k3 + 56);
        _t31_0 = _mm256_loadu_pd(Y + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t31_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 32);
        _t31_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 32);
        _t31_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 49);
        _t31_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t31_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t31_24), _mm256_mul_pd(_t31_18, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t31_26), _mm256_mul_pd(_t31_16, _t31_27)));
        _t31_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t31_24), _mm256_mul_pd(_t31_14, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t31_26), _mm256_mul_pd(_t31_12, _t31_27)));
        _t31_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t31_24), _mm256_mul_pd(_t31_10, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t31_26), _mm256_mul_pd(_t31_8, _t31_27)));
        _t31_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t31_24), _mm256_mul_pd(_t31_6, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t31_26), _mm256_mul_pd(_t31_4, _t31_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t28_4 = _mm256_add_pd(_t28_4, _t31_20);
        _t28_5 = _mm256_add_pd(_t28_5, _t31_21);
        _t28_6 = _mm256_add_pd(_t28_6, _t31_22);
        _t28_7 = _mm256_add_pd(_t28_7, _t31_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M1 + 28*i0 + k3, _t28_4);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 28, _t28_5);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 56, _t28_6);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 84, _t28_7);
    }
    _t32_3 = _mm256_loadu_pd(Y + 24);
    _t32_2 = _mm256_loadu_pd(Y + 52);
    _t32_1 = _mm256_loadu_pd(Y + 80);
    _t32_0 = _mm256_loadu_pd(Y + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t32_3), _mm256_mul_pd(_t24_14, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t32_1), _mm256_mul_pd(_t24_12, _t32_0)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t32_3), _mm256_mul_pd(_t24_10, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t32_1), _mm256_mul_pd(_t24_8, _t32_0)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t32_3), _mm256_mul_pd(_t24_6, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t32_1), _mm256_mul_pd(_t24_4, _t32_0)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t32_3), _mm256_mul_pd(_t24_2, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t32_1), _mm256_mul_pd(_t24_0, _t32_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t33_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t33_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t33_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t33_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t33_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t33_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t33_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t33_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t33_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t33_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t33_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t33_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t33_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t33_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t33_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t33_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t33_3 = _mm256_loadu_pd(Y + 28*k2 + 24);
      _t33_2 = _mm256_loadu_pd(Y + 28*k2 + 52);
      _t33_1 = _mm256_loadu_pd(Y + 28*k2 + 80);
      _t33_0 = _mm256_loadu_pd(Y + 28*k2 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t33_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_19, _t33_3), _mm256_mul_pd(_t33_18, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_17, _t33_1), _mm256_mul_pd(_t33_16, _t33_0)));
      _t33_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_15, _t33_3), _mm256_mul_pd(_t33_14, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_13, _t33_1), _mm256_mul_pd(_t33_12, _t33_0)));
      _t33_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_11, _t33_3), _mm256_mul_pd(_t33_10, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_9, _t33_1), _mm256_mul_pd(_t33_8, _t33_0)));
      _t33_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_7, _t33_3), _mm256_mul_pd(_t33_6, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_5, _t33_1), _mm256_mul_pd(_t33_4, _t33_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t32_4 = _mm256_add_pd(_t32_4, _t33_20);
      _t32_5 = _mm256_add_pd(_t32_5, _t33_21);
      _t32_6 = _mm256_add_pd(_t32_6, _t33_22);
      _t32_7 = _mm256_add_pd(_t32_7, _t33_23);

      // AVX Storer:
    }
    _t34_15 = _mm256_broadcast_sd(H + 28*i0 + 24);
    _t34_14 = _mm256_broadcast_sd(H + 28*i0 + 25);
    _t34_13 = _mm256_broadcast_sd(H + 28*i0 + 26);
    _t34_12 = _mm256_broadcast_sd(H + 28*i0 + 27);
    _t34_11 = _mm256_broadcast_sd(H + 28*i0 + 52);
    _t34_10 = _mm256_broadcast_sd(H + 28*i0 + 53);
    _t34_9 = _mm256_broadcast_sd(H + 28*i0 + 54);
    _t34_8 = _mm256_broadcast_sd(H + 28*i0 + 55);
    _t34_7 = _mm256_broadcast_sd(H + 28*i0 + 80);
    _t34_6 = _mm256_broadcast_sd(H + 28*i0 + 81);
    _t34_5 = _mm256_broadcast_sd(H + 28*i0 + 82);
    _t34_4 = _mm256_broadcast_sd(H + 28*i0 + 83);
    _t34_3 = _mm256_broadcast_sd(H + 28*i0 + 108);
    _t34_2 = _mm256_broadcast_sd(H + 28*i0 + 109);
    _t34_1 = _mm256_broadcast_sd(H + 28*i0 + 110);
    _t34_0 = _mm256_broadcast_sd(H + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t34_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_15, _t23_16), _mm256_mul_pd(_t34_14, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_13, _t23_18), _mm256_mul_pd(_t34_12, _t23_19)));
    _t34_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_11, _t23_16), _mm256_mul_pd(_t34_10, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_9, _t23_18), _mm256_mul_pd(_t34_8, _t23_19)));
    _t34_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_7, _t23_16), _mm256_mul_pd(_t34_6, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_5, _t23_18), _mm256_mul_pd(_t34_4, _t23_19)));
    _t34_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_3, _t23_16), _mm256_mul_pd(_t34_2, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_1, _t23_18), _mm256_mul_pd(_t34_0, _t23_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t32_4 = _mm256_add_pd(_t32_4, _t34_16);
    _t32_5 = _mm256_add_pd(_t32_5, _t34_17);
    _t32_6 = _mm256_add_pd(_t32_6, _t34_18);
    _t32_7 = _mm256_add_pd(_t32_7, _t34_19);

    // AVX Storer:
    _mm256_storeu_pd(M1 + 28*i0, _t24_16);
    _mm256_storeu_pd(M1 + 28*i0 + 28, _t24_17);
    _mm256_storeu_pd(M1 + 28*i0 + 56, _t24_18);
    _mm256_storeu_pd(M1 + 28*i0 + 84, _t24_19);
    _mm256_storeu_pd(M1 + 28*i0 + 4, _t26_20);
    _mm256_storeu_pd(M1 + 28*i0 + 32, _t26_21);
    _mm256_storeu_pd(M1 + 28*i0 + 60, _t26_22);
    _mm256_storeu_pd(M1 + 28*i0 + 88, _t26_23);
    _mm256_storeu_pd(M1 + 28*i0 + 24, _t32_4);
    _mm256_storeu_pd(M1 + 28*i0 + 52, _t32_5);
    _mm256_storeu_pd(M1 + 28*i0 + 80, _t32_6);
    _mm256_storeu_pd(M1 + 28*i0 + 108, _t32_7);
  }


  // Generating : M2[28,24] = ( ( ( Sum_{k3} ( ( S(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 0)) ) ),h(4, 24, k3)) + Sum_{k2} ( $(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, k2)) ) ),h(4, 24, k3)) ) ) ) + Sum_{k3} ( ( ( S(h(4, 28, 4), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 0)) ) ),h(4, 24, k3)) + $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 4)) ) ),h(4, 24, k3)) ) + Sum_{k2} ( $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, k2)) ) ),h(4, 24, k3)) ) ) ) ) + Sum_{i0} ( Sum_{k3} ( ( ( ( S(h(4, 28, i0), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 0)) ) ),h(4, 24, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 24, k3), H[24,28],h(4, 28, k2)) ) ),h(4, 24, k3)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, i0)) ) ),h(4, 24, k3)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, k2)) ) ),h(4, 24, k3)) ) ) ) ) ) + Sum_{k3} ( ( ( S(h(4, 28, 24), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 0)) ) ),h(4, 24, k3)) + Sum_{k2} ( $(h(4, 28, 24), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 24, k3), H[24,28],h(4, 28, k2)) ) ),h(4, 24, k3)) ) ) + $(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 24)) ) ),h(4, 24, k3)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t35_0 = _t23_7;
  _t35_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 3), _t23_6, 12);
  _t35_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 0), _t23_5, 49);
  _t35_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 12), _mm256_shuffle_pd(_t23_5, _t23_4, 12), 49);


  for( int k3 = 0; k3 <= 23; k3+=4 ) {
    _t36_19 = _mm256_loadu_pd(H + 28*k3);
    _t36_18 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t36_17 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t36_16 = _mm256_loadu_pd(H + 28*k3 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t36_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_19, _t36_18), _mm256_unpacklo_pd(_t36_17, _t36_16), 32);
    _t36_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_19, _t36_18), _mm256_unpackhi_pd(_t36_17, _t36_16), 32);
    _t36_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_19, _t36_18), _mm256_unpacklo_pd(_t36_17, _t36_16), 49);
    _t36_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_19, _t36_18), _mm256_unpackhi_pd(_t36_17, _t36_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t36_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_15, _t36_24), _mm256_mul_pd(_t36_14, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_13, _t36_26), _mm256_mul_pd(_t36_12, _t36_27)));
    _t36_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_11, _t36_24), _mm256_mul_pd(_t36_10, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_9, _t36_26), _mm256_mul_pd(_t36_8, _t36_27)));
    _t36_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_7, _t36_24), _mm256_mul_pd(_t36_6, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_5, _t36_26), _mm256_mul_pd(_t36_4, _t36_27)));
    _t36_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_3, _t36_24), _mm256_mul_pd(_t36_2, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_1, _t36_26), _mm256_mul_pd(_t36_0, _t36_27)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t37_19 = _mm256_broadcast_sd(Y + k2);
      _t37_18 = _mm256_broadcast_sd(Y + k2 + 1);
      _t37_17 = _mm256_broadcast_sd(Y + k2 + 2);
      _t37_16 = _mm256_broadcast_sd(Y + k2 + 3);
      _t37_15 = _mm256_broadcast_sd(Y + k2 + 28);
      _t37_14 = _mm256_broadcast_sd(Y + k2 + 29);
      _t37_13 = _mm256_broadcast_sd(Y + k2 + 30);
      _t37_12 = _mm256_broadcast_sd(Y + k2 + 31);
      _t37_11 = _mm256_broadcast_sd(Y + k2 + 56);
      _t37_10 = _mm256_broadcast_sd(Y + k2 + 57);
      _t37_9 = _mm256_broadcast_sd(Y + k2 + 58);
      _t37_8 = _mm256_broadcast_sd(Y + k2 + 59);
      _t37_7 = _mm256_broadcast_sd(Y + k2 + 84);
      _t37_6 = _mm256_broadcast_sd(Y + k2 + 85);
      _t37_5 = _mm256_broadcast_sd(Y + k2 + 86);
      _t37_4 = _mm256_broadcast_sd(Y + k2 + 87);
      _t37_3 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t37_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t37_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t37_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_19, _t37_24), _mm256_mul_pd(_t37_18, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_17, _t37_26), _mm256_mul_pd(_t37_16, _t37_27)));
      _t37_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_15, _t37_24), _mm256_mul_pd(_t37_14, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_13, _t37_26), _mm256_mul_pd(_t37_12, _t37_27)));
      _t37_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_11, _t37_24), _mm256_mul_pd(_t37_10, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_9, _t37_26), _mm256_mul_pd(_t37_8, _t37_27)));
      _t37_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_7, _t37_24), _mm256_mul_pd(_t37_6, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_5, _t37_26), _mm256_mul_pd(_t37_4, _t37_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t36_20 = _mm256_add_pd(_t36_20, _t37_20);
      _t36_21 = _mm256_add_pd(_t36_21, _t37_21);
      _t36_22 = _mm256_add_pd(_t36_22, _t37_22);
      _t36_23 = _mm256_add_pd(_t36_23, _t37_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + k3, _t36_20);
    _mm256_storeu_pd(M2 + k3 + 24, _t36_21);
    _mm256_storeu_pd(M2 + k3 + 48, _t36_22);
    _mm256_storeu_pd(M2 + k3 + 72, _t36_23);
  }

  _t38_3 = _mm256_loadu_pd(Y + 4);
  _t38_2 = _mm256_loadu_pd(Y + 32);
  _t38_1 = _mm256_loadu_pd(Y + 60);
  _t38_0 = _mm256_loadu_pd(Y + 88);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t38_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
  _t38_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
  _t38_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
  _t38_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t38_4 = _t23_3;
  _t38_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 3), _t23_2, 12);
  _t38_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 0), _t23_1, 49);
  _t38_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 12), _mm256_shuffle_pd(_t23_1, _t23_0, 12), 49);


  for( int k3 = 0; k3 <= 23; k3+=4 ) {
    _t39_39 = _mm256_loadu_pd(H + 28*k3);
    _t39_38 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t39_37 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t39_36 = _mm256_loadu_pd(H + 28*k3 + 84);
    _t39_35 = _mm256_loadu_pd(H + 28*k3 + 4);
    _t39_34 = _mm256_loadu_pd(H + 28*k3 + 32);
    _t39_33 = _mm256_loadu_pd(H + 28*k3 + 60);
    _t39_32 = _mm256_loadu_pd(H + 28*k3 + 88);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_39, _t39_38), _mm256_unpacklo_pd(_t39_37, _t39_36), 32);
    _t39_49 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_39, _t39_38), _mm256_unpackhi_pd(_t39_37, _t39_36), 32);
    _t39_50 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_39, _t39_38), _mm256_unpacklo_pd(_t39_37, _t39_36), 49);
    _t39_51 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_39, _t39_38), _mm256_unpackhi_pd(_t39_37, _t39_36), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_48), _mm256_mul_pd(_t39_14, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_50), _mm256_mul_pd(_t39_12, _t39_51)));
    _t39_41 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_48), _mm256_mul_pd(_t39_10, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_50), _mm256_mul_pd(_t39_8, _t39_51)));
    _t39_42 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_48), _mm256_mul_pd(_t39_6, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_50), _mm256_mul_pd(_t39_4, _t39_51)));
    _t39_43 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_3, _t39_48), _mm256_mul_pd(_t39_2, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_1, _t39_50), _mm256_mul_pd(_t39_0, _t39_51)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_52 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_35, _t39_34), _mm256_unpacklo_pd(_t39_33, _t39_32), 32);
    _t39_53 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_35, _t39_34), _mm256_unpackhi_pd(_t39_33, _t39_32), 32);
    _t39_54 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_35, _t39_34), _mm256_unpacklo_pd(_t39_33, _t39_32), 49);
    _t39_55 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_35, _t39_34), _mm256_unpackhi_pd(_t39_33, _t39_32), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_31, _t39_52), _mm256_mul_pd(_t39_30, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_29, _t39_54), _mm256_mul_pd(_t39_28, _t39_55)));
    _t39_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_27, _t39_52), _mm256_mul_pd(_t39_26, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_25, _t39_54), _mm256_mul_pd(_t39_24, _t39_55)));
    _t39_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_23, _t39_52), _mm256_mul_pd(_t39_22, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_21, _t39_54), _mm256_mul_pd(_t39_20, _t39_55)));
    _t39_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_52), _mm256_mul_pd(_t39_18, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_54), _mm256_mul_pd(_t39_16, _t39_55)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t39_40 = _mm256_add_pd(_t39_40, _t39_44);
    _t39_41 = _mm256_add_pd(_t39_41, _t39_45);
    _t39_42 = _mm256_add_pd(_t39_42, _t39_46);
    _t39_43 = _mm256_add_pd(_t39_43, _t39_47);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t40_19 = _mm256_broadcast_sd(Y + k2 + 112);
      _t40_18 = _mm256_broadcast_sd(Y + k2 + 113);
      _t40_17 = _mm256_broadcast_sd(Y + k2 + 114);
      _t40_16 = _mm256_broadcast_sd(Y + k2 + 115);
      _t40_15 = _mm256_broadcast_sd(Y + k2 + 140);
      _t40_14 = _mm256_broadcast_sd(Y + k2 + 141);
      _t40_13 = _mm256_broadcast_sd(Y + k2 + 142);
      _t40_12 = _mm256_broadcast_sd(Y + k2 + 143);
      _t40_11 = _mm256_broadcast_sd(Y + k2 + 168);
      _t40_10 = _mm256_broadcast_sd(Y + k2 + 169);
      _t40_9 = _mm256_broadcast_sd(Y + k2 + 170);
      _t40_8 = _mm256_broadcast_sd(Y + k2 + 171);
      _t40_7 = _mm256_broadcast_sd(Y + k2 + 196);
      _t40_6 = _mm256_broadcast_sd(Y + k2 + 197);
      _t40_5 = _mm256_broadcast_sd(Y + k2 + 198);
      _t40_4 = _mm256_broadcast_sd(Y + k2 + 199);
      _t40_3 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t40_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t40_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t40_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t40_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 32);
      _t40_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 32);
      _t40_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 49);
      _t40_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t40_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_19, _t40_24), _mm256_mul_pd(_t40_18, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_17, _t40_26), _mm256_mul_pd(_t40_16, _t40_27)));
      _t40_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_15, _t40_24), _mm256_mul_pd(_t40_14, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_13, _t40_26), _mm256_mul_pd(_t40_12, _t40_27)));
      _t40_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_11, _t40_24), _mm256_mul_pd(_t40_10, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_9, _t40_26), _mm256_mul_pd(_t40_8, _t40_27)));
      _t40_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_7, _t40_24), _mm256_mul_pd(_t40_6, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_5, _t40_26), _mm256_mul_pd(_t40_4, _t40_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_40 = _mm256_add_pd(_t39_40, _t40_20);
      _t39_41 = _mm256_add_pd(_t39_41, _t40_21);
      _t39_42 = _mm256_add_pd(_t39_42, _t40_22);
      _t39_43 = _mm256_add_pd(_t39_43, _t40_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + k3 + 96, _t39_40);
    _mm256_storeu_pd(M2 + k3 + 120, _t39_41);
    _mm256_storeu_pd(M2 + k3 + 144, _t39_42);
    _mm256_storeu_pd(M2 + k3 + 168, _t39_43);
  }


  for( int i0 = 8; i0 <= 23; i0+=4 ) {
    _t41_7 = _mm256_loadu_pd(Y + i0);
    _t41_6 = _mm256_loadu_pd(Y + i0 + 28);
    _t41_5 = _mm256_loadu_pd(Y + i0 + 56);
    _t41_4 = _mm256_loadu_pd(Y + i0 + 84);
    _t41_3 = _mm256_loadu_pd(Y + 29*i0);
    _t41_2 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t41_1 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t41_0 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t41_8 = _t41_3;
    _t41_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
    _t41_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
    _t41_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

    for( int k3 = 0; k3 <= 23; k3+=4 ) {
      _t42_19 = _mm256_loadu_pd(H + 28*k3);
      _t42_18 = _mm256_loadu_pd(H + 28*k3 + 28);
      _t42_17 = _mm256_loadu_pd(H + 28*k3 + 56);
      _t42_16 = _mm256_loadu_pd(H + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
      _t42_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
      _t42_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
      _t42_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_19, _t42_18), _mm256_unpacklo_pd(_t42_17, _t42_16), 32);
      _t42_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_19, _t42_18), _mm256_unpackhi_pd(_t42_17, _t42_16), 32);
      _t42_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_19, _t42_18), _mm256_unpacklo_pd(_t42_17, _t42_16), 49);
      _t42_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_19, _t42_18), _mm256_unpackhi_pd(_t42_17, _t42_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t42_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_28), _mm256_mul_pd(_t42_14, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_30), _mm256_mul_pd(_t42_12, _t42_31)));
      _t42_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_28), _mm256_mul_pd(_t42_10, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_30), _mm256_mul_pd(_t42_8, _t42_31)));
      _t42_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_7, _t42_28), _mm256_mul_pd(_t42_6, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_5, _t42_30), _mm256_mul_pd(_t42_4, _t42_31)));
      _t42_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_3, _t42_28), _mm256_mul_pd(_t42_2, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_1, _t42_30), _mm256_mul_pd(_t42_0, _t42_31)));

      // AVX Storer:

      for( int k2 = 4; k2 <= i0 - 1; k2+=4 ) {
        _t43_23 = _mm256_loadu_pd(Y + i0 + 28*k2);
        _t43_22 = _mm256_loadu_pd(Y + i0 + 28*k2 + 28);
        _t43_21 = _mm256_loadu_pd(Y + i0 + 28*k2 + 56);
        _t43_20 = _mm256_loadu_pd(Y + i0 + 28*k2 + 84);
        _t43_19 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t43_18 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t43_17 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t43_16 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_23, _t43_22), _mm256_unpacklo_pd(_t43_21, _t43_20), 32);
        _t43_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_23, _t43_22), _mm256_unpackhi_pd(_t43_21, _t43_20), 32);
        _t43_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_23, _t43_22), _mm256_unpacklo_pd(_t43_21, _t43_20), 49);
        _t43_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_23, _t43_22), _mm256_unpackhi_pd(_t43_21, _t43_20), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_19, _t43_18), _mm256_unpacklo_pd(_t43_17, _t43_16), 32);
        _t43_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_19, _t43_18), _mm256_unpackhi_pd(_t43_17, _t43_16), 32);
        _t43_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_19, _t43_18), _mm256_unpacklo_pd(_t43_17, _t43_16), 49);
        _t43_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_19, _t43_18), _mm256_unpackhi_pd(_t43_17, _t43_16), 49);

        // 4-BLAC: 4x4 * 4x4
        _t43_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_15, _t43_32), _mm256_mul_pd(_t43_14, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_13, _t43_34), _mm256_mul_pd(_t43_12, _t43_35)));
        _t43_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_11, _t43_32), _mm256_mul_pd(_t43_10, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_9, _t43_34), _mm256_mul_pd(_t43_8, _t43_35)));
        _t43_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_7, _t43_32), _mm256_mul_pd(_t43_6, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_5, _t43_34), _mm256_mul_pd(_t43_4, _t43_35)));
        _t43_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_3, _t43_32), _mm256_mul_pd(_t43_2, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_1, _t43_34), _mm256_mul_pd(_t43_0, _t43_35)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t42_20 = _mm256_add_pd(_t42_20, _t43_24);
        _t42_21 = _mm256_add_pd(_t42_21, _t43_25);
        _t42_22 = _mm256_add_pd(_t42_22, _t43_26);
        _t42_23 = _mm256_add_pd(_t42_23, _t43_27);

        // AVX Storer:
      }
      _t44_19 = _mm256_loadu_pd(H + i0 + 28*k3);
      _t44_18 = _mm256_loadu_pd(H + i0 + 28*k3 + 28);
      _t44_17 = _mm256_loadu_pd(H + i0 + 28*k3 + 56);
      _t44_16 = _mm256_loadu_pd(H + i0 + 28*k3 + 84);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t44_24 = _t41_3;
      _t44_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
      _t44_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
      _t44_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t44_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_19, _t44_18), _mm256_unpacklo_pd(_t44_17, _t44_16), 32);
      _t44_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_19, _t44_18), _mm256_unpackhi_pd(_t44_17, _t44_16), 32);
      _t44_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_19, _t44_18), _mm256_unpacklo_pd(_t44_17, _t44_16), 49);
      _t44_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_19, _t44_18), _mm256_unpackhi_pd(_t44_17, _t44_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t44_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_28), _mm256_mul_pd(_t44_14, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_30), _mm256_mul_pd(_t44_12, _t44_31)));
      _t44_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_28), _mm256_mul_pd(_t44_10, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_30), _mm256_mul_pd(_t44_8, _t44_31)));
      _t44_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_7, _t44_28), _mm256_mul_pd(_t44_6, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_5, _t44_30), _mm256_mul_pd(_t44_4, _t44_31)));
      _t44_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_3, _t44_28), _mm256_mul_pd(_t44_2, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_1, _t44_30), _mm256_mul_pd(_t44_0, _t44_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t42_20 = _mm256_add_pd(_t42_20, _t44_20);
      _t42_21 = _mm256_add_pd(_t42_21, _t44_21);
      _t42_22 = _mm256_add_pd(_t42_22, _t44_22);
      _t42_23 = _mm256_add_pd(_t42_23, _t44_23);

      // AVX Storer:

      for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t45_19 = _mm256_broadcast_sd(Y + 28*i0 + k2);
        _t45_18 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 1);
        _t45_17 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 2);
        _t45_16 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 3);
        _t45_15 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 28);
        _t45_14 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 29);
        _t45_13 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 30);
        _t45_12 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 31);
        _t45_11 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 56);
        _t45_10 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 57);
        _t45_9 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 58);
        _t45_8 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 59);
        _t45_7 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 84);
        _t45_6 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 85);
        _t45_5 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 86);
        _t45_4 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 87);
        _t45_3 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t45_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t45_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t45_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t45_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
        _t45_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
        _t45_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
        _t45_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t45_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_24), _mm256_mul_pd(_t45_18, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_26), _mm256_mul_pd(_t45_16, _t45_27)));
        _t45_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_24), _mm256_mul_pd(_t45_14, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_26), _mm256_mul_pd(_t45_12, _t45_27)));
        _t45_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_24), _mm256_mul_pd(_t45_10, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_26), _mm256_mul_pd(_t45_8, _t45_27)));
        _t45_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_24), _mm256_mul_pd(_t45_6, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_26), _mm256_mul_pd(_t45_4, _t45_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t42_20 = _mm256_add_pd(_t42_20, _t45_20);
        _t42_21 = _mm256_add_pd(_t42_21, _t45_21);
        _t42_22 = _mm256_add_pd(_t42_22, _t45_22);
        _t42_23 = _mm256_add_pd(_t42_23, _t45_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M2 + 24*i0 + k3, _t42_20);
      _mm256_storeu_pd(M2 + 24*i0 + k3 + 24, _t42_21);
      _mm256_storeu_pd(M2 + 24*i0 + k3 + 48, _t42_22);
      _mm256_storeu_pd(M2 + 24*i0 + k3 + 72, _t42_23);
    }
  }

  _t46_3 = _mm256_loadu_pd(Y + 24);
  _t46_2 = _mm256_loadu_pd(Y + 52);
  _t46_1 = _mm256_loadu_pd(Y + 80);
  _t46_0 = _mm256_loadu_pd(Y + 108);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t46_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
  _t46_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
  _t46_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
  _t46_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t46_4 = _t19_28;
  _t46_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t46_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t46_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);


  for( int k3 = 0; k3 <= 23; k3+=4 ) {
    _t47_19 = _mm256_loadu_pd(H + 28*k3);
    _t47_18 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t47_17 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t47_16 = _mm256_loadu_pd(H + 28*k3 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_19, _t47_18), _mm256_unpacklo_pd(_t47_17, _t47_16), 32);
    _t47_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_19, _t47_18), _mm256_unpackhi_pd(_t47_17, _t47_16), 32);
    _t47_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_19, _t47_18), _mm256_unpacklo_pd(_t47_17, _t47_16), 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_19, _t47_18), _mm256_unpackhi_pd(_t47_17, _t47_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_24), _mm256_mul_pd(_t47_14, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_26), _mm256_mul_pd(_t47_12, _t47_27)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_24), _mm256_mul_pd(_t47_10, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_26), _mm256_mul_pd(_t47_8, _t47_27)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_24), _mm256_mul_pd(_t47_6, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_26), _mm256_mul_pd(_t47_4, _t47_27)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_3, _t47_24), _mm256_mul_pd(_t47_2, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_1, _t47_26), _mm256_mul_pd(_t47_0, _t47_27)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t48_23 = _mm256_loadu_pd(Y + 28*k2 + 24);
      _t48_22 = _mm256_loadu_pd(Y + 28*k2 + 52);
      _t48_21 = _mm256_loadu_pd(Y + 28*k2 + 80);
      _t48_20 = _mm256_loadu_pd(Y + 28*k2 + 108);
      _t48_19 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t48_18 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t48_17 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t48_16 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_23, _t48_22), _mm256_unpacklo_pd(_t48_21, _t48_20), 32);
      _t48_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_23, _t48_22), _mm256_unpackhi_pd(_t48_21, _t48_20), 32);
      _t48_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_23, _t48_22), _mm256_unpacklo_pd(_t48_21, _t48_20), 49);
      _t48_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_23, _t48_22), _mm256_unpackhi_pd(_t48_21, _t48_20), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_19, _t48_18), _mm256_unpacklo_pd(_t48_17, _t48_16), 32);
      _t48_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_19, _t48_18), _mm256_unpackhi_pd(_t48_17, _t48_16), 32);
      _t48_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_19, _t48_18), _mm256_unpacklo_pd(_t48_17, _t48_16), 49);
      _t48_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_19, _t48_18), _mm256_unpackhi_pd(_t48_17, _t48_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t48_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_15, _t48_32), _mm256_mul_pd(_t48_14, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_13, _t48_34), _mm256_mul_pd(_t48_12, _t48_35)));
      _t48_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_11, _t48_32), _mm256_mul_pd(_t48_10, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_9, _t48_34), _mm256_mul_pd(_t48_8, _t48_35)));
      _t48_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_7, _t48_32), _mm256_mul_pd(_t48_6, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_5, _t48_34), _mm256_mul_pd(_t48_4, _t48_35)));
      _t48_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_3, _t48_32), _mm256_mul_pd(_t48_2, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_1, _t48_34), _mm256_mul_pd(_t48_0, _t48_35)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t47_20 = _mm256_add_pd(_t47_20, _t48_24);
      _t47_21 = _mm256_add_pd(_t47_21, _t48_25);
      _t47_22 = _mm256_add_pd(_t47_22, _t48_26);
      _t47_23 = _mm256_add_pd(_t47_23, _t48_27);

      // AVX Storer:
    }
    _t49_19 = _mm256_loadu_pd(H + 28*k3 + 24);
    _t49_18 = _mm256_loadu_pd(H + 28*k3 + 52);
    _t49_17 = _mm256_loadu_pd(H + 28*k3 + 80);
    _t49_16 = _mm256_loadu_pd(H + 28*k3 + 108);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t49_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t49_19, _t49_18), _mm256_unpacklo_pd(_t49_17, _t49_16), 32);
    _t49_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t49_19, _t49_18), _mm256_unpackhi_pd(_t49_17, _t49_16), 32);
    _t49_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t49_19, _t49_18), _mm256_unpacklo_pd(_t49_17, _t49_16), 49);
    _t49_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t49_19, _t49_18), _mm256_unpackhi_pd(_t49_17, _t49_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t49_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_15, _t49_24), _mm256_mul_pd(_t49_14, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_13, _t49_26), _mm256_mul_pd(_t49_12, _t49_27)));
    _t49_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_11, _t49_24), _mm256_mul_pd(_t49_10, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_9, _t49_26), _mm256_mul_pd(_t49_8, _t49_27)));
    _t49_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_7, _t49_24), _mm256_mul_pd(_t49_6, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_5, _t49_26), _mm256_mul_pd(_t49_4, _t49_27)));
    _t49_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_3, _t49_24), _mm256_mul_pd(_t49_2, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_1, _t49_26), _mm256_mul_pd(_t49_0, _t49_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t47_20 = _mm256_add_pd(_t47_20, _t49_20);
    _t47_21 = _mm256_add_pd(_t47_21, _t49_21);
    _t47_22 = _mm256_add_pd(_t47_22, _t49_22);
    _t47_23 = _mm256_add_pd(_t47_23, _t49_23);

    // AVX Storer:
    _mm256_storeu_pd(M2 + k3 + 576, _t47_20);
    _mm256_storeu_pd(M2 + k3 + 600, _t47_21);
    _mm256_storeu_pd(M2 + k3 + 624, _t47_22);
    _mm256_storeu_pd(M2 + k3 + 648, _t47_23);
  }


  // Generating : M3[24,24] = ( ( Sum_{i0} ( ( ( S(h(4, 24, i0), ( ( G(h(4, 24, i0), M1[24,28],h(4, 28, 0)) * T( G(h(4, 24, i0), H[24,28],h(4, 28, 0)) ) ) + G(h(4, 24, i0), R[24,24],h(4, 24, i0)) ),h(4, 24, i0)) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), M1[24,28],h(4, 28, k2)) * T( G(h(4, 24, i0), H[24,28],h(4, 28, k2)) ) ),h(4, 24, i0)) ) ) + Sum_{k3} ( ( S(h(4, 24, i0), ( ( G(h(4, 24, i0), M1[24,28],h(4, 28, 0)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, 0)) ) ) + G(h(4, 24, i0), R[24,24],h(4, 24, k3)) ),h(4, 24, k3)) + Sum_{k2} ( $(h(4, 24, i0), ( G(h(4, 24, i0), M1[24,28],h(4, 28, k2)) * T( G(h(4, 24, k3), H[24,28],h(4, 28, k2)) ) ),h(4, 24, k3)) ) ) ) ) ) + S(h(4, 24, 20), ( ( G(h(4, 24, 20), M1[24,28],h(4, 28, 0)) * T( G(h(4, 24, 20), H[24,28],h(4, 28, 0)) ) ) + G(h(4, 24, 20), R[24,24],h(4, 24, 20)) ),h(4, 24, 20)) ) + Sum_{k2} ( $(h(4, 24, 20), ( G(h(4, 24, 20), M1[24,28],h(4, 28, k2)) * T( G(h(4, 24, 20), H[24,28],h(4, 28, k2)) ) ),h(4, 24, 20)) ) )


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t50_23 = _mm256_broadcast_sd(M1 + 28*i0);
    _t50_22 = _mm256_broadcast_sd(M1 + 28*i0 + 1);
    _t50_21 = _mm256_broadcast_sd(M1 + 28*i0 + 2);
    _t50_20 = _mm256_broadcast_sd(M1 + 28*i0 + 3);
    _t50_19 = _mm256_broadcast_sd(M1 + 28*i0 + 28);
    _t50_18 = _mm256_broadcast_sd(M1 + 28*i0 + 29);
    _t50_17 = _mm256_broadcast_sd(M1 + 28*i0 + 30);
    _t50_16 = _mm256_broadcast_sd(M1 + 28*i0 + 31);
    _t50_15 = _mm256_broadcast_sd(M1 + 28*i0 + 56);
    _t50_14 = _mm256_broadcast_sd(M1 + 28*i0 + 57);
    _t50_13 = _mm256_broadcast_sd(M1 + 28*i0 + 58);
    _t50_12 = _mm256_broadcast_sd(M1 + 28*i0 + 59);
    _t50_11 = _mm256_broadcast_sd(M1 + 28*i0 + 84);
    _t50_10 = _mm256_broadcast_sd(M1 + 28*i0 + 85);
    _t50_9 = _mm256_broadcast_sd(M1 + 28*i0 + 86);
    _t50_8 = _mm256_broadcast_sd(M1 + 28*i0 + 87);
    _t50_7 = _mm256_loadu_pd(H + 28*i0);
    _t50_6 = _mm256_loadu_pd(H + 28*i0 + 28);
    _t50_5 = _mm256_loadu_pd(H + 28*i0 + 56);
    _t50_4 = _mm256_loadu_pd(H + 28*i0 + 84);
    _t50_3 = _mm256_loadu_pd(R + 25*i0);
    _t50_2 = _mm256_maskload_pd(R + 25*i0 + 24, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t50_1 = _mm256_maskload_pd(R + 25*i0 + 48, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t50_0 = _mm256_maskload_pd(R + 25*i0 + 72, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t50_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 32);
    _t50_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 32);
    _t50_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 49);
    _t50_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t50_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_23, _t50_40), _mm256_mul_pd(_t50_22, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_21, _t50_42), _mm256_mul_pd(_t50_20, _t50_43)));
    _t50_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t50_40), _mm256_mul_pd(_t50_18, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t50_42), _mm256_mul_pd(_t50_16, _t50_43)));
    _t50_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t50_40), _mm256_mul_pd(_t50_14, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t50_42), _mm256_mul_pd(_t50_12, _t50_43)));
    _t50_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t50_40), _mm256_mul_pd(_t50_10, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t50_42), _mm256_mul_pd(_t50_8, _t50_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t50_36 = _t50_3;
    _t50_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 3), _t50_2, 12);
    _t50_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 0), _t50_1, 49);
    _t50_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 12), _mm256_shuffle_pd(_t50_1, _t50_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t50_24 = _mm256_add_pd(_t50_32, _t50_36);
    _t50_25 = _mm256_add_pd(_t50_33, _t50_37);
    _t50_26 = _mm256_add_pd(_t50_34, _t50_38);
    _t50_27 = _mm256_add_pd(_t50_35, _t50_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t50_28 = _t50_24;
    _t50_29 = _t50_25;
    _t50_30 = _t50_26;
    _t50_31 = _t50_27;

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t51_19 = _mm256_broadcast_sd(M1 + 28*i0 + k2);
      _t51_18 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 1);
      _t51_17 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 2);
      _t51_16 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 3);
      _t51_15 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 28);
      _t51_14 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 29);
      _t51_13 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 30);
      _t51_12 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 31);
      _t51_11 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 56);
      _t51_10 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 57);
      _t51_9 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 58);
      _t51_8 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 59);
      _t51_7 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 84);
      _t51_6 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 85);
      _t51_5 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 86);
      _t51_4 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 87);
      _t51_3 = _mm256_loadu_pd(H + 28*i0 + k2);
      _t51_2 = _mm256_loadu_pd(H + 28*i0 + k2 + 28);
      _t51_1 = _mm256_loadu_pd(H + 28*i0 + k2 + 56);
      _t51_0 = _mm256_loadu_pd(H + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t51_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 32);
      _t51_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 32);
      _t51_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 49);
      _t51_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t51_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_19, _t51_28), _mm256_mul_pd(_t51_18, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_17, _t51_30), _mm256_mul_pd(_t51_16, _t51_31)));
      _t51_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_15, _t51_28), _mm256_mul_pd(_t51_14, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_13, _t51_30), _mm256_mul_pd(_t51_12, _t51_31)));
      _t51_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_11, _t51_28), _mm256_mul_pd(_t51_10, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_9, _t51_30), _mm256_mul_pd(_t51_8, _t51_31)));
      _t51_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_7, _t51_28), _mm256_mul_pd(_t51_6, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_5, _t51_30), _mm256_mul_pd(_t51_4, _t51_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t51_24 = _t50_28;
      _t51_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 3), _t50_29, 12);
      _t51_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 0), _t50_30, 49);
      _t51_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 12), _mm256_shuffle_pd(_t50_30, _t50_31, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t51_24 = _mm256_add_pd(_t51_24, _t51_20);
      _t51_25 = _mm256_add_pd(_t51_25, _t51_21);
      _t51_26 = _mm256_add_pd(_t51_26, _t51_22);
      _t51_27 = _mm256_add_pd(_t51_27, _t51_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t50_28 = _t51_24;
      _t50_29 = _t51_25;
      _t50_30 = _t51_26;
      _t50_31 = _t51_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 23; k3+=4 ) {
      _t52_7 = _mm256_loadu_pd(H + 28*k3);
      _t52_6 = _mm256_loadu_pd(H + 28*k3 + 28);
      _t52_5 = _mm256_loadu_pd(H + 28*k3 + 56);
      _t52_4 = _mm256_loadu_pd(H + 28*k3 + 84);
      _t52_3 = _mm256_loadu_pd(R + 24*i0 + k3);
      _t52_2 = _mm256_loadu_pd(R + 24*i0 + k3 + 24);
      _t52_1 = _mm256_loadu_pd(R + 24*i0 + k3 + 48);
      _t52_0 = _mm256_loadu_pd(R + 24*i0 + k3 + 72);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t52_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 32);
      _t52_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 32);
      _t52_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 49);
      _t52_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t52_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_23, _t52_16), _mm256_mul_pd(_t50_22, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_21, _t52_18), _mm256_mul_pd(_t50_20, _t52_19)));
      _t52_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t52_16), _mm256_mul_pd(_t50_18, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t52_18), _mm256_mul_pd(_t50_16, _t52_19)));
      _t52_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t52_16), _mm256_mul_pd(_t50_14, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t52_18), _mm256_mul_pd(_t50_12, _t52_19)));
      _t52_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t52_16), _mm256_mul_pd(_t50_10, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t52_18), _mm256_mul_pd(_t50_8, _t52_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t52_8 = _mm256_add_pd(_t52_12, _t52_3);
      _t52_9 = _mm256_add_pd(_t52_13, _t52_2);
      _t52_10 = _mm256_add_pd(_t52_14, _t52_1);
      _t52_11 = _mm256_add_pd(_t52_15, _t52_0);

      // AVX Storer:

      for( int k2 = 4; k2 <= 27; k2+=4 ) {
        _t53_19 = _mm256_broadcast_sd(M1 + 28*i0 + k2);
        _t53_18 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 1);
        _t53_17 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 2);
        _t53_16 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 3);
        _t53_15 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 28);
        _t53_14 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 29);
        _t53_13 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 30);
        _t53_12 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 31);
        _t53_11 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 56);
        _t53_10 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 57);
        _t53_9 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 58);
        _t53_8 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 59);
        _t53_7 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 84);
        _t53_6 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 85);
        _t53_5 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 86);
        _t53_4 = _mm256_broadcast_sd(M1 + 28*i0 + k2 + 87);
        _t53_3 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t53_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t53_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t53_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t53_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_3, _t53_2), _mm256_unpacklo_pd(_t53_1, _t53_0), 32);
        _t53_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_3, _t53_2), _mm256_unpackhi_pd(_t53_1, _t53_0), 32);
        _t53_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_3, _t53_2), _mm256_unpacklo_pd(_t53_1, _t53_0), 49);
        _t53_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_3, _t53_2), _mm256_unpackhi_pd(_t53_1, _t53_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t53_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_19, _t53_24), _mm256_mul_pd(_t53_18, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_17, _t53_26), _mm256_mul_pd(_t53_16, _t53_27)));
        _t53_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_15, _t53_24), _mm256_mul_pd(_t53_14, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_13, _t53_26), _mm256_mul_pd(_t53_12, _t53_27)));
        _t53_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_11, _t53_24), _mm256_mul_pd(_t53_10, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_9, _t53_26), _mm256_mul_pd(_t53_8, _t53_27)));
        _t53_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_7, _t53_24), _mm256_mul_pd(_t53_6, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_5, _t53_26), _mm256_mul_pd(_t53_4, _t53_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t52_8 = _mm256_add_pd(_t52_8, _t53_20);
        _t52_9 = _mm256_add_pd(_t52_9, _t53_21);
        _t52_10 = _mm256_add_pd(_t52_10, _t53_22);
        _t52_11 = _mm256_add_pd(_t52_11, _t53_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M3 + 24*i0 + k3, _t52_8);
      _mm256_storeu_pd(M3 + 24*i0 + k3 + 24, _t52_9);
      _mm256_storeu_pd(M3 + 24*i0 + k3 + 48, _t52_10);
      _mm256_storeu_pd(M3 + 24*i0 + k3 + 72, _t52_11);
    }
    _mm256_storeu_pd(M3 + 25*i0, _t50_28);
    _mm256_maskstore_pd(M3 + 25*i0 + 24, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t50_29);
    _mm256_maskstore_pd(M3 + 25*i0 + 48, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t50_30);
    _mm256_maskstore_pd(M3 + 25*i0 + 72, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t50_31);
  }

  _t54_23 = _mm256_broadcast_sd(M1 + 560);
  _t54_22 = _mm256_broadcast_sd(M1 + 561);
  _t54_21 = _mm256_broadcast_sd(M1 + 562);
  _t54_20 = _mm256_broadcast_sd(M1 + 563);
  _t54_19 = _mm256_broadcast_sd(M1 + 588);
  _t54_18 = _mm256_broadcast_sd(M1 + 589);
  _t54_17 = _mm256_broadcast_sd(M1 + 590);
  _t54_16 = _mm256_broadcast_sd(M1 + 591);
  _t54_15 = _mm256_broadcast_sd(M1 + 616);
  _t54_14 = _mm256_broadcast_sd(M1 + 617);
  _t54_13 = _mm256_broadcast_sd(M1 + 618);
  _t54_12 = _mm256_broadcast_sd(M1 + 619);
  _t54_11 = _mm256_broadcast_sd(M1 + 644);
  _t54_10 = _mm256_broadcast_sd(M1 + 645);
  _t54_9 = _mm256_broadcast_sd(M1 + 646);
  _t54_8 = _mm256_broadcast_sd(M1 + 647);
  _t54_7 = _mm256_loadu_pd(H + 560);
  _t54_6 = _mm256_loadu_pd(H + 588);
  _t54_5 = _mm256_loadu_pd(H + 616);
  _t54_4 = _mm256_loadu_pd(H + 644);
  _t54_3 = _mm256_loadu_pd(R + 500);
  _t54_2 = _mm256_maskload_pd(R + 524, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t54_1 = _mm256_maskload_pd(R + 548, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t54_0 = _mm256_maskload_pd(R + 572, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_7, _t54_6), _mm256_unpacklo_pd(_t54_5, _t54_4), 32);
  _t54_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_7, _t54_6), _mm256_unpackhi_pd(_t54_5, _t54_4), 32);
  _t54_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_7, _t54_6), _mm256_unpacklo_pd(_t54_5, _t54_4), 49);
  _t54_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_7, _t54_6), _mm256_unpackhi_pd(_t54_5, _t54_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t54_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_23, _t54_40), _mm256_mul_pd(_t54_22, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_21, _t54_42), _mm256_mul_pd(_t54_20, _t54_43)));
  _t54_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_19, _t54_40), _mm256_mul_pd(_t54_18, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_17, _t54_42), _mm256_mul_pd(_t54_16, _t54_43)));
  _t54_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_15, _t54_40), _mm256_mul_pd(_t54_14, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_13, _t54_42), _mm256_mul_pd(_t54_12, _t54_43)));
  _t54_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_11, _t54_40), _mm256_mul_pd(_t54_10, _t54_41)), _mm256_add_pd(_mm256_mul_pd(_t54_9, _t54_42), _mm256_mul_pd(_t54_8, _t54_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t54_36 = _t54_3;
  _t54_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 3), _t54_2, 12);
  _t54_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 0), _t54_1, 49);
  _t54_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 12), _mm256_shuffle_pd(_t54_1, _t54_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t54_24 = _mm256_add_pd(_t54_32, _t54_36);
  _t54_25 = _mm256_add_pd(_t54_33, _t54_37);
  _t54_26 = _mm256_add_pd(_t54_34, _t54_38);
  _t54_27 = _mm256_add_pd(_t54_35, _t54_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t54_28 = _t54_24;
  _t54_29 = _t54_25;
  _t54_30 = _t54_26;
  _t54_31 = _t54_27;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t55_19 = _mm256_broadcast_sd(M1 + k2 + 560);
    _t55_18 = _mm256_broadcast_sd(M1 + k2 + 561);
    _t55_17 = _mm256_broadcast_sd(M1 + k2 + 562);
    _t55_16 = _mm256_broadcast_sd(M1 + k2 + 563);
    _t55_15 = _mm256_broadcast_sd(M1 + k2 + 588);
    _t55_14 = _mm256_broadcast_sd(M1 + k2 + 589);
    _t55_13 = _mm256_broadcast_sd(M1 + k2 + 590);
    _t55_12 = _mm256_broadcast_sd(M1 + k2 + 591);
    _t55_11 = _mm256_broadcast_sd(M1 + k2 + 616);
    _t55_10 = _mm256_broadcast_sd(M1 + k2 + 617);
    _t55_9 = _mm256_broadcast_sd(M1 + k2 + 618);
    _t55_8 = _mm256_broadcast_sd(M1 + k2 + 619);
    _t55_7 = _mm256_broadcast_sd(M1 + k2 + 644);
    _t55_6 = _mm256_broadcast_sd(M1 + k2 + 645);
    _t55_5 = _mm256_broadcast_sd(M1 + k2 + 646);
    _t55_4 = _mm256_broadcast_sd(M1 + k2 + 647);
    _t55_3 = _mm256_loadu_pd(H + k2 + 560);
    _t55_2 = _mm256_loadu_pd(H + k2 + 588);
    _t55_1 = _mm256_loadu_pd(H + k2 + 616);
    _t55_0 = _mm256_loadu_pd(H + k2 + 644);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t55_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32);
    _t55_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_3, _t55_2), _mm256_unpackhi_pd(_t55_1, _t55_0), 32);
    _t55_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 49);
    _t55_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_3, _t55_2), _mm256_unpackhi_pd(_t55_1, _t55_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t55_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_19, _t55_28), _mm256_mul_pd(_t55_18, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_17, _t55_30), _mm256_mul_pd(_t55_16, _t55_31)));
    _t55_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_15, _t55_28), _mm256_mul_pd(_t55_14, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_13, _t55_30), _mm256_mul_pd(_t55_12, _t55_31)));
    _t55_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_11, _t55_28), _mm256_mul_pd(_t55_10, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_9, _t55_30), _mm256_mul_pd(_t55_8, _t55_31)));
    _t55_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_7, _t55_28), _mm256_mul_pd(_t55_6, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_5, _t55_30), _mm256_mul_pd(_t55_4, _t55_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t55_24 = _t54_28;
    _t55_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 3), _t54_29, 12);
    _t55_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 0), _t54_30, 49);
    _t55_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_28, _t54_29, 12), _mm256_shuffle_pd(_t54_30, _t54_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t55_24 = _mm256_add_pd(_t55_24, _t55_20);
    _t55_25 = _mm256_add_pd(_t55_25, _t55_21);
    _t55_26 = _mm256_add_pd(_t55_26, _t55_22);
    _t55_27 = _mm256_add_pd(_t55_27, _t55_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t54_28 = _t55_24;
    _t54_29 = _t55_25;
    _t54_30 = _t55_26;
    _t54_31 = _t55_27;
  }

  _t56_21 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_23 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_24 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_25 = _mm256_maskload_pd(M3 + 49, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_26 = _mm256_maskload_pd(M3 + 73, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t56_27 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_29 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_30 = _mm256_maskload_pd(M3 + 50, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_31 = _mm256_maskload_pd(M3 + 74, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t56_32 = _mm256_maskload_pd(M3 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_33 = _mm256_maskload_pd(M3 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_34 = _mm256_maskload_pd(M3 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_40 = _mm256_loadu_pd(M3 + 4);
  _t56_37 = _mm256_loadu_pd(M3 + 28);
  _t56_38 = _mm256_loadu_pd(M3 + 52);
  _t56_39 = _mm256_loadu_pd(M3 + 76);
  _t56_17 = _mm256_broadcast_sd(&(M3[51]));

  // Generating : M3[24,24] = S(h(1, 24, fi39), Sqrt( G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ),h(1, 24, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_42 = _t56_21;

  // 4-BLAC: sqrt(1x4)
  _t56_43 = _mm256_sqrt_pd(_t56_42);

  // AVX Storer:
  _t56_21 = _t56_43;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ),h(1, 24, fi39))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_44 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_45 = _t56_21;

  // 4-BLAC: 1x4 / 1x4
  _t56_46 = _mm256_div_pd(_t56_44, _t56_45);

  // AVX Storer:
  _t56_22 = _t56_46;

  // Generating : M3[24,24] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ),h(3, 24, fi39 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_47 = _t56_20;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_48 = _t56_23;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_49 = _mm256_mul_pd(_t56_47, _t56_48);

  // AVX Storer:
  _t56_23 = _t56_49;

  // Generating : M3[24,24] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M3[24,24],h(3, 24, fi39 + 1)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ),h(3, 24, fi39 + 1))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t56_50 = _t56_24;
  _t56_51 = _mm256_blend_pd(_mm256_shuffle_pd(_t56_24, _t56_25, 3), _t56_25, 12);
  _t56_52 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t56_24, _t56_25, 0), _t56_26, 49);
  _t56_53 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_54 = _t56_23;

  // 4-BLAC: (1x4)^T
  _t56_55 = _t56_54;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_56 = _t56_23;

  // 4-BLAC: 4x1 * 1x4
  _t56_57 = _mm256_mul_pd(_t56_15, _t56_56);
  _t56_58 = _mm256_mul_pd(_t56_14, _t56_56);
  _t56_59 = _mm256_mul_pd(_t56_13, _t56_56);
  _t56_60 = _mm256_mul_pd(_t56_12, _t56_56);

  // 4-BLAC: 4x4 - 4x4
  _t56_61 = _mm256_sub_pd(_t56_50, _t56_57);
  _t56_62 = _mm256_sub_pd(_t56_51, _t56_58);
  _t56_63 = _mm256_sub_pd(_t56_52, _t56_59);
  _t56_64 = _mm256_sub_pd(_t56_53, _t56_60);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t56_24 = _t56_61;
  _t56_25 = _t56_62;
  _t56_26 = _t56_63;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), Sqrt( G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ),h(1, 24, fi39 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_65 = _t56_27;

  // 4-BLAC: sqrt(1x4)
  _t56_66 = _mm256_sqrt_pd(_t56_65);

  // AVX Storer:
  _t56_27 = _t56_66;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ),h(1, 24, fi39 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_67 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_68 = _t56_27;

  // 4-BLAC: 1x4 / 1x4
  _t56_69 = _mm256_div_pd(_t56_67, _t56_68);

  // AVX Storer:
  _t56_28 = _t56_69;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ),h(2, 24, fi39 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_70 = _t56_19;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_71 = _t56_29;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_72 = _mm256_mul_pd(_t56_70, _t56_71);

  // AVX Storer:
  _t56_29 = _t56_72;

  // Generating : M3[24,24] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M3[24,24],h(2, 24, fi39 + 2)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ),h(2, 24, fi39 + 2))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t56_73 = _t56_30;
  _t56_74 = _mm256_shuffle_pd(_t56_30, _t56_31, 3);
  _t56_75 = _mm256_setzero_pd();
  _t56_76 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_77 = _t56_29;

  // 4-BLAC: (1x4)^T
  _t56_78 = _t56_77;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_79 = _t56_29;

  // 4-BLAC: 4x1 * 1x4
  _t56_80 = _mm256_mul_pd(_t56_11, _t56_79);
  _t56_81 = _mm256_mul_pd(_t56_10, _t56_79);
  _t56_82 = _mm256_mul_pd(_t56_9, _t56_79);
  _t56_83 = _mm256_mul_pd(_t56_8, _t56_79);

  // 4-BLAC: 4x4 - 4x4
  _t56_84 = _mm256_sub_pd(_t56_73, _t56_80);
  _t56_85 = _mm256_sub_pd(_t56_74, _t56_81);
  _t56_86 = _mm256_sub_pd(_t56_75, _t56_82);
  _t56_87 = _mm256_sub_pd(_t56_76, _t56_83);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t56_30 = _t56_84;
  _t56_31 = _t56_85;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), Sqrt( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_88 = _t56_32;

  // 4-BLAC: sqrt(1x4)
  _t56_89 = _mm256_sqrt_pd(_t56_88);

  // AVX Storer:
  _t56_32 = _t56_89;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), ( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) Div G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_90 = _t56_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_91 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t56_92 = _mm256_div_pd(_t56_90, _t56_91);

  // AVX Storer:
  _t56_33 = _t56_92;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) ),h(1, 24, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_93 = _t56_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_94 = _t56_33;

  // 4-BLAC: (4x1)^T
  _t56_95 = _t56_94;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_96 = _t56_33;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_97 = _mm256_mul_pd(_t56_95, _t56_96);

  // 4-BLAC: 1x4 - 1x4
  _t56_98 = _mm256_sub_pd(_t56_93, _t56_97);

  // AVX Storer:
  _t56_34 = _t56_98;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), Sqrt( G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ),h(1, 24, fi39 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_99 = _t56_34;

  // 4-BLAC: sqrt(1x4)
  _t56_100 = _mm256_sqrt_pd(_t56_99);

  // AVX Storer:
  _t56_34 = _t56_100;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_101 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_102 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t56_103 = _mm256_div_pd(_t56_101, _t56_102);

  // AVX Storer:
  _t56_35 = _t56_103;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ),h(1, 24, fi39 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_104 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_105 = _t56_34;

  // 4-BLAC: 1x4 / 1x4
  _t56_106 = _mm256_div_pd(_t56_104, _t56_105);

  // AVX Storer:
  _t56_36 = _t56_106;

  // Generating : M3[24,24] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_107 = _t56_20;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_40 = _mm256_mul_pd(_t56_107, _t56_40);

  // AVX Storer:

  // Generating : M3[24,24] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t56_108 = _t56_37;
  _t56_109 = _t56_38;
  _t56_110 = _t56_39;
  _t56_111 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_112 = _t56_23;

  // 4-BLAC: (1x4)^T
  _t56_113 = _t56_112;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_114 = _mm256_mul_pd(_t56_7, _t56_40);
  _t56_115 = _mm256_mul_pd(_t56_6, _t56_40);
  _t56_116 = _mm256_mul_pd(_t56_5, _t56_40);
  _t56_117 = _mm256_mul_pd(_t56_4, _t56_40);

  // 4-BLAC: 4x4 - 4x4
  _t56_118 = _mm256_sub_pd(_t56_108, _t56_114);
  _t56_119 = _mm256_sub_pd(_t56_109, _t56_115);
  _t56_120 = _mm256_sub_pd(_t56_110, _t56_116);
  _t56_121 = _mm256_sub_pd(_t56_111, _t56_117);

  // AVX Storer:
  _t56_37 = _t56_118;
  _t56_38 = _t56_119;
  _t56_39 = _t56_120;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_122 = _t56_19;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_37 = _mm256_mul_pd(_t56_122, _t56_37);

  // AVX Storer:

  // Generating : M3[24,24] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t56_123 = _t56_38;
  _t56_124 = _t56_39;
  _t56_125 = _mm256_setzero_pd();
  _t56_126 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_127 = _t56_29;

  // 4-BLAC: (1x4)^T
  _t56_128 = _t56_127;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_129 = _mm256_mul_pd(_t56_3, _t56_37);
  _t56_130 = _mm256_mul_pd(_t56_2, _t56_37);
  _t56_131 = _mm256_mul_pd(_t56_1, _t56_37);
  _t56_132 = _mm256_mul_pd(_t56_0, _t56_37);

  // 4-BLAC: 4x4 - 4x4
  _t56_133 = _mm256_sub_pd(_t56_123, _t56_129);
  _t56_134 = _mm256_sub_pd(_t56_124, _t56_130);
  _t56_135 = _mm256_sub_pd(_t56_125, _t56_131);
  _t56_136 = _mm256_sub_pd(_t56_126, _t56_132);

  // AVX Storer:
  _t56_38 = _t56_133;
  _t56_39 = _t56_134;

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_137 = _t56_18;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_38 = _mm256_mul_pd(_t56_137, _t56_38);

  // AVX Storer:

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_138 = _t56_17;

  // 4-BLAC: (4x1)^T
  _t56_139 = _t56_138;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_41 = _mm256_mul_pd(_t56_139, _t56_38);

  // 4-BLAC: 1x4 - 1x4
  _t56_39 = _mm256_sub_pd(_t56_39, _t56_41);

  // AVX Storer:

  // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) Kro G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_140 = _t56_16;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_39 = _mm256_mul_pd(_t56_140, _t56_39);

  // AVX Storer:

  _mm256_maskstore_pd(M3 + 25, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_24);
  _mm256_maskstore_pd(M3 + 50, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_30);
  _mm256_maskstore_pd(M3 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_33);

  for( int fi100 = 4; fi100 <= 16; fi100+=4 ) {
    _t57_3 = _mm256_loadu_pd(M3 + fi100 + 4);
    _t57_0 = _mm256_loadu_pd(M3 + fi100 + 28);
    _t57_1 = _mm256_loadu_pd(M3 + fi100 + 52);
    _t57_2 = _mm256_loadu_pd(M3 + fi100 + 76);

    // Generating : M3[24,24] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_4 = _t56_20;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_3 = _mm256_mul_pd(_t57_4, _t57_3);

    // AVX Storer:

    // Generating : M3[24,24] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t57_5 = _t57_0;
    _t57_6 = _t57_1;
    _t57_7 = _t57_2;
    _t57_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t57_9 = _t56_23;

    // 4-BLAC: (1x4)^T
    _t57_10 = _t57_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t56_114 = _mm256_mul_pd(_t56_7, _t57_3);
    _t56_115 = _mm256_mul_pd(_t56_6, _t57_3);
    _t56_116 = _mm256_mul_pd(_t56_5, _t57_3);
    _t56_117 = _mm256_mul_pd(_t56_4, _t57_3);

    // 4-BLAC: 4x4 - 4x4
    _t57_11 = _mm256_sub_pd(_t57_5, _t56_114);
    _t57_12 = _mm256_sub_pd(_t57_6, _t56_115);
    _t57_13 = _mm256_sub_pd(_t57_7, _t56_116);
    _t57_14 = _mm256_sub_pd(_t57_8, _t56_117);

    // AVX Storer:
    _t57_0 = _t57_11;
    _t57_1 = _t57_12;
    _t57_2 = _t57_13;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_15 = _t56_19;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_0 = _mm256_mul_pd(_t57_15, _t57_0);

    // AVX Storer:

    // Generating : M3[24,24] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t57_16 = _t57_1;
    _t57_17 = _t57_2;
    _t57_18 = _mm256_setzero_pd();
    _t57_19 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t57_20 = _t56_29;

    // 4-BLAC: (1x4)^T
    _t57_21 = _t57_20;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t56_129 = _mm256_mul_pd(_t56_3, _t57_0);
    _t56_130 = _mm256_mul_pd(_t56_2, _t57_0);
    _t56_131 = _mm256_mul_pd(_t56_1, _t57_0);
    _t56_132 = _mm256_mul_pd(_t56_0, _t57_0);

    // 4-BLAC: 4x4 - 4x4
    _t57_22 = _mm256_sub_pd(_t57_16, _t56_129);
    _t57_23 = _mm256_sub_pd(_t57_17, _t56_130);
    _t57_24 = _mm256_sub_pd(_t57_18, _t56_131);
    _t57_25 = _mm256_sub_pd(_t57_19, _t56_132);

    // AVX Storer:
    _t57_1 = _t57_22;
    _t57_2 = _t57_23;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_26 = _t56_18;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_1 = _mm256_mul_pd(_t57_26, _t57_1);

    // AVX Storer:

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_27 = _t56_17;

    // 4-BLAC: (4x1)^T
    _t56_139 = _t57_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t56_41 = _mm256_mul_pd(_t56_139, _t57_1);

    // 4-BLAC: 1x4 - 1x4
    _t57_2 = _mm256_sub_pd(_t57_2, _t56_41);

    // AVX Storer:

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) Kro G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_28 = _t56_16;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_2 = _mm256_mul_pd(_t57_28, _t57_2);

    // AVX Storer:
    _mm256_storeu_pd(M3 + fi100 + 4, _t57_3);
    _mm256_storeu_pd(M3 + fi100 + 28, _t57_0);
    _mm256_storeu_pd(M3 + fi100 + 52, _t57_1);
    _mm256_storeu_pd(M3 + fi100 + 76, _t57_2);
  }


  // Generating : M3[24,24] = Sum_{i0} ( S(h(4, 24, fi39 + i0 + 4), ( G(h(4, 24, fi39 + i0 + 4), M3[24,24],h(4, 24, fi39 + i0 + 4)) - ( T( G(h(4, 24, fi39), M3[24,24],h(4, 24, fi39 + i0 + 4)) ) * G(h(4, 24, fi39), M3[24,24],h(4, 24, fi39 + i0 + 4)) ) ),h(4, 24, fi39 + i0 + 4)) )

  _mm256_storeu_pd(M3 + 500, _t54_28);
  _mm256_maskstore_pd(M3 + 524, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t54_29);
  _mm256_maskstore_pd(M3 + 548, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t54_30);
  _mm256_maskstore_pd(M3 + 572, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t54_31);
  _mm256_storeu_pd(M3 + 4, _t56_40);
  _mm256_storeu_pd(M3 + 28, _t56_37);
  _mm256_storeu_pd(M3 + 52, _t56_38);
  _mm256_storeu_pd(M3 + 76, _t56_39);

  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t58_20 = _mm256_loadu_pd(M3 + 25*i0 + 100);
    _t58_21 = _mm256_maskload_pd(M3 + 25*i0 + 124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t58_22 = _mm256_maskload_pd(M3 + 25*i0 + 148, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t58_23 = _mm256_maskload_pd(M3 + 25*i0 + 172, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t58_19 = _mm256_loadu_pd(M3 + i0 + 4);
    _t58_18 = _mm256_loadu_pd(M3 + i0 + 28);
    _t58_17 = _mm256_loadu_pd(M3 + i0 + 52);
    _t58_16 = _mm256_loadu_pd(M3 + i0 + 76);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t58_32 = _t58_20;
    _t58_33 = _mm256_blend_pd(_mm256_shuffle_pd(_t58_20, _t58_21, 3), _t58_21, 12);
    _t58_34 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t58_20, _t58_21, 0), _t58_22, 49);
    _t58_35 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t58_20, _t58_21, 12), _mm256_shuffle_pd(_t58_22, _t58_23, 12), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t58_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_19, _t58_18), _mm256_unpacklo_pd(_t58_17, _t58_16), 32);
    _t58_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_19, _t58_18), _mm256_unpackhi_pd(_t58_17, _t58_16), 32);
    _t58_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_19, _t58_18), _mm256_unpacklo_pd(_t58_17, _t58_16), 49);
    _t58_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_19, _t58_18), _mm256_unpackhi_pd(_t58_17, _t58_16), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t58_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_15, _t58_19), _mm256_mul_pd(_t58_14, _t58_18)), _mm256_add_pd(_mm256_mul_pd(_t58_13, _t58_17), _mm256_mul_pd(_t58_12, _t58_16)));
    _t58_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_11, _t58_19), _mm256_mul_pd(_t58_10, _t58_18)), _mm256_add_pd(_mm256_mul_pd(_t58_9, _t58_17), _mm256_mul_pd(_t58_8, _t58_16)));
    _t58_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_7, _t58_19), _mm256_mul_pd(_t58_6, _t58_18)), _mm256_add_pd(_mm256_mul_pd(_t58_5, _t58_17), _mm256_mul_pd(_t58_4, _t58_16)));
    _t58_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t58_3, _t58_19), _mm256_mul_pd(_t58_2, _t58_18)), _mm256_add_pd(_mm256_mul_pd(_t58_1, _t58_17), _mm256_mul_pd(_t58_0, _t58_16)));

    // 4-BLAC: 4x4 - 4x4
    _t58_28 = _mm256_sub_pd(_t58_32, _t58_24);
    _t58_29 = _mm256_sub_pd(_t58_33, _t58_25);
    _t58_30 = _mm256_sub_pd(_t58_34, _t58_26);
    _t58_31 = _mm256_sub_pd(_t58_35, _t58_27);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t58_20 = _t58_28;
    _t58_21 = _t58_29;
    _t58_22 = _t58_30;
    _t58_23 = _t58_31;
    _mm256_storeu_pd(M3 + 25*i0 + 100, _t58_20);
    _mm256_maskstore_pd(M3 + 25*i0 + 124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t58_21);
    _mm256_maskstore_pd(M3 + 25*i0 + 148, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t58_22);
    _mm256_maskstore_pd(M3 + 25*i0 + 172, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t58_23);
  }


  for( int fi39 = 4; fi39 <= 16; fi39+=4 ) {
    _t59_2 = _mm256_maskload_pd(M3 + 25*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_4 = _mm256_maskload_pd(M3 + 25*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t59_5 = _mm256_maskload_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t59_6 = _mm256_maskload_pd(M3 + 25*fi39 + 49, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t59_7 = _mm256_maskload_pd(M3 + 25*fi39 + 73, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
    _t59_8 = _mm256_maskload_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_10 = _mm256_maskload_pd(M3 + 25*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t59_11 = _mm256_maskload_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t59_12 = _mm256_maskload_pd(M3 + 25*fi39 + 74, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
    _t59_13 = _mm256_maskload_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_14 = _mm256_maskload_pd(M3 + 25*fi39 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t59_15 = _mm256_maskload_pd(M3 + 25*fi39 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

    // Generating : M3[24,24] = S(h(1, 24, fi39), Sqrt( G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ),h(1, 24, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_18 = _t59_2;

    // 4-BLAC: sqrt(1x4)
    _t59_19 = _mm256_sqrt_pd(_t59_18);

    // AVX Storer:
    _t59_2 = _t59_19;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ),h(1, 24, fi39))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t59_20 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_21 = _t59_2;

    // 4-BLAC: 1x4 / 1x4
    _t59_22 = _mm256_div_pd(_t59_20, _t59_21);

    // AVX Storer:
    _t59_3 = _t59_22;

    // Generating : M3[24,24] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ),h(3, 24, fi39 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_23 = _t59_1;

    // AVX Loader:

    // 1x3 -> 1x4
    _t59_24 = _t59_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t59_25 = _mm256_mul_pd(_t59_23, _t59_24);

    // AVX Storer:
    _t59_4 = _t59_25;

    // Generating : M3[24,24] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M3[24,24],h(3, 24, fi39 + 1)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) ),h(3, 24, fi39 + 1))

    // AVX Loader:

    // 3x3 -> 4x4 - UpSymm
    _t59_26 = _t59_5;
    _t59_27 = _mm256_blend_pd(_mm256_shuffle_pd(_t59_5, _t59_6, 3), _t59_6, 12);
    _t59_28 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t59_5, _t59_6, 0), _t59_7, 49);
    _t59_29 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t59_30 = _t59_4;

    // 4-BLAC: (1x4)^T
    _t59_31 = _t59_30;

    // AVX Loader:

    // 1x3 -> 1x4
    _t59_32 = _t59_4;

    // 4-BLAC: 4x1 * 1x4
    _t56_57 = _mm256_mul_pd(_t56_15, _t59_32);
    _t56_58 = _mm256_mul_pd(_t56_14, _t59_32);
    _t56_59 = _mm256_mul_pd(_t56_13, _t59_32);
    _t56_60 = _mm256_mul_pd(_t56_12, _t59_32);

    // 4-BLAC: 4x4 - 4x4
    _t59_33 = _mm256_sub_pd(_t59_26, _t56_57);
    _t59_34 = _mm256_sub_pd(_t59_27, _t56_58);
    _t59_35 = _mm256_sub_pd(_t59_28, _t56_59);
    _t59_36 = _mm256_sub_pd(_t59_29, _t56_60);

    // AVX Storer:

    // 4x4 -> 3x3 - UpSymm
    _t59_5 = _t59_33;
    _t59_6 = _t59_34;
    _t59_7 = _t59_35;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), Sqrt( G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ),h(1, 24, fi39 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_37 = _t59_8;

    // 4-BLAC: sqrt(1x4)
    _t59_38 = _mm256_sqrt_pd(_t59_37);

    // AVX Storer:
    _t59_8 = _t59_38;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ),h(1, 24, fi39 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t59_39 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_40 = _t59_8;

    // 4-BLAC: 1x4 / 1x4
    _t59_41 = _mm256_div_pd(_t59_39, _t59_40);

    // AVX Storer:
    _t59_9 = _t59_41;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ),h(2, 24, fi39 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_42 = _t59_0;

    // AVX Loader:

    // 1x2 -> 1x4
    _t59_43 = _t59_10;

    // 4-BLAC: 1x4 Kro 1x4
    _t59_44 = _mm256_mul_pd(_t59_42, _t59_43);

    // AVX Storer:
    _t59_10 = _t59_44;

    // Generating : M3[24,24] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M3[24,24],h(2, 24, fi39 + 2)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) ),h(2, 24, fi39 + 2))

    // AVX Loader:

    // 2x2 -> 4x4 - UpSymm
    _t59_45 = _t59_11;
    _t59_46 = _mm256_shuffle_pd(_t59_11, _t59_12, 3);
    _t59_47 = _mm256_setzero_pd();
    _t59_48 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t59_49 = _t59_10;

    // 4-BLAC: (1x4)^T
    _t59_50 = _t59_49;

    // AVX Loader:

    // 1x2 -> 1x4
    _t59_51 = _t59_10;

    // 4-BLAC: 4x1 * 1x4
    _t56_80 = _mm256_mul_pd(_t56_11, _t59_51);
    _t56_81 = _mm256_mul_pd(_t56_10, _t59_51);
    _t56_82 = _mm256_mul_pd(_t56_9, _t59_51);
    _t56_83 = _mm256_mul_pd(_t56_8, _t59_51);

    // 4-BLAC: 4x4 - 4x4
    _t59_52 = _mm256_sub_pd(_t59_45, _t56_80);
    _t59_53 = _mm256_sub_pd(_t59_46, _t56_81);
    _t59_54 = _mm256_sub_pd(_t59_47, _t56_82);
    _t59_55 = _mm256_sub_pd(_t59_48, _t56_83);

    // AVX Storer:

    // 4x4 -> 2x2 - UpSymm
    _t59_11 = _t59_52;
    _t59_12 = _t59_53;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), Sqrt( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_56 = _t59_13;

    // 4-BLAC: sqrt(1x4)
    _t59_57 = _mm256_sqrt_pd(_t59_56);

    // AVX Storer:
    _t59_13 = _t59_57;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), ( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) Div G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_58 = _t59_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_59 = _t59_13;

    // 4-BLAC: 1x4 / 1x4
    _t59_60 = _mm256_div_pd(_t59_58, _t59_59);

    // AVX Storer:
    _t59_14 = _t59_60;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) ),h(1, 24, fi39 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_61 = _t59_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_62 = _t59_14;

    // 4-BLAC: (4x1)^T
    _t56_95 = _t59_62;

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_63 = _t59_14;

    // 4-BLAC: 1x4 Kro 1x4
    _t56_97 = _mm256_mul_pd(_t56_95, _t59_63);

    // 4-BLAC: 1x4 - 1x4
    _t59_64 = _mm256_sub_pd(_t59_61, _t56_97);

    // AVX Storer:
    _t59_15 = _t59_64;

    // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), Sqrt( G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ),h(1, 24, fi39 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_65 = _t59_15;

    // 4-BLAC: sqrt(1x4)
    _t59_66 = _mm256_sqrt_pd(_t59_65);

    // AVX Storer:
    _t59_15 = _t59_66;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t59_67 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_68 = _t59_13;

    // 4-BLAC: 1x4 / 1x4
    _t59_69 = _mm256_div_pd(_t59_67, _t59_68);

    // AVX Storer:
    _t59_16 = _t59_69;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ),h(1, 24, fi39 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t59_70 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_71 = _t59_15;

    // 4-BLAC: 1x4 / 1x4
    _t59_72 = _mm256_div_pd(_t59_70, _t59_71);

    // AVX Storer:
    _t59_17 = _t59_72;
    _mm256_maskstore_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t59_5);
    _mm256_maskstore_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t59_11);
    _mm256_maskstore_pd(M3 + 25*fi39 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_14);

    for( int fi100 = 0; fi100 <= -fi39 + 16; fi100+=4 ) {
      _t60_6 = _mm256_loadu_pd(M3 + fi100 + 25*fi39 + 4);
      _t60_3 = _mm256_loadu_pd(M3 + fi100 + 25*fi39 + 28);
      _t60_4 = _mm256_loadu_pd(M3 + fi100 + 25*fi39 + 52);
      _t60_5 = _mm256_loadu_pd(M3 + fi100 + 25*fi39 + 76);
      _t60_1 = _mm256_broadcast_sd(&(M3[25*fi39 + 51]));

      // Generating : M3[24,24] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t60_7 = _t59_1;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t60_6 = _mm256_mul_pd(_t60_7, _t60_6);

      // AVX Storer:

      // Generating : M3[24,24] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // 3x4 -> 4x4
      _t60_8 = _t60_3;
      _t60_9 = _t60_4;
      _t60_10 = _t60_5;
      _t60_11 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t60_12 = _t59_4;

      // 4-BLAC: (1x4)^T
      _t60_13 = _t60_12;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t56_114 = _mm256_mul_pd(_t56_7, _t60_6);
      _t56_115 = _mm256_mul_pd(_t56_6, _t60_6);
      _t56_116 = _mm256_mul_pd(_t56_5, _t60_6);
      _t56_117 = _mm256_mul_pd(_t56_4, _t60_6);

      // 4-BLAC: 4x4 - 4x4
      _t60_14 = _mm256_sub_pd(_t60_8, _t56_114);
      _t60_15 = _mm256_sub_pd(_t60_9, _t56_115);
      _t60_16 = _mm256_sub_pd(_t60_10, _t56_116);
      _t60_17 = _mm256_sub_pd(_t60_11, _t56_117);

      // AVX Storer:
      _t60_3 = _t60_14;
      _t60_4 = _t60_15;
      _t60_5 = _t60_16;

      // Generating : M3[24,24] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t60_18 = _t59_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t60_3 = _mm256_mul_pd(_t60_18, _t60_3);

      // AVX Storer:

      // Generating : M3[24,24] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // 2x4 -> 4x4
      _t60_19 = _t60_4;
      _t60_20 = _t60_5;
      _t60_21 = _mm256_setzero_pd();
      _t60_22 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t60_23 = _t59_10;

      // 4-BLAC: (1x4)^T
      _t60_24 = _t60_23;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t56_129 = _mm256_mul_pd(_t56_3, _t60_3);
      _t56_130 = _mm256_mul_pd(_t56_2, _t60_3);
      _t56_131 = _mm256_mul_pd(_t56_1, _t60_3);
      _t56_132 = _mm256_mul_pd(_t56_0, _t60_3);

      // 4-BLAC: 4x4 - 4x4
      _t60_25 = _mm256_sub_pd(_t60_19, _t56_129);
      _t60_26 = _mm256_sub_pd(_t60_20, _t56_130);
      _t60_27 = _mm256_sub_pd(_t60_21, _t56_131);
      _t60_28 = _mm256_sub_pd(_t60_22, _t56_132);

      // AVX Storer:
      _t60_4 = _t60_25;
      _t60_5 = _t60_26;

      // Generating : M3[24,24] = S(h(1, 24, fi39 + 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t60_29 = _t60_2;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t60_4 = _mm256_mul_pd(_t60_29, _t60_4);

      // AVX Storer:

      // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t60_30 = _t60_1;

      // 4-BLAC: (4x1)^T
      _t56_139 = _t60_30;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t56_41 = _mm256_mul_pd(_t56_139, _t60_4);

      // 4-BLAC: 1x4 - 1x4
      _t60_5 = _mm256_sub_pd(_t60_5, _t56_41);

      // AVX Storer:

      // Generating : M3[24,24] = S(h(1, 24, fi39 + 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) Kro G(h(1, 24, fi39 + 3), M3[24,24],h(4, 24, fi100 + fi39 + 4)) ),h(4, 24, fi100 + fi39 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t60_31 = _t60_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t60_5 = _mm256_mul_pd(_t60_31, _t60_5);

      // AVX Storer:
      _mm256_storeu_pd(M3 + fi100 + 25*fi39 + 4, _t60_6);
      _mm256_storeu_pd(M3 + fi100 + 25*fi39 + 28, _t60_3);
      _mm256_storeu_pd(M3 + fi100 + 25*fi39 + 52, _t60_4);
      _mm256_storeu_pd(M3 + fi100 + 25*fi39 + 76, _t60_5);
    }

    // Generating : M3[24,24] = Sum_{i0} ( S(h(4, 24, fi39 + i0 + 4), ( G(h(4, 24, fi39 + i0 + 4), M3[24,24],h(4, 24, fi39 + i0 + 4)) - ( T( G(h(4, 24, fi39), M3[24,24],h(4, 24, fi39 + i0 + 4)) ) * G(h(4, 24, fi39), M3[24,24],h(4, 24, fi39 + i0 + 4)) ) ),h(4, 24, fi39 + i0 + 4)) )
    _mm256_maskstore_pd(M3 + 25*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_2);
    _mm256_maskstore_pd(M3 + 25*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t59_4);
    _mm256_maskstore_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_8);
    _mm256_maskstore_pd(M3 + 25*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t59_10);
    _mm256_maskstore_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_13);
    _mm256_maskstore_pd(M3 + 25*fi39 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t59_15);

    for( int i0 = 0; i0 <= -fi39 + 19; i0+=4 ) {
      _t61_20 = _mm256_loadu_pd(M3 + 25*fi39 + 25*i0 + 100);
      _t61_21 = _mm256_maskload_pd(M3 + 25*fi39 + 25*i0 + 124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t61_22 = _mm256_maskload_pd(M3 + 25*fi39 + 25*i0 + 148, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t61_23 = _mm256_maskload_pd(M3 + 25*fi39 + 25*i0 + 172, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
      _t61_19 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 4);
      _t61_18 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 28);
      _t61_17 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 52);
      _t61_16 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 76);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t61_32 = _t61_20;
      _t61_33 = _mm256_blend_pd(_mm256_shuffle_pd(_t61_20, _t61_21, 3), _t61_21, 12);
      _t61_34 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t61_20, _t61_21, 0), _t61_22, 49);
      _t61_35 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t61_20, _t61_21, 12), _mm256_shuffle_pd(_t61_22, _t61_23, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t61_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_19, _t61_18), _mm256_unpacklo_pd(_t61_17, _t61_16), 32);
      _t61_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_19, _t61_18), _mm256_unpackhi_pd(_t61_17, _t61_16), 32);
      _t61_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_19, _t61_18), _mm256_unpacklo_pd(_t61_17, _t61_16), 49);
      _t61_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_19, _t61_18), _mm256_unpackhi_pd(_t61_17, _t61_16), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t61_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t61_15, _t61_19), _mm256_mul_pd(_t61_14, _t61_18)), _mm256_add_pd(_mm256_mul_pd(_t61_13, _t61_17), _mm256_mul_pd(_t61_12, _t61_16)));
      _t61_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t61_11, _t61_19), _mm256_mul_pd(_t61_10, _t61_18)), _mm256_add_pd(_mm256_mul_pd(_t61_9, _t61_17), _mm256_mul_pd(_t61_8, _t61_16)));
      _t61_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t61_7, _t61_19), _mm256_mul_pd(_t61_6, _t61_18)), _mm256_add_pd(_mm256_mul_pd(_t61_5, _t61_17), _mm256_mul_pd(_t61_4, _t61_16)));
      _t61_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t61_3, _t61_19), _mm256_mul_pd(_t61_2, _t61_18)), _mm256_add_pd(_mm256_mul_pd(_t61_1, _t61_17), _mm256_mul_pd(_t61_0, _t61_16)));

      // 4-BLAC: 4x4 - 4x4
      _t61_28 = _mm256_sub_pd(_t61_32, _t61_24);
      _t61_29 = _mm256_sub_pd(_t61_33, _t61_25);
      _t61_30 = _mm256_sub_pd(_t61_34, _t61_26);
      _t61_31 = _mm256_sub_pd(_t61_35, _t61_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t61_20 = _t61_28;
      _t61_21 = _t61_29;
      _t61_22 = _t61_30;
      _t61_23 = _t61_31;
      _mm256_storeu_pd(M3 + 25*fi39 + 25*i0 + 100, _t61_20);
      _mm256_maskstore_pd(M3 + 25*fi39 + 25*i0 + 124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t61_21);
      _mm256_maskstore_pd(M3 + 25*fi39 + 25*i0 + 148, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t61_22);
      _mm256_maskstore_pd(M3 + 25*fi39 + 25*i0 + 172, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t61_23);
    }
  }

  _t62_10 = _mm256_maskload_pd(M3 + 500, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_12 = _mm256_maskload_pd(M3 + 501, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t62_13 = _mm256_maskload_pd(M3 + 525, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t62_14 = _mm256_maskload_pd(M3 + 549, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t62_15 = _mm256_maskload_pd(M3 + 573, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t62_16 = _mm256_maskload_pd(M3 + 525, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_18 = _mm256_maskload_pd(M3 + 526, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t62_19 = _mm256_maskload_pd(M3 + 550, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t62_20 = _mm256_maskload_pd(M3 + 574, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t62_21 = _mm256_maskload_pd(M3 + 550, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_22 = _mm256_maskload_pd(M3 + 551, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_23 = _mm256_maskload_pd(M3 + 575, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

  // Generating : M3[24,24] = S(h(1, 24, 20), Sqrt( G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ),h(1, 24, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_24 = _t62_10;

  // 4-BLAC: sqrt(1x4)
  _t62_25 = _mm256_sqrt_pd(_t62_24);

  // AVX Storer:
  _t62_10 = _t62_25;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ),h(1, 24, 20))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t62_26 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_27 = _t62_10;

  // 4-BLAC: 1x4 / 1x4
  _t62_28 = _mm256_div_pd(_t62_26, _t62_27);

  // AVX Storer:
  _t62_11 = _t62_28;

  // Generating : M3[24,24] = S(h(1, 24, 20), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 20)) Kro G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ),h(3, 24, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_29 = _t62_9;

  // AVX Loader:

  // 1x3 -> 1x4
  _t62_30 = _t62_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t62_31 = _mm256_mul_pd(_t62_29, _t62_30);

  // AVX Storer:
  _t62_12 = _t62_31;

  // Generating : M3[24,24] = S(h(3, 24, 21), ( G(h(3, 24, 21), M3[24,24],h(3, 24, 21)) - ( T( G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) * G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) ),h(3, 24, 21))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t62_32 = _t62_13;
  _t62_33 = _mm256_blend_pd(_mm256_shuffle_pd(_t62_13, _t62_14, 3), _t62_14, 12);
  _t62_34 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t62_13, _t62_14, 0), _t62_15, 49);
  _t62_35 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t62_36 = _t62_12;

  // 4-BLAC: (1x4)^T
  _t62_37 = _t62_36;

  // AVX Loader:

  // 1x3 -> 1x4
  _t62_38 = _t62_12;

  // 4-BLAC: 4x1 * 1x4
  _t62_39 = _mm256_mul_pd(_t62_7, _t62_38);
  _t62_40 = _mm256_mul_pd(_t62_6, _t62_38);
  _t62_41 = _mm256_mul_pd(_t62_5, _t62_38);
  _t62_42 = _mm256_mul_pd(_t62_4, _t62_38);

  // 4-BLAC: 4x4 - 4x4
  _t62_43 = _mm256_sub_pd(_t62_32, _t62_39);
  _t62_44 = _mm256_sub_pd(_t62_33, _t62_40);
  _t62_45 = _mm256_sub_pd(_t62_34, _t62_41);
  _t62_46 = _mm256_sub_pd(_t62_35, _t62_42);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t62_13 = _t62_43;
  _t62_14 = _t62_44;
  _t62_15 = _t62_45;

  // Generating : M3[24,24] = S(h(1, 24, 21), Sqrt( G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ),h(1, 24, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_47 = _t62_16;

  // 4-BLAC: sqrt(1x4)
  _t62_48 = _mm256_sqrt_pd(_t62_47);

  // AVX Storer:
  _t62_16 = _t62_48;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ),h(1, 24, 21))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t62_49 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_50 = _t62_16;

  // 4-BLAC: 1x4 / 1x4
  _t62_51 = _mm256_div_pd(_t62_49, _t62_50);

  // AVX Storer:
  _t62_17 = _t62_51;

  // Generating : M3[24,24] = S(h(1, 24, 21), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 21)) Kro G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ),h(2, 24, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_52 = _t62_8;

  // AVX Loader:

  // 1x2 -> 1x4
  _t62_53 = _t62_18;

  // 4-BLAC: 1x4 Kro 1x4
  _t62_54 = _mm256_mul_pd(_t62_52, _t62_53);

  // AVX Storer:
  _t62_18 = _t62_54;

  // Generating : M3[24,24] = S(h(2, 24, 22), ( G(h(2, 24, 22), M3[24,24],h(2, 24, 22)) - ( T( G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) * G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) ),h(2, 24, 22))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t62_55 = _t62_19;
  _t62_56 = _mm256_shuffle_pd(_t62_19, _t62_20, 3);
  _t62_57 = _mm256_setzero_pd();
  _t62_58 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t62_59 = _t62_18;

  // 4-BLAC: (1x4)^T
  _t62_60 = _t62_59;

  // AVX Loader:

  // 1x2 -> 1x4
  _t62_61 = _t62_18;

  // 4-BLAC: 4x1 * 1x4
  _t62_62 = _mm256_mul_pd(_t62_3, _t62_61);
  _t62_63 = _mm256_mul_pd(_t62_2, _t62_61);
  _t62_64 = _mm256_mul_pd(_t62_1, _t62_61);
  _t62_65 = _mm256_mul_pd(_t62_0, _t62_61);

  // 4-BLAC: 4x4 - 4x4
  _t62_66 = _mm256_sub_pd(_t62_55, _t62_62);
  _t62_67 = _mm256_sub_pd(_t62_56, _t62_63);
  _t62_68 = _mm256_sub_pd(_t62_57, _t62_64);
  _t62_69 = _mm256_sub_pd(_t62_58, _t62_65);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t62_19 = _t62_66;
  _t62_20 = _t62_67;

  // Generating : M3[24,24] = S(h(1, 24, 22), Sqrt( G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ),h(1, 24, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_70 = _t62_21;

  // 4-BLAC: sqrt(1x4)
  _t62_71 = _mm256_sqrt_pd(_t62_70);

  // AVX Storer:
  _t62_21 = _t62_71;

  // Generating : M3[24,24] = S(h(1, 24, 22), ( G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) Div G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ),h(1, 24, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_72 = _t62_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_73 = _t62_21;

  // 4-BLAC: 1x4 / 1x4
  _t62_74 = _mm256_div_pd(_t62_72, _t62_73);

  // AVX Storer:
  _t62_22 = _t62_74;

  // Generating : M3[24,24] = S(h(1, 24, 23), ( G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) - ( T( G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) Kro G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) ),h(1, 24, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_75 = _t62_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_76 = _t62_22;

  // 4-BLAC: (4x1)^T
  _t62_77 = _t62_76;

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_78 = _t62_22;

  // 4-BLAC: 1x4 Kro 1x4
  _t62_79 = _mm256_mul_pd(_t62_77, _t62_78);

  // 4-BLAC: 1x4 - 1x4
  _t62_80 = _mm256_sub_pd(_t62_75, _t62_79);

  // AVX Storer:
  _t62_23 = _t62_80;

  // Generating : M3[24,24] = S(h(1, 24, 23), Sqrt( G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ),h(1, 24, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_81 = _t62_23;

  // 4-BLAC: sqrt(1x4)
  _t62_82 = _mm256_sqrt_pd(_t62_81);

  // AVX Storer:
  _t62_23 = _t62_82;

  _mm256_maskstore_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_21);
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_23);
  _mm256_maskstore_pd(M3 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_27);
  _mm256_maskstore_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_29);
  _mm256_maskstore_pd(M3 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_32);
  _mm256_maskstore_pd(M3 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_34);

  for( int fi39 = 0; fi39 <= 19; fi39+=4 ) {
    _t63_8 = _mm256_maskload_pd(M3 + 25*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_2 = _mm256_maskload_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_4 = _mm256_maskload_pd(M3 + 25*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t63_7 = _mm256_maskload_pd(M3 + 25*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t63_5 = _mm256_maskload_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_0 = _mm256_maskload_pd(M3 + 25*fi39 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_1 = _mm256_maskload_pd(M3 + 25*fi39 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_9 = _mm256_maskload_pd(v0 + fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_8 = _mm256_maskload_pd(M3 + 25*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_10 = _mm256_maskload_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t63_7 = _mm256_maskload_pd(M3 + 25*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t63_6 = _mm256_broadcast_sd(&(v0[fi39]));
    _t63_11 = _mm256_maskload_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_5 = _mm256_maskload_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_12 = _mm256_maskload_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t63_4 = _mm256_maskload_pd(M3 + 25*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t63_3 = _mm256_broadcast_sd(&(v0[fi39 + 1]));
    _t63_13 = _mm256_maskload_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_2 = _mm256_maskload_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_14 = _mm256_maskload_pd(v0 + fi39 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_1 = _mm256_maskload_pd(M3 + 25*fi39 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t63_0 = _mm256_maskload_pd(M3 + 25*fi39 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

    // Generating : v0[24,1] = S(h(1, 24, fi39), ( G(h(1, 24, fi39), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_15 = _t63_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_16 = _t63_8;

    // 4-BLAC: 1x4 / 1x4
    _t63_17 = _mm256_div_pd(_t63_15, _t63_16);

    // AVX Storer:
    _t63_9 = _t63_17;

    // Generating : v0[24,1] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), v0[24,1],h(1, 1, 0)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) Kro G(h(1, 24, fi39), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t63_18 = _t63_10;

    // AVX Loader:

    // 1x3 -> 1x4
    _t63_19 = _t63_7;

    // 4-BLAC: (1x4)^T
    _t63_20 = _t63_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_21 = _t63_6;

    // 4-BLAC: 4x1 Kro 1x4
    _t63_22 = _mm256_mul_pd(_t63_20, _t63_21);

    // 4-BLAC: 4x1 - 4x1
    _t63_23 = _mm256_sub_pd(_t63_18, _t63_22);

    // AVX Storer:
    _t63_10 = _t63_23;

    // Generating : v0[24,1] = S(h(1, 24, fi39 + 1), ( G(h(1, 24, fi39 + 1), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_24 = _t63_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_25 = _t63_5;

    // 4-BLAC: 1x4 / 1x4
    _t63_26 = _mm256_div_pd(_t63_24, _t63_25);

    // AVX Storer:
    _t63_11 = _t63_26;

    // Generating : v0[24,1] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), v0[24,1],h(1, 1, 0)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) Kro G(h(1, 24, fi39 + 1), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t63_27 = _t63_12;

    // AVX Loader:

    // 1x2 -> 1x4
    _t63_28 = _t63_4;

    // 4-BLAC: (1x4)^T
    _t63_29 = _t63_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_30 = _t63_3;

    // 4-BLAC: 4x1 Kro 1x4
    _t63_31 = _mm256_mul_pd(_t63_29, _t63_30);

    // 4-BLAC: 4x1 - 4x1
    _t63_32 = _mm256_sub_pd(_t63_27, _t63_31);

    // AVX Storer:
    _t63_12 = _t63_32;

    // Generating : v0[24,1] = S(h(1, 24, fi39 + 2), ( G(h(1, 24, fi39 + 2), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_33 = _t63_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_34 = _t63_2;

    // 4-BLAC: 1x4 / 1x4
    _t63_35 = _mm256_div_pd(_t63_33, _t63_34);

    // AVX Storer:
    _t63_13 = _t63_35;

    // Generating : v0[24,1] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), v0[24,1],h(1, 1, 0)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_36 = _t63_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_37 = _t63_1;

    // 4-BLAC: (4x1)^T
    _t63_38 = _t63_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_39 = _t63_13;

    // 4-BLAC: 1x4 Kro 1x4
    _t63_40 = _mm256_mul_pd(_t63_38, _t63_39);

    // 4-BLAC: 1x4 - 1x4
    _t63_41 = _mm256_sub_pd(_t63_36, _t63_40);

    // AVX Storer:
    _t63_14 = _t63_41;

    // Generating : v0[24,1] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_42 = _t63_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_43 = _t63_0;

    // 4-BLAC: 1x4 / 1x4
    _t63_44 = _mm256_div_pd(_t63_42, _t63_43);

    // AVX Storer:
    _t63_14 = _t63_44;

    // Generating : v0[24,1] = Sum_{i0} ( S(h(4, 24, fi39 + i0 + 4), ( G(h(4, 24, fi39 + i0 + 4), v0[24,1],h(1, 1, 0)) - ( T( G(h(4, 24, fi39), M3[24,24],h(4, 24, fi39 + i0 + 4)) ) * G(h(4, 24, fi39), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm256_maskstore_pd(v0 + fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t63_9);
    _mm256_maskstore_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t63_10);
    _mm256_maskstore_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t63_12);

    for( int i0 = 0; i0 <= -fi39 + 19; i0+=4 ) {
      _t64_6 = _mm256_loadu_pd(v0 + fi39 + i0 + 4);
      _t64_4 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 4);
      _t64_3 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 28);
      _t64_2 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 52);
      _t64_1 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 76);
      _t64_0 = _mm256_maskload_pd(v0 + fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t64_7 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_4, _t64_3), _mm256_unpacklo_pd(_t64_2, _t64_1), 32);
      _t64_8 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t64_4, _t64_3), _mm256_unpackhi_pd(_t64_2, _t64_1), 32);
      _t64_9 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_4, _t64_3), _mm256_unpacklo_pd(_t64_2, _t64_1), 49);
      _t64_10 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t64_4, _t64_3), _mm256_unpackhi_pd(_t64_2, _t64_1), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t64_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t64_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32)), _mm256_mul_pd(_t64_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t64_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32)), _mm256_mul_pd(_t64_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t64_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32)), _mm256_mul_pd(_t64_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t64_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32)), _mm256_mul_pd(_t64_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_0, _t63_11), _mm256_unpacklo_pd(_t63_13, _t63_14), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t64_6 = _mm256_sub_pd(_t64_6, _t64_5);

      // AVX Storer:
      _mm256_storeu_pd(v0 + fi39 + i0 + 4, _t64_6);
    }
    _mm256_maskstore_pd(v0 + fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t63_11);
    _mm256_maskstore_pd(v0 + fi39 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t63_13);
    _mm256_maskstore_pd(v0 + fi39 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t63_14);
  }

  _t65_2 = _mm256_maskload_pd(v0 + 20, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t65_3 = _mm256_maskload_pd(v0 + 21, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t65_1 = _mm256_broadcast_sd(&(v0[20]));
  _t65_4 = _mm256_maskload_pd(v0 + 21, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t65_5 = _mm256_maskload_pd(v0 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t65_0 = _mm256_broadcast_sd(&(v0[21]));
  _t65_6 = _mm256_maskload_pd(v0 + 22, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t65_7 = _mm256_maskload_pd(v0 + 23, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

  // Generating : v0[24,1] = S(h(1, 24, 20), ( G(h(1, 24, 20), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 20), M3[24,24],h(1, 24, 20)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_8 = _t65_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_9 = _t62_10;

  // 4-BLAC: 1x4 / 1x4
  _t65_10 = _mm256_div_pd(_t65_8, _t65_9);

  // AVX Storer:
  _t65_2 = _t65_10;

  // Generating : v0[24,1] = S(h(3, 24, 21), ( G(h(3, 24, 21), v0[24,1],h(1, 1, 0)) - ( T( G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) Kro G(h(1, 24, 20), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t65_11 = _t65_3;

  // AVX Loader:

  // 1x3 -> 1x4
  _t65_12 = _t62_12;

  // 4-BLAC: (1x4)^T
  _t65_13 = _t65_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_14 = _t65_1;

  // 4-BLAC: 4x1 Kro 1x4
  _t65_15 = _mm256_mul_pd(_t65_13, _t65_14);

  // 4-BLAC: 4x1 - 4x1
  _t65_16 = _mm256_sub_pd(_t65_11, _t65_15);

  // AVX Storer:
  _t65_3 = _t65_16;

  // Generating : v0[24,1] = S(h(1, 24, 21), ( G(h(1, 24, 21), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 21), M3[24,24],h(1, 24, 21)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_17 = _t65_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_18 = _t62_16;

  // 4-BLAC: 1x4 / 1x4
  _t65_19 = _mm256_div_pd(_t65_17, _t65_18);

  // AVX Storer:
  _t65_4 = _t65_19;

  // Generating : v0[24,1] = S(h(2, 24, 22), ( G(h(2, 24, 22), v0[24,1],h(1, 1, 0)) - ( T( G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) Kro G(h(1, 24, 21), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t65_20 = _t65_5;

  // AVX Loader:

  // 1x2 -> 1x4
  _t65_21 = _t62_18;

  // 4-BLAC: (1x4)^T
  _t65_22 = _t65_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_23 = _t65_0;

  // 4-BLAC: 4x1 Kro 1x4
  _t65_24 = _mm256_mul_pd(_t65_22, _t65_23);

  // 4-BLAC: 4x1 - 4x1
  _t65_25 = _mm256_sub_pd(_t65_20, _t65_24);

  // AVX Storer:
  _t65_5 = _t65_25;

  // Generating : v0[24,1] = S(h(1, 24, 22), ( G(h(1, 24, 22), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_26 = _t65_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_27 = _t62_21;

  // 4-BLAC: 1x4 / 1x4
  _t65_28 = _mm256_div_pd(_t65_26, _t65_27);

  // AVX Storer:
  _t65_6 = _t65_28;

  // Generating : v0[24,1] = S(h(1, 24, 23), ( G(h(1, 24, 23), v0[24,1],h(1, 1, 0)) - ( T( G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) Kro G(h(1, 24, 22), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_29 = _t65_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_30 = _t62_22;

  // 4-BLAC: (4x1)^T
  _t65_31 = _t65_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_32 = _t65_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t65_33 = _mm256_mul_pd(_t65_31, _t65_32);

  // 4-BLAC: 1x4 - 1x4
  _t65_34 = _mm256_sub_pd(_t65_29, _t65_33);

  // AVX Storer:
  _t65_7 = _t65_34;

  // Generating : v0[24,1] = S(h(1, 24, 23), ( G(h(1, 24, 23), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_35 = _t65_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_36 = _t62_23;

  // 4-BLAC: 1x4 / 1x4
  _t65_37 = _mm256_div_pd(_t65_35, _t65_36);

  // AVX Storer:
  _t65_7 = _t65_37;

  _mm256_maskstore_pd(M3 + 500, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_10);
  _mm256_maskstore_pd(M3 + 501, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t62_12);
  _mm256_maskstore_pd(M3 + 525, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_16);
  _mm256_maskstore_pd(M3 + 526, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t62_18);
  _mm256_maskstore_pd(M3 + 550, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_21);
  _mm256_maskstore_pd(M3 + 551, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_22);
  _mm256_maskstore_pd(M3 + 575, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t62_23);
  _mm256_maskstore_pd(v0 + 20, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t65_2);
  _mm256_maskstore_pd(v0 + 21, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t65_4);
  _mm256_maskstore_pd(v0 + 22, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t65_6);
  _mm256_maskstore_pd(v0 + 23, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t65_7);

  for( int fi39 = 0; fi39 <= 19; fi39+=4 ) {
    _t66_9 = _mm256_maskload_pd(v0 + -fi39 + 23, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_8 = _mm256_maskload_pd(M3 + -25*fi39 + 575, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_10 = _mm256_maskload_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t66_7 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(M3 + -25*fi39 + 503, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -25*fi39 + 527, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(M3 + -25*fi39 + 551, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t66_6 = _mm256_broadcast_sd(&(v0[-fi39 + 23]));
    _t66_11 = _mm256_maskload_pd(v0 + -fi39 + 22, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_5 = _mm256_maskload_pd(M3 + -25*fi39 + 550, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_12 = _mm256_maskload_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t66_4 = _mm256_shuffle_pd(_mm256_maskload_pd(M3 + -25*fi39 + 502, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -25*fi39 + 526, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);
    _t66_3 = _mm256_broadcast_sd(&(v0[-fi39 + 22]));
    _t66_13 = _mm256_maskload_pd(v0 + -fi39 + 21, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_2 = _mm256_maskload_pd(M3 + -25*fi39 + 525, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_14 = _mm256_maskload_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_1 = _mm256_maskload_pd(M3 + -25*fi39 + 501, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t66_0 = _mm256_maskload_pd(M3 + -25*fi39 + 500, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

    // Generating : v0[24,1] = S(h(1, 24, -fi39 + 23), ( G(h(1, 24, -fi39 + 23), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 23), M3[24,24],h(1, 24, -fi39 + 23)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_15 = _t66_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_16 = _t66_8;

    // 4-BLAC: 1x4 / 1x4
    _t66_17 = _mm256_div_pd(_t66_15, _t66_16);

    // AVX Storer:
    _t66_9 = _t66_17;

    // Generating : v0[24,1] = S(h(3, 24, -fi39 + 20), ( G(h(3, 24, -fi39 + 20), v0[24,1],h(1, 1, 0)) - ( G(h(3, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 23)) Kro G(h(1, 24, -fi39 + 23), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t66_18 = _t66_10;

    // AVX Loader:

    // 3x1 -> 4x1
    _t66_19 = _t66_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_20 = _t66_6;

    // 4-BLAC: 4x1 Kro 1x4
    _t66_21 = _mm256_mul_pd(_t66_19, _t66_20);

    // 4-BLAC: 4x1 - 4x1
    _t66_22 = _mm256_sub_pd(_t66_18, _t66_21);

    // AVX Storer:
    _t66_10 = _t66_22;

    // Generating : v0[24,1] = S(h(1, 24, -fi39 + 22), ( G(h(1, 24, -fi39 + 22), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 22), M3[24,24],h(1, 24, -fi39 + 22)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_23 = _t66_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_24 = _t66_5;

    // 4-BLAC: 1x4 / 1x4
    _t66_25 = _mm256_div_pd(_t66_23, _t66_24);

    // AVX Storer:
    _t66_11 = _t66_25;

    // Generating : v0[24,1] = S(h(2, 24, -fi39 + 20), ( G(h(2, 24, -fi39 + 20), v0[24,1],h(1, 1, 0)) - ( G(h(2, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 22)) Kro G(h(1, 24, -fi39 + 22), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t66_26 = _t66_12;

    // AVX Loader:

    // 2x1 -> 4x1
    _t66_27 = _t66_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_28 = _t66_3;

    // 4-BLAC: 4x1 Kro 1x4
    _t66_29 = _mm256_mul_pd(_t66_27, _t66_28);

    // 4-BLAC: 4x1 - 4x1
    _t66_30 = _mm256_sub_pd(_t66_26, _t66_29);

    // AVX Storer:
    _t66_12 = _t66_30;

    // Generating : v0[24,1] = S(h(1, 24, -fi39 + 21), ( G(h(1, 24, -fi39 + 21), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 21), M3[24,24],h(1, 24, -fi39 + 21)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_31 = _t66_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_32 = _t66_2;

    // 4-BLAC: 1x4 / 1x4
    _t66_33 = _mm256_div_pd(_t66_31, _t66_32);

    // AVX Storer:
    _t66_13 = _t66_33;

    // Generating : v0[24,1] = S(h(1, 24, -fi39 + 20), ( G(h(1, 24, -fi39 + 20), v0[24,1],h(1, 1, 0)) - ( G(h(1, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 21)) Kro G(h(1, 24, -fi39 + 21), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_34 = _t66_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_35 = _t66_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_36 = _t66_13;

    // 4-BLAC: 1x4 Kro 1x4
    _t66_37 = _mm256_mul_pd(_t66_35, _t66_36);

    // 4-BLAC: 1x4 - 1x4
    _t66_38 = _mm256_sub_pd(_t66_34, _t66_37);

    // AVX Storer:
    _t66_14 = _t66_38;

    // Generating : v0[24,1] = S(h(1, 24, -fi39 + 20), ( G(h(1, 24, -fi39 + 20), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 20)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_39 = _t66_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_40 = _t66_0;

    // 4-BLAC: 1x4 / 1x4
    _t66_41 = _mm256_div_pd(_t66_39, _t66_40);

    // AVX Storer:
    _t66_14 = _t66_41;

    // Generating : v0[24,1] = Sum_{i0} ( S(h(4, 24, i0), ( G(h(4, 24, i0), v0[24,1],h(1, 1, 0)) - ( G(h(4, 24, i0), M3[24,24],h(4, 24, -fi39 + 20)) * G(h(4, 24, -fi39 + 20), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm256_maskstore_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t66_10);
    _mm256_maskstore_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t66_12);
    _mm256_maskstore_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t66_14);

    for( int i0 = 0; i0 <= -fi39 + 19; i0+=4 ) {
      _t67_6 = _mm256_loadu_pd(v0 + i0);
      _t67_4 = _mm256_loadu_pd(M3 + -fi39 + 24*i0 + 20);
      _t67_3 = _mm256_loadu_pd(M3 + -fi39 + 24*i0 + 44);
      _t67_2 = _mm256_loadu_pd(M3 + -fi39 + 24*i0 + 68);
      _t67_1 = _mm256_loadu_pd(M3 + -fi39 + 24*i0 + 92);
      _t67_0 = _mm256_maskload_pd(v0 + -fi39 + 20, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t67_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t67_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32)), _mm256_mul_pd(_t67_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t67_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32)), _mm256_mul_pd(_t67_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t67_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32)), _mm256_mul_pd(_t67_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t67_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32)), _mm256_mul_pd(_t67_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_0, _t66_13), _mm256_unpacklo_pd(_t66_11, _t66_9), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t67_6 = _mm256_sub_pd(_t67_6, _t67_5);

      // AVX Storer:
      _mm256_storeu_pd(v0 + i0, _t67_6);
    }
    _mm256_maskstore_pd(v0 + -fi39 + 23, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t66_9);
    _mm256_maskstore_pd(v0 + -fi39 + 22, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t66_11);
    _mm256_maskstore_pd(v0 + -fi39 + 21, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t66_13);
  }

  _t56_32 = _mm256_maskload_pd(M3 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_34 = _mm256_maskload_pd(M3 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_33 = _mm256_maskload_pd(M3 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_23 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_21 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_29 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_27 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t68_3 = _mm256_maskload_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t68_4 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t68_2 = _mm256_broadcast_sd(&(v0[3]));
  _t68_5 = _mm256_maskload_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t68_6 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t68_1 = _mm256_broadcast_sd(&(v0[2]));
  _t68_7 = _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t68_8 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t68_0 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

  // Generating : v0[24,1] = S(h(1, 24, 3), ( G(h(1, 24, 3), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 3), M3[24,24],h(1, 24, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_9 = _t68_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_10 = _t56_34;

  // 4-BLAC: 1x4 / 1x4
  _t68_11 = _mm256_div_pd(_t68_9, _t68_10);

  // AVX Storer:
  _t68_3 = _t68_11;

  // Generating : v0[24,1] = S(h(3, 24, 0), ( G(h(3, 24, 0), v0[24,1],h(1, 1, 0)) - ( G(h(3, 24, 0), M3[24,24],h(1, 24, 3)) Kro G(h(1, 24, 3), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t68_12 = _t68_4;

  // AVX Loader:

  // 3x1 -> 4x1
  _t68_13 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_23, _t56_33, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_29, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_14 = _t68_2;

  // 4-BLAC: 4x1 Kro 1x4
  _t68_15 = _mm256_mul_pd(_t68_13, _t68_14);

  // 4-BLAC: 4x1 - 4x1
  _t68_16 = _mm256_sub_pd(_t68_12, _t68_15);

  // AVX Storer:
  _t68_4 = _t68_16;

  // Generating : v0[24,1] = S(h(1, 24, 2), ( G(h(1, 24, 2), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 2), M3[24,24],h(1, 24, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_17 = _t68_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_18 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t68_19 = _mm256_div_pd(_t68_17, _t68_18);

  // AVX Storer:
  _t68_5 = _t68_19;

  // Generating : v0[24,1] = S(h(2, 24, 0), ( G(h(2, 24, 0), v0[24,1],h(1, 1, 0)) - ( G(h(2, 24, 0), M3[24,24],h(1, 24, 2)) Kro G(h(1, 24, 2), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t68_20 = _t68_6;

  // AVX Loader:

  // 2x1 -> 4x1
  _t68_21 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_29, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_22 = _t68_1;

  // 4-BLAC: 4x1 Kro 1x4
  _t68_23 = _mm256_mul_pd(_t68_21, _t68_22);

  // 4-BLAC: 4x1 - 4x1
  _t68_24 = _mm256_sub_pd(_t68_20, _t68_23);

  // AVX Storer:
  _t68_6 = _t68_24;

  // Generating : v0[24,1] = S(h(1, 24, 1), ( G(h(1, 24, 1), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 1), M3[24,24],h(1, 24, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_25 = _t68_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_26 = _t56_27;

  // 4-BLAC: 1x4 / 1x4
  _t68_27 = _mm256_div_pd(_t68_25, _t68_26);

  // AVX Storer:
  _t68_7 = _t68_27;

  // Generating : v0[24,1] = S(h(1, 24, 0), ( G(h(1, 24, 0), v0[24,1],h(1, 1, 0)) - ( G(h(1, 24, 0), M3[24,24],h(1, 24, 1)) Kro G(h(1, 24, 1), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_28 = _t68_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_29 = _t68_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_30 = _t68_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t68_31 = _mm256_mul_pd(_t68_29, _t68_30);

  // 4-BLAC: 1x4 - 1x4
  _t68_32 = _mm256_sub_pd(_t68_28, _t68_31);

  // AVX Storer:
  _t68_8 = _t68_32;

  // Generating : v0[24,1] = S(h(1, 24, 0), ( G(h(1, 24, 0), v0[24,1],h(1, 1, 0)) Div G(h(1, 24, 0), M3[24,24],h(1, 24, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_33 = _t68_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_34 = _t56_21;

  // 4-BLAC: 1x4 / 1x4
  _t68_35 = _mm256_div_pd(_t68_33, _t68_34);

  // AVX Storer:
  _t68_8 = _t68_35;


  for( int fi39 = 0; fi39 <= 19; fi39+=4 ) {
    _t69_18 = _mm256_maskload_pd(M3 + 25*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_17 = _mm256_maskload_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_16 = _mm256_maskload_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_11 = _mm256_maskload_pd(M3 + 25*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t69_13 = _mm256_maskload_pd(M3 + 25*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t69_15 = _mm256_maskload_pd(M3 + 25*fi39 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_18 = _mm256_maskload_pd(M3 + 25*fi39, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_17 = _mm256_maskload_pd(M3 + 25*fi39 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_16 = _mm256_maskload_pd(M3 + 25*fi39 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_15 = _mm256_maskload_pd(M3 + 25*fi39 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t69_26 = _mm256_loadu_pd(M1 + 28*fi39);
    _t69_23 = _mm256_loadu_pd(M1 + 28*fi39 + 28);
    _t69_24 = _mm256_loadu_pd(M1 + 28*fi39 + 56);
    _t69_25 = _mm256_loadu_pd(M1 + 28*fi39 + 84);
    _t69_13 = _mm256_maskload_pd(M3 + 25*fi39 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t69_11 = _mm256_maskload_pd(M3 + 25*fi39 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t69_9 = _mm256_broadcast_sd(&(M3[25*fi39 + 51]));

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39), M3[24,24],h(1, 24, fi39)) ),h(1, 24, fi39))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t69_28 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_29 = _t69_18;

    // 4-BLAC: 1x4 / 1x4
    _t69_30 = _mm256_div_pd(_t69_28, _t69_29);

    // AVX Storer:
    _t69_19 = _t69_30;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 1), M3[24,24],h(1, 24, fi39 + 1)) ),h(1, 24, fi39 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t69_31 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_32 = _t69_17;

    // 4-BLAC: 1x4 / 1x4
    _t69_33 = _mm256_div_pd(_t69_31, _t69_32);

    // AVX Storer:
    _t69_20 = _t69_33;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 2)) ),h(1, 24, fi39 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t69_34 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_35 = _t69_16;

    // 4-BLAC: 1x4 / 1x4
    _t69_36 = _mm256_div_pd(_t69_34, _t69_35);

    // AVX Storer:
    _t69_21 = _t69_36;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, fi39 + 3), M3[24,24],h(1, 24, fi39 + 3)) ),h(1, 24, fi39 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t69_37 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_38 = _t69_15;

    // 4-BLAC: 1x4 / 1x4
    _t69_39 = _mm256_div_pd(_t69_37, _t69_38);

    // AVX Storer:
    _t69_22 = _t69_39;

    // Generating : M1[24,28] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_40 = _t69_14;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_26 = _mm256_mul_pd(_t69_40, _t69_26);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M1[24,28],h(4, 28, fi100)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 3x4 -> 4x4
    _t69_41 = _t69_23;
    _t69_42 = _t69_24;
    _t69_43 = _t69_25;
    _t69_44 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t69_45 = _t69_13;

    // 4-BLAC: (1x4)^T
    _t69_46 = _t69_45;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t69_47 = _mm256_mul_pd(_t69_7, _t69_26);
    _t69_48 = _mm256_mul_pd(_t69_6, _t69_26);
    _t69_49 = _mm256_mul_pd(_t69_5, _t69_26);
    _t69_50 = _mm256_mul_pd(_t69_4, _t69_26);

    // 4-BLAC: 4x4 - 4x4
    _t69_51 = _mm256_sub_pd(_t69_41, _t69_47);
    _t69_52 = _mm256_sub_pd(_t69_42, _t69_48);
    _t69_53 = _mm256_sub_pd(_t69_43, _t69_49);
    _t69_54 = _mm256_sub_pd(_t69_44, _t69_50);

    // AVX Storer:
    _t69_23 = _t69_51;
    _t69_24 = _t69_52;
    _t69_25 = _t69_53;

    // Generating : M1[24,28] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_55 = _t69_12;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_23 = _mm256_mul_pd(_t69_55, _t69_23);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M1[24,28],h(4, 28, fi100)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 2x4 -> 4x4
    _t69_56 = _t69_24;
    _t69_57 = _t69_25;
    _t69_58 = _mm256_setzero_pd();
    _t69_59 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t69_60 = _t69_11;

    // 4-BLAC: (1x4)^T
    _t69_61 = _t69_60;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t69_62 = _mm256_mul_pd(_t69_3, _t69_23);
    _t69_63 = _mm256_mul_pd(_t69_2, _t69_23);
    _t69_64 = _mm256_mul_pd(_t69_1, _t69_23);
    _t69_65 = _mm256_mul_pd(_t69_0, _t69_23);

    // 4-BLAC: 4x4 - 4x4
    _t69_66 = _mm256_sub_pd(_t69_56, _t69_62);
    _t69_67 = _mm256_sub_pd(_t69_57, _t69_63);
    _t69_68 = _mm256_sub_pd(_t69_58, _t69_64);
    _t69_69 = _mm256_sub_pd(_t69_59, _t69_65);

    // AVX Storer:
    _t69_24 = _t69_66;
    _t69_25 = _t69_67;

    // Generating : M1[24,28] = S(h(1, 24, fi39 + 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) Kro G(h(1, 24, fi39 + 2), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_70 = _t69_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_24 = _mm256_mul_pd(_t69_70, _t69_24);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M1[24,28],h(4, 28, fi100)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_71 = _t69_9;

    // 4-BLAC: (4x1)^T
    _t69_72 = _t69_71;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_27 = _mm256_mul_pd(_t69_72, _t69_24);

    // 4-BLAC: 1x4 - 1x4
    _t69_25 = _mm256_sub_pd(_t69_25, _t69_27);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, fi39 + 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) Kro G(h(1, 24, fi39 + 3), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_73 = _t69_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_25 = _mm256_mul_pd(_t69_73, _t69_25);

    // AVX Storer:

    for( int fi100 = 4; fi100 <= 24; fi100+=4 ) {
      _t70_3 = _mm256_loadu_pd(M1 + fi100 + 28*fi39);
      _t70_0 = _mm256_loadu_pd(M1 + fi100 + 28*fi39 + 28);
      _t70_1 = _mm256_loadu_pd(M1 + fi100 + 28*fi39 + 56);
      _t70_2 = _mm256_loadu_pd(M1 + fi100 + 28*fi39 + 84);

      // Generating : M1[24,28] = S(h(1, 24, fi39), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39)) Kro G(h(1, 24, fi39), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t70_4 = _t69_14;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t70_3 = _mm256_mul_pd(_t70_4, _t70_3);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(3, 24, fi39 + 1), ( G(h(3, 24, fi39 + 1), M1[24,28],h(4, 28, fi100)) - ( T( G(h(1, 24, fi39), M3[24,24],h(3, 24, fi39 + 1)) ) * G(h(1, 24, fi39), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 3x4 -> 4x4
      _t70_5 = _t70_0;
      _t70_6 = _t70_1;
      _t70_7 = _t70_2;
      _t70_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t70_9 = _t69_13;

      // 4-BLAC: (1x4)^T
      _t70_10 = _t70_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t69_47 = _mm256_mul_pd(_t69_7, _t70_3);
      _t69_48 = _mm256_mul_pd(_t69_6, _t70_3);
      _t69_49 = _mm256_mul_pd(_t69_5, _t70_3);
      _t69_50 = _mm256_mul_pd(_t69_4, _t70_3);

      // 4-BLAC: 4x4 - 4x4
      _t70_11 = _mm256_sub_pd(_t70_5, _t69_47);
      _t70_12 = _mm256_sub_pd(_t70_6, _t69_48);
      _t70_13 = _mm256_sub_pd(_t70_7, _t69_49);
      _t70_14 = _mm256_sub_pd(_t70_8, _t69_50);

      // AVX Storer:
      _t70_0 = _t70_11;
      _t70_1 = _t70_12;
      _t70_2 = _t70_13;

      // Generating : M1[24,28] = S(h(1, 24, fi39 + 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 1)) Kro G(h(1, 24, fi39 + 1), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t70_15 = _t69_12;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t70_0 = _mm256_mul_pd(_t70_15, _t70_0);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(2, 24, fi39 + 2), ( G(h(2, 24, fi39 + 2), M1[24,28],h(4, 28, fi100)) - ( T( G(h(1, 24, fi39 + 1), M3[24,24],h(2, 24, fi39 + 2)) ) * G(h(1, 24, fi39 + 1), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 2x4 -> 4x4
      _t70_16 = _t70_1;
      _t70_17 = _t70_2;
      _t70_18 = _mm256_setzero_pd();
      _t70_19 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t70_20 = _t69_11;

      // 4-BLAC: (1x4)^T
      _t70_21 = _t70_20;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t69_62 = _mm256_mul_pd(_t69_3, _t70_0);
      _t69_63 = _mm256_mul_pd(_t69_2, _t70_0);
      _t69_64 = _mm256_mul_pd(_t69_1, _t70_0);
      _t69_65 = _mm256_mul_pd(_t69_0, _t70_0);

      // 4-BLAC: 4x4 - 4x4
      _t70_22 = _mm256_sub_pd(_t70_16, _t69_62);
      _t70_23 = _mm256_sub_pd(_t70_17, _t69_63);
      _t70_24 = _mm256_sub_pd(_t70_18, _t69_64);
      _t70_25 = _mm256_sub_pd(_t70_19, _t69_65);

      // AVX Storer:
      _t70_1 = _t70_22;
      _t70_2 = _t70_23;

      // Generating : M1[24,28] = S(h(1, 24, fi39 + 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 2)) Kro G(h(1, 24, fi39 + 2), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t70_26 = _t69_10;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t70_1 = _mm256_mul_pd(_t70_26, _t70_1);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(1, 24, fi39 + 3), ( G(h(1, 24, fi39 + 3), M1[24,28],h(4, 28, fi100)) - ( T( G(h(1, 24, fi39 + 2), M3[24,24],h(1, 24, fi39 + 3)) ) Kro G(h(1, 24, fi39 + 2), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t70_27 = _t69_9;

      // 4-BLAC: (4x1)^T
      _t69_72 = _t70_27;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t69_27 = _mm256_mul_pd(_t69_72, _t70_1);

      // 4-BLAC: 1x4 - 1x4
      _t70_2 = _mm256_sub_pd(_t70_2, _t69_27);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(1, 24, fi39 + 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, fi39 + 3)) Kro G(h(1, 24, fi39 + 3), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t70_28 = _t69_8;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t70_2 = _mm256_mul_pd(_t70_28, _t70_2);

      // AVX Storer:
      _mm256_storeu_pd(M1 + fi100 + 28*fi39, _t70_3);
      _mm256_storeu_pd(M1 + fi100 + 28*fi39 + 28, _t70_0);
      _mm256_storeu_pd(M1 + fi100 + 28*fi39 + 56, _t70_1);
      _mm256_storeu_pd(M1 + fi100 + 28*fi39 + 84, _t70_2);
    }

    // Generating : M1[24,28] = Sum_{i0} ( Sum_{k3} ( S(h(4, 24, fi39 + i0 + 4), ( G(h(4, 24, fi39 + i0 + 4), M1[24,28],h(4, 28, k3)) - ( T( G(h(4, 24, fi39), M3[24,24],h(4, 24, fi39 + i0 + 4)) ) * G(h(4, 24, fi39), M1[24,28],h(4, 28, k3)) ) ),h(4, 28, k3)) ) )
    _mm256_storeu_pd(M1 + 28*fi39, _t69_26);
    _mm256_storeu_pd(M1 + 28*fi39 + 28, _t69_23);
    _mm256_storeu_pd(M1 + 28*fi39 + 56, _t69_24);
    _mm256_storeu_pd(M1 + 28*fi39 + 84, _t69_25);

    for( int i0 = 0; i0 <= -fi39 + 19; i0+=4 ) {

      for( int k3 = 0; k3 <= 27; k3+=4 ) {
        _t71_28 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 112);
        _t71_29 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 140);
        _t71_30 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 168);
        _t71_31 = _mm256_loadu_pd(M1 + 28*fi39 + 28*i0 + k3 + 196);
        _t71_23 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 4);
        _t71_22 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 28);
        _t71_21 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 52);
        _t71_20 = _mm256_loadu_pd(M3 + 25*fi39 + i0 + 76);
        _t71_19 = _mm256_loadu_pd(M1 + 28*fi39 + k3);
        _t71_18 = _mm256_loadu_pd(M1 + 28*fi39 + k3 + 28);
        _t71_17 = _mm256_loadu_pd(M1 + 28*fi39 + k3 + 56);
        _t71_16 = _mm256_loadu_pd(M1 + 28*fi39 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t71_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_23, _t71_22), _mm256_unpacklo_pd(_t71_21, _t71_20), 32);
        _t71_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t71_23, _t71_22), _mm256_unpackhi_pd(_t71_21, _t71_20), 32);
        _t71_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_23, _t71_22), _mm256_unpacklo_pd(_t71_21, _t71_20), 49);
        _t71_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t71_23, _t71_22), _mm256_unpackhi_pd(_t71_21, _t71_20), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t71_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t71_15, _t71_19), _mm256_mul_pd(_t71_14, _t71_18)), _mm256_add_pd(_mm256_mul_pd(_t71_13, _t71_17), _mm256_mul_pd(_t71_12, _t71_16)));
        _t71_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t71_11, _t71_19), _mm256_mul_pd(_t71_10, _t71_18)), _mm256_add_pd(_mm256_mul_pd(_t71_9, _t71_17), _mm256_mul_pd(_t71_8, _t71_16)));
        _t71_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t71_7, _t71_19), _mm256_mul_pd(_t71_6, _t71_18)), _mm256_add_pd(_mm256_mul_pd(_t71_5, _t71_17), _mm256_mul_pd(_t71_4, _t71_16)));
        _t71_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t71_3, _t71_19), _mm256_mul_pd(_t71_2, _t71_18)), _mm256_add_pd(_mm256_mul_pd(_t71_1, _t71_17), _mm256_mul_pd(_t71_0, _t71_16)));

        // 4-BLAC: 4x4 - 4x4
        _t71_28 = _mm256_sub_pd(_t71_28, _t71_24);
        _t71_29 = _mm256_sub_pd(_t71_29, _t71_25);
        _t71_30 = _mm256_sub_pd(_t71_30, _t71_26);
        _t71_31 = _mm256_sub_pd(_t71_31, _t71_27);

        // AVX Storer:
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 112, _t71_28);
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 140, _t71_29);
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 168, _t71_30);
        _mm256_storeu_pd(M1 + 28*fi39 + 28*i0 + k3 + 196, _t71_31);
      }
    }
  }

  _t62_18 = _mm256_maskload_pd(M3 + 526, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t62_21 = _mm256_maskload_pd(M3 + 550, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_23 = _mm256_maskload_pd(M3 + 575, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_12 = _mm256_maskload_pd(M3 + 501, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t72_16 = _mm256_loadu_pd(M1 + 560);
  _t72_13 = _mm256_loadu_pd(M1 + 588);
  _t72_14 = _mm256_loadu_pd(M1 + 616);
  _t72_15 = _mm256_loadu_pd(M1 + 644);
  _t72_9 = _mm256_broadcast_sd(&(M3[551]));

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 22), M3[24,24],h(1, 24, 22)) ),h(1, 24, 22))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t72_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_19 = _t62_21;

  // 4-BLAC: 1x4 / 1x4
  _t72_20 = _mm256_div_pd(_t72_18, _t72_19);

  // AVX Storer:
  _t72_11 = _t72_20;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 23), M3[24,24],h(1, 24, 23)) ),h(1, 24, 23))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t72_21 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_22 = _t62_23;

  // 4-BLAC: 1x4 / 1x4
  _t72_23 = _mm256_div_pd(_t72_21, _t72_22);

  // AVX Storer:
  _t72_12 = _t72_23;

  // Generating : M1[24,28] = S(h(1, 24, 20), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 20)) Kro G(h(1, 24, 20), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_24 = _t62_9;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t72_16 = _mm256_mul_pd(_t72_24, _t72_16);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(3, 24, 21), ( G(h(3, 24, 21), M1[24,28],h(4, 28, fi39)) - ( T( G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) * G(h(1, 24, 20), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 3x4 -> 4x4
  _t72_25 = _t72_13;
  _t72_26 = _t72_14;
  _t72_27 = _t72_15;
  _t72_28 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t72_29 = _t62_12;

  // 4-BLAC: (1x4)^T
  _t72_30 = _t72_29;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t72_31 = _mm256_mul_pd(_t72_7, _t72_16);
  _t72_32 = _mm256_mul_pd(_t72_6, _t72_16);
  _t72_33 = _mm256_mul_pd(_t72_5, _t72_16);
  _t72_34 = _mm256_mul_pd(_t72_4, _t72_16);

  // 4-BLAC: 4x4 - 4x4
  _t72_35 = _mm256_sub_pd(_t72_25, _t72_31);
  _t72_36 = _mm256_sub_pd(_t72_26, _t72_32);
  _t72_37 = _mm256_sub_pd(_t72_27, _t72_33);
  _t72_38 = _mm256_sub_pd(_t72_28, _t72_34);

  // AVX Storer:
  _t72_13 = _t72_35;
  _t72_14 = _t72_36;
  _t72_15 = _t72_37;

  // Generating : M1[24,28] = S(h(1, 24, 21), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 21)) Kro G(h(1, 24, 21), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_39 = _t62_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t72_13 = _mm256_mul_pd(_t72_39, _t72_13);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(2, 24, 22), ( G(h(2, 24, 22), M1[24,28],h(4, 28, fi39)) - ( T( G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) * G(h(1, 24, 21), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 2x4 -> 4x4
  _t72_40 = _t72_14;
  _t72_41 = _t72_15;
  _t72_42 = _mm256_setzero_pd();
  _t72_43 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t72_44 = _t62_18;

  // 4-BLAC: (1x4)^T
  _t72_45 = _t72_44;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t72_46 = _mm256_mul_pd(_t72_3, _t72_13);
  _t72_47 = _mm256_mul_pd(_t72_2, _t72_13);
  _t72_48 = _mm256_mul_pd(_t72_1, _t72_13);
  _t72_49 = _mm256_mul_pd(_t72_0, _t72_13);

  // 4-BLAC: 4x4 - 4x4
  _t72_50 = _mm256_sub_pd(_t72_40, _t72_46);
  _t72_51 = _mm256_sub_pd(_t72_41, _t72_47);
  _t72_52 = _mm256_sub_pd(_t72_42, _t72_48);
  _t72_53 = _mm256_sub_pd(_t72_43, _t72_49);

  // AVX Storer:
  _t72_14 = _t72_50;
  _t72_15 = _t72_51;

  // Generating : M1[24,28] = S(h(1, 24, 22), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 22)) Kro G(h(1, 24, 22), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_54 = _t72_10;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t72_14 = _mm256_mul_pd(_t72_54, _t72_14);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(1, 24, 23), ( G(h(1, 24, 23), M1[24,28],h(4, 28, fi39)) - ( T( G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) Kro G(h(1, 24, 22), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_55 = _t72_9;

  // 4-BLAC: (4x1)^T
  _t72_56 = _t72_55;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t72_17 = _mm256_mul_pd(_t72_56, _t72_14);

  // 4-BLAC: 1x4 - 1x4
  _t72_15 = _mm256_sub_pd(_t72_15, _t72_17);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(1, 24, 23), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 23)) Kro G(h(1, 24, 23), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_57 = _t72_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t72_15 = _mm256_mul_pd(_t72_57, _t72_15);

  // AVX Storer:


  for( int fi39 = 4; fi39 <= 24; fi39+=4 ) {
    _t73_3 = _mm256_loadu_pd(M1 + fi39 + 560);
    _t73_0 = _mm256_loadu_pd(M1 + fi39 + 588);
    _t73_1 = _mm256_loadu_pd(M1 + fi39 + 616);
    _t73_2 = _mm256_loadu_pd(M1 + fi39 + 644);

    // Generating : M1[24,28] = S(h(1, 24, 20), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 20)) Kro G(h(1, 24, 20), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_4 = _t62_9;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t73_3 = _mm256_mul_pd(_t73_4, _t73_3);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(3, 24, 21), ( G(h(3, 24, 21), M1[24,28],h(4, 28, fi39)) - ( T( G(h(1, 24, 20), M3[24,24],h(3, 24, 21)) ) * G(h(1, 24, 20), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 3x4 -> 4x4
    _t73_5 = _t73_0;
    _t73_6 = _t73_1;
    _t73_7 = _t73_2;
    _t73_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t73_9 = _t62_12;

    // 4-BLAC: (1x4)^T
    _t73_10 = _t73_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t72_31 = _mm256_mul_pd(_t72_7, _t73_3);
    _t72_32 = _mm256_mul_pd(_t72_6, _t73_3);
    _t72_33 = _mm256_mul_pd(_t72_5, _t73_3);
    _t72_34 = _mm256_mul_pd(_t72_4, _t73_3);

    // 4-BLAC: 4x4 - 4x4
    _t73_11 = _mm256_sub_pd(_t73_5, _t72_31);
    _t73_12 = _mm256_sub_pd(_t73_6, _t72_32);
    _t73_13 = _mm256_sub_pd(_t73_7, _t72_33);
    _t73_14 = _mm256_sub_pd(_t73_8, _t72_34);

    // AVX Storer:
    _t73_0 = _t73_11;
    _t73_1 = _t73_12;
    _t73_2 = _t73_13;

    // Generating : M1[24,28] = S(h(1, 24, 21), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 21)) Kro G(h(1, 24, 21), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_15 = _t62_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t73_0 = _mm256_mul_pd(_t73_15, _t73_0);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(2, 24, 22), ( G(h(2, 24, 22), M1[24,28],h(4, 28, fi39)) - ( T( G(h(1, 24, 21), M3[24,24],h(2, 24, 22)) ) * G(h(1, 24, 21), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 2x4 -> 4x4
    _t73_16 = _t73_1;
    _t73_17 = _t73_2;
    _t73_18 = _mm256_setzero_pd();
    _t73_19 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t73_20 = _t62_18;

    // 4-BLAC: (1x4)^T
    _t73_21 = _t73_20;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t72_46 = _mm256_mul_pd(_t72_3, _t73_0);
    _t72_47 = _mm256_mul_pd(_t72_2, _t73_0);
    _t72_48 = _mm256_mul_pd(_t72_1, _t73_0);
    _t72_49 = _mm256_mul_pd(_t72_0, _t73_0);

    // 4-BLAC: 4x4 - 4x4
    _t73_22 = _mm256_sub_pd(_t73_16, _t72_46);
    _t73_23 = _mm256_sub_pd(_t73_17, _t72_47);
    _t73_24 = _mm256_sub_pd(_t73_18, _t72_48);
    _t73_25 = _mm256_sub_pd(_t73_19, _t72_49);

    // AVX Storer:
    _t73_1 = _t73_22;
    _t73_2 = _t73_23;

    // Generating : M1[24,28] = S(h(1, 24, 22), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 22)) Kro G(h(1, 24, 22), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_26 = _t72_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t73_1 = _mm256_mul_pd(_t73_26, _t73_1);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, 23), ( G(h(1, 24, 23), M1[24,28],h(4, 28, fi39)) - ( T( G(h(1, 24, 22), M3[24,24],h(1, 24, 23)) ) Kro G(h(1, 24, 22), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_27 = _t72_9;

    // 4-BLAC: (4x1)^T
    _t72_56 = _t73_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t72_17 = _mm256_mul_pd(_t72_56, _t73_1);

    // 4-BLAC: 1x4 - 1x4
    _t73_2 = _mm256_sub_pd(_t73_2, _t72_17);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, 23), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 23)) Kro G(h(1, 24, 23), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_28 = _t72_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t73_2 = _mm256_mul_pd(_t73_28, _t73_2);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi39 + 560, _t73_3);
    _mm256_storeu_pd(M1 + fi39 + 588, _t73_0);
    _mm256_storeu_pd(M1 + fi39 + 616, _t73_1);
    _mm256_storeu_pd(M1 + fi39 + 644, _t73_2);
  }

  _mm256_storeu_pd(M1 + 560, _t72_16);
  _mm256_storeu_pd(M1 + 588, _t72_13);
  _mm256_storeu_pd(M1 + 616, _t72_14);
  _mm256_storeu_pd(M1 + 644, _t72_15);

  for( int fi39 = 0; fi39 <= 19; fi39+=4 ) {
    _t74_18 = _mm256_maskload_pd(M3 + -25*fi39 + 575, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t74_17 = _mm256_maskload_pd(M3 + -25*fi39 + 550, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t74_16 = _mm256_maskload_pd(M3 + -25*fi39 + 525, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t74_15 = _mm256_maskload_pd(M3 + -25*fi39 + 500, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t74_26 = _mm256_loadu_pd(M1 + -28*fi39 + 644);
    _t74_23 = _mm256_loadu_pd(M1 + -28*fi39 + 560);
    _t74_24 = _mm256_loadu_pd(M1 + -28*fi39 + 588);
    _t74_25 = _mm256_loadu_pd(M1 + -28*fi39 + 616);
    _t74_13 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(M3 + -25*fi39 + 503, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -25*fi39 + 527, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(M3 + -25*fi39 + 551, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t74_11 = _mm256_shuffle_pd(_mm256_maskload_pd(M3 + -25*fi39 + 502, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(M3 + -25*fi39 + 526, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);
    _t74_9 = _mm256_broadcast_sd(&(M3[-25*fi39 + 501]));

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 23), M3[24,24],h(1, 24, -fi39 + 23)) ),h(1, 24, -fi39 + 23))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t74_60 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_61 = _t74_18;

    // 4-BLAC: 1x4 / 1x4
    _t74_62 = _mm256_div_pd(_t74_60, _t74_61);

    // AVX Storer:
    _t74_19 = _t74_62;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 22), M3[24,24],h(1, 24, -fi39 + 22)) ),h(1, 24, -fi39 + 22))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t74_63 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_64 = _t74_17;

    // 4-BLAC: 1x4 / 1x4
    _t74_65 = _mm256_div_pd(_t74_63, _t74_64);

    // AVX Storer:
    _t74_20 = _t74_65;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 21), M3[24,24],h(1, 24, -fi39 + 21)) ),h(1, 24, -fi39 + 21))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t74_66 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_67 = _t74_16;

    // 4-BLAC: 1x4 / 1x4
    _t74_68 = _mm256_div_pd(_t74_66, _t74_67);

    // AVX Storer:
    _t74_21 = _t74_68;

    // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 20)) ),h(1, 24, -fi39 + 20))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t74_69 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_70 = _t74_15;

    // 4-BLAC: 1x4 / 1x4
    _t74_28 = _mm256_div_pd(_t74_69, _t74_70);

    // AVX Storer:
    _t74_22 = _t74_28;

    // Generating : M1[24,28] = S(h(1, 24, -fi39 + 23), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 23)) Kro G(h(1, 24, -fi39 + 23), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_29 = _t74_14;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_26 = _mm256_mul_pd(_t74_29, _t74_26);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(3, 24, -fi39 + 20), ( G(h(3, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) - ( G(h(3, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 23)) * G(h(1, 24, -fi39 + 23), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 3x4 -> 4x4
    _t74_30 = _t74_23;
    _t74_31 = _t74_24;
    _t74_32 = _t74_25;
    _t74_33 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t74_34 = _t74_13;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t74_35 = _mm256_mul_pd(_t74_7, _t74_26);
    _t74_36 = _mm256_mul_pd(_t74_6, _t74_26);
    _t74_37 = _mm256_mul_pd(_t74_5, _t74_26);
    _t74_38 = _mm256_mul_pd(_t74_4, _t74_26);

    // 4-BLAC: 4x4 - 4x4
    _t74_39 = _mm256_sub_pd(_t74_30, _t74_35);
    _t74_40 = _mm256_sub_pd(_t74_31, _t74_36);
    _t74_41 = _mm256_sub_pd(_t74_32, _t74_37);
    _t74_42 = _mm256_sub_pd(_t74_33, _t74_38);

    // AVX Storer:
    _t74_23 = _t74_39;
    _t74_24 = _t74_40;
    _t74_25 = _t74_41;

    // Generating : M1[24,28] = S(h(1, 24, -fi39 + 22), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 22)) Kro G(h(1, 24, -fi39 + 22), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_43 = _t74_12;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_25 = _mm256_mul_pd(_t74_43, _t74_25);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(2, 24, -fi39 + 20), ( G(h(2, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) - ( G(h(2, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 22)) * G(h(1, 24, -fi39 + 22), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // 2x4 -> 4x4
    _t74_44 = _t74_23;
    _t74_45 = _t74_24;
    _t74_46 = _mm256_setzero_pd();
    _t74_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t74_48 = _t74_11;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t74_49 = _mm256_mul_pd(_t74_3, _t74_25);
    _t74_50 = _mm256_mul_pd(_t74_2, _t74_25);
    _t74_51 = _mm256_mul_pd(_t74_1, _t74_25);
    _t74_52 = _mm256_mul_pd(_t74_0, _t74_25);

    // 4-BLAC: 4x4 - 4x4
    _t74_53 = _mm256_sub_pd(_t74_44, _t74_49);
    _t74_54 = _mm256_sub_pd(_t74_45, _t74_50);
    _t74_55 = _mm256_sub_pd(_t74_46, _t74_51);
    _t74_56 = _mm256_sub_pd(_t74_47, _t74_52);

    // AVX Storer:
    _t74_23 = _t74_53;
    _t74_24 = _t74_54;

    // Generating : M1[24,28] = S(h(1, 24, -fi39 + 21), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 21)) Kro G(h(1, 24, -fi39 + 21), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_57 = _t74_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_24 = _mm256_mul_pd(_t74_57, _t74_24);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, -fi39 + 20), ( G(h(1, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) - ( G(h(1, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 21)) Kro G(h(1, 24, -fi39 + 21), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_58 = _t74_9;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_27 = _mm256_mul_pd(_t74_58, _t74_24);

    // 4-BLAC: 1x4 - 1x4
    _t74_23 = _mm256_sub_pd(_t74_23, _t74_27);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, -fi39 + 20), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 20)) Kro G(h(1, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

    // AVX Loader:

    // 1x1 -> 1x4
    _t74_59 = _t74_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_23 = _mm256_mul_pd(_t74_59, _t74_23);

    // AVX Storer:

    for( int fi100 = 4; fi100 <= 24; fi100+=4 ) {
      _t75_11 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 644);
      _t75_8 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 560);
      _t75_9 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 588);
      _t75_10 = _mm256_loadu_pd(M1 + fi100 - 28*fi39 + 616);

      // Generating : M1[24,28] = S(h(1, 24, -fi39 + 23), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 23)) Kro G(h(1, 24, -fi39 + 23), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t75_12 = _t74_14;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t75_11 = _mm256_mul_pd(_t75_12, _t75_11);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(3, 24, -fi39 + 20), ( G(h(3, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) - ( G(h(3, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 23)) * G(h(1, 24, -fi39 + 23), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 3x4 -> 4x4
      _t75_13 = _t75_8;
      _t75_14 = _t75_9;
      _t75_15 = _t75_10;
      _t75_16 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t75_17 = _t74_13;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t74_35 = _mm256_mul_pd(_t75_7, _t75_11);
      _t74_36 = _mm256_mul_pd(_t75_6, _t75_11);
      _t74_37 = _mm256_mul_pd(_t75_5, _t75_11);
      _t74_38 = _mm256_mul_pd(_t75_4, _t75_11);

      // 4-BLAC: 4x4 - 4x4
      _t75_18 = _mm256_sub_pd(_t75_13, _t74_35);
      _t75_19 = _mm256_sub_pd(_t75_14, _t74_36);
      _t75_20 = _mm256_sub_pd(_t75_15, _t74_37);
      _t75_21 = _mm256_sub_pd(_t75_16, _t74_38);

      // AVX Storer:
      _t75_8 = _t75_18;
      _t75_9 = _t75_19;
      _t75_10 = _t75_20;

      // Generating : M1[24,28] = S(h(1, 24, -fi39 + 22), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 22)) Kro G(h(1, 24, -fi39 + 22), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t75_22 = _t74_12;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t75_10 = _mm256_mul_pd(_t75_22, _t75_10);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(2, 24, -fi39 + 20), ( G(h(2, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) - ( G(h(2, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 22)) * G(h(1, 24, -fi39 + 22), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // 2x4 -> 4x4
      _t75_23 = _t75_8;
      _t75_24 = _t75_9;
      _t75_25 = _mm256_setzero_pd();
      _t75_26 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t75_27 = _t74_11;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t74_49 = _mm256_mul_pd(_t75_3, _t75_10);
      _t74_50 = _mm256_mul_pd(_t75_2, _t75_10);
      _t74_51 = _mm256_mul_pd(_t75_1, _t75_10);
      _t74_52 = _mm256_mul_pd(_t75_0, _t75_10);

      // 4-BLAC: 4x4 - 4x4
      _t75_28 = _mm256_sub_pd(_t75_23, _t74_49);
      _t75_29 = _mm256_sub_pd(_t75_24, _t74_50);
      _t75_30 = _mm256_sub_pd(_t75_25, _t74_51);
      _t75_31 = _mm256_sub_pd(_t75_26, _t74_52);

      // AVX Storer:
      _t75_8 = _t75_28;
      _t75_9 = _t75_29;

      // Generating : M1[24,28] = S(h(1, 24, -fi39 + 21), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 21)) Kro G(h(1, 24, -fi39 + 21), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t75_32 = _t74_10;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t75_9 = _mm256_mul_pd(_t75_32, _t75_9);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(1, 24, -fi39 + 20), ( G(h(1, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) - ( G(h(1, 24, -fi39 + 20), M3[24,24],h(1, 24, -fi39 + 21)) Kro G(h(1, 24, -fi39 + 21), M1[24,28],h(4, 28, fi100)) ) ),h(4, 28, fi100))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t75_33 = _t74_9;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t74_27 = _mm256_mul_pd(_t75_33, _t75_9);

      // 4-BLAC: 1x4 - 1x4
      _t75_8 = _mm256_sub_pd(_t75_8, _t74_27);

      // AVX Storer:

      // Generating : M1[24,28] = S(h(1, 24, -fi39 + 20), ( G(h(1, 1, 0), T614[1,24],h(1, 24, -fi39 + 20)) Kro G(h(1, 24, -fi39 + 20), M1[24,28],h(4, 28, fi100)) ),h(4, 28, fi100))

      // AVX Loader:

      // 1x1 -> 1x4
      _t75_34 = _t74_8;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t75_8 = _mm256_mul_pd(_t75_34, _t75_8);

      // AVX Storer:
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 644, _t75_11);
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 560, _t75_8);
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 588, _t75_9);
      _mm256_storeu_pd(M1 + fi100 - 28*fi39 + 616, _t75_10);
    }

    // Generating : M1[24,28] = Sum_{i0} ( Sum_{k3} ( S(h(4, 24, i0), ( G(h(4, 24, i0), M1[24,28],h(4, 28, k3)) - ( G(h(4, 24, i0), M3[24,24],h(4, 24, -fi39 + 20)) * G(h(4, 24, -fi39 + 20), M1[24,28],h(4, 28, k3)) ) ),h(4, 28, k3)) ) )
    _mm256_storeu_pd(M1 + -28*fi39 + 644, _t74_26);
    _mm256_storeu_pd(M1 + -28*fi39 + 616, _t74_25);
    _mm256_storeu_pd(M1 + -28*fi39 + 588, _t74_24);
    _mm256_storeu_pd(M1 + -28*fi39 + 560, _t74_23);

    for( int i0 = 0; i0 <= -fi39 + 19; i0+=4 ) {

      for( int k3 = 0; k3 <= 27; k3+=4 ) {
        _t76_24 = _mm256_loadu_pd(M1 + 28*i0 + k3);
        _t76_25 = _mm256_loadu_pd(M1 + 28*i0 + k3 + 28);
        _t76_26 = _mm256_loadu_pd(M1 + 28*i0 + k3 + 56);
        _t76_27 = _mm256_loadu_pd(M1 + 28*i0 + k3 + 84);
        _t76_19 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 20);
        _t76_18 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 21);
        _t76_17 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 22);
        _t76_16 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 23);
        _t76_15 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 44);
        _t76_14 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 45);
        _t76_13 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 46);
        _t76_12 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 47);
        _t76_11 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 68);
        _t76_10 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 69);
        _t76_9 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 70);
        _t76_8 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 71);
        _t76_7 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 92);
        _t76_6 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 93);
        _t76_5 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 94);
        _t76_4 = _mm256_broadcast_sd(M3 + -fi39 + 24*i0 + 95);
        _t76_3 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 560);
        _t76_2 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 588);
        _t76_1 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 616);
        _t76_0 = _mm256_loadu_pd(M1 + -28*fi39 + k3 + 644);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t76_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t76_19, _t76_3), _mm256_mul_pd(_t76_18, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t76_17, _t76_1), _mm256_mul_pd(_t76_16, _t76_0)));
        _t76_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t76_15, _t76_3), _mm256_mul_pd(_t76_14, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t76_13, _t76_1), _mm256_mul_pd(_t76_12, _t76_0)));
        _t76_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t76_11, _t76_3), _mm256_mul_pd(_t76_10, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t76_9, _t76_1), _mm256_mul_pd(_t76_8, _t76_0)));
        _t76_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t76_7, _t76_3), _mm256_mul_pd(_t76_6, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t76_5, _t76_1), _mm256_mul_pd(_t76_4, _t76_0)));

        // 4-BLAC: 4x4 - 4x4
        _t76_24 = _mm256_sub_pd(_t76_24, _t76_20);
        _t76_25 = _mm256_sub_pd(_t76_25, _t76_21);
        _t76_26 = _mm256_sub_pd(_t76_26, _t76_22);
        _t76_27 = _mm256_sub_pd(_t76_27, _t76_23);

        // AVX Storer:
        _mm256_storeu_pd(M1 + 28*i0 + k3, _t76_24);
        _mm256_storeu_pd(M1 + 28*i0 + k3 + 28, _t76_25);
        _mm256_storeu_pd(M1 + 28*i0 + k3 + 56, _t76_26);
        _mm256_storeu_pd(M1 + 28*i0 + k3 + 84, _t76_27);
      }
    }
  }

  _t56_32 = _mm256_maskload_pd(M3 + 50, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_34 = _mm256_maskload_pd(M3 + 75, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_33 = _mm256_maskload_pd(M3 + 51, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_23 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_21 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_29 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_27 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t77_12 = _mm256_loadu_pd(M1 + 84);
  _t77_9 = _mm256_loadu_pd(M1);
  _t77_10 = _mm256_loadu_pd(M1 + 28);
  _t77_11 = _mm256_loadu_pd(M1 + 56);
  _t77_8 = _mm256_broadcast_sd(&(M3[1]));

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 3), M3[24,24],h(1, 24, 3)) ),h(1, 24, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t77_14 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_15 = _t56_34;

  // 4-BLAC: 1x4 / 1x4
  _t77_16 = _mm256_div_pd(_t77_14, _t77_15);

  // AVX Storer:
  _t56_36 = _t77_16;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 2), M3[24,24],h(1, 24, 2)) ),h(1, 24, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t77_17 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_18 = _t56_32;

  // 4-BLAC: 1x4 / 1x4
  _t77_19 = _mm256_div_pd(_t77_17, _t77_18);

  // AVX Storer:
  _t56_35 = _t77_19;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 1), M3[24,24],h(1, 24, 1)) ),h(1, 24, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t77_20 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_21 = _t56_27;

  // 4-BLAC: 1x4 / 1x4
  _t77_22 = _mm256_div_pd(_t77_20, _t77_21);

  // AVX Storer:
  _t56_28 = _t77_22;

  // Generating : T614[1,24] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 24, 0), M3[24,24],h(1, 24, 0)) ),h(1, 24, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t77_23 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_24 = _t56_21;

  // 4-BLAC: 1x4 / 1x4
  _t77_25 = _mm256_div_pd(_t77_23, _t77_24);

  // AVX Storer:
  _t56_22 = _t77_25;

  // Generating : M1[24,28] = S(h(1, 24, 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 3)) Kro G(h(1, 24, 3), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_26 = _t56_16;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t77_12 = _mm256_mul_pd(_t77_26, _t77_12);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(3, 24, 0), ( G(h(3, 24, 0), M1[24,28],h(4, 28, fi39)) - ( G(h(3, 24, 0), M3[24,24],h(1, 24, 3)) * G(h(1, 24, 3), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 3x4 -> 4x4
  _t77_27 = _t77_9;
  _t77_28 = _t77_10;
  _t77_29 = _t77_11;
  _t77_30 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t77_31 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_23, _t56_33, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_29, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t77_32 = _mm256_mul_pd(_t77_7, _t77_12);
  _t77_33 = _mm256_mul_pd(_t77_6, _t77_12);
  _t77_34 = _mm256_mul_pd(_t77_5, _t77_12);
  _t77_35 = _mm256_mul_pd(_t77_4, _t77_12);

  // 4-BLAC: 4x4 - 4x4
  _t77_36 = _mm256_sub_pd(_t77_27, _t77_32);
  _t77_37 = _mm256_sub_pd(_t77_28, _t77_33);
  _t77_38 = _mm256_sub_pd(_t77_29, _t77_34);
  _t77_39 = _mm256_sub_pd(_t77_30, _t77_35);

  // AVX Storer:
  _t77_9 = _t77_36;
  _t77_10 = _t77_37;
  _t77_11 = _t77_38;

  // Generating : M1[24,28] = S(h(1, 24, 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 2)) Kro G(h(1, 24, 2), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_40 = _t56_18;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t77_11 = _mm256_mul_pd(_t77_40, _t77_11);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(2, 24, 0), ( G(h(2, 24, 0), M1[24,28],h(4, 28, fi39)) - ( G(h(2, 24, 0), M3[24,24],h(1, 24, 2)) * G(h(1, 24, 2), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // 2x4 -> 4x4
  _t77_41 = _t77_9;
  _t77_42 = _t77_10;
  _t77_43 = _mm256_setzero_pd();
  _t77_44 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t77_45 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_29, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t77_46 = _mm256_mul_pd(_t77_3, _t77_11);
  _t77_47 = _mm256_mul_pd(_t77_2, _t77_11);
  _t77_48 = _mm256_mul_pd(_t77_1, _t77_11);
  _t77_49 = _mm256_mul_pd(_t77_0, _t77_11);

  // 4-BLAC: 4x4 - 4x4
  _t77_50 = _mm256_sub_pd(_t77_41, _t77_46);
  _t77_51 = _mm256_sub_pd(_t77_42, _t77_47);
  _t77_52 = _mm256_sub_pd(_t77_43, _t77_48);
  _t77_53 = _mm256_sub_pd(_t77_44, _t77_49);

  // AVX Storer:
  _t77_9 = _t77_50;
  _t77_10 = _t77_51;

  // Generating : M1[24,28] = S(h(1, 24, 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 1)) Kro G(h(1, 24, 1), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_54 = _t56_19;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t77_10 = _mm256_mul_pd(_t77_54, _t77_10);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(1, 24, 0), ( G(h(1, 24, 0), M1[24,28],h(4, 28, fi39)) - ( G(h(1, 24, 0), M3[24,24],h(1, 24, 1)) Kro G(h(1, 24, 1), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_55 = _t77_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t77_13 = _mm256_mul_pd(_t77_55, _t77_10);

  // 4-BLAC: 1x4 - 1x4
  _t77_9 = _mm256_sub_pd(_t77_9, _t77_13);

  // AVX Storer:

  // Generating : M1[24,28] = S(h(1, 24, 0), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 0)) Kro G(h(1, 24, 0), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

  // AVX Loader:

  // 1x1 -> 1x4
  _t77_56 = _t56_20;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t77_9 = _mm256_mul_pd(_t77_56, _t77_9);

  // AVX Storer:


  for( int fi39 = 4; fi39 <= 24; fi39+=4 ) {
    _t78_12 = _mm256_loadu_pd(M1 + fi39 + 84);
    _t78_9 = _mm256_loadu_pd(M1 + fi39);
    _t78_10 = _mm256_loadu_pd(M1 + fi39 + 28);
    _t78_11 = _mm256_loadu_pd(M1 + fi39 + 56);
    _t78_8 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : M1[24,28] = S(h(1, 24, 3), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 3)) Kro G(h(1, 24, 3), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t78_13 = _t56_16;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t78_12 = _mm256_mul_pd(_t78_13, _t78_12);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(3, 24, 0), ( G(h(3, 24, 0), M1[24,28],h(4, 28, fi39)) - ( G(h(3, 24, 0), M3[24,24],h(1, 24, 3)) * G(h(1, 24, 3), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 3x4 -> 4x4
    _t78_14 = _t78_9;
    _t78_15 = _t78_10;
    _t78_16 = _t78_11;
    _t78_17 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t78_18 = _mm256_blend_pd(_mm256_permute2f128_pd(_t78_8, _t56_33, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_29, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t77_32 = _mm256_mul_pd(_t78_7, _t78_12);
    _t77_33 = _mm256_mul_pd(_t78_6, _t78_12);
    _t77_34 = _mm256_mul_pd(_t78_5, _t78_12);
    _t77_35 = _mm256_mul_pd(_t78_4, _t78_12);

    // 4-BLAC: 4x4 - 4x4
    _t78_19 = _mm256_sub_pd(_t78_14, _t77_32);
    _t78_20 = _mm256_sub_pd(_t78_15, _t77_33);
    _t78_21 = _mm256_sub_pd(_t78_16, _t77_34);
    _t78_22 = _mm256_sub_pd(_t78_17, _t77_35);

    // AVX Storer:
    _t78_9 = _t78_19;
    _t78_10 = _t78_20;
    _t78_11 = _t78_21;

    // Generating : M1[24,28] = S(h(1, 24, 2), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 2)) Kro G(h(1, 24, 2), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t78_23 = _t56_18;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t78_11 = _mm256_mul_pd(_t78_23, _t78_11);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(2, 24, 0), ( G(h(2, 24, 0), M1[24,28],h(4, 28, fi39)) - ( G(h(2, 24, 0), M3[24,24],h(1, 24, 2)) * G(h(1, 24, 2), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // 2x4 -> 4x4
    _t78_24 = _t78_9;
    _t78_25 = _t78_10;
    _t78_26 = _mm256_setzero_pd();
    _t78_27 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t78_28 = _mm256_shuffle_pd(_mm256_blend_pd(_t78_8, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_29, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t77_46 = _mm256_mul_pd(_t78_3, _t78_11);
    _t77_47 = _mm256_mul_pd(_t78_2, _t78_11);
    _t77_48 = _mm256_mul_pd(_t78_1, _t78_11);
    _t77_49 = _mm256_mul_pd(_t78_0, _t78_11);

    // 4-BLAC: 4x4 - 4x4
    _t78_29 = _mm256_sub_pd(_t78_24, _t77_46);
    _t78_30 = _mm256_sub_pd(_t78_25, _t77_47);
    _t78_31 = _mm256_sub_pd(_t78_26, _t77_48);
    _t78_32 = _mm256_sub_pd(_t78_27, _t77_49);

    // AVX Storer:
    _t78_9 = _t78_29;
    _t78_10 = _t78_30;

    // Generating : M1[24,28] = S(h(1, 24, 1), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 1)) Kro G(h(1, 24, 1), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t78_33 = _t56_19;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t78_10 = _mm256_mul_pd(_t78_33, _t78_10);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, 0), ( G(h(1, 24, 0), M1[24,28],h(4, 28, fi39)) - ( G(h(1, 24, 0), M3[24,24],h(1, 24, 1)) Kro G(h(1, 24, 1), M1[24,28],h(4, 28, fi39)) ) ),h(4, 28, fi39))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t78_34 = _t77_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t77_13 = _mm256_mul_pd(_t78_34, _t78_10);

    // 4-BLAC: 1x4 - 1x4
    _t78_9 = _mm256_sub_pd(_t78_9, _t77_13);

    // AVX Storer:

    // Generating : M1[24,28] = S(h(1, 24, 0), ( G(h(1, 1, 0), T614[1,24],h(1, 24, 0)) Kro G(h(1, 24, 0), M1[24,28],h(4, 28, fi39)) ),h(4, 28, fi39))

    // AVX Loader:

    // 1x1 -> 1x4
    _t78_35 = _t56_20;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t78_9 = _mm256_mul_pd(_t78_35, _t78_9);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi39 + 84, _t78_12);
    _mm256_storeu_pd(M1 + fi39, _t78_9);
    _mm256_storeu_pd(M1 + fi39 + 28, _t78_10);
    _mm256_storeu_pd(M1 + fi39 + 56, _t78_11);
  }


  // Generating : x[28,1] = Sum_{i0} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), y[28,1],h(1, 1, 0)) + ( G(h(4, 28, i0), M2[28,24],h(4, 24, 0)) * G(h(4, 24, 0), v0[24,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k3} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,24],h(4, 24, k3)) * G(h(4, 24, k3), v0[24,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t68_4);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t68_6);

  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t79_4 = _mm256_loadu_pd(y + i0);
    _t79_3 = _mm256_loadu_pd(M2 + 24*i0);
    _t79_2 = _mm256_loadu_pd(M2 + 24*i0 + 24);
    _t79_1 = _mm256_loadu_pd(M2 + 24*i0 + 48);
    _t79_0 = _mm256_loadu_pd(M2 + 24*i0 + 72);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t79_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t79_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32)), _mm256_mul_pd(_t79_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t79_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32)), _mm256_mul_pd(_t79_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t79_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32)), _mm256_mul_pd(_t79_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t79_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32)), _mm256_mul_pd(_t79_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_8, _t68_7), _mm256_unpacklo_pd(_t68_5, _t68_3), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t79_5 = _mm256_add_pd(_t79_4, _t79_6);

    // AVX Storer:

    for( int k3 = 4; k3 <= 23; k3+=4 ) {
      _t80_4 = _mm256_loadu_pd(M2 + 24*i0 + k3);
      _t80_3 = _mm256_loadu_pd(M2 + 24*i0 + k3 + 24);
      _t80_2 = _mm256_loadu_pd(M2 + 24*i0 + k3 + 48);
      _t80_1 = _mm256_loadu_pd(M2 + 24*i0 + k3 + 72);
      _t80_0 = _mm256_loadu_pd(v0 + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t80_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t80_4, _t80_0), _mm256_mul_pd(_t80_3, _t80_0)), _mm256_hadd_pd(_mm256_mul_pd(_t80_2, _t80_0), _mm256_mul_pd(_t80_1, _t80_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t80_4, _t80_0), _mm256_mul_pd(_t80_3, _t80_0)), _mm256_hadd_pd(_mm256_mul_pd(_t80_2, _t80_0), _mm256_mul_pd(_t80_1, _t80_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t79_5 = _mm256_add_pd(_t79_5, _t80_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(x + i0, _t79_5);
  }


  // Generating : P[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) - ( G(h(4, 28, i0), M2[28,24],h(4, 24, 0)) * G(h(4, 24, 0), M1[24,28],h(4, 28, i0)) ) ),h(4, 28, i0)) + Sum_{k2} ( -$(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,24],h(4, 24, k2)) * G(h(4, 24, k2), M1[24,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) + Sum_{k3} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k3)) - ( G(h(4, 28, i0), M2[28,24],h(4, 24, 0)) * G(h(4, 24, 0), M1[24,28],h(4, 28, k3)) ) ),h(4, 28, k3)) + Sum_{k2} ( -$(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,24],h(4, 24, k2)) * G(h(4, 24, k2), M1[24,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) ) ) ) + S(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) - ( G(h(4, 28, 24), M2[28,24],h(4, 24, 0)) * G(h(4, 24, 0), M1[24,28],h(4, 28, 24)) ) ),h(4, 28, 24)) ) + Sum_{k2} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), M2[28,24],h(4, 24, k2)) * G(h(4, 24, k2), M1[24,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  _mm256_storeu_pd(M1 + 84, _t77_12);
  _mm256_storeu_pd(M1 + 56, _t77_11);
  _mm256_storeu_pd(M1 + 28, _t77_10);
  _mm256_storeu_pd(M1, _t77_9);

  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t81_23 = _mm256_loadu_pd(Y + 29*i0);
    _t81_22 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t81_21 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t81_20 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t81_19 = _mm256_broadcast_sd(M2 + 24*i0);
    _t81_18 = _mm256_broadcast_sd(M2 + 24*i0 + 1);
    _t81_17 = _mm256_broadcast_sd(M2 + 24*i0 + 2);
    _t81_16 = _mm256_broadcast_sd(M2 + 24*i0 + 3);
    _t81_15 = _mm256_broadcast_sd(M2 + 24*i0 + 24);
    _t81_14 = _mm256_broadcast_sd(M2 + 24*i0 + 25);
    _t81_13 = _mm256_broadcast_sd(M2 + 24*i0 + 26);
    _t81_12 = _mm256_broadcast_sd(M2 + 24*i0 + 27);
    _t81_11 = _mm256_broadcast_sd(M2 + 24*i0 + 48);
    _t81_10 = _mm256_broadcast_sd(M2 + 24*i0 + 49);
    _t81_9 = _mm256_broadcast_sd(M2 + 24*i0 + 50);
    _t81_8 = _mm256_broadcast_sd(M2 + 24*i0 + 51);
    _t81_7 = _mm256_broadcast_sd(M2 + 24*i0 + 72);
    _t81_6 = _mm256_broadcast_sd(M2 + 24*i0 + 73);
    _t81_5 = _mm256_broadcast_sd(M2 + 24*i0 + 74);
    _t81_4 = _mm256_broadcast_sd(M2 + 24*i0 + 75);
    _t81_3 = _mm256_loadu_pd(M1 + i0);
    _t81_2 = _mm256_loadu_pd(M1 + i0 + 28);
    _t81_1 = _mm256_loadu_pd(M1 + i0 + 56);
    _t81_0 = _mm256_loadu_pd(M1 + i0 + 84);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t81_36 = _t81_23;
    _t81_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t81_23, _t81_22, 3), _t81_22, 12);
    _t81_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t81_23, _t81_22, 0), _t81_21, 49);
    _t81_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t81_23, _t81_22, 12), _mm256_shuffle_pd(_t81_21, _t81_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t81_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_19, _t81_3), _mm256_mul_pd(_t81_18, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_17, _t81_1), _mm256_mul_pd(_t81_16, _t81_0)));
    _t81_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_15, _t81_3), _mm256_mul_pd(_t81_14, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_13, _t81_1), _mm256_mul_pd(_t81_12, _t81_0)));
    _t81_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_11, _t81_3), _mm256_mul_pd(_t81_10, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_9, _t81_1), _mm256_mul_pd(_t81_8, _t81_0)));
    _t81_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_7, _t81_3), _mm256_mul_pd(_t81_6, _t81_2)), _mm256_add_pd(_mm256_mul_pd(_t81_5, _t81_1), _mm256_mul_pd(_t81_4, _t81_0)));

    // 4-BLAC: 4x4 - 4x4
    _t81_32 = _mm256_sub_pd(_t81_36, _t81_28);
    _t81_33 = _mm256_sub_pd(_t81_37, _t81_29);
    _t81_34 = _mm256_sub_pd(_t81_38, _t81_30);
    _t81_35 = _mm256_sub_pd(_t81_39, _t81_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t81_24 = _t81_32;
    _t81_25 = _t81_33;
    _t81_26 = _t81_34;
    _t81_27 = _t81_35;

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t82_19 = _mm256_broadcast_sd(M2 + 24*i0 + k2);
      _t82_18 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 1);
      _t82_17 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 2);
      _t82_16 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 3);
      _t82_15 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 24);
      _t82_14 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 25);
      _t82_13 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 26);
      _t82_12 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 27);
      _t82_11 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 48);
      _t82_10 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 49);
      _t82_9 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 50);
      _t82_8 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 51);
      _t82_7 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 72);
      _t82_6 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 73);
      _t82_5 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 74);
      _t82_4 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 75);
      _t82_3 = _mm256_loadu_pd(M1 + i0 + 28*k2);
      _t82_2 = _mm256_loadu_pd(M1 + i0 + 28*k2 + 28);
      _t82_1 = _mm256_loadu_pd(M1 + i0 + 28*k2 + 56);
      _t82_0 = _mm256_loadu_pd(M1 + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t82_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_19, _t82_3), _mm256_mul_pd(_t82_18, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_17, _t82_1), _mm256_mul_pd(_t82_16, _t82_0)));
      _t82_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_15, _t82_3), _mm256_mul_pd(_t82_14, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_13, _t82_1), _mm256_mul_pd(_t82_12, _t82_0)));
      _t82_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_11, _t82_3), _mm256_mul_pd(_t82_10, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_9, _t82_1), _mm256_mul_pd(_t82_8, _t82_0)));
      _t82_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t82_7, _t82_3), _mm256_mul_pd(_t82_6, _t82_2)), _mm256_add_pd(_mm256_mul_pd(_t82_5, _t82_1), _mm256_mul_pd(_t82_4, _t82_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t82_24 = _t81_24;
      _t82_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t81_24, _t81_25, 3), _t81_25, 12);
      _t82_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t81_24, _t81_25, 0), _t81_26, 49);
      _t82_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t81_24, _t81_25, 12), _mm256_shuffle_pd(_t81_26, _t81_27, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t82_24 = _mm256_sub_pd(_t82_24, _t82_20);
      _t82_25 = _mm256_sub_pd(_t82_25, _t82_21);
      _t82_26 = _mm256_sub_pd(_t82_26, _t82_22);
      _t82_27 = _mm256_sub_pd(_t82_27, _t82_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t81_24 = _t82_24;
      _t81_25 = _t82_25;
      _t81_26 = _t82_26;
      _t81_27 = _t82_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 27; k3+=4 ) {
      _t83_7 = _mm256_loadu_pd(Y + 28*i0 + k3);
      _t83_6 = _mm256_loadu_pd(Y + 28*i0 + k3 + 28);
      _t83_5 = _mm256_loadu_pd(Y + 28*i0 + k3 + 56);
      _t83_4 = _mm256_loadu_pd(Y + 28*i0 + k3 + 84);
      _t83_3 = _mm256_loadu_pd(M1 + k3);
      _t83_2 = _mm256_loadu_pd(M1 + k3 + 28);
      _t83_1 = _mm256_loadu_pd(M1 + k3 + 56);
      _t83_0 = _mm256_loadu_pd(M1 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t83_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_19, _t83_3), _mm256_mul_pd(_t81_18, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t81_17, _t83_1), _mm256_mul_pd(_t81_16, _t83_0)));
      _t83_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_15, _t83_3), _mm256_mul_pd(_t81_14, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t81_13, _t83_1), _mm256_mul_pd(_t81_12, _t83_0)));
      _t83_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_11, _t83_3), _mm256_mul_pd(_t81_10, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t81_9, _t83_1), _mm256_mul_pd(_t81_8, _t83_0)));
      _t83_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t81_7, _t83_3), _mm256_mul_pd(_t81_6, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t81_5, _t83_1), _mm256_mul_pd(_t81_4, _t83_0)));

      // 4-BLAC: 4x4 - 4x4
      _t83_12 = _mm256_sub_pd(_t83_7, _t83_8);
      _t83_13 = _mm256_sub_pd(_t83_6, _t83_9);
      _t83_14 = _mm256_sub_pd(_t83_5, _t83_10);
      _t83_15 = _mm256_sub_pd(_t83_4, _t83_11);

      // AVX Storer:

      for( int k2 = 4; k2 <= 23; k2+=4 ) {
        _t84_19 = _mm256_broadcast_sd(M2 + 24*i0 + k2);
        _t84_18 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 1);
        _t84_17 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 2);
        _t84_16 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 3);
        _t84_15 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 24);
        _t84_14 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 25);
        _t84_13 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 26);
        _t84_12 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 27);
        _t84_11 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 48);
        _t84_10 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 49);
        _t84_9 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 50);
        _t84_8 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 51);
        _t84_7 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 72);
        _t84_6 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 73);
        _t84_5 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 74);
        _t84_4 = _mm256_broadcast_sd(M2 + 24*i0 + k2 + 75);
        _t84_3 = _mm256_loadu_pd(M1 + 28*k2 + k3);
        _t84_2 = _mm256_loadu_pd(M1 + 28*k2 + k3 + 28);
        _t84_1 = _mm256_loadu_pd(M1 + 28*k2 + k3 + 56);
        _t84_0 = _mm256_loadu_pd(M1 + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t84_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t84_19, _t84_3), _mm256_mul_pd(_t84_18, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t84_17, _t84_1), _mm256_mul_pd(_t84_16, _t84_0)));
        _t84_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t84_15, _t84_3), _mm256_mul_pd(_t84_14, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t84_13, _t84_1), _mm256_mul_pd(_t84_12, _t84_0)));
        _t84_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t84_11, _t84_3), _mm256_mul_pd(_t84_10, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t84_9, _t84_1), _mm256_mul_pd(_t84_8, _t84_0)));
        _t84_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t84_7, _t84_3), _mm256_mul_pd(_t84_6, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t84_5, _t84_1), _mm256_mul_pd(_t84_4, _t84_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t83_12 = _mm256_sub_pd(_t83_12, _t84_20);
        _t83_13 = _mm256_sub_pd(_t83_13, _t84_21);
        _t83_14 = _mm256_sub_pd(_t83_14, _t84_22);
        _t83_15 = _mm256_sub_pd(_t83_15, _t84_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(P + 28*i0 + k3, _t83_12);
      _mm256_storeu_pd(P + 28*i0 + k3 + 28, _t83_13);
      _mm256_storeu_pd(P + 28*i0 + k3 + 56, _t83_14);
      _mm256_storeu_pd(P + 28*i0 + k3 + 84, _t83_15);
    }
    _mm256_storeu_pd(P + 29*i0, _t81_24);
    _mm256_maskstore_pd(P + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t81_25);
    _mm256_maskstore_pd(P + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t81_26);
    _mm256_maskstore_pd(P + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t81_27);
  }

  _t85_19 = _mm256_broadcast_sd(M2 + 576);
  _t85_18 = _mm256_broadcast_sd(M2 + 577);
  _t85_17 = _mm256_broadcast_sd(M2 + 578);
  _t85_16 = _mm256_broadcast_sd(M2 + 579);
  _t85_15 = _mm256_broadcast_sd(M2 + 600);
  _t85_14 = _mm256_broadcast_sd(M2 + 601);
  _t85_13 = _mm256_broadcast_sd(M2 + 602);
  _t85_12 = _mm256_broadcast_sd(M2 + 603);
  _t85_11 = _mm256_broadcast_sd(M2 + 624);
  _t85_10 = _mm256_broadcast_sd(M2 + 625);
  _t85_9 = _mm256_broadcast_sd(M2 + 626);
  _t85_8 = _mm256_broadcast_sd(M2 + 627);
  _t85_7 = _mm256_broadcast_sd(M2 + 648);
  _t85_6 = _mm256_broadcast_sd(M2 + 649);
  _t85_5 = _mm256_broadcast_sd(M2 + 650);
  _t85_4 = _mm256_broadcast_sd(M2 + 651);
  _t85_3 = _mm256_loadu_pd(M1 + 24);
  _t85_2 = _mm256_loadu_pd(M1 + 52);
  _t85_1 = _mm256_loadu_pd(M1 + 80);
  _t85_0 = _mm256_loadu_pd(M1 + 108);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t85_28 = _t19_28;
  _t85_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t85_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t85_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t85_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_19, _t85_3), _mm256_mul_pd(_t85_18, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_17, _t85_1), _mm256_mul_pd(_t85_16, _t85_0)));
  _t85_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_15, _t85_3), _mm256_mul_pd(_t85_14, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_13, _t85_1), _mm256_mul_pd(_t85_12, _t85_0)));
  _t85_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_11, _t85_3), _mm256_mul_pd(_t85_10, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_9, _t85_1), _mm256_mul_pd(_t85_8, _t85_0)));
  _t85_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_7, _t85_3), _mm256_mul_pd(_t85_6, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_5, _t85_1), _mm256_mul_pd(_t85_4, _t85_0)));

  // 4-BLAC: 4x4 - 4x4
  _t85_24 = _mm256_sub_pd(_t85_28, _t85_20);
  _t85_25 = _mm256_sub_pd(_t85_29, _t85_21);
  _t85_26 = _mm256_sub_pd(_t85_30, _t85_22);
  _t85_27 = _mm256_sub_pd(_t85_31, _t85_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t3_3 = _t85_24;
  _t3_2 = _t85_25;
  _t3_1 = _t85_26;
  _t3_0 = _t85_27;


  for( int k2 = 4; k2 <= 23; k2+=4 ) {
    _t86_19 = _mm256_broadcast_sd(M2 + k2 + 576);
    _t86_18 = _mm256_broadcast_sd(M2 + k2 + 577);
    _t86_17 = _mm256_broadcast_sd(M2 + k2 + 578);
    _t86_16 = _mm256_broadcast_sd(M2 + k2 + 579);
    _t86_15 = _mm256_broadcast_sd(M2 + k2 + 600);
    _t86_14 = _mm256_broadcast_sd(M2 + k2 + 601);
    _t86_13 = _mm256_broadcast_sd(M2 + k2 + 602);
    _t86_12 = _mm256_broadcast_sd(M2 + k2 + 603);
    _t86_11 = _mm256_broadcast_sd(M2 + k2 + 624);
    _t86_10 = _mm256_broadcast_sd(M2 + k2 + 625);
    _t86_9 = _mm256_broadcast_sd(M2 + k2 + 626);
    _t86_8 = _mm256_broadcast_sd(M2 + k2 + 627);
    _t86_7 = _mm256_broadcast_sd(M2 + k2 + 648);
    _t86_6 = _mm256_broadcast_sd(M2 + k2 + 649);
    _t86_5 = _mm256_broadcast_sd(M2 + k2 + 650);
    _t86_4 = _mm256_broadcast_sd(M2 + k2 + 651);
    _t86_3 = _mm256_loadu_pd(M1 + 28*k2 + 24);
    _t86_2 = _mm256_loadu_pd(M1 + 28*k2 + 52);
    _t86_1 = _mm256_loadu_pd(M1 + 28*k2 + 80);
    _t86_0 = _mm256_loadu_pd(M1 + 28*k2 + 108);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t86_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_19, _t86_3), _mm256_mul_pd(_t86_18, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_17, _t86_1), _mm256_mul_pd(_t86_16, _t86_0)));
    _t86_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_15, _t86_3), _mm256_mul_pd(_t86_14, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_13, _t86_1), _mm256_mul_pd(_t86_12, _t86_0)));
    _t86_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_11, _t86_3), _mm256_mul_pd(_t86_10, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_9, _t86_1), _mm256_mul_pd(_t86_8, _t86_0)));
    _t86_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_7, _t86_3), _mm256_mul_pd(_t86_6, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_5, _t86_1), _mm256_mul_pd(_t86_4, _t86_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t86_24 = _t3_3;
    _t86_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
    _t86_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
    _t86_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t86_24 = _mm256_sub_pd(_t86_24, _t86_20);
    _t86_25 = _mm256_sub_pd(_t86_25, _t86_21);
    _t86_26 = _mm256_sub_pd(_t86_26, _t86_22);
    _t86_27 = _mm256_sub_pd(_t86_27, _t86_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t3_3 = _t86_24;
    _t3_2 = _t86_25;
    _t3_1 = _t86_26;
    _t3_0 = _t86_27;
    _mm256_storeu_pd(P + 696, _t3_3);
    _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t3_2);
    _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t3_1);
    _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t3_0);
  }

  _mm256_storeu_pd(Y + 696, _t19_28);
  _mm256_maskstore_pd(Y + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t19_29);
  _mm256_maskstore_pd(Y + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t19_30);
  _mm256_maskstore_pd(Y + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t19_31);
  _mm256_maskstore_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t68_3);
  _mm256_maskstore_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t68_5);
  _mm256_maskstore_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t68_7);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t68_8);
  _mm256_storeu_pd(P + 696, _t3_3);
  _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t3_2);
  _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t3_1);
  _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t3_0);

}
