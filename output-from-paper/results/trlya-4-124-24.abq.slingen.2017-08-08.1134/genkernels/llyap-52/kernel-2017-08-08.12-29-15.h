/*
 * llyap_kernel.h
 *
Decl { {u'X': Symmetric[X, (52, 52), LSMatAccess], u'C': Symmetric[C, (52, 52), LSMatAccess], u'L': LowerTriangular[L, (52, 52), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt': {'m0': 'm01.ll'}, 'ftmpyozk_lwn_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt_m01_ftmpyozk_lwn_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(2, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(2, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), X[52,52],h(2, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(2, 52, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(3, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(3, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), X[52,52],h(3, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(3, 52, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(3, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 19:
For_{fi1407;4;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(fi1407, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), C[52,52],h(fi1407, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), L[52,52],h(fi1407, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1407, 52, 0), X[52,52],h(fi1407, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(3, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 32:
For_{fi1526;4;fi1407 - 4;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(4, 52, fi1526)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(4, 52, fi1526)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(fi1526, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1526), L[52,52],h(fi1526, 52, 0)) ) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(2, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(3, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(4, 52, fi1526)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, fi1526)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, fi1526)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1526)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ) )
Eq.ann: {}
 )Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(4, 52, fi1407)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), C[52,52],h(4, 52, fi1407)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), L[52,52],h(fi1407, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(fi1407, 52, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), X[52,52],h(fi1407, 52, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi1407), L[52,52],h(fi1407, 52, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1407), X[52,52],h(2, 52, fi1407)) ) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) ) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1407), X[52,52],h(3, 52, fi1407)) ) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, fi1407)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) ) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) ) ) ) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) ) )
Eq.ann: {}
 ) *
 * Created on: 2017-08-08
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 52

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205, _t0_206, _t0_207,
	_t0_208, _t0_209, _t0_210, _t0_211, _t0_212, _t0_213, _t0_214, _t0_215,
	_t0_216, _t0_217, _t0_218, _t0_219, _t0_220, _t0_221, _t0_222, _t0_223,
	_t0_224, _t0_225, _t0_226, _t0_227, _t0_228, _t0_229, _t0_230, _t0_231,
	_t0_232, _t0_233, _t0_234, _t0_235, _t0_236, _t0_237, _t0_238, _t0_239,
	_t0_240, _t0_241, _t0_242, _t0_243, _t0_244, _t0_245, _t0_246, _t0_247,
	_t0_248, _t0_249, _t0_250, _t0_251, _t0_252, _t0_253, _t0_254, _t0_255,
	_t0_256, _t0_257, _t0_258, _t0_259, _t0_260, _t0_261, _t0_262, _t0_263,
	_t0_264, _t0_265, _t0_266, _t0_267, _t0_268, _t0_269, _t0_270, _t0_271,
	_t0_272, _t0_273, _t0_274, _t0_275, _t0_276, _t0_277, _t0_278, _t0_279,
	_t0_280, _t0_281, _t0_282, _t0_283, _t0_284, _t0_285, _t0_286, _t0_287,
	_t0_288, _t0_289, _t0_290, _t0_291, _t0_292, _t0_293, _t0_294, _t0_295,
	_t0_296, _t0_297, _t0_298, _t0_299, _t0_300, _t0_301, _t0_302, _t0_303,
	_t0_304, _t0_305, _t0_306, _t0_307, _t0_308, _t0_309, _t0_310, _t0_311,
	_t0_312, _t0_313, _t0_314, _t0_315, _t0_316, _t0_317, _t0_318, _t0_319,
	_t0_320, _t0_321, _t0_322, _t0_323, _t0_324, _t0_325, _t0_326, _t0_327,
	_t0_328, _t0_329, _t0_330, _t0_331, _t0_332, _t0_333, _t0_334, _t0_335,
	_t0_336, _t0_337, _t0_338, _t0_339, _t0_340, _t0_341, _t0_342, _t0_343,
	_t0_344, _t0_345, _t0_346, _t0_347, _t0_348, _t0_349, _t0_350, _t0_351,
	_t0_352, _t0_353, _t0_354, _t0_355, _t0_356, _t0_357, _t0_358, _t0_359,
	_t0_360, _t0_361, _t0_362, _t0_363, _t0_364, _t0_365, _t0_366, _t0_367,
	_t0_368, _t0_369, _t0_370, _t0_371, _t0_372, _t0_373, _t0_374, _t0_375,
	_t0_376, _t0_377, _t0_378, _t0_379, _t0_380, _t0_381, _t0_382, _t0_383,
	_t0_384, _t0_385, _t0_386, _t0_387, _t0_388, _t0_389, _t0_390, _t0_391,
	_t0_392, _t0_393, _t0_394, _t0_395, _t0_396, _t0_397, _t0_398, _t0_399,
	_t0_400, _t0_401, _t0_402, _t0_403, _t0_404, _t0_405, _t0_406, _t0_407,
	_t0_408, _t0_409, _t0_410, _t0_411, _t0_412, _t0_413, _t0_414, _t0_415,
	_t0_416, _t0_417, _t0_418, _t0_419, _t0_420, _t0_421, _t0_422, _t0_423,
	_t0_424, _t0_425, _t0_426, _t0_427, _t0_428, _t0_429, _t0_430, _t0_431,
	_t0_432, _t0_433, _t0_434, _t0_435, _t0_436, _t0_437, _t0_438, _t0_439,
	_t0_440, _t0_441, _t0_442, _t0_443, _t0_444, _t0_445, _t0_446, _t0_447,
	_t0_448, _t0_449, _t0_450, _t0_451, _t0_452, _t0_453, _t0_454, _t0_455,
	_t0_456, _t0_457, _t0_458, _t0_459, _t0_460, _t0_461, _t0_462, _t0_463,
	_t0_464, _t0_465, _t0_466, _t0_467, _t0_468, _t0_469, _t0_470, _t0_471,
	_t0_472, _t0_473, _t0_474, _t0_475, _t0_476, _t0_477, _t0_478, _t0_479,
	_t0_480, _t0_481, _t0_482, _t0_483, _t0_484, _t0_485, _t0_486, _t0_487,
	_t0_488, _t0_489, _t0_490, _t0_491, _t0_492, _t0_493, _t0_494, _t0_495,
	_t0_496, _t0_497, _t0_498, _t0_499, _t0_500, _t0_501, _t0_502, _t0_503,
	_t0_504, _t0_505, _t0_506, _t0_507, _t0_508, _t0_509, _t0_510, _t0_511,
	_t0_512, _t0_513, _t0_514, _t0_515, _t0_516, _t0_517, _t0_518, _t0_519,
	_t0_520, _t0_521, _t0_522, _t0_523, _t0_524, _t0_525, _t0_526, _t0_527,
	_t0_528, _t0_529, _t0_530, _t0_531, _t0_532, _t0_533, _t0_534, _t0_535,
	_t0_536, _t0_537, _t0_538, _t0_539, _t0_540, _t0_541, _t0_542, _t0_543,
	_t0_544, _t0_545, _t0_546, _t0_547, _t0_548, _t0_549, _t0_550, _t0_551,
	_t0_552, _t0_553, _t0_554, _t0_555, _t0_556, _t0_557, _t0_558, _t0_559,
	_t0_560, _t0_561, _t0_562, _t0_563, _t0_564, _t0_565, _t0_566, _t0_567,
	_t0_568, _t0_569, _t0_570, _t0_571, _t0_572, _t0_573, _t0_574, _t0_575,
	_t0_576, _t0_577, _t0_578, _t0_579, _t0_580, _t0_581, _t0_582, _t0_583,
	_t0_584, _t0_585, _t0_586, _t0_587, _t0_588, _t0_589, _t0_590, _t0_591,
	_t0_592, _t0_593, _t0_594, _t0_595, _t0_596, _t0_597, _t0_598, _t0_599,
	_t0_600, _t0_601, _t0_602, _t0_603, _t0_604, _t0_605, _t0_606, _t0_607,
	_t0_608, _t0_609, _t0_610, _t0_611, _t0_612, _t0_613, _t0_614, _t0_615,
	_t0_616, _t0_617, _t0_618, _t0_619, _t0_620, _t0_621, _t0_622, _t0_623,
	_t0_624, _t0_625, _t0_626, _t0_627, _t0_628, _t0_629, _t0_630, _t0_631,
	_t0_632, _t0_633, _t0_634, _t0_635, _t0_636, _t0_637, _t0_638, _t0_639,
	_t0_640, _t0_641, _t0_642, _t0_643, _t0_644, _t0_645, _t0_646, _t0_647,
	_t0_648, _t0_649, _t0_650, _t0_651, _t0_652, _t0_653, _t0_654, _t0_655,
	_t0_656, _t0_657, _t0_658, _t0_659, _t0_660, _t0_661, _t0_662, _t0_663,
	_t0_664, _t0_665, _t0_666, _t0_667, _t0_668, _t0_669, _t0_670, _t0_671,
	_t0_672, _t0_673, _t0_674, _t0_675, _t0_676, _t0_677, _t0_678, _t0_679,
	_t0_680, _t0_681, _t0_682, _t0_683, _t0_684, _t0_685, _t0_686, _t0_687,
	_t0_688, _t0_689, _t0_690, _t0_691, _t0_692, _t0_693, _t0_694, _t0_695,
	_t0_696, _t0_697, _t0_698, _t0_699, _t0_700, _t0_701, _t0_702, _t0_703,
	_t0_704, _t0_705, _t0_706, _t0_707, _t0_708, _t0_709, _t0_710, _t0_711,
	_t0_712, _t0_713, _t0_714, _t0_715, _t0_716, _t0_717, _t0_718, _t0_719,
	_t0_720, _t0_721, _t0_722, _t0_723, _t0_724, _t0_725, _t0_726, _t0_727,
	_t0_728, _t0_729, _t0_730, _t0_731, _t0_732, _t0_733, _t0_734, _t0_735,
	_t0_736, _t0_737, _t0_738, _t0_739, _t0_740, _t0_741, _t0_742, _t0_743,
	_t0_744, _t0_745, _t0_746, _t0_747, _t0_748, _t0_749, _t0_750, _t0_751,
	_t0_752, _t0_753, _t0_754, _t0_755, _t0_756, _t0_757, _t0_758, _t0_759,
	_t0_760, _t0_761, _t0_762, _t0_763, _t0_764, _t0_765, _t0_766, _t0_767,
	_t0_768, _t0_769, _t0_770, _t0_771, _t0_772, _t0_773, _t0_774, _t0_775,
	_t0_776, _t0_777, _t0_778, _t0_779, _t0_780, _t0_781, _t0_782, _t0_783,
	_t0_784, _t0_785, _t0_786, _t0_787, _t0_788, _t0_789, _t0_790, _t0_791,
	_t0_792, _t0_793, _t0_794, _t0_795, _t0_796, _t0_797, _t0_798, _t0_799,
	_t0_800, _t0_801, _t0_802, _t0_803, _t0_804, _t0_805, _t0_806, _t0_807,
	_t0_808, _t0_809, _t0_810, _t0_811, _t0_812, _t0_813, _t0_814, _t0_815,
	_t0_816, _t0_817, _t0_818, _t0_819, _t0_820, _t0_821, _t0_822, _t0_823,
	_t0_824, _t0_825, _t0_826, _t0_827, _t0_828, _t0_829, _t0_830, _t0_831,
	_t0_832, _t0_833, _t0_834, _t0_835, _t0_836, _t0_837, _t0_838, _t0_839,
	_t0_840, _t0_841, _t0_842, _t0_843, _t0_844, _t0_845, _t0_846, _t0_847,
	_t0_848, _t0_849, _t0_850, _t0_851, _t0_852, _t0_853, _t0_854, _t0_855,
	_t0_856, _t0_857, _t0_858, _t0_859, _t0_860, _t0_861, _t0_862, _t0_863,
	_t0_864, _t0_865, _t0_866, _t0_867, _t0_868, _t0_869, _t0_870, _t0_871,
	_t0_872, _t0_873, _t0_874, _t0_875, _t0_876, _t0_877, _t0_878, _t0_879,
	_t0_880, _t0_881, _t0_882, _t0_883, _t0_884, _t0_885, _t0_886, _t0_887,
	_t0_888, _t0_889, _t0_890, _t0_891, _t0_892, _t0_893, _t0_894, _t0_895,
	_t0_896, _t0_897, _t0_898, _t0_899, _t0_900, _t0_901, _t0_902, _t0_903,
	_t0_904, _t0_905, _t0_906, _t0_907, _t0_908, _t0_909, _t0_910, _t0_911,
	_t0_912, _t0_913, _t0_914, _t0_915, _t0_916, _t0_917, _t0_918, _t0_919,
	_t0_920, _t0_921, _t0_922, _t0_923, _t0_924, _t0_925, _t0_926, _t0_927,
	_t0_928, _t0_929, _t0_930, _t0_931, _t0_932, _t0_933, _t0_934, _t0_935,
	_t0_936, _t0_937, _t0_938, _t0_939, _t0_940, _t0_941, _t0_942, _t0_943,
	_t0_944, _t0_945, _t0_946, _t0_947, _t0_948, _t0_949, _t0_950, _t0_951,
	_t0_952, _t0_953, _t0_954, _t0_955, _t0_956, _t0_957, _t0_958, _t0_959,
	_t0_960, _t0_961, _t0_962, _t0_963, _t0_964, _t0_965, _t0_966, _t0_967,
	_t0_968, _t0_969, _t0_970, _t0_971, _t0_972, _t0_973, _t0_974, _t0_975,
	_t0_976, _t0_977, _t0_978, _t0_979, _t0_980, _t0_981, _t0_982, _t0_983,
	_t0_984, _t0_985, _t0_986, _t0_987, _t0_988, _t0_989, _t0_990, _t0_991,
	_t0_992, _t0_993, _t0_994, _t0_995, _t0_996, _t0_997, _t0_998, _t0_999,
	_t0_1000, _t0_1001, _t0_1002, _t0_1003, _t0_1004, _t0_1005, _t0_1006, _t0_1007,
	_t0_1008, _t0_1009, _t0_1010, _t0_1011, _t0_1012, _t0_1013, _t0_1014, _t0_1015,
	_t0_1016, _t0_1017, _t0_1018, _t0_1019, _t0_1020, _t0_1021, _t0_1022, _t0_1023,
	_t0_1024, _t0_1025, _t0_1026, _t0_1027, _t0_1028, _t0_1029, _t0_1030, _t0_1031,
	_t0_1032, _t0_1033, _t0_1034, _t0_1035, _t0_1036, _t0_1037, _t0_1038, _t0_1039,
	_t0_1040, _t0_1041, _t0_1042, _t0_1043, _t0_1044, _t0_1045, _t0_1046, _t0_1047,
	_t0_1048, _t0_1049, _t0_1050, _t0_1051, _t0_1052, _t0_1053, _t0_1054, _t0_1055,
	_t0_1056, _t0_1057, _t0_1058, _t0_1059, _t0_1060, _t0_1061, _t0_1062, _t0_1063,
	_t0_1064, _t0_1065, _t0_1066, _t0_1067, _t0_1068, _t0_1069, _t0_1070, _t0_1071,
	_t0_1072, _t0_1073, _t0_1074, _t0_1075, _t0_1076, _t0_1077, _t0_1078, _t0_1079,
	_t0_1080, _t0_1081, _t0_1082, _t0_1083, _t0_1084, _t0_1085, _t0_1086, _t0_1087,
	_t0_1088, _t0_1089, _t0_1090, _t0_1091, _t0_1092, _t0_1093, _t0_1094, _t0_1095,
	_t0_1096, _t0_1097, _t0_1098, _t0_1099, _t0_1100, _t0_1101, _t0_1102, _t0_1103,
	_t0_1104, _t0_1105, _t0_1106, _t0_1107, _t0_1108, _t0_1109, _t0_1110, _t0_1111,
	_t0_1112, _t0_1113, _t0_1114, _t0_1115, _t0_1116, _t0_1117, _t0_1118, _t0_1119,
	_t0_1120, _t0_1121, _t0_1122, _t0_1123, _t0_1124, _t0_1125, _t0_1126, _t0_1127,
	_t0_1128, _t0_1129, _t0_1130, _t0_1131, _t0_1132, _t0_1133, _t0_1134, _t0_1135,
	_t0_1136, _t0_1137, _t0_1138, _t0_1139, _t0_1140, _t0_1141, _t0_1142, _t0_1143,
	_t0_1144, _t0_1145, _t0_1146, _t0_1147, _t0_1148, _t0_1149, _t0_1150, _t0_1151,
	_t0_1152, _t0_1153, _t0_1154, _t0_1155, _t0_1156, _t0_1157, _t0_1158, _t0_1159,
	_t0_1160, _t0_1161, _t0_1162, _t0_1163, _t0_1164, _t0_1165, _t0_1166, _t0_1167,
	_t0_1168, _t0_1169, _t0_1170, _t0_1171, _t0_1172, _t0_1173, _t0_1174, _t0_1175,
	_t0_1176, _t0_1177, _t0_1178, _t0_1179, _t0_1180, _t0_1181, _t0_1182, _t0_1183,
	_t0_1184, _t0_1185, _t0_1186, _t0_1187, _t0_1188, _t0_1189, _t0_1190, _t0_1191,
	_t0_1192, _t0_1193, _t0_1194, _t0_1195, _t0_1196, _t0_1197, _t0_1198, _t0_1199,
	_t0_1200, _t0_1201, _t0_1202, _t0_1203, _t0_1204, _t0_1205, _t0_1206, _t0_1207,
	_t0_1208, _t0_1209, _t0_1210, _t0_1211, _t0_1212, _t0_1213, _t0_1214, _t0_1215,
	_t0_1216, _t0_1217, _t0_1218, _t0_1219, _t0_1220, _t0_1221, _t0_1222, _t0_1223,
	_t0_1224, _t0_1225, _t0_1226, _t0_1227, _t0_1228, _t0_1229, _t0_1230, _t0_1231,
	_t0_1232, _t0_1233, _t0_1234, _t0_1235, _t0_1236, _t0_1237, _t0_1238, _t0_1239,
	_t0_1240, _t0_1241, _t0_1242, _t0_1243, _t0_1244, _t0_1245, _t0_1246;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143,
	_t4_144, _t4_145, _t4_146, _t4_147, _t4_148, _t4_149, _t4_150, _t4_151,
	_t4_152, _t4_153, _t4_154, _t4_155, _t4_156, _t4_157, _t4_158, _t4_159,
	_t4_160, _t4_161, _t4_162, _t4_163, _t4_164, _t4_165, _t4_166, _t4_167,
	_t4_168, _t4_169, _t4_170, _t4_171, _t4_172, _t4_173, _t4_174, _t4_175,
	_t4_176, _t4_177, _t4_178, _t4_179, _t4_180, _t4_181, _t4_182, _t4_183,
	_t4_184, _t4_185, _t4_186, _t4_187, _t4_188, _t4_189, _t4_190, _t4_191,
	_t4_192, _t4_193, _t4_194, _t4_195, _t4_196, _t4_197, _t4_198, _t4_199,
	_t4_200, _t4_201, _t4_202, _t4_203, _t4_204, _t4_205, _t4_206, _t4_207,
	_t4_208, _t4_209, _t4_210, _t4_211, _t4_212, _t4_213, _t4_214, _t4_215,
	_t4_216, _t4_217, _t4_218, _t4_219, _t4_220, _t4_221, _t4_222, _t4_223,
	_t4_224, _t4_225, _t4_226, _t4_227, _t4_228, _t4_229, _t4_230, _t4_231,
	_t4_232, _t4_233, _t4_234, _t4_235, _t4_236, _t4_237, _t4_238, _t4_239,
	_t4_240, _t4_241, _t4_242, _t4_243, _t4_244, _t4_245, _t4_246, _t4_247,
	_t4_248, _t4_249, _t4_250, _t4_251, _t4_252, _t4_253, _t4_254, _t4_255,
	_t4_256, _t4_257, _t4_258, _t4_259, _t4_260, _t4_261, _t4_262, _t4_263,
	_t4_264, _t4_265, _t4_266, _t4_267, _t4_268, _t4_269, _t4_270, _t4_271,
	_t4_272, _t4_273, _t4_274, _t4_275, _t4_276, _t4_277, _t4_278, _t4_279,
	_t4_280, _t4_281, _t4_282, _t4_283, _t4_284, _t4_285, _t4_286, _t4_287,
	_t4_288, _t4_289, _t4_290, _t4_291, _t4_292, _t4_293, _t4_294, _t4_295,
	_t4_296, _t4_297, _t4_298, _t4_299, _t4_300, _t4_301, _t4_302, _t4_303,
	_t4_304, _t4_305, _t4_306, _t4_307, _t4_308, _t4_309, _t4_310, _t4_311,
	_t4_312, _t4_313, _t4_314, _t4_315, _t4_316, _t4_317, _t4_318, _t4_319,
	_t4_320, _t4_321, _t4_322, _t4_323, _t4_324, _t4_325, _t4_326, _t4_327,
	_t4_328, _t4_329, _t4_330, _t4_331, _t4_332, _t4_333, _t4_334, _t4_335,
	_t4_336, _t4_337, _t4_338, _t4_339, _t4_340, _t4_341, _t4_342, _t4_343,
	_t4_344, _t4_345, _t4_346, _t4_347, _t4_348, _t4_349, _t4_350, _t4_351,
	_t4_352, _t4_353, _t4_354, _t4_355, _t4_356, _t4_357, _t4_358, _t4_359,
	_t4_360, _t4_361, _t4_362, _t4_363, _t4_364, _t4_365, _t4_366, _t4_367,
	_t4_368, _t4_369, _t4_370, _t4_371, _t4_372, _t4_373, _t4_374, _t4_375,
	_t4_376, _t4_377, _t4_378, _t4_379, _t4_380, _t4_381, _t4_382, _t4_383,
	_t4_384, _t4_385, _t4_386, _t4_387, _t4_388, _t4_389, _t4_390, _t4_391,
	_t4_392, _t4_393, _t4_394, _t4_395, _t4_396, _t4_397, _t4_398, _t4_399,
	_t4_400, _t4_401, _t4_402, _t4_403, _t4_404, _t4_405, _t4_406, _t4_407,
	_t4_408, _t4_409, _t4_410, _t4_411, _t4_412, _t4_413, _t4_414, _t4_415,
	_t4_416, _t4_417, _t4_418, _t4_419, _t4_420, _t4_421, _t4_422, _t4_423,
	_t4_424, _t4_425, _t4_426, _t4_427, _t4_428, _t4_429, _t4_430, _t4_431,
	_t4_432, _t4_433, _t4_434, _t4_435, _t4_436, _t4_437, _t4_438, _t4_439,
	_t4_440, _t4_441, _t4_442, _t4_443, _t4_444, _t4_445, _t4_446, _t4_447,
	_t4_448, _t4_449, _t4_450, _t4_451, _t4_452, _t4_453, _t4_454, _t4_455,
	_t4_456, _t4_457, _t4_458, _t4_459, _t4_460, _t4_461, _t4_462, _t4_463,
	_t4_464, _t4_465, _t4_466, _t4_467, _t4_468, _t4_469, _t4_470, _t4_471,
	_t4_472, _t4_473, _t4_474, _t4_475, _t4_476, _t4_477, _t4_478, _t4_479,
	_t4_480, _t4_481, _t4_482, _t4_483, _t4_484, _t4_485, _t4_486, _t4_487,
	_t4_488, _t4_489, _t4_490, _t4_491, _t4_492, _t4_493, _t4_494, _t4_495,
	_t4_496, _t4_497, _t4_498, _t4_499, _t4_500, _t4_501, _t4_502, _t4_503,
	_t4_504, _t4_505, _t4_506, _t4_507, _t4_508, _t4_509, _t4_510, _t4_511,
	_t4_512, _t4_513, _t4_514, _t4_515, _t4_516, _t4_517, _t4_518, _t4_519,
	_t4_520, _t4_521, _t4_522, _t4_523, _t4_524, _t4_525, _t4_526, _t4_527,
	_t4_528, _t4_529, _t4_530, _t4_531, _t4_532, _t4_533, _t4_534, _t4_535,
	_t4_536, _t4_537, _t4_538, _t4_539, _t4_540, _t4_541, _t4_542, _t4_543,
	_t4_544, _t4_545, _t4_546, _t4_547, _t4_548, _t4_549, _t4_550, _t4_551,
	_t4_552, _t4_553, _t4_554, _t4_555, _t4_556, _t4_557, _t4_558, _t4_559,
	_t4_560, _t4_561, _t4_562, _t4_563, _t4_564, _t4_565, _t4_566, _t4_567,
	_t4_568, _t4_569, _t4_570, _t4_571, _t4_572, _t4_573, _t4_574, _t4_575,
	_t4_576, _t4_577, _t4_578, _t4_579, _t4_580, _t4_581, _t4_582, _t4_583,
	_t4_584, _t4_585, _t4_586, _t4_587, _t4_588, _t4_589, _t4_590, _t4_591,
	_t4_592, _t4_593, _t4_594, _t4_595, _t4_596, _t4_597, _t4_598, _t4_599,
	_t4_600, _t4_601, _t4_602, _t4_603, _t4_604, _t4_605, _t4_606, _t4_607,
	_t4_608, _t4_609, _t4_610, _t4_611, _t4_612, _t4_613, _t4_614, _t4_615,
	_t4_616, _t4_617, _t4_618, _t4_619, _t4_620, _t4_621, _t4_622, _t4_623,
	_t4_624, _t4_625, _t4_626, _t4_627, _t4_628, _t4_629, _t4_630, _t4_631,
	_t4_632, _t4_633, _t4_634, _t4_635, _t4_636, _t4_637, _t4_638, _t4_639,
	_t4_640, _t4_641, _t4_642, _t4_643, _t4_644, _t4_645, _t4_646, _t4_647,
	_t4_648, _t4_649, _t4_650, _t4_651, _t4_652, _t4_653, _t4_654, _t4_655,
	_t4_656, _t4_657, _t4_658, _t4_659, _t4_660, _t4_661, _t4_662, _t4_663,
	_t4_664, _t4_665, _t4_666, _t4_667, _t4_668, _t4_669, _t4_670, _t4_671,
	_t4_672, _t4_673, _t4_674, _t4_675, _t4_676, _t4_677, _t4_678;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31,
	_t7_32, _t7_33, _t7_34, _t7_35, _t7_36, _t7_37, _t7_38, _t7_39,
	_t7_40, _t7_41, _t7_42, _t7_43, _t7_44, _t7_45, _t7_46, _t7_47,
	_t7_48, _t7_49, _t7_50, _t7_51, _t7_52, _t7_53, _t7_54, _t7_55,
	_t7_56, _t7_57, _t7_58, _t7_59, _t7_60, _t7_61, _t7_62, _t7_63,
	_t7_64, _t7_65, _t7_66, _t7_67, _t7_68, _t7_69, _t7_70, _t7_71,
	_t7_72, _t7_73, _t7_74, _t7_75, _t7_76, _t7_77, _t7_78, _t7_79,
	_t7_80, _t7_81, _t7_82, _t7_83, _t7_84, _t7_85, _t7_86, _t7_87,
	_t7_88, _t7_89, _t7_90, _t7_91, _t7_92, _t7_93, _t7_94, _t7_95,
	_t7_96, _t7_97, _t7_98, _t7_99, _t7_100, _t7_101, _t7_102, _t7_103,
	_t7_104, _t7_105, _t7_106, _t7_107, _t7_108, _t7_109, _t7_110, _t7_111,
	_t7_112, _t7_113, _t7_114, _t7_115, _t7_116, _t7_117, _t7_118, _t7_119,
	_t7_120, _t7_121, _t7_122, _t7_123, _t7_124, _t7_125, _t7_126, _t7_127,
	_t7_128, _t7_129, _t7_130, _t7_131, _t7_132, _t7_133, _t7_134, _t7_135;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35, _t14_36, _t14_37, _t14_38, _t14_39,
	_t14_40, _t14_41, _t14_42, _t14_43, _t14_44, _t14_45, _t14_46, _t14_47,
	_t14_48, _t14_49, _t14_50, _t14_51, _t14_52, _t14_53, _t14_54, _t14_55,
	_t14_56, _t14_57, _t14_58, _t14_59, _t14_60, _t14_61, _t14_62, _t14_63,
	_t14_64, _t14_65, _t14_66, _t14_67, _t14_68, _t14_69, _t14_70, _t14_71,
	_t14_72, _t14_73, _t14_74, _t14_75, _t14_76, _t14_77, _t14_78, _t14_79,
	_t14_80, _t14_81, _t14_82, _t14_83, _t14_84, _t14_85, _t14_86, _t14_87,
	_t14_88, _t14_89, _t14_90, _t14_91, _t14_92, _t14_93, _t14_94, _t14_95,
	_t14_96, _t14_97, _t14_98, _t14_99, _t14_100, _t14_101, _t14_102, _t14_103,
	_t14_104, _t14_105, _t14_106, _t14_107, _t14_108, _t14_109, _t14_110, _t14_111,
	_t14_112, _t14_113, _t14_114, _t14_115, _t14_116, _t14_117, _t14_118, _t14_119,
	_t14_120, _t14_121, _t14_122, _t14_123, _t14_124, _t14_125, _t14_126, _t14_127,
	_t14_128, _t14_129, _t14_130, _t14_131, _t14_132, _t14_133, _t14_134, _t14_135,
	_t14_136, _t14_137, _t14_138, _t14_139, _t14_140, _t14_141, _t14_142, _t14_143,
	_t14_144, _t14_145, _t14_146, _t14_147, _t14_148, _t14_149, _t14_150, _t14_151,
	_t14_152, _t14_153, _t14_154, _t14_155, _t14_156, _t14_157, _t14_158, _t14_159,
	_t14_160, _t14_161, _t14_162, _t14_163, _t14_164, _t14_165, _t14_166, _t14_167,
	_t14_168, _t14_169, _t14_170, _t14_171, _t14_172, _t14_173, _t14_174, _t14_175,
	_t14_176, _t14_177, _t14_178, _t14_179, _t14_180, _t14_181, _t14_182, _t14_183,
	_t14_184, _t14_185, _t14_186, _t14_187, _t14_188, _t14_189, _t14_190, _t14_191,
	_t14_192, _t14_193, _t14_194, _t14_195, _t14_196, _t14_197, _t14_198, _t14_199,
	_t14_200, _t14_201, _t14_202, _t14_203, _t14_204, _t14_205, _t14_206, _t14_207,
	_t14_208, _t14_209, _t14_210, _t14_211, _t14_212, _t14_213, _t14_214, _t14_215,
	_t14_216, _t14_217, _t14_218, _t14_219, _t14_220, _t14_221, _t14_222, _t14_223,
	_t14_224, _t14_225, _t14_226, _t14_227, _t14_228, _t14_229, _t14_230, _t14_231,
	_t14_232, _t14_233, _t14_234, _t14_235, _t14_236, _t14_237, _t14_238, _t14_239,
	_t14_240, _t14_241, _t14_242, _t14_243, _t14_244, _t14_245, _t14_246, _t14_247,
	_t14_248, _t14_249, _t14_250, _t14_251, _t14_252, _t14_253, _t14_254, _t14_255,
	_t14_256, _t14_257, _t14_258, _t14_259, _t14_260, _t14_261, _t14_262, _t14_263,
	_t14_264, _t14_265, _t14_266, _t14_267, _t14_268, _t14_269, _t14_270, _t14_271,
	_t14_272, _t14_273, _t14_274, _t14_275, _t14_276, _t14_277, _t14_278, _t14_279,
	_t14_280, _t14_281, _t14_282, _t14_283, _t14_284, _t14_285, _t14_286, _t14_287,
	_t14_288, _t14_289, _t14_290, _t14_291, _t14_292, _t14_293, _t14_294, _t14_295,
	_t14_296, _t14_297, _t14_298, _t14_299, _t14_300, _t14_301, _t14_302, _t14_303,
	_t14_304, _t14_305, _t14_306, _t14_307, _t14_308, _t14_309, _t14_310, _t14_311,
	_t14_312, _t14_313, _t14_314, _t14_315, _t14_316, _t14_317, _t14_318, _t14_319,
	_t14_320, _t14_321, _t14_322, _t14_323, _t14_324, _t14_325, _t14_326, _t14_327,
	_t14_328, _t14_329, _t14_330, _t14_331, _t14_332;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23,
	_t17_24, _t17_25, _t17_26, _t17_27, _t17_28, _t17_29, _t17_30, _t17_31,
	_t17_32, _t17_33, _t17_34, _t17_35, _t17_36, _t17_37, _t17_38, _t17_39,
	_t17_40, _t17_41, _t17_42, _t17_43, _t17_44, _t17_45, _t17_46, _t17_47,
	_t17_48, _t17_49, _t17_50, _t17_51, _t17_52, _t17_53, _t17_54, _t17_55,
	_t17_56, _t17_57, _t17_58, _t17_59, _t17_60, _t17_61, _t17_62, _t17_63,
	_t17_64, _t17_65, _t17_66, _t17_67, _t17_68, _t17_69, _t17_70, _t17_71,
	_t17_72, _t17_73, _t17_74, _t17_75, _t17_76, _t17_77, _t17_78, _t17_79,
	_t17_80, _t17_81, _t17_82, _t17_83, _t17_84, _t17_85, _t17_86, _t17_87,
	_t17_88, _t17_89, _t17_90, _t17_91, _t17_92, _t17_93, _t17_94, _t17_95,
	_t17_96, _t17_97, _t17_98, _t17_99, _t17_100, _t17_101, _t17_102, _t17_103,
	_t17_104, _t17_105, _t17_106, _t17_107, _t17_108, _t17_109, _t17_110, _t17_111,
	_t17_112, _t17_113, _t17_114, _t17_115, _t17_116, _t17_117, _t17_118, _t17_119,
	_t17_120, _t17_121, _t17_122, _t17_123, _t17_124, _t17_125, _t17_126, _t17_127,
	_t17_128, _t17_129, _t17_130, _t17_131, _t17_132, _t17_133, _t17_134, _t17_135,
	_t17_136, _t17_137, _t17_138, _t17_139, _t17_140, _t17_141, _t17_142, _t17_143,
	_t17_144, _t17_145, _t17_146, _t17_147, _t17_148, _t17_149, _t17_150, _t17_151,
	_t17_152, _t17_153;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7,
	_t21_8, _t21_9, _t21_10, _t21_11, _t21_12, _t21_13, _t21_14, _t21_15,
	_t21_16, _t21_17, _t21_18, _t21_19, _t21_20, _t21_21, _t21_22, _t21_23,
	_t21_24, _t21_25, _t21_26, _t21_27, _t21_28, _t21_29, _t21_30, _t21_31,
	_t21_32, _t21_33, _t21_34, _t21_35, _t21_36, _t21_37, _t21_38, _t21_39,
	_t21_40, _t21_41, _t21_42, _t21_43, _t21_44, _t21_45, _t21_46, _t21_47,
	_t21_48, _t21_49, _t21_50, _t21_51, _t21_52, _t21_53, _t21_54, _t21_55,
	_t21_56, _t21_57, _t21_58, _t21_59, _t21_60, _t21_61, _t21_62, _t21_63,
	_t21_64, _t21_65, _t21_66, _t21_67, _t21_68, _t21_69, _t21_70, _t21_71,
	_t21_72, _t21_73, _t21_74, _t21_75, _t21_76, _t21_77, _t21_78, _t21_79,
	_t21_80, _t21_81, _t21_82, _t21_83, _t21_84, _t21_85, _t21_86, _t21_87,
	_t21_88, _t21_89, _t21_90, _t21_91, _t21_92, _t21_93, _t21_94, _t21_95,
	_t21_96, _t21_97, _t21_98, _t21_99, _t21_100, _t21_101;

  _t0_111 = _mm256_castpd128_pd256(_mm_load_sd(&(C[0])));
  _t0_102 = _mm256_castpd128_pd256(_mm_load_sd(&(L[0])));
  _t0_112 = _mm256_castpd128_pd256(_mm_load_sd(&(C[52])));
  _t0_101 = _mm256_castpd128_pd256(_mm_load_sd(&(L[52])));
  _t0_100 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53])));
  _t0_113 = _mm256_castpd128_pd256(_mm_load_sd(&(C[53])));
  _t0_114 = _mm256_maskload_pd(C + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_99 = _mm256_maskload_pd(L + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_98 = _mm256_castpd128_pd256(_mm_load_sd(&(L[106])));
  _t0_117 = _mm256_castpd128_pd256(_mm_load_sd(&(C[106])));
  _t0_118 = _mm256_maskload_pd(C + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_97 = _mm256_maskload_pd(L + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_96 = _mm256_castpd128_pd256(_mm_load_sd(&(L[159])));
  _t0_122 = _mm256_castpd128_pd256(_mm_load_sd(&(C[159])));
  _t0_276 = _asm256_loadu_pd(C + 208);
  _t0_277 = _asm256_loadu_pd(C + 260);
  _t0_278 = _asm256_loadu_pd(C + 312);
  _t0_279 = _asm256_loadu_pd(C + 364);
  _t0_95 = _mm256_broadcast_sd(L + 208);
  _t0_94 = _mm256_broadcast_sd(L + 209);
  _t0_93 = _mm256_broadcast_sd(L + 210);
  _t0_92 = _mm256_broadcast_sd(L + 211);
  _t0_91 = _mm256_broadcast_sd(L + 260);
  _t0_90 = _mm256_broadcast_sd(L + 261);
  _t0_89 = _mm256_broadcast_sd(L + 262);
  _t0_88 = _mm256_broadcast_sd(L + 263);
  _t0_87 = _mm256_broadcast_sd(L + 312);
  _t0_86 = _mm256_broadcast_sd(L + 313);
  _t0_85 = _mm256_broadcast_sd(L + 314);
  _t0_84 = _mm256_broadcast_sd(L + 315);
  _t0_83 = _mm256_broadcast_sd(L + 364);
  _t0_82 = _mm256_broadcast_sd(L + 365);
  _t0_81 = _mm256_broadcast_sd(L + 366);
  _t0_80 = _mm256_broadcast_sd(L + 367);
  _t0_79 = _mm256_castpd128_pd256(_mm_load_sd(&(L[212])));
  _t0_78 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 264)), _mm256_castpd128_pd256(_mm_load_sd(L + 316))), _mm256_castpd128_pd256(_mm_load_sd(L + 368)), 32);
  _t0_77 = _mm256_castpd128_pd256(_mm_load_sd(&(L[265])));
  _t0_76 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 317)), _mm256_castpd128_pd256(_mm_load_sd(L + 369)), 0);
  _t0_75 = _mm256_castpd128_pd256(_mm_load_sd(&(L[318])));
  _t0_74 = _mm256_broadcast_sd(&(L[370]));
  _t0_73 = _mm256_castpd128_pd256(_mm_load_sd(&(L[371])));
  _t0_139 = _mm256_castpd128_pd256(_mm_load_sd(C + 212));
  _t0_140 = _mm256_maskload_pd(C + 264, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_141 = _mm256_maskload_pd(C + 316, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_142 = _asm256_loadu_pd(C + 368);
  _t0_72 = _asm256_loadu_pd(L + 208);
  _t0_71 = _asm256_loadu_pd(L + 260);
  _t0_70 = _asm256_loadu_pd(L + 312);
  _t0_69 = _asm256_loadu_pd(L + 364);
  _t0_68 = _mm256_castpd128_pd256(_mm_load_sd(&(L[264])));
  _t0_67 = _mm256_maskload_pd(L + 316, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_66 = _mm256_maskload_pd(L + 368, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_260 = _asm256_loadu_pd(C + 416);
  _t0_261 = _asm256_loadu_pd(C + 468);
  _t0_262 = _asm256_loadu_pd(C + 520);
  _t0_263 = _asm256_loadu_pd(C + 572);
  _t0_65 = _mm256_broadcast_sd(L + 416);
  _t0_64 = _mm256_broadcast_sd(L + 417);
  _t0_63 = _mm256_broadcast_sd(L + 418);
  _t0_62 = _mm256_broadcast_sd(L + 419);
  _t0_61 = _mm256_broadcast_sd(L + 468);
  _t0_60 = _mm256_broadcast_sd(L + 469);
  _t0_59 = _mm256_broadcast_sd(L + 470);
  _t0_58 = _mm256_broadcast_sd(L + 471);
  _t0_57 = _mm256_broadcast_sd(L + 520);
  _t0_56 = _mm256_broadcast_sd(L + 521);
  _t0_55 = _mm256_broadcast_sd(L + 522);
  _t0_54 = _mm256_broadcast_sd(L + 523);
  _t0_53 = _mm256_broadcast_sd(L + 572);
  _t0_52 = _mm256_broadcast_sd(L + 573);
  _t0_51 = _mm256_broadcast_sd(L + 574);
  _t0_50 = _mm256_broadcast_sd(L + 575);
  _t0_264 = _asm256_loadu_pd(C + 420);
  _t0_265 = _asm256_loadu_pd(C + 472);
  _t0_266 = _asm256_loadu_pd(C + 524);
  _t0_267 = _asm256_loadu_pd(C + 576);
  _t0_49 = _mm256_broadcast_sd(L + 420);
  _t0_48 = _mm256_broadcast_sd(L + 421);
  _t0_47 = _mm256_broadcast_sd(L + 422);
  _t0_46 = _mm256_broadcast_sd(L + 423);
  _t0_45 = _mm256_broadcast_sd(L + 472);
  _t0_44 = _mm256_broadcast_sd(L + 473);
  _t0_43 = _mm256_broadcast_sd(L + 474);
  _t0_42 = _mm256_broadcast_sd(L + 475);
  _t0_41 = _mm256_broadcast_sd(L + 524);
  _t0_40 = _mm256_broadcast_sd(L + 525);
  _t0_39 = _mm256_broadcast_sd(L + 526);
  _t0_38 = _mm256_broadcast_sd(L + 527);
  _t0_37 = _mm256_broadcast_sd(L + 576);
  _t0_36 = _mm256_broadcast_sd(L + 577);
  _t0_35 = _mm256_broadcast_sd(L + 578);
  _t0_34 = _mm256_broadcast_sd(L + 579);
  _t0_33 = _mm256_castpd128_pd256(_mm_load_sd(&(L[424])));
  _t0_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 476)), _mm256_castpd128_pd256(_mm_load_sd(L + 528))), _mm256_castpd128_pd256(_mm_load_sd(L + 580)), 32);
  _t0_31 = _mm256_castpd128_pd256(_mm_load_sd(&(L[477])));
  _t0_30 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 529)), _mm256_castpd128_pd256(_mm_load_sd(L + 581)), 0);
  _t0_29 = _mm256_castpd128_pd256(_mm_load_sd(&(L[530])));
  _t0_28 = _mm256_broadcast_sd(&(L[582]));
  _t0_27 = _mm256_castpd128_pd256(_mm_load_sd(&(L[583])));
  _t0_186 = _mm256_castpd128_pd256(_mm_load_sd(C + 424));
  _t0_187 = _mm256_maskload_pd(C + 476, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_188 = _mm256_maskload_pd(C + 528, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_189 = _asm256_loadu_pd(C + 580);
  _t0_26 = _asm256_loadu_pd(L + 416);
  _t0_25 = _asm256_loadu_pd(L + 468);
  _t0_24 = _asm256_loadu_pd(L + 520);
  _t0_23 = _asm256_loadu_pd(L + 572);
  _t0_22 = _asm256_loadu_pd(L + 420);
  _t0_21 = _asm256_loadu_pd(L + 472);
  _t0_20 = _asm256_loadu_pd(L + 524);
  _t0_19 = _asm256_loadu_pd(L + 576);
  _t0_18 = _mm256_castpd128_pd256(_mm_load_sd(&(L[476])));
  _t0_17 = _mm256_maskload_pd(L + 528, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_16 = _mm256_maskload_pd(L + 580, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_272 = _asm256_loadu_pd(C + 624);
  _t0_273 = _asm256_loadu_pd(C + 676);
  _t0_274 = _asm256_loadu_pd(C + 728);
  _t0_275 = _asm256_loadu_pd(C + 780);
  _t0_15 = _mm256_broadcast_sd(L + 624);
  _t0_14 = _mm256_broadcast_sd(L + 625);
  _t0_13 = _mm256_broadcast_sd(L + 626);
  _t0_12 = _mm256_broadcast_sd(L + 627);
  _t0_11 = _mm256_broadcast_sd(L + 676);
  _t0_10 = _mm256_broadcast_sd(L + 677);
  _t0_9 = _mm256_broadcast_sd(L + 678);
  _t0_8 = _mm256_broadcast_sd(L + 679);
  _t0_7 = _mm256_broadcast_sd(L + 728);
  _t0_6 = _mm256_broadcast_sd(L + 729);
  _t0_5 = _mm256_broadcast_sd(L + 730);
  _t0_4 = _mm256_broadcast_sd(L + 731);
  _t0_3 = _mm256_broadcast_sd(L + 780);
  _t0_2 = _mm256_broadcast_sd(L + 781);
  _t0_1 = _mm256_broadcast_sd(L + 782);
  _t0_0 = _mm256_broadcast_sd(L + 783);

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 0)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_368 = _t0_111;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_377 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_383 = _t0_102;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_397 = _mm256_mul_pd(_t0_377, _t0_383);

  // 4-BLAC: 1x4 / 1x4
  _t0_412 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_368), _mm256_castpd256_pd128(_t0_397)));

  // AVX Storer:
  _t0_111 = _t0_412;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 0)) - ( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_427 = _t0_112;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_432 = _t0_101;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_438 = _t0_111;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_452 = _mm256_mul_pd(_t0_432, _t0_438);

  // 4-BLAC: 1x4 - 1x4
  _t0_468 = _mm256_sub_pd(_t0_427, _t0_452);

  // AVX Storer:
  _t0_112 = _t0_468;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_482 = _t0_112;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_487 = _t0_100;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_493 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_517 = _mm256_add_pd(_t0_487, _t0_493);

  // 4-BLAC: 1x4 / 1x4
  _t0_532 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_482), _mm256_castpd256_pd128(_t0_517)));

  // AVX Storer:
  _t0_112 = _t0_532;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 1)) - ( ( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) ) + ( G(h(1, 52, 1), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_547 = _t0_113;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_552 = _t0_101;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_564 = _t0_112;

  // 4-BLAC: (4x1)^T
  _t0_577 = _t0_564;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_591 = _mm256_mul_pd(_t0_552, _t0_577);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_597 = _t0_112;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_603 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_612 = _t0_603;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_628 = _mm256_mul_pd(_t0_597, _t0_612);

  // 4-BLAC: 1x4 + 1x4
  _t0_642 = _mm256_add_pd(_t0_591, _t0_628);

  // 4-BLAC: 1x4 - 1x4
  _t0_667 = _mm256_sub_pd(_t0_547, _t0_642);

  // AVX Storer:
  _t0_113 = _t0_667;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_681 = _t0_113;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_688 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_693 = _t0_100;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_711 = _mm256_mul_pd(_t0_688, _t0_693);

  // 4-BLAC: 1x4 / 1x4
  _t0_725 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_681), _mm256_castpd256_pd128(_t0_711)));

  // AVX Storer:
  _t0_113 = _t0_725;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(2, 52, 0)) - ( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) * G(h(2, 52, 0), X[52,52],h(2, 52, 0)) ) ),h(2, 52, 0))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_741 = _t0_114;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_749 = _t0_99;

  // AVX Loader:

  // 2x2 -> 4x4 - LowSymm
  _t0_755 = _mm256_shuffle_pd(_t0_111, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), 0);
  _t0_756 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12);
  _t0_757 = _mm256_setzero_pd();
  _t0_758 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_772 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_749, _t0_749, 32), _mm256_permute2f128_pd(_t0_749, _t0_749, 32), 0), _t0_755), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_749, _t0_749, 32), _mm256_permute2f128_pd(_t0_749, _t0_749, 32), 15), _t0_756)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_749, _t0_749, 49), _mm256_permute2f128_pd(_t0_749, _t0_749, 49), 0), _t0_757), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_749, _t0_749, 49), _mm256_permute2f128_pd(_t0_749, _t0_749, 49), 15), _t0_758)));

  // 4-BLAC: 1x4 - 1x4
  _t0_788 = _mm256_sub_pd(_t0_741, _t0_772);

  // AVX Storer:
  _t0_114 = _t0_788;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_805 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_114, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_806 = _t0_98;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_807 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_808 = _mm256_add_pd(_t0_806, _t0_807);

  // 4-BLAC: 1x4 / 1x4
  _t0_809 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_805), _mm256_castpd256_pd128(_t0_808)));

  // AVX Storer:
  _t0_115 = _t0_809;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 2), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_810 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_114, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_811 = _t0_115;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_812 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_813 = _t0_812;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_814 = _mm256_mul_pd(_t0_811, _t0_813);

  // 4-BLAC: 1x4 - 1x4
  _t0_815 = _mm256_sub_pd(_t0_810, _t0_814);

  // AVX Storer:
  _t0_116 = _t0_815;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_816 = _t0_116;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_817 = _t0_98;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_818 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_819 = _mm256_add_pd(_t0_817, _t0_818);

  // 4-BLAC: 1x4 / 1x4
  _t0_820 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_816), _mm256_castpd256_pd128(_t0_819)));

  // AVX Storer:
  _t0_116 = _t0_820;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 2)) - ( ( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), X[52,52],h(2, 52, 0)) ) ) + ( G(h(1, 52, 2), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_821 = _t0_117;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_822 = _t0_99;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_823 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_824 = _t0_823;

  // 4-BLAC: 1x4 * 4x1
  _t0_825 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_822, _t0_824), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_822, _t0_824), _mm256_mul_pd(_t0_822, _t0_824), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_822, _t0_824), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_822, _t0_824), _mm256_mul_pd(_t0_822, _t0_824), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_822, _t0_824), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_822, _t0_824), _mm256_mul_pd(_t0_822, _t0_824), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_826 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_827 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_828 = _t0_827;

  // 4-BLAC: 1x4 * 4x1
  _t0_829 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_826, _t0_828), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_826, _t0_828), _mm256_mul_pd(_t0_826, _t0_828), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_826, _t0_828), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_826, _t0_828), _mm256_mul_pd(_t0_826, _t0_828), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_826, _t0_828), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_826, _t0_828), _mm256_mul_pd(_t0_826, _t0_828), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_830 = _mm256_add_pd(_t0_825, _t0_829);

  // 4-BLAC: 1x4 - 1x4
  _t0_831 = _mm256_sub_pd(_t0_821, _t0_830);

  // AVX Storer:
  _t0_117 = _t0_831;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_832 = _t0_117;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_833 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_834 = _t0_98;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_835 = _mm256_mul_pd(_t0_833, _t0_834);

  // 4-BLAC: 1x4 / 1x4
  _t0_836 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_832), _mm256_castpd256_pd128(_t0_835)));

  // AVX Storer:
  _t0_117 = _t0_836;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(3, 52, 0)) - ( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) * G(h(3, 52, 0), X[52,52],h(3, 52, 0)) ) ),h(3, 52, 0))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_837 = _t0_118;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_838 = _t0_97;

  // AVX Loader:

  // 3x3 -> 4x4 - LowSymm
  _t0_839 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_111, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), 32), _t0_111, 8);
  _t0_840 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), 32), 6), _t0_111, 8);
  _t0_841 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32);
  _t0_842 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_843 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_838, _t0_838, 32), _mm256_permute2f128_pd(_t0_838, _t0_838, 32), 0), _t0_839), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_838, _t0_838, 32), _mm256_permute2f128_pd(_t0_838, _t0_838, 32), 15), _t0_840)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_838, _t0_838, 49), _mm256_permute2f128_pd(_t0_838, _t0_838, 49), 0), _t0_841), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_838, _t0_838, 49), _mm256_permute2f128_pd(_t0_838, _t0_838, 49), 15), _t0_842)));

  // 4-BLAC: 1x4 - 1x4
  _t0_844 = _mm256_sub_pd(_t0_837, _t0_843);

  // AVX Storer:
  _t0_118 = _t0_844;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_845 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_846 = _t0_96;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_847 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_848 = _mm256_add_pd(_t0_846, _t0_847);

  // 4-BLAC: 1x4 / 1x4
  _t0_849 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_845), _mm256_castpd256_pd128(_t0_848)));

  // AVX Storer:
  _t0_119 = _t0_849;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 3), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_850 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_851 = _t0_119;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_852 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_853 = _t0_852;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_854 = _mm256_mul_pd(_t0_851, _t0_853);

  // 4-BLAC: 1x4 - 1x4
  _t0_855 = _mm256_sub_pd(_t0_850, _t0_854);

  // AVX Storer:
  _t0_120 = _t0_855;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_856 = _t0_120;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_857 = _t0_96;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_858 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_859 = _mm256_add_pd(_t0_857, _t0_858);

  // 4-BLAC: 1x4 / 1x4
  _t0_860 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_856), _mm256_castpd256_pd128(_t0_859)));

  // AVX Storer:
  _t0_120 = _t0_860;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 3), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_861 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_862 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_863 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_864 = _t0_863;

  // 4-BLAC: 1x4 * 4x1
  _t0_865 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_862, _t0_864), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_862, _t0_864), _mm256_mul_pd(_t0_862, _t0_864), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_862, _t0_864), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_862, _t0_864), _mm256_mul_pd(_t0_862, _t0_864), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_862, _t0_864), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_862, _t0_864), _mm256_mul_pd(_t0_862, _t0_864), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_866 = _mm256_sub_pd(_t0_861, _t0_865);

  // AVX Storer:
  _t0_121 = _t0_866;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_867 = _t0_121;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_868 = _t0_96;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_869 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_870 = _mm256_add_pd(_t0_868, _t0_869);

  // 4-BLAC: 1x4 / 1x4
  _t0_871 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_867), _mm256_castpd256_pd128(_t0_870)));

  // AVX Storer:
  _t0_121 = _t0_871;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 3)) - ( ( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), X[52,52],h(3, 52, 0)) ) ) + ( G(h(1, 52, 3), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_872 = _t0_122;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_873 = _t0_97;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_874 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_875 = _t0_874;

  // 4-BLAC: 1x4 * 4x1
  _t0_876 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_873, _t0_875), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_873, _t0_875), _mm256_mul_pd(_t0_873, _t0_875), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_873, _t0_875), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_873, _t0_875), _mm256_mul_pd(_t0_873, _t0_875), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_873, _t0_875), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_873, _t0_875), _mm256_mul_pd(_t0_873, _t0_875), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_877 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_878 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_879 = _t0_878;

  // 4-BLAC: 1x4 * 4x1
  _t0_880 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_877, _t0_879), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_877, _t0_879), _mm256_mul_pd(_t0_877, _t0_879), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_877, _t0_879), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_877, _t0_879), _mm256_mul_pd(_t0_877, _t0_879), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_877, _t0_879), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_877, _t0_879), _mm256_mul_pd(_t0_877, _t0_879), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_881 = _mm256_add_pd(_t0_876, _t0_880);

  // 4-BLAC: 1x4 - 1x4
  _t0_882 = _mm256_sub_pd(_t0_872, _t0_881);

  // AVX Storer:
  _t0_122 = _t0_882;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_883 = _t0_122;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_884 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_885 = _t0_96;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_886 = _mm256_mul_pd(_t0_884, _t0_885);

  // 4-BLAC: 1x4 / 1x4
  _t0_887 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_883), _mm256_castpd256_pd128(_t0_886)));

  // AVX Storer:
  _t0_122 = _t0_887;

  // Generating : X[52,52] = S(h(4, 52, 4), ( G(h(4, 52, 4), C[52,52],h(4, 52, 0)) - ( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_888 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_111, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 0), 32);
  _t0_889 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 32);
  _t0_890 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 12);
  _t0_891 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32);

  // 4-BLAC: 4x4 * 4x4
  _t0_252 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_95, _t0_888), _mm256_mul_pd(_t0_94, _t0_889)), _mm256_add_pd(_mm256_mul_pd(_t0_93, _t0_890), _mm256_mul_pd(_t0_92, _t0_891)));
  _t0_253 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_91, _t0_888), _mm256_mul_pd(_t0_90, _t0_889)), _mm256_add_pd(_mm256_mul_pd(_t0_89, _t0_890), _mm256_mul_pd(_t0_88, _t0_891)));
  _t0_254 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_87, _t0_888), _mm256_mul_pd(_t0_86, _t0_889)), _mm256_add_pd(_mm256_mul_pd(_t0_85, _t0_890), _mm256_mul_pd(_t0_84, _t0_891)));
  _t0_255 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_83, _t0_888), _mm256_mul_pd(_t0_82, _t0_889)), _mm256_add_pd(_mm256_mul_pd(_t0_81, _t0_890), _mm256_mul_pd(_t0_80, _t0_891)));

  // 4-BLAC: 4x4 - 4x4
  _t0_276 = _mm256_sub_pd(_t0_276, _t0_252);
  _t0_277 = _mm256_sub_pd(_t0_277, _t0_253);
  _t0_278 = _mm256_sub_pd(_t0_278, _t0_254);
  _t0_279 = _mm256_sub_pd(_t0_279, _t0_255);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_892 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_276, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_893 = _t0_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_894 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_895 = _mm256_add_pd(_t0_893, _t0_894);

  // 4-BLAC: 1x4 / 1x4
  _t0_896 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_892), _mm256_castpd256_pd128(_t0_895)));

  // AVX Storer:
  _t0_123 = _t0_896;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 4), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_897 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_276, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_898 = _t0_123;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_899 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_900 = _t0_899;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_901 = _mm256_mul_pd(_t0_898, _t0_900);

  // 4-BLAC: 1x4 - 1x4
  _t0_902 = _mm256_sub_pd(_t0_897, _t0_901);

  // AVX Storer:
  _t0_124 = _t0_902;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_903 = _t0_124;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_904 = _t0_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_905 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_906 = _mm256_add_pd(_t0_904, _t0_905);

  // 4-BLAC: 1x4 / 1x4
  _t0_907 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_903), _mm256_castpd256_pd128(_t0_906)));

  // AVX Storer:
  _t0_124 = _t0_907;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 4), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_908 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_276, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_276, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_909 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_910 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_911 = _t0_910;

  // 4-BLAC: 1x4 * 4x1
  _t0_912 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_909, _t0_911), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_909, _t0_911), _mm256_mul_pd(_t0_909, _t0_911), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_909, _t0_911), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_909, _t0_911), _mm256_mul_pd(_t0_909, _t0_911), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_909, _t0_911), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_909, _t0_911), _mm256_mul_pd(_t0_909, _t0_911), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_913 = _mm256_sub_pd(_t0_908, _t0_912);

  // AVX Storer:
  _t0_125 = _t0_913;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_914 = _t0_125;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_915 = _t0_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_916 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_917 = _mm256_add_pd(_t0_915, _t0_916);

  // 4-BLAC: 1x4 / 1x4
  _t0_918 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_914), _mm256_castpd256_pd128(_t0_917)));

  // AVX Storer:
  _t0_125 = _t0_918;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 4), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_919 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_276, _t0_276, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_920 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_921 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_922 = _t0_921;

  // 4-BLAC: 1x4 * 4x1
  _t0_923 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_920, _t0_922), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_920, _t0_922), _mm256_mul_pd(_t0_920, _t0_922), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_920, _t0_922), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_920, _t0_922), _mm256_mul_pd(_t0_920, _t0_922), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_920, _t0_922), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_920, _t0_922), _mm256_mul_pd(_t0_920, _t0_922), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_924 = _mm256_sub_pd(_t0_919, _t0_923);

  // AVX Storer:
  _t0_126 = _t0_924;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_925 = _t0_126;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_926 = _t0_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_927 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_928 = _mm256_add_pd(_t0_926, _t0_927);

  // 4-BLAC: 1x4 / 1x4
  _t0_929 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_925), _mm256_castpd256_pd128(_t0_928)));

  // AVX Storer:
  _t0_126 = _t0_929;

  // Generating : X[52,52] = S(h(3, 52, 5), ( G(h(3, 52, 5), X[52,52],h(4, 52, 0)) - ( G(h(3, 52, 5), L[52,52],h(1, 52, 4)) * G(h(1, 52, 4), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t0_930 = _t0_277;
  _t0_931 = _t0_278;
  _t0_932 = _t0_279;
  _t0_933 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_934 = _t0_78;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t0_935 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_934, _t0_934, 32), _mm256_permute2f128_pd(_t0_934, _t0_934, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32));
  _t0_936 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_934, _t0_934, 32), _mm256_permute2f128_pd(_t0_934, _t0_934, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32));
  _t0_937 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_934, _t0_934, 49), _mm256_permute2f128_pd(_t0_934, _t0_934, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32));
  _t0_938 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_934, _t0_934, 49), _mm256_permute2f128_pd(_t0_934, _t0_934, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32));

  // 4-BLAC: 4x4 - 4x4
  _t0_939 = _mm256_sub_pd(_t0_930, _t0_935);
  _t0_940 = _mm256_sub_pd(_t0_931, _t0_936);
  _t0_941 = _mm256_sub_pd(_t0_932, _t0_937);
  _t0_942 = _mm256_sub_pd(_t0_933, _t0_938);

  // AVX Storer:
  _t0_277 = _t0_939;
  _t0_278 = _t0_940;
  _t0_279 = _t0_941;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_943 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_277, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_944 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_945 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_946 = _mm256_add_pd(_t0_944, _t0_945);

  // 4-BLAC: 1x4 / 1x4
  _t0_947 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_943), _mm256_castpd256_pd128(_t0_946)));

  // AVX Storer:
  _t0_127 = _t0_947;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 5), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_948 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_277, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_949 = _t0_127;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_950 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_951 = _t0_950;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_952 = _mm256_mul_pd(_t0_949, _t0_951);

  // 4-BLAC: 1x4 - 1x4
  _t0_953 = _mm256_sub_pd(_t0_948, _t0_952);

  // AVX Storer:
  _t0_128 = _t0_953;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_954 = _t0_128;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_955 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_956 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_957 = _mm256_add_pd(_t0_955, _t0_956);

  // 4-BLAC: 1x4 / 1x4
  _t0_958 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_954), _mm256_castpd256_pd128(_t0_957)));

  // AVX Storer:
  _t0_128 = _t0_958;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 5), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_959 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_277, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_277, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_960 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_961 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_962 = _t0_961;

  // 4-BLAC: 1x4 * 4x1
  _t0_963 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_960, _t0_962), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_960, _t0_962), _mm256_mul_pd(_t0_960, _t0_962), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_960, _t0_962), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_960, _t0_962), _mm256_mul_pd(_t0_960, _t0_962), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_960, _t0_962), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_960, _t0_962), _mm256_mul_pd(_t0_960, _t0_962), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_964 = _mm256_sub_pd(_t0_959, _t0_963);

  // AVX Storer:
  _t0_129 = _t0_964;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_965 = _t0_129;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_966 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_967 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_968 = _mm256_add_pd(_t0_966, _t0_967);

  // 4-BLAC: 1x4 / 1x4
  _t0_969 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_965), _mm256_castpd256_pd128(_t0_968)));

  // AVX Storer:
  _t0_129 = _t0_969;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 5), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_970 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_277, _t0_277, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_971 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_972 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_973 = _t0_972;

  // 4-BLAC: 1x4 * 4x1
  _t0_974 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_971, _t0_973), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_971, _t0_973), _mm256_mul_pd(_t0_971, _t0_973), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_971, _t0_973), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_971, _t0_973), _mm256_mul_pd(_t0_971, _t0_973), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_971, _t0_973), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_971, _t0_973), _mm256_mul_pd(_t0_971, _t0_973), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_975 = _mm256_sub_pd(_t0_970, _t0_974);

  // AVX Storer:
  _t0_130 = _t0_975;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_976 = _t0_130;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_977 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_978 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_979 = _mm256_add_pd(_t0_977, _t0_978);

  // 4-BLAC: 1x4 / 1x4
  _t0_980 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_976), _mm256_castpd256_pd128(_t0_979)));

  // AVX Storer:
  _t0_130 = _t0_980;

  // Generating : X[52,52] = S(h(2, 52, 6), ( G(h(2, 52, 6), X[52,52],h(4, 52, 0)) - ( G(h(2, 52, 6), L[52,52],h(1, 52, 5)) * G(h(1, 52, 5), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t0_981 = _t0_278;
  _t0_982 = _t0_279;
  _t0_983 = _mm256_setzero_pd();
  _t0_984 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_985 = _t0_76;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t0_986 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_985, _t0_985, 32), _mm256_permute2f128_pd(_t0_985, _t0_985, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32));
  _t0_987 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_985, _t0_985, 32), _mm256_permute2f128_pd(_t0_985, _t0_985, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32));
  _t0_988 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_985, _t0_985, 49), _mm256_permute2f128_pd(_t0_985, _t0_985, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32));
  _t0_989 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_985, _t0_985, 49), _mm256_permute2f128_pd(_t0_985, _t0_985, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32));

  // 4-BLAC: 4x4 - 4x4
  _t0_990 = _mm256_sub_pd(_t0_981, _t0_986);
  _t0_991 = _mm256_sub_pd(_t0_982, _t0_987);
  _t0_992 = _mm256_sub_pd(_t0_983, _t0_988);
  _t0_993 = _mm256_sub_pd(_t0_984, _t0_989);

  // AVX Storer:
  _t0_278 = _t0_990;
  _t0_279 = _t0_991;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_994 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_278, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_995 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_996 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_997 = _mm256_add_pd(_t0_995, _t0_996);

  // 4-BLAC: 1x4 / 1x4
  _t0_998 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_994), _mm256_castpd256_pd128(_t0_997)));

  // AVX Storer:
  _t0_131 = _t0_998;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 6), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_999 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_278, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1000 = _t0_131;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1001 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_1002 = _t0_1001;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1003 = _mm256_mul_pd(_t0_1000, _t0_1002);

  // 4-BLAC: 1x4 - 1x4
  _t0_1004 = _mm256_sub_pd(_t0_999, _t0_1003);

  // AVX Storer:
  _t0_132 = _t0_1004;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1005 = _t0_132;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1006 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1007 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_1008 = _mm256_add_pd(_t0_1006, _t0_1007);

  // 4-BLAC: 1x4 / 1x4
  _t0_1009 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1005), _mm256_castpd256_pd128(_t0_1008)));

  // AVX Storer:
  _t0_132 = _t0_1009;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 6), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1010 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_278, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_278, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1011 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1012 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_1013 = _t0_1012;

  // 4-BLAC: 1x4 * 4x1
  _t0_1014 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1011, _t0_1013), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1011, _t0_1013), _mm256_mul_pd(_t0_1011, _t0_1013), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1011, _t0_1013), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1011, _t0_1013), _mm256_mul_pd(_t0_1011, _t0_1013), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1011, _t0_1013), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1011, _t0_1013), _mm256_mul_pd(_t0_1011, _t0_1013), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_1015 = _mm256_sub_pd(_t0_1010, _t0_1014);

  // AVX Storer:
  _t0_133 = _t0_1015;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1016 = _t0_133;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1017 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1018 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_1019 = _mm256_add_pd(_t0_1017, _t0_1018);

  // 4-BLAC: 1x4 / 1x4
  _t0_1020 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1016), _mm256_castpd256_pd128(_t0_1019)));

  // AVX Storer:
  _t0_133 = _t0_1020;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 6), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1021 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_278, _t0_278, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1022 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1023 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_1024 = _t0_1023;

  // 4-BLAC: 1x4 * 4x1
  _t0_1025 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1022, _t0_1024), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1022, _t0_1024), _mm256_mul_pd(_t0_1022, _t0_1024), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1022, _t0_1024), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1022, _t0_1024), _mm256_mul_pd(_t0_1022, _t0_1024), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1022, _t0_1024), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1022, _t0_1024), _mm256_mul_pd(_t0_1022, _t0_1024), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_1026 = _mm256_sub_pd(_t0_1021, _t0_1025);

  // AVX Storer:
  _t0_134 = _t0_1026;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1027 = _t0_134;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1028 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1029 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_1030 = _mm256_add_pd(_t0_1028, _t0_1029);

  // 4-BLAC: 1x4 / 1x4
  _t0_1031 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1027), _mm256_castpd256_pd128(_t0_1030)));

  // AVX Storer:
  _t0_134 = _t0_1031;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 7), L[52,52],h(1, 52, 6)) Kro G(h(1, 52, 6), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1032 = _t0_74;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t0_203 = _mm256_mul_pd(_t0_1032, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_279 = _mm256_sub_pd(_t0_279, _t0_203);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1033 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_279, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1034 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1035 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_1036 = _mm256_add_pd(_t0_1034, _t0_1035);

  // 4-BLAC: 1x4 / 1x4
  _t0_1037 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1033), _mm256_castpd256_pd128(_t0_1036)));

  // AVX Storer:
  _t0_135 = _t0_1037;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 7), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1038 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_279, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1039 = _t0_135;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1040 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_1041 = _t0_1040;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1042 = _mm256_mul_pd(_t0_1039, _t0_1041);

  // 4-BLAC: 1x4 - 1x4
  _t0_1043 = _mm256_sub_pd(_t0_1038, _t0_1042);

  // AVX Storer:
  _t0_136 = _t0_1043;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1044 = _t0_136;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1045 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1046 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_1047 = _mm256_add_pd(_t0_1045, _t0_1046);

  // 4-BLAC: 1x4 / 1x4
  _t0_1048 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1044), _mm256_castpd256_pd128(_t0_1047)));

  // AVX Storer:
  _t0_136 = _t0_1048;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 7), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1049 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_279, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_279, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1050 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1051 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_1052 = _t0_1051;

  // 4-BLAC: 1x4 * 4x1
  _t0_1053 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1050, _t0_1052), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1050, _t0_1052), _mm256_mul_pd(_t0_1050, _t0_1052), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1050, _t0_1052), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1050, _t0_1052), _mm256_mul_pd(_t0_1050, _t0_1052), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1050, _t0_1052), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1050, _t0_1052), _mm256_mul_pd(_t0_1050, _t0_1052), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_1054 = _mm256_sub_pd(_t0_1049, _t0_1053);

  // AVX Storer:
  _t0_137 = _t0_1054;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1055 = _t0_137;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1056 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1057 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_1058 = _mm256_add_pd(_t0_1056, _t0_1057);

  // 4-BLAC: 1x4 / 1x4
  _t0_1059 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1055), _mm256_castpd256_pd128(_t0_1058)));

  // AVX Storer:
  _t0_137 = _t0_1059;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 7), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1060 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_279, _t0_279, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1061 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1062 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_1063 = _t0_1062;

  // 4-BLAC: 1x4 * 4x1
  _t0_1064 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1061, _t0_1063), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1061, _t0_1063), _mm256_mul_pd(_t0_1061, _t0_1063), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1061, _t0_1063), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1061, _t0_1063), _mm256_mul_pd(_t0_1061, _t0_1063), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1061, _t0_1063), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1061, _t0_1063), _mm256_mul_pd(_t0_1061, _t0_1063), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_1065 = _mm256_sub_pd(_t0_1060, _t0_1064);

  // AVX Storer:
  _t0_138 = _t0_1065;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1066 = _t0_138;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1067 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1068 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_1069 = _mm256_add_pd(_t0_1067, _t0_1068);

  // 4-BLAC: 1x4 / 1x4
  _t0_1070 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1066), _mm256_castpd256_pd128(_t0_1069)));

  // AVX Storer:
  _t0_138 = _t0_1070;

  // Generating : X[52,52] = S(h(4, 52, 4), ( G(h(4, 52, 4), C[52,52],h(4, 52, 4)) - ( ( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, 4), X[52,52],h(4, 52, 0)) ) ) + ( G(h(4, 52, 4), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) ) ) ) ),h(4, 52, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_1071 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_139, _t0_140, 0), _mm256_shuffle_pd(_t0_141, _t0_142, 0), 32);
  _t0_1072 = _mm256_permute2f128_pd(_t0_140, _mm256_shuffle_pd(_t0_141, _t0_142, 3), 32);
  _t0_1073 = _mm256_blend_pd(_t0_141, _mm256_shuffle_pd(_t0_141, _t0_142, 3), 12);
  _t0_1074 = _t0_142;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1215 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 32);
  _t0_1216 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 32);
  _t0_1217 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 49);
  _t0_1218 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_204 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_95, _t0_1215), _mm256_mul_pd(_t0_94, _t0_1216)), _mm256_add_pd(_mm256_mul_pd(_t0_93, _t0_1217), _mm256_mul_pd(_t0_92, _t0_1218)));
  _t0_205 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_91, _t0_1215), _mm256_mul_pd(_t0_90, _t0_1216)), _mm256_add_pd(_mm256_mul_pd(_t0_89, _t0_1217), _mm256_mul_pd(_t0_88, _t0_1218)));
  _t0_206 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_87, _t0_1215), _mm256_mul_pd(_t0_86, _t0_1216)), _mm256_add_pd(_mm256_mul_pd(_t0_85, _t0_1217), _mm256_mul_pd(_t0_84, _t0_1218)));
  _t0_207 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_83, _t0_1215), _mm256_mul_pd(_t0_82, _t0_1216)), _mm256_add_pd(_mm256_mul_pd(_t0_81, _t0_1217), _mm256_mul_pd(_t0_80, _t0_1218)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1219 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 32);
  _t0_1220 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 32);
  _t0_1221 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 49);
  _t0_1222 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_208 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_123, _t0_123, 32), _mm256_permute2f128_pd(_t0_123, _t0_123, 32), 0), _t0_1219), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_124, _t0_124, 32), _mm256_permute2f128_pd(_t0_124, _t0_124, 32), 0), _t0_1220)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_125, _t0_125, 32), _mm256_permute2f128_pd(_t0_125, _t0_125, 32), 0), _t0_1221), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_126, _t0_126, 32), _mm256_permute2f128_pd(_t0_126, _t0_126, 32), 0), _t0_1222)));
  _t0_209 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_127, _t0_127, 32), _mm256_permute2f128_pd(_t0_127, _t0_127, 32), 0), _t0_1219), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_128, _t0_128, 32), _mm256_permute2f128_pd(_t0_128, _t0_128, 32), 0), _t0_1220)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_129, _t0_129, 32), _mm256_permute2f128_pd(_t0_129, _t0_129, 32), 0), _t0_1221), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_130, _t0_130, 32), _mm256_permute2f128_pd(_t0_130, _t0_130, 32), 0), _t0_1222)));
  _t0_210 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_131, _t0_131, 32), _mm256_permute2f128_pd(_t0_131, _t0_131, 32), 0), _t0_1219), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_132, _t0_132, 32), _mm256_permute2f128_pd(_t0_132, _t0_132, 32), 0), _t0_1220)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_133, _t0_133, 32), _mm256_permute2f128_pd(_t0_133, _t0_133, 32), 0), _t0_1221), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_134, _t0_134, 32), _mm256_permute2f128_pd(_t0_134, _t0_134, 32), 0), _t0_1222)));
  _t0_211 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_135, _t0_135, 32), _mm256_permute2f128_pd(_t0_135, _t0_135, 32), 0), _t0_1219), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_136, _t0_136, 32), _mm256_permute2f128_pd(_t0_136, _t0_136, 32), 0), _t0_1220)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_137, _t0_137, 32), _mm256_permute2f128_pd(_t0_137, _t0_137, 32), 0), _t0_1221), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_138, _t0_138, 32), _mm256_permute2f128_pd(_t0_138, _t0_138, 32), 0), _t0_1222)));

  // 4-BLAC: 4x4 + 4x4
  _t0_103 = _mm256_add_pd(_t0_204, _t0_208);
  _t0_104 = _mm256_add_pd(_t0_205, _t0_209);
  _t0_105 = _mm256_add_pd(_t0_206, _t0_210);
  _t0_106 = _mm256_add_pd(_t0_207, _t0_211);

  // 4-BLAC: 4x4 - 4x4
  _t0_256 = _mm256_sub_pd(_t0_1071, _t0_103);
  _t0_257 = _mm256_sub_pd(_t0_1072, _t0_104);
  _t0_258 = _mm256_sub_pd(_t0_1073, _t0_105);
  _t0_259 = _mm256_sub_pd(_t0_1074, _t0_106);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t0_139 = _t0_256;
  _t0_140 = _t0_257;
  _t0_141 = _t0_258;
  _t0_142 = _t0_259;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 4)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1075 = _t0_139;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_1076 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1077 = _t0_79;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1078 = _mm256_mul_pd(_t0_1076, _t0_1077);

  // 4-BLAC: 1x4 / 1x4
  _t0_1079 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1075), _mm256_castpd256_pd128(_t0_1078)));

  // AVX Storer:
  _t0_139 = _t0_1079;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 4)) - ( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), X[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1080 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_140, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1081 = _t0_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1082 = _t0_139;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1083 = _mm256_mul_pd(_t0_1081, _t0_1082);

  // 4-BLAC: 1x4 - 1x4
  _t0_1084 = _mm256_sub_pd(_t0_1080, _t0_1083);

  // AVX Storer:
  _t0_143 = _t0_1084;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1085 = _t0_143;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1086 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1087 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_1088 = _mm256_add_pd(_t0_1086, _t0_1087);

  // 4-BLAC: 1x4 / 1x4
  _t0_1089 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1085), _mm256_castpd256_pd128(_t0_1088)));

  // AVX Storer:
  _t0_143 = _t0_1089;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 5)) - ( ( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), X[52,52],h(1, 52, 4)) ) ) + ( G(h(1, 52, 5), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1090 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_140, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1091 = _t0_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1092 = _t0_143;

  // 4-BLAC: (4x1)^T
  _t0_1093 = _t0_1092;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1094 = _mm256_mul_pd(_t0_1091, _t0_1093);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1095 = _t0_143;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1096 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_1097 = _t0_1096;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1098 = _mm256_mul_pd(_t0_1095, _t0_1097);

  // 4-BLAC: 1x4 + 1x4
  _t0_1099 = _mm256_add_pd(_t0_1094, _t0_1098);

  // 4-BLAC: 1x4 - 1x4
  _t0_1100 = _mm256_sub_pd(_t0_1090, _t0_1099);

  // AVX Storer:
  _t0_144 = _t0_1100;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 5)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1101 = _t0_144;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_1102 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1103 = _t0_77;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1104 = _mm256_mul_pd(_t0_1102, _t0_1103);

  // 4-BLAC: 1x4 / 1x4
  _t0_1105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1101), _mm256_castpd256_pd128(_t0_1104)));

  // AVX Storer:
  _t0_144 = _t0_1105;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(2, 52, 4)) - ( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) * G(h(2, 52, 4), X[52,52],h(2, 52, 4)) ) ),h(2, 52, 4))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1106 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_141, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1107 = _t0_67;

  // AVX Loader:

  // 2x2 -> 4x4 - LowSymm
  _t0_1108 = _mm256_shuffle_pd(_t0_139, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), 0);
  _t0_1109 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12);
  _t0_1110 = _mm256_setzero_pd();
  _t0_1111 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_1112 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1107, _t0_1107, 32), _mm256_permute2f128_pd(_t0_1107, _t0_1107, 32), 0), _t0_1108), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1107, _t0_1107, 32), _mm256_permute2f128_pd(_t0_1107, _t0_1107, 32), 15), _t0_1109)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1107, _t0_1107, 49), _mm256_permute2f128_pd(_t0_1107, _t0_1107, 49), 0), _t0_1110), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1107, _t0_1107, 49), _mm256_permute2f128_pd(_t0_1107, _t0_1107, 49), 15), _t0_1111)));

  // 4-BLAC: 1x4 - 1x4
  _t0_1113 = _mm256_sub_pd(_t0_1106, _t0_1112);

  // AVX Storer:
  _t0_145 = _t0_1113;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1114 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_145, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1115 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1116 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_1117 = _mm256_add_pd(_t0_1115, _t0_1116);

  // 4-BLAC: 1x4 / 1x4
  _t0_1118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1114), _mm256_castpd256_pd128(_t0_1117)));

  // AVX Storer:
  _t0_146 = _t0_1118;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 6), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1119 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_145, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1120 = _t0_146;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1121 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_1122 = _t0_1121;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1123 = _mm256_mul_pd(_t0_1120, _t0_1122);

  // 4-BLAC: 1x4 - 1x4
  _t0_1124 = _mm256_sub_pd(_t0_1119, _t0_1123);

  // AVX Storer:
  _t0_147 = _t0_1124;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1125 = _t0_147;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1126 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1127 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t0_1128 = _mm256_add_pd(_t0_1126, _t0_1127);

  // 4-BLAC: 1x4 / 1x4
  _t0_1129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1125), _mm256_castpd256_pd128(_t0_1128)));

  // AVX Storer:
  _t0_147 = _t0_1129;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 6)) - ( ( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), X[52,52],h(2, 52, 4)) ) ) + ( G(h(1, 52, 6), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1130 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_141, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_141, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1131 = _t0_67;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1132 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_1133 = _t0_1132;

  // 4-BLAC: 1x4 * 4x1
  _t0_1134 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1131, _t0_1133), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1131, _t0_1133), _mm256_mul_pd(_t0_1131, _t0_1133), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1131, _t0_1133), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1131, _t0_1133), _mm256_mul_pd(_t0_1131, _t0_1133), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1131, _t0_1133), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1131, _t0_1133), _mm256_mul_pd(_t0_1131, _t0_1133), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1135 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1136 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t0_1137 = _t0_1136;

  // 4-BLAC: 1x4 * 4x1
  _t0_1138 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1135, _t0_1137), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1135, _t0_1137), _mm256_mul_pd(_t0_1135, _t0_1137), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1135, _t0_1137), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1135, _t0_1137), _mm256_mul_pd(_t0_1135, _t0_1137), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1135, _t0_1137), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1135, _t0_1137), _mm256_mul_pd(_t0_1135, _t0_1137), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_1139 = _mm256_add_pd(_t0_1134, _t0_1138);

  // 4-BLAC: 1x4 - 1x4
  _t0_1140 = _mm256_sub_pd(_t0_1130, _t0_1139);

  // AVX Storer:
  _t0_148 = _t0_1140;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 6)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1141 = _t0_148;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_1142 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1143 = _t0_75;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1144 = _mm256_mul_pd(_t0_1142, _t0_1143);

  // 4-BLAC: 1x4 / 1x4
  _t0_1145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1141), _mm256_castpd256_pd128(_t0_1144)));

  // AVX Storer:
  _t0_148 = _t0_1145;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(3, 52, 4)) - ( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) * G(h(3, 52, 4), X[52,52],h(3, 52, 4)) ) ),h(3, 52, 4))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1146 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_142, 7);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1147 = _t0_66;

  // AVX Loader:

  // 3x3 -> 4x4 - LowSymm
  _t0_1148 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_139, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), 32), _t0_139, 8);
  _t0_1149 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), 32), 6), _t0_139, 8);
  _t0_1150 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32);
  _t0_1151 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_1152 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1147, _t0_1147, 32), _mm256_permute2f128_pd(_t0_1147, _t0_1147, 32), 0), _t0_1148), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1147, _t0_1147, 32), _mm256_permute2f128_pd(_t0_1147, _t0_1147, 32), 15), _t0_1149)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1147, _t0_1147, 49), _mm256_permute2f128_pd(_t0_1147, _t0_1147, 49), 0), _t0_1150), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1147, _t0_1147, 49), _mm256_permute2f128_pd(_t0_1147, _t0_1147, 49), 15), _t0_1151)));

  // 4-BLAC: 1x4 - 1x4
  _t0_1153 = _mm256_sub_pd(_t0_1146, _t0_1152);

  // AVX Storer:
  _t0_149 = _t0_1153;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1154 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_149, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1155 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1156 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_1157 = _mm256_add_pd(_t0_1155, _t0_1156);

  // 4-BLAC: 1x4 / 1x4
  _t0_1158 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1154), _mm256_castpd256_pd128(_t0_1157)));

  // AVX Storer:
  _t0_150 = _t0_1158;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 7), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1159 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_149, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1160 = _t0_150;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1161 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_1162 = _t0_1161;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1163 = _mm256_mul_pd(_t0_1160, _t0_1162);

  // 4-BLAC: 1x4 - 1x4
  _t0_1164 = _mm256_sub_pd(_t0_1159, _t0_1163);

  // AVX Storer:
  _t0_151 = _t0_1164;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1165 = _t0_151;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1166 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1167 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t0_1168 = _mm256_add_pd(_t0_1166, _t0_1167);

  // 4-BLAC: 1x4 / 1x4
  _t0_1169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1165), _mm256_castpd256_pd128(_t0_1168)));

  // AVX Storer:
  _t0_151 = _t0_1169;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 7), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1170 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_149, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_149, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1171 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_1172 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t0_1173 = _t0_1172;

  // 4-BLAC: 1x4 * 4x1
  _t0_1174 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1171, _t0_1173), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1171, _t0_1173), _mm256_mul_pd(_t0_1171, _t0_1173), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1171, _t0_1173), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1171, _t0_1173), _mm256_mul_pd(_t0_1171, _t0_1173), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1171, _t0_1173), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1171, _t0_1173), _mm256_mul_pd(_t0_1171, _t0_1173), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_1175 = _mm256_sub_pd(_t0_1170, _t0_1174);

  // AVX Storer:
  _t0_152 = _t0_1175;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1176 = _t0_152;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1177 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1178 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t0_1179 = _mm256_add_pd(_t0_1177, _t0_1178);

  // 4-BLAC: 1x4 / 1x4
  _t0_1180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1176), _mm256_castpd256_pd128(_t0_1179)));

  // AVX Storer:
  _t0_152 = _t0_1180;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 7)) - ( ( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), X[52,52],h(3, 52, 4)) ) ) + ( G(h(1, 52, 7), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1181 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_142, _t0_142, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1182 = _t0_66;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1183 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_1184 = _t0_1183;

  // 4-BLAC: 1x4 * 4x1
  _t0_1185 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1182, _t0_1184), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1182, _t0_1184), _mm256_mul_pd(_t0_1182, _t0_1184), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1182, _t0_1184), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1182, _t0_1184), _mm256_mul_pd(_t0_1182, _t0_1184), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1182, _t0_1184), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1182, _t0_1184), _mm256_mul_pd(_t0_1182, _t0_1184), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1186 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_1187 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t0_1188 = _t0_1187;

  // 4-BLAC: 1x4 * 4x1
  _t0_1189 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1186, _t0_1188), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1186, _t0_1188), _mm256_mul_pd(_t0_1186, _t0_1188), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_1186, _t0_1188), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1186, _t0_1188), _mm256_mul_pd(_t0_1186, _t0_1188), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_1186, _t0_1188), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_1186, _t0_1188), _mm256_mul_pd(_t0_1186, _t0_1188), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_1190 = _mm256_add_pd(_t0_1185, _t0_1189);

  // 4-BLAC: 1x4 - 1x4
  _t0_1191 = _mm256_sub_pd(_t0_1181, _t0_1190);

  // AVX Storer:
  _t0_153 = _t0_1191;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 7)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1192 = _t0_153;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_1193 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1194 = _t0_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1195 = _mm256_mul_pd(_t0_1193, _t0_1194);

  // 4-BLAC: 1x4 / 1x4
  _t0_1196 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1192), _mm256_castpd256_pd128(_t0_1195)));

  // AVX Storer:
  _t0_153 = _t0_1196;

  // Generating : X[52,52] = ( ( ( S(h(4, 52, 8), ( G(h(4, 52, 8), C[52,52],h(4, 52, 0)) - ( G(h(4, 52, 8), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0)) + S(h(4, 52, 8), ( G(h(4, 52, 8), C[52,52],h(4, 52, 4)) - ( G(h(4, 52, 8), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, 4), X[52,52],h(4, 52, 0)) ) ) ),h(4, 52, 4)) ) + -$(h(4, 52, 8), ( G(h(4, 52, 8), L[52,52],h(4, 52, 4)) * G(h(4, 52, 4), X[52,52],h(4, 52, 0)) ),h(4, 52, 0)) ) + -$(h(4, 52, 8), ( G(h(4, 52, 8), L[52,52],h(4, 52, 4)) * G(h(4, 52, 4), X[52,52],h(4, 52, 4)) ),h(4, 52, 4)) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_1197 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_111, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 0), 32);
  _t0_1198 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 32);
  _t0_1199 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 12);
  _t0_1200 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32);

  // 4-BLAC: 4x4 * 4x4
  _t0_212 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_65, _t0_1197), _mm256_mul_pd(_t0_64, _t0_1198)), _mm256_add_pd(_mm256_mul_pd(_t0_63, _t0_1199), _mm256_mul_pd(_t0_62, _t0_1200)));
  _t0_213 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_61, _t0_1197), _mm256_mul_pd(_t0_60, _t0_1198)), _mm256_add_pd(_mm256_mul_pd(_t0_59, _t0_1199), _mm256_mul_pd(_t0_58, _t0_1200)));
  _t0_214 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_57, _t0_1197), _mm256_mul_pd(_t0_56, _t0_1198)), _mm256_add_pd(_mm256_mul_pd(_t0_55, _t0_1199), _mm256_mul_pd(_t0_54, _t0_1200)));
  _t0_215 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_53, _t0_1197), _mm256_mul_pd(_t0_52, _t0_1198)), _mm256_add_pd(_mm256_mul_pd(_t0_51, _t0_1199), _mm256_mul_pd(_t0_50, _t0_1200)));

  // 4-BLAC: 4x4 - 4x4
  _t0_260 = _mm256_sub_pd(_t0_260, _t0_212);
  _t0_261 = _mm256_sub_pd(_t0_261, _t0_213);
  _t0_262 = _mm256_sub_pd(_t0_262, _t0_214);
  _t0_263 = _mm256_sub_pd(_t0_263, _t0_215);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1223 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 32);
  _t0_1224 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 32);
  _t0_1225 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 49);
  _t0_1226 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_216 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_65, _t0_1223), _mm256_mul_pd(_t0_64, _t0_1224)), _mm256_add_pd(_mm256_mul_pd(_t0_63, _t0_1225), _mm256_mul_pd(_t0_62, _t0_1226)));
  _t0_217 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_61, _t0_1223), _mm256_mul_pd(_t0_60, _t0_1224)), _mm256_add_pd(_mm256_mul_pd(_t0_59, _t0_1225), _mm256_mul_pd(_t0_58, _t0_1226)));
  _t0_218 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_57, _t0_1223), _mm256_mul_pd(_t0_56, _t0_1224)), _mm256_add_pd(_mm256_mul_pd(_t0_55, _t0_1225), _mm256_mul_pd(_t0_54, _t0_1226)));
  _t0_219 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_53, _t0_1223), _mm256_mul_pd(_t0_52, _t0_1224)), _mm256_add_pd(_mm256_mul_pd(_t0_51, _t0_1225), _mm256_mul_pd(_t0_50, _t0_1226)));

  // 4-BLAC: 4x4 - 4x4
  _t0_264 = _mm256_sub_pd(_t0_264, _t0_216);
  _t0_265 = _mm256_sub_pd(_t0_265, _t0_217);
  _t0_266 = _mm256_sub_pd(_t0_266, _t0_218);
  _t0_267 = _mm256_sub_pd(_t0_267, _t0_219);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t0_220 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_49, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t0_48, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t0_47, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t0_46, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));
  _t0_221 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_45, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t0_44, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t0_43, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t0_42, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));
  _t0_222 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t0_40, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t0_39, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t0_38, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));
  _t0_223 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_37, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t0_36, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t0_35, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t0_34, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t0_260 = _mm256_sub_pd(_t0_260, _t0_220);
  _t0_261 = _mm256_sub_pd(_t0_261, _t0_221);
  _t0_262 = _mm256_sub_pd(_t0_262, _t0_222);
  _t0_263 = _mm256_sub_pd(_t0_263, _t0_223);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_1201 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_139, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32), 0), 32);
  _t0_1202 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32), 3), 32);
  _t0_1203 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32), 3), 12);
  _t0_1204 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32);

  // 4-BLAC: 4x4 * 4x4
  _t0_224 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_49, _t0_1201), _mm256_mul_pd(_t0_48, _t0_1202)), _mm256_add_pd(_mm256_mul_pd(_t0_47, _t0_1203), _mm256_mul_pd(_t0_46, _t0_1204)));
  _t0_225 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_45, _t0_1201), _mm256_mul_pd(_t0_44, _t0_1202)), _mm256_add_pd(_mm256_mul_pd(_t0_43, _t0_1203), _mm256_mul_pd(_t0_42, _t0_1204)));
  _t0_226 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_1201), _mm256_mul_pd(_t0_40, _t0_1202)), _mm256_add_pd(_mm256_mul_pd(_t0_39, _t0_1203), _mm256_mul_pd(_t0_38, _t0_1204)));
  _t0_227 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_37, _t0_1201), _mm256_mul_pd(_t0_36, _t0_1202)), _mm256_add_pd(_mm256_mul_pd(_t0_35, _t0_1203), _mm256_mul_pd(_t0_34, _t0_1204)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t0_264 = _mm256_sub_pd(_t0_264, _t0_224);
  _t0_265 = _mm256_sub_pd(_t0_265, _t0_225);
  _t0_266 = _mm256_sub_pd(_t0_266, _t0_226);
  _t0_267 = _mm256_sub_pd(_t0_267, _t0_227);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1205 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_260, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1206 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1207 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_1208 = _mm256_add_pd(_t0_1206, _t0_1207);

  // 4-BLAC: 1x4 / 1x4
  _t0_1209 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_1205), _mm256_castpd256_pd128(_t0_1208)));

  // AVX Storer:
  _t0_154 = _t0_1209;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 8), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1210 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_260, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1211 = _t0_154;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_1212 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_1213 = _t0_1212;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_1214 = _mm256_mul_pd(_t0_1211, _t0_1213);

  // 4-BLAC: 1x4 - 1x4
  _t0_280 = _mm256_sub_pd(_t0_1210, _t0_1214);

  // AVX Storer:
  _t0_155 = _t0_280;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_281 = _t0_155;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_282 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_283 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_284 = _mm256_add_pd(_t0_282, _t0_283);

  // 4-BLAC: 1x4 / 1x4
  _t0_285 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_281), _mm256_castpd256_pd128(_t0_284)));

  // AVX Storer:
  _t0_155 = _t0_285;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 8), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_286 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_260, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_260, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_287 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_288 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_289 = _t0_288;

  // 4-BLAC: 1x4 * 4x1
  _t0_290 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_287, _t0_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_287, _t0_289), _mm256_mul_pd(_t0_287, _t0_289), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_287, _t0_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_287, _t0_289), _mm256_mul_pd(_t0_287, _t0_289), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_287, _t0_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_287, _t0_289), _mm256_mul_pd(_t0_287, _t0_289), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_291 = _mm256_sub_pd(_t0_286, _t0_290);

  // AVX Storer:
  _t0_156 = _t0_291;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_292 = _t0_156;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_293 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_294 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_295 = _mm256_add_pd(_t0_293, _t0_294);

  // 4-BLAC: 1x4 / 1x4
  _t0_296 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_292), _mm256_castpd256_pd128(_t0_295)));

  // AVX Storer:
  _t0_156 = _t0_296;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 8), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_297 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_260, _t0_260, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_298 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_299 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_300 = _t0_299;

  // 4-BLAC: 1x4 * 4x1
  _t0_301 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_298, _t0_300), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_298, _t0_300), _mm256_mul_pd(_t0_298, _t0_300), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_298, _t0_300), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_298, _t0_300), _mm256_mul_pd(_t0_298, _t0_300), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_298, _t0_300), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_298, _t0_300), _mm256_mul_pd(_t0_298, _t0_300), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_302 = _mm256_sub_pd(_t0_297, _t0_301);

  // AVX Storer:
  _t0_157 = _t0_302;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_303 = _t0_157;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_304 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_305 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_306 = _mm256_add_pd(_t0_304, _t0_305);

  // 4-BLAC: 1x4 / 1x4
  _t0_307 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_303), _mm256_castpd256_pd128(_t0_306)));

  // AVX Storer:
  _t0_157 = _t0_307;

  // Generating : X[52,52] = S(h(3, 52, 9), ( G(h(3, 52, 9), X[52,52],h(4, 52, 0)) - ( G(h(3, 52, 9), L[52,52],h(1, 52, 8)) * G(h(1, 52, 8), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t0_308 = _t0_261;
  _t0_309 = _t0_262;
  _t0_310 = _t0_263;
  _t0_311 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_312 = _t0_32;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t0_313 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_312, _t0_312, 32), _mm256_permute2f128_pd(_t0_312, _t0_312, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32));
  _t0_314 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_312, _t0_312, 32), _mm256_permute2f128_pd(_t0_312, _t0_312, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32));
  _t0_315 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_312, _t0_312, 49), _mm256_permute2f128_pd(_t0_312, _t0_312, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32));
  _t0_316 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_312, _t0_312, 49), _mm256_permute2f128_pd(_t0_312, _t0_312, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32));

  // 4-BLAC: 4x4 - 4x4
  _t0_317 = _mm256_sub_pd(_t0_308, _t0_313);
  _t0_318 = _mm256_sub_pd(_t0_309, _t0_314);
  _t0_319 = _mm256_sub_pd(_t0_310, _t0_315);
  _t0_320 = _mm256_sub_pd(_t0_311, _t0_316);

  // AVX Storer:
  _t0_261 = _t0_317;
  _t0_262 = _t0_318;
  _t0_263 = _t0_319;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_321 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_261, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_322 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_323 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_324 = _mm256_add_pd(_t0_322, _t0_323);

  // 4-BLAC: 1x4 / 1x4
  _t0_325 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_321), _mm256_castpd256_pd128(_t0_324)));

  // AVX Storer:
  _t0_158 = _t0_325;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 9), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_326 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_261, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_327 = _t0_158;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_328 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_329 = _t0_328;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_330 = _mm256_mul_pd(_t0_327, _t0_329);

  // 4-BLAC: 1x4 - 1x4
  _t0_331 = _mm256_sub_pd(_t0_326, _t0_330);

  // AVX Storer:
  _t0_159 = _t0_331;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_332 = _t0_159;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_333 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_334 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_335 = _mm256_add_pd(_t0_333, _t0_334);

  // 4-BLAC: 1x4 / 1x4
  _t0_336 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_332), _mm256_castpd256_pd128(_t0_335)));

  // AVX Storer:
  _t0_159 = _t0_336;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 9), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_337 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_261, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_261, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_338 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_339 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_340 = _t0_339;

  // 4-BLAC: 1x4 * 4x1
  _t0_341 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_338, _t0_340), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_338, _t0_340), _mm256_mul_pd(_t0_338, _t0_340), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_338, _t0_340), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_338, _t0_340), _mm256_mul_pd(_t0_338, _t0_340), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_338, _t0_340), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_338, _t0_340), _mm256_mul_pd(_t0_338, _t0_340), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_342 = _mm256_sub_pd(_t0_337, _t0_341);

  // AVX Storer:
  _t0_160 = _t0_342;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_343 = _t0_160;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_344 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_345 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_346 = _mm256_add_pd(_t0_344, _t0_345);

  // 4-BLAC: 1x4 / 1x4
  _t0_347 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_343), _mm256_castpd256_pd128(_t0_346)));

  // AVX Storer:
  _t0_160 = _t0_347;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 9), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_348 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_349 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_350 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_351 = _t0_350;

  // 4-BLAC: 1x4 * 4x1
  _t0_352 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_349, _t0_351), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_349, _t0_351), _mm256_mul_pd(_t0_349, _t0_351), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_349, _t0_351), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_349, _t0_351), _mm256_mul_pd(_t0_349, _t0_351), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_349, _t0_351), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_349, _t0_351), _mm256_mul_pd(_t0_349, _t0_351), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_353 = _mm256_sub_pd(_t0_348, _t0_352);

  // AVX Storer:
  _t0_161 = _t0_353;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_354 = _t0_161;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_355 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_356 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_357 = _mm256_add_pd(_t0_355, _t0_356);

  // 4-BLAC: 1x4 / 1x4
  _t0_358 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_354), _mm256_castpd256_pd128(_t0_357)));

  // AVX Storer:
  _t0_161 = _t0_358;

  // Generating : X[52,52] = S(h(2, 52, 10), ( G(h(2, 52, 10), X[52,52],h(4, 52, 0)) - ( G(h(2, 52, 10), L[52,52],h(1, 52, 9)) * G(h(1, 52, 9), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t0_359 = _t0_262;
  _t0_360 = _t0_263;
  _t0_361 = _mm256_setzero_pd();
  _t0_362 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_363 = _t0_30;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t0_364 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_363, _t0_363, 32), _mm256_permute2f128_pd(_t0_363, _t0_363, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32));
  _t0_365 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_363, _t0_363, 32), _mm256_permute2f128_pd(_t0_363, _t0_363, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32));
  _t0_366 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_363, _t0_363, 49), _mm256_permute2f128_pd(_t0_363, _t0_363, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32));
  _t0_367 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_363, _t0_363, 49), _mm256_permute2f128_pd(_t0_363, _t0_363, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32));

  // 4-BLAC: 4x4 - 4x4
  _t0_369 = _mm256_sub_pd(_t0_359, _t0_364);
  _t0_370 = _mm256_sub_pd(_t0_360, _t0_365);
  _t0_371 = _mm256_sub_pd(_t0_361, _t0_366);
  _t0_372 = _mm256_sub_pd(_t0_362, _t0_367);

  // AVX Storer:
  _t0_262 = _t0_369;
  _t0_263 = _t0_370;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_373 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_262, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_374 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_375 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_376 = _mm256_add_pd(_t0_374, _t0_375);

  // 4-BLAC: 1x4 / 1x4
  _t0_378 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_373), _mm256_castpd256_pd128(_t0_376)));

  // AVX Storer:
  _t0_162 = _t0_378;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 10), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_379 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_262, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_380 = _t0_162;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_381 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_382 = _t0_381;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_384 = _mm256_mul_pd(_t0_380, _t0_382);

  // 4-BLAC: 1x4 - 1x4
  _t0_385 = _mm256_sub_pd(_t0_379, _t0_384);

  // AVX Storer:
  _t0_163 = _t0_385;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_386 = _t0_163;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_387 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_388 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_389 = _mm256_add_pd(_t0_387, _t0_388);

  // 4-BLAC: 1x4 / 1x4
  _t0_390 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_386), _mm256_castpd256_pd128(_t0_389)));

  // AVX Storer:
  _t0_163 = _t0_390;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 10), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_391 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_262, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_262, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_392 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_393 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_394 = _t0_393;

  // 4-BLAC: 1x4 * 4x1
  _t0_395 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_392, _t0_394), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_392, _t0_394), _mm256_mul_pd(_t0_392, _t0_394), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_392, _t0_394), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_392, _t0_394), _mm256_mul_pd(_t0_392, _t0_394), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_392, _t0_394), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_392, _t0_394), _mm256_mul_pd(_t0_392, _t0_394), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_396 = _mm256_sub_pd(_t0_391, _t0_395);

  // AVX Storer:
  _t0_164 = _t0_396;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_398 = _t0_164;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_399 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_400 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_401 = _mm256_add_pd(_t0_399, _t0_400);

  // 4-BLAC: 1x4 / 1x4
  _t0_402 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_398), _mm256_castpd256_pd128(_t0_401)));

  // AVX Storer:
  _t0_164 = _t0_402;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 10), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_403 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_262, _t0_262, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_404 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_unpacklo_pd(_t0_164, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_405 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_406 = _t0_405;

  // 4-BLAC: 1x4 * 4x1
  _t0_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_404, _t0_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_404, _t0_406), _mm256_mul_pd(_t0_404, _t0_406), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_404, _t0_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_404, _t0_406), _mm256_mul_pd(_t0_404, _t0_406), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_404, _t0_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_404, _t0_406), _mm256_mul_pd(_t0_404, _t0_406), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_408 = _mm256_sub_pd(_t0_403, _t0_407);

  // AVX Storer:
  _t0_165 = _t0_408;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_409 = _t0_165;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_410 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_411 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_413 = _mm256_add_pd(_t0_410, _t0_411);

  // 4-BLAC: 1x4 / 1x4
  _t0_414 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_409), _mm256_castpd256_pd128(_t0_413)));

  // AVX Storer:
  _t0_165 = _t0_414;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 11), L[52,52],h(1, 52, 10)) Kro G(h(1, 52, 10), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_415 = _t0_28;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t0_201 = _mm256_mul_pd(_t0_415, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_unpacklo_pd(_t0_164, _t0_165), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_263 = _mm256_sub_pd(_t0_263, _t0_201);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_416 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_263, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_417 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_418 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t0_419 = _mm256_add_pd(_t0_417, _t0_418);

  // 4-BLAC: 1x4 / 1x4
  _t0_420 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_416), _mm256_castpd256_pd128(_t0_419)));

  // AVX Storer:
  _t0_166 = _t0_420;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 11), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_421 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_263, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_422 = _t0_166;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_423 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t0_424 = _t0_423;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_425 = _mm256_mul_pd(_t0_422, _t0_424);

  // 4-BLAC: 1x4 - 1x4
  _t0_426 = _mm256_sub_pd(_t0_421, _t0_425);

  // AVX Storer:
  _t0_167 = _t0_426;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_428 = _t0_167;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_429 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_430 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t0_431 = _mm256_add_pd(_t0_429, _t0_430);

  // 4-BLAC: 1x4 / 1x4
  _t0_433 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_428), _mm256_castpd256_pd128(_t0_431)));

  // AVX Storer:
  _t0_167 = _t0_433;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 11), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_434 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_263, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_263, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_435 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_166, _t0_167), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_436 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t0_437 = _t0_436;

  // 4-BLAC: 1x4 * 4x1
  _t0_439 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_435, _t0_437), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_435, _t0_437), _mm256_mul_pd(_t0_435, _t0_437), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_435, _t0_437), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_435, _t0_437), _mm256_mul_pd(_t0_435, _t0_437), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_435, _t0_437), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_435, _t0_437), _mm256_mul_pd(_t0_435, _t0_437), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_440 = _mm256_sub_pd(_t0_434, _t0_439);

  // AVX Storer:
  _t0_168 = _t0_440;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_441 = _t0_168;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_442 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_443 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t0_444 = _mm256_add_pd(_t0_442, _t0_443);

  // 4-BLAC: 1x4 / 1x4
  _t0_445 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_441), _mm256_castpd256_pd128(_t0_444)));

  // AVX Storer:
  _t0_168 = _t0_445;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 11), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_446 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_263, _t0_263, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_447 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_166, _t0_167), _mm256_unpacklo_pd(_t0_168, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_448 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t0_449 = _t0_448;

  // 4-BLAC: 1x4 * 4x1
  _t0_450 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_447, _t0_449), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_447, _t0_449), _mm256_mul_pd(_t0_447, _t0_449), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_447, _t0_449), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_447, _t0_449), _mm256_mul_pd(_t0_447, _t0_449), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_447, _t0_449), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_447, _t0_449), _mm256_mul_pd(_t0_447, _t0_449), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_451 = _mm256_sub_pd(_t0_446, _t0_450);

  // AVX Storer:
  _t0_169 = _t0_451;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_453 = _t0_169;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_454 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_455 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t0_456 = _mm256_add_pd(_t0_454, _t0_455);

  // 4-BLAC: 1x4 / 1x4
  _t0_457 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_453), _mm256_castpd256_pd128(_t0_456)));

  // AVX Storer:
  _t0_169 = _t0_457;

  // Generating : X[52,52] = S(h(4, 52, 8), ( G(h(4, 52, 8), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, 8), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) ) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1227 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 32);
  _t0_1228 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 32);
  _t0_1229 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 49);
  _t0_1230 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_228 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_154, _t0_154, 32), _mm256_permute2f128_pd(_t0_154, _t0_154, 32), 0), _t0_1227), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_155, _t0_155, 32), _mm256_permute2f128_pd(_t0_155, _t0_155, 32), 0), _t0_1228)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_156, _t0_156, 32), _mm256_permute2f128_pd(_t0_156, _t0_156, 32), 0), _t0_1229), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_157, _t0_157, 32), _mm256_permute2f128_pd(_t0_157, _t0_157, 32), 0), _t0_1230)));
  _t0_229 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_158, _t0_158, 32), _mm256_permute2f128_pd(_t0_158, _t0_158, 32), 0), _t0_1227), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_159, _t0_159, 32), _mm256_permute2f128_pd(_t0_159, _t0_159, 32), 0), _t0_1228)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_160, _t0_160, 32), _mm256_permute2f128_pd(_t0_160, _t0_160, 32), 0), _t0_1229), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_161, _t0_161, 32), _mm256_permute2f128_pd(_t0_161, _t0_161, 32), 0), _t0_1230)));
  _t0_230 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_162, _t0_162, 32), _mm256_permute2f128_pd(_t0_162, _t0_162, 32), 0), _t0_1227), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_163, _t0_163, 32), _mm256_permute2f128_pd(_t0_163, _t0_163, 32), 0), _t0_1228)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_164, _t0_164, 32), _mm256_permute2f128_pd(_t0_164, _t0_164, 32), 0), _t0_1229), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_165, _t0_165, 32), _mm256_permute2f128_pd(_t0_165, _t0_165, 32), 0), _t0_1230)));
  _t0_231 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_166, _t0_166, 32), _mm256_permute2f128_pd(_t0_166, _t0_166, 32), 0), _t0_1227), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_167, _t0_167, 32), _mm256_permute2f128_pd(_t0_167, _t0_167, 32), 0), _t0_1228)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_168, _t0_168, 32), _mm256_permute2f128_pd(_t0_168, _t0_168, 32), 0), _t0_1229), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_169, _t0_169, 32), _mm256_permute2f128_pd(_t0_169, _t0_169, 32), 0), _t0_1230)));

  // 4-BLAC: 4x4 - 4x4
  _t0_264 = _mm256_sub_pd(_t0_264, _t0_228);
  _t0_265 = _mm256_sub_pd(_t0_265, _t0_229);
  _t0_266 = _mm256_sub_pd(_t0_266, _t0_230);
  _t0_267 = _mm256_sub_pd(_t0_267, _t0_231);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_458 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_264, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_459 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_460 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_461 = _mm256_add_pd(_t0_459, _t0_460);

  // 4-BLAC: 1x4 / 1x4
  _t0_462 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_458), _mm256_castpd256_pd128(_t0_461)));

  // AVX Storer:
  _t0_170 = _t0_462;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 8), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_463 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_264, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_464 = _t0_170;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_465 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_466 = _t0_465;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_467 = _mm256_mul_pd(_t0_464, _t0_466);

  // 4-BLAC: 1x4 - 1x4
  _t0_469 = _mm256_sub_pd(_t0_463, _t0_467);

  // AVX Storer:
  _t0_171 = _t0_469;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_470 = _t0_171;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_471 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_472 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t0_473 = _mm256_add_pd(_t0_471, _t0_472);

  // 4-BLAC: 1x4 / 1x4
  _t0_474 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_470), _mm256_castpd256_pd128(_t0_473)));

  // AVX Storer:
  _t0_171 = _t0_474;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 8), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_475 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_264, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_264, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_476 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_477 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t0_478 = _t0_477;

  // 4-BLAC: 1x4 * 4x1
  _t0_479 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_476, _t0_478), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_476, _t0_478), _mm256_mul_pd(_t0_476, _t0_478), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_476, _t0_478), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_476, _t0_478), _mm256_mul_pd(_t0_476, _t0_478), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_476, _t0_478), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_476, _t0_478), _mm256_mul_pd(_t0_476, _t0_478), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_480 = _mm256_sub_pd(_t0_475, _t0_479);

  // AVX Storer:
  _t0_172 = _t0_480;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_481 = _t0_172;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_483 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_484 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t0_485 = _mm256_add_pd(_t0_483, _t0_484);

  // 4-BLAC: 1x4 / 1x4
  _t0_486 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_481), _mm256_castpd256_pd128(_t0_485)));

  // AVX Storer:
  _t0_172 = _t0_486;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 8), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_488 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_264, _t0_264, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_489 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_490 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t0_491 = _t0_490;

  // 4-BLAC: 1x4 * 4x1
  _t0_492 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_489, _t0_491), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_489, _t0_491), _mm256_mul_pd(_t0_489, _t0_491), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_489, _t0_491), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_489, _t0_491), _mm256_mul_pd(_t0_489, _t0_491), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_489, _t0_491), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_489, _t0_491), _mm256_mul_pd(_t0_489, _t0_491), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_494 = _mm256_sub_pd(_t0_488, _t0_492);

  // AVX Storer:
  _t0_173 = _t0_494;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 8), L[52,52],h(1, 52, 8)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_495 = _t0_173;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_496 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_497 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t0_498 = _mm256_add_pd(_t0_496, _t0_497);

  // 4-BLAC: 1x4 / 1x4
  _t0_499 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_495), _mm256_castpd256_pd128(_t0_498)));

  // AVX Storer:
  _t0_173 = _t0_499;

  // Generating : X[52,52] = S(h(3, 52, 9), ( G(h(3, 52, 9), X[52,52],h(4, 52, 4)) - ( G(h(3, 52, 9), L[52,52],h(1, 52, 8)) * G(h(1, 52, 8), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t0_500 = _t0_265;
  _t0_501 = _t0_266;
  _t0_502 = _t0_267;
  _t0_503 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_504 = _t0_32;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t0_505 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_504, _t0_504, 32), _mm256_permute2f128_pd(_t0_504, _t0_504, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32));
  _t0_506 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_504, _t0_504, 32), _mm256_permute2f128_pd(_t0_504, _t0_504, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32));
  _t0_507 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_504, _t0_504, 49), _mm256_permute2f128_pd(_t0_504, _t0_504, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32));
  _t0_508 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_504, _t0_504, 49), _mm256_permute2f128_pd(_t0_504, _t0_504, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32));

  // 4-BLAC: 4x4 - 4x4
  _t0_509 = _mm256_sub_pd(_t0_500, _t0_505);
  _t0_510 = _mm256_sub_pd(_t0_501, _t0_506);
  _t0_511 = _mm256_sub_pd(_t0_502, _t0_507);
  _t0_512 = _mm256_sub_pd(_t0_503, _t0_508);

  // AVX Storer:
  _t0_265 = _t0_509;
  _t0_266 = _t0_510;
  _t0_267 = _t0_511;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_513 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_265, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_514 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_515 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_516 = _mm256_add_pd(_t0_514, _t0_515);

  // 4-BLAC: 1x4 / 1x4
  _t0_518 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_513), _mm256_castpd256_pd128(_t0_516)));

  // AVX Storer:
  _t0_174 = _t0_518;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 9), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_519 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_265, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_520 = _t0_174;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_521 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_522 = _t0_521;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_523 = _mm256_mul_pd(_t0_520, _t0_522);

  // 4-BLAC: 1x4 - 1x4
  _t0_524 = _mm256_sub_pd(_t0_519, _t0_523);

  // AVX Storer:
  _t0_175 = _t0_524;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_525 = _t0_175;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_526 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_527 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t0_528 = _mm256_add_pd(_t0_526, _t0_527);

  // 4-BLAC: 1x4 / 1x4
  _t0_529 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_525), _mm256_castpd256_pd128(_t0_528)));

  // AVX Storer:
  _t0_175 = _t0_529;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 9), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_530 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_265, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_265, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_531 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_533 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t0_534 = _t0_533;

  // 4-BLAC: 1x4 * 4x1
  _t0_535 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_531, _t0_534), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_531, _t0_534), _mm256_mul_pd(_t0_531, _t0_534), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_531, _t0_534), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_531, _t0_534), _mm256_mul_pd(_t0_531, _t0_534), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_531, _t0_534), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_531, _t0_534), _mm256_mul_pd(_t0_531, _t0_534), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_536 = _mm256_sub_pd(_t0_530, _t0_535);

  // AVX Storer:
  _t0_176 = _t0_536;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_537 = _t0_176;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_538 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_539 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t0_540 = _mm256_add_pd(_t0_538, _t0_539);

  // 4-BLAC: 1x4 / 1x4
  _t0_541 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_537), _mm256_castpd256_pd128(_t0_540)));

  // AVX Storer:
  _t0_176 = _t0_541;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 9), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_542 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_265, _t0_265, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_543 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_544 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t0_545 = _t0_544;

  // 4-BLAC: 1x4 * 4x1
  _t0_546 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_543, _t0_545), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_543, _t0_545), _mm256_mul_pd(_t0_543, _t0_545), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_543, _t0_545), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_543, _t0_545), _mm256_mul_pd(_t0_543, _t0_545), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_543, _t0_545), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_543, _t0_545), _mm256_mul_pd(_t0_543, _t0_545), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_548 = _mm256_sub_pd(_t0_542, _t0_546);

  // AVX Storer:
  _t0_177 = _t0_548;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_549 = _t0_177;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_550 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_551 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t0_553 = _mm256_add_pd(_t0_550, _t0_551);

  // 4-BLAC: 1x4 / 1x4
  _t0_554 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_549), _mm256_castpd256_pd128(_t0_553)));

  // AVX Storer:
  _t0_177 = _t0_554;

  // Generating : X[52,52] = S(h(2, 52, 10), ( G(h(2, 52, 10), X[52,52],h(4, 52, 4)) - ( G(h(2, 52, 10), L[52,52],h(1, 52, 9)) * G(h(1, 52, 9), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t0_555 = _t0_266;
  _t0_556 = _t0_267;
  _t0_557 = _mm256_setzero_pd();
  _t0_558 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_559 = _t0_30;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t0_560 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 32), _mm256_permute2f128_pd(_t0_559, _t0_559, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32));
  _t0_561 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 32), _mm256_permute2f128_pd(_t0_559, _t0_559, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32));
  _t0_562 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 49), _mm256_permute2f128_pd(_t0_559, _t0_559, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32));
  _t0_563 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 49), _mm256_permute2f128_pd(_t0_559, _t0_559, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32));

  // 4-BLAC: 4x4 - 4x4
  _t0_565 = _mm256_sub_pd(_t0_555, _t0_560);
  _t0_566 = _mm256_sub_pd(_t0_556, _t0_561);
  _t0_567 = _mm256_sub_pd(_t0_557, _t0_562);
  _t0_568 = _mm256_sub_pd(_t0_558, _t0_563);

  // AVX Storer:
  _t0_266 = _t0_565;
  _t0_267 = _t0_566;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_569 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_266, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_570 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_571 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_572 = _mm256_add_pd(_t0_570, _t0_571);

  // 4-BLAC: 1x4 / 1x4
  _t0_573 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_569), _mm256_castpd256_pd128(_t0_572)));

  // AVX Storer:
  _t0_178 = _t0_573;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 10), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_574 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_266, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_575 = _t0_178;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_576 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_578 = _t0_576;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_579 = _mm256_mul_pd(_t0_575, _t0_578);

  // 4-BLAC: 1x4 - 1x4
  _t0_580 = _mm256_sub_pd(_t0_574, _t0_579);

  // AVX Storer:
  _t0_179 = _t0_580;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_581 = _t0_179;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_582 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_583 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t0_584 = _mm256_add_pd(_t0_582, _t0_583);

  // 4-BLAC: 1x4 / 1x4
  _t0_585 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_581), _mm256_castpd256_pd128(_t0_584)));

  // AVX Storer:
  _t0_179 = _t0_585;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 10), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_586 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_266, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_266, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_587 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_588 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t0_589 = _t0_588;

  // 4-BLAC: 1x4 * 4x1
  _t0_590 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_587, _t0_589), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_587, _t0_589), _mm256_mul_pd(_t0_587, _t0_589), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_587, _t0_589), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_587, _t0_589), _mm256_mul_pd(_t0_587, _t0_589), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_587, _t0_589), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_587, _t0_589), _mm256_mul_pd(_t0_587, _t0_589), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_592 = _mm256_sub_pd(_t0_586, _t0_590);

  // AVX Storer:
  _t0_180 = _t0_592;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_593 = _t0_180;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_594 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_595 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t0_596 = _mm256_add_pd(_t0_594, _t0_595);

  // 4-BLAC: 1x4 / 1x4
  _t0_598 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_593), _mm256_castpd256_pd128(_t0_596)));

  // AVX Storer:
  _t0_180 = _t0_598;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 10), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_599 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_266, _t0_266, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_600 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_601 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t0_602 = _t0_601;

  // 4-BLAC: 1x4 * 4x1
  _t0_604 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_600, _t0_602), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_600, _t0_602), _mm256_mul_pd(_t0_600, _t0_602), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_600, _t0_602), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_600, _t0_602), _mm256_mul_pd(_t0_600, _t0_602), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_600, _t0_602), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_600, _t0_602), _mm256_mul_pd(_t0_600, _t0_602), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_605 = _mm256_sub_pd(_t0_599, _t0_604);

  // AVX Storer:
  _t0_181 = _t0_605;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_606 = _t0_181;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_607 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_608 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t0_609 = _mm256_add_pd(_t0_607, _t0_608);

  // 4-BLAC: 1x4 / 1x4
  _t0_610 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_606), _mm256_castpd256_pd128(_t0_609)));

  // AVX Storer:
  _t0_181 = _t0_610;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 11), L[52,52],h(1, 52, 10)) Kro G(h(1, 52, 10), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_611 = _t0_28;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t0_202 = _mm256_mul_pd(_t0_611, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_267 = _mm256_sub_pd(_t0_267, _t0_202);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_613 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_267, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_614 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_615 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t0_616 = _mm256_add_pd(_t0_614, _t0_615);

  // 4-BLAC: 1x4 / 1x4
  _t0_617 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_613), _mm256_castpd256_pd128(_t0_616)));

  // AVX Storer:
  _t0_182 = _t0_617;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 11), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_618 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_267, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_619 = _t0_182;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_620 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t0_621 = _t0_620;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_622 = _mm256_mul_pd(_t0_619, _t0_621);

  // 4-BLAC: 1x4 - 1x4
  _t0_623 = _mm256_sub_pd(_t0_618, _t0_622);

  // AVX Storer:
  _t0_183 = _t0_623;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_624 = _t0_183;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_625 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_626 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t0_627 = _mm256_add_pd(_t0_625, _t0_626);

  // 4-BLAC: 1x4 / 1x4
  _t0_629 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_624), _mm256_castpd256_pd128(_t0_627)));

  // AVX Storer:
  _t0_183 = _t0_629;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 11), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_630 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_267, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_267, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_631 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_632 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t0_633 = _t0_632;

  // 4-BLAC: 1x4 * 4x1
  _t0_634 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_631, _t0_633), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_631, _t0_633), _mm256_mul_pd(_t0_631, _t0_633), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_631, _t0_633), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_631, _t0_633), _mm256_mul_pd(_t0_631, _t0_633), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_631, _t0_633), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_631, _t0_633), _mm256_mul_pd(_t0_631, _t0_633), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_635 = _mm256_sub_pd(_t0_630, _t0_634);

  // AVX Storer:
  _t0_184 = _t0_635;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_636 = _t0_184;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_637 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_638 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t0_639 = _mm256_add_pd(_t0_637, _t0_638);

  // 4-BLAC: 1x4 / 1x4
  _t0_640 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_636), _mm256_castpd256_pd128(_t0_639)));

  // AVX Storer:
  _t0_184 = _t0_640;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 11), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_641 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_267, _t0_267, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_643 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_644 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t0_645 = _t0_644;

  // 4-BLAC: 1x4 * 4x1
  _t0_646 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_643, _t0_645), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_643, _t0_645), _mm256_mul_pd(_t0_643, _t0_645), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_643, _t0_645), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_643, _t0_645), _mm256_mul_pd(_t0_643, _t0_645), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_643, _t0_645), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_643, _t0_645), _mm256_mul_pd(_t0_643, _t0_645), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_647 = _mm256_sub_pd(_t0_641, _t0_646);

  // AVX Storer:
  _t0_185 = _t0_647;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_648 = _t0_185;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_649 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_650 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t0_651 = _mm256_add_pd(_t0_649, _t0_650);

  // 4-BLAC: 1x4 / 1x4
  _t0_652 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_648), _mm256_castpd256_pd128(_t0_651)));

  // AVX Storer:
  _t0_185 = _t0_652;

  // Generating : X[52,52] = ( ( S(h(4, 52, 8), ( G(h(4, 52, 8), C[52,52],h(4, 52, 8)) - ( ( G(h(4, 52, 8), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, 8), X[52,52],h(4, 52, 0)) ) ) + ( G(h(4, 52, 8), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, 8), L[52,52],h(4, 52, 0)) ) ) ) ),h(4, 52, 8)) + -$(h(4, 52, 8), ( G(h(4, 52, 8), X[52,52],h(4, 52, 4)) * T( G(h(4, 52, 8), L[52,52],h(4, 52, 4)) ) ),h(4, 52, 8)) ) + -$(h(4, 52, 8), ( G(h(4, 52, 8), L[52,52],h(4, 52, 4)) * T( G(h(4, 52, 8), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 8)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_653 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_186, _t0_187, 0), _mm256_shuffle_pd(_t0_188, _t0_189, 0), 32);
  _t0_654 = _mm256_permute2f128_pd(_t0_187, _mm256_shuffle_pd(_t0_188, _t0_189, 3), 32);
  _t0_655 = _mm256_blend_pd(_t0_188, _mm256_shuffle_pd(_t0_188, _t0_189, 3), 12);
  _t0_656 = _t0_189;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1231 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_unpacklo_pd(_t0_164, _t0_165), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_166, _t0_167), _mm256_unpacklo_pd(_t0_168, _t0_169), 32)), 32);
  _t0_1232 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_unpacklo_pd(_t0_164, _t0_165), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_166, _t0_167), _mm256_unpacklo_pd(_t0_168, _t0_169), 32)), 32);
  _t0_1233 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_unpacklo_pd(_t0_164, _t0_165), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_166, _t0_167), _mm256_unpacklo_pd(_t0_168, _t0_169), 32)), 49);
  _t0_1234 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_154, _t0_155), _mm256_unpacklo_pd(_t0_156, _t0_157), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_158, _t0_159), _mm256_unpacklo_pd(_t0_160, _t0_161), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_162, _t0_163), _mm256_unpacklo_pd(_t0_164, _t0_165), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_166, _t0_167), _mm256_unpacklo_pd(_t0_168, _t0_169), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_232 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_65, _t0_1231), _mm256_mul_pd(_t0_64, _t0_1232)), _mm256_add_pd(_mm256_mul_pd(_t0_63, _t0_1233), _mm256_mul_pd(_t0_62, _t0_1234)));
  _t0_233 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_61, _t0_1231), _mm256_mul_pd(_t0_60, _t0_1232)), _mm256_add_pd(_mm256_mul_pd(_t0_59, _t0_1233), _mm256_mul_pd(_t0_58, _t0_1234)));
  _t0_234 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_57, _t0_1231), _mm256_mul_pd(_t0_56, _t0_1232)), _mm256_add_pd(_mm256_mul_pd(_t0_55, _t0_1233), _mm256_mul_pd(_t0_54, _t0_1234)));
  _t0_235 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_53, _t0_1231), _mm256_mul_pd(_t0_52, _t0_1232)), _mm256_add_pd(_mm256_mul_pd(_t0_51, _t0_1233), _mm256_mul_pd(_t0_50, _t0_1234)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1235 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_26, _t0_25), _mm256_unpacklo_pd(_t0_24, _t0_23), 32);
  _t0_1236 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_26, _t0_25), _mm256_unpackhi_pd(_t0_24, _t0_23), 32);
  _t0_1237 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_26, _t0_25), _mm256_unpacklo_pd(_t0_24, _t0_23), 49);
  _t0_1238 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_26, _t0_25), _mm256_unpackhi_pd(_t0_24, _t0_23), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_236 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_154, _t0_154, 32), _mm256_permute2f128_pd(_t0_154, _t0_154, 32), 0), _t0_1235), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_155, _t0_155, 32), _mm256_permute2f128_pd(_t0_155, _t0_155, 32), 0), _t0_1236)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_156, _t0_156, 32), _mm256_permute2f128_pd(_t0_156, _t0_156, 32), 0), _t0_1237), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_157, _t0_157, 32), _mm256_permute2f128_pd(_t0_157, _t0_157, 32), 0), _t0_1238)));
  _t0_237 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_158, _t0_158, 32), _mm256_permute2f128_pd(_t0_158, _t0_158, 32), 0), _t0_1235), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_159, _t0_159, 32), _mm256_permute2f128_pd(_t0_159, _t0_159, 32), 0), _t0_1236)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_160, _t0_160, 32), _mm256_permute2f128_pd(_t0_160, _t0_160, 32), 0), _t0_1237), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_161, _t0_161, 32), _mm256_permute2f128_pd(_t0_161, _t0_161, 32), 0), _t0_1238)));
  _t0_238 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_162, _t0_162, 32), _mm256_permute2f128_pd(_t0_162, _t0_162, 32), 0), _t0_1235), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_163, _t0_163, 32), _mm256_permute2f128_pd(_t0_163, _t0_163, 32), 0), _t0_1236)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_164, _t0_164, 32), _mm256_permute2f128_pd(_t0_164, _t0_164, 32), 0), _t0_1237), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_165, _t0_165, 32), _mm256_permute2f128_pd(_t0_165, _t0_165, 32), 0), _t0_1238)));
  _t0_239 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_166, _t0_166, 32), _mm256_permute2f128_pd(_t0_166, _t0_166, 32), 0), _t0_1235), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_167, _t0_167, 32), _mm256_permute2f128_pd(_t0_167, _t0_167, 32), 0), _t0_1236)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_168, _t0_168, 32), _mm256_permute2f128_pd(_t0_168, _t0_168, 32), 0), _t0_1237), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_169, _t0_169, 32), _mm256_permute2f128_pd(_t0_169, _t0_169, 32), 0), _t0_1238)));

  // 4-BLAC: 4x4 + 4x4
  _t0_107 = _mm256_add_pd(_t0_232, _t0_236);
  _t0_108 = _mm256_add_pd(_t0_233, _t0_237);
  _t0_109 = _mm256_add_pd(_t0_234, _t0_238);
  _t0_110 = _mm256_add_pd(_t0_235, _t0_239);

  // 4-BLAC: 4x4 - 4x4
  _t0_268 = _mm256_sub_pd(_t0_653, _t0_107);
  _t0_269 = _mm256_sub_pd(_t0_654, _t0_108);
  _t0_270 = _mm256_sub_pd(_t0_655, _t0_109);
  _t0_271 = _mm256_sub_pd(_t0_656, _t0_110);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t0_186 = _t0_268;
  _t0_187 = _t0_269;
  _t0_188 = _t0_270;
  _t0_189 = _t0_271;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1239 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_22, _t0_21), _mm256_unpacklo_pd(_t0_20, _t0_19), 32);
  _t0_1240 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_22, _t0_21), _mm256_unpackhi_pd(_t0_20, _t0_19), 32);
  _t0_1241 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_22, _t0_21), _mm256_unpacklo_pd(_t0_20, _t0_19), 49);
  _t0_1242 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_22, _t0_21), _mm256_unpackhi_pd(_t0_20, _t0_19), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_240 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_170, _t0_170, 32), _mm256_permute2f128_pd(_t0_170, _t0_170, 32), 0), _t0_1239), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_171, _t0_171, 32), _mm256_permute2f128_pd(_t0_171, _t0_171, 32), 0), _t0_1240)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_172, _t0_172, 32), _mm256_permute2f128_pd(_t0_172, _t0_172, 32), 0), _t0_1241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_173, _t0_173, 32), _mm256_permute2f128_pd(_t0_173, _t0_173, 32), 0), _t0_1242)));
  _t0_241 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_174, _t0_174, 32), _mm256_permute2f128_pd(_t0_174, _t0_174, 32), 0), _t0_1239), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_175, _t0_175, 32), _mm256_permute2f128_pd(_t0_175, _t0_175, 32), 0), _t0_1240)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_176, _t0_176, 32), _mm256_permute2f128_pd(_t0_176, _t0_176, 32), 0), _t0_1241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_177, _t0_177, 32), _mm256_permute2f128_pd(_t0_177, _t0_177, 32), 0), _t0_1242)));
  _t0_242 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_178, _t0_178, 32), _mm256_permute2f128_pd(_t0_178, _t0_178, 32), 0), _t0_1239), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_179, _t0_179, 32), _mm256_permute2f128_pd(_t0_179, _t0_179, 32), 0), _t0_1240)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_180, _t0_180, 32), _mm256_permute2f128_pd(_t0_180, _t0_180, 32), 0), _t0_1241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_181, _t0_181, 32), _mm256_permute2f128_pd(_t0_181, _t0_181, 32), 0), _t0_1242)));
  _t0_243 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_182, _t0_182, 32), _mm256_permute2f128_pd(_t0_182, _t0_182, 32), 0), _t0_1239), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_183, _t0_183, 32), _mm256_permute2f128_pd(_t0_183, _t0_183, 32), 0), _t0_1240)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_184, _t0_184, 32), _mm256_permute2f128_pd(_t0_184, _t0_184, 32), 0), _t0_1241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_185, _t0_185, 32), _mm256_permute2f128_pd(_t0_185, _t0_185, 32), 0), _t0_1242)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_657 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_186, _t0_187, 0), _mm256_shuffle_pd(_t0_188, _t0_189, 0), 32);
  _t0_658 = _mm256_permute2f128_pd(_t0_187, _mm256_shuffle_pd(_t0_188, _t0_189, 3), 32);
  _t0_659 = _mm256_blend_pd(_t0_188, _mm256_shuffle_pd(_t0_188, _t0_189, 3), 12);
  _t0_660 = _t0_189;

  // 4-BLAC: 4x4 - 4x4
  _t0_657 = _mm256_sub_pd(_t0_657, _t0_240);
  _t0_658 = _mm256_sub_pd(_t0_658, _t0_241);
  _t0_659 = _mm256_sub_pd(_t0_659, _t0_242);
  _t0_660 = _mm256_sub_pd(_t0_660, _t0_243);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t0_186 = _t0_657;
  _t0_187 = _t0_658;
  _t0_188 = _t0_659;
  _t0_189 = _t0_660;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_1243 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 32);
  _t0_1244 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 32);
  _t0_1245 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 49);
  _t0_1246 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_244 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_49, _t0_1243), _mm256_mul_pd(_t0_48, _t0_1244)), _mm256_add_pd(_mm256_mul_pd(_t0_47, _t0_1245), _mm256_mul_pd(_t0_46, _t0_1246)));
  _t0_245 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_45, _t0_1243), _mm256_mul_pd(_t0_44, _t0_1244)), _mm256_add_pd(_mm256_mul_pd(_t0_43, _t0_1245), _mm256_mul_pd(_t0_42, _t0_1246)));
  _t0_246 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_1243), _mm256_mul_pd(_t0_40, _t0_1244)), _mm256_add_pd(_mm256_mul_pd(_t0_39, _t0_1245), _mm256_mul_pd(_t0_38, _t0_1246)));
  _t0_247 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_37, _t0_1243), _mm256_mul_pd(_t0_36, _t0_1244)), _mm256_add_pd(_mm256_mul_pd(_t0_35, _t0_1245), _mm256_mul_pd(_t0_34, _t0_1246)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_661 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_186, _t0_187, 0), _mm256_shuffle_pd(_t0_188, _t0_189, 0), 32);
  _t0_662 = _mm256_permute2f128_pd(_t0_187, _mm256_shuffle_pd(_t0_188, _t0_189, 3), 32);
  _t0_663 = _mm256_blend_pd(_t0_188, _mm256_shuffle_pd(_t0_188, _t0_189, 3), 12);
  _t0_664 = _t0_189;

  // 4-BLAC: 4x4 - 4x4
  _t0_661 = _mm256_sub_pd(_t0_661, _t0_244);
  _t0_662 = _mm256_sub_pd(_t0_662, _t0_245);
  _t0_663 = _mm256_sub_pd(_t0_663, _t0_246);
  _t0_664 = _mm256_sub_pd(_t0_664, _t0_247);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t0_186 = _t0_661;
  _t0_187 = _t0_662;
  _t0_188 = _t0_663;
  _t0_189 = _t0_664;

  // Generating : X[52,52] = S(h(1, 52, 8), ( G(h(1, 52, 8), X[52,52],h(1, 52, 8)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 8), L[52,52],h(1, 52, 8)) ) ),h(1, 52, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_665 = _t0_186;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_666 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_668 = _t0_33;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_669 = _mm256_mul_pd(_t0_666, _t0_668);

  // 4-BLAC: 1x4 / 1x4
  _t0_670 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_665), _mm256_castpd256_pd128(_t0_669)));

  // AVX Storer:
  _t0_186 = _t0_670;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 8)) - ( G(h(1, 52, 9), L[52,52],h(1, 52, 8)) Kro G(h(1, 52, 8), X[52,52],h(1, 52, 8)) ) ),h(1, 52, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_671 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_187, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_672 = _t0_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_673 = _t0_186;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_674 = _mm256_mul_pd(_t0_672, _t0_673);

  // 4-BLAC: 1x4 - 1x4
  _t0_675 = _mm256_sub_pd(_t0_671, _t0_674);

  // AVX Storer:
  _t0_190 = _t0_675;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 8)) Div ( G(h(1, 52, 9), L[52,52],h(1, 52, 9)) + G(h(1, 52, 8), L[52,52],h(1, 52, 8)) ) ),h(1, 52, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_676 = _t0_190;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_677 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_678 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t0_679 = _mm256_add_pd(_t0_677, _t0_678);

  // 4-BLAC: 1x4 / 1x4
  _t0_680 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_676), _mm256_castpd256_pd128(_t0_679)));

  // AVX Storer:
  _t0_190 = _t0_680;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 9)) - ( ( G(h(1, 52, 9), L[52,52],h(1, 52, 8)) Kro T( G(h(1, 52, 9), X[52,52],h(1, 52, 8)) ) ) + ( G(h(1, 52, 9), X[52,52],h(1, 52, 8)) Kro T( G(h(1, 52, 9), L[52,52],h(1, 52, 8)) ) ) ) ),h(1, 52, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_682 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_187, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_683 = _t0_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_684 = _t0_190;

  // 4-BLAC: (4x1)^T
  _t0_685 = _t0_684;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_686 = _mm256_mul_pd(_t0_683, _t0_685);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_687 = _t0_190;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_689 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t0_690 = _t0_689;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_691 = _mm256_mul_pd(_t0_687, _t0_690);

  // 4-BLAC: 1x4 + 1x4
  _t0_692 = _mm256_add_pd(_t0_686, _t0_691);

  // 4-BLAC: 1x4 - 1x4
  _t0_694 = _mm256_sub_pd(_t0_682, _t0_692);

  // AVX Storer:
  _t0_191 = _t0_694;

  // Generating : X[52,52] = S(h(1, 52, 9), ( G(h(1, 52, 9), X[52,52],h(1, 52, 9)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 9), L[52,52],h(1, 52, 9)) ) ),h(1, 52, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_695 = _t0_191;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_696 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_697 = _t0_31;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_698 = _mm256_mul_pd(_t0_696, _t0_697);

  // 4-BLAC: 1x4 / 1x4
  _t0_699 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_695), _mm256_castpd256_pd128(_t0_698)));

  // AVX Storer:
  _t0_191 = _t0_699;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(2, 52, 8)) - ( G(h(1, 52, 10), L[52,52],h(2, 52, 8)) * G(h(2, 52, 8), X[52,52],h(2, 52, 8)) ) ),h(2, 52, 8))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_700 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_188, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_701 = _t0_17;

  // AVX Loader:

  // 2x2 -> 4x4 - LowSymm
  _t0_702 = _mm256_shuffle_pd(_t0_186, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_190, _t0_191), _mm256_setzero_pd(), 12), 0);
  _t0_703 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_190, _t0_191), _mm256_setzero_pd(), 12);
  _t0_704 = _mm256_setzero_pd();
  _t0_705 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_706 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_701, _t0_701, 32), _mm256_permute2f128_pd(_t0_701, _t0_701, 32), 0), _t0_702), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_701, _t0_701, 32), _mm256_permute2f128_pd(_t0_701, _t0_701, 32), 15), _t0_703)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_701, _t0_701, 49), _mm256_permute2f128_pd(_t0_701, _t0_701, 49), 0), _t0_704), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_701, _t0_701, 49), _mm256_permute2f128_pd(_t0_701, _t0_701, 49), 15), _t0_705)));

  // 4-BLAC: 1x4 - 1x4
  _t0_707 = _mm256_sub_pd(_t0_700, _t0_706);

  // AVX Storer:
  _t0_192 = _t0_707;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 8)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 8), L[52,52],h(1, 52, 8)) ) ),h(1, 52, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_708 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_192, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_709 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_710 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t0_712 = _mm256_add_pd(_t0_709, _t0_710);

  // 4-BLAC: 1x4 / 1x4
  _t0_713 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_708), _mm256_castpd256_pd128(_t0_712)));

  // AVX Storer:
  _t0_193 = _t0_713;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 9)) - ( G(h(1, 52, 10), X[52,52],h(1, 52, 8)) Kro T( G(h(1, 52, 9), L[52,52],h(1, 52, 8)) ) ) ),h(1, 52, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_714 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_192, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_715 = _t0_193;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_716 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t0_717 = _t0_716;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_718 = _mm256_mul_pd(_t0_715, _t0_717);

  // 4-BLAC: 1x4 - 1x4
  _t0_719 = _mm256_sub_pd(_t0_714, _t0_718);

  // AVX Storer:
  _t0_194 = _t0_719;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 9)) Div ( G(h(1, 52, 10), L[52,52],h(1, 52, 10)) + G(h(1, 52, 9), L[52,52],h(1, 52, 9)) ) ),h(1, 52, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_720 = _t0_194;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_721 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_722 = _t0_31;

  // 4-BLAC: 1x4 + 1x4
  _t0_723 = _mm256_add_pd(_t0_721, _t0_722);

  // 4-BLAC: 1x4 / 1x4
  _t0_724 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_720), _mm256_castpd256_pd128(_t0_723)));

  // AVX Storer:
  _t0_194 = _t0_724;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 10)) - ( ( G(h(1, 52, 10), L[52,52],h(2, 52, 8)) * T( G(h(1, 52, 10), X[52,52],h(2, 52, 8)) ) ) + ( G(h(1, 52, 10), X[52,52],h(2, 52, 8)) * T( G(h(1, 52, 10), L[52,52],h(2, 52, 8)) ) ) ) ),h(1, 52, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_726 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_188, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_188, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_727 = _t0_17;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_728 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_729 = _t0_728;

  // 4-BLAC: 1x4 * 4x1
  _t0_730 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_727, _t0_729), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_727, _t0_729), _mm256_mul_pd(_t0_727, _t0_729), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_727, _t0_729), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_727, _t0_729), _mm256_mul_pd(_t0_727, _t0_729), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_727, _t0_729), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_727, _t0_729), _mm256_mul_pd(_t0_727, _t0_729), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_731 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_732 = _t0_17;

  // 4-BLAC: (1x4)^T
  _t0_733 = _t0_732;

  // 4-BLAC: 1x4 * 4x1
  _t0_734 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_731, _t0_733), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_731, _t0_733), _mm256_mul_pd(_t0_731, _t0_733), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_731, _t0_733), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_731, _t0_733), _mm256_mul_pd(_t0_731, _t0_733), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_731, _t0_733), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_731, _t0_733), _mm256_mul_pd(_t0_731, _t0_733), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_735 = _mm256_add_pd(_t0_730, _t0_734);

  // 4-BLAC: 1x4 - 1x4
  _t0_736 = _mm256_sub_pd(_t0_726, _t0_735);

  // AVX Storer:
  _t0_195 = _t0_736;

  // Generating : X[52,52] = S(h(1, 52, 10), ( G(h(1, 52, 10), X[52,52],h(1, 52, 10)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 10), L[52,52],h(1, 52, 10)) ) ),h(1, 52, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_737 = _t0_195;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_738 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_739 = _t0_29;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_740 = _mm256_mul_pd(_t0_738, _t0_739);

  // 4-BLAC: 1x4 / 1x4
  _t0_742 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_737), _mm256_castpd256_pd128(_t0_740)));

  // AVX Storer:
  _t0_195 = _t0_742;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(3, 52, 8)) - ( G(h(1, 52, 11), L[52,52],h(3, 52, 8)) * G(h(3, 52, 8), X[52,52],h(3, 52, 8)) ) ),h(3, 52, 8))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_743 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_189, 7);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_744 = _t0_16;

  // AVX Loader:

  // 3x3 -> 4x4 - LowSymm
  _t0_745 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_186, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_190, _t0_191), _mm256_setzero_pd(), 12), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32), 32), _t0_186, 8);
  _t0_746 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_190, _t0_191), _mm256_setzero_pd(), 12), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32), 32), 6), _t0_186, 8);
  _t0_747 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32);
  _t0_748 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_750 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_744, _t0_744, 32), _mm256_permute2f128_pd(_t0_744, _t0_744, 32), 0), _t0_745), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_744, _t0_744, 32), _mm256_permute2f128_pd(_t0_744, _t0_744, 32), 15), _t0_746)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_744, _t0_744, 49), _mm256_permute2f128_pd(_t0_744, _t0_744, 49), 0), _t0_747), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_744, _t0_744, 49), _mm256_permute2f128_pd(_t0_744, _t0_744, 49), 15), _t0_748)));

  // 4-BLAC: 1x4 - 1x4
  _t0_751 = _mm256_sub_pd(_t0_743, _t0_750);

  // AVX Storer:
  _t0_196 = _t0_751;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 8)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 8), L[52,52],h(1, 52, 8)) ) ),h(1, 52, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_752 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_196, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_753 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_754 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t0_759 = _mm256_add_pd(_t0_753, _t0_754);

  // 4-BLAC: 1x4 / 1x4
  _t0_760 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_752), _mm256_castpd256_pd128(_t0_759)));

  // AVX Storer:
  _t0_197 = _t0_760;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 9)) - ( G(h(1, 52, 11), X[52,52],h(1, 52, 8)) Kro T( G(h(1, 52, 9), L[52,52],h(1, 52, 8)) ) ) ),h(1, 52, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_761 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_196, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_762 = _t0_197;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_763 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t0_764 = _t0_763;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_765 = _mm256_mul_pd(_t0_762, _t0_764);

  // 4-BLAC: 1x4 - 1x4
  _t0_766 = _mm256_sub_pd(_t0_761, _t0_765);

  // AVX Storer:
  _t0_198 = _t0_766;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 9)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 9), L[52,52],h(1, 52, 9)) ) ),h(1, 52, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_767 = _t0_198;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_768 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_769 = _t0_31;

  // 4-BLAC: 1x4 + 1x4
  _t0_770 = _mm256_add_pd(_t0_768, _t0_769);

  // 4-BLAC: 1x4 / 1x4
  _t0_771 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_767), _mm256_castpd256_pd128(_t0_770)));

  // AVX Storer:
  _t0_198 = _t0_771;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 10)) - ( G(h(1, 52, 11), X[52,52],h(2, 52, 8)) * T( G(h(1, 52, 10), L[52,52],h(2, 52, 8)) ) ) ),h(1, 52, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_773 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_196, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_196, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_774 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_775 = _t0_17;

  // 4-BLAC: (1x4)^T
  _t0_776 = _t0_775;

  // 4-BLAC: 1x4 * 4x1
  _t0_777 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_774, _t0_776), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_774, _t0_776), _mm256_mul_pd(_t0_774, _t0_776), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_774, _t0_776), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_774, _t0_776), _mm256_mul_pd(_t0_774, _t0_776), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_774, _t0_776), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_774, _t0_776), _mm256_mul_pd(_t0_774, _t0_776), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_778 = _mm256_sub_pd(_t0_773, _t0_777);

  // AVX Storer:
  _t0_199 = _t0_778;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 10)) Div ( G(h(1, 52, 11), L[52,52],h(1, 52, 11)) + G(h(1, 52, 10), L[52,52],h(1, 52, 10)) ) ),h(1, 52, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_779 = _t0_199;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_780 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_781 = _t0_29;

  // 4-BLAC: 1x4 + 1x4
  _t0_782 = _mm256_add_pd(_t0_780, _t0_781);

  // 4-BLAC: 1x4 / 1x4
  _t0_783 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_779), _mm256_castpd256_pd128(_t0_782)));

  // AVX Storer:
  _t0_199 = _t0_783;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 11)) - ( ( G(h(1, 52, 11), L[52,52],h(3, 52, 8)) * T( G(h(1, 52, 11), X[52,52],h(3, 52, 8)) ) ) + ( G(h(1, 52, 11), X[52,52],h(3, 52, 8)) * T( G(h(1, 52, 11), L[52,52],h(3, 52, 8)) ) ) ) ),h(1, 52, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_784 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_189, _t0_189, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_785 = _t0_16;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_786 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_unpacklo_pd(_t0_199, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_787 = _t0_786;

  // 4-BLAC: 1x4 * 4x1
  _t0_789 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_785, _t0_787), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_785, _t0_787), _mm256_mul_pd(_t0_785, _t0_787), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_785, _t0_787), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_785, _t0_787), _mm256_mul_pd(_t0_785, _t0_787), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_785, _t0_787), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_785, _t0_787), _mm256_mul_pd(_t0_785, _t0_787), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_790 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_unpacklo_pd(_t0_199, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_791 = _t0_16;

  // 4-BLAC: (1x4)^T
  _t0_792 = _t0_791;

  // 4-BLAC: 1x4 * 4x1
  _t0_793 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_790, _t0_792), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_790, _t0_792), _mm256_mul_pd(_t0_790, _t0_792), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_790, _t0_792), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_790, _t0_792), _mm256_mul_pd(_t0_790, _t0_792), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_790, _t0_792), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_790, _t0_792), _mm256_mul_pd(_t0_790, _t0_792), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_794 = _mm256_add_pd(_t0_789, _t0_793);

  // 4-BLAC: 1x4 - 1x4
  _t0_795 = _mm256_sub_pd(_t0_784, _t0_794);

  // AVX Storer:
  _t0_200 = _t0_795;

  // Generating : X[52,52] = S(h(1, 52, 11), ( G(h(1, 52, 11), X[52,52],h(1, 52, 11)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, 11), L[52,52],h(1, 52, 11)) ) ),h(1, 52, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_796 = _t0_200;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_797 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_798 = _t0_27;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_799 = _mm256_mul_pd(_t0_797, _t0_798);

  // 4-BLAC: 1x4 / 1x4
  _t0_800 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_796), _mm256_castpd256_pd128(_t0_799)));

  // AVX Storer:
  _t0_200 = _t0_800;

  // Generating : X[52,52] = ( ( ( ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), C[52,52],h(4, 52, 0)) - ( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0)) + Sum_{j84} ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), C[52,52],h(4, 52, j84)) - ( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, j84), X[52,52],h(4, 52, 0)) ) ) ),h(4, 52, j84)) ) ) + Sum_{k85} ( ( ( Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * G(h(4, 52, k85), X[52,52],h(4, 52, j84)) ),h(4, 52, j84)) ) + -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * G(h(4, 52, k85), X[52,52],h(4, 52, k85)) ),h(4, 52, k85)) ) + Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * T( G(h(4, 52, j84), X[52,52],h(4, 52, k85)) ) ),h(4, 52, j84)) ) ) ) ) + Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, fi1407 - 4)) * G(h(4, 52, fi1407 - 4), X[52,52],h(4, 52, j84)) ),h(4, 52, j84)) ) ) + -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, fi1407 - 4)) * G(h(4, 52, fi1407 - 4), X[52,52],h(4, 52, fi1407 - 4)) ),h(4, 52, fi1407 - 4)) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_801 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_111, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 0), 32);
  _t0_802 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 32);
  _t0_803 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 12);
  _t0_804 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32);

  // 4-BLAC: 4x4 * 4x4
  _t0_248 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_15, _t0_801), _mm256_mul_pd(_t0_14, _t0_802)), _mm256_add_pd(_mm256_mul_pd(_t0_13, _t0_803), _mm256_mul_pd(_t0_12, _t0_804)));
  _t0_249 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_11, _t0_801), _mm256_mul_pd(_t0_10, _t0_802)), _mm256_add_pd(_mm256_mul_pd(_t0_9, _t0_803), _mm256_mul_pd(_t0_8, _t0_804)));
  _t0_250 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_7, _t0_801), _mm256_mul_pd(_t0_6, _t0_802)), _mm256_add_pd(_mm256_mul_pd(_t0_5, _t0_803), _mm256_mul_pd(_t0_4, _t0_804)));
  _t0_251 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_3, _t0_801), _mm256_mul_pd(_t0_2, _t0_802)), _mm256_add_pd(_mm256_mul_pd(_t0_1, _t0_803), _mm256_mul_pd(_t0_0, _t0_804)));

  // 4-BLAC: 4x4 - 4x4
  _t0_272 = _mm256_sub_pd(_t0_272, _t0_248);
  _t0_273 = _mm256_sub_pd(_t0_273, _t0_249);
  _t0_274 = _mm256_sub_pd(_t0_274, _t0_250);
  _t0_275 = _mm256_sub_pd(_t0_275, _t0_251);

  // AVX Storer:

  // AVX Loader:

  _mm_store_sd(&(C[208]), _mm256_castpd256_pd128(_t0_123));
  _mm_store_sd(&(C[209]), _mm256_castpd256_pd128(_t0_124));
  _mm_store_sd(&(C[210]), _mm256_castpd256_pd128(_t0_125));
  _mm_store_sd(&(C[211]), _mm256_castpd256_pd128(_t0_126));
  _mm_store_sd(&(C[260]), _mm256_castpd256_pd128(_t0_127));
  _mm_store_sd(&(C[261]), _mm256_castpd256_pd128(_t0_128));
  _mm_store_sd(&(C[262]), _mm256_castpd256_pd128(_t0_129));
  _mm_store_sd(&(C[263]), _mm256_castpd256_pd128(_t0_130));
  _mm_store_sd(&(C[312]), _mm256_castpd256_pd128(_t0_131));
  _mm_store_sd(&(C[313]), _mm256_castpd256_pd128(_t0_132));
  _mm_store_sd(&(C[314]), _mm256_castpd256_pd128(_t0_133));
  _mm_store_sd(&(C[315]), _mm256_castpd256_pd128(_t0_134));
  _mm_store_sd(&(C[364]), _mm256_castpd256_pd128(_t0_135));
  _mm_store_sd(&(C[365]), _mm256_castpd256_pd128(_t0_136));
  _mm_store_sd(&(C[366]), _mm256_castpd256_pd128(_t0_137));
  _mm_store_sd(&(C[367]), _mm256_castpd256_pd128(_t0_138));
  _mm_store_sd(&(C[416]), _mm256_castpd256_pd128(_t0_154));
  _mm_store_sd(&(C[417]), _mm256_castpd256_pd128(_t0_155));
  _mm_store_sd(&(C[418]), _mm256_castpd256_pd128(_t0_156));
  _mm_store_sd(&(C[419]), _mm256_castpd256_pd128(_t0_157));
  _mm_store_sd(&(C[468]), _mm256_castpd256_pd128(_t0_158));
  _mm_store_sd(&(C[469]), _mm256_castpd256_pd128(_t0_159));
  _mm_store_sd(&(C[470]), _mm256_castpd256_pd128(_t0_160));
  _mm_store_sd(&(C[471]), _mm256_castpd256_pd128(_t0_161));
  _mm_store_sd(&(C[520]), _mm256_castpd256_pd128(_t0_162));
  _mm_store_sd(&(C[521]), _mm256_castpd256_pd128(_t0_163));
  _mm_store_sd(&(C[522]), _mm256_castpd256_pd128(_t0_164));
  _mm_store_sd(&(C[523]), _mm256_castpd256_pd128(_t0_165));
  _mm_store_sd(&(C[572]), _mm256_castpd256_pd128(_t0_166));
  _mm_store_sd(&(C[573]), _mm256_castpd256_pd128(_t0_167));
  _mm_store_sd(&(C[574]), _mm256_castpd256_pd128(_t0_168));
  _mm_store_sd(&(C[575]), _mm256_castpd256_pd128(_t0_169));

  for( int j84 = 4; j84 <= 11; j84+=4 ) {
    _t1_8 = _asm256_loadu_pd(C + j84 + 624);
    _t1_9 = _asm256_loadu_pd(C + j84 + 676);
    _t1_10 = _asm256_loadu_pd(C + j84 + 728);
    _t1_11 = _asm256_loadu_pd(C + j84 + 780);
    _t1_3 = _asm256_loadu_pd(C + 52*j84);
    _t1_2 = _asm256_loadu_pd(C + 52*j84 + 52);
    _t1_1 = _asm256_loadu_pd(C + 52*j84 + 104);
    _t1_0 = _asm256_loadu_pd(C + 52*j84 + 156);

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t1_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t1_3, _t1_2), _mm256_unpacklo_pd(_t1_1, _t1_0), 32);
    _t1_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t1_3, _t1_2), _mm256_unpackhi_pd(_t1_1, _t1_0), 32);
    _t1_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t1_3, _t1_2), _mm256_unpacklo_pd(_t1_1, _t1_0), 49);
    _t1_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t1_3, _t1_2), _mm256_unpackhi_pd(_t1_1, _t1_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t1_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_15, _t1_12), _mm256_mul_pd(_t0_14, _t1_13)), _mm256_add_pd(_mm256_mul_pd(_t0_13, _t1_14), _mm256_mul_pd(_t0_12, _t1_15)));
    _t1_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_11, _t1_12), _mm256_mul_pd(_t0_10, _t1_13)), _mm256_add_pd(_mm256_mul_pd(_t0_9, _t1_14), _mm256_mul_pd(_t0_8, _t1_15)));
    _t1_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_7, _t1_12), _mm256_mul_pd(_t0_6, _t1_13)), _mm256_add_pd(_mm256_mul_pd(_t0_5, _t1_14), _mm256_mul_pd(_t0_4, _t1_15)));
    _t1_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_3, _t1_12), _mm256_mul_pd(_t0_2, _t1_13)), _mm256_add_pd(_mm256_mul_pd(_t0_1, _t1_14), _mm256_mul_pd(_t0_0, _t1_15)));

    // 4-BLAC: 4x4 - 4x4
    _t1_8 = _mm256_sub_pd(_t1_8, _t1_4);
    _t1_9 = _mm256_sub_pd(_t1_9, _t1_5);
    _t1_10 = _mm256_sub_pd(_t1_10, _t1_6);
    _t1_11 = _mm256_sub_pd(_t1_11, _t1_7);

    // AVX Storer:
    _asm256_storeu_pd(C + j84 + 624, _t1_8);
    _asm256_storeu_pd(C + j84 + 676, _t1_9);
    _asm256_storeu_pd(C + j84 + 728, _t1_10);
    _asm256_storeu_pd(C + j84 + 780, _t1_11);
  }

  _t0_125 = _mm256_castpd128_pd256(_mm_load_sd(&(C[210])));
  _t0_123 = _mm256_castpd128_pd256(_mm_load_sd(&(C[208])));
  _t0_128 = _mm256_castpd128_pd256(_mm_load_sd(&(C[261])));
  _t0_134 = _mm256_castpd128_pd256(_mm_load_sd(&(C[315])));
  _t0_129 = _mm256_castpd128_pd256(_mm_load_sd(&(C[262])));
  _t0_130 = _mm256_castpd128_pd256(_mm_load_sd(&(C[263])));
  _t0_135 = _mm256_castpd128_pd256(_mm_load_sd(&(C[364])));
  _t0_124 = _mm256_castpd128_pd256(_mm_load_sd(&(C[209])));
  _t0_138 = _mm256_castpd128_pd256(_mm_load_sd(&(C[367])));
  _t0_126 = _mm256_castpd128_pd256(_mm_load_sd(&(C[211])));
  _t0_133 = _mm256_castpd128_pd256(_mm_load_sd(&(C[314])));
  _t0_136 = _mm256_castpd128_pd256(_mm_load_sd(&(C[365])));
  _t0_137 = _mm256_castpd128_pd256(_mm_load_sd(&(C[366])));
  _t0_132 = _mm256_castpd128_pd256(_mm_load_sd(&(C[313])));
  _t0_127 = _mm256_castpd128_pd256(_mm_load_sd(&(C[260])));
  _t0_131 = _mm256_castpd128_pd256(_mm_load_sd(&(C[312])));
  _t2_15 = _mm256_broadcast_sd(L + 628);
  _t2_14 = _mm256_broadcast_sd(L + 629);
  _t2_13 = _mm256_broadcast_sd(L + 630);
  _t2_12 = _mm256_broadcast_sd(L + 631);
  _t2_11 = _mm256_broadcast_sd(L + 680);
  _t2_10 = _mm256_broadcast_sd(L + 681);
  _t2_9 = _mm256_broadcast_sd(L + 682);
  _t2_8 = _mm256_broadcast_sd(L + 683);
  _t2_7 = _mm256_broadcast_sd(L + 732);
  _t2_6 = _mm256_broadcast_sd(L + 733);
  _t2_5 = _mm256_broadcast_sd(L + 734);
  _t2_4 = _mm256_broadcast_sd(L + 735);
  _t2_3 = _mm256_broadcast_sd(L + 784);
  _t2_2 = _mm256_broadcast_sd(L + 785);
  _t2_1 = _mm256_broadcast_sd(L + 786);
  _t2_0 = _mm256_broadcast_sd(L + 787);
  _t2_16 = _asm256_loadu_pd(C + 628);
  _t2_17 = _asm256_loadu_pd(C + 680);
  _t2_18 = _asm256_loadu_pd(C + 732);
  _t2_19 = _asm256_loadu_pd(C + 784);
  _t2_20 = _asm256_loadu_pd(C + 632);
  _t2_21 = _asm256_loadu_pd(C + 684);
  _t2_22 = _asm256_loadu_pd(C + 736);
  _t2_23 = _asm256_loadu_pd(C + 788);

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t2_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_15, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t2_14, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t2_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t2_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));
  _t2_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t2_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t2_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t2_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));
  _t2_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t2_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t2_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t2_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));
  _t2_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_123, _t0_124), _mm256_unpacklo_pd(_t0_125, _t0_126), 32)), _mm256_mul_pd(_t2_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_127, _t0_128), _mm256_unpacklo_pd(_t0_129, _t0_130), 32))), _mm256_add_pd(_mm256_mul_pd(_t2_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_131, _t0_132), _mm256_unpacklo_pd(_t0_133, _t0_134), 32)), _mm256_mul_pd(_t2_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_135, _t0_136), _mm256_unpacklo_pd(_t0_137, _t0_138), 32))));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t0_272 = _mm256_sub_pd(_t0_272, _t2_24);
  _t0_273 = _mm256_sub_pd(_t0_273, _t2_25);
  _t0_274 = _mm256_sub_pd(_t0_274, _t2_26);
  _t0_275 = _mm256_sub_pd(_t0_275, _t2_27);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t2_36 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_139, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32), 0), 32);
  _t2_37 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_143, _t0_144), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32), 3), 32);
  _t2_38 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_146, _t0_147), _mm256_unpacklo_pd(_t0_148, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32), 3), 12);
  _t2_39 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_150, _t0_151), _mm256_unpacklo_pd(_t0_152, _t0_153), 32);

  // 4-BLAC: 4x4 * 4x4
  _t2_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_15, _t2_36), _mm256_mul_pd(_t2_14, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_13, _t2_38), _mm256_mul_pd(_t2_12, _t2_39)));
  _t2_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_11, _t2_36), _mm256_mul_pd(_t2_10, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_9, _t2_38), _mm256_mul_pd(_t2_8, _t2_39)));
  _t2_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_7, _t2_36), _mm256_mul_pd(_t2_6, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_5, _t2_38), _mm256_mul_pd(_t2_4, _t2_39)));
  _t2_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_3, _t2_36), _mm256_mul_pd(_t2_2, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_1, _t2_38), _mm256_mul_pd(_t2_0, _t2_39)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t2_16 = _mm256_sub_pd(_t2_16, _t2_28);
  _t2_17 = _mm256_sub_pd(_t2_17, _t2_29);
  _t2_18 = _mm256_sub_pd(_t2_18, _t2_30);
  _t2_19 = _mm256_sub_pd(_t2_19, _t2_31);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t2_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 32);
  _t2_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 32);
  _t2_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 49);
  _t2_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_170, _t0_171), _mm256_unpacklo_pd(_t0_172, _t0_173), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_174, _t0_175), _mm256_unpacklo_pd(_t0_176, _t0_177), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_178, _t0_179), _mm256_unpacklo_pd(_t0_180, _t0_181), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_182, _t0_183), _mm256_unpacklo_pd(_t0_184, _t0_185), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t2_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_15, _t2_40), _mm256_mul_pd(_t2_14, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t2_13, _t2_42), _mm256_mul_pd(_t2_12, _t2_43)));
  _t2_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_11, _t2_40), _mm256_mul_pd(_t2_10, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t2_9, _t2_42), _mm256_mul_pd(_t2_8, _t2_43)));
  _t2_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_7, _t2_40), _mm256_mul_pd(_t2_6, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t2_5, _t2_42), _mm256_mul_pd(_t2_4, _t2_43)));
  _t2_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_3, _t2_40), _mm256_mul_pd(_t2_2, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t2_1, _t2_42), _mm256_mul_pd(_t2_0, _t2_43)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t2_20 = _mm256_sub_pd(_t2_20, _t2_32);
  _t2_21 = _mm256_sub_pd(_t2_21, _t2_33);
  _t2_22 = _mm256_sub_pd(_t2_22, _t2_34);
  _t2_23 = _mm256_sub_pd(_t2_23, _t2_35);

  // AVX Storer:

  // AVX Loader:

  _mm_store_sd(&(C[420]), _mm256_castpd256_pd128(_t0_170));
  _mm_store_sd(&(C[421]), _mm256_castpd256_pd128(_t0_171));
  _mm_store_sd(&(C[422]), _mm256_castpd256_pd128(_t0_172));
  _mm_store_sd(&(C[423]), _mm256_castpd256_pd128(_t0_173));
  _mm_store_sd(&(C[472]), _mm256_castpd256_pd128(_t0_174));
  _mm_store_sd(&(C[473]), _mm256_castpd256_pd128(_t0_175));
  _mm_store_sd(&(C[474]), _mm256_castpd256_pd128(_t0_176));
  _mm_store_sd(&(C[475]), _mm256_castpd256_pd128(_t0_177));
  _mm_store_sd(&(C[524]), _mm256_castpd256_pd128(_t0_178));
  _mm_store_sd(&(C[525]), _mm256_castpd256_pd128(_t0_179));
  _mm_store_sd(&(C[526]), _mm256_castpd256_pd128(_t0_180));
  _mm_store_sd(&(C[527]), _mm256_castpd256_pd128(_t0_181));
  _mm_store_sd(&(C[576]), _mm256_castpd256_pd128(_t0_182));
  _mm_store_sd(&(C[577]), _mm256_castpd256_pd128(_t0_183));
  _mm_store_sd(&(C[578]), _mm256_castpd256_pd128(_t0_184));
  _mm_store_sd(&(C[579]), _mm256_castpd256_pd128(_t0_185));
  _asm256_storeu_pd(C + 624, _t0_272);
  _asm256_storeu_pd(C + 676, _t0_273);
  _asm256_storeu_pd(C + 728, _t0_274);
  _asm256_storeu_pd(C + 780, _t0_275);
  _asm256_storeu_pd(C + 628, _t2_16);
  _asm256_storeu_pd(C + 680, _t2_17);
  _asm256_storeu_pd(C + 732, _t2_18);
  _asm256_storeu_pd(C + 784, _t2_19);

  for( int j84 = 0; j84 <= 7; j84+=4 ) {
    _t3_19 = _mm256_broadcast_sd(L + 632);
    _t3_18 = _mm256_broadcast_sd(L + 633);
    _t3_17 = _mm256_broadcast_sd(L + 634);
    _t3_16 = _mm256_broadcast_sd(L + 635);
    _t3_15 = _mm256_broadcast_sd(L + 684);
    _t3_14 = _mm256_broadcast_sd(L + 685);
    _t3_13 = _mm256_broadcast_sd(L + 686);
    _t3_12 = _mm256_broadcast_sd(L + 687);
    _t3_11 = _mm256_broadcast_sd(L + 736);
    _t3_10 = _mm256_broadcast_sd(L + 737);
    _t3_9 = _mm256_broadcast_sd(L + 738);
    _t3_8 = _mm256_broadcast_sd(L + 739);
    _t3_7 = _mm256_broadcast_sd(L + 788);
    _t3_6 = _mm256_broadcast_sd(L + 789);
    _t3_5 = _mm256_broadcast_sd(L + 790);
    _t3_4 = _mm256_broadcast_sd(L + 791);
    _t3_3 = _asm256_loadu_pd(C + j84 + 416);
    _t3_2 = _asm256_loadu_pd(C + j84 + 468);
    _t3_1 = _asm256_loadu_pd(C + j84 + 520);
    _t3_0 = _asm256_loadu_pd(C + j84 + 572);
    _t3_20 = _asm256_loadu_pd(C + j84 + 624);
    _t3_21 = _asm256_loadu_pd(C + j84 + 676);
    _t3_22 = _asm256_loadu_pd(C + j84 + 728);
    _t3_23 = _asm256_loadu_pd(C + j84 + 780);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t3_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_19, _t3_3), _mm256_mul_pd(_t3_18, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_17, _t3_1), _mm256_mul_pd(_t3_16, _t3_0)));
    _t3_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t3_3), _mm256_mul_pd(_t3_14, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t3_1), _mm256_mul_pd(_t3_12, _t3_0)));
    _t3_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t3_3), _mm256_mul_pd(_t3_10, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t3_1), _mm256_mul_pd(_t3_8, _t3_0)));
    _t3_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t3_3), _mm256_mul_pd(_t3_6, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t3_1), _mm256_mul_pd(_t3_4, _t3_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t3_20 = _mm256_sub_pd(_t3_20, _t3_24);
    _t3_21 = _mm256_sub_pd(_t3_21, _t3_25);
    _t3_22 = _mm256_sub_pd(_t3_22, _t3_26);
    _t3_23 = _mm256_sub_pd(_t3_23, _t3_27);

    // AVX Storer:
    _asm256_storeu_pd(C + j84 + 624, _t3_20);
    _asm256_storeu_pd(C + j84 + 676, _t3_21);
    _asm256_storeu_pd(C + j84 + 728, _t3_22);
    _asm256_storeu_pd(C + j84 + 780, _t3_23);
  }

  _t0_272 = _asm256_loadu_pd(C + 624);
  _t0_274 = _asm256_loadu_pd(C + 728);
  _t2_17 = _asm256_loadu_pd(C + 680);
  _t0_273 = _asm256_loadu_pd(C + 676);
  _t2_16 = _asm256_loadu_pd(C + 628);
  _t2_18 = _asm256_loadu_pd(C + 732);
  _t0_275 = _asm256_loadu_pd(C + 780);
  _t2_19 = _asm256_loadu_pd(C + 784);
  _t4_26 = _mm256_broadcast_sd(L + 632);
  _t4_25 = _mm256_broadcast_sd(L + 633);
  _t4_24 = _mm256_broadcast_sd(L + 634);
  _t4_23 = _mm256_broadcast_sd(L + 635);
  _t4_22 = _mm256_broadcast_sd(L + 684);
  _t4_21 = _mm256_broadcast_sd(L + 685);
  _t4_20 = _mm256_broadcast_sd(L + 686);
  _t4_19 = _mm256_broadcast_sd(L + 687);
  _t4_18 = _mm256_broadcast_sd(L + 736);
  _t4_17 = _mm256_broadcast_sd(L + 737);
  _t4_16 = _mm256_broadcast_sd(L + 738);
  _t4_15 = _mm256_broadcast_sd(L + 739);
  _t4_14 = _mm256_broadcast_sd(L + 788);
  _t4_13 = _mm256_broadcast_sd(L + 789);
  _t4_12 = _mm256_broadcast_sd(L + 790);
  _t4_11 = _mm256_broadcast_sd(L + 791);
  _t4_10 = _mm256_castpd128_pd256(_mm_load_sd(&(L[636])));
  _t4_9 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 688)), _mm256_castpd128_pd256(_mm_load_sd(L + 740))), _mm256_castpd128_pd256(_mm_load_sd(L + 792)), 32);
  _t4_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[689])));
  _t4_7 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 741)), _mm256_castpd128_pd256(_mm_load_sd(L + 793)), 0);
  _t4_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[742])));
  _t4_5 = _mm256_broadcast_sd(&(L[794]));
  _t4_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[795])));
  _t4_79 = _mm256_castpd128_pd256(_mm_load_sd(C + 636));
  _t4_80 = _mm256_maskload_pd(C + 688, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_81 = _mm256_maskload_pd(C + 740, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_82 = _asm256_loadu_pd(C + 792);
  _t4_3 = _asm256_loadu_pd(L + 624);
  _t4_2 = _asm256_loadu_pd(L + 676);
  _t4_1 = _asm256_loadu_pd(L + 728);
  _t4_0 = _asm256_loadu_pd(L + 780);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t4_114 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_186, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_190, _t0_191), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_unpacklo_pd(_t0_199, _t0_200), 32), 0), 32);
  _t4_115 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_190, _t0_191), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_unpacklo_pd(_t0_199, _t0_200), 32), 3), 32);
  _t4_116 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_193, _t0_194), _mm256_unpacklo_pd(_t0_195, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_unpacklo_pd(_t0_199, _t0_200), 32), 3), 12);
  _t4_117 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_197, _t0_198), _mm256_unpacklo_pd(_t0_199, _t0_200), 32);

  // 4-BLAC: 4x4 * 4x4
  _t4_86 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_26, _t4_114), _mm256_mul_pd(_t4_25, _t4_115)), _mm256_add_pd(_mm256_mul_pd(_t4_24, _t4_116), _mm256_mul_pd(_t4_23, _t4_117)));
  _t4_87 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_22, _t4_114), _mm256_mul_pd(_t4_21, _t4_115)), _mm256_add_pd(_mm256_mul_pd(_t4_20, _t4_116), _mm256_mul_pd(_t4_19, _t4_117)));
  _t4_88 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_18, _t4_114), _mm256_mul_pd(_t4_17, _t4_115)), _mm256_add_pd(_mm256_mul_pd(_t4_16, _t4_116), _mm256_mul_pd(_t4_15, _t4_117)));
  _t4_89 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_14, _t4_114), _mm256_mul_pd(_t4_13, _t4_115)), _mm256_add_pd(_mm256_mul_pd(_t4_12, _t4_116), _mm256_mul_pd(_t4_11, _t4_117)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t2_20 = _mm256_sub_pd(_t2_20, _t4_86);
  _t2_21 = _mm256_sub_pd(_t2_21, _t4_87);
  _t2_22 = _mm256_sub_pd(_t2_22, _t4_88);
  _t2_23 = _mm256_sub_pd(_t2_23, _t4_89);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_118 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_272, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_119 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_120 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t4_121 = _mm256_add_pd(_t4_119, _t4_120);

  // 4-BLAC: 1x4 / 1x4
  _t4_122 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_118), _mm256_castpd256_pd128(_t4_121)));

  // AVX Storer:
  _t4_31 = _t4_122;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_123 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_272, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_124 = _t4_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_125 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t4_126 = _t4_125;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_127 = _mm256_mul_pd(_t4_124, _t4_126);

  // 4-BLAC: 1x4 - 1x4
  _t4_128 = _mm256_sub_pd(_t4_123, _t4_127);

  // AVX Storer:
  _t4_32 = _t4_128;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_129 = _t4_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_130 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_131 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t4_132 = _mm256_add_pd(_t4_130, _t4_131);

  // 4-BLAC: 1x4 / 1x4
  _t4_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_129), _mm256_castpd256_pd128(_t4_132)));

  // AVX Storer:
  _t4_32 = _t4_133;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_134 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_272, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_272, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_135 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_136 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t4_137 = _t4_136;

  // 4-BLAC: 1x4 * 4x1
  _t4_138 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_135, _t4_137), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_135, _t4_137), _mm256_mul_pd(_t4_135, _t4_137), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_135, _t4_137), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_135, _t4_137), _mm256_mul_pd(_t4_135, _t4_137), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_135, _t4_137), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_135, _t4_137), _mm256_mul_pd(_t4_135, _t4_137), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_139 = _mm256_sub_pd(_t4_134, _t4_138);

  // AVX Storer:
  _t4_33 = _t4_139;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_140 = _t4_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_141 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_142 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t4_143 = _mm256_add_pd(_t4_141, _t4_142);

  // 4-BLAC: 1x4 / 1x4
  _t4_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_140), _mm256_castpd256_pd128(_t4_143)));

  // AVX Storer:
  _t4_33 = _t4_144;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_145 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_272, _t0_272, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_146 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_147 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t4_148 = _t4_147;

  // 4-BLAC: 1x4 * 4x1
  _t4_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_146, _t4_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_146, _t4_148), _mm256_mul_pd(_t4_146, _t4_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_146, _t4_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_146, _t4_148), _mm256_mul_pd(_t4_146, _t4_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_146, _t4_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_146, _t4_148), _mm256_mul_pd(_t4_146, _t4_148), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_150 = _mm256_sub_pd(_t4_145, _t4_149);

  // AVX Storer:
  _t4_34 = _t4_150;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_151 = _t4_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_152 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_153 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t4_154 = _mm256_add_pd(_t4_152, _t4_153);

  // 4-BLAC: 1x4 / 1x4
  _t4_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_151), _mm256_castpd256_pd128(_t4_154)));

  // AVX Storer:
  _t4_34 = _t4_155;

  // Generating : X[52,52] = S(h(3, 52, fi1407 + 1), ( G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) - ( G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) * G(h(1, 52, fi1407), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t4_156 = _t0_273;
  _t4_157 = _t0_274;
  _t4_158 = _t0_275;
  _t4_159 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_160 = _t4_9;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_161 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_160, _t4_160, 32), _mm256_permute2f128_pd(_t4_160, _t4_160, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32));
  _t4_162 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_160, _t4_160, 32), _mm256_permute2f128_pd(_t4_160, _t4_160, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32));
  _t4_163 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_160, _t4_160, 49), _mm256_permute2f128_pd(_t4_160, _t4_160, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32));
  _t4_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_160, _t4_160, 49), _mm256_permute2f128_pd(_t4_160, _t4_160, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_165 = _mm256_sub_pd(_t4_156, _t4_161);
  _t4_166 = _mm256_sub_pd(_t4_157, _t4_162);
  _t4_167 = _mm256_sub_pd(_t4_158, _t4_163);
  _t4_168 = _mm256_sub_pd(_t4_159, _t4_164);

  // AVX Storer:
  _t0_273 = _t4_165;
  _t0_274 = _t4_166;
  _t0_275 = _t4_167;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_169 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_273, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_170 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_171 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t4_172 = _mm256_add_pd(_t4_170, _t4_171);

  // 4-BLAC: 1x4 / 1x4
  _t4_173 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_169), _mm256_castpd256_pd128(_t4_172)));

  // AVX Storer:
  _t4_35 = _t4_173;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_174 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_273, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_175 = _t4_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_176 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t4_177 = _t4_176;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_178 = _mm256_mul_pd(_t4_175, _t4_177);

  // 4-BLAC: 1x4 - 1x4
  _t4_179 = _mm256_sub_pd(_t4_174, _t4_178);

  // AVX Storer:
  _t4_36 = _t4_179;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_180 = _t4_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_181 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_182 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t4_183 = _mm256_add_pd(_t4_181, _t4_182);

  // 4-BLAC: 1x4 / 1x4
  _t4_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_180), _mm256_castpd256_pd128(_t4_183)));

  // AVX Storer:
  _t4_36 = _t4_184;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_185 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_273, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_273, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_186 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_187 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t4_188 = _t4_187;

  // 4-BLAC: 1x4 * 4x1
  _t4_189 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_mul_pd(_t4_186, _t4_188), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_mul_pd(_t4_186, _t4_188), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_mul_pd(_t4_186, _t4_188), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_190 = _mm256_sub_pd(_t4_185, _t4_189);

  // AVX Storer:
  _t4_37 = _t4_190;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_191 = _t4_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_192 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_193 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t4_194 = _mm256_add_pd(_t4_192, _t4_193);

  // 4-BLAC: 1x4 / 1x4
  _t4_195 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_191), _mm256_castpd256_pd128(_t4_194)));

  // AVX Storer:
  _t4_37 = _t4_195;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_196 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_273, _t0_273, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_197 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_198 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t4_199 = _t4_198;

  // 4-BLAC: 1x4 * 4x1
  _t4_200 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_197, _t4_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_197, _t4_199), _mm256_mul_pd(_t4_197, _t4_199), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_197, _t4_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_197, _t4_199), _mm256_mul_pd(_t4_197, _t4_199), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_197, _t4_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_197, _t4_199), _mm256_mul_pd(_t4_197, _t4_199), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_201 = _mm256_sub_pd(_t4_196, _t4_200);

  // AVX Storer:
  _t4_38 = _t4_201;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_202 = _t4_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_203 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_204 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t4_205 = _mm256_add_pd(_t4_203, _t4_204);

  // 4-BLAC: 1x4 / 1x4
  _t4_206 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_202), _mm256_castpd256_pd128(_t4_205)));

  // AVX Storer:
  _t4_38 = _t4_206;

  // Generating : X[52,52] = S(h(2, 52, fi1407 + 2), ( G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) - ( G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) * G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t4_207 = _t0_274;
  _t4_208 = _t0_275;
  _t4_209 = _mm256_setzero_pd();
  _t4_210 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_211 = _t4_7;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_212 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_211, _t4_211, 32), _mm256_permute2f128_pd(_t4_211, _t4_211, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32));
  _t4_213 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_211, _t4_211, 32), _mm256_permute2f128_pd(_t4_211, _t4_211, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32));
  _t4_214 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_211, _t4_211, 49), _mm256_permute2f128_pd(_t4_211, _t4_211, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32));
  _t4_215 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_211, _t4_211, 49), _mm256_permute2f128_pd(_t4_211, _t4_211, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_216 = _mm256_sub_pd(_t4_207, _t4_212);
  _t4_217 = _mm256_sub_pd(_t4_208, _t4_213);
  _t4_218 = _mm256_sub_pd(_t4_209, _t4_214);
  _t4_219 = _mm256_sub_pd(_t4_210, _t4_215);

  // AVX Storer:
  _t0_274 = _t4_216;
  _t0_275 = _t4_217;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_220 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_274, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_221 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_222 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t4_223 = _mm256_add_pd(_t4_221, _t4_222);

  // 4-BLAC: 1x4 / 1x4
  _t4_224 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_220), _mm256_castpd256_pd128(_t4_223)));

  // AVX Storer:
  _t4_39 = _t4_224;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_225 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_274, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_226 = _t4_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_227 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t4_228 = _t4_227;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_229 = _mm256_mul_pd(_t4_226, _t4_228);

  // 4-BLAC: 1x4 - 1x4
  _t4_230 = _mm256_sub_pd(_t4_225, _t4_229);

  // AVX Storer:
  _t4_40 = _t4_230;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_231 = _t4_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_232 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_233 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t4_234 = _mm256_add_pd(_t4_232, _t4_233);

  // 4-BLAC: 1x4 / 1x4
  _t4_235 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_231), _mm256_castpd256_pd128(_t4_234)));

  // AVX Storer:
  _t4_40 = _t4_235;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_236 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_274, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_274, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_237 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_238 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t4_239 = _t4_238;

  // 4-BLAC: 1x4 * 4x1
  _t4_240 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_237, _t4_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_237, _t4_239), _mm256_mul_pd(_t4_237, _t4_239), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_237, _t4_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_237, _t4_239), _mm256_mul_pd(_t4_237, _t4_239), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_237, _t4_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_237, _t4_239), _mm256_mul_pd(_t4_237, _t4_239), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_241 = _mm256_sub_pd(_t4_236, _t4_240);

  // AVX Storer:
  _t4_41 = _t4_241;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_242 = _t4_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_243 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_244 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t4_245 = _mm256_add_pd(_t4_243, _t4_244);

  // 4-BLAC: 1x4 / 1x4
  _t4_246 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_242), _mm256_castpd256_pd128(_t4_245)));

  // AVX Storer:
  _t4_41 = _t4_246;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_247 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_274, _t0_274, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_248 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_41, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_249 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t4_250 = _t4_249;

  // 4-BLAC: 1x4 * 4x1
  _t4_251 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_248, _t4_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_248, _t4_250), _mm256_mul_pd(_t4_248, _t4_250), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_248, _t4_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_248, _t4_250), _mm256_mul_pd(_t4_248, _t4_250), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_248, _t4_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_248, _t4_250), _mm256_mul_pd(_t4_248, _t4_250), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_252 = _mm256_sub_pd(_t4_247, _t4_251);

  // AVX Storer:
  _t4_42 = _t4_252;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_253 = _t4_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_254 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_255 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t4_256 = _mm256_add_pd(_t4_254, _t4_255);

  // 4-BLAC: 1x4 / 1x4
  _t4_257 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_253), _mm256_castpd256_pd128(_t4_256)));

  // AVX Storer:
  _t4_42 = _t4_257;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) Kro G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_258 = _t4_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t4_83 = _mm256_mul_pd(_t4_258, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_41, _t4_42), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_275 = _mm256_sub_pd(_t0_275, _t4_83);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_259 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_275, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_260 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_261 = _t0_102;

  // 4-BLAC: 1x4 + 1x4
  _t4_262 = _mm256_add_pd(_t4_260, _t4_261);

  // 4-BLAC: 1x4 / 1x4
  _t4_263 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_259), _mm256_castpd256_pd128(_t4_262)));

  // AVX Storer:
  _t4_43 = _t4_263;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_264 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_275, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_265 = _t4_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_266 = _t0_101;

  // 4-BLAC: (4x1)^T
  _t4_267 = _t4_266;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_268 = _mm256_mul_pd(_t4_265, _t4_267);

  // 4-BLAC: 1x4 - 1x4
  _t4_269 = _mm256_sub_pd(_t4_264, _t4_268);

  // AVX Storer:
  _t4_44 = _t4_269;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_270 = _t4_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_271 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_272 = _t0_100;

  // 4-BLAC: 1x4 + 1x4
  _t4_273 = _mm256_add_pd(_t4_271, _t4_272);

  // 4-BLAC: 1x4 / 1x4
  _t4_274 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_270), _mm256_castpd256_pd128(_t4_273)));

  // AVX Storer:
  _t4_44 = _t4_274;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_275 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_275, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_275, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_276 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_43, _t4_44), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_277 = _t0_99;

  // 4-BLAC: (1x4)^T
  _t4_278 = _t4_277;

  // 4-BLAC: 1x4 * 4x1
  _t4_279 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_276, _t4_278), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_276, _t4_278), _mm256_mul_pd(_t4_276, _t4_278), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_276, _t4_278), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_276, _t4_278), _mm256_mul_pd(_t4_276, _t4_278), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_276, _t4_278), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_276, _t4_278), _mm256_mul_pd(_t4_276, _t4_278), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_280 = _mm256_sub_pd(_t4_275, _t4_279);

  // AVX Storer:
  _t4_45 = _t4_280;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_281 = _t4_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_282 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_283 = _t0_98;

  // 4-BLAC: 1x4 + 1x4
  _t4_284 = _mm256_add_pd(_t4_282, _t4_283);

  // 4-BLAC: 1x4 / 1x4
  _t4_285 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_281), _mm256_castpd256_pd128(_t4_284)));

  // AVX Storer:
  _t4_45 = _t4_285;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_286 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_275, _t0_275, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_287 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_43, _t4_44), _mm256_unpacklo_pd(_t4_45, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_288 = _t0_97;

  // 4-BLAC: (1x4)^T
  _t4_289 = _t4_288;

  // 4-BLAC: 1x4 * 4x1
  _t4_290 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_287, _t4_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_287, _t4_289), _mm256_mul_pd(_t4_287, _t4_289), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_287, _t4_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_287, _t4_289), _mm256_mul_pd(_t4_287, _t4_289), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_287, _t4_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_287, _t4_289), _mm256_mul_pd(_t4_287, _t4_289), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_291 = _mm256_sub_pd(_t4_286, _t4_290);

  // AVX Storer:
  _t4_46 = _t4_291;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_292 = _t4_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_293 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_294 = _t0_96;

  // 4-BLAC: 1x4 + 1x4
  _t4_295 = _mm256_add_pd(_t4_293, _t4_294);

  // 4-BLAC: 1x4 / 1x4
  _t4_296 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_292), _mm256_castpd256_pd128(_t4_295)));

  // AVX Storer:
  _t4_46 = _t4_296;

  // Generating : X[52,52] = S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) ) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_659 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 32);
  _t4_660 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 32);
  _t4_661 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 49);
  _t4_662 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_90 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_31, _t4_31, 32), _mm256_permute2f128_pd(_t4_31, _t4_31, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_32, _t4_32, 32), _mm256_permute2f128_pd(_t4_32, _t4_32, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_33, _t4_33, 32), _mm256_permute2f128_pd(_t4_33, _t4_33, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 0), _t4_662)));
  _t4_91 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_35, _t4_35, 32), _mm256_permute2f128_pd(_t4_35, _t4_35, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_36, _t4_36, 32), _mm256_permute2f128_pd(_t4_36, _t4_36, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_37, _t4_37, 32), _mm256_permute2f128_pd(_t4_37, _t4_37, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_38, _t4_38, 32), _mm256_permute2f128_pd(_t4_38, _t4_38, 32), 0), _t4_662)));
  _t4_92 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_39, _t4_39, 32), _mm256_permute2f128_pd(_t4_39, _t4_39, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_40, _t4_40, 32), _mm256_permute2f128_pd(_t4_40, _t4_40, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_41, _t4_41, 32), _mm256_permute2f128_pd(_t4_41, _t4_41, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_42, _t4_42, 32), _mm256_permute2f128_pd(_t4_42, _t4_42, 32), 0), _t4_662)));
  _t4_93 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_43, _t4_43, 32), _mm256_permute2f128_pd(_t4_43, _t4_43, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_44, _t4_44, 32), _mm256_permute2f128_pd(_t4_44, _t4_44, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_45, _t4_45, 32), _mm256_permute2f128_pd(_t4_45, _t4_45, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_46, _t4_46, 32), _mm256_permute2f128_pd(_t4_46, _t4_46, 32), 0), _t4_662)));

  // 4-BLAC: 4x4 - 4x4
  _t2_16 = _mm256_sub_pd(_t2_16, _t4_90);
  _t2_17 = _mm256_sub_pd(_t2_17, _t4_91);
  _t2_18 = _mm256_sub_pd(_t2_18, _t4_92);
  _t2_19 = _mm256_sub_pd(_t2_19, _t4_93);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_297 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_298 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_299 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t4_300 = _mm256_add_pd(_t4_298, _t4_299);

  // 4-BLAC: 1x4 / 1x4
  _t4_301 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_297), _mm256_castpd256_pd128(_t4_300)));

  // AVX Storer:
  _t4_47 = _t4_301;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_302 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_303 = _t4_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_304 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t4_305 = _t4_304;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_306 = _mm256_mul_pd(_t4_303, _t4_305);

  // 4-BLAC: 1x4 - 1x4
  _t4_307 = _mm256_sub_pd(_t4_302, _t4_306);

  // AVX Storer:
  _t4_48 = _t4_307;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_308 = _t4_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_309 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_310 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t4_311 = _mm256_add_pd(_t4_309, _t4_310);

  // 4-BLAC: 1x4 / 1x4
  _t4_312 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_308), _mm256_castpd256_pd128(_t4_311)));

  // AVX Storer:
  _t4_48 = _t4_312;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_313 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_314 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_47, _t4_48), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_315 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t4_316 = _t4_315;

  // 4-BLAC: 1x4 * 4x1
  _t4_317 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_314, _t4_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_314, _t4_316), _mm256_mul_pd(_t4_314, _t4_316), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_314, _t4_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_314, _t4_316), _mm256_mul_pd(_t4_314, _t4_316), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_314, _t4_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_314, _t4_316), _mm256_mul_pd(_t4_314, _t4_316), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_318 = _mm256_sub_pd(_t4_313, _t4_317);

  // AVX Storer:
  _t4_49 = _t4_318;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_319 = _t4_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_320 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_321 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t4_322 = _mm256_add_pd(_t4_320, _t4_321);

  // 4-BLAC: 1x4 / 1x4
  _t4_323 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_319), _mm256_castpd256_pd128(_t4_322)));

  // AVX Storer:
  _t4_49 = _t4_323;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_324 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_16, _t2_16, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_325 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_47, _t4_48), _mm256_unpacklo_pd(_t4_49, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_326 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t4_327 = _t4_326;

  // 4-BLAC: 1x4 * 4x1
  _t4_328 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_325, _t4_327), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_325, _t4_327), _mm256_mul_pd(_t4_325, _t4_327), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_325, _t4_327), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_325, _t4_327), _mm256_mul_pd(_t4_325, _t4_327), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_325, _t4_327), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_325, _t4_327), _mm256_mul_pd(_t4_325, _t4_327), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_329 = _mm256_sub_pd(_t4_324, _t4_328);

  // AVX Storer:
  _t4_50 = _t4_329;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_330 = _t4_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_331 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_332 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t4_333 = _mm256_add_pd(_t4_331, _t4_332);

  // 4-BLAC: 1x4 / 1x4
  _t4_334 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_330), _mm256_castpd256_pd128(_t4_333)));

  // AVX Storer:
  _t4_50 = _t4_334;

  // Generating : X[52,52] = S(h(3, 52, fi1407 + 1), ( G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, 4)) - ( G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) * G(h(1, 52, fi1407), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t4_335 = _t2_17;
  _t4_336 = _t2_18;
  _t4_337 = _t2_19;
  _t4_338 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_339 = _t4_9;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_340 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_339, _t4_339, 32), _mm256_permute2f128_pd(_t4_339, _t4_339, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_47, _t4_48), _mm256_unpacklo_pd(_t4_49, _t4_50), 32));
  _t4_341 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_339, _t4_339, 32), _mm256_permute2f128_pd(_t4_339, _t4_339, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_47, _t4_48), _mm256_unpacklo_pd(_t4_49, _t4_50), 32));
  _t4_342 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_339, _t4_339, 49), _mm256_permute2f128_pd(_t4_339, _t4_339, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_47, _t4_48), _mm256_unpacklo_pd(_t4_49, _t4_50), 32));
  _t4_343 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_339, _t4_339, 49), _mm256_permute2f128_pd(_t4_339, _t4_339, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_47, _t4_48), _mm256_unpacklo_pd(_t4_49, _t4_50), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_344 = _mm256_sub_pd(_t4_335, _t4_340);
  _t4_345 = _mm256_sub_pd(_t4_336, _t4_341);
  _t4_346 = _mm256_sub_pd(_t4_337, _t4_342);
  _t4_347 = _mm256_sub_pd(_t4_338, _t4_343);

  // AVX Storer:
  _t2_17 = _t4_344;
  _t2_18 = _t4_345;
  _t2_19 = _t4_346;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_348 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_17, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_349 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_350 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t4_351 = _mm256_add_pd(_t4_349, _t4_350);

  // 4-BLAC: 1x4 / 1x4
  _t4_352 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_348), _mm256_castpd256_pd128(_t4_351)));

  // AVX Storer:
  _t4_51 = _t4_352;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_353 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_17, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_354 = _t4_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_355 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t4_356 = _t4_355;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_357 = _mm256_mul_pd(_t4_354, _t4_356);

  // 4-BLAC: 1x4 - 1x4
  _t4_358 = _mm256_sub_pd(_t4_353, _t4_357);

  // AVX Storer:
  _t4_52 = _t4_358;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_359 = _t4_52;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_360 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_361 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t4_362 = _mm256_add_pd(_t4_360, _t4_361);

  // 4-BLAC: 1x4 / 1x4
  _t4_363 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_359), _mm256_castpd256_pd128(_t4_362)));

  // AVX Storer:
  _t4_52 = _t4_363;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_364 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_17, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_17, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_365 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_51, _t4_52), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_366 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t4_367 = _t4_366;

  // 4-BLAC: 1x4 * 4x1
  _t4_368 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_365, _t4_367), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_365, _t4_367), _mm256_mul_pd(_t4_365, _t4_367), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_365, _t4_367), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_365, _t4_367), _mm256_mul_pd(_t4_365, _t4_367), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_365, _t4_367), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_365, _t4_367), _mm256_mul_pd(_t4_365, _t4_367), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_369 = _mm256_sub_pd(_t4_364, _t4_368);

  // AVX Storer:
  _t4_53 = _t4_369;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_370 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_371 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_372 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t4_373 = _mm256_add_pd(_t4_371, _t4_372);

  // 4-BLAC: 1x4 / 1x4
  _t4_374 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_370), _mm256_castpd256_pd128(_t4_373)));

  // AVX Storer:
  _t4_53 = _t4_374;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_375 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_17, _t2_17, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_376 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_51, _t4_52), _mm256_unpacklo_pd(_t4_53, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_377 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t4_378 = _t4_377;

  // 4-BLAC: 1x4 * 4x1
  _t4_379 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_376, _t4_378), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_376, _t4_378), _mm256_mul_pd(_t4_376, _t4_378), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_376, _t4_378), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_376, _t4_378), _mm256_mul_pd(_t4_376, _t4_378), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_376, _t4_378), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_376, _t4_378), _mm256_mul_pd(_t4_376, _t4_378), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_380 = _mm256_sub_pd(_t4_375, _t4_379);

  // AVX Storer:
  _t4_54 = _t4_380;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_381 = _t4_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_382 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_383 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t4_384 = _mm256_add_pd(_t4_382, _t4_383);

  // 4-BLAC: 1x4 / 1x4
  _t4_385 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_381), _mm256_castpd256_pd128(_t4_384)));

  // AVX Storer:
  _t4_54 = _t4_385;

  // Generating : X[52,52] = S(h(2, 52, fi1407 + 2), ( G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, 4)) - ( G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) * G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t4_386 = _t2_18;
  _t4_387 = _t2_19;
  _t4_388 = _mm256_setzero_pd();
  _t4_389 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_390 = _t4_7;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_391 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_390, _t4_390, 32), _mm256_permute2f128_pd(_t4_390, _t4_390, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_51, _t4_52), _mm256_unpacklo_pd(_t4_53, _t4_54), 32));
  _t4_392 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_390, _t4_390, 32), _mm256_permute2f128_pd(_t4_390, _t4_390, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_51, _t4_52), _mm256_unpacklo_pd(_t4_53, _t4_54), 32));
  _t4_393 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_390, _t4_390, 49), _mm256_permute2f128_pd(_t4_390, _t4_390, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_51, _t4_52), _mm256_unpacklo_pd(_t4_53, _t4_54), 32));
  _t4_394 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_390, _t4_390, 49), _mm256_permute2f128_pd(_t4_390, _t4_390, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_51, _t4_52), _mm256_unpacklo_pd(_t4_53, _t4_54), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_395 = _mm256_sub_pd(_t4_386, _t4_391);
  _t4_396 = _mm256_sub_pd(_t4_387, _t4_392);
  _t4_397 = _mm256_sub_pd(_t4_388, _t4_393);
  _t4_398 = _mm256_sub_pd(_t4_389, _t4_394);

  // AVX Storer:
  _t2_18 = _t4_395;
  _t2_19 = _t4_396;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_399 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_18, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_400 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_401 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t4_402 = _mm256_add_pd(_t4_400, _t4_401);

  // 4-BLAC: 1x4 / 1x4
  _t4_403 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_399), _mm256_castpd256_pd128(_t4_402)));

  // AVX Storer:
  _t4_55 = _t4_403;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_404 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_18, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_405 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_406 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t4_407 = _t4_406;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_408 = _mm256_mul_pd(_t4_405, _t4_407);

  // 4-BLAC: 1x4 - 1x4
  _t4_409 = _mm256_sub_pd(_t4_404, _t4_408);

  // AVX Storer:
  _t4_56 = _t4_409;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_410 = _t4_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_411 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_412 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t4_413 = _mm256_add_pd(_t4_411, _t4_412);

  // 4-BLAC: 1x4 / 1x4
  _t4_414 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_410), _mm256_castpd256_pd128(_t4_413)));

  // AVX Storer:
  _t4_56 = _t4_414;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_415 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_18, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_18, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_416 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_55, _t4_56), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_417 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t4_418 = _t4_417;

  // 4-BLAC: 1x4 * 4x1
  _t4_419 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_416, _t4_418), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_416, _t4_418), _mm256_mul_pd(_t4_416, _t4_418), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_416, _t4_418), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_416, _t4_418), _mm256_mul_pd(_t4_416, _t4_418), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_416, _t4_418), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_416, _t4_418), _mm256_mul_pd(_t4_416, _t4_418), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_420 = _mm256_sub_pd(_t4_415, _t4_419);

  // AVX Storer:
  _t4_57 = _t4_420;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_421 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_422 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_423 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t4_424 = _mm256_add_pd(_t4_422, _t4_423);

  // 4-BLAC: 1x4 / 1x4
  _t4_425 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_421), _mm256_castpd256_pd128(_t4_424)));

  // AVX Storer:
  _t4_57 = _t4_425;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_426 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_18, _t2_18, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_427 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_55, _t4_56), _mm256_unpacklo_pd(_t4_57, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_428 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t4_429 = _t4_428;

  // 4-BLAC: 1x4 * 4x1
  _t4_430 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_427, _t4_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_427, _t4_429), _mm256_mul_pd(_t4_427, _t4_429), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_427, _t4_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_427, _t4_429), _mm256_mul_pd(_t4_427, _t4_429), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_427, _t4_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_427, _t4_429), _mm256_mul_pd(_t4_427, _t4_429), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_431 = _mm256_sub_pd(_t4_426, _t4_430);

  // AVX Storer:
  _t4_58 = _t4_431;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_432 = _t4_58;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_433 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_434 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t4_435 = _mm256_add_pd(_t4_433, _t4_434);

  // 4-BLAC: 1x4 / 1x4
  _t4_436 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_432), _mm256_castpd256_pd128(_t4_435)));

  // AVX Storer:
  _t4_58 = _t4_436;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) Kro G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_437 = _t4_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t4_84 = _mm256_mul_pd(_t4_437, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_55, _t4_56), _mm256_unpacklo_pd(_t4_57, _t4_58), 32));

  // 4-BLAC: 1x4 - 1x4
  _t2_19 = _mm256_sub_pd(_t2_19, _t4_84);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_438 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_439 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_440 = _t0_79;

  // 4-BLAC: 1x4 + 1x4
  _t4_441 = _mm256_add_pd(_t4_439, _t4_440);

  // 4-BLAC: 1x4 / 1x4
  _t4_442 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_438), _mm256_castpd256_pd128(_t4_441)));

  // AVX Storer:
  _t4_59 = _t4_442;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_443 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_19, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_444 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_445 = _t0_68;

  // 4-BLAC: (4x1)^T
  _t4_446 = _t4_445;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_447 = _mm256_mul_pd(_t4_444, _t4_446);

  // 4-BLAC: 1x4 - 1x4
  _t4_448 = _mm256_sub_pd(_t4_443, _t4_447);

  // AVX Storer:
  _t4_60 = _t4_448;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_449 = _t4_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_450 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_451 = _t0_77;

  // 4-BLAC: 1x4 + 1x4
  _t4_452 = _mm256_add_pd(_t4_450, _t4_451);

  // 4-BLAC: 1x4 / 1x4
  _t4_453 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_449), _mm256_castpd256_pd128(_t4_452)));

  // AVX Storer:
  _t4_60 = _t4_453;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_454 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_19, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_19, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_455 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_59, _t4_60), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_456 = _t0_67;

  // 4-BLAC: (1x4)^T
  _t4_457 = _t4_456;

  // 4-BLAC: 1x4 * 4x1
  _t4_458 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_455, _t4_457), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_455, _t4_457), _mm256_mul_pd(_t4_455, _t4_457), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_455, _t4_457), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_455, _t4_457), _mm256_mul_pd(_t4_455, _t4_457), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_455, _t4_457), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_455, _t4_457), _mm256_mul_pd(_t4_455, _t4_457), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_459 = _mm256_sub_pd(_t4_454, _t4_458);

  // AVX Storer:
  _t4_61 = _t4_459;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_460 = _t4_61;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_461 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_462 = _t0_75;

  // 4-BLAC: 1x4 + 1x4
  _t4_463 = _mm256_add_pd(_t4_461, _t4_462);

  // 4-BLAC: 1x4 / 1x4
  _t4_464 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_460), _mm256_castpd256_pd128(_t4_463)));

  // AVX Storer:
  _t4_61 = _t4_464;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_465 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_19, _t2_19, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_466 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_59, _t4_60), _mm256_unpacklo_pd(_t4_61, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_467 = _t0_66;

  // 4-BLAC: (1x4)^T
  _t4_468 = _t4_467;

  // 4-BLAC: 1x4 * 4x1
  _t4_469 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_466, _t4_468), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_466, _t4_468), _mm256_mul_pd(_t4_466, _t4_468), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_466, _t4_468), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_466, _t4_468), _mm256_mul_pd(_t4_466, _t4_468), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_466, _t4_468), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_466, _t4_468), _mm256_mul_pd(_t4_466, _t4_468), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_470 = _mm256_sub_pd(_t4_465, _t4_469);

  // AVX Storer:
  _t4_62 = _t4_470;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_471 = _t4_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_472 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_473 = _t0_73;

  // 4-BLAC: 1x4 + 1x4
  _t4_474 = _mm256_add_pd(_t4_472, _t4_473);

  // 4-BLAC: 1x4 / 1x4
  _t4_475 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_471), _mm256_castpd256_pd128(_t4_474)));

  // AVX Storer:
  _t4_62 = _t4_475;

  // Generating : X[52,52] = ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, fi1526)) - ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, fi1526), L[52,52],h(4, 52, 0)) ) ) ),h(4, 52, fi1526)) + Sum_{k85} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, k85)) * T( G(h(4, 52, fi1526), L[52,52],h(4, 52, k85)) ) ),h(4, 52, fi1526)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_663 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_26, _t0_25), _mm256_unpacklo_pd(_t0_24, _t0_23), 32);
  _t4_664 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_26, _t0_25), _mm256_unpackhi_pd(_t0_24, _t0_23), 32);
  _t4_665 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_26, _t0_25), _mm256_unpacklo_pd(_t0_24, _t0_23), 49);
  _t4_666 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_26, _t0_25), _mm256_unpackhi_pd(_t0_24, _t0_23), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_31, _t4_31, 32), _mm256_permute2f128_pd(_t4_31, _t4_31, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_32, _t4_32, 32), _mm256_permute2f128_pd(_t4_32, _t4_32, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_33, _t4_33, 32), _mm256_permute2f128_pd(_t4_33, _t4_33, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 0), _t4_666)));
  _t4_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_35, _t4_35, 32), _mm256_permute2f128_pd(_t4_35, _t4_35, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_36, _t4_36, 32), _mm256_permute2f128_pd(_t4_36, _t4_36, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_37, _t4_37, 32), _mm256_permute2f128_pd(_t4_37, _t4_37, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_38, _t4_38, 32), _mm256_permute2f128_pd(_t4_38, _t4_38, 32), 0), _t4_666)));
  _t4_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_39, _t4_39, 32), _mm256_permute2f128_pd(_t4_39, _t4_39, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_40, _t4_40, 32), _mm256_permute2f128_pd(_t4_40, _t4_40, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_41, _t4_41, 32), _mm256_permute2f128_pd(_t4_41, _t4_41, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_42, _t4_42, 32), _mm256_permute2f128_pd(_t4_42, _t4_42, 32), 0), _t4_666)));
  _t4_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_43, _t4_43, 32), _mm256_permute2f128_pd(_t4_43, _t4_43, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_44, _t4_44, 32), _mm256_permute2f128_pd(_t4_44, _t4_44, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_45, _t4_45, 32), _mm256_permute2f128_pd(_t4_45, _t4_45, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_46, _t4_46, 32), _mm256_permute2f128_pd(_t4_46, _t4_46, 32), 0), _t4_666)));

  // 4-BLAC: 4x4 - 4x4
  _t2_20 = _mm256_sub_pd(_t2_20, _t4_94);
  _t2_21 = _mm256_sub_pd(_t2_21, _t4_95);
  _t2_22 = _mm256_sub_pd(_t2_22, _t4_96);
  _t2_23 = _mm256_sub_pd(_t2_23, _t4_97);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_667 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_22, _t0_21), _mm256_unpacklo_pd(_t0_20, _t0_19), 32);
  _t4_668 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_22, _t0_21), _mm256_unpackhi_pd(_t0_20, _t0_19), 32);
  _t4_669 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_22, _t0_21), _mm256_unpacklo_pd(_t0_20, _t0_19), 49);
  _t4_670 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_22, _t0_21), _mm256_unpackhi_pd(_t0_20, _t0_19), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_47, _t4_47, 32), _mm256_permute2f128_pd(_t4_47, _t4_47, 32), 0), _t4_667), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_48, _t4_48, 32), _mm256_permute2f128_pd(_t4_48, _t4_48, 32), 0), _t4_668)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_49, _t4_49, 32), _mm256_permute2f128_pd(_t4_49, _t4_49, 32), 0), _t4_669), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_50, _t4_50, 32), _mm256_permute2f128_pd(_t4_50, _t4_50, 32), 0), _t4_670)));
  _t4_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_51, _t4_51, 32), _mm256_permute2f128_pd(_t4_51, _t4_51, 32), 0), _t4_667), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_52, _t4_52, 32), _mm256_permute2f128_pd(_t4_52, _t4_52, 32), 0), _t4_668)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_53, _t4_53, 32), _mm256_permute2f128_pd(_t4_53, _t4_53, 32), 0), _t4_669), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_54, _t4_54, 32), _mm256_permute2f128_pd(_t4_54, _t4_54, 32), 0), _t4_670)));
  _t4_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_55, _t4_55, 32), _mm256_permute2f128_pd(_t4_55, _t4_55, 32), 0), _t4_667), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_56, _t4_56, 32), _mm256_permute2f128_pd(_t4_56, _t4_56, 32), 0), _t4_668)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_57, _t4_57, 32), _mm256_permute2f128_pd(_t4_57, _t4_57, 32), 0), _t4_669), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_58, _t4_58, 32), _mm256_permute2f128_pd(_t4_58, _t4_58, 32), 0), _t4_670)));
  _t4_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_59, _t4_59, 32), _mm256_permute2f128_pd(_t4_59, _t4_59, 32), 0), _t4_667), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_60, _t4_60, 32), _mm256_permute2f128_pd(_t4_60, _t4_60, 32), 0), _t4_668)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_61, _t4_61, 32), _mm256_permute2f128_pd(_t4_61, _t4_61, 32), 0), _t4_669), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_62, _t4_62, 32), _mm256_permute2f128_pd(_t4_62, _t4_62, 32), 0), _t4_670)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t2_20 = _mm256_sub_pd(_t2_20, _t4_98);
  _t2_21 = _mm256_sub_pd(_t2_21, _t4_99);
  _t2_22 = _mm256_sub_pd(_t2_22, _t4_100);
  _t2_23 = _mm256_sub_pd(_t2_23, _t4_101);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_476 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_20, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_477 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_478 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t4_479 = _mm256_add_pd(_t4_477, _t4_478);

  // 4-BLAC: 1x4 / 1x4
  _t4_480 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_476), _mm256_castpd256_pd128(_t4_479)));

  // AVX Storer:
  _t4_63 = _t4_480;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_481 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_20, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_482 = _t4_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_483 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t4_484 = _t4_483;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_485 = _mm256_mul_pd(_t4_482, _t4_484);

  // 4-BLAC: 1x4 - 1x4
  _t4_486 = _mm256_sub_pd(_t4_481, _t4_485);

  // AVX Storer:
  _t4_64 = _t4_486;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_487 = _t4_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_488 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_489 = _t0_31;

  // 4-BLAC: 1x4 + 1x4
  _t4_490 = _mm256_add_pd(_t4_488, _t4_489);

  // 4-BLAC: 1x4 / 1x4
  _t4_491 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_487), _mm256_castpd256_pd128(_t4_490)));

  // AVX Storer:
  _t4_64 = _t4_491;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_492 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_20, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_20, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_493 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_63, _t4_64), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_494 = _t0_17;

  // 4-BLAC: (1x4)^T
  _t4_495 = _t4_494;

  // 4-BLAC: 1x4 * 4x1
  _t4_496 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_493, _t4_495), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_493, _t4_495), _mm256_mul_pd(_t4_493, _t4_495), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_493, _t4_495), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_493, _t4_495), _mm256_mul_pd(_t4_493, _t4_495), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_493, _t4_495), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_493, _t4_495), _mm256_mul_pd(_t4_493, _t4_495), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_497 = _mm256_sub_pd(_t4_492, _t4_496);

  // AVX Storer:
  _t4_65 = _t4_497;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_498 = _t4_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_499 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_500 = _t0_29;

  // 4-BLAC: 1x4 + 1x4
  _t4_501 = _mm256_add_pd(_t4_499, _t4_500);

  // 4-BLAC: 1x4 / 1x4
  _t4_502 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_498), _mm256_castpd256_pd128(_t4_501)));

  // AVX Storer:
  _t4_65 = _t4_502;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_503 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_20, _t2_20, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_504 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_63, _t4_64), _mm256_unpacklo_pd(_t4_65, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_505 = _t0_16;

  // 4-BLAC: (1x4)^T
  _t4_506 = _t4_505;

  // 4-BLAC: 1x4 * 4x1
  _t4_507 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_504, _t4_506), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_504, _t4_506), _mm256_mul_pd(_t4_504, _t4_506), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_504, _t4_506), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_504, _t4_506), _mm256_mul_pd(_t4_504, _t4_506), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_504, _t4_506), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_504, _t4_506), _mm256_mul_pd(_t4_504, _t4_506), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_508 = _mm256_sub_pd(_t4_503, _t4_507);

  // AVX Storer:
  _t4_66 = _t4_508;

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_509 = _t4_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_510 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_511 = _t0_27;

  // 4-BLAC: 1x4 + 1x4
  _t4_512 = _mm256_add_pd(_t4_510, _t4_511);

  // 4-BLAC: 1x4 / 1x4
  _t4_513 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_509), _mm256_castpd256_pd128(_t4_512)));

  // AVX Storer:
  _t4_66 = _t4_513;

  // Generating : X[52,52] = S(h(3, 52, fi1407 + 1), ( G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) - ( G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) * G(h(1, 52, fi1407), X[52,52],h(4, 52, fi1526)) ) ),h(4, 52, fi1526))

  // AVX Loader:

  // 3x4 -> 4x4
  _t4_514 = _t2_21;
  _t4_515 = _t2_22;
  _t4_516 = _t2_23;
  _t4_517 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_518 = _t4_9;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_519 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_518, _t4_518, 32), _mm256_permute2f128_pd(_t4_518, _t4_518, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_63, _t4_64), _mm256_unpacklo_pd(_t4_65, _t4_66), 32));
  _t4_520 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_518, _t4_518, 32), _mm256_permute2f128_pd(_t4_518, _t4_518, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_63, _t4_64), _mm256_unpacklo_pd(_t4_65, _t4_66), 32));
  _t4_521 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_518, _t4_518, 49), _mm256_permute2f128_pd(_t4_518, _t4_518, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_63, _t4_64), _mm256_unpacklo_pd(_t4_65, _t4_66), 32));
  _t4_522 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_518, _t4_518, 49), _mm256_permute2f128_pd(_t4_518, _t4_518, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_63, _t4_64), _mm256_unpacklo_pd(_t4_65, _t4_66), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_523 = _mm256_sub_pd(_t4_514, _t4_519);
  _t4_524 = _mm256_sub_pd(_t4_515, _t4_520);
  _t4_525 = _mm256_sub_pd(_t4_516, _t4_521);
  _t4_526 = _mm256_sub_pd(_t4_517, _t4_522);

  // AVX Storer:
  _t2_21 = _t4_523;
  _t2_22 = _t4_524;
  _t2_23 = _t4_525;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_527 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_528 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_529 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t4_530 = _mm256_add_pd(_t4_528, _t4_529);

  // 4-BLAC: 1x4 / 1x4
  _t4_531 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_527), _mm256_castpd256_pd128(_t4_530)));

  // AVX Storer:
  _t4_67 = _t4_531;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_532 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_533 = _t4_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_534 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t4_535 = _t4_534;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_536 = _mm256_mul_pd(_t4_533, _t4_535);

  // 4-BLAC: 1x4 - 1x4
  _t4_537 = _mm256_sub_pd(_t4_532, _t4_536);

  // AVX Storer:
  _t4_68 = _t4_537;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_538 = _t4_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_539 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_540 = _t0_31;

  // 4-BLAC: 1x4 + 1x4
  _t4_541 = _mm256_add_pd(_t4_539, _t4_540);

  // 4-BLAC: 1x4 / 1x4
  _t4_542 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_538), _mm256_castpd256_pd128(_t4_541)));

  // AVX Storer:
  _t4_68 = _t4_542;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_543 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_21, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_21, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_544 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_67, _t4_68), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_545 = _t0_17;

  // 4-BLAC: (1x4)^T
  _t4_546 = _t4_545;

  // 4-BLAC: 1x4 * 4x1
  _t4_547 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_544, _t4_546), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_544, _t4_546), _mm256_mul_pd(_t4_544, _t4_546), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_544, _t4_546), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_544, _t4_546), _mm256_mul_pd(_t4_544, _t4_546), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_544, _t4_546), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_544, _t4_546), _mm256_mul_pd(_t4_544, _t4_546), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_548 = _mm256_sub_pd(_t4_543, _t4_547);

  // AVX Storer:
  _t4_69 = _t4_548;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_549 = _t4_69;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_550 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_551 = _t0_29;

  // 4-BLAC: 1x4 + 1x4
  _t4_552 = _mm256_add_pd(_t4_550, _t4_551);

  // 4-BLAC: 1x4 / 1x4
  _t4_553 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_549), _mm256_castpd256_pd128(_t4_552)));

  // AVX Storer:
  _t4_69 = _t4_553;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_554 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_21, _t2_21, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_555 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_67, _t4_68), _mm256_unpacklo_pd(_t4_69, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_556 = _t0_16;

  // 4-BLAC: (1x4)^T
  _t4_557 = _t4_556;

  // 4-BLAC: 1x4 * 4x1
  _t4_558 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_555, _t4_557), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_555, _t4_557), _mm256_mul_pd(_t4_555, _t4_557), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_555, _t4_557), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_555, _t4_557), _mm256_mul_pd(_t4_555, _t4_557), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_555, _t4_557), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_555, _t4_557), _mm256_mul_pd(_t4_555, _t4_557), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_559 = _mm256_sub_pd(_t4_554, _t4_558);

  // AVX Storer:
  _t4_70 = _t4_559;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_560 = _t4_70;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_561 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_562 = _t0_27;

  // 4-BLAC: 1x4 + 1x4
  _t4_563 = _mm256_add_pd(_t4_561, _t4_562);

  // 4-BLAC: 1x4 / 1x4
  _t4_564 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_560), _mm256_castpd256_pd128(_t4_563)));

  // AVX Storer:
  _t4_70 = _t4_564;

  // Generating : X[52,52] = S(h(2, 52, fi1407 + 2), ( G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) - ( G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) * G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) ) ),h(4, 52, fi1526))

  // AVX Loader:

  // 2x4 -> 4x4
  _t4_565 = _t2_22;
  _t4_566 = _t2_23;
  _t4_567 = _mm256_setzero_pd();
  _t4_568 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_569 = _t4_7;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_570 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_569, _t4_569, 32), _mm256_permute2f128_pd(_t4_569, _t4_569, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_67, _t4_68), _mm256_unpacklo_pd(_t4_69, _t4_70), 32));
  _t4_571 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_569, _t4_569, 32), _mm256_permute2f128_pd(_t4_569, _t4_569, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_67, _t4_68), _mm256_unpacklo_pd(_t4_69, _t4_70), 32));
  _t4_572 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_569, _t4_569, 49), _mm256_permute2f128_pd(_t4_569, _t4_569, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_67, _t4_68), _mm256_unpacklo_pd(_t4_69, _t4_70), 32));
  _t4_573 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_569, _t4_569, 49), _mm256_permute2f128_pd(_t4_569, _t4_569, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_67, _t4_68), _mm256_unpacklo_pd(_t4_69, _t4_70), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_574 = _mm256_sub_pd(_t4_565, _t4_570);
  _t4_575 = _mm256_sub_pd(_t4_566, _t4_571);
  _t4_576 = _mm256_sub_pd(_t4_567, _t4_572);
  _t4_577 = _mm256_sub_pd(_t4_568, _t4_573);

  // AVX Storer:
  _t2_22 = _t4_574;
  _t2_23 = _t4_575;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_578 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_22, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_579 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_580 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t4_581 = _mm256_add_pd(_t4_579, _t4_580);

  // 4-BLAC: 1x4 / 1x4
  _t4_582 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_578), _mm256_castpd256_pd128(_t4_581)));

  // AVX Storer:
  _t4_71 = _t4_582;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_583 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_22, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_584 = _t4_71;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_585 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t4_586 = _t4_585;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_587 = _mm256_mul_pd(_t4_584, _t4_586);

  // 4-BLAC: 1x4 - 1x4
  _t4_588 = _mm256_sub_pd(_t4_583, _t4_587);

  // AVX Storer:
  _t4_72 = _t4_588;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_589 = _t4_72;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_590 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_591 = _t0_31;

  // 4-BLAC: 1x4 + 1x4
  _t4_592 = _mm256_add_pd(_t4_590, _t4_591);

  // 4-BLAC: 1x4 / 1x4
  _t4_593 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_589), _mm256_castpd256_pd128(_t4_592)));

  // AVX Storer:
  _t4_72 = _t4_593;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_594 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_22, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_22, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_595 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_596 = _t0_17;

  // 4-BLAC: (1x4)^T
  _t4_597 = _t4_596;

  // 4-BLAC: 1x4 * 4x1
  _t4_598 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_595, _t4_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_595, _t4_597), _mm256_mul_pd(_t4_595, _t4_597), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_595, _t4_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_595, _t4_597), _mm256_mul_pd(_t4_595, _t4_597), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_595, _t4_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_595, _t4_597), _mm256_mul_pd(_t4_595, _t4_597), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_599 = _mm256_sub_pd(_t4_594, _t4_598);

  // AVX Storer:
  _t4_73 = _t4_599;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_600 = _t4_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_601 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_602 = _t0_29;

  // 4-BLAC: 1x4 + 1x4
  _t4_603 = _mm256_add_pd(_t4_601, _t4_602);

  // 4-BLAC: 1x4 / 1x4
  _t4_604 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_600), _mm256_castpd256_pd128(_t4_603)));

  // AVX Storer:
  _t4_73 = _t4_604;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_605 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_22, _t2_22, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_606 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_607 = _t0_16;

  // 4-BLAC: (1x4)^T
  _t4_608 = _t4_607;

  // 4-BLAC: 1x4 * 4x1
  _t4_609 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_606, _t4_608), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_606, _t4_608), _mm256_mul_pd(_t4_606, _t4_608), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_606, _t4_608), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_606, _t4_608), _mm256_mul_pd(_t4_606, _t4_608), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_606, _t4_608), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_606, _t4_608), _mm256_mul_pd(_t4_606, _t4_608), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_610 = _mm256_sub_pd(_t4_605, _t4_609);

  // AVX Storer:
  _t4_74 = _t4_610;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_611 = _t4_74;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_612 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_613 = _t0_27;

  // 4-BLAC: 1x4 + 1x4
  _t4_614 = _mm256_add_pd(_t4_612, _t4_613);

  // 4-BLAC: 1x4 / 1x4
  _t4_615 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_611), _mm256_castpd256_pd128(_t4_614)));

  // AVX Storer:
  _t4_74 = _t4_615;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, fi1526)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) Kro G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) ) ),h(4, 52, fi1526))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_616 = _t4_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t4_85 = _mm256_mul_pd(_t4_616, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32));

  // 4-BLAC: 1x4 - 1x4
  _t2_23 = _mm256_sub_pd(_t2_23, _t4_85);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_617 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_23, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_618 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_619 = _t0_33;

  // 4-BLAC: 1x4 + 1x4
  _t4_620 = _mm256_add_pd(_t4_618, _t4_619);

  // 4-BLAC: 1x4 / 1x4
  _t4_621 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_617), _mm256_castpd256_pd128(_t4_620)));

  // AVX Storer:
  _t4_75 = _t4_621;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_622 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_23, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_623 = _t4_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_624 = _t0_18;

  // 4-BLAC: (4x1)^T
  _t4_625 = _t4_624;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_626 = _mm256_mul_pd(_t4_623, _t4_625);

  // 4-BLAC: 1x4 - 1x4
  _t4_627 = _mm256_sub_pd(_t4_622, _t4_626);

  // AVX Storer:
  _t4_76 = _t4_627;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_628 = _t4_76;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_629 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_630 = _t0_31;

  // 4-BLAC: 1x4 + 1x4
  _t4_631 = _mm256_add_pd(_t4_629, _t4_630);

  // 4-BLAC: 1x4 / 1x4
  _t4_632 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_628), _mm256_castpd256_pd128(_t4_631)));

  // AVX Storer:
  _t4_76 = _t4_632;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_633 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_23, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_23, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_634 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_635 = _t0_17;

  // 4-BLAC: (1x4)^T
  _t4_636 = _t4_635;

  // 4-BLAC: 1x4 * 4x1
  _t4_637 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_634, _t4_636), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_634, _t4_636), _mm256_mul_pd(_t4_634, _t4_636), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_634, _t4_636), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_634, _t4_636), _mm256_mul_pd(_t4_634, _t4_636), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_634, _t4_636), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_634, _t4_636), _mm256_mul_pd(_t4_634, _t4_636), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_638 = _mm256_sub_pd(_t4_633, _t4_637);

  // AVX Storer:
  _t4_77 = _t4_638;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_639 = _t4_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_640 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_641 = _t0_29;

  // 4-BLAC: 1x4 + 1x4
  _t4_642 = _mm256_add_pd(_t4_640, _t4_641);

  // 4-BLAC: 1x4 / 1x4
  _t4_643 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_639), _mm256_castpd256_pd128(_t4_642)));

  // AVX Storer:
  _t4_77 = _t4_643;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_644 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_23, _t2_23, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_645 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_646 = _t0_16;

  // 4-BLAC: (1x4)^T
  _t4_647 = _t4_646;

  // 4-BLAC: 1x4 * 4x1
  _t4_648 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_645, _t4_647), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_645, _t4_647), _mm256_mul_pd(_t4_645, _t4_647), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_645, _t4_647), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_645, _t4_647), _mm256_mul_pd(_t4_645, _t4_647), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_645, _t4_647), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_645, _t4_647), _mm256_mul_pd(_t4_645, _t4_647), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_649 = _mm256_sub_pd(_t4_644, _t4_648);

  // AVX Storer:
  _t4_78 = _t4_649;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_650 = _t4_78;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_651 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_652 = _t0_27;

  // 4-BLAC: 1x4 + 1x4
  _t4_653 = _mm256_add_pd(_t4_651, _t4_652);

  // 4-BLAC: 1x4 / 1x4
  _t4_654 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_650), _mm256_castpd256_pd128(_t4_653)));

  // AVX Storer:
  _t4_78 = _t4_654;

  // Generating : X[52,52] = ( ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), C[52,52],h(4, 52, fi1407)) - ( ( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) ) ) + ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) ) ) ) ),h(4, 52, fi1407)) + Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, j84)) * T( G(h(4, 52, fi1407), L[52,52],h(4, 52, j84)) ) ),h(4, 52, fi1407)) ) ) + Sum_{k85} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * T( G(h(4, 52, fi1407), X[52,52],h(4, 52, k85)) ) ),h(4, 52, fi1407)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t4_655 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_79, _t4_80, 0), _mm256_shuffle_pd(_t4_81, _t4_82, 0), 32);
  _t4_656 = _mm256_permute2f128_pd(_t4_80, _mm256_shuffle_pd(_t4_81, _t4_82, 3), 32);
  _t4_657 = _mm256_blend_pd(_t4_81, _mm256_shuffle_pd(_t4_81, _t4_82, 3), 12);
  _t4_658 = _t4_82;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_671 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_41, _t4_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_43, _t4_44), _mm256_unpacklo_pd(_t4_45, _t4_46), 32)), 32);
  _t4_672 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_41, _t4_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_43, _t4_44), _mm256_unpacklo_pd(_t4_45, _t4_46), 32)), 32);
  _t4_673 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_41, _t4_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_43, _t4_44), _mm256_unpacklo_pd(_t4_45, _t4_46), 32)), 49);
  _t4_674 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _t4_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_35, _t4_36), _mm256_unpacklo_pd(_t4_37, _t4_38), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_41, _t4_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_43, _t4_44), _mm256_unpacklo_pd(_t4_45, _t4_46), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_15, _t4_671), _mm256_mul_pd(_t0_14, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t0_13, _t4_673), _mm256_mul_pd(_t0_12, _t4_674)));
  _t4_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_11, _t4_671), _mm256_mul_pd(_t0_10, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t0_9, _t4_673), _mm256_mul_pd(_t0_8, _t4_674)));
  _t4_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_7, _t4_671), _mm256_mul_pd(_t0_6, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t0_5, _t4_673), _mm256_mul_pd(_t0_4, _t4_674)));
  _t4_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t0_3, _t4_671), _mm256_mul_pd(_t0_2, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t0_1, _t4_673), _mm256_mul_pd(_t0_0, _t4_674)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_675 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 32);
  _t4_676 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 32);
  _t4_677 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 49);
  _t4_678 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_106 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_31, _t4_31, 32), _mm256_permute2f128_pd(_t4_31, _t4_31, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_32, _t4_32, 32), _mm256_permute2f128_pd(_t4_32, _t4_32, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_33, _t4_33, 32), _mm256_permute2f128_pd(_t4_33, _t4_33, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 0), _t4_678)));
  _t4_107 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_35, _t4_35, 32), _mm256_permute2f128_pd(_t4_35, _t4_35, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_36, _t4_36, 32), _mm256_permute2f128_pd(_t4_36, _t4_36, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_37, _t4_37, 32), _mm256_permute2f128_pd(_t4_37, _t4_37, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_38, _t4_38, 32), _mm256_permute2f128_pd(_t4_38, _t4_38, 32), 0), _t4_678)));
  _t4_108 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_39, _t4_39, 32), _mm256_permute2f128_pd(_t4_39, _t4_39, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_40, _t4_40, 32), _mm256_permute2f128_pd(_t4_40, _t4_40, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_41, _t4_41, 32), _mm256_permute2f128_pd(_t4_41, _t4_41, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_42, _t4_42, 32), _mm256_permute2f128_pd(_t4_42, _t4_42, 32), 0), _t4_678)));
  _t4_109 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_43, _t4_43, 32), _mm256_permute2f128_pd(_t4_43, _t4_43, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_44, _t4_44, 32), _mm256_permute2f128_pd(_t4_44, _t4_44, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_45, _t4_45, 32), _mm256_permute2f128_pd(_t4_45, _t4_45, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_46, _t4_46, 32), _mm256_permute2f128_pd(_t4_46, _t4_46, 32), 0), _t4_678)));

  // 4-BLAC: 4x4 + 4x4
  _t4_27 = _mm256_add_pd(_t4_102, _t4_106);
  _t4_28 = _mm256_add_pd(_t4_103, _t4_107);
  _t4_29 = _mm256_add_pd(_t4_104, _t4_108);
  _t4_30 = _mm256_add_pd(_t4_105, _t4_109);

  // 4-BLAC: 4x4 - 4x4
  _t4_110 = _mm256_sub_pd(_t4_655, _t4_27);
  _t4_111 = _mm256_sub_pd(_t4_656, _t4_28);
  _t4_112 = _mm256_sub_pd(_t4_657, _t4_29);
  _t4_113 = _mm256_sub_pd(_t4_658, _t4_30);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t4_79 = _t4_110;
  _t4_80 = _t4_111;
  _t4_81 = _t4_112;
  _t4_82 = _t4_113;

  _mm_store_sd(&(C[628]), _mm256_castpd256_pd128(_t4_47));
  _mm_store_sd(&(C[629]), _mm256_castpd256_pd128(_t4_48));
  _mm_store_sd(&(C[630]), _mm256_castpd256_pd128(_t4_49));
  _mm_store_sd(&(C[631]), _mm256_castpd256_pd128(_t4_50));
  _mm_store_sd(&(C[680]), _mm256_castpd256_pd128(_t4_51));
  _mm_store_sd(&(C[681]), _mm256_castpd256_pd128(_t4_52));
  _mm_store_sd(&(C[682]), _mm256_castpd256_pd128(_t4_53));
  _mm_store_sd(&(C[683]), _mm256_castpd256_pd128(_t4_54));
  _mm_store_sd(&(C[732]), _mm256_castpd256_pd128(_t4_55));
  _mm_store_sd(&(C[733]), _mm256_castpd256_pd128(_t4_56));
  _mm_store_sd(&(C[734]), _mm256_castpd256_pd128(_t4_57));
  _mm_store_sd(&(C[735]), _mm256_castpd256_pd128(_t4_58));
  _mm_store_sd(&(C[784]), _mm256_castpd256_pd128(_t4_59));
  _mm_store_sd(&(C[785]), _mm256_castpd256_pd128(_t4_60));
  _mm_store_sd(&(C[786]), _mm256_castpd256_pd128(_t4_61));
  _mm_store_sd(&(C[787]), _mm256_castpd256_pd128(_t4_62));
  _mm_store_sd(&(C[632]), _mm256_castpd256_pd128(_t4_63));
  _mm_store_sd(&(C[633]), _mm256_castpd256_pd128(_t4_64));
  _mm_store_sd(&(C[634]), _mm256_castpd256_pd128(_t4_65));
  _mm_store_sd(&(C[635]), _mm256_castpd256_pd128(_t4_66));
  _mm_store_sd(&(C[684]), _mm256_castpd256_pd128(_t4_67));
  _mm_store_sd(&(C[685]), _mm256_castpd256_pd128(_t4_68));
  _mm_store_sd(&(C[686]), _mm256_castpd256_pd128(_t4_69));
  _mm_store_sd(&(C[687]), _mm256_castpd256_pd128(_t4_70));
  _mm_store_sd(&(C[736]), _mm256_castpd256_pd128(_t4_71));
  _mm_store_sd(&(C[737]), _mm256_castpd256_pd128(_t4_72));
  _mm_store_sd(&(C[738]), _mm256_castpd256_pd128(_t4_73));
  _mm_store_sd(&(C[739]), _mm256_castpd256_pd128(_t4_74));
  _mm_store_sd(&(C[788]), _mm256_castpd256_pd128(_t4_75));
  _mm_store_sd(&(C[789]), _mm256_castpd256_pd128(_t4_76));
  _mm_store_sd(&(C[790]), _mm256_castpd256_pd128(_t4_77));
  _mm_store_sd(&(C[791]), _mm256_castpd256_pd128(_t4_78));

  for( int j84 = 4; j84 <= 11; j84+=4 ) {
    _t5_19 = _mm256_broadcast_sd(C + j84 + 624);
    _t5_18 = _mm256_broadcast_sd(C + j84 + 625);
    _t5_17 = _mm256_broadcast_sd(C + j84 + 626);
    _t5_16 = _mm256_broadcast_sd(C + j84 + 627);
    _t5_15 = _mm256_broadcast_sd(C + j84 + 676);
    _t5_14 = _mm256_broadcast_sd(C + j84 + 677);
    _t5_13 = _mm256_broadcast_sd(C + j84 + 678);
    _t5_12 = _mm256_broadcast_sd(C + j84 + 679);
    _t5_11 = _mm256_broadcast_sd(C + j84 + 728);
    _t5_10 = _mm256_broadcast_sd(C + j84 + 729);
    _t5_9 = _mm256_broadcast_sd(C + j84 + 730);
    _t5_8 = _mm256_broadcast_sd(C + j84 + 731);
    _t5_7 = _mm256_broadcast_sd(C + j84 + 780);
    _t5_6 = _mm256_broadcast_sd(C + j84 + 781);
    _t5_5 = _mm256_broadcast_sd(C + j84 + 782);
    _t5_4 = _mm256_broadcast_sd(C + j84 + 783);
    _t5_3 = _asm256_loadu_pd(L + j84 + 624);
    _t5_2 = _asm256_loadu_pd(L + j84 + 676);
    _t5_1 = _asm256_loadu_pd(L + j84 + 728);
    _t5_0 = _asm256_loadu_pd(L + j84 + 780);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t5_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 32);
    _t5_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 32);
    _t5_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 49);
    _t5_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t5_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_19, _t5_28), _mm256_mul_pd(_t5_18, _t5_29)), _mm256_add_pd(_mm256_mul_pd(_t5_17, _t5_30), _mm256_mul_pd(_t5_16, _t5_31)));
    _t5_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t5_28), _mm256_mul_pd(_t5_14, _t5_29)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t5_30), _mm256_mul_pd(_t5_12, _t5_31)));
    _t5_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t5_28), _mm256_mul_pd(_t5_10, _t5_29)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t5_30), _mm256_mul_pd(_t5_8, _t5_31)));
    _t5_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t5_28), _mm256_mul_pd(_t5_6, _t5_29)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t5_30), _mm256_mul_pd(_t5_4, _t5_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t5_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_79, _t4_80, 0), _mm256_shuffle_pd(_t4_81, _t4_82, 0), 32);
    _t5_25 = _mm256_permute2f128_pd(_t4_80, _mm256_shuffle_pd(_t4_81, _t4_82, 3), 32);
    _t5_26 = _mm256_blend_pd(_t4_81, _mm256_shuffle_pd(_t4_81, _t4_82, 3), 12);
    _t5_27 = _t4_82;

    // 4-BLAC: 4x4 - 4x4
    _t5_24 = _mm256_sub_pd(_t5_24, _t5_20);
    _t5_25 = _mm256_sub_pd(_t5_25, _t5_21);
    _t5_26 = _mm256_sub_pd(_t5_26, _t5_22);
    _t5_27 = _mm256_sub_pd(_t5_27, _t5_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t4_79 = _t5_24;
    _t4_80 = _t5_25;
    _t4_81 = _t5_26;
    _t4_82 = _t5_27;
  }


  for( int k85 = 4; k85 <= 11; k85+=4 ) {
    _t6_19 = _mm256_broadcast_sd(L + k85 + 624);
    _t6_18 = _mm256_broadcast_sd(L + k85 + 625);
    _t6_17 = _mm256_broadcast_sd(L + k85 + 626);
    _t6_16 = _mm256_broadcast_sd(L + k85 + 627);
    _t6_15 = _mm256_broadcast_sd(L + k85 + 676);
    _t6_14 = _mm256_broadcast_sd(L + k85 + 677);
    _t6_13 = _mm256_broadcast_sd(L + k85 + 678);
    _t6_12 = _mm256_broadcast_sd(L + k85 + 679);
    _t6_11 = _mm256_broadcast_sd(L + k85 + 728);
    _t6_10 = _mm256_broadcast_sd(L + k85 + 729);
    _t6_9 = _mm256_broadcast_sd(L + k85 + 730);
    _t6_8 = _mm256_broadcast_sd(L + k85 + 731);
    _t6_7 = _mm256_broadcast_sd(L + k85 + 780);
    _t6_6 = _mm256_broadcast_sd(L + k85 + 781);
    _t6_5 = _mm256_broadcast_sd(L + k85 + 782);
    _t6_4 = _mm256_broadcast_sd(L + k85 + 783);
    _t6_3 = _asm256_loadu_pd(C + k85 + 624);
    _t6_2 = _asm256_loadu_pd(C + k85 + 676);
    _t6_1 = _asm256_loadu_pd(C + k85 + 728);
    _t6_0 = _asm256_loadu_pd(C + k85 + 780);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t6_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
    _t6_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
    _t6_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
    _t6_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_19, _t6_28), _mm256_mul_pd(_t6_18, _t6_29)), _mm256_add_pd(_mm256_mul_pd(_t6_17, _t6_30), _mm256_mul_pd(_t6_16, _t6_31)));
    _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t6_28), _mm256_mul_pd(_t6_14, _t6_29)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t6_30), _mm256_mul_pd(_t6_12, _t6_31)));
    _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t6_28), _mm256_mul_pd(_t6_10, _t6_29)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t6_30), _mm256_mul_pd(_t6_8, _t6_31)));
    _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t6_28), _mm256_mul_pd(_t6_6, _t6_29)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t6_30), _mm256_mul_pd(_t6_4, _t6_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t6_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_79, _t4_80, 0), _mm256_shuffle_pd(_t4_81, _t4_82, 0), 32);
    _t6_25 = _mm256_permute2f128_pd(_t4_80, _mm256_shuffle_pd(_t4_81, _t4_82, 3), 32);
    _t6_26 = _mm256_blend_pd(_t4_81, _mm256_shuffle_pd(_t4_81, _t4_82, 3), 12);
    _t6_27 = _t4_82;

    // 4-BLAC: 4x4 - 4x4
    _t6_24 = _mm256_sub_pd(_t6_24, _t6_20);
    _t6_25 = _mm256_sub_pd(_t6_25, _t6_21);
    _t6_26 = _mm256_sub_pd(_t6_26, _t6_22);
    _t6_27 = _mm256_sub_pd(_t6_27, _t6_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t4_79 = _t6_24;
    _t4_80 = _t6_25;
    _t4_81 = _t6_26;
    _t4_82 = _t6_27;
  }

  _t7_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[688])));
  _t7_1 = _mm256_maskload_pd(L + 740, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t7_0 = _mm256_maskload_pd(L + 792, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_14 = _t4_79;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_15 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_16 = _t4_10;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_17 = _mm256_mul_pd(_t7_15, _t7_16);

  // 4-BLAC: 1x4 / 1x4
  _t7_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_14), _mm256_castpd256_pd128(_t7_17)));

  // AVX Storer:
  _t4_79 = _t7_18;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) - ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) Kro G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_19 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_80, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_20 = _t7_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_21 = _t4_79;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_22 = _mm256_mul_pd(_t7_20, _t7_21);

  // 4-BLAC: 1x4 - 1x4
  _t7_23 = _mm256_sub_pd(_t7_19, _t7_22);

  // AVX Storer:
  _t7_3 = _t7_23;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_24 = _t7_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_25 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_26 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t7_27 = _mm256_add_pd(_t7_25, _t7_26);

  // 4-BLAC: 1x4 / 1x4
  _t7_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_24), _mm256_castpd256_pd128(_t7_27)));

  // AVX Storer:
  _t7_3 = _t7_28;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) - ( ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) ) + ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ) ),h(1, 52, fi1407 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_80, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_30 = _t7_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_31 = _t7_3;

  // 4-BLAC: (4x1)^T
  _t7_32 = _t7_31;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_33 = _mm256_mul_pd(_t7_30, _t7_32);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_34 = _t7_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_35 = _t7_2;

  // 4-BLAC: (4x1)^T
  _t7_36 = _t7_35;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_37 = _mm256_mul_pd(_t7_34, _t7_36);

  // 4-BLAC: 1x4 + 1x4
  _t7_38 = _mm256_add_pd(_t7_33, _t7_37);

  // 4-BLAC: 1x4 - 1x4
  _t7_39 = _mm256_sub_pd(_t7_29, _t7_38);

  // AVX Storer:
  _t7_4 = _t7_39;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ),h(1, 52, fi1407 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_40 = _t7_4;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_41 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_42 = _t4_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_43 = _mm256_mul_pd(_t7_41, _t7_42);

  // 4-BLAC: 1x4 / 1x4
  _t7_44 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_40), _mm256_castpd256_pd128(_t7_43)));

  // AVX Storer:
  _t7_4 = _t7_44;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) - ( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) * G(h(2, 52, fi1407), X[52,52],h(2, 52, fi1407)) ) ),h(2, 52, fi1407))

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_45 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_81, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_46 = _t7_1;

  // AVX Loader:

  // 2x2 -> 4x4 - LowSymm
  _t7_47 = _mm256_shuffle_pd(_t4_79, _mm256_blend_pd(_mm256_unpacklo_pd(_t7_3, _t7_4), _mm256_setzero_pd(), 12), 0);
  _t7_48 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_3, _t7_4), _mm256_setzero_pd(), 12);
  _t7_49 = _mm256_setzero_pd();
  _t7_50 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t7_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_46, _t7_46, 32), _mm256_permute2f128_pd(_t7_46, _t7_46, 32), 0), _t7_47), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_46, _t7_46, 32), _mm256_permute2f128_pd(_t7_46, _t7_46, 32), 15), _t7_48)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_46, _t7_46, 49), _mm256_permute2f128_pd(_t7_46, _t7_46, 49), 0), _t7_49), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_46, _t7_46, 49), _mm256_permute2f128_pd(_t7_46, _t7_46, 49), 15), _t7_50)));

  // 4-BLAC: 1x4 - 1x4
  _t7_52 = _mm256_sub_pd(_t7_45, _t7_51);

  // AVX Storer:
  _t7_5 = _t7_52;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_53 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_54 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_55 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t7_56 = _mm256_add_pd(_t7_54, _t7_55);

  // 4-BLAC: 1x4 / 1x4
  _t7_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_53), _mm256_castpd256_pd128(_t7_56)));

  // AVX Storer:
  _t7_6 = _t7_57;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ),h(1, 52, fi1407 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_58 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_59 = _t7_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_60 = _t7_2;

  // 4-BLAC: (4x1)^T
  _t7_61 = _t7_60;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_62 = _mm256_mul_pd(_t7_59, _t7_61);

  // 4-BLAC: 1x4 - 1x4
  _t7_63 = _mm256_sub_pd(_t7_58, _t7_62);

  // AVX Storer:
  _t7_7 = _t7_63;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ),h(1, 52, fi1407 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_64 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_65 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_66 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t7_67 = _mm256_add_pd(_t7_65, _t7_66);

  // 4-BLAC: 1x4 / 1x4
  _t7_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_64), _mm256_castpd256_pd128(_t7_67)));

  // AVX Storer:
  _t7_7 = _t7_68;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) - ( ( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) * T( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) ) ) + ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) * T( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) ) ),h(1, 52, fi1407 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_69 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_81, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_81, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_70 = _t7_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_71 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_6, _t7_7), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t7_72 = _t7_71;

  // 4-BLAC: 1x4 * 4x1
  _t7_73 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_70, _t7_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_70, _t7_72), _mm256_mul_pd(_t7_70, _t7_72), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_70, _t7_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_70, _t7_72), _mm256_mul_pd(_t7_70, _t7_72), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_70, _t7_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_70, _t7_72), _mm256_mul_pd(_t7_70, _t7_72), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_74 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_6, _t7_7), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_75 = _t7_1;

  // 4-BLAC: (1x4)^T
  _t7_76 = _t7_75;

  // 4-BLAC: 1x4 * 4x1
  _t7_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_74, _t7_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_74, _t7_76), _mm256_mul_pd(_t7_74, _t7_76), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_74, _t7_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_74, _t7_76), _mm256_mul_pd(_t7_74, _t7_76), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_74, _t7_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_74, _t7_76), _mm256_mul_pd(_t7_74, _t7_76), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t7_78 = _mm256_add_pd(_t7_73, _t7_77);

  // 4-BLAC: 1x4 - 1x4
  _t7_79 = _mm256_sub_pd(_t7_69, _t7_78);

  // AVX Storer:
  _t7_8 = _t7_79;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) ),h(1, 52, fi1407 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_80 = _t7_8;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_81 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_82 = _t4_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_83 = _mm256_mul_pd(_t7_81, _t7_82);

  // 4-BLAC: 1x4 / 1x4
  _t7_84 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_80), _mm256_castpd256_pd128(_t7_83)));

  // AVX Storer:
  _t7_8 = _t7_84;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) * G(h(3, 52, fi1407), X[52,52],h(3, 52, fi1407)) ) ),h(3, 52, fi1407))

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_85 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_82, 7);

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_86 = _t7_0;

  // AVX Loader:

  // 3x3 -> 4x4 - LowSymm
  _t7_87 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_79, _mm256_blend_pd(_mm256_unpacklo_pd(_t7_3, _t7_4), _mm256_setzero_pd(), 12), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_6, _t7_7), _mm256_unpacklo_pd(_t7_8, _mm256_setzero_pd()), 32), 32), _t4_79, 8);
  _t7_88 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t7_3, _t7_4), _mm256_setzero_pd(), 12), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_6, _t7_7), _mm256_unpacklo_pd(_t7_8, _mm256_setzero_pd()), 32), 32), 6), _t4_79, 8);
  _t7_89 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_6, _t7_7), _mm256_unpacklo_pd(_t7_8, _mm256_setzero_pd()), 32);
  _t7_90 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t7_91 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_86, _t7_86, 32), _mm256_permute2f128_pd(_t7_86, _t7_86, 32), 0), _t7_87), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_86, _t7_86, 32), _mm256_permute2f128_pd(_t7_86, _t7_86, 32), 15), _t7_88)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_86, _t7_86, 49), _mm256_permute2f128_pd(_t7_86, _t7_86, 49), 0), _t7_89), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_86, _t7_86, 49), _mm256_permute2f128_pd(_t7_86, _t7_86, 49), 15), _t7_90)));

  // 4-BLAC: 1x4 - 1x4
  _t7_92 = _mm256_sub_pd(_t7_85, _t7_91);

  // AVX Storer:
  _t7_9 = _t7_92;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_9, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_94 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_95 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t7_96 = _mm256_add_pd(_t7_94, _t7_95);

  // 4-BLAC: 1x4 / 1x4
  _t7_97 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_93), _mm256_castpd256_pd128(_t7_96)));

  // AVX Storer:
  _t7_10 = _t7_97;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ),h(1, 52, fi1407 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_98 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_99 = _t7_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_100 = _t7_2;

  // 4-BLAC: (4x1)^T
  _t7_101 = _t7_100;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_102 = _mm256_mul_pd(_t7_99, _t7_101);

  // 4-BLAC: 1x4 - 1x4
  _t7_103 = _mm256_sub_pd(_t7_98, _t7_102);

  // AVX Storer:
  _t7_11 = _t7_103;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ),h(1, 52, fi1407 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_104 = _t7_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_105 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_106 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t7_107 = _mm256_add_pd(_t7_105, _t7_106);

  // 4-BLAC: 1x4 / 1x4
  _t7_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_104), _mm256_castpd256_pd128(_t7_107)));

  // AVX Storer:
  _t7_11 = _t7_108;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, fi1407)) * T( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) ),h(1, 52, fi1407 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_109 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_9, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t7_9, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_110 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_10, _t7_11), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_111 = _t7_1;

  // 4-BLAC: (1x4)^T
  _t7_112 = _t7_111;

  // 4-BLAC: 1x4 * 4x1
  _t7_113 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_110, _t7_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_110, _t7_112), _mm256_mul_pd(_t7_110, _t7_112), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_110, _t7_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_110, _t7_112), _mm256_mul_pd(_t7_110, _t7_112), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_110, _t7_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_110, _t7_112), _mm256_mul_pd(_t7_110, _t7_112), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_114 = _mm256_sub_pd(_t7_109, _t7_113);

  // AVX Storer:
  _t7_12 = _t7_114;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) ),h(1, 52, fi1407 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_115 = _t7_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_116 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_117 = _t4_6;

  // 4-BLAC: 1x4 + 1x4
  _t7_118 = _mm256_add_pd(_t7_116, _t7_117);

  // 4-BLAC: 1x4 / 1x4
  _t7_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_115), _mm256_castpd256_pd128(_t7_118)));

  // AVX Storer:
  _t7_12 = _t7_119;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) - ( ( G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) * T( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) ) ) + ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) * T( G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) ) ) ) ),h(1, 52, fi1407 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_120 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_82, _t4_82, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_121 = _t7_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_122 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_10, _t7_11), _mm256_unpacklo_pd(_t7_12, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t7_123 = _t7_122;

  // 4-BLAC: 1x4 * 4x1
  _t7_124 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_121, _t7_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_121, _t7_123), _mm256_mul_pd(_t7_121, _t7_123), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_121, _t7_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_121, _t7_123), _mm256_mul_pd(_t7_121, _t7_123), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_121, _t7_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_121, _t7_123), _mm256_mul_pd(_t7_121, _t7_123), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_125 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_10, _t7_11), _mm256_unpacklo_pd(_t7_12, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_126 = _t7_0;

  // 4-BLAC: (1x4)^T
  _t7_127 = _t7_126;

  // 4-BLAC: 1x4 * 4x1
  _t7_128 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_125, _t7_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_125, _t7_127), _mm256_mul_pd(_t7_125, _t7_127), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_125, _t7_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_125, _t7_127), _mm256_mul_pd(_t7_125, _t7_127), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_125, _t7_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_125, _t7_127), _mm256_mul_pd(_t7_125, _t7_127), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t7_129 = _mm256_add_pd(_t7_124, _t7_128);

  // 4-BLAC: 1x4 - 1x4
  _t7_130 = _mm256_sub_pd(_t7_120, _t7_129);

  // AVX Storer:
  _t7_13 = _t7_130;

  // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) ),h(1, 52, fi1407 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_131 = _t7_13;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_132 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_133 = _t4_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_134 = _mm256_mul_pd(_t7_132, _t7_133);

  // 4-BLAC: 1x4 / 1x4
  _t7_135 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_131), _mm256_castpd256_pd128(_t7_134)));

  // AVX Storer:
  _t7_13 = _t7_135;

  _mm256_maskstore_pd(C + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_114);
  _mm256_maskstore_pd(C + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t0_118);
  _mm_store_sd(C + 212, _mm256_castpd256_pd128(_t0_139));
  _mm_store_sd(&(C[264]), _mm256_castpd256_pd128(_t0_143));
  _mm_store_sd(&(C[265]), _mm256_castpd256_pd128(_t0_144));
  _mm_store_sd(&(C[316]), _mm256_castpd256_pd128(_t0_146));
  _mm_store_sd(&(C[317]), _mm256_castpd256_pd128(_t0_147));
  _mm_store_sd(&(C[318]), _mm256_castpd256_pd128(_t0_148));
  _mm_store_sd(&(C[368]), _mm256_castpd256_pd128(_t0_150));
  _mm_store_sd(&(C[369]), _mm256_castpd256_pd128(_t0_151));
  _mm_store_sd(&(C[370]), _mm256_castpd256_pd128(_t0_152));
  _mm_store_sd(&(C[371]), _mm256_castpd256_pd128(_t0_153));
  _mm_store_sd(C + 424, _mm256_castpd256_pd128(_t0_186));
  _mm_store_sd(&(C[476]), _mm256_castpd256_pd128(_t0_190));
  _mm_store_sd(&(C[477]), _mm256_castpd256_pd128(_t0_191));
  _mm_store_sd(&(C[528]), _mm256_castpd256_pd128(_t0_193));
  _mm_store_sd(&(C[529]), _mm256_castpd256_pd128(_t0_194));
  _mm_store_sd(&(C[530]), _mm256_castpd256_pd128(_t0_195));
  _mm_store_sd(&(C[580]), _mm256_castpd256_pd128(_t0_197));
  _mm_store_sd(&(C[581]), _mm256_castpd256_pd128(_t0_198));
  _mm_store_sd(&(C[582]), _mm256_castpd256_pd128(_t0_199));
  _mm_store_sd(&(C[583]), _mm256_castpd256_pd128(_t0_200));
  _mm_store_sd(&(C[624]), _mm256_castpd256_pd128(_t4_31));
  _mm_store_sd(&(C[625]), _mm256_castpd256_pd128(_t4_32));
  _mm_store_sd(&(C[626]), _mm256_castpd256_pd128(_t4_33));
  _mm_store_sd(&(C[627]), _mm256_castpd256_pd128(_t4_34));
  _mm_store_sd(&(C[676]), _mm256_castpd256_pd128(_t4_35));
  _mm_store_sd(&(C[677]), _mm256_castpd256_pd128(_t4_36));
  _mm_store_sd(&(C[678]), _mm256_castpd256_pd128(_t4_37));
  _mm_store_sd(&(C[679]), _mm256_castpd256_pd128(_t4_38));
  _mm_store_sd(&(C[728]), _mm256_castpd256_pd128(_t4_39));
  _mm_store_sd(&(C[729]), _mm256_castpd256_pd128(_t4_40));
  _mm_store_sd(&(C[730]), _mm256_castpd256_pd128(_t4_41));
  _mm_store_sd(&(C[731]), _mm256_castpd256_pd128(_t4_42));
  _mm_store_sd(&(C[780]), _mm256_castpd256_pd128(_t4_43));
  _mm_store_sd(&(C[781]), _mm256_castpd256_pd128(_t4_44));
  _mm_store_sd(&(C[782]), _mm256_castpd256_pd128(_t4_45));
  _mm_store_sd(&(C[783]), _mm256_castpd256_pd128(_t4_46));
  _mm_store_sd(C + 636, _mm256_castpd256_pd128(_t4_79));
  _mm_store_sd(&(C[688]), _mm256_castpd256_pd128(_t7_3));
  _mm_store_sd(&(C[689]), _mm256_castpd256_pd128(_t7_4));
  _mm_store_sd(&(C[740]), _mm256_castpd256_pd128(_t7_6));
  _mm_store_sd(&(C[741]), _mm256_castpd256_pd128(_t7_7));
  _mm_store_sd(&(C[742]), _mm256_castpd256_pd128(_t7_8));
  _mm_store_sd(&(C[792]), _mm256_castpd256_pd128(_t7_10));
  _mm_store_sd(&(C[793]), _mm256_castpd256_pd128(_t7_11));
  _mm_store_sd(&(C[794]), _mm256_castpd256_pd128(_t7_12));
  _mm_store_sd(&(C[795]), _mm256_castpd256_pd128(_t7_13));

  for( int fi1407 = 16; fi1407 <= 48; fi1407+=4 ) {
    _t8_16 = _asm256_loadu_pd(C + 52*fi1407);
    _t8_17 = _asm256_loadu_pd(C + 52*fi1407 + 52);
    _t8_18 = _asm256_loadu_pd(C + 52*fi1407 + 104);
    _t8_19 = _asm256_loadu_pd(C + 52*fi1407 + 156);
    _t8_15 = _mm256_broadcast_sd(L + 52*fi1407);
    _t8_14 = _mm256_broadcast_sd(L + 52*fi1407 + 1);
    _t8_13 = _mm256_broadcast_sd(L + 52*fi1407 + 2);
    _t8_12 = _mm256_broadcast_sd(L + 52*fi1407 + 3);
    _t8_11 = _mm256_broadcast_sd(L + 52*fi1407 + 52);
    _t8_10 = _mm256_broadcast_sd(L + 52*fi1407 + 53);
    _t8_9 = _mm256_broadcast_sd(L + 52*fi1407 + 54);
    _t8_8 = _mm256_broadcast_sd(L + 52*fi1407 + 55);
    _t8_7 = _mm256_broadcast_sd(L + 52*fi1407 + 104);
    _t8_6 = _mm256_broadcast_sd(L + 52*fi1407 + 105);
    _t8_5 = _mm256_broadcast_sd(L + 52*fi1407 + 106);
    _t8_4 = _mm256_broadcast_sd(L + 52*fi1407 + 107);
    _t8_3 = _mm256_broadcast_sd(L + 52*fi1407 + 156);
    _t8_2 = _mm256_broadcast_sd(L + 52*fi1407 + 157);
    _t8_1 = _mm256_broadcast_sd(L + 52*fi1407 + 158);
    _t8_0 = _mm256_broadcast_sd(L + 52*fi1407 + 159);

    // Generating : X[52,52] = ( ( ( ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), C[52,52],h(4, 52, 0)) - ( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0)) + Sum_{j84} ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), C[52,52],h(4, 52, j84)) - ( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, j84), X[52,52],h(4, 52, 0)) ) ) ),h(4, 52, j84)) ) ) + Sum_{k85} ( ( ( Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * G(h(4, 52, k85), X[52,52],h(4, 52, j84)) ),h(4, 52, j84)) ) + -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * G(h(4, 52, k85), X[52,52],h(4, 52, k85)) ),h(4, 52, k85)) ) + Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * T( G(h(4, 52, j84), X[52,52],h(4, 52, k85)) ) ),h(4, 52, j84)) ) ) ) ) + Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, fi1407 - 4)) * G(h(4, 52, fi1407 - 4), X[52,52],h(4, 52, j84)) ),h(4, 52, j84)) ) ) + -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, fi1407 - 4)) * G(h(4, 52, fi1407 - 4), X[52,52],h(4, 52, fi1407 - 4)) ),h(4, 52, fi1407 - 4)) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t8_20 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_111, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 0), 32);
    _t8_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_112, _t0_113), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 32);
    _t8_22 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_115, _t0_116), _mm256_unpacklo_pd(_t0_117, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32), 3), 12);
    _t8_23 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_119, _t0_120), _mm256_unpacklo_pd(_t0_121, _t0_122), 32);

    // 4-BLAC: 4x4 * 4x4
    _t0_248 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_20), _mm256_mul_pd(_t8_14, _t8_21)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_22), _mm256_mul_pd(_t8_12, _t8_23)));
    _t0_249 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_20), _mm256_mul_pd(_t8_10, _t8_21)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_22), _mm256_mul_pd(_t8_8, _t8_23)));
    _t0_250 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_20), _mm256_mul_pd(_t8_6, _t8_21)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_22), _mm256_mul_pd(_t8_4, _t8_23)));
    _t0_251 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t8_20), _mm256_mul_pd(_t8_2, _t8_21)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t8_22), _mm256_mul_pd(_t8_0, _t8_23)));

    // 4-BLAC: 4x4 - 4x4
    _t8_16 = _mm256_sub_pd(_t8_16, _t0_248);
    _t8_17 = _mm256_sub_pd(_t8_17, _t0_249);
    _t8_18 = _mm256_sub_pd(_t8_18, _t0_250);
    _t8_19 = _mm256_sub_pd(_t8_19, _t0_251);

    // AVX Storer:

    // AVX Loader:
    _asm256_storeu_pd(C + 52*fi1407, _t8_16);
    _asm256_storeu_pd(C + 52*fi1407 + 52, _t8_17);
    _asm256_storeu_pd(C + 52*fi1407 + 104, _t8_18);
    _asm256_storeu_pd(C + 52*fi1407 + 156, _t8_19);

    for( int j84 = 4; j84 <= fi1407 - 1; j84+=4 ) {
      _t9_8 = _asm256_loadu_pd(C + 52*fi1407 + j84);
      _t9_9 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 52);
      _t9_10 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 104);
      _t9_11 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 156);
      _t9_3 = _asm256_loadu_pd(C + 52*j84);
      _t9_2 = _asm256_loadu_pd(C + 52*j84 + 52);
      _t9_1 = _asm256_loadu_pd(C + 52*j84 + 104);
      _t9_0 = _asm256_loadu_pd(C + 52*j84 + 156);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t9_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_3, _t9_2), _mm256_unpacklo_pd(_t9_1, _t9_0), 32);
      _t9_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t9_3, _t9_2), _mm256_unpackhi_pd(_t9_1, _t9_0), 32);
      _t9_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_3, _t9_2), _mm256_unpacklo_pd(_t9_1, _t9_0), 49);
      _t9_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t9_3, _t9_2), _mm256_unpackhi_pd(_t9_1, _t9_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t9_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t9_12), _mm256_mul_pd(_t8_14, _t9_13)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t9_14), _mm256_mul_pd(_t8_12, _t9_15)));
      _t9_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t9_12), _mm256_mul_pd(_t8_10, _t9_13)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t9_14), _mm256_mul_pd(_t8_8, _t9_15)));
      _t9_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t9_12), _mm256_mul_pd(_t8_6, _t9_13)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t9_14), _mm256_mul_pd(_t8_4, _t9_15)));
      _t9_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t9_12), _mm256_mul_pd(_t8_2, _t9_13)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t9_14), _mm256_mul_pd(_t8_0, _t9_15)));

      // 4-BLAC: 4x4 - 4x4
      _t9_8 = _mm256_sub_pd(_t9_8, _t9_4);
      _t9_9 = _mm256_sub_pd(_t9_9, _t9_5);
      _t9_10 = _mm256_sub_pd(_t9_10, _t9_6);
      _t9_11 = _mm256_sub_pd(_t9_11, _t9_7);

      // AVX Storer:
      _asm256_storeu_pd(C + 52*fi1407 + j84, _t9_8);
      _asm256_storeu_pd(C + 52*fi1407 + j84 + 52, _t9_9);
      _asm256_storeu_pd(C + 52*fi1407 + j84 + 104, _t9_10);
      _asm256_storeu_pd(C + 52*fi1407 + j84 + 156, _t9_11);
    }

    for( int k85 = 4; k85 <= fi1407 - 8; k85+=4 ) {

      for( int j84 = 0; j84 <= k85 - 1; j84+=4 ) {
        _t10_19 = _mm256_broadcast_sd(L + 52*fi1407 + k85);
        _t10_18 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 1);
        _t10_17 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 2);
        _t10_16 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 3);
        _t10_15 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 52);
        _t10_14 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 53);
        _t10_13 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 54);
        _t10_12 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 55);
        _t10_11 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 104);
        _t10_10 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 105);
        _t10_9 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 106);
        _t10_8 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 107);
        _t10_7 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 156);
        _t10_6 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 157);
        _t10_5 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 158);
        _t10_4 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 159);
        _t10_3 = _asm256_loadu_pd(C + j84 + 52*k85);
        _t10_2 = _asm256_loadu_pd(C + j84 + 52*k85 + 52);
        _t10_1 = _asm256_loadu_pd(C + j84 + 52*k85 + 104);
        _t10_0 = _asm256_loadu_pd(C + j84 + 52*k85 + 156);
        _t10_20 = _asm256_loadu_pd(C + 52*fi1407 + j84);
        _t10_21 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 52);
        _t10_22 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 104);
        _t10_23 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t2_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_3), _mm256_mul_pd(_t10_18, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_1), _mm256_mul_pd(_t10_16, _t10_0)));
        _t2_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_3), _mm256_mul_pd(_t10_14, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_1), _mm256_mul_pd(_t10_12, _t10_0)));
        _t2_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_3), _mm256_mul_pd(_t10_10, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_1), _mm256_mul_pd(_t10_8, _t10_0)));
        _t2_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_3), _mm256_mul_pd(_t10_6, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t10_1), _mm256_mul_pd(_t10_4, _t10_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t10_20 = _mm256_sub_pd(_t10_20, _t2_24);
        _t10_21 = _mm256_sub_pd(_t10_21, _t2_25);
        _t10_22 = _mm256_sub_pd(_t10_22, _t2_26);
        _t10_23 = _mm256_sub_pd(_t10_23, _t2_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 52*fi1407 + j84, _t10_20);
        _asm256_storeu_pd(C + 52*fi1407 + j84 + 52, _t10_21);
        _asm256_storeu_pd(C + 52*fi1407 + j84 + 104, _t10_22);
        _asm256_storeu_pd(C + 52*fi1407 + j84 + 156, _t10_23);
      }
      _t11_19 = _mm256_broadcast_sd(L + 52*fi1407 + k85);
      _t11_18 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 1);
      _t11_17 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 2);
      _t11_16 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 3);
      _t11_15 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 52);
      _t11_14 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 53);
      _t11_13 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 54);
      _t11_12 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 55);
      _t11_11 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 104);
      _t11_10 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 105);
      _t11_9 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 106);
      _t11_8 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 107);
      _t11_7 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 156);
      _t11_6 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 157);
      _t11_5 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 158);
      _t11_4 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 159);
      _t11_3 = _mm256_castpd128_pd256(_mm_load_sd(C + 53*k85));
      _t11_2 = _mm256_maskload_pd(C + 53*k85 + 52, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t11_1 = _mm256_maskload_pd(C + 53*k85 + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t11_0 = _asm256_loadu_pd(C + 53*k85 + 156);
      _t11_20 = _asm256_loadu_pd(C + 52*fi1407 + k85);
      _t11_21 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 52);
      _t11_22 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 104);
      _t11_23 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t11_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t11_3, _t11_2, 0), _mm256_shuffle_pd(_t11_1, _t11_0, 0), 32);
      _t11_25 = _mm256_permute2f128_pd(_t11_2, _mm256_shuffle_pd(_t11_1, _t11_0, 3), 32);
      _t11_26 = _mm256_blend_pd(_t11_1, _mm256_shuffle_pd(_t11_1, _t11_0, 3), 12);
      _t11_27 = _t11_0;

      // 4-BLAC: 4x4 * 4x4
      _t2_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_24), _mm256_mul_pd(_t11_18, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_26), _mm256_mul_pd(_t11_16, _t11_27)));
      _t2_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_24), _mm256_mul_pd(_t11_14, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_26), _mm256_mul_pd(_t11_12, _t11_27)));
      _t2_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_24), _mm256_mul_pd(_t11_10, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_26), _mm256_mul_pd(_t11_8, _t11_27)));
      _t2_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_24), _mm256_mul_pd(_t11_6, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_26), _mm256_mul_pd(_t11_4, _t11_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t11_20 = _mm256_sub_pd(_t11_20, _t2_28);
      _t11_21 = _mm256_sub_pd(_t11_21, _t2_29);
      _t11_22 = _mm256_sub_pd(_t11_22, _t2_30);
      _t11_23 = _mm256_sub_pd(_t11_23, _t2_31);

      // AVX Storer:
      _asm256_storeu_pd(C + 52*fi1407 + k85, _t11_20);
      _asm256_storeu_pd(C + 52*fi1407 + k85 + 52, _t11_21);
      _asm256_storeu_pd(C + 52*fi1407 + k85 + 104, _t11_22);
      _asm256_storeu_pd(C + 52*fi1407 + k85 + 156, _t11_23);

      for( int j84 = 4*floord(k85 - 1, 4) + 8; j84 <= fi1407 - 1; j84+=4 ) {
        _t12_3 = _asm256_loadu_pd(C + 52*j84 + k85);
        _t12_2 = _asm256_loadu_pd(C + 52*j84 + k85 + 52);
        _t12_1 = _asm256_loadu_pd(C + 52*j84 + k85 + 104);
        _t12_0 = _asm256_loadu_pd(C + 52*j84 + k85 + 156);
        _t12_4 = _asm256_loadu_pd(C + 52*fi1407 + j84);
        _t12_5 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 52);
        _t12_6 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 104);
        _t12_7 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t2_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 32);
        _t2_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 32);
        _t2_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 49);
        _t2_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t2_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t2_40), _mm256_mul_pd(_t11_18, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t2_42), _mm256_mul_pd(_t11_16, _t2_43)));
        _t2_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t2_40), _mm256_mul_pd(_t11_14, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t2_42), _mm256_mul_pd(_t11_12, _t2_43)));
        _t2_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t2_40), _mm256_mul_pd(_t11_10, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t2_42), _mm256_mul_pd(_t11_8, _t2_43)));
        _t2_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t2_40), _mm256_mul_pd(_t11_6, _t2_41)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t2_42), _mm256_mul_pd(_t11_4, _t2_43)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t12_4 = _mm256_sub_pd(_t12_4, _t2_32);
        _t12_5 = _mm256_sub_pd(_t12_5, _t2_33);
        _t12_6 = _mm256_sub_pd(_t12_6, _t2_34);
        _t12_7 = _mm256_sub_pd(_t12_7, _t2_35);

        // AVX Storer:
        _asm256_storeu_pd(C + 52*fi1407 + j84, _t12_4);
        _asm256_storeu_pd(C + 52*fi1407 + j84 + 52, _t12_5);
        _asm256_storeu_pd(C + 52*fi1407 + j84 + 104, _t12_6);
        _asm256_storeu_pd(C + 52*fi1407 + j84 + 156, _t12_7);
      }
    }

    // AVX Loader:

    for( int j84 = 0; j84 <= fi1407 - 5; j84+=4 ) {
      _t13_19 = _mm256_broadcast_sd(L + 53*fi1407 - 4);
      _t13_18 = _mm256_broadcast_sd(L + 53*fi1407 - 3);
      _t13_17 = _mm256_broadcast_sd(L + 53*fi1407 - 2);
      _t13_16 = _mm256_broadcast_sd(L + 53*fi1407 - 1);
      _t13_15 = _mm256_broadcast_sd(L + 53*fi1407 + 48);
      _t13_14 = _mm256_broadcast_sd(L + 53*fi1407 + 49);
      _t13_13 = _mm256_broadcast_sd(L + 53*fi1407 + 50);
      _t13_12 = _mm256_broadcast_sd(L + 53*fi1407 + 51);
      _t13_11 = _mm256_broadcast_sd(L + 53*fi1407 + 100);
      _t13_10 = _mm256_broadcast_sd(L + 53*fi1407 + 101);
      _t13_9 = _mm256_broadcast_sd(L + 53*fi1407 + 102);
      _t13_8 = _mm256_broadcast_sd(L + 53*fi1407 + 103);
      _t13_7 = _mm256_broadcast_sd(L + 53*fi1407 + 152);
      _t13_6 = _mm256_broadcast_sd(L + 53*fi1407 + 153);
      _t13_5 = _mm256_broadcast_sd(L + 53*fi1407 + 154);
      _t13_4 = _mm256_broadcast_sd(L + 53*fi1407 + 155);
      _t13_3 = _asm256_loadu_pd(C + 52*fi1407 + j84 - 208);
      _t13_2 = _asm256_loadu_pd(C + 52*fi1407 + j84 - 156);
      _t13_1 = _asm256_loadu_pd(C + 52*fi1407 + j84 - 104);
      _t13_0 = _asm256_loadu_pd(C + 52*fi1407 + j84 - 52);
      _t13_20 = _asm256_loadu_pd(C + 52*fi1407 + j84);
      _t13_21 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 52);
      _t13_22 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 104);
      _t13_23 = _asm256_loadu_pd(C + 52*fi1407 + j84 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t13_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_3), _mm256_mul_pd(_t13_18, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_1), _mm256_mul_pd(_t13_16, _t13_0)));
      _t13_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_3), _mm256_mul_pd(_t13_14, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_1), _mm256_mul_pd(_t13_12, _t13_0)));
      _t13_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_3), _mm256_mul_pd(_t13_10, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_1), _mm256_mul_pd(_t13_8, _t13_0)));
      _t13_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_3), _mm256_mul_pd(_t13_6, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_1), _mm256_mul_pd(_t13_4, _t13_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t13_20 = _mm256_sub_pd(_t13_20, _t13_24);
      _t13_21 = _mm256_sub_pd(_t13_21, _t13_25);
      _t13_22 = _mm256_sub_pd(_t13_22, _t13_26);
      _t13_23 = _mm256_sub_pd(_t13_23, _t13_27);

      // AVX Storer:
      _asm256_storeu_pd(C + 52*fi1407 + j84, _t13_20);
      _asm256_storeu_pd(C + 52*fi1407 + j84 + 52, _t13_21);
      _asm256_storeu_pd(C + 52*fi1407 + j84 + 104, _t13_22);
      _asm256_storeu_pd(C + 52*fi1407 + j84 + 156, _t13_23);
    }
    _t8_19 = _asm256_loadu_pd(C + 52*fi1407 + 156);
    _t8_16 = _asm256_loadu_pd(C + 52*fi1407);
    _t8_18 = _asm256_loadu_pd(C + 52*fi1407 + 104);
    _t8_17 = _asm256_loadu_pd(C + 52*fi1407 + 52);
    _t14_26 = _mm256_broadcast_sd(L + 53*fi1407 - 4);
    _t14_25 = _mm256_broadcast_sd(L + 53*fi1407 - 3);
    _t14_24 = _mm256_broadcast_sd(L + 53*fi1407 - 2);
    _t14_23 = _mm256_broadcast_sd(L + 53*fi1407 - 1);
    _t14_22 = _mm256_broadcast_sd(L + 53*fi1407 + 48);
    _t14_21 = _mm256_broadcast_sd(L + 53*fi1407 + 49);
    _t14_20 = _mm256_broadcast_sd(L + 53*fi1407 + 50);
    _t14_19 = _mm256_broadcast_sd(L + 53*fi1407 + 51);
    _t14_18 = _mm256_broadcast_sd(L + 53*fi1407 + 100);
    _t14_17 = _mm256_broadcast_sd(L + 53*fi1407 + 101);
    _t14_16 = _mm256_broadcast_sd(L + 53*fi1407 + 102);
    _t14_15 = _mm256_broadcast_sd(L + 53*fi1407 + 103);
    _t14_14 = _mm256_broadcast_sd(L + 53*fi1407 + 152);
    _t14_13 = _mm256_broadcast_sd(L + 53*fi1407 + 153);
    _t14_12 = _mm256_broadcast_sd(L + 53*fi1407 + 154);
    _t14_11 = _mm256_broadcast_sd(L + 53*fi1407 + 155);
    _t14_10 = _mm256_castpd128_pd256(_mm_load_sd(C + 53*fi1407 - 212));
    _t14_9 = _mm256_maskload_pd(C + 53*fi1407 - 160, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t14_8 = _mm256_maskload_pd(C + 53*fi1407 - 108, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t14_7 = _asm256_loadu_pd(C + 53*fi1407 - 56);
    _t14_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1407])));
    _t14_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 53*fi1407 + 52)), _mm256_castpd128_pd256(_mm_load_sd(L + 53*fi1407 + 104))), _mm256_castpd128_pd256(_mm_load_sd(L + 53*fi1407 + 156)), 32);
    _t14_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1407 + 53])));
    _t14_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 53*fi1407 + 105)), _mm256_castpd128_pd256(_mm_load_sd(L + 53*fi1407 + 157)), 0);
    _t14_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1407 + 106])));
    _t14_1 = _mm256_broadcast_sd(&(L[53*fi1407 + 158]));
    _t14_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1407 + 159])));
    _t14_63 = _asm256_loadu_pd(C + 52*fi1407 + 4);
    _t14_64 = _asm256_loadu_pd(C + 52*fi1407 + 56);
    _t14_65 = _asm256_loadu_pd(C + 52*fi1407 + 108);
    _t14_66 = _asm256_loadu_pd(C + 52*fi1407 + 160);
    _t14_27 = _asm256_loadu_pd(C + 53*fi1407 - 4);
    _t14_28 = _asm256_loadu_pd(C + 53*fi1407 + 48);
    _t14_29 = _asm256_loadu_pd(C + 53*fi1407 + 100);
    _t14_30 = _asm256_loadu_pd(C + 53*fi1407 + 152);

    // AVX Loader:

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t14_67 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_10, _t14_9, 0), _mm256_shuffle_pd(_t14_8, _t14_7, 0), 32);
    _t14_68 = _mm256_permute2f128_pd(_t14_9, _mm256_shuffle_pd(_t14_8, _t14_7, 3), 32);
    _t14_69 = _mm256_blend_pd(_t14_8, _mm256_shuffle_pd(_t14_8, _t14_7, 3), 12);
    _t14_70 = _t14_7;

    // 4-BLAC: 4x4 * 4x4
    _t4_86 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_26, _t14_67), _mm256_mul_pd(_t14_25, _t14_68)), _mm256_add_pd(_mm256_mul_pd(_t14_24, _t14_69), _mm256_mul_pd(_t14_23, _t14_70)));
    _t4_87 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_22, _t14_67), _mm256_mul_pd(_t14_21, _t14_68)), _mm256_add_pd(_mm256_mul_pd(_t14_20, _t14_69), _mm256_mul_pd(_t14_19, _t14_70)));
    _t4_88 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_18, _t14_67), _mm256_mul_pd(_t14_17, _t14_68)), _mm256_add_pd(_mm256_mul_pd(_t14_16, _t14_69), _mm256_mul_pd(_t14_15, _t14_70)));
    _t4_89 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_14, _t14_67), _mm256_mul_pd(_t14_13, _t14_68)), _mm256_add_pd(_mm256_mul_pd(_t14_12, _t14_69), _mm256_mul_pd(_t14_11, _t14_70)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t14_27 = _mm256_sub_pd(_t14_27, _t4_86);
    _t14_28 = _mm256_sub_pd(_t14_28, _t4_87);
    _t14_29 = _mm256_sub_pd(_t14_29, _t4_88);
    _t14_30 = _mm256_sub_pd(_t14_30, _t4_89);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_71 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_16, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_72 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_73 = _t0_102;

    // 4-BLAC: 1x4 + 1x4
    _t4_121 = _mm256_add_pd(_t14_72, _t14_73);

    // 4-BLAC: 1x4 / 1x4
    _t14_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_71), _mm256_castpd256_pd128(_t4_121)));

    // AVX Storer:
    _t14_31 = _t14_74;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_75 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_16, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_76 = _t14_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_77 = _t0_101;

    // 4-BLAC: (4x1)^T
    _t4_126 = _t14_77;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_127 = _mm256_mul_pd(_t14_76, _t4_126);

    // 4-BLAC: 1x4 - 1x4
    _t14_78 = _mm256_sub_pd(_t14_75, _t4_127);

    // AVX Storer:
    _t14_32 = _t14_78;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_79 = _t14_32;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_80 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_81 = _t0_100;

    // 4-BLAC: 1x4 + 1x4
    _t4_132 = _mm256_add_pd(_t14_80, _t14_81);

    // 4-BLAC: 1x4 / 1x4
    _t14_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_79), _mm256_castpd256_pd128(_t4_132)));

    // AVX Storer:
    _t14_32 = _t14_82;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_83 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_16, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_16, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_84 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_85 = _t0_99;

    // 4-BLAC: (1x4)^T
    _t4_137 = _t14_85;

    // 4-BLAC: 1x4 * 4x1
    _t4_138 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_84, _t4_137), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_84, _t4_137), _mm256_mul_pd(_t14_84, _t4_137), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_84, _t4_137), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_84, _t4_137), _mm256_mul_pd(_t14_84, _t4_137), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_84, _t4_137), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_84, _t4_137), _mm256_mul_pd(_t14_84, _t4_137), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_86 = _mm256_sub_pd(_t14_83, _t4_138);

    // AVX Storer:
    _t14_33 = _t14_86;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_87 = _t14_33;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_88 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_89 = _t0_98;

    // 4-BLAC: 1x4 + 1x4
    _t4_143 = _mm256_add_pd(_t14_88, _t14_89);

    // 4-BLAC: 1x4 / 1x4
    _t14_90 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_87), _mm256_castpd256_pd128(_t4_143)));

    // AVX Storer:
    _t14_33 = _t14_90;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_91 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_16, _t8_16, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_93 = _t0_97;

    // 4-BLAC: (1x4)^T
    _t4_148 = _t14_93;

    // 4-BLAC: 1x4 * 4x1
    _t4_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_92, _t4_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_92, _t4_148), _mm256_mul_pd(_t14_92, _t4_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_92, _t4_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_92, _t4_148), _mm256_mul_pd(_t14_92, _t4_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_92, _t4_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_92, _t4_148), _mm256_mul_pd(_t14_92, _t4_148), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_94 = _mm256_sub_pd(_t14_91, _t4_149);

    // AVX Storer:
    _t14_34 = _t14_94;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_95 = _t14_34;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_96 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_97 = _t0_96;

    // 4-BLAC: 1x4 + 1x4
    _t4_154 = _mm256_add_pd(_t14_96, _t14_97);

    // 4-BLAC: 1x4 / 1x4
    _t14_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_95), _mm256_castpd256_pd128(_t4_154)));

    // AVX Storer:
    _t14_34 = _t14_98;

    // Generating : X[52,52] = S(h(3, 52, fi1407 + 1), ( G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) - ( G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) * G(h(1, 52, fi1407), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

    // AVX Loader:

    // 3x4 -> 4x4
    _t14_99 = _t8_17;
    _t14_100 = _t8_18;
    _t14_101 = _t8_19;
    _t14_102 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_103 = _t14_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t4_161 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_103, _t14_103, 32), _mm256_permute2f128_pd(_t14_103, _t14_103, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32));
    _t4_162 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_103, _t14_103, 32), _mm256_permute2f128_pd(_t14_103, _t14_103, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32));
    _t4_163 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_103, _t14_103, 49), _mm256_permute2f128_pd(_t14_103, _t14_103, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32));
    _t4_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_103, _t14_103, 49), _mm256_permute2f128_pd(_t14_103, _t14_103, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32));

    // 4-BLAC: 4x4 - 4x4
    _t14_104 = _mm256_sub_pd(_t14_99, _t4_161);
    _t14_105 = _mm256_sub_pd(_t14_100, _t4_162);
    _t14_106 = _mm256_sub_pd(_t14_101, _t4_163);
    _t14_107 = _mm256_sub_pd(_t14_102, _t4_164);

    // AVX Storer:
    _t8_17 = _t14_104;
    _t8_18 = _t14_105;
    _t8_19 = _t14_106;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_108 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_17, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_109 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_110 = _t0_102;

    // 4-BLAC: 1x4 + 1x4
    _t4_172 = _mm256_add_pd(_t14_109, _t14_110);

    // 4-BLAC: 1x4 / 1x4
    _t14_111 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_108), _mm256_castpd256_pd128(_t4_172)));

    // AVX Storer:
    _t14_35 = _t14_111;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_112 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_17, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_113 = _t14_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_114 = _t0_101;

    // 4-BLAC: (4x1)^T
    _t4_177 = _t14_114;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_178 = _mm256_mul_pd(_t14_113, _t4_177);

    // 4-BLAC: 1x4 - 1x4
    _t14_115 = _mm256_sub_pd(_t14_112, _t4_178);

    // AVX Storer:
    _t14_36 = _t14_115;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_116 = _t14_36;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_117 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_118 = _t0_100;

    // 4-BLAC: 1x4 + 1x4
    _t4_183 = _mm256_add_pd(_t14_117, _t14_118);

    // 4-BLAC: 1x4 / 1x4
    _t14_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_116), _mm256_castpd256_pd128(_t4_183)));

    // AVX Storer:
    _t14_36 = _t14_119;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_120 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_17, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_17, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_121 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_122 = _t0_99;

    // 4-BLAC: (1x4)^T
    _t4_188 = _t14_122;

    // 4-BLAC: 1x4 * 4x1
    _t4_189 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_121, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_121, _t4_188), _mm256_mul_pd(_t14_121, _t4_188), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_121, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_121, _t4_188), _mm256_mul_pd(_t14_121, _t4_188), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_121, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_121, _t4_188), _mm256_mul_pd(_t14_121, _t4_188), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_123 = _mm256_sub_pd(_t14_120, _t4_189);

    // AVX Storer:
    _t14_37 = _t14_123;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_124 = _t14_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_125 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_126 = _t0_98;

    // 4-BLAC: 1x4 + 1x4
    _t4_194 = _mm256_add_pd(_t14_125, _t14_126);

    // 4-BLAC: 1x4 / 1x4
    _t14_127 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_124), _mm256_castpd256_pd128(_t4_194)));

    // AVX Storer:
    _t14_37 = _t14_127;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_128 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_17, _t8_17, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_129 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_130 = _t0_97;

    // 4-BLAC: (1x4)^T
    _t4_199 = _t14_130;

    // 4-BLAC: 1x4 * 4x1
    _t4_200 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_129, _t4_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_129, _t4_199), _mm256_mul_pd(_t14_129, _t4_199), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_129, _t4_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_129, _t4_199), _mm256_mul_pd(_t14_129, _t4_199), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_129, _t4_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_129, _t4_199), _mm256_mul_pd(_t14_129, _t4_199), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_131 = _mm256_sub_pd(_t14_128, _t4_200);

    // AVX Storer:
    _t14_38 = _t14_131;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_132 = _t14_38;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_133 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_134 = _t0_96;

    // 4-BLAC: 1x4 + 1x4
    _t4_205 = _mm256_add_pd(_t14_133, _t14_134);

    // 4-BLAC: 1x4 / 1x4
    _t14_135 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_132), _mm256_castpd256_pd128(_t4_205)));

    // AVX Storer:
    _t14_38 = _t14_135;

    // Generating : X[52,52] = S(h(2, 52, fi1407 + 2), ( G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) - ( G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) * G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

    // AVX Loader:

    // 2x4 -> 4x4
    _t14_136 = _t8_18;
    _t14_137 = _t8_19;
    _t14_138 = _mm256_setzero_pd();
    _t14_139 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_140 = _t14_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t4_212 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_140, _t14_140, 32), _mm256_permute2f128_pd(_t14_140, _t14_140, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32));
    _t4_213 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_140, _t14_140, 32), _mm256_permute2f128_pd(_t14_140, _t14_140, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32));
    _t4_214 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_140, _t14_140, 49), _mm256_permute2f128_pd(_t14_140, _t14_140, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32));
    _t4_215 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_140, _t14_140, 49), _mm256_permute2f128_pd(_t14_140, _t14_140, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32));

    // 4-BLAC: 4x4 - 4x4
    _t14_141 = _mm256_sub_pd(_t14_136, _t4_212);
    _t14_142 = _mm256_sub_pd(_t14_137, _t4_213);
    _t14_143 = _mm256_sub_pd(_t14_138, _t4_214);
    _t14_144 = _mm256_sub_pd(_t14_139, _t4_215);

    // AVX Storer:
    _t8_18 = _t14_141;
    _t8_19 = _t14_142;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_145 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_18, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_146 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_147 = _t0_102;

    // 4-BLAC: 1x4 + 1x4
    _t4_223 = _mm256_add_pd(_t14_146, _t14_147);

    // 4-BLAC: 1x4 / 1x4
    _t14_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_145), _mm256_castpd256_pd128(_t4_223)));

    // AVX Storer:
    _t14_39 = _t14_148;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_149 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_18, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_150 = _t14_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_151 = _t0_101;

    // 4-BLAC: (4x1)^T
    _t4_228 = _t14_151;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_229 = _mm256_mul_pd(_t14_150, _t4_228);

    // 4-BLAC: 1x4 - 1x4
    _t14_152 = _mm256_sub_pd(_t14_149, _t4_229);

    // AVX Storer:
    _t14_40 = _t14_152;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_153 = _t14_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_154 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_155 = _t0_100;

    // 4-BLAC: 1x4 + 1x4
    _t4_234 = _mm256_add_pd(_t14_154, _t14_155);

    // 4-BLAC: 1x4 / 1x4
    _t14_156 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_153), _mm256_castpd256_pd128(_t4_234)));

    // AVX Storer:
    _t14_40 = _t14_156;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_157 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_18, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_18, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_158 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_159 = _t0_99;

    // 4-BLAC: (1x4)^T
    _t4_239 = _t14_159;

    // 4-BLAC: 1x4 * 4x1
    _t4_240 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_158, _t4_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_158, _t4_239), _mm256_mul_pd(_t14_158, _t4_239), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_158, _t4_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_158, _t4_239), _mm256_mul_pd(_t14_158, _t4_239), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_158, _t4_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_158, _t4_239), _mm256_mul_pd(_t14_158, _t4_239), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_160 = _mm256_sub_pd(_t14_157, _t4_240);

    // AVX Storer:
    _t14_41 = _t14_160;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_161 = _t14_41;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_162 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_163 = _t0_98;

    // 4-BLAC: 1x4 + 1x4
    _t4_245 = _mm256_add_pd(_t14_162, _t14_163);

    // 4-BLAC: 1x4 / 1x4
    _t14_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_161), _mm256_castpd256_pd128(_t4_245)));

    // AVX Storer:
    _t14_41 = _t14_164;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_165 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_18, _t8_18, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_166 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_unpacklo_pd(_t14_41, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_167 = _t0_97;

    // 4-BLAC: (1x4)^T
    _t4_250 = _t14_167;

    // 4-BLAC: 1x4 * 4x1
    _t4_251 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_166, _t4_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_166, _t4_250), _mm256_mul_pd(_t14_166, _t4_250), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_166, _t4_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_166, _t4_250), _mm256_mul_pd(_t14_166, _t4_250), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_166, _t4_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_166, _t4_250), _mm256_mul_pd(_t14_166, _t4_250), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_168 = _mm256_sub_pd(_t14_165, _t4_251);

    // AVX Storer:
    _t14_42 = _t14_168;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_169 = _t14_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_170 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_171 = _t0_96;

    // 4-BLAC: 1x4 + 1x4
    _t4_256 = _mm256_add_pd(_t14_170, _t14_171);

    // 4-BLAC: 1x4 / 1x4
    _t14_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_169), _mm256_castpd256_pd128(_t4_256)));

    // AVX Storer:
    _t14_42 = _t14_172;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) Kro G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_173 = _t14_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t4_83 = _mm256_mul_pd(_t14_173, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_unpacklo_pd(_t14_41, _t14_42), 32));

    // 4-BLAC: 1x4 - 1x4
    _t8_19 = _mm256_sub_pd(_t8_19, _t4_83);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_174 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_175 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_176 = _t0_102;

    // 4-BLAC: 1x4 + 1x4
    _t4_262 = _mm256_add_pd(_t14_175, _t14_176);

    // 4-BLAC: 1x4 / 1x4
    _t14_177 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_174), _mm256_castpd256_pd128(_t4_262)));

    // AVX Storer:
    _t14_43 = _t14_177;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 0)) Kro T( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_178 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_19, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_179 = _t14_43;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_180 = _t0_101;

    // 4-BLAC: (4x1)^T
    _t4_267 = _t14_180;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_268 = _mm256_mul_pd(_t14_179, _t4_267);

    // 4-BLAC: 1x4 - 1x4
    _t14_181 = _mm256_sub_pd(_t14_178, _t4_268);

    // AVX Storer:
    _t14_44 = _t14_181;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_182 = _t14_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_183 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_184 = _t0_100;

    // 4-BLAC: 1x4 + 1x4
    _t4_273 = _mm256_add_pd(_t14_183, _t14_184);

    // 4-BLAC: 1x4 / 1x4
    _t14_185 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_182), _mm256_castpd256_pd128(_t4_273)));

    // AVX Storer:
    _t14_44 = _t14_185;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, 0)) * T( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_186 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_19, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_19, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_187 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_43, _t14_44), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_188 = _t0_99;

    // 4-BLAC: (1x4)^T
    _t4_278 = _t14_188;

    // 4-BLAC: 1x4 * 4x1
    _t4_279 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_187, _t4_278), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_187, _t4_278), _mm256_mul_pd(_t14_187, _t4_278), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_187, _t4_278), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_187, _t4_278), _mm256_mul_pd(_t14_187, _t4_278), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_187, _t4_278), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_187, _t4_278), _mm256_mul_pd(_t14_187, _t4_278), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_189 = _mm256_sub_pd(_t14_186, _t4_279);

    // AVX Storer:
    _t14_45 = _t14_189;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_190 = _t14_45;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_191 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_192 = _t0_98;

    // 4-BLAC: 1x4 + 1x4
    _t4_284 = _mm256_add_pd(_t14_191, _t14_192);

    // 4-BLAC: 1x4 / 1x4
    _t14_193 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_190), _mm256_castpd256_pd128(_t4_284)));

    // AVX Storer:
    _t14_45 = _t14_193;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, 0)) * T( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_194 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_195 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_43, _t14_44), _mm256_unpacklo_pd(_t14_45, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_196 = _t0_97;

    // 4-BLAC: (1x4)^T
    _t4_289 = _t14_196;

    // 4-BLAC: 1x4 * 4x1
    _t4_290 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_195, _t4_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_195, _t4_289), _mm256_mul_pd(_t14_195, _t4_289), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_195, _t4_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_195, _t4_289), _mm256_mul_pd(_t14_195, _t4_289), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_195, _t4_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_195, _t4_289), _mm256_mul_pd(_t14_195, _t4_289), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_197 = _mm256_sub_pd(_t14_194, _t4_290);

    // AVX Storer:
    _t14_46 = _t14_197;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_198 = _t14_46;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_199 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_200 = _t0_96;

    // 4-BLAC: 1x4 + 1x4
    _t4_295 = _mm256_add_pd(_t14_199, _t14_200);

    // 4-BLAC: 1x4 / 1x4
    _t14_201 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_198), _mm256_castpd256_pd128(_t4_295)));

    // AVX Storer:
    _t14_46 = _t14_201;

    // Generating : X[52,52] = S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) ) ) ),h(4, 52, 4))

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t4_659 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 32);
    _t4_660 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 32);
    _t4_661 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_71), _mm256_unpacklo_pd(_t0_70, _t0_69), 49);
    _t4_662 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_72, _t0_71), _mm256_unpackhi_pd(_t0_70, _t0_69), 49);

    // 4-BLAC: 4x4 * 4x4
    _t4_90 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_31, _t14_31, 32), _mm256_permute2f128_pd(_t14_31, _t14_31, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_32, _t14_32, 32), _mm256_permute2f128_pd(_t14_32, _t14_32, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_33, _t14_33, 32), _mm256_permute2f128_pd(_t14_33, _t14_33, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_34, _t14_34, 32), _mm256_permute2f128_pd(_t14_34, _t14_34, 32), 0), _t4_662)));
    _t4_91 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_35, _t14_35, 32), _mm256_permute2f128_pd(_t14_35, _t14_35, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_36, _t14_36, 32), _mm256_permute2f128_pd(_t14_36, _t14_36, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_37, _t14_37, 32), _mm256_permute2f128_pd(_t14_37, _t14_37, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_38, _t14_38, 32), _mm256_permute2f128_pd(_t14_38, _t14_38, 32), 0), _t4_662)));
    _t4_92 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_39, _t14_39, 32), _mm256_permute2f128_pd(_t14_39, _t14_39, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_40, _t14_40, 32), _mm256_permute2f128_pd(_t14_40, _t14_40, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_41, _t14_41, 32), _mm256_permute2f128_pd(_t14_41, _t14_41, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_42, _t14_42, 32), _mm256_permute2f128_pd(_t14_42, _t14_42, 32), 0), _t4_662)));
    _t4_93 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_43, _t14_43, 32), _mm256_permute2f128_pd(_t14_43, _t14_43, 32), 0), _t4_659), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_44, _t14_44, 32), _mm256_permute2f128_pd(_t14_44, _t14_44, 32), 0), _t4_660)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_45, _t14_45, 32), _mm256_permute2f128_pd(_t14_45, _t14_45, 32), 0), _t4_661), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_46, _t14_46, 32), _mm256_permute2f128_pd(_t14_46, _t14_46, 32), 0), _t4_662)));

    // 4-BLAC: 4x4 - 4x4
    _t14_63 = _mm256_sub_pd(_t14_63, _t4_90);
    _t14_64 = _mm256_sub_pd(_t14_64, _t4_91);
    _t14_65 = _mm256_sub_pd(_t14_65, _t4_92);
    _t14_66 = _mm256_sub_pd(_t14_66, _t4_93);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_202 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_63, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_203 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_204 = _t0_79;

    // 4-BLAC: 1x4 + 1x4
    _t4_300 = _mm256_add_pd(_t14_203, _t14_204);

    // 4-BLAC: 1x4 / 1x4
    _t14_205 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_202), _mm256_castpd256_pd128(_t4_300)));

    // AVX Storer:
    _t14_47 = _t14_205;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_206 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_63, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_207 = _t14_47;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_208 = _t0_68;

    // 4-BLAC: (4x1)^T
    _t4_305 = _t14_208;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_306 = _mm256_mul_pd(_t14_207, _t4_305);

    // 4-BLAC: 1x4 - 1x4
    _t14_209 = _mm256_sub_pd(_t14_206, _t4_306);

    // AVX Storer:
    _t14_48 = _t14_209;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_210 = _t14_48;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_211 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_212 = _t0_77;

    // 4-BLAC: 1x4 + 1x4
    _t4_311 = _mm256_add_pd(_t14_211, _t14_212);

    // 4-BLAC: 1x4 / 1x4
    _t14_213 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_210), _mm256_castpd256_pd128(_t4_311)));

    // AVX Storer:
    _t14_48 = _t14_213;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_214 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_63, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t14_63, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_215 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_47, _t14_48), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_216 = _t0_67;

    // 4-BLAC: (1x4)^T
    _t4_316 = _t14_216;

    // 4-BLAC: 1x4 * 4x1
    _t4_317 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_215, _t4_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_215, _t4_316), _mm256_mul_pd(_t14_215, _t4_316), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_215, _t4_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_215, _t4_316), _mm256_mul_pd(_t14_215, _t4_316), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_215, _t4_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_215, _t4_316), _mm256_mul_pd(_t14_215, _t4_316), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_217 = _mm256_sub_pd(_t14_214, _t4_317);

    // AVX Storer:
    _t14_49 = _t14_217;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_218 = _t14_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_219 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_220 = _t0_75;

    // 4-BLAC: 1x4 + 1x4
    _t4_322 = _mm256_add_pd(_t14_219, _t14_220);

    // 4-BLAC: 1x4 / 1x4
    _t14_221 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_218), _mm256_castpd256_pd128(_t4_322)));

    // AVX Storer:
    _t14_49 = _t14_221;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_222 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t14_63, _t14_63, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_223 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_47, _t14_48), _mm256_unpacklo_pd(_t14_49, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_224 = _t0_66;

    // 4-BLAC: (1x4)^T
    _t4_327 = _t14_224;

    // 4-BLAC: 1x4 * 4x1
    _t4_328 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_223, _t4_327), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_223, _t4_327), _mm256_mul_pd(_t14_223, _t4_327), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_223, _t4_327), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_223, _t4_327), _mm256_mul_pd(_t14_223, _t4_327), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_223, _t4_327), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_223, _t4_327), _mm256_mul_pd(_t14_223, _t4_327), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_225 = _mm256_sub_pd(_t14_222, _t4_328);

    // AVX Storer:
    _t14_50 = _t14_225;

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_226 = _t14_50;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_227 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_228 = _t0_73;

    // 4-BLAC: 1x4 + 1x4
    _t4_333 = _mm256_add_pd(_t14_227, _t14_228);

    // 4-BLAC: 1x4 / 1x4
    _t14_229 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_226), _mm256_castpd256_pd128(_t4_333)));

    // AVX Storer:
    _t14_50 = _t14_229;

    // Generating : X[52,52] = S(h(3, 52, fi1407 + 1), ( G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, 4)) - ( G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) * G(h(1, 52, fi1407), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t14_230 = _t14_64;
    _t14_231 = _t14_65;
    _t14_232 = _t14_66;
    _t14_233 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_234 = _t14_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t4_340 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_234, _t14_234, 32), _mm256_permute2f128_pd(_t14_234, _t14_234, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_47, _t14_48), _mm256_unpacklo_pd(_t14_49, _t14_50), 32));
    _t4_341 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_234, _t14_234, 32), _mm256_permute2f128_pd(_t14_234, _t14_234, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_47, _t14_48), _mm256_unpacklo_pd(_t14_49, _t14_50), 32));
    _t4_342 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_234, _t14_234, 49), _mm256_permute2f128_pd(_t14_234, _t14_234, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_47, _t14_48), _mm256_unpacklo_pd(_t14_49, _t14_50), 32));
    _t4_343 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_234, _t14_234, 49), _mm256_permute2f128_pd(_t14_234, _t14_234, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_47, _t14_48), _mm256_unpacklo_pd(_t14_49, _t14_50), 32));

    // 4-BLAC: 4x4 - 4x4
    _t14_235 = _mm256_sub_pd(_t14_230, _t4_340);
    _t14_236 = _mm256_sub_pd(_t14_231, _t4_341);
    _t14_237 = _mm256_sub_pd(_t14_232, _t4_342);
    _t14_238 = _mm256_sub_pd(_t14_233, _t4_343);

    // AVX Storer:
    _t14_64 = _t14_235;
    _t14_65 = _t14_236;
    _t14_66 = _t14_237;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_239 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_64, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_240 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_241 = _t0_79;

    // 4-BLAC: 1x4 + 1x4
    _t4_351 = _mm256_add_pd(_t14_240, _t14_241);

    // 4-BLAC: 1x4 / 1x4
    _t14_242 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_239), _mm256_castpd256_pd128(_t4_351)));

    // AVX Storer:
    _t14_51 = _t14_242;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_243 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_64, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_244 = _t14_51;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_245 = _t0_68;

    // 4-BLAC: (4x1)^T
    _t4_356 = _t14_245;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_357 = _mm256_mul_pd(_t14_244, _t4_356);

    // 4-BLAC: 1x4 - 1x4
    _t14_246 = _mm256_sub_pd(_t14_243, _t4_357);

    // AVX Storer:
    _t14_52 = _t14_246;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_247 = _t14_52;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_248 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_249 = _t0_77;

    // 4-BLAC: 1x4 + 1x4
    _t4_362 = _mm256_add_pd(_t14_248, _t14_249);

    // 4-BLAC: 1x4 / 1x4
    _t14_250 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_247), _mm256_castpd256_pd128(_t4_362)));

    // AVX Storer:
    _t14_52 = _t14_250;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_251 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_64, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t14_64, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_252 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_51, _t14_52), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_253 = _t0_67;

    // 4-BLAC: (1x4)^T
    _t4_367 = _t14_253;

    // 4-BLAC: 1x4 * 4x1
    _t4_368 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_252, _t4_367), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_252, _t4_367), _mm256_mul_pd(_t14_252, _t4_367), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_252, _t4_367), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_252, _t4_367), _mm256_mul_pd(_t14_252, _t4_367), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_252, _t4_367), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_252, _t4_367), _mm256_mul_pd(_t14_252, _t4_367), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_254 = _mm256_sub_pd(_t14_251, _t4_368);

    // AVX Storer:
    _t14_53 = _t14_254;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_255 = _t14_53;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_256 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_257 = _t0_75;

    // 4-BLAC: 1x4 + 1x4
    _t4_373 = _mm256_add_pd(_t14_256, _t14_257);

    // 4-BLAC: 1x4 / 1x4
    _t14_258 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_255), _mm256_castpd256_pd128(_t4_373)));

    // AVX Storer:
    _t14_53 = _t14_258;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_259 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t14_64, _t14_64, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_260 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_51, _t14_52), _mm256_unpacklo_pd(_t14_53, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_261 = _t0_66;

    // 4-BLAC: (1x4)^T
    _t4_378 = _t14_261;

    // 4-BLAC: 1x4 * 4x1
    _t4_379 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_260, _t4_378), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_260, _t4_378), _mm256_mul_pd(_t14_260, _t4_378), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_260, _t4_378), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_260, _t4_378), _mm256_mul_pd(_t14_260, _t4_378), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_260, _t4_378), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_260, _t4_378), _mm256_mul_pd(_t14_260, _t4_378), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_262 = _mm256_sub_pd(_t14_259, _t4_379);

    // AVX Storer:
    _t14_54 = _t14_262;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_263 = _t14_54;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_264 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_265 = _t0_73;

    // 4-BLAC: 1x4 + 1x4
    _t4_384 = _mm256_add_pd(_t14_264, _t14_265);

    // 4-BLAC: 1x4 / 1x4
    _t14_266 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_263), _mm256_castpd256_pd128(_t4_384)));

    // AVX Storer:
    _t14_54 = _t14_266;

    // Generating : X[52,52] = S(h(2, 52, fi1407 + 2), ( G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, 4)) - ( G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) * G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t14_267 = _t14_65;
    _t14_268 = _t14_66;
    _t14_269 = _mm256_setzero_pd();
    _t14_270 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_271 = _t14_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t4_391 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_271, _t14_271, 32), _mm256_permute2f128_pd(_t14_271, _t14_271, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_51, _t14_52), _mm256_unpacklo_pd(_t14_53, _t14_54), 32));
    _t4_392 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_271, _t14_271, 32), _mm256_permute2f128_pd(_t14_271, _t14_271, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_51, _t14_52), _mm256_unpacklo_pd(_t14_53, _t14_54), 32));
    _t4_393 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_271, _t14_271, 49), _mm256_permute2f128_pd(_t14_271, _t14_271, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_51, _t14_52), _mm256_unpacklo_pd(_t14_53, _t14_54), 32));
    _t4_394 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_271, _t14_271, 49), _mm256_permute2f128_pd(_t14_271, _t14_271, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_51, _t14_52), _mm256_unpacklo_pd(_t14_53, _t14_54), 32));

    // 4-BLAC: 4x4 - 4x4
    _t14_272 = _mm256_sub_pd(_t14_267, _t4_391);
    _t14_273 = _mm256_sub_pd(_t14_268, _t4_392);
    _t14_274 = _mm256_sub_pd(_t14_269, _t4_393);
    _t14_275 = _mm256_sub_pd(_t14_270, _t4_394);

    // AVX Storer:
    _t14_65 = _t14_272;
    _t14_66 = _t14_273;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_276 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_65, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_277 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_278 = _t0_79;

    // 4-BLAC: 1x4 + 1x4
    _t4_402 = _mm256_add_pd(_t14_277, _t14_278);

    // 4-BLAC: 1x4 / 1x4
    _t14_279 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_276), _mm256_castpd256_pd128(_t4_402)));

    // AVX Storer:
    _t14_55 = _t14_279;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_280 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_65, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_281 = _t14_55;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_282 = _t0_68;

    // 4-BLAC: (4x1)^T
    _t4_407 = _t14_282;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_408 = _mm256_mul_pd(_t14_281, _t4_407);

    // 4-BLAC: 1x4 - 1x4
    _t14_283 = _mm256_sub_pd(_t14_280, _t4_408);

    // AVX Storer:
    _t14_56 = _t14_283;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_284 = _t14_56;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_285 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_286 = _t0_77;

    // 4-BLAC: 1x4 + 1x4
    _t4_413 = _mm256_add_pd(_t14_285, _t14_286);

    // 4-BLAC: 1x4 / 1x4
    _t14_287 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_284), _mm256_castpd256_pd128(_t4_413)));

    // AVX Storer:
    _t14_56 = _t14_287;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_288 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_65, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t14_65, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_289 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_55, _t14_56), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_290 = _t0_67;

    // 4-BLAC: (1x4)^T
    _t4_418 = _t14_290;

    // 4-BLAC: 1x4 * 4x1
    _t4_419 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_289, _t4_418), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_289, _t4_418), _mm256_mul_pd(_t14_289, _t4_418), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_289, _t4_418), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_289, _t4_418), _mm256_mul_pd(_t14_289, _t4_418), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_289, _t4_418), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_289, _t4_418), _mm256_mul_pd(_t14_289, _t4_418), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_291 = _mm256_sub_pd(_t14_288, _t4_419);

    // AVX Storer:
    _t14_57 = _t14_291;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_292 = _t14_57;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_293 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_294 = _t0_75;

    // 4-BLAC: 1x4 + 1x4
    _t4_424 = _mm256_add_pd(_t14_293, _t14_294);

    // 4-BLAC: 1x4 / 1x4
    _t14_295 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_292), _mm256_castpd256_pd128(_t4_424)));

    // AVX Storer:
    _t14_57 = _t14_295;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_296 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t14_65, _t14_65, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_297 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_55, _t14_56), _mm256_unpacklo_pd(_t14_57, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_298 = _t0_66;

    // 4-BLAC: (1x4)^T
    _t4_429 = _t14_298;

    // 4-BLAC: 1x4 * 4x1
    _t4_430 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_297, _t4_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_297, _t4_429), _mm256_mul_pd(_t14_297, _t4_429), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_297, _t4_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_297, _t4_429), _mm256_mul_pd(_t14_297, _t4_429), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_297, _t4_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_297, _t4_429), _mm256_mul_pd(_t14_297, _t4_429), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_299 = _mm256_sub_pd(_t14_296, _t4_430);

    // AVX Storer:
    _t14_58 = _t14_299;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_300 = _t14_58;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_301 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_302 = _t0_73;

    // 4-BLAC: 1x4 + 1x4
    _t4_435 = _mm256_add_pd(_t14_301, _t14_302);

    // 4-BLAC: 1x4 / 1x4
    _t14_303 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_300), _mm256_castpd256_pd128(_t4_435)));

    // AVX Storer:
    _t14_58 = _t14_303;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) Kro G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_304 = _t14_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t4_84 = _mm256_mul_pd(_t14_304, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_55, _t14_56), _mm256_unpacklo_pd(_t14_57, _t14_58), 32));

    // 4-BLAC: 1x4 - 1x4
    _t14_66 = _mm256_sub_pd(_t14_66, _t4_84);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 4), L[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_305 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_66, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_306 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_307 = _t0_79;

    // 4-BLAC: 1x4 + 1x4
    _t4_441 = _mm256_add_pd(_t14_306, _t14_307);

    // 4-BLAC: 1x4 / 1x4
    _t14_308 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_305), _mm256_castpd256_pd128(_t4_441)));

    // AVX Storer:
    _t14_59 = _t14_308;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 4)) Kro T( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) ) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_309 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_66, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_310 = _t14_59;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_311 = _t0_68;

    // 4-BLAC: (4x1)^T
    _t4_446 = _t14_311;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_447 = _mm256_mul_pd(_t14_310, _t4_446);

    // 4-BLAC: 1x4 - 1x4
    _t14_312 = _mm256_sub_pd(_t14_309, _t4_447);

    // AVX Storer:
    _t14_60 = _t14_312;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 5), L[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_313 = _t14_60;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_314 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_315 = _t0_77;

    // 4-BLAC: 1x4 + 1x4
    _t4_452 = _mm256_add_pd(_t14_314, _t14_315);

    // 4-BLAC: 1x4 / 1x4
    _t14_316 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_313), _mm256_castpd256_pd128(_t4_452)));

    // AVX Storer:
    _t14_60 = _t14_316;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, 4)) * T( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) ) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_317 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_66, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t14_66, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_318 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_59, _t14_60), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_319 = _t0_67;

    // 4-BLAC: (1x4)^T
    _t4_457 = _t14_319;

    // 4-BLAC: 1x4 * 4x1
    _t4_458 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_318, _t4_457), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_318, _t4_457), _mm256_mul_pd(_t14_318, _t4_457), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_318, _t4_457), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_318, _t4_457), _mm256_mul_pd(_t14_318, _t4_457), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_318, _t4_457), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_318, _t4_457), _mm256_mul_pd(_t14_318, _t4_457), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_320 = _mm256_sub_pd(_t14_317, _t4_458);

    // AVX Storer:
    _t14_61 = _t14_320;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 6), L[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_321 = _t14_61;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_322 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_323 = _t0_75;

    // 4-BLAC: 1x4 + 1x4
    _t4_463 = _mm256_add_pd(_t14_322, _t14_323);

    // 4-BLAC: 1x4 / 1x4
    _t14_324 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_321), _mm256_castpd256_pd128(_t4_463)));

    // AVX Storer:
    _t14_61 = _t14_324;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, 4)) * T( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) ) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_325 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t14_66, _t14_66, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_326 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_59, _t14_60), _mm256_unpacklo_pd(_t14_61, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_327 = _t0_66;

    // 4-BLAC: (1x4)^T
    _t4_468 = _t14_327;

    // 4-BLAC: 1x4 * 4x1
    _t4_469 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_326, _t4_468), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_326, _t4_468), _mm256_mul_pd(_t14_326, _t4_468), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_326, _t4_468), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_326, _t4_468), _mm256_mul_pd(_t14_326, _t4_468), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_326, _t4_468), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_326, _t4_468), _mm256_mul_pd(_t14_326, _t4_468), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_328 = _mm256_sub_pd(_t14_325, _t4_469);

    // AVX Storer:
    _t14_62 = _t14_328;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, 7), L[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_329 = _t14_62;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_330 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_331 = _t0_73;

    // 4-BLAC: 1x4 + 1x4
    _t4_474 = _mm256_add_pd(_t14_330, _t14_331);

    // 4-BLAC: 1x4 / 1x4
    _t14_332 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_329), _mm256_castpd256_pd128(_t4_474)));

    // AVX Storer:
    _t14_62 = _t14_332;
    _asm256_storeu_pd(C + 52*fi1407 + 52, _t8_17);
    _asm256_storeu_pd(C + 52*fi1407 + 104, _t8_18);
    _asm256_storeu_pd(C + 52*fi1407 + 156, _t8_19);
    _asm256_storeu_pd(C + 53*fi1407 - 4, _t14_27);
    _asm256_storeu_pd(C + 53*fi1407 + 48, _t14_28);
    _asm256_storeu_pd(C + 53*fi1407 + 100, _t14_29);
    _asm256_storeu_pd(C + 53*fi1407 + 152, _t14_30);
    _mm_store_sd(&(C[52*fi1407 + 4]), _mm256_castpd256_pd128(_t14_47));
    _mm_store_sd(&(C[52*fi1407 + 5]), _mm256_castpd256_pd128(_t14_48));
    _mm_store_sd(&(C[52*fi1407 + 6]), _mm256_castpd256_pd128(_t14_49));
    _mm_store_sd(&(C[52*fi1407 + 7]), _mm256_castpd256_pd128(_t14_50));
    _mm_store_sd(&(C[52*fi1407 + 56]), _mm256_castpd256_pd128(_t14_51));
    _mm_store_sd(&(C[52*fi1407 + 57]), _mm256_castpd256_pd128(_t14_52));
    _mm_store_sd(&(C[52*fi1407 + 58]), _mm256_castpd256_pd128(_t14_53));
    _mm_store_sd(&(C[52*fi1407 + 59]), _mm256_castpd256_pd128(_t14_54));
    _mm_store_sd(&(C[52*fi1407 + 108]), _mm256_castpd256_pd128(_t14_55));
    _mm_store_sd(&(C[52*fi1407 + 109]), _mm256_castpd256_pd128(_t14_56));
    _mm_store_sd(&(C[52*fi1407 + 110]), _mm256_castpd256_pd128(_t14_57));
    _mm_store_sd(&(C[52*fi1407 + 111]), _mm256_castpd256_pd128(_t14_58));
    _mm_store_sd(&(C[52*fi1407 + 160]), _mm256_castpd256_pd128(_t14_59));
    _mm_store_sd(&(C[52*fi1407 + 161]), _mm256_castpd256_pd128(_t14_60));
    _mm_store_sd(&(C[52*fi1407 + 162]), _mm256_castpd256_pd128(_t14_61));
    _mm_store_sd(&(C[52*fi1407 + 163]), _mm256_castpd256_pd128(_t14_62));

    for( int fi1526 = 8; fi1526 <= fi1407 - 4; fi1526+=4 ) {
      _t15_4 = _asm256_loadu_pd(C + 52*fi1407 + fi1526);
      _t15_5 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 52);
      _t15_6 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 104);
      _t15_7 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 156);
      _t15_3 = _asm256_loadu_pd(L + 52*fi1526);
      _t15_2 = _asm256_loadu_pd(L + 52*fi1526 + 52);
      _t15_1 = _asm256_loadu_pd(L + 52*fi1526 + 104);
      _t15_0 = _asm256_loadu_pd(L + 52*fi1526 + 156);

      // Generating : X[52,52] = ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, fi1526)) - ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, fi1526), L[52,52],h(4, 52, 0)) ) ) ),h(4, 52, fi1526)) + Sum_{k85} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, k85)) * T( G(h(4, 52, fi1526), L[52,52],h(4, 52, k85)) ) ),h(4, 52, fi1526)) ) )

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t4_663 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 32);
      _t4_664 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 32);
      _t4_665 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 49);
      _t4_666 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t4_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_31, _t14_31, 32), _mm256_permute2f128_pd(_t14_31, _t14_31, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_32, _t14_32, 32), _mm256_permute2f128_pd(_t14_32, _t14_32, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_33, _t14_33, 32), _mm256_permute2f128_pd(_t14_33, _t14_33, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_34, _t14_34, 32), _mm256_permute2f128_pd(_t14_34, _t14_34, 32), 0), _t4_666)));
      _t4_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_35, _t14_35, 32), _mm256_permute2f128_pd(_t14_35, _t14_35, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_36, _t14_36, 32), _mm256_permute2f128_pd(_t14_36, _t14_36, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_37, _t14_37, 32), _mm256_permute2f128_pd(_t14_37, _t14_37, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_38, _t14_38, 32), _mm256_permute2f128_pd(_t14_38, _t14_38, 32), 0), _t4_666)));
      _t4_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_39, _t14_39, 32), _mm256_permute2f128_pd(_t14_39, _t14_39, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_40, _t14_40, 32), _mm256_permute2f128_pd(_t14_40, _t14_40, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_41, _t14_41, 32), _mm256_permute2f128_pd(_t14_41, _t14_41, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_42, _t14_42, 32), _mm256_permute2f128_pd(_t14_42, _t14_42, 32), 0), _t4_666)));
      _t4_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_43, _t14_43, 32), _mm256_permute2f128_pd(_t14_43, _t14_43, 32), 0), _t4_663), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_44, _t14_44, 32), _mm256_permute2f128_pd(_t14_44, _t14_44, 32), 0), _t4_664)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_45, _t14_45, 32), _mm256_permute2f128_pd(_t14_45, _t14_45, 32), 0), _t4_665), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_46, _t14_46, 32), _mm256_permute2f128_pd(_t14_46, _t14_46, 32), 0), _t4_666)));

      // 4-BLAC: 4x4 - 4x4
      _t15_4 = _mm256_sub_pd(_t15_4, _t4_94);
      _t15_5 = _mm256_sub_pd(_t15_5, _t4_95);
      _t15_6 = _mm256_sub_pd(_t15_6, _t4_96);
      _t15_7 = _mm256_sub_pd(_t15_7, _t4_97);

      // AVX Storer:
      _asm256_storeu_pd(C + 52*fi1407 + fi1526, _t15_4);
      _asm256_storeu_pd(C + 52*fi1407 + fi1526 + 52, _t15_5);
      _asm256_storeu_pd(C + 52*fi1407 + fi1526 + 104, _t15_6);
      _asm256_storeu_pd(C + 52*fi1407 + fi1526 + 156, _t15_7);

      for( int k85 = 4; k85 <= fi1526 - 1; k85+=4 ) {
        _t16_19 = _mm256_broadcast_sd(C + 52*fi1407 + k85);
        _t16_18 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 1);
        _t16_17 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 2);
        _t16_16 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 3);
        _t16_15 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 52);
        _t16_14 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 53);
        _t16_13 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 54);
        _t16_12 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 55);
        _t16_11 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 104);
        _t16_10 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 105);
        _t16_9 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 106);
        _t16_8 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 107);
        _t16_7 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 156);
        _t16_6 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 157);
        _t16_5 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 158);
        _t16_4 = _mm256_broadcast_sd(C + 52*fi1407 + k85 + 159);
        _t16_3 = _asm256_loadu_pd(L + 52*fi1526 + k85);
        _t16_2 = _asm256_loadu_pd(L + 52*fi1526 + k85 + 52);
        _t16_1 = _asm256_loadu_pd(L + 52*fi1526 + k85 + 104);
        _t16_0 = _asm256_loadu_pd(L + 52*fi1526 + k85 + 156);
        _t16_20 = _asm256_loadu_pd(C + 52*fi1407 + fi1526);
        _t16_21 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 52);
        _t16_22 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 104);
        _t16_23 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t4_667 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
        _t4_668 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
        _t4_669 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
        _t4_670 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t4_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t4_667), _mm256_mul_pd(_t16_18, _t4_668)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t4_669), _mm256_mul_pd(_t16_16, _t4_670)));
        _t4_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t4_667), _mm256_mul_pd(_t16_14, _t4_668)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t4_669), _mm256_mul_pd(_t16_12, _t4_670)));
        _t4_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t4_667), _mm256_mul_pd(_t16_10, _t4_668)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t4_669), _mm256_mul_pd(_t16_8, _t4_670)));
        _t4_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t4_667), _mm256_mul_pd(_t16_6, _t4_668)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t4_669), _mm256_mul_pd(_t16_4, _t4_670)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t16_20 = _mm256_sub_pd(_t16_20, _t4_98);
        _t16_21 = _mm256_sub_pd(_t16_21, _t4_99);
        _t16_22 = _mm256_sub_pd(_t16_22, _t4_100);
        _t16_23 = _mm256_sub_pd(_t16_23, _t4_101);

        // AVX Storer:
        _asm256_storeu_pd(C + 52*fi1407 + fi1526, _t16_20);
        _asm256_storeu_pd(C + 52*fi1407 + fi1526 + 52, _t16_21);
        _asm256_storeu_pd(C + 52*fi1407 + fi1526 + 104, _t16_22);
        _asm256_storeu_pd(C + 52*fi1407 + fi1526 + 156, _t16_23);
      }
      _t15_7 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 156);
      _t15_4 = _asm256_loadu_pd(C + 52*fi1407 + fi1526);
      _t15_5 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 52);
      _t15_6 = _asm256_loadu_pd(C + 52*fi1407 + fi1526 + 104);
      _t17_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1526])));
      _t17_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1526 + 52])));
      _t17_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1526 + 53])));
      _t17_3 = _mm256_maskload_pd(L + 53*fi1526 + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t17_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1526 + 106])));
      _t17_1 = _mm256_maskload_pd(L + 53*fi1526 + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t17_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1526 + 159])));

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t15_4, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_24 = _t14_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_25 = _t17_6;

      // 4-BLAC: 1x4 + 1x4
      _t4_479 = _mm256_add_pd(_t17_24, _t17_25);

      // 4-BLAC: 1x4 / 1x4
      _t17_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_23), _mm256_castpd256_pd128(_t4_479)));

      // AVX Storer:
      _t17_7 = _t17_26;

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_4, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_28 = _t17_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_29 = _t17_5;

      // 4-BLAC: (4x1)^T
      _t4_484 = _t17_29;

      // 4-BLAC: 1x4 Kro 1x4
      _t4_485 = _mm256_mul_pd(_t17_28, _t4_484);

      // 4-BLAC: 1x4 - 1x4
      _t17_30 = _mm256_sub_pd(_t17_27, _t4_485);

      // AVX Storer:
      _t17_8 = _t17_30;

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_31 = _t17_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_32 = _t14_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_33 = _t17_4;

      // 4-BLAC: 1x4 + 1x4
      _t4_490 = _mm256_add_pd(_t17_32, _t17_33);

      // 4-BLAC: 1x4 / 1x4
      _t17_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_31), _mm256_castpd256_pd128(_t4_490)));

      // AVX Storer:
      _t17_8 = _t17_34;

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t15_4, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_37 = _t17_3;

      // 4-BLAC: (1x4)^T
      _t4_495 = _t17_37;

      // 4-BLAC: 1x4 * 4x1
      _t4_496 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_36, _t4_495), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_36, _t4_495), _mm256_mul_pd(_t17_36, _t4_495), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_36, _t4_495), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_36, _t4_495), _mm256_mul_pd(_t17_36, _t4_495), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_36, _t4_495), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_36, _t4_495), _mm256_mul_pd(_t17_36, _t4_495), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_38 = _mm256_sub_pd(_t17_35, _t4_496);

      // AVX Storer:
      _t17_9 = _t17_38;

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_39 = _t17_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_40 = _t14_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_41 = _t17_2;

      // 4-BLAC: 1x4 + 1x4
      _t4_501 = _mm256_add_pd(_t17_40, _t17_41);

      // 4-BLAC: 1x4 / 1x4
      _t17_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_39), _mm256_castpd256_pd128(_t4_501)));

      // AVX Storer:
      _t17_9 = _t17_42;

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t15_4, _t15_4, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_45 = _t17_1;

      // 4-BLAC: (1x4)^T
      _t4_506 = _t17_45;

      // 4-BLAC: 1x4 * 4x1
      _t4_507 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_44, _t4_506), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_44, _t4_506), _mm256_mul_pd(_t17_44, _t4_506), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_44, _t4_506), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_44, _t4_506), _mm256_mul_pd(_t17_44, _t4_506), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_44, _t4_506), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_44, _t4_506), _mm256_mul_pd(_t17_44, _t4_506), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_46 = _mm256_sub_pd(_t17_43, _t4_507);

      // AVX Storer:
      _t17_10 = _t17_46;

      // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_47 = _t17_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_48 = _t14_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_49 = _t17_0;

      // 4-BLAC: 1x4 + 1x4
      _t4_512 = _mm256_add_pd(_t17_48, _t17_49);

      // 4-BLAC: 1x4 / 1x4
      _t17_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_47), _mm256_castpd256_pd128(_t4_512)));

      // AVX Storer:
      _t17_10 = _t17_50;

      // Generating : X[52,52] = S(h(3, 52, fi1407 + 1), ( G(h(3, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) - ( G(h(3, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) * G(h(1, 52, fi1407), X[52,52],h(4, 52, fi1526)) ) ),h(4, 52, fi1526))

      // AVX Loader:

      // 3x4 -> 4x4
      _t17_51 = _t15_5;
      _t17_52 = _t15_6;
      _t17_53 = _t15_7;
      _t17_54 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t17_55 = _t14_5;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t4_519 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_55, _t17_55, 32), _mm256_permute2f128_pd(_t17_55, _t17_55, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32));
      _t4_520 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_55, _t17_55, 32), _mm256_permute2f128_pd(_t17_55, _t17_55, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32));
      _t4_521 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_55, _t17_55, 49), _mm256_permute2f128_pd(_t17_55, _t17_55, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32));
      _t4_522 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_55, _t17_55, 49), _mm256_permute2f128_pd(_t17_55, _t17_55, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32));

      // 4-BLAC: 4x4 - 4x4
      _t17_56 = _mm256_sub_pd(_t17_51, _t4_519);
      _t17_57 = _mm256_sub_pd(_t17_52, _t4_520);
      _t17_58 = _mm256_sub_pd(_t17_53, _t4_521);
      _t17_59 = _mm256_sub_pd(_t17_54, _t4_522);

      // AVX Storer:
      _t15_5 = _t17_56;
      _t15_6 = _t17_57;
      _t15_7 = _t17_58;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_60 = _mm256_blend_pd(_mm256_setzero_pd(), _t15_5, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_61 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_62 = _t17_6;

      // 4-BLAC: 1x4 + 1x4
      _t4_530 = _mm256_add_pd(_t17_61, _t17_62);

      // 4-BLAC: 1x4 / 1x4
      _t17_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_60), _mm256_castpd256_pd128(_t4_530)));

      // AVX Storer:
      _t17_11 = _t17_63;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_64 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_5, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_65 = _t17_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_66 = _t17_5;

      // 4-BLAC: (4x1)^T
      _t4_535 = _t17_66;

      // 4-BLAC: 1x4 Kro 1x4
      _t4_536 = _mm256_mul_pd(_t17_65, _t4_535);

      // 4-BLAC: 1x4 - 1x4
      _t17_67 = _mm256_sub_pd(_t17_64, _t4_536);

      // AVX Storer:
      _t17_12 = _t17_67;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_68 = _t17_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_69 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_70 = _t17_4;

      // 4-BLAC: 1x4 + 1x4
      _t4_541 = _mm256_add_pd(_t17_69, _t17_70);

      // 4-BLAC: 1x4 / 1x4
      _t17_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_68), _mm256_castpd256_pd128(_t4_541)));

      // AVX Storer:
      _t17_12 = _t17_71;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_72 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t15_5, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_73 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_74 = _t17_3;

      // 4-BLAC: (1x4)^T
      _t4_546 = _t17_74;

      // 4-BLAC: 1x4 * 4x1
      _t4_547 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_73, _t4_546), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_73, _t4_546), _mm256_mul_pd(_t17_73, _t4_546), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_73, _t4_546), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_73, _t4_546), _mm256_mul_pd(_t17_73, _t4_546), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_73, _t4_546), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_73, _t4_546), _mm256_mul_pd(_t17_73, _t4_546), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_75 = _mm256_sub_pd(_t17_72, _t4_547);

      // AVX Storer:
      _t17_13 = _t17_75;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_76 = _t17_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_77 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_78 = _t17_2;

      // 4-BLAC: 1x4 + 1x4
      _t4_552 = _mm256_add_pd(_t17_77, _t17_78);

      // 4-BLAC: 1x4 / 1x4
      _t17_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_76), _mm256_castpd256_pd128(_t4_552)));

      // AVX Storer:
      _t17_13 = _t17_79;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407 + 1), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_80 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t15_5, _t15_5, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_82 = _t17_1;

      // 4-BLAC: (1x4)^T
      _t4_557 = _t17_82;

      // 4-BLAC: 1x4 * 4x1
      _t4_558 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_81, _t4_557), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_81, _t4_557), _mm256_mul_pd(_t17_81, _t4_557), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_81, _t4_557), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_81, _t4_557), _mm256_mul_pd(_t17_81, _t4_557), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_81, _t4_557), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_81, _t4_557), _mm256_mul_pd(_t17_81, _t4_557), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_83 = _mm256_sub_pd(_t17_80, _t4_558);

      // AVX Storer:
      _t17_14 = _t17_83;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_84 = _t17_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_85 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_86 = _t17_0;

      // 4-BLAC: 1x4 + 1x4
      _t4_563 = _mm256_add_pd(_t17_85, _t17_86);

      // 4-BLAC: 1x4 / 1x4
      _t17_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_84), _mm256_castpd256_pd128(_t4_563)));

      // AVX Storer:
      _t17_14 = _t17_87;

      // Generating : X[52,52] = S(h(2, 52, fi1407 + 2), ( G(h(2, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) - ( G(h(2, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 1)) * G(h(1, 52, fi1407 + 1), X[52,52],h(4, 52, fi1526)) ) ),h(4, 52, fi1526))

      // AVX Loader:

      // 2x4 -> 4x4
      _t17_88 = _t15_6;
      _t17_89 = _t15_7;
      _t17_90 = _mm256_setzero_pd();
      _t17_91 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t17_92 = _t14_3;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t4_570 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_92, _t17_92, 32), _mm256_permute2f128_pd(_t17_92, _t17_92, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _t17_14), 32));
      _t4_571 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_92, _t17_92, 32), _mm256_permute2f128_pd(_t17_92, _t17_92, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _t17_14), 32));
      _t4_572 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_92, _t17_92, 49), _mm256_permute2f128_pd(_t17_92, _t17_92, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _t17_14), 32));
      _t4_573 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_92, _t17_92, 49), _mm256_permute2f128_pd(_t17_92, _t17_92, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _t17_14), 32));

      // 4-BLAC: 4x4 - 4x4
      _t17_93 = _mm256_sub_pd(_t17_88, _t4_570);
      _t17_94 = _mm256_sub_pd(_t17_89, _t4_571);
      _t17_95 = _mm256_sub_pd(_t17_90, _t4_572);
      _t17_96 = _mm256_sub_pd(_t17_91, _t4_573);

      // AVX Storer:
      _t15_6 = _t17_93;
      _t15_7 = _t17_94;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_97 = _mm256_blend_pd(_mm256_setzero_pd(), _t15_6, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_98 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_99 = _t17_6;

      // 4-BLAC: 1x4 + 1x4
      _t4_581 = _mm256_add_pd(_t17_98, _t17_99);

      // 4-BLAC: 1x4 / 1x4
      _t17_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_97), _mm256_castpd256_pd128(_t4_581)));

      // AVX Storer:
      _t17_15 = _t17_100;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_101 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_6, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_102 = _t17_15;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_103 = _t17_5;

      // 4-BLAC: (4x1)^T
      _t4_586 = _t17_103;

      // 4-BLAC: 1x4 Kro 1x4
      _t4_587 = _mm256_mul_pd(_t17_102, _t4_586);

      // 4-BLAC: 1x4 - 1x4
      _t17_104 = _mm256_sub_pd(_t17_101, _t4_587);

      // AVX Storer:
      _t17_16 = _t17_104;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_105 = _t17_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_106 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_107 = _t17_4;

      // 4-BLAC: 1x4 + 1x4
      _t4_592 = _mm256_add_pd(_t17_106, _t17_107);

      // 4-BLAC: 1x4 / 1x4
      _t17_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_105), _mm256_castpd256_pd128(_t4_592)));

      // AVX Storer:
      _t17_16 = _t17_108;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_109 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t15_6, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_110 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_15, _t17_16), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_111 = _t17_3;

      // 4-BLAC: (1x4)^T
      _t4_597 = _t17_111;

      // 4-BLAC: 1x4 * 4x1
      _t4_598 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_110, _t4_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_110, _t4_597), _mm256_mul_pd(_t17_110, _t4_597), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_110, _t4_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_110, _t4_597), _mm256_mul_pd(_t17_110, _t4_597), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_110, _t4_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_110, _t4_597), _mm256_mul_pd(_t17_110, _t4_597), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_112 = _mm256_sub_pd(_t17_109, _t4_598);

      // AVX Storer:
      _t17_17 = _t17_112;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_113 = _t17_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_114 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_115 = _t17_2;

      // 4-BLAC: 1x4 + 1x4
      _t4_603 = _mm256_add_pd(_t17_114, _t17_115);

      // 4-BLAC: 1x4 / 1x4
      _t17_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_113), _mm256_castpd256_pd128(_t4_603)));

      // AVX Storer:
      _t17_17 = _t17_116;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_117 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t15_6, _t15_6, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_118 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_15, _t17_16), _mm256_unpacklo_pd(_t17_17, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_119 = _t17_1;

      // 4-BLAC: (1x4)^T
      _t4_608 = _t17_119;

      // 4-BLAC: 1x4 * 4x1
      _t4_609 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_118, _t4_608), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_118, _t4_608), _mm256_mul_pd(_t17_118, _t4_608), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_118, _t4_608), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_118, _t4_608), _mm256_mul_pd(_t17_118, _t4_608), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_118, _t4_608), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_118, _t4_608), _mm256_mul_pd(_t17_118, _t4_608), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_120 = _mm256_sub_pd(_t17_117, _t4_609);

      // AVX Storer:
      _t17_18 = _t17_120;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_121 = _t17_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_122 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_123 = _t17_0;

      // 4-BLAC: 1x4 + 1x4
      _t4_614 = _mm256_add_pd(_t17_122, _t17_123);

      // 4-BLAC: 1x4 / 1x4
      _t17_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_121), _mm256_castpd256_pd128(_t4_614)));

      // AVX Storer:
      _t17_18 = _t17_124;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(4, 52, fi1526)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 2)) Kro G(h(1, 52, fi1407 + 2), X[52,52],h(4, 52, fi1526)) ) ),h(4, 52, fi1526))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_125 = _t14_1;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t4_85 = _mm256_mul_pd(_t17_125, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_15, _t17_16), _mm256_unpacklo_pd(_t17_17, _t17_18), 32));

      // 4-BLAC: 1x4 - 1x4
      _t15_7 = _mm256_sub_pd(_t15_7, _t4_85);

      // AVX Storer:

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526), L[52,52],h(1, 52, fi1526)) ) ),h(1, 52, fi1526))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t15_7, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_127 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_128 = _t17_6;

      // 4-BLAC: 1x4 + 1x4
      _t4_620 = _mm256_add_pd(_t17_127, _t17_128);

      // 4-BLAC: 1x4 / 1x4
      _t17_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_126), _mm256_castpd256_pd128(_t4_620)));

      // AVX Storer:
      _t17_19 = _t17_129;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526)) Kro T( G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_130 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_7, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_131 = _t17_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_132 = _t17_5;

      // 4-BLAC: (4x1)^T
      _t4_625 = _t17_132;

      // 4-BLAC: 1x4 Kro 1x4
      _t4_626 = _mm256_mul_pd(_t17_131, _t4_625);

      // 4-BLAC: 1x4 - 1x4
      _t17_133 = _mm256_sub_pd(_t17_130, _t4_626);

      // AVX Storer:
      _t17_20 = _t17_133;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 1)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526 + 1), L[52,52],h(1, 52, fi1526 + 1)) ) ),h(1, 52, fi1526 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_134 = _t17_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_135 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_136 = _t17_4;

      // 4-BLAC: 1x4 + 1x4
      _t4_631 = _mm256_add_pd(_t17_135, _t17_136);

      // 4-BLAC: 1x4 / 1x4
      _t17_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_134), _mm256_castpd256_pd128(_t4_631)));

      // AVX Storer:
      _t17_20 = _t17_137;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, fi1526)) * T( G(h(1, 52, fi1526 + 2), L[52,52],h(2, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t15_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t15_7, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_19, _t17_20), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_140 = _t17_3;

      // 4-BLAC: (1x4)^T
      _t4_636 = _t17_140;

      // 4-BLAC: 1x4 * 4x1
      _t4_637 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_139, _t4_636), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_139, _t4_636), _mm256_mul_pd(_t17_139, _t4_636), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_139, _t4_636), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_139, _t4_636), _mm256_mul_pd(_t17_139, _t4_636), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_139, _t4_636), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_139, _t4_636), _mm256_mul_pd(_t17_139, _t4_636), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_141 = _mm256_sub_pd(_t17_138, _t4_637);

      // AVX Storer:
      _t17_21 = _t17_141;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 2)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526 + 2), L[52,52],h(1, 52, fi1526 + 2)) ) ),h(1, 52, fi1526 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_142 = _t17_21;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_143 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_144 = _t17_2;

      // 4-BLAC: 1x4 + 1x4
      _t4_642 = _mm256_add_pd(_t17_143, _t17_144);

      // 4-BLAC: 1x4 / 1x4
      _t17_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_142), _mm256_castpd256_pd128(_t4_642)));

      // AVX Storer:
      _t17_21 = _t17_145;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1526)) * T( G(h(1, 52, fi1526 + 3), L[52,52],h(3, 52, fi1526)) ) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_146 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t15_7, _t15_7, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_147 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_19, _t17_20), _mm256_unpacklo_pd(_t17_21, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_148 = _t17_1;

      // 4-BLAC: (1x4)^T
      _t4_647 = _t17_148;

      // 4-BLAC: 1x4 * 4x1
      _t4_648 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_147, _t4_647), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_147, _t4_647), _mm256_mul_pd(_t17_147, _t4_647), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_147, _t4_647), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_147, _t4_647), _mm256_mul_pd(_t17_147, _t4_647), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_147, _t4_647), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_147, _t4_647), _mm256_mul_pd(_t17_147, _t4_647), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t17_149 = _mm256_sub_pd(_t17_146, _t4_648);

      // AVX Storer:
      _t17_22 = _t17_149;

      // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1526 + 3)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1526 + 3), L[52,52],h(1, 52, fi1526 + 3)) ) ),h(1, 52, fi1526 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_150 = _t17_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_151 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_152 = _t17_0;

      // 4-BLAC: 1x4 + 1x4
      _t4_653 = _mm256_add_pd(_t17_151, _t17_152);

      // 4-BLAC: 1x4 / 1x4
      _t17_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_150), _mm256_castpd256_pd128(_t4_653)));

      // AVX Storer:
      _t17_22 = _t17_153;
      _mm_store_sd(&(C[52*fi1407 + fi1526]), _mm256_castpd256_pd128(_t17_7));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 1]), _mm256_castpd256_pd128(_t17_8));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 2]), _mm256_castpd256_pd128(_t17_9));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 3]), _mm256_castpd256_pd128(_t17_10));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 52]), _mm256_castpd256_pd128(_t17_11));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 53]), _mm256_castpd256_pd128(_t17_12));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 54]), _mm256_castpd256_pd128(_t17_13));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 55]), _mm256_castpd256_pd128(_t17_14));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 104]), _mm256_castpd256_pd128(_t17_15));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 105]), _mm256_castpd256_pd128(_t17_16));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 106]), _mm256_castpd256_pd128(_t17_17));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 107]), _mm256_castpd256_pd128(_t17_18));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 156]), _mm256_castpd256_pd128(_t17_19));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 157]), _mm256_castpd256_pd128(_t17_20));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 158]), _mm256_castpd256_pd128(_t17_21));
      _mm_store_sd(&(C[52*fi1407 + fi1526 + 159]), _mm256_castpd256_pd128(_t17_22));
    }
    _t18_4 = _mm256_castpd128_pd256(_mm_load_sd(C + 53*fi1407));
    _t18_5 = _mm256_maskload_pd(C + 53*fi1407 + 52, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t18_6 = _mm256_maskload_pd(C + 53*fi1407 + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t18_7 = _asm256_loadu_pd(C + 53*fi1407 + 156);
    _t18_3 = _asm256_loadu_pd(L + 52*fi1407);
    _t18_2 = _asm256_loadu_pd(L + 52*fi1407 + 52);
    _t18_1 = _asm256_loadu_pd(L + 52*fi1407 + 104);
    _t18_0 = _asm256_loadu_pd(L + 52*fi1407 + 156);

    // Generating : X[52,52] = ( ( S(h(4, 52, fi1407), ( G(h(4, 52, fi1407), C[52,52],h(4, 52, fi1407)) - ( ( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) * T( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) ) ) + ( G(h(4, 52, fi1407), X[52,52],h(4, 52, 0)) * T( G(h(4, 52, fi1407), L[52,52],h(4, 52, 0)) ) ) ) ),h(4, 52, fi1407)) + Sum_{j84} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), X[52,52],h(4, 52, j84)) * T( G(h(4, 52, fi1407), L[52,52],h(4, 52, j84)) ) ),h(4, 52, fi1407)) ) ) + Sum_{k85} ( -$(h(4, 52, fi1407), ( G(h(4, 52, fi1407), L[52,52],h(4, 52, k85)) * T( G(h(4, 52, fi1407), X[52,52],h(4, 52, k85)) ) ),h(4, 52, fi1407)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t18_8 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_4, _t18_5, 0), _mm256_shuffle_pd(_t18_6, _t18_7, 0), 32);
    _t18_9 = _mm256_permute2f128_pd(_t18_5, _mm256_shuffle_pd(_t18_6, _t18_7, 3), 32);
    _t18_10 = _mm256_blend_pd(_t18_6, _mm256_shuffle_pd(_t18_6, _t18_7, 3), 12);
    _t18_11 = _t18_7;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t4_671 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_unpacklo_pd(_t14_41, _t14_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_43, _t14_44), _mm256_unpacklo_pd(_t14_45, _t14_46), 32)), 32);
    _t4_672 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_unpacklo_pd(_t14_41, _t14_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_43, _t14_44), _mm256_unpacklo_pd(_t14_45, _t14_46), 32)), 32);
    _t4_673 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_unpacklo_pd(_t14_41, _t14_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_43, _t14_44), _mm256_unpacklo_pd(_t14_45, _t14_46), 32)), 49);
    _t4_674 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_31, _t14_32), _mm256_unpacklo_pd(_t14_33, _t14_34), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_35, _t14_36), _mm256_unpacklo_pd(_t14_37, _t14_38), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_39, _t14_40), _mm256_unpacklo_pd(_t14_41, _t14_42), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_43, _t14_44), _mm256_unpacklo_pd(_t14_45, _t14_46), 32)), 49);

    // 4-BLAC: 4x4 * 4x4
    _t4_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t4_671), _mm256_mul_pd(_t8_14, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t4_673), _mm256_mul_pd(_t8_12, _t4_674)));
    _t4_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t4_671), _mm256_mul_pd(_t8_10, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t4_673), _mm256_mul_pd(_t8_8, _t4_674)));
    _t4_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t4_671), _mm256_mul_pd(_t8_6, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t4_673), _mm256_mul_pd(_t8_4, _t4_674)));
    _t4_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t4_671), _mm256_mul_pd(_t8_2, _t4_672)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t4_673), _mm256_mul_pd(_t8_0, _t4_674)));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t4_675 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
    _t4_676 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
    _t4_677 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
    _t4_678 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t4_106 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_31, _t14_31, 32), _mm256_permute2f128_pd(_t14_31, _t14_31, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_32, _t14_32, 32), _mm256_permute2f128_pd(_t14_32, _t14_32, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_33, _t14_33, 32), _mm256_permute2f128_pd(_t14_33, _t14_33, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_34, _t14_34, 32), _mm256_permute2f128_pd(_t14_34, _t14_34, 32), 0), _t4_678)));
    _t4_107 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_35, _t14_35, 32), _mm256_permute2f128_pd(_t14_35, _t14_35, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_36, _t14_36, 32), _mm256_permute2f128_pd(_t14_36, _t14_36, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_37, _t14_37, 32), _mm256_permute2f128_pd(_t14_37, _t14_37, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_38, _t14_38, 32), _mm256_permute2f128_pd(_t14_38, _t14_38, 32), 0), _t4_678)));
    _t4_108 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_39, _t14_39, 32), _mm256_permute2f128_pd(_t14_39, _t14_39, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_40, _t14_40, 32), _mm256_permute2f128_pd(_t14_40, _t14_40, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_41, _t14_41, 32), _mm256_permute2f128_pd(_t14_41, _t14_41, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_42, _t14_42, 32), _mm256_permute2f128_pd(_t14_42, _t14_42, 32), 0), _t4_678)));
    _t4_109 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_43, _t14_43, 32), _mm256_permute2f128_pd(_t14_43, _t14_43, 32), 0), _t4_675), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_44, _t14_44, 32), _mm256_permute2f128_pd(_t14_44, _t14_44, 32), 0), _t4_676)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_45, _t14_45, 32), _mm256_permute2f128_pd(_t14_45, _t14_45, 32), 0), _t4_677), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_46, _t14_46, 32), _mm256_permute2f128_pd(_t14_46, _t14_46, 32), 0), _t4_678)));

    // 4-BLAC: 4x4 + 4x4
    _t4_27 = _mm256_add_pd(_t4_102, _t4_106);
    _t4_28 = _mm256_add_pd(_t4_103, _t4_107);
    _t4_29 = _mm256_add_pd(_t4_104, _t4_108);
    _t4_30 = _mm256_add_pd(_t4_105, _t4_109);

    // 4-BLAC: 4x4 - 4x4
    _t4_110 = _mm256_sub_pd(_t18_8, _t4_27);
    _t4_111 = _mm256_sub_pd(_t18_9, _t4_28);
    _t4_112 = _mm256_sub_pd(_t18_10, _t4_29);
    _t4_113 = _mm256_sub_pd(_t18_11, _t4_30);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t18_4 = _t4_110;
    _t18_5 = _t4_111;
    _t18_6 = _t4_112;
    _t18_7 = _t4_113;

    for( int j84 = 4; j84 <= fi1407 - 1; j84+=4 ) {
      _t19_19 = _mm256_broadcast_sd(C + 52*fi1407 + j84);
      _t19_18 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 1);
      _t19_17 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 2);
      _t19_16 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 3);
      _t19_15 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 52);
      _t19_14 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 53);
      _t19_13 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 54);
      _t19_12 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 55);
      _t19_11 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 104);
      _t19_10 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 105);
      _t19_9 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 106);
      _t19_8 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 107);
      _t19_7 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 156);
      _t19_6 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 157);
      _t19_5 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 158);
      _t19_4 = _mm256_broadcast_sd(C + 52*fi1407 + j84 + 159);
      _t19_3 = _asm256_loadu_pd(L + 52*fi1407 + j84);
      _t19_2 = _asm256_loadu_pd(L + 52*fi1407 + j84 + 52);
      _t19_1 = _asm256_loadu_pd(L + 52*fi1407 + j84 + 104);
      _t19_0 = _asm256_loadu_pd(L + 52*fi1407 + j84 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t19_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_3, _t19_2), _mm256_unpacklo_pd(_t19_1, _t19_0), 32);
      _t19_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_3, _t19_2), _mm256_unpackhi_pd(_t19_1, _t19_0), 32);
      _t19_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_3, _t19_2), _mm256_unpacklo_pd(_t19_1, _t19_0), 49);
      _t19_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_3, _t19_2), _mm256_unpackhi_pd(_t19_1, _t19_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t19_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_28), _mm256_mul_pd(_t19_18, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_30), _mm256_mul_pd(_t19_16, _t19_31)));
      _t19_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_28), _mm256_mul_pd(_t19_14, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_30), _mm256_mul_pd(_t19_12, _t19_31)));
      _t19_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_28), _mm256_mul_pd(_t19_10, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_30), _mm256_mul_pd(_t19_8, _t19_31)));
      _t19_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_7, _t19_28), _mm256_mul_pd(_t19_6, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_5, _t19_30), _mm256_mul_pd(_t19_4, _t19_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t19_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_4, _t18_5, 0), _mm256_shuffle_pd(_t18_6, _t18_7, 0), 32);
      _t19_25 = _mm256_permute2f128_pd(_t18_5, _mm256_shuffle_pd(_t18_6, _t18_7, 3), 32);
      _t19_26 = _mm256_blend_pd(_t18_6, _mm256_shuffle_pd(_t18_6, _t18_7, 3), 12);
      _t19_27 = _t18_7;

      // 4-BLAC: 4x4 - 4x4
      _t19_24 = _mm256_sub_pd(_t19_24, _t19_20);
      _t19_25 = _mm256_sub_pd(_t19_25, _t19_21);
      _t19_26 = _mm256_sub_pd(_t19_26, _t19_22);
      _t19_27 = _mm256_sub_pd(_t19_27, _t19_23);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t18_4 = _t19_24;
      _t18_5 = _t19_25;
      _t18_6 = _t19_26;
      _t18_7 = _t19_27;
    }

    for( int k85 = 4; k85 <= fi1407 - 1; k85+=4 ) {
      _t20_2 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 52);
      _t20_3 = _asm256_loadu_pd(C + 52*fi1407 + k85);
      _t20_1 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 104);
      _t20_0 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 156);
      _t20_19 = _mm256_broadcast_sd(L + 52*fi1407 + k85);
      _t20_18 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 1);
      _t20_17 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 2);
      _t20_16 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 3);
      _t20_15 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 52);
      _t20_14 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 53);
      _t20_13 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 54);
      _t20_12 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 55);
      _t20_11 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 104);
      _t20_10 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 105);
      _t20_9 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 106);
      _t20_8 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 107);
      _t20_7 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 156);
      _t20_6 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 157);
      _t20_5 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 158);
      _t20_4 = _mm256_broadcast_sd(L + 52*fi1407 + k85 + 159);
      _t20_3 = _asm256_loadu_pd(C + 52*fi1407 + k85);
      _t20_2 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 52);
      _t20_1 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 104);
      _t20_0 = _asm256_loadu_pd(C + 52*fi1407 + k85 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t20_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 32);
      _t20_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 32);
      _t20_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 49);
      _t20_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t20_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_19, _t20_28), _mm256_mul_pd(_t20_18, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_17, _t20_30), _mm256_mul_pd(_t20_16, _t20_31)));
      _t20_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t20_28), _mm256_mul_pd(_t20_14, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t20_30), _mm256_mul_pd(_t20_12, _t20_31)));
      _t20_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t20_28), _mm256_mul_pd(_t20_10, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t20_30), _mm256_mul_pd(_t20_8, _t20_31)));
      _t20_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t20_28), _mm256_mul_pd(_t20_6, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t20_30), _mm256_mul_pd(_t20_4, _t20_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t20_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_4, _t18_5, 0), _mm256_shuffle_pd(_t18_6, _t18_7, 0), 32);
      _t20_25 = _mm256_permute2f128_pd(_t18_5, _mm256_shuffle_pd(_t18_6, _t18_7, 3), 32);
      _t20_26 = _mm256_blend_pd(_t18_6, _mm256_shuffle_pd(_t18_6, _t18_7, 3), 12);
      _t20_27 = _t18_7;

      // 4-BLAC: 4x4 - 4x4
      _t20_24 = _mm256_sub_pd(_t20_24, _t20_20);
      _t20_25 = _mm256_sub_pd(_t20_25, _t20_21);
      _t20_26 = _mm256_sub_pd(_t20_26, _t20_22);
      _t20_27 = _mm256_sub_pd(_t20_27, _t20_23);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t18_4 = _t20_24;
      _t18_5 = _t20_25;
      _t18_6 = _t20_26;
      _t18_7 = _t20_27;
    }
    _t21_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi1407 + 52])));
    _t21_1 = _mm256_maskload_pd(L + 53*fi1407 + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t21_0 = _mm256_maskload_pd(L + 53*fi1407 + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : X[52,52] = S(h(1, 52, fi1407), ( G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_14 = _t18_4;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t21_15 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_16 = _t14_6;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_17 = _mm256_mul_pd(_t21_15, _t21_16);

    // 4-BLAC: 1x4 / 1x4
    _t21_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_14), _mm256_castpd256_pd128(_t7_17)));

    // AVX Storer:
    _t18_4 = _t21_17;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) - ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) Kro G(h(1, 52, fi1407), X[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_19 = _t21_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_20 = _t18_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_22 = _mm256_mul_pd(_t21_19, _t21_20);

    // 4-BLAC: 1x4 - 1x4
    _t21_21 = _mm256_sub_pd(_t21_18, _t7_22);

    // AVX Storer:
    _t21_3 = _t21_21;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) + G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_22 = _t21_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_23 = _t14_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_24 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t7_27 = _mm256_add_pd(_t21_23, _t21_24);

    // 4-BLAC: 1x4 / 1x4
    _t21_25 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_22), _mm256_castpd256_pd128(_t7_27)));

    // AVX Storer:
    _t21_3 = _t21_25;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) - ( ( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) ) ) + ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ) ),h(1, 52, fi1407 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_26 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_27 = _t21_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_28 = _t21_3;

    // 4-BLAC: (4x1)^T
    _t7_32 = _t21_28;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_33 = _mm256_mul_pd(_t21_27, _t7_32);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_29 = _t21_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_30 = _t21_2;

    // 4-BLAC: (4x1)^T
    _t7_36 = _t21_30;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_37 = _mm256_mul_pd(_t21_29, _t7_36);

    // 4-BLAC: 1x4 + 1x4
    _t7_38 = _mm256_add_pd(_t7_33, _t7_37);

    // 4-BLAC: 1x4 - 1x4
    _t21_31 = _mm256_sub_pd(_t21_26, _t7_38);

    // AVX Storer:
    _t21_4 = _t21_31;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 1), ( G(h(1, 52, fi1407 + 1), X[52,52],h(1, 52, fi1407 + 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ),h(1, 52, fi1407 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_32 = _t21_4;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t21_33 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_34 = _t14_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_43 = _mm256_mul_pd(_t21_33, _t21_34);

    // 4-BLAC: 1x4 / 1x4
    _t21_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_32), _mm256_castpd256_pd128(_t7_43)));

    // AVX Storer:
    _t21_4 = _t21_35;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) - ( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) * G(h(2, 52, fi1407), X[52,52],h(2, 52, fi1407)) ) ),h(2, 52, fi1407))

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_36 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 3);

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_37 = _t21_1;

    // AVX Loader:

    // 2x2 -> 4x4 - LowSymm
    _t21_38 = _mm256_shuffle_pd(_t18_4, _mm256_blend_pd(_mm256_unpacklo_pd(_t21_3, _t21_4), _mm256_setzero_pd(), 12), 0);
    _t21_39 = _mm256_blend_pd(_mm256_unpacklo_pd(_t21_3, _t21_4), _mm256_setzero_pd(), 12);
    _t21_40 = _mm256_setzero_pd();
    _t21_41 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t7_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_37, _t21_37, 32), _mm256_permute2f128_pd(_t21_37, _t21_37, 32), 0), _t21_38), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_37, _t21_37, 32), _mm256_permute2f128_pd(_t21_37, _t21_37, 32), 15), _t21_39)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_37, _t21_37, 49), _mm256_permute2f128_pd(_t21_37, _t21_37, 49), 0), _t21_40), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_37, _t21_37, 49), _mm256_permute2f128_pd(_t21_37, _t21_37, 49), 15), _t21_41)));

    // 4-BLAC: 1x4 - 1x4
    _t21_42 = _mm256_sub_pd(_t21_36, _t7_51);

    // AVX Storer:
    _t21_5 = _t21_42;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_43 = _mm256_blend_pd(_mm256_setzero_pd(), _t21_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_44 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_45 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t7_56 = _mm256_add_pd(_t21_44, _t21_45);

    // 4-BLAC: 1x4 / 1x4
    _t21_46 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_43), _mm256_castpd256_pd128(_t7_56)));

    // AVX Storer:
    _t21_6 = _t21_46;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) - ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ),h(1, 52, fi1407 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_47 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_48 = _t21_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_49 = _t21_2;

    // 4-BLAC: (4x1)^T
    _t7_61 = _t21_49;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_62 = _mm256_mul_pd(_t21_48, _t7_61);

    // 4-BLAC: 1x4 - 1x4
    _t21_50 = _mm256_sub_pd(_t21_47, _t7_62);

    // AVX Storer:
    _t21_7 = _t21_50;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 1)) Div ( G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) + G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ),h(1, 52, fi1407 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_51 = _t21_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_52 = _t14_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_53 = _t14_4;

    // 4-BLAC: 1x4 + 1x4
    _t7_67 = _mm256_add_pd(_t21_52, _t21_53);

    // 4-BLAC: 1x4 / 1x4
    _t21_54 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_51), _mm256_castpd256_pd128(_t7_67)));

    // AVX Storer:
    _t21_7 = _t21_54;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) - ( ( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) * T( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) ) ) + ( G(h(1, 52, fi1407 + 2), X[52,52],h(2, 52, fi1407)) * T( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) ) ),h(1, 52, fi1407 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_55 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_56 = _t21_1;

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_57 = _mm256_blend_pd(_mm256_unpacklo_pd(_t21_6, _t21_7), _mm256_setzero_pd(), 12);

    // 4-BLAC: (1x4)^T
    _t7_72 = _t21_57;

    // 4-BLAC: 1x4 * 4x1
    _t7_73 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_56, _t7_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_56, _t7_72), _mm256_mul_pd(_t21_56, _t7_72), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_56, _t7_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_56, _t7_72), _mm256_mul_pd(_t21_56, _t7_72), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_56, _t7_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_56, _t7_72), _mm256_mul_pd(_t21_56, _t7_72), 129)), 1));

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_58 = _mm256_blend_pd(_mm256_unpacklo_pd(_t21_6, _t21_7), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_59 = _t21_1;

    // 4-BLAC: (1x4)^T
    _t7_76 = _t21_59;

    // 4-BLAC: 1x4 * 4x1
    _t7_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_58, _t7_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_58, _t7_76), _mm256_mul_pd(_t21_58, _t7_76), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_58, _t7_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_58, _t7_76), _mm256_mul_pd(_t21_58, _t7_76), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_58, _t7_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_58, _t7_76), _mm256_mul_pd(_t21_58, _t7_76), 129)), 1));

    // 4-BLAC: 1x4 + 1x4
    _t7_78 = _mm256_add_pd(_t7_73, _t7_77);

    // 4-BLAC: 1x4 - 1x4
    _t21_60 = _mm256_sub_pd(_t21_55, _t7_78);

    // AVX Storer:
    _t21_8 = _t21_60;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 2), ( G(h(1, 52, fi1407 + 2), X[52,52],h(1, 52, fi1407 + 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) ),h(1, 52, fi1407 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_61 = _t21_8;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t21_62 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_63 = _t14_2;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_83 = _mm256_mul_pd(_t21_62, _t21_63);

    // 4-BLAC: 1x4 / 1x4
    _t21_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_61), _mm256_castpd256_pd128(_t7_83)));

    // AVX Storer:
    _t21_8 = _t21_64;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) - ( G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) * G(h(3, 52, fi1407), X[52,52],h(3, 52, fi1407)) ) ),h(3, 52, fi1407))

    // AVX Loader:

    // 1x3 -> 1x4
    _t21_65 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_7, 7);

    // AVX Loader:

    // 1x3 -> 1x4
    _t21_66 = _t21_0;

    // AVX Loader:

    // 3x3 -> 4x4 - LowSymm
    _t21_67 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_4, _mm256_blend_pd(_mm256_unpacklo_pd(_t21_3, _t21_4), _mm256_setzero_pd(), 12), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_6, _t21_7), _mm256_unpacklo_pd(_t21_8, _mm256_setzero_pd()), 32), 32), _t18_4, 8);
    _t21_68 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t21_3, _t21_4), _mm256_setzero_pd(), 12), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_6, _t21_7), _mm256_unpacklo_pd(_t21_8, _mm256_setzero_pd()), 32), 32), 6), _t18_4, 8);
    _t21_69 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_6, _t21_7), _mm256_unpacklo_pd(_t21_8, _mm256_setzero_pd()), 32);
    _t21_70 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t7_91 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_66, _t21_66, 32), _mm256_permute2f128_pd(_t21_66, _t21_66, 32), 0), _t21_67), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_66, _t21_66, 32), _mm256_permute2f128_pd(_t21_66, _t21_66, 32), 15), _t21_68)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_66, _t21_66, 49), _mm256_permute2f128_pd(_t21_66, _t21_66, 49), 0), _t21_69), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_66, _t21_66, 49), _mm256_permute2f128_pd(_t21_66, _t21_66, 49), 15), _t21_70)));

    // 4-BLAC: 1x4 - 1x4
    _t21_71 = _mm256_sub_pd(_t21_65, _t7_91);

    // AVX Storer:
    _t21_9 = _t21_71;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1407), L[52,52],h(1, 52, fi1407)) ) ),h(1, 52, fi1407))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_72 = _mm256_blend_pd(_mm256_setzero_pd(), _t21_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_73 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_74 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t7_96 = _mm256_add_pd(_t21_73, _t21_74);

    // 4-BLAC: 1x4 / 1x4
    _t21_75 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_72), _mm256_castpd256_pd128(_t7_96)));

    // AVX Storer:
    _t21_10 = _t21_75;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407)) Kro T( G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407)) ) ) ),h(1, 52, fi1407 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_76 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_77 = _t21_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_78 = _t21_2;

    // 4-BLAC: (4x1)^T
    _t7_101 = _t21_78;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_102 = _mm256_mul_pd(_t21_77, _t7_101);

    // 4-BLAC: 1x4 - 1x4
    _t21_79 = _mm256_sub_pd(_t21_76, _t7_102);

    // AVX Storer:
    _t21_11 = _t21_79;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 1)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1407 + 1), L[52,52],h(1, 52, fi1407 + 1)) ) ),h(1, 52, fi1407 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_80 = _t21_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_81 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_82 = _t14_4;

    // 4-BLAC: 1x4 + 1x4
    _t7_107 = _mm256_add_pd(_t21_81, _t21_82);

    // 4-BLAC: 1x4 / 1x4
    _t21_83 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_80), _mm256_castpd256_pd128(_t7_107)));

    // AVX Storer:
    _t21_11 = _t21_83;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) - ( G(h(1, 52, fi1407 + 3), X[52,52],h(2, 52, fi1407)) * T( G(h(1, 52, fi1407 + 2), L[52,52],h(2, 52, fi1407)) ) ) ),h(1, 52, fi1407 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_84 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_9, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t21_9, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_85 = _mm256_blend_pd(_mm256_unpacklo_pd(_t21_10, _t21_11), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t21_86 = _t21_1;

    // 4-BLAC: (1x4)^T
    _t7_112 = _t21_86;

    // 4-BLAC: 1x4 * 4x1
    _t7_113 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_85, _t7_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_85, _t7_112), _mm256_mul_pd(_t21_85, _t7_112), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_85, _t7_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_85, _t7_112), _mm256_mul_pd(_t21_85, _t7_112), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_85, _t7_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_85, _t7_112), _mm256_mul_pd(_t21_85, _t7_112), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t21_87 = _mm256_sub_pd(_t21_84, _t7_113);

    // AVX Storer:
    _t21_12 = _t21_87;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 2)) Div ( G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) + G(h(1, 52, fi1407 + 2), L[52,52],h(1, 52, fi1407 + 2)) ) ),h(1, 52, fi1407 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_88 = _t21_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_89 = _t14_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_90 = _t14_2;

    // 4-BLAC: 1x4 + 1x4
    _t7_118 = _mm256_add_pd(_t21_89, _t21_90);

    // 4-BLAC: 1x4 / 1x4
    _t21_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_88), _mm256_castpd256_pd128(_t7_118)));

    // AVX Storer:
    _t21_12 = _t21_91;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) - ( ( G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) * T( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) ) ) + ( G(h(1, 52, fi1407 + 3), X[52,52],h(3, 52, fi1407)) * T( G(h(1, 52, fi1407 + 3), L[52,52],h(3, 52, fi1407)) ) ) ) ),h(1, 52, fi1407 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_92 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t18_7, _t18_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t21_93 = _t21_0;

    // AVX Loader:

    // 1x3 -> 1x4
    _t21_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_10, _t21_11), _mm256_unpacklo_pd(_t21_12, _mm256_setzero_pd()), 32);

    // 4-BLAC: (1x4)^T
    _t7_123 = _t21_94;

    // 4-BLAC: 1x4 * 4x1
    _t7_124 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_93, _t7_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_93, _t7_123), _mm256_mul_pd(_t21_93, _t7_123), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_93, _t7_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_93, _t7_123), _mm256_mul_pd(_t21_93, _t7_123), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_93, _t7_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_93, _t7_123), _mm256_mul_pd(_t21_93, _t7_123), 129)), 1));

    // AVX Loader:

    // 1x3 -> 1x4
    _t21_95 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_10, _t21_11), _mm256_unpacklo_pd(_t21_12, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t21_96 = _t21_0;

    // 4-BLAC: (1x4)^T
    _t7_127 = _t21_96;

    // 4-BLAC: 1x4 * 4x1
    _t7_128 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_95, _t7_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_95, _t7_127), _mm256_mul_pd(_t21_95, _t7_127), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_95, _t7_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_95, _t7_127), _mm256_mul_pd(_t21_95, _t7_127), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_95, _t7_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_95, _t7_127), _mm256_mul_pd(_t21_95, _t7_127), 129)), 1));

    // 4-BLAC: 1x4 + 1x4
    _t7_129 = _mm256_add_pd(_t7_124, _t7_128);

    // 4-BLAC: 1x4 - 1x4
    _t21_97 = _mm256_sub_pd(_t21_92, _t7_129);

    // AVX Storer:
    _t21_13 = _t21_97;

    // Generating : X[52,52] = S(h(1, 52, fi1407 + 3), ( G(h(1, 52, fi1407 + 3), X[52,52],h(1, 52, fi1407 + 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 52, fi1407 + 3), L[52,52],h(1, 52, fi1407 + 3)) ) ),h(1, 52, fi1407 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_98 = _t21_13;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t21_99 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_100 = _t14_0;

    // 4-BLAC: 1x4 Kro 1x4
    _t7_134 = _mm256_mul_pd(_t21_99, _t21_100);

    // 4-BLAC: 1x4 / 1x4
    _t21_101 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_98), _mm256_castpd256_pd128(_t7_134)));

    // AVX Storer:
    _t21_13 = _t21_101;
    _mm_store_sd(&(C[52*fi1407]), _mm256_castpd256_pd128(_t14_31));
    _mm_store_sd(&(C[52*fi1407 + 1]), _mm256_castpd256_pd128(_t14_32));
    _mm_store_sd(&(C[52*fi1407 + 2]), _mm256_castpd256_pd128(_t14_33));
    _mm_store_sd(&(C[52*fi1407 + 3]), _mm256_castpd256_pd128(_t14_34));
    _mm_store_sd(&(C[52*fi1407 + 52]), _mm256_castpd256_pd128(_t14_35));
    _mm_store_sd(&(C[52*fi1407 + 53]), _mm256_castpd256_pd128(_t14_36));
    _mm_store_sd(&(C[52*fi1407 + 54]), _mm256_castpd256_pd128(_t14_37));
    _mm_store_sd(&(C[52*fi1407 + 55]), _mm256_castpd256_pd128(_t14_38));
    _mm_store_sd(&(C[52*fi1407 + 104]), _mm256_castpd256_pd128(_t14_39));
    _mm_store_sd(&(C[52*fi1407 + 105]), _mm256_castpd256_pd128(_t14_40));
    _mm_store_sd(&(C[52*fi1407 + 106]), _mm256_castpd256_pd128(_t14_41));
    _mm_store_sd(&(C[52*fi1407 + 107]), _mm256_castpd256_pd128(_t14_42));
    _mm_store_sd(&(C[52*fi1407 + 156]), _mm256_castpd256_pd128(_t14_43));
    _mm_store_sd(&(C[52*fi1407 + 157]), _mm256_castpd256_pd128(_t14_44));
    _mm_store_sd(&(C[52*fi1407 + 158]), _mm256_castpd256_pd128(_t14_45));
    _mm_store_sd(&(C[52*fi1407 + 159]), _mm256_castpd256_pd128(_t14_46));
    _mm_store_sd(C + 53*fi1407, _mm256_castpd256_pd128(_t18_4));
    _mm_store_sd(&(C[53*fi1407 + 52]), _mm256_castpd256_pd128(_t21_3));
    _mm_store_sd(&(C[53*fi1407 + 53]), _mm256_castpd256_pd128(_t21_4));
    _mm_store_sd(&(C[53*fi1407 + 104]), _mm256_castpd256_pd128(_t21_6));
    _mm_store_sd(&(C[53*fi1407 + 105]), _mm256_castpd256_pd128(_t21_7));
    _mm_store_sd(&(C[53*fi1407 + 106]), _mm256_castpd256_pd128(_t21_8));
    _mm_store_sd(&(C[53*fi1407 + 156]), _mm256_castpd256_pd128(_t21_10));
    _mm_store_sd(&(C[53*fi1407 + 157]), _mm256_castpd256_pd128(_t21_11));
    _mm_store_sd(&(C[53*fi1407 + 158]), _mm256_castpd256_pd128(_t21_12));
    _mm_store_sd(&(C[53*fi1407 + 159]), _mm256_castpd256_pd128(_t21_13));
  }

  _mm_store_sd(&(C[0]), _mm256_castpd256_pd128(_t0_111));
  _mm_store_sd(&(C[52]), _mm256_castpd256_pd128(_t0_112));
  _mm_store_sd(&(C[53]), _mm256_castpd256_pd128(_t0_113));
  _mm_store_sd(&(C[104]), _mm256_castpd256_pd128(_t0_115));
  _mm_store_sd(&(C[105]), _mm256_castpd256_pd128(_t0_116));
  _mm_store_sd(&(C[106]), _mm256_castpd256_pd128(_t0_117));
  _mm_store_sd(&(C[156]), _mm256_castpd256_pd128(_t0_119));
  _mm_store_sd(&(C[157]), _mm256_castpd256_pd128(_t0_120));
  _mm_store_sd(&(C[158]), _mm256_castpd256_pd128(_t0_121));
  _mm_store_sd(&(C[159]), _mm256_castpd256_pd128(_t0_122));

}
