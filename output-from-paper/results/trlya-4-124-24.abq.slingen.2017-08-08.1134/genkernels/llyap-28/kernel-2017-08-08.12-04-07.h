/*
 * llyap_kernel.h
 *
Decl { {u'X': Symmetric[X, (28, 28), LSMatAccess], u'C': Symmetric[C, (28, 28), LSMatAccess], u'L': LowerTriangular[L, (28, 28), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt': {'m0': 'm04.ll'}, 'ftmpyozk_lwn_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt_m04_ftmpyozk_lwn_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
For_{fi4;0;23;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, fi4), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4), X[28,28],h(1, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), X[28,28],h(3, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), X[28,28],h(3, 28, fi4 + 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4 + 1)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(2, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(2, 28, fi4 + 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) ) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 2)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) ) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), L[28,28],h(4, 28, fi4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi4), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 20:
For_{fi123;0;-fi4 + 19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi123 + fi4 + 5), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi123 + fi4 + 5), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi123 + fi4 + 6), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi123 + fi4 + 6), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 5)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi123 - fi4 + 20, 28, fi123 + fi4 + 8), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi123 - fi4 + 20, 28, fi123 + fi4 + 8), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi123 - fi4 + 20, 28, fi123 + fi4 + 8), L[28,28],h(4, 28, fi123 + fi4 + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi123 + fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
 )Entry 21:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(4, 28, fi4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(4, 28, fi4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(4, 28, fi4)) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(2, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(3, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), X[28,28],h(-fi4 + 24, 28, fi4 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), X[28,28],h(-fi4 + 24, 28, fi4 + 4)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), L[28,28],h(4, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 24, 28, fi4 + 4), L[28,28],h(4, 28, fi4)) ) ) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(3, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(3, 28, 25)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(2, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(2, 28, 26)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-08
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 28

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84, _t2_85, _t2_86, _t2_87,
	_t2_88, _t2_89, _t2_90, _t2_91, _t2_92, _t2_93, _t2_94, _t2_95,
	_t2_96, _t2_97, _t2_98, _t2_99, _t2_100, _t2_101, _t2_102, _t2_103,
	_t2_104, _t2_105, _t2_106, _t2_107, _t2_108, _t2_109, _t2_110, _t2_111,
	_t2_112, _t2_113, _t2_114, _t2_115, _t2_116, _t2_117, _t2_118, _t2_119,
	_t2_120, _t2_121, _t2_122, _t2_123, _t2_124, _t2_125, _t2_126, _t2_127,
	_t2_128, _t2_129, _t2_130, _t2_131, _t2_132, _t2_133, _t2_134, _t2_135,
	_t2_136, _t2_137, _t2_138, _t2_139, _t2_140, _t2_141, _t2_142, _t2_143,
	_t2_144, _t2_145, _t2_146, _t2_147, _t2_148, _t2_149, _t2_150, _t2_151,
	_t2_152, _t2_153, _t2_154, _t2_155, _t2_156, _t2_157, _t2_158, _t2_159,
	_t2_160, _t2_161, _t2_162, _t2_163, _t2_164, _t2_165, _t2_166, _t2_167,
	_t2_168, _t2_169, _t2_170, _t2_171, _t2_172, _t2_173, _t2_174, _t2_175,
	_t2_176, _t2_177, _t2_178, _t2_179, _t2_180, _t2_181, _t2_182, _t2_183,
	_t2_184, _t2_185, _t2_186, _t2_187, _t2_188, _t2_189, _t2_190, _t2_191,
	_t2_192, _t2_193, _t2_194, _t2_195, _t2_196, _t2_197, _t2_198, _t2_199,
	_t2_200, _t2_201, _t2_202, _t2_203, _t2_204, _t2_205, _t2_206, _t2_207,
	_t2_208;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31,
	_t3_32, _t3_33, _t3_34, _t3_35, _t3_36, _t3_37, _t3_38, _t3_39;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143,
	_t4_144, _t4_145, _t4_146, _t4_147, _t4_148, _t4_149, _t4_150, _t4_151,
	_t4_152, _t4_153, _t4_154, _t4_155, _t4_156, _t4_157, _t4_158, _t4_159,
	_t4_160, _t4_161, _t4_162, _t4_163, _t4_164, _t4_165, _t4_166, _t4_167,
	_t4_168, _t4_169, _t4_170, _t4_171, _t4_172, _t4_173, _t4_174, _t4_175,
	_t4_176, _t4_177, _t4_178, _t4_179, _t4_180, _t4_181, _t4_182, _t4_183,
	_t4_184, _t4_185, _t4_186, _t4_187, _t4_188, _t4_189, _t4_190, _t4_191,
	_t4_192, _t4_193, _t4_194, _t4_195, _t4_196, _t4_197, _t4_198, _t4_199,
	_t4_200, _t4_201, _t4_202, _t4_203, _t4_204, _t4_205, _t4_206, _t4_207,
	_t4_208, _t4_209, _t4_210, _t4_211, _t4_212, _t4_213, _t4_214, _t4_215,
	_t4_216, _t4_217, _t4_218, _t4_219, _t4_220, _t4_221, _t4_222, _t4_223,
	_t4_224, _t4_225, _t4_226, _t4_227, _t4_228, _t4_229, _t4_230, _t4_231,
	_t4_232, _t4_233, _t4_234, _t4_235, _t4_236, _t4_237, _t4_238, _t4_239,
	_t4_240, _t4_241, _t4_242, _t4_243, _t4_244, _t4_245, _t4_246, _t4_247,
	_t4_248, _t4_249, _t4_250, _t4_251, _t4_252, _t4_253, _t4_254, _t4_255,
	_t4_256, _t4_257, _t4_258, _t4_259, _t4_260, _t4_261, _t4_262, _t4_263,
	_t4_264, _t4_265, _t4_266, _t4_267, _t4_268, _t4_269, _t4_270, _t4_271,
	_t4_272, _t4_273, _t4_274, _t4_275, _t4_276, _t4_277, _t4_278, _t4_279,
	_t4_280;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60, _t5_61, _t5_62, _t5_63;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39,
	_t6_40, _t6_41, _t6_42, _t6_43, _t6_44, _t6_45, _t6_46, _t6_47,
	_t6_48, _t6_49, _t6_50, _t6_51, _t6_52, _t6_53, _t6_54, _t6_55,
	_t6_56, _t6_57, _t6_58, _t6_59, _t6_60, _t6_61, _t6_62, _t6_63,
	_t6_64, _t6_65, _t6_66, _t6_67, _t6_68, _t6_69, _t6_70, _t6_71;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31,
	_t7_32, _t7_33, _t7_34, _t7_35, _t7_36, _t7_37, _t7_38, _t7_39,
	_t7_40, _t7_41, _t7_42, _t7_43, _t7_44, _t7_45, _t7_46, _t7_47,
	_t7_48, _t7_49, _t7_50, _t7_51, _t7_52, _t7_53, _t7_54, _t7_55,
	_t7_56, _t7_57, _t7_58, _t7_59, _t7_60, _t7_61, _t7_62, _t7_63,
	_t7_64, _t7_65, _t7_66, _t7_67, _t7_68, _t7_69, _t7_70, _t7_71,
	_t7_72, _t7_73, _t7_74, _t7_75, _t7_76, _t7_77, _t7_78, _t7_79,
	_t7_80, _t7_81, _t7_82, _t7_83, _t7_84, _t7_85, _t7_86, _t7_87,
	_t7_88, _t7_89, _t7_90, _t7_91, _t7_92, _t7_93, _t7_94, _t7_95,
	_t7_96, _t7_97, _t7_98, _t7_99, _t7_100, _t7_101, _t7_102, _t7_103,
	_t7_104, _t7_105, _t7_106, _t7_107, _t7_108, _t7_109, _t7_110, _t7_111,
	_t7_112, _t7_113, _t7_114, _t7_115, _t7_116, _t7_117, _t7_118, _t7_119,
	_t7_120, _t7_121, _t7_122, _t7_123, _t7_124, _t7_125, _t7_126, _t7_127,
	_t7_128, _t7_129, _t7_130, _t7_131, _t7_132, _t7_133, _t7_134, _t7_135,
	_t7_136, _t7_137, _t7_138, _t7_139, _t7_140, _t7_141, _t7_142, _t7_143,
	_t7_144, _t7_145, _t7_146, _t7_147, _t7_148, _t7_149, _t7_150, _t7_151,
	_t7_152, _t7_153, _t7_154, _t7_155, _t7_156, _t7_157, _t7_158, _t7_159,
	_t7_160, _t7_161, _t7_162, _t7_163, _t7_164, _t7_165, _t7_166, _t7_167,
	_t7_168, _t7_169, _t7_170, _t7_171, _t7_172, _t7_173, _t7_174, _t7_175,
	_t7_176, _t7_177, _t7_178, _t7_179, _t7_180, _t7_181, _t7_182, _t7_183,
	_t7_184, _t7_185, _t7_186, _t7_187, _t7_188, _t7_189, _t7_190, _t7_191,
	_t7_192, _t7_193, _t7_194, _t7_195, _t7_196, _t7_197, _t7_198, _t7_199,
	_t7_200, _t7_201, _t7_202, _t7_203, _t7_204, _t7_205, _t7_206, _t7_207,
	_t7_208, _t7_209, _t7_210, _t7_211, _t7_212, _t7_213, _t7_214, _t7_215,
	_t7_216, _t7_217, _t7_218, _t7_219, _t7_220, _t7_221, _t7_222, _t7_223,
	_t7_224, _t7_225, _t7_226, _t7_227, _t7_228, _t7_229, _t7_230, _t7_231,
	_t7_232, _t7_233, _t7_234, _t7_235, _t7_236, _t7_237, _t7_238, _t7_239,
	_t7_240, _t7_241, _t7_242, _t7_243, _t7_244, _t7_245, _t7_246, _t7_247,
	_t7_248, _t7_249, _t7_250, _t7_251, _t7_252, _t7_253, _t7_254, _t7_255,
	_t7_256, _t7_257, _t7_258, _t7_259, _t7_260, _t7_261, _t7_262, _t7_263,
	_t7_264, _t7_265, _t7_266, _t7_267, _t7_268, _t7_269, _t7_270, _t7_271,
	_t7_272, _t7_273, _t7_274, _t7_275, _t7_276, _t7_277, _t7_278, _t7_279,
	_t7_280, _t7_281, _t7_282, _t7_283, _t7_284, _t7_285, _t7_286, _t7_287,
	_t7_288, _t7_289, _t7_290, _t7_291, _t7_292, _t7_293, _t7_294, _t7_295,
	_t7_296, _t7_297, _t7_298, _t7_299, _t7_300, _t7_301, _t7_302, _t7_303,
	_t7_304, _t7_305, _t7_306, _t7_307, _t7_308, _t7_309, _t7_310, _t7_311,
	_t7_312, _t7_313, _t7_314, _t7_315, _t7_316, _t7_317, _t7_318, _t7_319,
	_t7_320, _t7_321, _t7_322, _t7_323, _t7_324, _t7_325, _t7_326, _t7_327,
	_t7_328, _t7_329, _t7_330, _t7_331, _t7_332, _t7_333, _t7_334, _t7_335,
	_t7_336, _t7_337, _t7_338, _t7_339, _t7_340, _t7_341, _t7_342, _t7_343,
	_t7_344, _t7_345, _t7_346, _t7_347, _t7_348, _t7_349, _t7_350, _t7_351,
	_t7_352, _t7_353, _t7_354, _t7_355, _t7_356, _t7_357, _t7_358, _t7_359,
	_t7_360, _t7_361, _t7_362, _t7_363, _t7_364, _t7_365, _t7_366, _t7_367,
	_t7_368, _t7_369, _t7_370, _t7_371, _t7_372, _t7_373, _t7_374, _t7_375,
	_t7_376, _t7_377, _t7_378, _t7_379, _t7_380, _t7_381, _t7_382, _t7_383,
	_t7_384, _t7_385, _t7_386, _t7_387, _t7_388, _t7_389, _t7_390, _t7_391,
	_t7_392, _t7_393, _t7_394, _t7_395, _t7_396, _t7_397, _t7_398, _t7_399,
	_t7_400, _t7_401, _t7_402, _t7_403, _t7_404, _t7_405, _t7_406, _t7_407,
	_t7_408, _t7_409, _t7_410, _t7_411, _t7_412, _t7_413, _t7_414, _t7_415,
	_t7_416, _t7_417, _t7_418, _t7_419, _t7_420, _t7_421, _t7_422, _t7_423,
	_t7_424, _t7_425, _t7_426, _t7_427, _t7_428, _t7_429, _t7_430, _t7_431,
	_t7_432, _t7_433, _t7_434, _t7_435, _t7_436, _t7_437, _t7_438, _t7_439,
	_t7_440, _t7_441, _t7_442, _t7_443, _t7_444, _t7_445, _t7_446, _t7_447,
	_t7_448, _t7_449, _t7_450, _t7_451, _t7_452, _t7_453, _t7_454, _t7_455,
	_t7_456, _t7_457, _t7_458, _t7_459, _t7_460, _t7_461, _t7_462, _t7_463,
	_t7_464, _t7_465, _t7_466, _t7_467, _t7_468, _t7_469, _t7_470, _t7_471,
	_t7_472, _t7_473, _t7_474, _t7_475, _t7_476, _t7_477, _t7_478, _t7_479,
	_t7_480, _t7_481, _t7_482, _t7_483, _t7_484, _t7_485, _t7_486, _t7_487,
	_t7_488, _t7_489, _t7_490, _t7_491, _t7_492, _t7_493, _t7_494, _t7_495,
	_t7_496, _t7_497, _t7_498, _t7_499, _t7_500, _t7_501, _t7_502, _t7_503,
	_t7_504, _t7_505, _t7_506, _t7_507, _t7_508, _t7_509, _t7_510, _t7_511,
	_t7_512, _t7_513, _t7_514, _t7_515, _t7_516, _t7_517, _t7_518, _t7_519,
	_t7_520, _t7_521, _t7_522, _t7_523, _t7_524, _t7_525, _t7_526, _t7_527,
	_t7_528, _t7_529, _t7_530, _t7_531, _t7_532, _t7_533, _t7_534, _t7_535,
	_t7_536, _t7_537, _t7_538, _t7_539, _t7_540, _t7_541, _t7_542, _t7_543,
	_t7_544, _t7_545, _t7_546, _t7_547, _t7_548, _t7_549, _t7_550, _t7_551,
	_t7_552, _t7_553, _t7_554, _t7_555, _t7_556, _t7_557, _t7_558, _t7_559,
	_t7_560, _t7_561, _t7_562, _t7_563, _t7_564, _t7_565, _t7_566, _t7_567,
	_t7_568, _t7_569, _t7_570, _t7_571, _t7_572, _t7_573, _t7_574, _t7_575,
	_t7_576, _t7_577, _t7_578, _t7_579, _t7_580, _t7_581, _t7_582, _t7_583,
	_t7_584, _t7_585, _t7_586, _t7_587, _t7_588, _t7_589, _t7_590, _t7_591,
	_t7_592, _t7_593, _t7_594;


  for( int fi4 = 0; fi4 <= 19; fi4+=4 ) {
    _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi4])));
    _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4])));
    _t0_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(C + 29*fi4 + 28)), _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi4 + 56))), _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi4 + 84)), 32);
    _t0_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 28)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 56))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 84)), 32);
    _t0_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 29])));
    _t0_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 57)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 85)), 0);
    _t0_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 58])));
    _t0_1 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 86])));
    _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 87])));
    _t0_13 = _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi4 + 29));
    _t0_14 = _mm256_maskload_pd(C + 29*fi4 + 57, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t0_15 = _mm256_maskload_pd(C + 29*fi4 + 85, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : X[28,28] = S(h(1, 28, fi4), ( G(h(1, 28, fi4), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_149 = _t0_7;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t0_150 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_151 = _t0_6;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_152 = _mm256_mul_pd(_t0_150, _t0_151);

    // 4-BLAC: 1x4 / 1x4
    _t0_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_149), _mm256_castpd256_pd128(_t0_152)));

    // AVX Storer:
    _t0_7 = _t0_153;

    // Generating : X[28,28] = S(h(3, 28, fi4 + 1), ( G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) - ( G(h(3, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) Kro G(h(1, 28, fi4), X[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_154 = _t0_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_155 = _t0_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_156 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_7, _t0_7, 32), _mm256_permute2f128_pd(_t0_7, _t0_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t0_157 = _mm256_mul_pd(_t0_155, _t0_156);

    // 4-BLAC: 4x1 - 4x1
    _t0_158 = _mm256_sub_pd(_t0_154, _t0_157);

    // AVX Storer:
    _t0_8 = _t0_158;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 1), ( G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_159 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_160 = _t0_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_161 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_162 = _mm256_add_pd(_t0_160, _t0_161);

    // 4-BLAC: 1x4 / 1x4
    _t0_163 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_159), _mm256_castpd256_pd128(_t0_162)));

    // AVX Storer:
    _t0_9 = _t0_163;

    // Generating : X[28,28] = S(h(2, 28, fi4 + 2), ( G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) - ( G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) Kro G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_164 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_8, 2), _mm256_permute2f128_pd(_t0_8, _t0_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_165 = _t0_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_166 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t0_167 = _mm256_mul_pd(_t0_165, _t0_166);

    // 4-BLAC: 4x1 - 4x1
    _t0_168 = _mm256_sub_pd(_t0_164, _t0_167);

    // AVX Storer:
    _t0_10 = _t0_168;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 2), ( G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_169 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_23 = _t0_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_24 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_25 = _mm256_add_pd(_t0_23, _t0_24);

    // 4-BLAC: 1x4 / 1x4
    _t0_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_169), _mm256_castpd256_pd128(_t0_25)));

    // AVX Storer:
    _t0_11 = _t0_26;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4)) - ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) Kro G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_28 = _t0_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_29 = _t0_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_30 = _mm256_mul_pd(_t0_28, _t0_29);

    // 4-BLAC: 1x4 - 1x4
    _t0_31 = _mm256_sub_pd(_t0_27, _t0_30);

    // AVX Storer:
    _t0_12 = _t0_31;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_32 = _t0_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_33 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_34 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_35 = _mm256_add_pd(_t0_33, _t0_34);

    // 4-BLAC: 1x4 / 1x4
    _t0_36 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_32), _mm256_castpd256_pd128(_t0_35)));

    // AVX Storer:
    _t0_12 = _t0_36;

    // Generating : X[28,28] = S(h(3, 28, fi4 + 1), ( G(h(3, 28, fi4 + 1), X[28,28],h(3, 28, fi4 + 1)) - ( ( G(h(3, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) * T( G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) ) ) + ( G(h(3, 28, fi4 + 1), X[28,28],h(1, 28, fi4)) * T( G(h(3, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ) ),h(3, 28, fi4 + 1))

    // AVX Loader:

    // 3x3 -> 4x4 - LowSymm
    _t0_37 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_13, _t0_14, 0), _t0_15, 32), _t0_13, 8);
    _t0_38 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_t0_14, _t0_15, 32), 6), _t0_13, 8);
    _t0_39 = _t0_15;
    _t0_40 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_41 = _t0_5;

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_12, _mm256_setzero_pd()), 32);

    // 4-BLAC: (4x1)^T
    _t0_43 = _t0_42;

    // 4-BLAC: 4x1 * 1x4
    _t0_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t0_43);
    _t0_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 15), _t0_43);
    _t0_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 49), _mm256_permute2f128_pd(_t0_41, _t0_41, 49), 0), _t0_43);
    _t0_47 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 49), _mm256_permute2f128_pd(_t0_41, _t0_41, 49), 15), _t0_43);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_12, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_49 = _t0_5;

    // 4-BLAC: (4x1)^T
    _t0_50 = _t0_49;

    // 4-BLAC: 4x1 * 1x4
    _t0_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t0_50);
    _t0_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 15), _t0_50);
    _t0_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 49), _mm256_permute2f128_pd(_t0_48, _t0_48, 49), 0), _t0_50);
    _t0_54 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 49), _mm256_permute2f128_pd(_t0_48, _t0_48, 49), 15), _t0_50);

    // 4-BLAC: 4x4 + 4x4
    _t0_55 = _mm256_add_pd(_t0_44, _t0_51);
    _t0_56 = _mm256_add_pd(_t0_45, _t0_52);
    _t0_57 = _mm256_add_pd(_t0_46, _t0_53);
    _t0_58 = _mm256_add_pd(_t0_47, _t0_54);

    // 4-BLAC: 4x4 - 4x4
    _t0_59 = _mm256_sub_pd(_t0_37, _t0_55);
    _t0_60 = _mm256_sub_pd(_t0_38, _t0_56);
    _t0_61 = _mm256_sub_pd(_t0_39, _t0_57);
    _t0_62 = _mm256_sub_pd(_t0_40, _t0_58);

    // AVX Storer:

    // 4x4 -> 3x3 - LowSymm
    _t0_13 = _t0_59;
    _t0_14 = _t0_60;
    _t0_15 = _t0_61;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 1), ( G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_63 = _t0_13;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t0_64 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_65 = _t0_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_66 = _mm256_mul_pd(_t0_64, _t0_65);

    // 4-BLAC: 1x4 / 1x4
    _t0_67 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_63), _mm256_castpd256_pd128(_t0_66)));

    // AVX Storer:
    _t0_13 = _t0_67;

    // Generating : X[28,28] = S(h(2, 28, fi4 + 2), ( G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) Kro G(h(1, 28, fi4 + 1), X[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_68 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_14, _t0_15), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_69 = _t0_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_70 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t0_71 = _mm256_mul_pd(_t0_69, _t0_70);

    // 4-BLAC: 4x1 - 4x1
    _t0_72 = _mm256_sub_pd(_t0_68, _t0_71);

    // AVX Storer:
    _t0_16 = _t0_72;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 2), ( G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_73 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_74 = _t0_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_75 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_76 = _mm256_add_pd(_t0_74, _t0_75);

    // 4-BLAC: 1x4 / 1x4
    _t0_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_73), _mm256_castpd256_pd128(_t0_76)));

    // AVX Storer:
    _t0_17 = _t0_77;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) Kro G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_78 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_79 = _t0_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_80 = _t0_17;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_81 = _mm256_mul_pd(_t0_79, _t0_80);

    // 4-BLAC: 1x4 - 1x4
    _t0_82 = _mm256_sub_pd(_t0_78, _t0_81);

    // AVX Storer:
    _t0_18 = _t0_82;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_83 = _t0_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_84 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_85 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_86 = _mm256_add_pd(_t0_84, _t0_85);

    // 4-BLAC: 1x4 / 1x4
    _t0_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_83), _mm256_castpd256_pd128(_t0_86)));

    // AVX Storer:
    _t0_18 = _t0_87;

    // Generating : X[28,28] = S(h(2, 28, fi4 + 2), ( G(h(2, 28, fi4 + 2), X[28,28],h(2, 28, fi4 + 2)) - ( ( G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) * T( G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) ) ) + ( G(h(2, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 1)) * T( G(h(2, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 1)) ) ) ) ),h(2, 28, fi4 + 2))

    // AVX Loader:

    // 2x2 -> 4x4 - LowSymm
    _t0_88 = _mm256_shuffle_pd(_mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 2), _mm256_setzero_pd()), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_15, 6), _mm256_permute2f128_pd(_t0_15, _t0_15, 129), 5), 0);
    _t0_89 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_15, 6), _mm256_permute2f128_pd(_t0_15, _t0_15, 129), 5);
    _t0_90 = _mm256_setzero_pd();
    _t0_91 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_92 = _t0_3;

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_93 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_17, _t0_18), _mm256_setzero_pd(), 12);

    // 4-BLAC: (4x1)^T
    _t0_94 = _t0_93;

    // 4-BLAC: 4x1 * 1x4
    _t0_95 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_92, _t0_92, 32), _mm256_permute2f128_pd(_t0_92, _t0_92, 32), 0), _t0_94);
    _t0_96 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_92, _t0_92, 32), _mm256_permute2f128_pd(_t0_92, _t0_92, 32), 15), _t0_94);
    _t0_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_92, _t0_92, 49), _mm256_permute2f128_pd(_t0_92, _t0_92, 49), 0), _t0_94);
    _t0_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_92, _t0_92, 49), _mm256_permute2f128_pd(_t0_92, _t0_92, 49), 15), _t0_94);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_99 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_17, _t0_18), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_100 = _t0_3;

    // 4-BLAC: (4x1)^T
    _t0_101 = _t0_100;

    // 4-BLAC: 4x1 * 1x4
    _t0_102 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_99, _t0_99, 32), _mm256_permute2f128_pd(_t0_99, _t0_99, 32), 0), _t0_101);
    _t0_103 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_99, _t0_99, 32), _mm256_permute2f128_pd(_t0_99, _t0_99, 32), 15), _t0_101);
    _t0_104 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_99, _t0_99, 49), _mm256_permute2f128_pd(_t0_99, _t0_99, 49), 0), _t0_101);
    _t0_105 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_99, _t0_99, 49), _mm256_permute2f128_pd(_t0_99, _t0_99, 49), 15), _t0_101);

    // 4-BLAC: 4x4 + 4x4
    _t0_106 = _mm256_add_pd(_t0_95, _t0_102);
    _t0_107 = _mm256_add_pd(_t0_96, _t0_103);
    _t0_108 = _mm256_add_pd(_t0_97, _t0_104);
    _t0_109 = _mm256_add_pd(_t0_98, _t0_105);

    // 4-BLAC: 4x4 - 4x4
    _t0_110 = _mm256_sub_pd(_t0_88, _t0_106);
    _t0_111 = _mm256_sub_pd(_t0_89, _t0_107);
    _t0_112 = _mm256_sub_pd(_t0_90, _t0_108);
    _t0_113 = _mm256_sub_pd(_t0_91, _t0_109);

    // AVX Storer:

    // 4x4 -> 2x2 - LowSymm
    _t0_19 = _t0_110;
    _t0_20 = _t0_111;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 2), ( G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_114 = _t0_19;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t0_115 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_116 = _t0_2;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_117 = _mm256_mul_pd(_t0_115, _t0_116);

    // 4-BLAC: 1x4 / 1x4
    _t0_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_114), _mm256_castpd256_pd128(_t0_117)));

    // AVX Storer:
    _t0_19 = _t0_118;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) Kro G(h(1, 28, fi4 + 2), X[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_119 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_20, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_120 = _t0_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_121 = _t0_19;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_122 = _mm256_mul_pd(_t0_120, _t0_121);

    // 4-BLAC: 1x4 - 1x4
    _t0_123 = _mm256_sub_pd(_t0_119, _t0_122);

    // AVX Storer:
    _t0_21 = _t0_123;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_124 = _t0_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_125 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_126 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_127 = _mm256_add_pd(_t0_125, _t0_126);

    // 4-BLAC: 1x4 / 1x4
    _t0_128 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_124), _mm256_castpd256_pd128(_t0_127)));

    // AVX Storer:
    _t0_21 = _t0_128;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 3)) - ( ( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) Kro T( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) ) ) + ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 2)) Kro T( G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 2)) ) ) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_129 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_20, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_130 = _t0_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_131 = _t0_21;

    // 4-BLAC: (4x1)^T
    _t0_132 = _t0_131;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_133 = _mm256_mul_pd(_t0_130, _t0_132);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_134 = _t0_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_135 = _t0_1;

    // 4-BLAC: (4x1)^T
    _t0_136 = _t0_135;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_137 = _mm256_mul_pd(_t0_134, _t0_136);

    // 4-BLAC: 1x4 + 1x4
    _t0_138 = _mm256_add_pd(_t0_133, _t0_137);

    // 4-BLAC: 1x4 - 1x4
    _t0_139 = _mm256_sub_pd(_t0_129, _t0_138);

    // AVX Storer:
    _t0_22 = _t0_139;

    // Generating : X[28,28] = S(h(1, 28, fi4 + 3), ( G(h(1, 28, fi4 + 3), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_140 = _t0_22;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t0_141 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_142 = _t0_0;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_143 = _mm256_mul_pd(_t0_141, _t0_142);

    // 4-BLAC: 1x4 / 1x4
    _t0_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_140), _mm256_castpd256_pd128(_t0_143)));

    // AVX Storer:
    _t0_22 = _t0_144;

    // Generating : X[28,28] = Sum_{i86} ( S(h(4, 28, fi4 + i86 + 4), ( G(h(4, 28, fi4 + i86 + 4), X[28,28],h(4, 28, fi4)) - ( G(h(4, 28, fi4 + i86 + 4), L[28,28],h(4, 28, fi4)) * G(h(4, 28, fi4), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4)) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t0_145 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_7, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_9, _t0_13), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32), 0), 32);
    _t0_146 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_9, _t0_13), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32), 3), 32);
    _t0_147 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32), 3), 12);
    _t0_148 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32);
    _mm256_maskstore_pd(C + 29*fi4 + 28, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_8);
    _mm256_maskstore_pd(C + 29*fi4 + 56, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_8, _t0_8, 1));
    _mm256_maskstore_pd(C + 29*fi4 + 84, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t0_8, _t0_8, 129));
    _mm256_maskstore_pd(C + 29*fi4 + 56, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_10);
    _mm256_maskstore_pd(C + 29*fi4 + 84, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_10, _t0_10, 1));
    _mm256_maskstore_pd(C + 29*fi4 + 57, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_14);
    _mm256_maskstore_pd(C + 29*fi4 + 57, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_16);
    _mm256_maskstore_pd(C + 29*fi4 + 85, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_16, _t0_16, 1));
    _mm256_maskstore_pd(C + 29*fi4 + 86, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_20);

    for( int i86 = 0; i86 <= -fi4 + 23; i86+=4 ) {
      _t1_20 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 112);
      _t1_21 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 140);
      _t1_22 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 168);
      _t1_23 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 196);
      _t1_15 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 112);
      _t1_14 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 113);
      _t1_13 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 114);
      _t1_12 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 115);
      _t1_11 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 140);
      _t1_10 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 141);
      _t1_9 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 142);
      _t1_8 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 143);
      _t1_7 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 168);
      _t1_6 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 169);
      _t1_5 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 170);
      _t1_4 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 171);
      _t1_3 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 196);
      _t1_2 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 197);
      _t1_1 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 198);
      _t1_0 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 199);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t1_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_7, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_9, _t0_13), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32), 0), 32);
      _t1_25 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_9, _t0_13), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32), 3), 32);
      _t1_26 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_17), _mm256_unpacklo_pd(_t0_19, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32), 3), 12);
      _t1_27 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_12, _t0_18), _mm256_unpacklo_pd(_t0_21, _t0_22), 32);

      // 4-BLAC: 4x4 * 4x4
      _t1_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_15, _t1_24), _mm256_mul_pd(_t1_14, _t1_25)), _mm256_add_pd(_mm256_mul_pd(_t1_13, _t1_26), _mm256_mul_pd(_t1_12, _t1_27)));
      _t1_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_11, _t1_24), _mm256_mul_pd(_t1_10, _t1_25)), _mm256_add_pd(_mm256_mul_pd(_t1_9, _t1_26), _mm256_mul_pd(_t1_8, _t1_27)));
      _t1_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_7, _t1_24), _mm256_mul_pd(_t1_6, _t1_25)), _mm256_add_pd(_mm256_mul_pd(_t1_5, _t1_26), _mm256_mul_pd(_t1_4, _t1_27)));
      _t1_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_3, _t1_24), _mm256_mul_pd(_t1_2, _t1_25)), _mm256_add_pd(_mm256_mul_pd(_t1_1, _t1_26), _mm256_mul_pd(_t1_0, _t1_27)));

      // 4-BLAC: 4x4 - 4x4
      _t1_20 = _mm256_sub_pd(_t1_20, _t1_16);
      _t1_21 = _mm256_sub_pd(_t1_21, _t1_17);
      _t1_22 = _mm256_sub_pd(_t1_22, _t1_18);
      _t1_23 = _mm256_sub_pd(_t1_23, _t1_19);

      // AVX Storer:
      _asm256_storeu_pd(C + 29*fi4 + 28*i86 + 112, _t1_20);
      _asm256_storeu_pd(C + 29*fi4 + 28*i86 + 140, _t1_21);
      _asm256_storeu_pd(C + 29*fi4 + 28*i86 + 168, _t1_22);
      _asm256_storeu_pd(C + 29*fi4 + 28*i86 + 196, _t1_23);
    }

    for( int fi123 = 0; fi123 <= -fi4 + 19; fi123+=4 ) {
      _t2_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 112])));
      _t2_9 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi123 + 29*fi4 + 116])));
      _t2_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 113])));
      _t2_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 28])));
      _t2_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 114])));
      _t2_7 = _mm256_maskload_pd(L + 29*fi4 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t2_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 115])));
      _t2_6 = _mm256_maskload_pd(L + 29*fi4 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t2_14 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 140);
      _t2_15 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 168);
      _t2_16 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 196);
      _t2_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi123 + 29*fi4 + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi123 + 29*fi4 + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi123 + 29*fi4 + 200)), 32);
      _t2_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi123 + 29*fi4 + 145])));
      _t2_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi123 + 29*fi4 + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi123 + 29*fi4 + 201)), 0);
      _t2_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi123 + 29*fi4 + 174])));
      _t2_1 = _mm256_broadcast_sd(&(L[29*fi123 + 29*fi4 + 202]));
      _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi123 + 29*fi4 + 203])));

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_30 = _t2_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_31 = _t2_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_32 = _t0_6;

      // 4-BLAC: 1x4 + 1x4
      _t2_33 = _mm256_add_pd(_t2_31, _t2_32);

      // 4-BLAC: 1x4 / 1x4
      _t2_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_30), _mm256_castpd256_pd128(_t2_33)));

      // AVX Storer:
      _t2_10 = _t2_34;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_35 = _t2_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_36 = _t2_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_37 = _t2_8;

      // 4-BLAC: (4x1)^T
      _t2_38 = _t2_37;

      // 4-BLAC: 1x4 Kro 1x4
      _t2_39 = _mm256_mul_pd(_t2_36, _t2_38);

      // 4-BLAC: 1x4 - 1x4
      _t2_40 = _mm256_sub_pd(_t2_35, _t2_39);

      // AVX Storer:
      _t2_11 = _t2_40;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_41 = _t2_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_42 = _t2_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_43 = _t0_4;

      // 4-BLAC: 1x4 + 1x4
      _t2_44 = _mm256_add_pd(_t2_42, _t2_43);

      // 4-BLAC: 1x4 / 1x4
      _t2_45 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_41), _mm256_castpd256_pd128(_t2_44)));

      // AVX Storer:
      _t2_11 = _t2_45;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_46 = _t2_12;

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_47 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_10, _t2_11), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_48 = _t2_7;

      // 4-BLAC: (1x4)^T
      _t2_49 = _t2_48;

      // 4-BLAC: 1x4 * 4x1
      _t2_50 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_47, _t2_49), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_47, _t2_49), _mm256_mul_pd(_t2_47, _t2_49), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_47, _t2_49), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_47, _t2_49), _mm256_mul_pd(_t2_47, _t2_49), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_47, _t2_49), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_47, _t2_49), _mm256_mul_pd(_t2_47, _t2_49), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_51 = _mm256_sub_pd(_t2_46, _t2_50);

      // AVX Storer:
      _t2_12 = _t2_51;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_52 = _t2_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_53 = _t2_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_54 = _t0_2;

      // 4-BLAC: 1x4 + 1x4
      _t2_55 = _mm256_add_pd(_t2_53, _t2_54);

      // 4-BLAC: 1x4 / 1x4
      _t2_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_52), _mm256_castpd256_pd128(_t2_55)));

      // AVX Storer:
      _t2_12 = _t2_56;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_57 = _t2_13;

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_58 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_10, _t2_11), _mm256_unpacklo_pd(_t2_12, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_59 = _t2_6;

      // 4-BLAC: (1x4)^T
      _t2_60 = _t2_59;

      // 4-BLAC: 1x4 * 4x1
      _t2_61 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_58, _t2_60), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_58, _t2_60), _mm256_mul_pd(_t2_58, _t2_60), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_58, _t2_60), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_58, _t2_60), _mm256_mul_pd(_t2_58, _t2_60), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_58, _t2_60), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_58, _t2_60), _mm256_mul_pd(_t2_58, _t2_60), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_62 = _mm256_sub_pd(_t2_57, _t2_61);

      // AVX Storer:
      _t2_13 = _t2_62;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 4), ( G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi123 + fi4 + 4), L[28,28],h(1, 28, fi123 + fi4 + 4)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_63 = _t2_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_64 = _t2_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_65 = _t0_0;

      // 4-BLAC: 1x4 + 1x4
      _t2_66 = _mm256_add_pd(_t2_64, _t2_65);

      // 4-BLAC: 1x4 / 1x4
      _t2_67 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_63), _mm256_castpd256_pd128(_t2_66)));

      // AVX Storer:
      _t2_13 = _t2_67;

      // Generating : X[28,28] = S(h(3, 28, fi123 + fi4 + 5), ( G(h(3, 28, fi123 + fi4 + 5), X[28,28],h(4, 28, fi4)) - ( G(h(3, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 4)) * G(h(1, 28, fi123 + fi4 + 4), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4))

      // AVX Loader:

      // 3x4 -> 4x4
      _t2_68 = _t2_14;
      _t2_69 = _t2_15;
      _t2_70 = _t2_16;
      _t2_71 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t2_72 = _t2_5;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t2_73 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_72, _t2_72, 32), _mm256_permute2f128_pd(_t2_72, _t2_72, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_10, _t2_11), _mm256_unpacklo_pd(_t2_12, _t2_13), 32));
      _t2_74 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_72, _t2_72, 32), _mm256_permute2f128_pd(_t2_72, _t2_72, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_10, _t2_11), _mm256_unpacklo_pd(_t2_12, _t2_13), 32));
      _t2_75 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_72, _t2_72, 49), _mm256_permute2f128_pd(_t2_72, _t2_72, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_10, _t2_11), _mm256_unpacklo_pd(_t2_12, _t2_13), 32));
      _t2_76 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_72, _t2_72, 49), _mm256_permute2f128_pd(_t2_72, _t2_72, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_10, _t2_11), _mm256_unpacklo_pd(_t2_12, _t2_13), 32));

      // 4-BLAC: 4x4 - 4x4
      _t2_77 = _mm256_sub_pd(_t2_68, _t2_73);
      _t2_78 = _mm256_sub_pd(_t2_69, _t2_74);
      _t2_79 = _mm256_sub_pd(_t2_70, _t2_75);
      _t2_80 = _mm256_sub_pd(_t2_71, _t2_76);

      // AVX Storer:
      _t2_14 = _t2_77;
      _t2_15 = _t2_78;
      _t2_16 = _t2_79;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_81 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_14, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_82 = _t2_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_83 = _t0_6;

      // 4-BLAC: 1x4 + 1x4
      _t2_84 = _mm256_add_pd(_t2_82, _t2_83);

      // 4-BLAC: 1x4 / 1x4
      _t2_85 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_81), _mm256_castpd256_pd128(_t2_84)));

      // AVX Storer:
      _t2_17 = _t2_85;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_86 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_14, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_87 = _t2_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_88 = _t2_8;

      // 4-BLAC: (4x1)^T
      _t2_89 = _t2_88;

      // 4-BLAC: 1x4 Kro 1x4
      _t2_90 = _mm256_mul_pd(_t2_87, _t2_89);

      // 4-BLAC: 1x4 - 1x4
      _t2_91 = _mm256_sub_pd(_t2_86, _t2_90);

      // AVX Storer:
      _t2_18 = _t2_91;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_92 = _t2_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_93 = _t2_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_94 = _t0_4;

      // 4-BLAC: 1x4 + 1x4
      _t2_95 = _mm256_add_pd(_t2_93, _t2_94);

      // 4-BLAC: 1x4 / 1x4
      _t2_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_92), _mm256_castpd256_pd128(_t2_95)));

      // AVX Storer:
      _t2_18 = _t2_96;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_97 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_14, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_14, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_98 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_17, _t2_18), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_99 = _t2_7;

      // 4-BLAC: (1x4)^T
      _t2_100 = _t2_99;

      // 4-BLAC: 1x4 * 4x1
      _t2_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_98, _t2_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_98, _t2_100), _mm256_mul_pd(_t2_98, _t2_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_98, _t2_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_98, _t2_100), _mm256_mul_pd(_t2_98, _t2_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_98, _t2_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_98, _t2_100), _mm256_mul_pd(_t2_98, _t2_100), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_102 = _mm256_sub_pd(_t2_97, _t2_101);

      // AVX Storer:
      _t2_19 = _t2_102;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_103 = _t2_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_104 = _t2_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_105 = _t0_2;

      // 4-BLAC: 1x4 + 1x4
      _t2_106 = _mm256_add_pd(_t2_104, _t2_105);

      // 4-BLAC: 1x4 / 1x4
      _t2_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_103), _mm256_castpd256_pd128(_t2_106)));

      // AVX Storer:
      _t2_19 = _t2_107;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_108 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_14, _t2_14, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_17, _t2_18), _mm256_unpacklo_pd(_t2_19, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_110 = _t2_6;

      // 4-BLAC: (1x4)^T
      _t2_111 = _t2_110;

      // 4-BLAC: 1x4 * 4x1
      _t2_112 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_109, _t2_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_109, _t2_111), _mm256_mul_pd(_t2_109, _t2_111), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_109, _t2_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_109, _t2_111), _mm256_mul_pd(_t2_109, _t2_111), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_109, _t2_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_109, _t2_111), _mm256_mul_pd(_t2_109, _t2_111), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_113 = _mm256_sub_pd(_t2_108, _t2_112);

      // AVX Storer:
      _t2_20 = _t2_113;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 5), ( G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi123 + fi4 + 5), L[28,28],h(1, 28, fi123 + fi4 + 5)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_114 = _t2_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_115 = _t2_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_116 = _t0_0;

      // 4-BLAC: 1x4 + 1x4
      _t2_117 = _mm256_add_pd(_t2_115, _t2_116);

      // 4-BLAC: 1x4 / 1x4
      _t2_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_114), _mm256_castpd256_pd128(_t2_117)));

      // AVX Storer:
      _t2_20 = _t2_118;

      // Generating : X[28,28] = S(h(2, 28, fi123 + fi4 + 6), ( G(h(2, 28, fi123 + fi4 + 6), X[28,28],h(4, 28, fi4)) - ( G(h(2, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 5)) * G(h(1, 28, fi123 + fi4 + 5), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4))

      // AVX Loader:

      // 2x4 -> 4x4
      _t2_119 = _t2_15;
      _t2_120 = _t2_16;
      _t2_121 = _mm256_setzero_pd();
      _t2_122 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t2_123 = _t2_3;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t2_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_123, _t2_123, 32), _mm256_permute2f128_pd(_t2_123, _t2_123, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_17, _t2_18), _mm256_unpacklo_pd(_t2_19, _t2_20), 32));
      _t2_125 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_123, _t2_123, 32), _mm256_permute2f128_pd(_t2_123, _t2_123, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_17, _t2_18), _mm256_unpacklo_pd(_t2_19, _t2_20), 32));
      _t2_126 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_123, _t2_123, 49), _mm256_permute2f128_pd(_t2_123, _t2_123, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_17, _t2_18), _mm256_unpacklo_pd(_t2_19, _t2_20), 32));
      _t2_127 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_123, _t2_123, 49), _mm256_permute2f128_pd(_t2_123, _t2_123, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_17, _t2_18), _mm256_unpacklo_pd(_t2_19, _t2_20), 32));

      // 4-BLAC: 4x4 - 4x4
      _t2_128 = _mm256_sub_pd(_t2_119, _t2_124);
      _t2_129 = _mm256_sub_pd(_t2_120, _t2_125);
      _t2_130 = _mm256_sub_pd(_t2_121, _t2_126);
      _t2_131 = _mm256_sub_pd(_t2_122, _t2_127);

      // AVX Storer:
      _t2_15 = _t2_128;
      _t2_16 = _t2_129;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_132 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_15, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_133 = _t2_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_134 = _t0_6;

      // 4-BLAC: 1x4 + 1x4
      _t2_135 = _mm256_add_pd(_t2_133, _t2_134);

      // 4-BLAC: 1x4 / 1x4
      _t2_136 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_132), _mm256_castpd256_pd128(_t2_135)));

      // AVX Storer:
      _t2_21 = _t2_136;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_137 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_15, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_138 = _t2_21;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_139 = _t2_8;

      // 4-BLAC: (4x1)^T
      _t2_140 = _t2_139;

      // 4-BLAC: 1x4 Kro 1x4
      _t2_141 = _mm256_mul_pd(_t2_138, _t2_140);

      // 4-BLAC: 1x4 - 1x4
      _t2_142 = _mm256_sub_pd(_t2_137, _t2_141);

      // AVX Storer:
      _t2_22 = _t2_142;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_143 = _t2_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_144 = _t2_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_145 = _t0_4;

      // 4-BLAC: 1x4 + 1x4
      _t2_146 = _mm256_add_pd(_t2_144, _t2_145);

      // 4-BLAC: 1x4 / 1x4
      _t2_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_143), _mm256_castpd256_pd128(_t2_146)));

      // AVX Storer:
      _t2_22 = _t2_147;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_148 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_15, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_15, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_149 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_21, _t2_22), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_150 = _t2_7;

      // 4-BLAC: (1x4)^T
      _t2_151 = _t2_150;

      // 4-BLAC: 1x4 * 4x1
      _t2_152 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_149, _t2_151), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_149, _t2_151), _mm256_mul_pd(_t2_149, _t2_151), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_149, _t2_151), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_149, _t2_151), _mm256_mul_pd(_t2_149, _t2_151), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_149, _t2_151), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_149, _t2_151), _mm256_mul_pd(_t2_149, _t2_151), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_153 = _mm256_sub_pd(_t2_148, _t2_152);

      // AVX Storer:
      _t2_23 = _t2_153;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_154 = _t2_23;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_155 = _t2_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_156 = _t0_2;

      // 4-BLAC: 1x4 + 1x4
      _t2_157 = _mm256_add_pd(_t2_155, _t2_156);

      // 4-BLAC: 1x4 / 1x4
      _t2_158 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_154), _mm256_castpd256_pd128(_t2_157)));

      // AVX Storer:
      _t2_23 = _t2_158;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_159 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_15, _t2_15, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_160 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_21, _t2_22), _mm256_unpacklo_pd(_t2_23, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_161 = _t2_6;

      // 4-BLAC: (1x4)^T
      _t2_162 = _t2_161;

      // 4-BLAC: 1x4 * 4x1
      _t2_163 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_160, _t2_162), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_160, _t2_162), _mm256_mul_pd(_t2_160, _t2_162), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_160, _t2_162), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_160, _t2_162), _mm256_mul_pd(_t2_160, _t2_162), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_160, _t2_162), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_160, _t2_162), _mm256_mul_pd(_t2_160, _t2_162), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_164 = _mm256_sub_pd(_t2_159, _t2_163);

      // AVX Storer:
      _t2_24 = _t2_164;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 6), ( G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi123 + fi4 + 6), L[28,28],h(1, 28, fi123 + fi4 + 6)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_165 = _t2_24;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_166 = _t2_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_167 = _t0_0;

      // 4-BLAC: 1x4 + 1x4
      _t2_168 = _mm256_add_pd(_t2_166, _t2_167);

      // 4-BLAC: 1x4 / 1x4
      _t2_169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_165), _mm256_castpd256_pd128(_t2_168)));

      // AVX Storer:
      _t2_24 = _t2_169;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(4, 28, fi4)) - ( G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 6)) Kro G(h(1, 28, fi123 + fi4 + 6), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_170 = _t2_1;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t2_29 = _mm256_mul_pd(_t2_170, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_21, _t2_22), _mm256_unpacklo_pd(_t2_23, _t2_24), 32));

      // 4-BLAC: 1x4 - 1x4
      _t2_16 = _mm256_sub_pd(_t2_16, _t2_29);

      // AVX Storer:

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_171 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_172 = _t2_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_173 = _t0_6;

      // 4-BLAC: 1x4 + 1x4
      _t2_174 = _mm256_add_pd(_t2_172, _t2_173);

      // 4-BLAC: 1x4 / 1x4
      _t2_175 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_171), _mm256_castpd256_pd128(_t2_174)));

      // AVX Storer:
      _t2_25 = _t2_175;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_176 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_177 = _t2_25;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_178 = _t2_8;

      // 4-BLAC: (4x1)^T
      _t2_179 = _t2_178;

      // 4-BLAC: 1x4 Kro 1x4
      _t2_180 = _mm256_mul_pd(_t2_177, _t2_179);

      // 4-BLAC: 1x4 - 1x4
      _t2_181 = _mm256_sub_pd(_t2_176, _t2_180);

      // AVX Storer:
      _t2_26 = _t2_181;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_182 = _t2_26;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_183 = _t2_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_184 = _t0_4;

      // 4-BLAC: 1x4 + 1x4
      _t2_185 = _mm256_add_pd(_t2_183, _t2_184);

      // 4-BLAC: 1x4 / 1x4
      _t2_186 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_182), _mm256_castpd256_pd128(_t2_185)));

      // AVX Storer:
      _t2_26 = _t2_186;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_187 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_16, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_188 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_25, _t2_26), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t2_189 = _t2_7;

      // 4-BLAC: (1x4)^T
      _t2_190 = _t2_189;

      // 4-BLAC: 1x4 * 4x1
      _t2_191 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_188, _t2_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_188, _t2_190), _mm256_mul_pd(_t2_188, _t2_190), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_188, _t2_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_188, _t2_190), _mm256_mul_pd(_t2_188, _t2_190), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_188, _t2_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_188, _t2_190), _mm256_mul_pd(_t2_188, _t2_190), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_192 = _mm256_sub_pd(_t2_187, _t2_191);

      // AVX Storer:
      _t2_27 = _t2_192;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_193 = _t2_27;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_194 = _t2_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_195 = _t0_2;

      // 4-BLAC: 1x4 + 1x4
      _t2_196 = _mm256_add_pd(_t2_194, _t2_195);

      // 4-BLAC: 1x4 / 1x4
      _t2_197 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_193), _mm256_castpd256_pd128(_t2_196)));

      // AVX Storer:
      _t2_27 = _t2_197;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_198 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_16, _t2_16, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_199 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_25, _t2_26), _mm256_unpacklo_pd(_t2_27, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t2_200 = _t2_6;

      // 4-BLAC: (1x4)^T
      _t2_201 = _t2_200;

      // 4-BLAC: 1x4 * 4x1
      _t2_202 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_199, _t2_201), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_199, _t2_201), _mm256_mul_pd(_t2_199, _t2_201), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_199, _t2_201), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_199, _t2_201), _mm256_mul_pd(_t2_199, _t2_201), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_199, _t2_201), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_199, _t2_201), _mm256_mul_pd(_t2_199, _t2_201), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t2_203 = _mm256_sub_pd(_t2_198, _t2_202);

      // AVX Storer:
      _t2_28 = _t2_203;

      // Generating : X[28,28] = S(h(1, 28, fi123 + fi4 + 7), ( G(h(1, 28, fi123 + fi4 + 7), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi123 + fi4 + 7), L[28,28],h(1, 28, fi123 + fi4 + 7)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_204 = _t2_28;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_205 = _t2_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t2_206 = _t0_0;

      // 4-BLAC: 1x4 + 1x4
      _t2_207 = _mm256_add_pd(_t2_205, _t2_206);

      // 4-BLAC: 1x4 / 1x4
      _t2_208 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_204), _mm256_castpd256_pd128(_t2_207)));

      // AVX Storer:
      _t2_28 = _t2_208;

      // Generating : X[28,28] = Sum_{i86} ( S(h(4, 28, fi123 + fi4 + i86 + 8), ( G(h(4, 28, fi123 + fi4 + i86 + 8), X[28,28],h(4, 28, fi4)) - ( G(h(4, 28, fi123 + fi4 + i86 + 8), L[28,28],h(4, 28, fi123 + fi4 + 4)) * G(h(4, 28, fi123 + fi4 + 4), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4)) )
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 112]), _mm256_castpd256_pd128(_t2_10));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 113]), _mm256_castpd256_pd128(_t2_11));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 114]), _mm256_castpd256_pd128(_t2_12));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 115]), _mm256_castpd256_pd128(_t2_13));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 140]), _mm256_castpd256_pd128(_t2_17));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 141]), _mm256_castpd256_pd128(_t2_18));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 142]), _mm256_castpd256_pd128(_t2_19));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 143]), _mm256_castpd256_pd128(_t2_20));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 168]), _mm256_castpd256_pd128(_t2_21));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 169]), _mm256_castpd256_pd128(_t2_22));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 170]), _mm256_castpd256_pd128(_t2_23));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 171]), _mm256_castpd256_pd128(_t2_24));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 196]), _mm256_castpd256_pd128(_t2_25));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 197]), _mm256_castpd256_pd128(_t2_26));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 198]), _mm256_castpd256_pd128(_t2_27));
      _mm_store_sd(&(C[28*fi123 + 29*fi4 + 199]), _mm256_castpd256_pd128(_t2_28));

      for( int i86 = 0; i86 <= -fi123 - fi4 + 19; i86+=4 ) {
        _t3_36 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 224);
        _t3_37 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 252);
        _t3_38 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 280);
        _t3_39 = _asm256_loadu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 308);
        _t3_31 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 228);
        _t3_30 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 229);
        _t3_29 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 230);
        _t3_28 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 231);
        _t3_27 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 256);
        _t3_26 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 257);
        _t3_25 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 258);
        _t3_24 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 259);
        _t3_23 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 284);
        _t3_22 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 285);
        _t3_21 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 286);
        _t3_20 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 287);
        _t3_19 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 312);
        _t3_18 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 313);
        _t3_17 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 314);
        _t3_16 = _mm256_broadcast_sd(L + 29*fi123 + 29*fi4 + 28*i86 + 315);
        _t3_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 112])));
        _t3_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 113])));
        _t3_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 114])));
        _t3_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 115])));
        _t3_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 140])));
        _t3_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 141])));
        _t3_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 142])));
        _t3_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 143])));
        _t3_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 168])));
        _t3_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 169])));
        _t3_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 170])));
        _t3_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 171])));
        _t3_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 196])));
        _t3_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 197])));
        _t3_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 198])));
        _t3_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi123 + 29*fi4 + 199])));

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t3_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_31, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_30, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_29, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_28, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));
        _t3_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_27, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_26, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_25, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_24, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));
        _t3_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_23, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_22, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_21, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_20, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));
        _t3_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_19, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_18, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_17, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_16, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));

        // 4-BLAC: 4x4 - 4x4
        _t3_36 = _mm256_sub_pd(_t3_36, _t3_32);
        _t3_37 = _mm256_sub_pd(_t3_37, _t3_33);
        _t3_38 = _mm256_sub_pd(_t3_38, _t3_34);
        _t3_39 = _mm256_sub_pd(_t3_39, _t3_35);

        // AVX Storer:
        _asm256_storeu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 224, _t3_36);
        _asm256_storeu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 252, _t3_37);
        _asm256_storeu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 280, _t3_38);
        _asm256_storeu_pd(C + 28*fi123 + 29*fi4 + 28*i86 + 308, _t3_39);
      }
    }
    _t4_54 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 112])));
    _t4_49 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 29*Max(0, -fi4 + 20) + 116])));
    _t4_55 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 113])));
    _t4_48 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 28])));
    _t4_56 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 114])));
    _t4_47 = _mm256_maskload_pd(L + 29*fi4 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t4_57 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 115])));
    _t4_46 = _mm256_maskload_pd(L + 29*fi4 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t4_58 = _asm256_loadu_pd(C + 29*fi4 + 28*Max(0, -fi4 + 20) + 140);
    _t4_59 = _asm256_loadu_pd(C + 29*fi4 + 28*Max(0, -fi4 + 20) + 168);
    _t4_60 = _asm256_loadu_pd(C + 29*fi4 + 28*Max(0, -fi4 + 20) + 196);
    _t4_45 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 29*Max(0, -fi4 + 20) + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 29*Max(0, -fi4 + 20) + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 29*Max(0, -fi4 + 20) + 200)), 32);
    _t4_44 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 29*Max(0, -fi4 + 20) + 145])));
    _t4_43 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 29*Max(0, -fi4 + 20) + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi4 + 29*Max(0, -fi4 + 20) + 201)), 0);
    _t4_42 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 29*Max(0, -fi4 + 20) + 174])));
    _t4_41 = _mm256_broadcast_sd(&(L[29*fi4 + 29*Max(0, -fi4 + 20) + 202]));
    _t4_40 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi4 + 29*Max(0, -fi4 + 20) + 203])));
    _t4_73 = _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi4 + 116));
    _t4_74 = _mm256_maskload_pd(C + 29*fi4 + 144, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t4_75 = _mm256_maskload_pd(C + 29*fi4 + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t4_76 = _asm256_loadu_pd(C + 29*fi4 + 200);
    _t4_39 = _mm256_broadcast_sd(L + 29*fi4 + 112);
    _t4_38 = _mm256_broadcast_sd(L + 29*fi4 + 113);
    _t4_37 = _mm256_broadcast_sd(L + 29*fi4 + 114);
    _t4_36 = _mm256_broadcast_sd(L + 29*fi4 + 115);
    _t4_35 = _mm256_broadcast_sd(L + 29*fi4 + 140);
    _t4_34 = _mm256_broadcast_sd(L + 29*fi4 + 141);
    _t4_33 = _mm256_broadcast_sd(L + 29*fi4 + 142);
    _t4_32 = _mm256_broadcast_sd(L + 29*fi4 + 143);
    _t4_31 = _mm256_broadcast_sd(L + 29*fi4 + 168);
    _t4_30 = _mm256_broadcast_sd(L + 29*fi4 + 169);
    _t4_29 = _mm256_broadcast_sd(L + 29*fi4 + 170);
    _t4_28 = _mm256_broadcast_sd(L + 29*fi4 + 171);
    _t4_27 = _mm256_broadcast_sd(L + 29*fi4 + 196);
    _t4_26 = _mm256_broadcast_sd(L + 29*fi4 + 197);
    _t4_25 = _mm256_broadcast_sd(L + 29*fi4 + 198);
    _t4_24 = _mm256_broadcast_sd(L + 29*fi4 + 199);
    _t4_23 = _asm256_loadu_pd(C + 29*fi4 + 112);
    _t4_22 = _asm256_loadu_pd(C + 29*fi4 + 140);
    _t4_21 = _asm256_loadu_pd(C + 29*fi4 + 168);
    _t4_20 = _asm256_loadu_pd(C + 29*fi4 + 196);
    _t4_19 = _mm256_broadcast_sd(C + 29*fi4 + 112);
    _t4_18 = _mm256_broadcast_sd(C + 29*fi4 + 113);
    _t4_17 = _mm256_broadcast_sd(C + 29*fi4 + 114);
    _t4_16 = _mm256_broadcast_sd(C + 29*fi4 + 115);
    _t4_15 = _mm256_broadcast_sd(C + 29*fi4 + 140);
    _t4_14 = _mm256_broadcast_sd(C + 29*fi4 + 141);
    _t4_13 = _mm256_broadcast_sd(C + 29*fi4 + 142);
    _t4_12 = _mm256_broadcast_sd(C + 29*fi4 + 143);
    _t4_11 = _mm256_broadcast_sd(C + 29*fi4 + 168);
    _t4_10 = _mm256_broadcast_sd(C + 29*fi4 + 169);
    _t4_9 = _mm256_broadcast_sd(C + 29*fi4 + 170);
    _t4_8 = _mm256_broadcast_sd(C + 29*fi4 + 171);
    _t4_7 = _mm256_broadcast_sd(C + 29*fi4 + 196);
    _t4_6 = _mm256_broadcast_sd(C + 29*fi4 + 197);
    _t4_5 = _mm256_broadcast_sd(C + 29*fi4 + 198);
    _t4_4 = _mm256_broadcast_sd(C + 29*fi4 + 199);
    _t4_3 = _asm256_loadu_pd(L + 29*fi4 + 112);
    _t4_2 = _asm256_loadu_pd(L + 29*fi4 + 140);
    _t4_1 = _asm256_loadu_pd(L + 29*fi4 + 168);
    _t4_0 = _asm256_loadu_pd(L + 29*fi4 + 196);

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_96 = _t4_54;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_97 = _t4_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_98 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_99 = _mm256_add_pd(_t4_97, _t4_98);

    // 4-BLAC: 1x4 / 1x4
    _t4_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_96), _mm256_castpd256_pd128(_t4_99)));

    // AVX Storer:
    _t4_54 = _t4_100;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_101 = _t4_55;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_102 = _t4_54;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_103 = _t4_48;

    // 4-BLAC: (4x1)^T
    _t4_104 = _t4_103;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_105 = _mm256_mul_pd(_t4_102, _t4_104);

    // 4-BLAC: 1x4 - 1x4
    _t4_106 = _mm256_sub_pd(_t4_101, _t4_105);

    // AVX Storer:
    _t4_55 = _t4_106;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_107 = _t4_55;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_108 = _t4_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_109 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t4_110 = _mm256_add_pd(_t4_108, _t4_109);

    // 4-BLAC: 1x4 / 1x4
    _t4_111 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_107), _mm256_castpd256_pd128(_t4_110)));

    // AVX Storer:
    _t4_55 = _t4_111;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_112 = _t4_56;

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_113 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_54, _t4_55), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_114 = _t4_47;

    // 4-BLAC: (1x4)^T
    _t4_115 = _t4_114;

    // 4-BLAC: 1x4 * 4x1
    _t4_116 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_113, _t4_115), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_113, _t4_115), _mm256_mul_pd(_t4_113, _t4_115), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_113, _t4_115), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_113, _t4_115), _mm256_mul_pd(_t4_113, _t4_115), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_113, _t4_115), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_113, _t4_115), _mm256_mul_pd(_t4_113, _t4_115), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_117 = _mm256_sub_pd(_t4_112, _t4_116);

    // AVX Storer:
    _t4_56 = _t4_117;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_118 = _t4_56;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_119 = _t4_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_120 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t4_121 = _mm256_add_pd(_t4_119, _t4_120);

    // 4-BLAC: 1x4 / 1x4
    _t4_122 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_118), _mm256_castpd256_pd128(_t4_121)));

    // AVX Storer:
    _t4_56 = _t4_122;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_123 = _t4_57;

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_124 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_54, _t4_55), _mm256_unpacklo_pd(_t4_56, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_125 = _t4_46;

    // 4-BLAC: (1x4)^T
    _t4_126 = _t4_125;

    // 4-BLAC: 1x4 * 4x1
    _t4_127 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_124, _t4_126), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_124, _t4_126), _mm256_mul_pd(_t4_124, _t4_126), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_124, _t4_126), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_124, _t4_126), _mm256_mul_pd(_t4_124, _t4_126), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_124, _t4_126), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_124, _t4_126), _mm256_mul_pd(_t4_124, _t4_126), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_128 = _mm256_sub_pd(_t4_123, _t4_127);

    // AVX Storer:
    _t4_57 = _t4_128;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_129 = _t4_57;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_130 = _t4_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_131 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t4_132 = _mm256_add_pd(_t4_130, _t4_131);

    // 4-BLAC: 1x4 / 1x4
    _t4_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_129), _mm256_castpd256_pd128(_t4_132)));

    // AVX Storer:
    _t4_57 = _t4_133;

    // Generating : X[28,28] = S(h(3, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(3, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(4, 28, fi4)) - ( G(h(3, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 4)) * G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 4), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t4_134 = _t4_58;
    _t4_135 = _t4_59;
    _t4_136 = _t4_60;
    _t4_137 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t4_138 = _t4_45;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t4_139 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_138, _t4_138, 32), _mm256_permute2f128_pd(_t4_138, _t4_138, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_54, _t4_55), _mm256_unpacklo_pd(_t4_56, _t4_57), 32));
    _t4_140 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_138, _t4_138, 32), _mm256_permute2f128_pd(_t4_138, _t4_138, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_54, _t4_55), _mm256_unpacklo_pd(_t4_56, _t4_57), 32));
    _t4_141 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_138, _t4_138, 49), _mm256_permute2f128_pd(_t4_138, _t4_138, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_54, _t4_55), _mm256_unpacklo_pd(_t4_56, _t4_57), 32));
    _t4_142 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_138, _t4_138, 49), _mm256_permute2f128_pd(_t4_138, _t4_138, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_54, _t4_55), _mm256_unpacklo_pd(_t4_56, _t4_57), 32));

    // 4-BLAC: 4x4 - 4x4
    _t4_143 = _mm256_sub_pd(_t4_134, _t4_139);
    _t4_144 = _mm256_sub_pd(_t4_135, _t4_140);
    _t4_145 = _mm256_sub_pd(_t4_136, _t4_141);
    _t4_146 = _mm256_sub_pd(_t4_137, _t4_142);

    // AVX Storer:
    _t4_58 = _t4_143;
    _t4_59 = _t4_144;
    _t4_60 = _t4_145;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_147 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_58, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_148 = _t4_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_149 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_150 = _mm256_add_pd(_t4_148, _t4_149);

    // 4-BLAC: 1x4 / 1x4
    _t4_151 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_147), _mm256_castpd256_pd128(_t4_150)));

    // AVX Storer:
    _t4_61 = _t4_151;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_152 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_58, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_153 = _t4_61;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_154 = _t4_48;

    // 4-BLAC: (4x1)^T
    _t4_155 = _t4_154;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_156 = _mm256_mul_pd(_t4_153, _t4_155);

    // 4-BLAC: 1x4 - 1x4
    _t4_157 = _mm256_sub_pd(_t4_152, _t4_156);

    // AVX Storer:
    _t4_62 = _t4_157;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_158 = _t4_62;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_159 = _t4_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_160 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t4_161 = _mm256_add_pd(_t4_159, _t4_160);

    // 4-BLAC: 1x4 / 1x4
    _t4_162 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_158), _mm256_castpd256_pd128(_t4_161)));

    // AVX Storer:
    _t4_62 = _t4_162;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_163 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_58, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_58, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_164 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_61, _t4_62), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_165 = _t4_47;

    // 4-BLAC: (1x4)^T
    _t4_166 = _t4_165;

    // 4-BLAC: 1x4 * 4x1
    _t4_167 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_mul_pd(_t4_164, _t4_166), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_mul_pd(_t4_164, _t4_166), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_mul_pd(_t4_164, _t4_166), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_168 = _mm256_sub_pd(_t4_163, _t4_167);

    // AVX Storer:
    _t4_63 = _t4_168;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_169 = _t4_63;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_170 = _t4_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_171 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t4_172 = _mm256_add_pd(_t4_170, _t4_171);

    // 4-BLAC: 1x4 / 1x4
    _t4_173 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_169), _mm256_castpd256_pd128(_t4_172)));

    // AVX Storer:
    _t4_63 = _t4_173;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_174 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_58, _t4_58, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_175 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_61, _t4_62), _mm256_unpacklo_pd(_t4_63, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_176 = _t4_46;

    // 4-BLAC: (1x4)^T
    _t4_177 = _t4_176;

    // 4-BLAC: 1x4 * 4x1
    _t4_178 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_175, _t4_177), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_175, _t4_177), _mm256_mul_pd(_t4_175, _t4_177), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_175, _t4_177), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_175, _t4_177), _mm256_mul_pd(_t4_175, _t4_177), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_175, _t4_177), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_175, _t4_177), _mm256_mul_pd(_t4_175, _t4_177), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_179 = _mm256_sub_pd(_t4_174, _t4_178);

    // AVX Storer:
    _t4_64 = _t4_179;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_180 = _t4_64;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_181 = _t4_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_182 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t4_183 = _mm256_add_pd(_t4_181, _t4_182);

    // 4-BLAC: 1x4 / 1x4
    _t4_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_180), _mm256_castpd256_pd128(_t4_183)));

    // AVX Storer:
    _t4_64 = _t4_184;

    // Generating : X[28,28] = S(h(2, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(2, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(4, 28, fi4)) - ( G(h(2, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 5)) * G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 5), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t4_185 = _t4_59;
    _t4_186 = _t4_60;
    _t4_187 = _mm256_setzero_pd();
    _t4_188 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t4_189 = _t4_43;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t4_190 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_189, _t4_189, 32), _mm256_permute2f128_pd(_t4_189, _t4_189, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_61, _t4_62), _mm256_unpacklo_pd(_t4_63, _t4_64), 32));
    _t4_191 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_189, _t4_189, 32), _mm256_permute2f128_pd(_t4_189, _t4_189, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_61, _t4_62), _mm256_unpacklo_pd(_t4_63, _t4_64), 32));
    _t4_192 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_189, _t4_189, 49), _mm256_permute2f128_pd(_t4_189, _t4_189, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_61, _t4_62), _mm256_unpacklo_pd(_t4_63, _t4_64), 32));
    _t4_193 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_189, _t4_189, 49), _mm256_permute2f128_pd(_t4_189, _t4_189, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_61, _t4_62), _mm256_unpacklo_pd(_t4_63, _t4_64), 32));

    // 4-BLAC: 4x4 - 4x4
    _t4_194 = _mm256_sub_pd(_t4_185, _t4_190);
    _t4_195 = _mm256_sub_pd(_t4_186, _t4_191);
    _t4_196 = _mm256_sub_pd(_t4_187, _t4_192);
    _t4_197 = _mm256_sub_pd(_t4_188, _t4_193);

    // AVX Storer:
    _t4_59 = _t4_194;
    _t4_60 = _t4_195;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_198 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_59, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_199 = _t4_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_200 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_201 = _mm256_add_pd(_t4_199, _t4_200);

    // 4-BLAC: 1x4 / 1x4
    _t4_202 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_198), _mm256_castpd256_pd128(_t4_201)));

    // AVX Storer:
    _t4_65 = _t4_202;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_203 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_59, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_204 = _t4_65;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_205 = _t4_48;

    // 4-BLAC: (4x1)^T
    _t4_206 = _t4_205;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_207 = _mm256_mul_pd(_t4_204, _t4_206);

    // 4-BLAC: 1x4 - 1x4
    _t4_208 = _mm256_sub_pd(_t4_203, _t4_207);

    // AVX Storer:
    _t4_66 = _t4_208;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_209 = _t4_66;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_210 = _t4_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_211 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t4_212 = _mm256_add_pd(_t4_210, _t4_211);

    // 4-BLAC: 1x4 / 1x4
    _t4_213 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_209), _mm256_castpd256_pd128(_t4_212)));

    // AVX Storer:
    _t4_66 = _t4_213;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_214 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_59, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_59, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_215 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_65, _t4_66), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_216 = _t4_47;

    // 4-BLAC: (1x4)^T
    _t4_217 = _t4_216;

    // 4-BLAC: 1x4 * 4x1
    _t4_218 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_215, _t4_217), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_215, _t4_217), _mm256_mul_pd(_t4_215, _t4_217), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_215, _t4_217), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_215, _t4_217), _mm256_mul_pd(_t4_215, _t4_217), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_215, _t4_217), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_215, _t4_217), _mm256_mul_pd(_t4_215, _t4_217), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_219 = _mm256_sub_pd(_t4_214, _t4_218);

    // AVX Storer:
    _t4_67 = _t4_219;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_220 = _t4_67;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_221 = _t4_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_222 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t4_223 = _mm256_add_pd(_t4_221, _t4_222);

    // 4-BLAC: 1x4 / 1x4
    _t4_224 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_220), _mm256_castpd256_pd128(_t4_223)));

    // AVX Storer:
    _t4_67 = _t4_224;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_225 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_59, _t4_59, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_226 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_65, _t4_66), _mm256_unpacklo_pd(_t4_67, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_227 = _t4_46;

    // 4-BLAC: (1x4)^T
    _t4_228 = _t4_227;

    // 4-BLAC: 1x4 * 4x1
    _t4_229 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_226, _t4_228), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_226, _t4_228), _mm256_mul_pd(_t4_226, _t4_228), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_226, _t4_228), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_226, _t4_228), _mm256_mul_pd(_t4_226, _t4_228), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_226, _t4_228), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_226, _t4_228), _mm256_mul_pd(_t4_226, _t4_228), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_230 = _mm256_sub_pd(_t4_225, _t4_229);

    // AVX Storer:
    _t4_68 = _t4_230;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_231 = _t4_68;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_232 = _t4_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_233 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t4_234 = _mm256_add_pd(_t4_232, _t4_233);

    // 4-BLAC: 1x4 / 1x4
    _t4_235 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_231), _mm256_castpd256_pd128(_t4_234)));

    // AVX Storer:
    _t4_68 = _t4_235;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(4, 28, fi4)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 6)) Kro G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 6), X[28,28],h(4, 28, fi4)) ) ),h(4, 28, fi4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_236 = _t4_41;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t4_77 = _mm256_mul_pd(_t4_236, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_65, _t4_66), _mm256_unpacklo_pd(_t4_67, _t4_68), 32));

    // 4-BLAC: 1x4 - 1x4
    _t4_60 = _mm256_sub_pd(_t4_60, _t4_77);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) + G(h(1, 28, fi4), L[28,28],h(1, 28, fi4)) ) ),h(1, 28, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_237 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_60, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_238 = _t4_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_239 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_240 = _mm256_add_pd(_t4_238, _t4_239);

    // 4-BLAC: 1x4 / 1x4
    _t4_241 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_237), _mm256_castpd256_pd128(_t4_240)));

    // AVX Storer:
    _t4_69 = _t4_241;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 1)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4)) Kro T( G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4)) ) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_242 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_60, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_243 = _t4_69;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_244 = _t4_48;

    // 4-BLAC: (4x1)^T
    _t4_245 = _t4_244;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_246 = _mm256_mul_pd(_t4_243, _t4_245);

    // 4-BLAC: 1x4 - 1x4
    _t4_247 = _mm256_sub_pd(_t4_242, _t4_246);

    // AVX Storer:
    _t4_70 = _t4_247;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 1)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) + G(h(1, 28, fi4 + 1), L[28,28],h(1, 28, fi4 + 1)) ) ),h(1, 28, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_248 = _t4_70;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_249 = _t4_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_250 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t4_251 = _mm256_add_pd(_t4_249, _t4_250);

    // 4-BLAC: 1x4 / 1x4
    _t4_252 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_248), _mm256_castpd256_pd128(_t4_251)));

    // AVX Storer:
    _t4_70 = _t4_252;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 2)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(2, 28, fi4)) * T( G(h(1, 28, fi4 + 2), L[28,28],h(2, 28, fi4)) ) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_253 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_60, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_60, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_254 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_69, _t4_70), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_255 = _t4_47;

    // 4-BLAC: (1x4)^T
    _t4_256 = _t4_255;

    // 4-BLAC: 1x4 * 4x1
    _t4_257 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_254, _t4_256), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_254, _t4_256), _mm256_mul_pd(_t4_254, _t4_256), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_254, _t4_256), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_254, _t4_256), _mm256_mul_pd(_t4_254, _t4_256), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_254, _t4_256), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_254, _t4_256), _mm256_mul_pd(_t4_254, _t4_256), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_258 = _mm256_sub_pd(_t4_253, _t4_257);

    // AVX Storer:
    _t4_71 = _t4_258;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 2)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) + G(h(1, 28, fi4 + 2), L[28,28],h(1, 28, fi4 + 2)) ) ),h(1, 28, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_259 = _t4_71;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_260 = _t4_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_261 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t4_262 = _mm256_add_pd(_t4_260, _t4_261);

    // 4-BLAC: 1x4 / 1x4
    _t4_263 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_259), _mm256_castpd256_pd128(_t4_262)));

    // AVX Storer:
    _t4_71 = _t4_263;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 3)) - ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(3, 28, fi4)) * T( G(h(1, 28, fi4 + 3), L[28,28],h(3, 28, fi4)) ) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_264 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_60, _t4_60, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_265 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_69, _t4_70), _mm256_unpacklo_pd(_t4_71, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_266 = _t4_46;

    // 4-BLAC: (1x4)^T
    _t4_267 = _t4_266;

    // 4-BLAC: 1x4 * 4x1
    _t4_268 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_265, _t4_267), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_265, _t4_267), _mm256_mul_pd(_t4_265, _t4_267), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_265, _t4_267), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_265, _t4_267), _mm256_mul_pd(_t4_265, _t4_267), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_265, _t4_267), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_265, _t4_267), _mm256_mul_pd(_t4_265, _t4_267), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t4_269 = _mm256_sub_pd(_t4_264, _t4_268);

    // AVX Storer:
    _t4_72 = _t4_269;

    // Generating : X[28,28] = S(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), X[28,28],h(1, 28, fi4 + 3)) Div ( G(h(1, 28, fi4 + Max(0, -fi4 + 20) + 7), L[28,28],h(1, 28, fi4 + Max(0, -fi4 + 20) + 7)) + G(h(1, 28, fi4 + 3), L[28,28],h(1, 28, fi4 + 3)) ) ),h(1, 28, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_270 = _t4_72;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_271 = _t4_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_272 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t4_90 = _mm256_add_pd(_t4_271, _t4_272);

    // 4-BLAC: 1x4 / 1x4
    _t4_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_270), _mm256_castpd256_pd128(_t4_90)));

    // AVX Storer:
    _t4_72 = _t4_91;

    // Generating : X[28,28] = ( S(h(4, 28, fi4 + 4), ( G(h(4, 28, fi4 + 4), X[28,28],h(4, 28, fi4 + 4)) - ( ( G(h(4, 28, fi4 + 4), L[28,28],h(4, 28, fi4)) * T( G(h(4, 28, fi4 + 4), X[28,28],h(4, 28, fi4)) ) ) + ( G(h(4, 28, fi4 + 4), X[28,28],h(4, 28, fi4)) * T( G(h(4, 28, fi4 + 4), L[28,28],h(4, 28, fi4)) ) ) ) ),h(4, 28, fi4 + 4)) + Sum_{i86} ( ( Sum_{i203} ( S(h(4, 28, fi4 + i86 + 4), ( G(h(4, 28, fi4 + i86 + 4), X[28,28],h(4, 28, fi4 + i203 + 4)) - ( ( G(h(4, 28, fi4 + i86 + 4), L[28,28],h(4, 28, fi4)) * T( G(h(4, 28, fi4 + i203 + 4), X[28,28],h(4, 28, fi4)) ) ) + ( G(h(4, 28, fi4 + i86 + 4), X[28,28],h(4, 28, fi4)) * T( G(h(4, 28, fi4 + i203 + 4), L[28,28],h(4, 28, fi4)) ) ) ) ),h(4, 28, fi4 + i203 + 4)) ) + S(h(4, 28, fi4 + i86 + 4), ( G(h(4, 28, fi4 + i86 + 4), X[28,28],h(4, 28, fi4 + i86 + 4)) - ( ( G(h(4, 28, fi4 + i86 + 4), L[28,28],h(4, 28, fi4)) * T( G(h(4, 28, fi4 + i86 + 4), X[28,28],h(4, 28, fi4)) ) ) + ( G(h(4, 28, fi4 + i86 + 4), X[28,28],h(4, 28, fi4)) * T( G(h(4, 28, fi4 + i86 + 4), L[28,28],h(4, 28, fi4)) ) ) ) ),h(4, 28, fi4 + i86 + 4)) ) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t4_92 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_73, _t4_74, 0), _mm256_shuffle_pd(_t4_75, _t4_76, 0), 32);
    _t4_93 = _mm256_permute2f128_pd(_t4_74, _mm256_shuffle_pd(_t4_75, _t4_76, 3), 32);
    _t4_94 = _mm256_blend_pd(_t4_75, _mm256_shuffle_pd(_t4_75, _t4_76, 3), 12);
    _t4_95 = _t4_76;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t4_273 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_23, _t4_22), _mm256_unpacklo_pd(_t4_21, _t4_20), 32);
    _t4_274 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_23, _t4_22), _mm256_unpackhi_pd(_t4_21, _t4_20), 32);
    _t4_275 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_23, _t4_22), _mm256_unpacklo_pd(_t4_21, _t4_20), 49);
    _t4_276 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_23, _t4_22), _mm256_unpackhi_pd(_t4_21, _t4_20), 49);

    // 4-BLAC: 4x4 * 4x4
    _t4_78 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_39, _t4_273), _mm256_mul_pd(_t4_38, _t4_274)), _mm256_add_pd(_mm256_mul_pd(_t4_37, _t4_275), _mm256_mul_pd(_t4_36, _t4_276)));
    _t4_79 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_35, _t4_273), _mm256_mul_pd(_t4_34, _t4_274)), _mm256_add_pd(_mm256_mul_pd(_t4_33, _t4_275), _mm256_mul_pd(_t4_32, _t4_276)));
    _t4_80 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_31, _t4_273), _mm256_mul_pd(_t4_30, _t4_274)), _mm256_add_pd(_mm256_mul_pd(_t4_29, _t4_275), _mm256_mul_pd(_t4_28, _t4_276)));
    _t4_81 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_27, _t4_273), _mm256_mul_pd(_t4_26, _t4_274)), _mm256_add_pd(_mm256_mul_pd(_t4_25, _t4_275), _mm256_mul_pd(_t4_24, _t4_276)));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t4_277 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 32);
    _t4_278 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 32);
    _t4_279 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 49);
    _t4_280 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t4_82 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_19, _t4_277), _mm256_mul_pd(_t4_18, _t4_278)), _mm256_add_pd(_mm256_mul_pd(_t4_17, _t4_279), _mm256_mul_pd(_t4_16, _t4_280)));
    _t4_83 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t4_277), _mm256_mul_pd(_t4_14, _t4_278)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t4_279), _mm256_mul_pd(_t4_12, _t4_280)));
    _t4_84 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t4_277), _mm256_mul_pd(_t4_10, _t4_278)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t4_279), _mm256_mul_pd(_t4_8, _t4_280)));
    _t4_85 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t4_277), _mm256_mul_pd(_t4_6, _t4_278)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t4_279), _mm256_mul_pd(_t4_4, _t4_280)));

    // 4-BLAC: 4x4 + 4x4
    _t4_50 = _mm256_add_pd(_t4_78, _t4_82);
    _t4_51 = _mm256_add_pd(_t4_79, _t4_83);
    _t4_52 = _mm256_add_pd(_t4_80, _t4_84);
    _t4_53 = _mm256_add_pd(_t4_81, _t4_85);

    // 4-BLAC: 4x4 - 4x4
    _t4_86 = _mm256_sub_pd(_t4_92, _t4_50);
    _t4_87 = _mm256_sub_pd(_t4_93, _t4_51);
    _t4_88 = _mm256_sub_pd(_t4_94, _t4_52);
    _t4_89 = _mm256_sub_pd(_t4_95, _t4_53);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t4_73 = _t4_86;
    _t4_74 = _t4_87;
    _t4_75 = _t4_88;
    _t4_76 = _t4_89;
    _mm_store_sd(&(C[29*fi4]), _mm256_castpd256_pd128(_t0_7));
    _mm_store_sd(&(C[29*fi4 + 28]), _mm256_castpd256_pd128(_t0_9));
    _mm_store_sd(&(C[29*fi4 + 56]), _mm256_castpd256_pd128(_t0_11));
    _mm_store_sd(&(C[29*fi4 + 84]), _mm256_castpd256_pd128(_t0_12));
    _mm_store_sd(C + 29*fi4 + 29, _mm256_castpd256_pd128(_t0_13));
    _mm_store_sd(&(C[29*fi4 + 57]), _mm256_castpd256_pd128(_t0_17));
    _mm_store_sd(&(C[29*fi4 + 85]), _mm256_castpd256_pd128(_t0_18));
    _mm_store_sd(C + 29*fi4 + 58, _mm256_castpd256_pd128(_t0_19));
    _mm_store_sd(&(C[29*fi4 + 86]), _mm256_castpd256_pd128(_t0_21));
    _mm_store_sd(&(C[29*fi4 + 87]), _mm256_castpd256_pd128(_t0_22));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 112]), _mm256_castpd256_pd128(_t4_54));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 113]), _mm256_castpd256_pd128(_t4_55));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 114]), _mm256_castpd256_pd128(_t4_56));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 115]), _mm256_castpd256_pd128(_t4_57));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 140]), _mm256_castpd256_pd128(_t4_61));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 141]), _mm256_castpd256_pd128(_t4_62));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 142]), _mm256_castpd256_pd128(_t4_63));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 143]), _mm256_castpd256_pd128(_t4_64));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 168]), _mm256_castpd256_pd128(_t4_65));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 169]), _mm256_castpd256_pd128(_t4_66));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 170]), _mm256_castpd256_pd128(_t4_67));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 171]), _mm256_castpd256_pd128(_t4_68));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 196]), _mm256_castpd256_pd128(_t4_69));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 197]), _mm256_castpd256_pd128(_t4_70));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 198]), _mm256_castpd256_pd128(_t4_71));
    _mm_store_sd(&(C[29*fi4 + 28*Max(0, -fi4 + 20) + 199]), _mm256_castpd256_pd128(_t4_72));
    _mm_store_sd(C + 29*fi4 + 116, _mm256_castpd256_pd128(_t4_73));
    _mm256_maskstore_pd(C + 29*fi4 + 144, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_74);
    _mm256_maskstore_pd(C + 29*fi4 + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_75);
    _asm256_storeu_pd(C + 29*fi4 + 200, _t4_76);

    for( int i86 = 4; i86 <= -fi4 + 23; i86+=4 ) {

      for( int i203 = 0; i203 <= i86 - 1; i203+=4 ) {
        _t5_52 = _asm256_loadu_pd(C + 29*fi4 + i203 + 28*i86 + 116);
        _t5_53 = _asm256_loadu_pd(C + 29*fi4 + i203 + 28*i86 + 144);
        _t5_54 = _asm256_loadu_pd(C + 29*fi4 + i203 + 28*i86 + 172);
        _t5_55 = _asm256_loadu_pd(C + 29*fi4 + i203 + 28*i86 + 200);
        _t5_39 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 112);
        _t5_38 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 113);
        _t5_37 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 114);
        _t5_36 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 115);
        _t5_35 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 140);
        _t5_34 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 141);
        _t5_33 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 142);
        _t5_32 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 143);
        _t5_31 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 168);
        _t5_30 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 169);
        _t5_29 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 170);
        _t5_28 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 171);
        _t5_27 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 196);
        _t5_26 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 197);
        _t5_25 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 198);
        _t5_24 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 199);
        _t5_23 = _asm256_loadu_pd(C + 29*fi4 + 28*i203 + 112);
        _t5_22 = _asm256_loadu_pd(C + 29*fi4 + 28*i203 + 140);
        _t5_21 = _asm256_loadu_pd(C + 29*fi4 + 28*i203 + 168);
        _t5_20 = _asm256_loadu_pd(C + 29*fi4 + 28*i203 + 196);
        _t5_19 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 112);
        _t5_18 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 113);
        _t5_17 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 114);
        _t5_16 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 115);
        _t5_15 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 140);
        _t5_14 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 141);
        _t5_13 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 142);
        _t5_12 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 143);
        _t5_11 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 168);
        _t5_10 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 169);
        _t5_9 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 170);
        _t5_8 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 171);
        _t5_7 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 196);
        _t5_6 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 197);
        _t5_5 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 198);
        _t5_4 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 199);
        _t5_3 = _asm256_loadu_pd(L + 29*fi4 + 28*i203 + 112);
        _t5_2 = _asm256_loadu_pd(L + 29*fi4 + 28*i203 + 140);
        _t5_1 = _asm256_loadu_pd(L + 29*fi4 + 28*i203 + 168);
        _t5_0 = _asm256_loadu_pd(L + 29*fi4 + 28*i203 + 196);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t5_56 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_23, _t5_22), _mm256_unpacklo_pd(_t5_21, _t5_20), 32);
        _t5_57 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_23, _t5_22), _mm256_unpackhi_pd(_t5_21, _t5_20), 32);
        _t5_58 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_23, _t5_22), _mm256_unpacklo_pd(_t5_21, _t5_20), 49);
        _t5_59 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_23, _t5_22), _mm256_unpackhi_pd(_t5_21, _t5_20), 49);

        // 4-BLAC: 4x4 * 4x4
        _t5_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_39, _t5_56), _mm256_mul_pd(_t5_38, _t5_57)), _mm256_add_pd(_mm256_mul_pd(_t5_37, _t5_58), _mm256_mul_pd(_t5_36, _t5_59)));
        _t5_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_35, _t5_56), _mm256_mul_pd(_t5_34, _t5_57)), _mm256_add_pd(_mm256_mul_pd(_t5_33, _t5_58), _mm256_mul_pd(_t5_32, _t5_59)));
        _t5_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_31, _t5_56), _mm256_mul_pd(_t5_30, _t5_57)), _mm256_add_pd(_mm256_mul_pd(_t5_29, _t5_58), _mm256_mul_pd(_t5_28, _t5_59)));
        _t5_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_27, _t5_56), _mm256_mul_pd(_t5_26, _t5_57)), _mm256_add_pd(_mm256_mul_pd(_t5_25, _t5_58), _mm256_mul_pd(_t5_24, _t5_59)));

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t5_60 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 32);
        _t5_61 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 32);
        _t5_62 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 49);
        _t5_63 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t5_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_19, _t5_60), _mm256_mul_pd(_t5_18, _t5_61)), _mm256_add_pd(_mm256_mul_pd(_t5_17, _t5_62), _mm256_mul_pd(_t5_16, _t5_63)));
        _t5_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t5_60), _mm256_mul_pd(_t5_14, _t5_61)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t5_62), _mm256_mul_pd(_t5_12, _t5_63)));
        _t5_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t5_60), _mm256_mul_pd(_t5_10, _t5_61)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t5_62), _mm256_mul_pd(_t5_8, _t5_63)));
        _t5_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t5_60), _mm256_mul_pd(_t5_6, _t5_61)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t5_62), _mm256_mul_pd(_t5_4, _t5_63)));

        // 4-BLAC: 4x4 + 4x4
        _t5_40 = _mm256_add_pd(_t5_44, _t5_48);
        _t5_41 = _mm256_add_pd(_t5_45, _t5_49);
        _t5_42 = _mm256_add_pd(_t5_46, _t5_50);
        _t5_43 = _mm256_add_pd(_t5_47, _t5_51);

        // 4-BLAC: 4x4 - 4x4
        _t5_52 = _mm256_sub_pd(_t5_52, _t5_40);
        _t5_53 = _mm256_sub_pd(_t5_53, _t5_41);
        _t5_54 = _mm256_sub_pd(_t5_54, _t5_42);
        _t5_55 = _mm256_sub_pd(_t5_55, _t5_43);

        // AVX Storer:
        _asm256_storeu_pd(C + 29*fi4 + i203 + 28*i86 + 116, _t5_52);
        _asm256_storeu_pd(C + 29*fi4 + i203 + 28*i86 + 144, _t5_53);
        _asm256_storeu_pd(C + 29*fi4 + i203 + 28*i86 + 172, _t5_54);
        _asm256_storeu_pd(C + 29*fi4 + i203 + 28*i86 + 200, _t5_55);
      }
      _t6_44 = _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi4 + 29*i86 + 116));
      _t6_45 = _mm256_maskload_pd(C + 29*fi4 + 29*i86 + 144, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t6_46 = _mm256_maskload_pd(C + 29*fi4 + 29*i86 + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t6_47 = _asm256_loadu_pd(C + 29*fi4 + 29*i86 + 200);
      _t6_39 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 112);
      _t6_38 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 113);
      _t6_37 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 114);
      _t6_36 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 115);
      _t6_35 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 140);
      _t6_34 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 141);
      _t6_33 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 142);
      _t6_32 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 143);
      _t6_31 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 168);
      _t6_30 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 169);
      _t6_29 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 170);
      _t6_28 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 171);
      _t6_27 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 196);
      _t6_26 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 197);
      _t6_25 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 198);
      _t6_24 = _mm256_broadcast_sd(L + 29*fi4 + 28*i86 + 199);
      _t6_23 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 112);
      _t6_22 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 140);
      _t6_21 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 168);
      _t6_20 = _asm256_loadu_pd(C + 29*fi4 + 28*i86 + 196);
      _t6_19 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 112);
      _t6_18 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 113);
      _t6_17 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 114);
      _t6_16 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 115);
      _t6_15 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 140);
      _t6_14 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 141);
      _t6_13 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 142);
      _t6_12 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 143);
      _t6_11 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 168);
      _t6_10 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 169);
      _t6_9 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 170);
      _t6_8 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 171);
      _t6_7 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 196);
      _t6_6 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 197);
      _t6_5 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 198);
      _t6_4 = _mm256_broadcast_sd(C + 29*fi4 + 28*i86 + 199);
      _t6_3 = _asm256_loadu_pd(L + 29*fi4 + 28*i86 + 112);
      _t6_2 = _asm256_loadu_pd(L + 29*fi4 + 28*i86 + 140);
      _t6_1 = _asm256_loadu_pd(L + 29*fi4 + 28*i86 + 168);
      _t6_0 = _asm256_loadu_pd(L + 29*fi4 + 28*i86 + 196);

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t6_60 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_44, _t6_45, 0), _mm256_shuffle_pd(_t6_46, _t6_47, 0), 32);
      _t6_61 = _mm256_permute2f128_pd(_t6_45, _mm256_shuffle_pd(_t6_46, _t6_47, 3), 32);
      _t6_62 = _mm256_blend_pd(_t6_46, _mm256_shuffle_pd(_t6_46, _t6_47, 3), 12);
      _t6_63 = _t6_47;

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t6_64 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_23, _t6_22), _mm256_unpacklo_pd(_t6_21, _t6_20), 32);
      _t6_65 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_23, _t6_22), _mm256_unpackhi_pd(_t6_21, _t6_20), 32);
      _t6_66 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_23, _t6_22), _mm256_unpacklo_pd(_t6_21, _t6_20), 49);
      _t6_67 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_23, _t6_22), _mm256_unpackhi_pd(_t6_21, _t6_20), 49);

      // 4-BLAC: 4x4 * 4x4
      _t6_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_39, _t6_64), _mm256_mul_pd(_t6_38, _t6_65)), _mm256_add_pd(_mm256_mul_pd(_t6_37, _t6_66), _mm256_mul_pd(_t6_36, _t6_67)));
      _t6_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_35, _t6_64), _mm256_mul_pd(_t6_34, _t6_65)), _mm256_add_pd(_mm256_mul_pd(_t6_33, _t6_66), _mm256_mul_pd(_t6_32, _t6_67)));
      _t6_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_31, _t6_64), _mm256_mul_pd(_t6_30, _t6_65)), _mm256_add_pd(_mm256_mul_pd(_t6_29, _t6_66), _mm256_mul_pd(_t6_28, _t6_67)));
      _t6_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_27, _t6_64), _mm256_mul_pd(_t6_26, _t6_65)), _mm256_add_pd(_mm256_mul_pd(_t6_25, _t6_66), _mm256_mul_pd(_t6_24, _t6_67)));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t6_68 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
      _t6_69 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
      _t6_70 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
      _t6_71 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t6_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_19, _t6_68), _mm256_mul_pd(_t6_18, _t6_69)), _mm256_add_pd(_mm256_mul_pd(_t6_17, _t6_70), _mm256_mul_pd(_t6_16, _t6_71)));
      _t6_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t6_68), _mm256_mul_pd(_t6_14, _t6_69)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t6_70), _mm256_mul_pd(_t6_12, _t6_71)));
      _t6_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t6_68), _mm256_mul_pd(_t6_10, _t6_69)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t6_70), _mm256_mul_pd(_t6_8, _t6_71)));
      _t6_55 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t6_68), _mm256_mul_pd(_t6_6, _t6_69)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t6_70), _mm256_mul_pd(_t6_4, _t6_71)));

      // 4-BLAC: 4x4 + 4x4
      _t6_40 = _mm256_add_pd(_t6_48, _t6_52);
      _t6_41 = _mm256_add_pd(_t6_49, _t6_53);
      _t6_42 = _mm256_add_pd(_t6_50, _t6_54);
      _t6_43 = _mm256_add_pd(_t6_51, _t6_55);

      // 4-BLAC: 4x4 - 4x4
      _t6_56 = _mm256_sub_pd(_t6_60, _t6_40);
      _t6_57 = _mm256_sub_pd(_t6_61, _t6_41);
      _t6_58 = _mm256_sub_pd(_t6_62, _t6_42);
      _t6_59 = _mm256_sub_pd(_t6_63, _t6_43);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t6_44 = _t6_56;
      _t6_45 = _t6_57;
      _t6_46 = _t6_58;
      _t6_47 = _t6_59;
      _mm_store_sd(C + 29*fi4 + 29*i86 + 116, _mm256_castpd256_pd128(_t6_44));
      _mm256_maskstore_pd(C + 29*fi4 + 29*i86 + 144, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t6_45);
      _mm256_maskstore_pd(C + 29*fi4 + 29*i86 + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t6_46);
      _asm256_storeu_pd(C + 29*fi4 + 29*i86 + 200, _t6_47);
    }
  }

  _t7_42 = _mm256_castpd128_pd256(_mm_load_sd(&(C[580])));
  _t7_37 = _mm256_castpd128_pd256(_mm_load_sd(&(L[580])));
  _t7_43 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(C + 608)), _mm256_castpd128_pd256(_mm_load_sd(C + 636))), _mm256_castpd128_pd256(_mm_load_sd(C + 664)), 32);
  _t7_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 608)), _mm256_castpd128_pd256(_mm_load_sd(L + 636))), _mm256_castpd128_pd256(_mm_load_sd(L + 664)), 32);
  _t7_35 = _mm256_castpd128_pd256(_mm_load_sd(&(L[609])));
  _t7_34 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 637)), _mm256_castpd128_pd256(_mm_load_sd(L + 665)), 0);
  _t7_33 = _mm256_castpd128_pd256(_mm_load_sd(&(L[638])));
  _t7_32 = _mm256_castpd128_pd256(_mm_load_sd(&(L[666])));
  _t7_31 = _mm256_castpd128_pd256(_mm_load_sd(&(L[667])));
  _t7_48 = _mm256_castpd128_pd256(_mm_load_sd(C + 609));
  _t7_49 = _mm256_maskload_pd(C + 637, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t7_50 = _mm256_maskload_pd(C + 665, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t7_106 = _asm256_loadu_pd(C + 692);
  _t7_107 = _asm256_loadu_pd(C + 720);
  _t7_108 = _asm256_loadu_pd(C + 748);
  _t7_109 = _asm256_loadu_pd(C + 776);
  _t7_30 = _mm256_broadcast_sd(L + 692);
  _t7_29 = _mm256_broadcast_sd(L + 693);
  _t7_28 = _mm256_broadcast_sd(L + 694);
  _t7_27 = _mm256_broadcast_sd(L + 695);
  _t7_26 = _mm256_broadcast_sd(L + 720);
  _t7_25 = _mm256_broadcast_sd(L + 721);
  _t7_24 = _mm256_broadcast_sd(L + 722);
  _t7_23 = _mm256_broadcast_sd(L + 723);
  _t7_22 = _mm256_broadcast_sd(L + 748);
  _t7_21 = _mm256_broadcast_sd(L + 749);
  _t7_20 = _mm256_broadcast_sd(L + 750);
  _t7_19 = _mm256_broadcast_sd(L + 751);
  _t7_18 = _mm256_broadcast_sd(L + 776);
  _t7_17 = _mm256_broadcast_sd(L + 777);
  _t7_16 = _mm256_broadcast_sd(L + 778);
  _t7_15 = _mm256_broadcast_sd(L + 779);
  _t7_14 = _mm256_castpd128_pd256(_mm_load_sd(&(L[696])));
  _t7_13 = _mm256_castpd128_pd256(_mm_load_sd(&(L[608])));
  _t7_12 = _mm256_maskload_pd(L + 636, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t7_11 = _mm256_maskload_pd(L + 664, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t7_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 724)), _mm256_castpd128_pd256(_mm_load_sd(L + 752))), _mm256_castpd128_pd256(_mm_load_sd(L + 780)), 32);
  _t7_9 = _mm256_castpd128_pd256(_mm_load_sd(&(L[725])));
  _t7_8 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 753)), _mm256_castpd128_pd256(_mm_load_sd(L + 781)), 0);
  _t7_7 = _mm256_castpd128_pd256(_mm_load_sd(&(L[754])));
  _t7_6 = _mm256_broadcast_sd(&(L[782]));
  _t7_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[783])));
  _t7_74 = _mm256_castpd128_pd256(_mm_load_sd(C + 696));
  _t7_75 = _mm256_maskload_pd(C + 724, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t7_76 = _mm256_maskload_pd(C + 752, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t7_77 = _asm256_loadu_pd(C + 780);
  _t7_4 = _asm256_loadu_pd(L + 692);
  _t7_3 = _asm256_loadu_pd(L + 720);
  _t7_2 = _asm256_loadu_pd(L + 748);
  _t7_1 = _asm256_loadu_pd(L + 776);
  _t7_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[782])));

  // Generating : X[28,28] = S(h(1, 28, 20), ( G(h(1, 28, 20), X[28,28],h(1, 28, 20)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_114 = _t7_42;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_115 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_116 = _t7_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_117 = _mm256_mul_pd(_t7_115, _t7_116);

  // 4-BLAC: 1x4 / 1x4
  _t7_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_114), _mm256_castpd256_pd128(_t7_117)));

  // AVX Storer:
  _t7_42 = _t7_118;

  // Generating : X[28,28] = S(h(3, 28, 21), ( G(h(3, 28, 21), X[28,28],h(1, 28, 20)) - ( G(h(3, 28, 21), L[28,28],h(1, 28, 20)) Kro G(h(1, 28, 20), X[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_119 = _t7_43;

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_120 = _t7_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_121 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_42, _t7_42, 32), _mm256_permute2f128_pd(_t7_42, _t7_42, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t7_122 = _mm256_mul_pd(_t7_120, _t7_121);

  // 4-BLAC: 4x1 - 4x1
  _t7_123 = _mm256_sub_pd(_t7_119, _t7_122);

  // AVX Storer:
  _t7_43 = _t7_123;

  // Generating : X[28,28] = S(h(1, 28, 21), ( G(h(1, 28, 21), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 21), L[28,28],h(1, 28, 21)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_124 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_43, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_125 = _t7_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_126 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_127 = _mm256_add_pd(_t7_125, _t7_126);

  // 4-BLAC: 1x4 / 1x4
  _t7_128 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_124), _mm256_castpd256_pd128(_t7_127)));

  // AVX Storer:
  _t7_44 = _t7_128;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(1, 28, 20)) - ( G(h(2, 28, 22), L[28,28],h(1, 28, 21)) Kro G(h(1, 28, 21), X[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_129 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_43, 2), _mm256_permute2f128_pd(_t7_43, _t7_43, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_130 = _t7_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_131 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_44, _t7_44, 32), _mm256_permute2f128_pd(_t7_44, _t7_44, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t7_132 = _mm256_mul_pd(_t7_130, _t7_131);

  // 4-BLAC: 4x1 - 4x1
  _t7_133 = _mm256_sub_pd(_t7_129, _t7_132);

  // AVX Storer:
  _t7_45 = _t7_133;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 22), L[28,28],h(1, 28, 22)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_134 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_45, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_135 = _t7_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_136 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_137 = _mm256_add_pd(_t7_135, _t7_136);

  // 4-BLAC: 1x4 / 1x4
  _t7_138 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_134), _mm256_castpd256_pd128(_t7_137)));

  // AVX Storer:
  _t7_46 = _t7_138;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 20)) - ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro G(h(1, 28, 22), X[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_139 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_45, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_140 = _t7_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_141 = _t7_46;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_142 = _mm256_mul_pd(_t7_140, _t7_141);

  // 4-BLAC: 1x4 - 1x4
  _t7_143 = _mm256_sub_pd(_t7_139, _t7_142);

  // AVX Storer:
  _t7_47 = _t7_143;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 23), L[28,28],h(1, 28, 23)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_144 = _t7_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_145 = _t7_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_146 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_147 = _mm256_add_pd(_t7_145, _t7_146);

  // 4-BLAC: 1x4 / 1x4
  _t7_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_144), _mm256_castpd256_pd128(_t7_147)));

  // AVX Storer:
  _t7_47 = _t7_148;

  // Generating : X[28,28] = S(h(3, 28, 21), ( G(h(3, 28, 21), X[28,28],h(3, 28, 21)) - ( ( G(h(3, 28, 21), L[28,28],h(1, 28, 20)) * T( G(h(3, 28, 21), X[28,28],h(1, 28, 20)) ) ) + ( G(h(3, 28, 21), X[28,28],h(1, 28, 20)) * T( G(h(3, 28, 21), L[28,28],h(1, 28, 20)) ) ) ) ),h(3, 28, 21))

  // AVX Loader:

  // 3x3 -> 4x4 - LowSymm
  _t7_149 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_t7_48, _t7_49, 0), _t7_50, 32), _t7_48, 8);
  _t7_150 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_t7_49, _t7_50, 32), 6), _t7_48, 8);
  _t7_151 = _t7_50;
  _t7_152 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_153 = _t7_36;

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_154 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_44, _t7_46), _mm256_unpacklo_pd(_t7_47, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t7_155 = _t7_154;

  // 4-BLAC: 4x1 * 1x4
  _t7_156 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_153, _t7_153, 32), _mm256_permute2f128_pd(_t7_153, _t7_153, 32), 0), _t7_155);
  _t7_157 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_153, _t7_153, 32), _mm256_permute2f128_pd(_t7_153, _t7_153, 32), 15), _t7_155);
  _t7_158 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_153, _t7_153, 49), _mm256_permute2f128_pd(_t7_153, _t7_153, 49), 0), _t7_155);
  _t7_159 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_153, _t7_153, 49), _mm256_permute2f128_pd(_t7_153, _t7_153, 49), 15), _t7_155);

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_160 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_44, _t7_46), _mm256_unpacklo_pd(_t7_47, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_161 = _t7_36;

  // 4-BLAC: (4x1)^T
  _t7_162 = _t7_161;

  // 4-BLAC: 4x1 * 1x4
  _t7_163 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_160, _t7_160, 32), _mm256_permute2f128_pd(_t7_160, _t7_160, 32), 0), _t7_162);
  _t7_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_160, _t7_160, 32), _mm256_permute2f128_pd(_t7_160, _t7_160, 32), 15), _t7_162);
  _t7_165 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_160, _t7_160, 49), _mm256_permute2f128_pd(_t7_160, _t7_160, 49), 0), _t7_162);
  _t7_166 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_160, _t7_160, 49), _mm256_permute2f128_pd(_t7_160, _t7_160, 49), 15), _t7_162);

  // 4-BLAC: 4x4 + 4x4
  _t7_167 = _mm256_add_pd(_t7_156, _t7_163);
  _t7_168 = _mm256_add_pd(_t7_157, _t7_164);
  _t7_169 = _mm256_add_pd(_t7_158, _t7_165);
  _t7_170 = _mm256_add_pd(_t7_159, _t7_166);

  // 4-BLAC: 4x4 - 4x4
  _t7_171 = _mm256_sub_pd(_t7_149, _t7_167);
  _t7_172 = _mm256_sub_pd(_t7_150, _t7_168);
  _t7_173 = _mm256_sub_pd(_t7_151, _t7_169);
  _t7_174 = _mm256_sub_pd(_t7_152, _t7_170);

  // AVX Storer:

  // 4x4 -> 3x3 - LowSymm
  _t7_48 = _t7_171;
  _t7_49 = _t7_172;
  _t7_50 = _t7_173;

  // Generating : X[28,28] = S(h(1, 28, 21), ( G(h(1, 28, 21), X[28,28],h(1, 28, 21)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_175 = _t7_48;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_176 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_177 = _t7_35;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_178 = _mm256_mul_pd(_t7_176, _t7_177);

  // 4-BLAC: 1x4 / 1x4
  _t7_179 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_175), _mm256_castpd256_pd128(_t7_178)));

  // AVX Storer:
  _t7_48 = _t7_179;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(1, 28, 21)) - ( G(h(2, 28, 22), L[28,28],h(1, 28, 21)) Kro G(h(1, 28, 21), X[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_180 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_49, _t7_50), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_181 = _t7_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_182 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_48, _t7_48, 32), _mm256_permute2f128_pd(_t7_48, _t7_48, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t7_183 = _mm256_mul_pd(_t7_181, _t7_182);

  // 4-BLAC: 4x1 - 4x1
  _t7_184 = _mm256_sub_pd(_t7_180, _t7_183);

  // AVX Storer:
  _t7_51 = _t7_184;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 22), L[28,28],h(1, 28, 22)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_185 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_51, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_186 = _t7_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_187 = _t7_35;

  // 4-BLAC: 1x4 + 1x4
  _t7_188 = _mm256_add_pd(_t7_186, _t7_187);

  // 4-BLAC: 1x4 / 1x4
  _t7_189 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_185), _mm256_castpd256_pd128(_t7_188)));

  // AVX Storer:
  _t7_52 = _t7_189;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro G(h(1, 28, 22), X[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_190 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_51, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_191 = _t7_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_192 = _t7_52;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_193 = _mm256_mul_pd(_t7_191, _t7_192);

  // 4-BLAC: 1x4 - 1x4
  _t7_194 = _mm256_sub_pd(_t7_190, _t7_193);

  // AVX Storer:
  _t7_53 = _t7_194;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 23), L[28,28],h(1, 28, 23)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_195 = _t7_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_196 = _t7_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_197 = _t7_35;

  // 4-BLAC: 1x4 + 1x4
  _t7_198 = _mm256_add_pd(_t7_196, _t7_197);

  // 4-BLAC: 1x4 / 1x4
  _t7_199 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_195), _mm256_castpd256_pd128(_t7_198)));

  // AVX Storer:
  _t7_53 = _t7_199;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(2, 28, 22)) - ( ( G(h(2, 28, 22), L[28,28],h(1, 28, 21)) * T( G(h(2, 28, 22), X[28,28],h(1, 28, 21)) ) ) + ( G(h(2, 28, 22), X[28,28],h(1, 28, 21)) * T( G(h(2, 28, 22), L[28,28],h(1, 28, 21)) ) ) ) ),h(2, 28, 22))

  // AVX Loader:

  // 2x2 -> 4x4 - LowSymm
  _t7_200 = _mm256_shuffle_pd(_mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_49, 2), _mm256_setzero_pd()), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_50, 6), _mm256_permute2f128_pd(_t7_50, _t7_50, 129), 5), 0);
  _t7_201 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_50, 6), _mm256_permute2f128_pd(_t7_50, _t7_50, 129), 5);
  _t7_202 = _mm256_setzero_pd();
  _t7_203 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_204 = _t7_34;

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_205 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_52, _t7_53), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t7_206 = _t7_205;

  // 4-BLAC: 4x1 * 1x4
  _t7_207 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_204, _t7_204, 32), _mm256_permute2f128_pd(_t7_204, _t7_204, 32), 0), _t7_206);
  _t7_208 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_204, _t7_204, 32), _mm256_permute2f128_pd(_t7_204, _t7_204, 32), 15), _t7_206);
  _t7_209 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_204, _t7_204, 49), _mm256_permute2f128_pd(_t7_204, _t7_204, 49), 0), _t7_206);
  _t7_210 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_204, _t7_204, 49), _mm256_permute2f128_pd(_t7_204, _t7_204, 49), 15), _t7_206);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_211 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_52, _t7_53), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_212 = _t7_34;

  // 4-BLAC: (4x1)^T
  _t7_213 = _t7_212;

  // 4-BLAC: 4x1 * 1x4
  _t7_214 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_211, _t7_211, 32), _mm256_permute2f128_pd(_t7_211, _t7_211, 32), 0), _t7_213);
  _t7_215 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_211, _t7_211, 32), _mm256_permute2f128_pd(_t7_211, _t7_211, 32), 15), _t7_213);
  _t7_216 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_211, _t7_211, 49), _mm256_permute2f128_pd(_t7_211, _t7_211, 49), 0), _t7_213);
  _t7_217 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_211, _t7_211, 49), _mm256_permute2f128_pd(_t7_211, _t7_211, 49), 15), _t7_213);

  // 4-BLAC: 4x4 + 4x4
  _t7_218 = _mm256_add_pd(_t7_207, _t7_214);
  _t7_219 = _mm256_add_pd(_t7_208, _t7_215);
  _t7_220 = _mm256_add_pd(_t7_209, _t7_216);
  _t7_221 = _mm256_add_pd(_t7_210, _t7_217);

  // 4-BLAC: 4x4 - 4x4
  _t7_222 = _mm256_sub_pd(_t7_200, _t7_218);
  _t7_223 = _mm256_sub_pd(_t7_201, _t7_219);
  _t7_224 = _mm256_sub_pd(_t7_202, _t7_220);
  _t7_225 = _mm256_sub_pd(_t7_203, _t7_221);

  // AVX Storer:

  // 4x4 -> 2x2 - LowSymm
  _t7_54 = _t7_222;
  _t7_55 = _t7_223;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 22)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_226 = _t7_54;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_227 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_228 = _t7_33;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_229 = _mm256_mul_pd(_t7_227, _t7_228);

  // 4-BLAC: 1x4 / 1x4
  _t7_230 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_226), _mm256_castpd256_pd128(_t7_229)));

  // AVX Storer:
  _t7_54 = _t7_230;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro G(h(1, 28, 22), X[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_231 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_55, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_232 = _t7_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_233 = _t7_54;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_234 = _mm256_mul_pd(_t7_232, _t7_233);

  // 4-BLAC: 1x4 - 1x4
  _t7_235 = _mm256_sub_pd(_t7_231, _t7_234);

  // AVX Storer:
  _t7_56 = _t7_235;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 23), L[28,28],h(1, 28, 23)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_236 = _t7_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_237 = _t7_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_238 = _t7_33;

  // 4-BLAC: 1x4 + 1x4
  _t7_239 = _mm256_add_pd(_t7_237, _t7_238);

  // 4-BLAC: 1x4 / 1x4
  _t7_240 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_236), _mm256_castpd256_pd128(_t7_239)));

  // AVX Storer:
  _t7_56 = _t7_240;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 23)) - ( ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro T( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) ) ) + ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) Kro T( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) ) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_241 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_55, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_242 = _t7_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_243 = _t7_56;

  // 4-BLAC: (4x1)^T
  _t7_244 = _t7_243;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_245 = _mm256_mul_pd(_t7_242, _t7_244);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_246 = _t7_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_247 = _t7_32;

  // 4-BLAC: (4x1)^T
  _t7_248 = _t7_247;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_249 = _mm256_mul_pd(_t7_246, _t7_248);

  // 4-BLAC: 1x4 + 1x4
  _t7_250 = _mm256_add_pd(_t7_245, _t7_249);

  // 4-BLAC: 1x4 - 1x4
  _t7_251 = _mm256_sub_pd(_t7_241, _t7_250);

  // AVX Storer:
  _t7_57 = _t7_251;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 23)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_252 = _t7_57;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_253 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_254 = _t7_31;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_255 = _mm256_mul_pd(_t7_253, _t7_254);

  // 4-BLAC: 1x4 / 1x4
  _t7_256 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_252), _mm256_castpd256_pd128(_t7_255)));

  // AVX Storer:
  _t7_57 = _t7_256;

  // Generating : X[28,28] = S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, 20)) - ( G(h(4, 28, 24), L[28,28],h(4, 28, 20)) * G(h(4, 28, 20), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t7_257 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t7_42, _mm256_blend_pd(_mm256_unpacklo_pd(_t7_44, _t7_48), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_46, _t7_52), _mm256_unpacklo_pd(_t7_54, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_47, _t7_53), _mm256_unpacklo_pd(_t7_56, _t7_57), 32), 0), 32);
  _t7_258 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t7_44, _t7_48), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_46, _t7_52), _mm256_unpacklo_pd(_t7_54, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_47, _t7_53), _mm256_unpacklo_pd(_t7_56, _t7_57), 32), 3), 32);
  _t7_259 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_46, _t7_52), _mm256_unpacklo_pd(_t7_54, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_46, _t7_52), _mm256_unpacklo_pd(_t7_54, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_47, _t7_53), _mm256_unpacklo_pd(_t7_56, _t7_57), 32), 3), 12);
  _t7_260 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_47, _t7_53), _mm256_unpacklo_pd(_t7_56, _t7_57), 32);

  // 4-BLAC: 4x4 * 4x4
  _t7_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_30, _t7_257), _mm256_mul_pd(_t7_29, _t7_258)), _mm256_add_pd(_mm256_mul_pd(_t7_28, _t7_259), _mm256_mul_pd(_t7_27, _t7_260)));
  _t7_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_26, _t7_257), _mm256_mul_pd(_t7_25, _t7_258)), _mm256_add_pd(_mm256_mul_pd(_t7_24, _t7_259), _mm256_mul_pd(_t7_23, _t7_260)));
  _t7_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_22, _t7_257), _mm256_mul_pd(_t7_21, _t7_258)), _mm256_add_pd(_mm256_mul_pd(_t7_20, _t7_259), _mm256_mul_pd(_t7_19, _t7_260)));
  _t7_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_18, _t7_257), _mm256_mul_pd(_t7_17, _t7_258)), _mm256_add_pd(_mm256_mul_pd(_t7_16, _t7_259), _mm256_mul_pd(_t7_15, _t7_260)));

  // 4-BLAC: 4x4 - 4x4
  _t7_106 = _mm256_sub_pd(_t7_106, _t7_94);
  _t7_107 = _mm256_sub_pd(_t7_107, _t7_95);
  _t7_108 = _mm256_sub_pd(_t7_108, _t7_96);
  _t7_109 = _mm256_sub_pd(_t7_109, _t7_97);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_261 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_106, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_262 = _t7_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_263 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_264 = _mm256_add_pd(_t7_262, _t7_263);

  // 4-BLAC: 1x4 / 1x4
  _t7_265 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_261), _mm256_castpd256_pd128(_t7_264)));

  // AVX Storer:
  _t7_58 = _t7_265;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_266 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_106, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_267 = _t7_58;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_268 = _t7_13;

  // 4-BLAC: (4x1)^T
  _t7_269 = _t7_268;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_270 = _mm256_mul_pd(_t7_267, _t7_269);

  // 4-BLAC: 1x4 - 1x4
  _t7_271 = _mm256_sub_pd(_t7_266, _t7_270);

  // AVX Storer:
  _t7_59 = _t7_271;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_272 = _t7_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_273 = _t7_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_274 = _t7_35;

  // 4-BLAC: 1x4 + 1x4
  _t7_275 = _mm256_add_pd(_t7_273, _t7_274);

  // 4-BLAC: 1x4 / 1x4
  _t7_276 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_272), _mm256_castpd256_pd128(_t7_275)));

  // AVX Storer:
  _t7_59 = _t7_276;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_277 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_106, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t7_106, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_278 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_279 = _t7_12;

  // 4-BLAC: (1x4)^T
  _t7_280 = _t7_279;

  // 4-BLAC: 1x4 * 4x1
  _t7_281 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_278, _t7_280), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_278, _t7_280), _mm256_mul_pd(_t7_278, _t7_280), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_278, _t7_280), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_278, _t7_280), _mm256_mul_pd(_t7_278, _t7_280), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_278, _t7_280), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_278, _t7_280), _mm256_mul_pd(_t7_278, _t7_280), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_282 = _mm256_sub_pd(_t7_277, _t7_281);

  // AVX Storer:
  _t7_60 = _t7_282;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_283 = _t7_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_284 = _t7_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_285 = _t7_33;

  // 4-BLAC: 1x4 + 1x4
  _t7_286 = _mm256_add_pd(_t7_284, _t7_285);

  // 4-BLAC: 1x4 / 1x4
  _t7_287 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_283), _mm256_castpd256_pd128(_t7_286)));

  // AVX Storer:
  _t7_60 = _t7_287;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_288 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t7_106, _t7_106, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_289 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_290 = _t7_11;

  // 4-BLAC: (1x4)^T
  _t7_291 = _t7_290;

  // 4-BLAC: 1x4 * 4x1
  _t7_292 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_289, _t7_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_289, _t7_291), _mm256_mul_pd(_t7_289, _t7_291), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_289, _t7_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_289, _t7_291), _mm256_mul_pd(_t7_289, _t7_291), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_289, _t7_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_289, _t7_291), _mm256_mul_pd(_t7_289, _t7_291), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_293 = _mm256_sub_pd(_t7_288, _t7_292);

  // AVX Storer:
  _t7_61 = _t7_293;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_294 = _t7_61;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_295 = _t7_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_296 = _t7_31;

  // 4-BLAC: 1x4 + 1x4
  _t7_297 = _mm256_add_pd(_t7_295, _t7_296);

  // 4-BLAC: 1x4 / 1x4
  _t7_298 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_294), _mm256_castpd256_pd128(_t7_297)));

  // AVX Storer:
  _t7_61 = _t7_298;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, 20)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // 3x4 -> 4x4
  _t7_299 = _t7_107;
  _t7_300 = _t7_108;
  _t7_301 = _t7_109;
  _t7_302 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_303 = _t7_10;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t7_304 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_303, _t7_303, 32), _mm256_permute2f128_pd(_t7_303, _t7_303, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32));
  _t7_305 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_303, _t7_303, 32), _mm256_permute2f128_pd(_t7_303, _t7_303, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32));
  _t7_306 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_303, _t7_303, 49), _mm256_permute2f128_pd(_t7_303, _t7_303, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32));
  _t7_307 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_303, _t7_303, 49), _mm256_permute2f128_pd(_t7_303, _t7_303, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32));

  // 4-BLAC: 4x4 - 4x4
  _t7_308 = _mm256_sub_pd(_t7_299, _t7_304);
  _t7_309 = _mm256_sub_pd(_t7_300, _t7_305);
  _t7_310 = _mm256_sub_pd(_t7_301, _t7_306);
  _t7_311 = _mm256_sub_pd(_t7_302, _t7_307);

  // AVX Storer:
  _t7_107 = _t7_308;
  _t7_108 = _t7_309;
  _t7_109 = _t7_310;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_312 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_107, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_313 = _t7_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_314 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_315 = _mm256_add_pd(_t7_313, _t7_314);

  // 4-BLAC: 1x4 / 1x4
  _t7_316 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_312), _mm256_castpd256_pd128(_t7_315)));

  // AVX Storer:
  _t7_62 = _t7_316;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_317 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_107, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_318 = _t7_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_319 = _t7_13;

  // 4-BLAC: (4x1)^T
  _t7_320 = _t7_319;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_321 = _mm256_mul_pd(_t7_318, _t7_320);

  // 4-BLAC: 1x4 - 1x4
  _t7_322 = _mm256_sub_pd(_t7_317, _t7_321);

  // AVX Storer:
  _t7_63 = _t7_322;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_323 = _t7_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_324 = _t7_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_325 = _t7_35;

  // 4-BLAC: 1x4 + 1x4
  _t7_326 = _mm256_add_pd(_t7_324, _t7_325);

  // 4-BLAC: 1x4 / 1x4
  _t7_327 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_323), _mm256_castpd256_pd128(_t7_326)));

  // AVX Storer:
  _t7_63 = _t7_327;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_328 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_107, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t7_107, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_329 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_330 = _t7_12;

  // 4-BLAC: (1x4)^T
  _t7_331 = _t7_330;

  // 4-BLAC: 1x4 * 4x1
  _t7_332 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_329, _t7_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_329, _t7_331), _mm256_mul_pd(_t7_329, _t7_331), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_329, _t7_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_329, _t7_331), _mm256_mul_pd(_t7_329, _t7_331), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_329, _t7_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_329, _t7_331), _mm256_mul_pd(_t7_329, _t7_331), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_333 = _mm256_sub_pd(_t7_328, _t7_332);

  // AVX Storer:
  _t7_64 = _t7_333;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_334 = _t7_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_335 = _t7_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_336 = _t7_33;

  // 4-BLAC: 1x4 + 1x4
  _t7_337 = _mm256_add_pd(_t7_335, _t7_336);

  // 4-BLAC: 1x4 / 1x4
  _t7_338 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_334), _mm256_castpd256_pd128(_t7_337)));

  // AVX Storer:
  _t7_64 = _t7_338;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_339 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t7_107, _t7_107, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_340 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_341 = _t7_11;

  // 4-BLAC: (1x4)^T
  _t7_342 = _t7_341;

  // 4-BLAC: 1x4 * 4x1
  _t7_343 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_340, _t7_342), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_340, _t7_342), _mm256_mul_pd(_t7_340, _t7_342), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_340, _t7_342), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_340, _t7_342), _mm256_mul_pd(_t7_340, _t7_342), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_340, _t7_342), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_340, _t7_342), _mm256_mul_pd(_t7_340, _t7_342), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_344 = _mm256_sub_pd(_t7_339, _t7_343);

  // AVX Storer:
  _t7_65 = _t7_344;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_345 = _t7_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_346 = _t7_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_347 = _t7_31;

  // 4-BLAC: 1x4 + 1x4
  _t7_348 = _mm256_add_pd(_t7_346, _t7_347);

  // 4-BLAC: 1x4 / 1x4
  _t7_349 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_345), _mm256_castpd256_pd128(_t7_348)));

  // AVX Storer:
  _t7_65 = _t7_349;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, 20)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // 2x4 -> 4x4
  _t7_350 = _t7_108;
  _t7_351 = _t7_109;
  _t7_352 = _mm256_setzero_pd();
  _t7_353 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_354 = _t7_8;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t7_355 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_354, _t7_354, 32), _mm256_permute2f128_pd(_t7_354, _t7_354, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32));
  _t7_356 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_354, _t7_354, 32), _mm256_permute2f128_pd(_t7_354, _t7_354, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32));
  _t7_357 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_354, _t7_354, 49), _mm256_permute2f128_pd(_t7_354, _t7_354, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32));
  _t7_358 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_354, _t7_354, 49), _mm256_permute2f128_pd(_t7_354, _t7_354, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32));

  // 4-BLAC: 4x4 - 4x4
  _t7_359 = _mm256_sub_pd(_t7_350, _t7_355);
  _t7_360 = _mm256_sub_pd(_t7_351, _t7_356);
  _t7_361 = _mm256_sub_pd(_t7_352, _t7_357);
  _t7_362 = _mm256_sub_pd(_t7_353, _t7_358);

  // AVX Storer:
  _t7_108 = _t7_359;
  _t7_109 = _t7_360;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_363 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_108, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_364 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_365 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_366 = _mm256_add_pd(_t7_364, _t7_365);

  // 4-BLAC: 1x4 / 1x4
  _t7_367 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_363), _mm256_castpd256_pd128(_t7_366)));

  // AVX Storer:
  _t7_66 = _t7_367;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_368 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_108, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_369 = _t7_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_370 = _t7_13;

  // 4-BLAC: (4x1)^T
  _t7_371 = _t7_370;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_372 = _mm256_mul_pd(_t7_369, _t7_371);

  // 4-BLAC: 1x4 - 1x4
  _t7_373 = _mm256_sub_pd(_t7_368, _t7_372);

  // AVX Storer:
  _t7_67 = _t7_373;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_374 = _t7_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_375 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_376 = _t7_35;

  // 4-BLAC: 1x4 + 1x4
  _t7_377 = _mm256_add_pd(_t7_375, _t7_376);

  // 4-BLAC: 1x4 / 1x4
  _t7_378 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_374), _mm256_castpd256_pd128(_t7_377)));

  // AVX Storer:
  _t7_67 = _t7_378;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_379 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_108, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t7_108, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_380 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_381 = _t7_12;

  // 4-BLAC: (1x4)^T
  _t7_382 = _t7_381;

  // 4-BLAC: 1x4 * 4x1
  _t7_383 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_380, _t7_382), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_380, _t7_382), _mm256_mul_pd(_t7_380, _t7_382), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_380, _t7_382), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_380, _t7_382), _mm256_mul_pd(_t7_380, _t7_382), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_380, _t7_382), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_380, _t7_382), _mm256_mul_pd(_t7_380, _t7_382), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_384 = _mm256_sub_pd(_t7_379, _t7_383);

  // AVX Storer:
  _t7_68 = _t7_384;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_385 = _t7_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_386 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_387 = _t7_33;

  // 4-BLAC: 1x4 + 1x4
  _t7_388 = _mm256_add_pd(_t7_386, _t7_387);

  // 4-BLAC: 1x4 / 1x4
  _t7_389 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_385), _mm256_castpd256_pd128(_t7_388)));

  // AVX Storer:
  _t7_68 = _t7_389;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_390 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t7_108, _t7_108, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_391 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_unpacklo_pd(_t7_68, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_392 = _t7_11;

  // 4-BLAC: (1x4)^T
  _t7_393 = _t7_392;

  // 4-BLAC: 1x4 * 4x1
  _t7_394 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_391, _t7_393), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_391, _t7_393), _mm256_mul_pd(_t7_391, _t7_393), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_391, _t7_393), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_391, _t7_393), _mm256_mul_pd(_t7_391, _t7_393), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_391, _t7_393), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_391, _t7_393), _mm256_mul_pd(_t7_391, _t7_393), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_395 = _mm256_sub_pd(_t7_390, _t7_394);

  // AVX Storer:
  _t7_69 = _t7_395;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_396 = _t7_69;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_397 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_398 = _t7_31;

  // 4-BLAC: 1x4 + 1x4
  _t7_399 = _mm256_add_pd(_t7_397, _t7_398);

  // 4-BLAC: 1x4 / 1x4
  _t7_400 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_396), _mm256_castpd256_pd128(_t7_399)));

  // AVX Storer:
  _t7_69 = _t7_400;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, 20)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_401 = _t7_6;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t7_93 = _mm256_mul_pd(_t7_401, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_unpacklo_pd(_t7_68, _t7_69), 32));

  // 4-BLAC: 1x4 - 1x4
  _t7_109 = _mm256_sub_pd(_t7_109, _t7_93);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_402 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_109, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_403 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_404 = _t7_37;

  // 4-BLAC: 1x4 + 1x4
  _t7_405 = _mm256_add_pd(_t7_403, _t7_404);

  // 4-BLAC: 1x4 / 1x4
  _t7_406 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_402), _mm256_castpd256_pd128(_t7_405)));

  // AVX Storer:
  _t7_70 = _t7_406;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_407 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_109, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_408 = _t7_70;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_409 = _t7_13;

  // 4-BLAC: (4x1)^T
  _t7_410 = _t7_409;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_411 = _mm256_mul_pd(_t7_408, _t7_410);

  // 4-BLAC: 1x4 - 1x4
  _t7_412 = _mm256_sub_pd(_t7_407, _t7_411);

  // AVX Storer:
  _t7_71 = _t7_412;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_413 = _t7_71;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_414 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_415 = _t7_35;

  // 4-BLAC: 1x4 + 1x4
  _t7_416 = _mm256_add_pd(_t7_414, _t7_415);

  // 4-BLAC: 1x4 / 1x4
  _t7_417 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_413), _mm256_castpd256_pd128(_t7_416)));

  // AVX Storer:
  _t7_71 = _t7_417;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_418 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_109, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t7_109, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_419 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_70, _t7_71), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t7_420 = _t7_12;

  // 4-BLAC: (1x4)^T
  _t7_421 = _t7_420;

  // 4-BLAC: 1x4 * 4x1
  _t7_422 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_419, _t7_421), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_419, _t7_421), _mm256_mul_pd(_t7_419, _t7_421), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_419, _t7_421), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_419, _t7_421), _mm256_mul_pd(_t7_419, _t7_421), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_419, _t7_421), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_419, _t7_421), _mm256_mul_pd(_t7_419, _t7_421), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_423 = _mm256_sub_pd(_t7_418, _t7_422);

  // AVX Storer:
  _t7_72 = _t7_423;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_424 = _t7_72;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_425 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_426 = _t7_33;

  // 4-BLAC: 1x4 + 1x4
  _t7_427 = _mm256_add_pd(_t7_425, _t7_426);

  // 4-BLAC: 1x4 / 1x4
  _t7_428 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_424), _mm256_castpd256_pd128(_t7_427)));

  // AVX Storer:
  _t7_72 = _t7_428;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_429 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t7_109, _t7_109, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_430 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_70, _t7_71), _mm256_unpacklo_pd(_t7_72, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t7_431 = _t7_11;

  // 4-BLAC: (1x4)^T
  _t7_432 = _t7_431;

  // 4-BLAC: 1x4 * 4x1
  _t7_433 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t7_430, _t7_432), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_430, _t7_432), _mm256_mul_pd(_t7_430, _t7_432), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t7_430, _t7_432), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_430, _t7_432), _mm256_mul_pd(_t7_430, _t7_432), 129)), _mm256_add_pd(_mm256_mul_pd(_t7_430, _t7_432), _mm256_permute2f128_pd(_mm256_mul_pd(_t7_430, _t7_432), _mm256_mul_pd(_t7_430, _t7_432), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t7_434 = _mm256_sub_pd(_t7_429, _t7_433);

  // AVX Storer:
  _t7_73 = _t7_434;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_435 = _t7_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_436 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_437 = _t7_31;

  // 4-BLAC: 1x4 + 1x4
  _t7_438 = _mm256_add_pd(_t7_436, _t7_437);

  // 4-BLAC: 1x4 / 1x4
  _t7_439 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_435), _mm256_castpd256_pd128(_t7_438)));

  // AVX Storer:
  _t7_73 = _t7_439;

  // Generating : X[28,28] = S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, 24)) - ( ( G(h(4, 28, 24), L[28,28],h(4, 28, 20)) * T( G(h(4, 28, 24), X[28,28],h(4, 28, 20)) ) ) + ( G(h(4, 28, 24), X[28,28],h(4, 28, 20)) * T( G(h(4, 28, 24), L[28,28],h(4, 28, 20)) ) ) ) ),h(4, 28, 24))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t7_440 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t7_74, _t7_75, 0), _mm256_shuffle_pd(_t7_76, _t7_77, 0), 32);
  _t7_441 = _mm256_permute2f128_pd(_t7_75, _mm256_shuffle_pd(_t7_76, _t7_77, 3), 32);
  _t7_442 = _mm256_blend_pd(_t7_76, _mm256_shuffle_pd(_t7_76, _t7_77, 3), 12);
  _t7_443 = _t7_77;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t7_587 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_unpacklo_pd(_t7_68, _t7_69), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_70, _t7_71), _mm256_unpacklo_pd(_t7_72, _t7_73), 32)), 32);
  _t7_588 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_unpacklo_pd(_t7_68, _t7_69), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_70, _t7_71), _mm256_unpacklo_pd(_t7_72, _t7_73), 32)), 32);
  _t7_589 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_unpacklo_pd(_t7_68, _t7_69), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_70, _t7_71), _mm256_unpacklo_pd(_t7_72, _t7_73), 32)), 49);
  _t7_590 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_58, _t7_59), _mm256_unpacklo_pd(_t7_60, _t7_61), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_62, _t7_63), _mm256_unpacklo_pd(_t7_64, _t7_65), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_66, _t7_67), _mm256_unpacklo_pd(_t7_68, _t7_69), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_70, _t7_71), _mm256_unpacklo_pd(_t7_72, _t7_73), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t7_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_30, _t7_587), _mm256_mul_pd(_t7_29, _t7_588)), _mm256_add_pd(_mm256_mul_pd(_t7_28, _t7_589), _mm256_mul_pd(_t7_27, _t7_590)));
  _t7_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_26, _t7_587), _mm256_mul_pd(_t7_25, _t7_588)), _mm256_add_pd(_mm256_mul_pd(_t7_24, _t7_589), _mm256_mul_pd(_t7_23, _t7_590)));
  _t7_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_22, _t7_587), _mm256_mul_pd(_t7_21, _t7_588)), _mm256_add_pd(_mm256_mul_pd(_t7_20, _t7_589), _mm256_mul_pd(_t7_19, _t7_590)));
  _t7_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_18, _t7_587), _mm256_mul_pd(_t7_17, _t7_588)), _mm256_add_pd(_mm256_mul_pd(_t7_16, _t7_589), _mm256_mul_pd(_t7_15, _t7_590)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t7_591 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_4, _t7_3), _mm256_unpacklo_pd(_t7_2, _t7_1), 32);
  _t7_592 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_4, _t7_3), _mm256_unpackhi_pd(_t7_2, _t7_1), 32);
  _t7_593 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_4, _t7_3), _mm256_unpacklo_pd(_t7_2, _t7_1), 49);
  _t7_594 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_4, _t7_3), _mm256_unpackhi_pd(_t7_2, _t7_1), 49);

  // 4-BLAC: 4x4 * 4x4
  _t7_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_58, _t7_58, 32), _mm256_permute2f128_pd(_t7_58, _t7_58, 32), 0), _t7_591), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_59, _t7_59, 32), _mm256_permute2f128_pd(_t7_59, _t7_59, 32), 0), _t7_592)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_60, _t7_60, 32), _mm256_permute2f128_pd(_t7_60, _t7_60, 32), 0), _t7_593), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_61, _t7_61, 32), _mm256_permute2f128_pd(_t7_61, _t7_61, 32), 0), _t7_594)));
  _t7_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_62, _t7_62, 32), _mm256_permute2f128_pd(_t7_62, _t7_62, 32), 0), _t7_591), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_63, _t7_63, 32), _mm256_permute2f128_pd(_t7_63, _t7_63, 32), 0), _t7_592)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_64, _t7_64, 32), _mm256_permute2f128_pd(_t7_64, _t7_64, 32), 0), _t7_593), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_65, _t7_65, 32), _mm256_permute2f128_pd(_t7_65, _t7_65, 32), 0), _t7_594)));
  _t7_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_66, _t7_66, 32), _mm256_permute2f128_pd(_t7_66, _t7_66, 32), 0), _t7_591), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_67, _t7_67, 32), _mm256_permute2f128_pd(_t7_67, _t7_67, 32), 0), _t7_592)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_68, _t7_68, 32), _mm256_permute2f128_pd(_t7_68, _t7_68, 32), 0), _t7_593), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_69, _t7_69, 32), _mm256_permute2f128_pd(_t7_69, _t7_69, 32), 0), _t7_594)));
  _t7_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_70, _t7_70, 32), _mm256_permute2f128_pd(_t7_70, _t7_70, 32), 0), _t7_591), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_71, _t7_71, 32), _mm256_permute2f128_pd(_t7_71, _t7_71, 32), 0), _t7_592)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_72, _t7_72, 32), _mm256_permute2f128_pd(_t7_72, _t7_72, 32), 0), _t7_593), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_73, _t7_73, 32), _mm256_permute2f128_pd(_t7_73, _t7_73, 32), 0), _t7_594)));

  // 4-BLAC: 4x4 + 4x4
  _t7_38 = _mm256_add_pd(_t7_98, _t7_102);
  _t7_39 = _mm256_add_pd(_t7_99, _t7_103);
  _t7_40 = _mm256_add_pd(_t7_100, _t7_104);
  _t7_41 = _mm256_add_pd(_t7_101, _t7_105);

  // 4-BLAC: 4x4 - 4x4
  _t7_110 = _mm256_sub_pd(_t7_440, _t7_38);
  _t7_111 = _mm256_sub_pd(_t7_441, _t7_39);
  _t7_112 = _mm256_sub_pd(_t7_442, _t7_40);
  _t7_113 = _mm256_sub_pd(_t7_443, _t7_41);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t7_74 = _t7_110;
  _t7_75 = _t7_111;
  _t7_76 = _t7_112;
  _t7_77 = _t7_113;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 24)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_444 = _t7_74;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_445 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_446 = _t7_14;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_447 = _mm256_mul_pd(_t7_445, _t7_446);

  // 4-BLAC: 1x4 / 1x4
  _t7_448 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_444), _mm256_castpd256_pd128(_t7_447)));

  // AVX Storer:
  _t7_74 = _t7_448;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(1, 28, 24)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) Kro G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_449 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_75, _t7_76), _mm256_unpacklo_pd(_t7_77, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_450 = _t7_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_451 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_74, _t7_74, 32), _mm256_permute2f128_pd(_t7_74, _t7_74, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t7_452 = _mm256_mul_pd(_t7_450, _t7_451);

  // 4-BLAC: 4x1 - 4x1
  _t7_453 = _mm256_sub_pd(_t7_449, _t7_452);

  // AVX Storer:
  _t7_78 = _t7_453;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_454 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_78, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_455 = _t7_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_456 = _t7_14;

  // 4-BLAC: 1x4 + 1x4
  _t7_457 = _mm256_add_pd(_t7_455, _t7_456);

  // 4-BLAC: 1x4 / 1x4
  _t7_458 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_454), _mm256_castpd256_pd128(_t7_457)));

  // AVX Storer:
  _t7_79 = _t7_458;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 24)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) Kro G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_459 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_78, 2), _mm256_permute2f128_pd(_t7_78, _t7_78, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_460 = _t7_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_461 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_79, _t7_79, 32), _mm256_permute2f128_pd(_t7_79, _t7_79, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t7_462 = _mm256_mul_pd(_t7_460, _t7_461);

  // 4-BLAC: 4x1 - 4x1
  _t7_463 = _mm256_sub_pd(_t7_459, _t7_462);

  // AVX Storer:
  _t7_80 = _t7_463;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_464 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_80, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_465 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_466 = _t7_14;

  // 4-BLAC: 1x4 + 1x4
  _t7_467 = _mm256_add_pd(_t7_465, _t7_466);

  // 4-BLAC: 1x4 / 1x4
  _t7_468 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_464), _mm256_castpd256_pd128(_t7_467)));

  // AVX Storer:
  _t7_81 = _t7_468;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 24)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_469 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_80, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_470 = _t7_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_471 = _t7_81;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_472 = _mm256_mul_pd(_t7_470, _t7_471);

  // 4-BLAC: 1x4 - 1x4
  _t7_473 = _mm256_sub_pd(_t7_469, _t7_472);

  // AVX Storer:
  _t7_82 = _t7_473;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_474 = _t7_82;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_475 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_476 = _t7_14;

  // 4-BLAC: 1x4 + 1x4
  _t7_477 = _mm256_add_pd(_t7_475, _t7_476);

  // 4-BLAC: 1x4 / 1x4
  _t7_478 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_474), _mm256_castpd256_pd128(_t7_477)));

  // AVX Storer:
  _t7_82 = _t7_478;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(3, 28, 25)) - ( ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * T( G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) + ( G(h(3, 28, 25), X[28,28],h(1, 28, 24)) * T( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ),h(3, 28, 25))

  // AVX Loader:

  // 3x3 -> 4x4 - LowSymm
  _t7_479 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_75, 2), _mm256_setzero_pd()), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_76, 6), _mm256_permute2f128_pd(_t7_76, _t7_76, 129), 5), 0), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_77, 14), _mm256_permute2f128_pd(_t7_77, _t7_77, 129), 5), 32), _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_75, 2), _mm256_setzero_pd()), 8);
  _t7_480 = _mm256_blend_pd(_mm256_permute_pd(_mm256_permute2f128_pd(_mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_76, 6), _mm256_permute2f128_pd(_t7_76, _t7_76, 129), 5), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_77, 14), _mm256_permute2f128_pd(_t7_77, _t7_77, 129), 5), 32), 6), _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_75, 2), _mm256_setzero_pd()), 8);
  _t7_481 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_77, 14), _mm256_permute2f128_pd(_t7_77, _t7_77, 129), 5);
  _t7_482 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_483 = _t7_10;

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_484 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_79, _t7_81), _mm256_unpacklo_pd(_t7_82, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t7_485 = _t7_484;

  // 4-BLAC: 4x1 * 1x4
  _t7_486 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_483, _t7_483, 32), _mm256_permute2f128_pd(_t7_483, _t7_483, 32), 0), _t7_485);
  _t7_487 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_483, _t7_483, 32), _mm256_permute2f128_pd(_t7_483, _t7_483, 32), 15), _t7_485);
  _t7_488 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_483, _t7_483, 49), _mm256_permute2f128_pd(_t7_483, _t7_483, 49), 0), _t7_485);
  _t7_489 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_483, _t7_483, 49), _mm256_permute2f128_pd(_t7_483, _t7_483, 49), 15), _t7_485);

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_490 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_79, _t7_81), _mm256_unpacklo_pd(_t7_82, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t7_491 = _t7_10;

  // 4-BLAC: (4x1)^T
  _t7_492 = _t7_491;

  // 4-BLAC: 4x1 * 1x4
  _t7_493 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_490, _t7_490, 32), _mm256_permute2f128_pd(_t7_490, _t7_490, 32), 0), _t7_492);
  _t7_494 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_490, _t7_490, 32), _mm256_permute2f128_pd(_t7_490, _t7_490, 32), 15), _t7_492);
  _t7_495 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_490, _t7_490, 49), _mm256_permute2f128_pd(_t7_490, _t7_490, 49), 0), _t7_492);
  _t7_496 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_490, _t7_490, 49), _mm256_permute2f128_pd(_t7_490, _t7_490, 49), 15), _t7_492);

  // 4-BLAC: 4x4 + 4x4
  _t7_497 = _mm256_add_pd(_t7_486, _t7_493);
  _t7_498 = _mm256_add_pd(_t7_487, _t7_494);
  _t7_499 = _mm256_add_pd(_t7_488, _t7_495);
  _t7_500 = _mm256_add_pd(_t7_489, _t7_496);

  // 4-BLAC: 4x4 - 4x4
  _t7_501 = _mm256_sub_pd(_t7_479, _t7_497);
  _t7_502 = _mm256_sub_pd(_t7_480, _t7_498);
  _t7_503 = _mm256_sub_pd(_t7_481, _t7_499);
  _t7_504 = _mm256_sub_pd(_t7_482, _t7_500);

  // AVX Storer:

  // 4x4 -> 3x3 - LowSymm
  _t7_83 = _t7_501;
  _t7_84 = _t7_502;
  _t7_85 = _t7_503;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 25)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_505 = _t7_83;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_506 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_507 = _t7_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_508 = _mm256_mul_pd(_t7_506, _t7_507);

  // 4-BLAC: 1x4 / 1x4
  _t7_509 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_505), _mm256_castpd256_pd128(_t7_508)));

  // AVX Storer:
  _t7_83 = _t7_509;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) Kro G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_510 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_84, _t7_85), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_511 = _t7_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_512 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_83, _t7_83, 32), _mm256_permute2f128_pd(_t7_83, _t7_83, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t7_513 = _mm256_mul_pd(_t7_511, _t7_512);

  // 4-BLAC: 4x1 - 4x1
  _t7_514 = _mm256_sub_pd(_t7_510, _t7_513);

  // AVX Storer:
  _t7_86 = _t7_514;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 25)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_515 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_86, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_516 = _t7_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_517 = _t7_9;

  // 4-BLAC: 1x4 + 1x4
  _t7_518 = _mm256_add_pd(_t7_516, _t7_517);

  // 4-BLAC: 1x4 / 1x4
  _t7_519 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_515), _mm256_castpd256_pd128(_t7_518)));

  // AVX Storer:
  _t7_87 = _t7_519;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 25)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_520 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_86, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_521 = _t7_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_522 = _t7_87;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_523 = _mm256_mul_pd(_t7_521, _t7_522);

  // 4-BLAC: 1x4 - 1x4
  _t7_524 = _mm256_sub_pd(_t7_520, _t7_523);

  // AVX Storer:
  _t7_88 = _t7_524;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 25)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_525 = _t7_88;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_526 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_527 = _t7_9;

  // 4-BLAC: 1x4 + 1x4
  _t7_528 = _mm256_add_pd(_t7_526, _t7_527);

  // 4-BLAC: 1x4 / 1x4
  _t7_529 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_525), _mm256_castpd256_pd128(_t7_528)));

  // AVX Storer:
  _t7_88 = _t7_529;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(2, 28, 26)) - ( ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * T( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) + ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) * T( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) ) ),h(2, 28, 26))

  // AVX Loader:

  // 2x2 -> 4x4 - LowSymm
  _t7_530 = _mm256_shuffle_pd(_mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_84, 2), _mm256_setzero_pd()), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_85, 6), _mm256_permute2f128_pd(_t7_85, _t7_85, 129), 5), 0);
  _t7_531 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_85, 6), _mm256_permute2f128_pd(_t7_85, _t7_85, 129), 5);
  _t7_532 = _mm256_setzero_pd();
  _t7_533 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_534 = _t7_8;

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_535 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_87, _t7_88), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t7_536 = _t7_535;

  // 4-BLAC: 4x1 * 1x4
  _t7_537 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_534, _t7_534, 32), _mm256_permute2f128_pd(_t7_534, _t7_534, 32), 0), _t7_536);
  _t7_538 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_534, _t7_534, 32), _mm256_permute2f128_pd(_t7_534, _t7_534, 32), 15), _t7_536);
  _t7_539 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_534, _t7_534, 49), _mm256_permute2f128_pd(_t7_534, _t7_534, 49), 0), _t7_536);
  _t7_540 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_534, _t7_534, 49), _mm256_permute2f128_pd(_t7_534, _t7_534, 49), 15), _t7_536);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_541 = _mm256_blend_pd(_mm256_unpacklo_pd(_t7_87, _t7_88), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t7_542 = _t7_8;

  // 4-BLAC: (4x1)^T
  _t7_543 = _t7_542;

  // 4-BLAC: 4x1 * 1x4
  _t7_544 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_541, _t7_541, 32), _mm256_permute2f128_pd(_t7_541, _t7_541, 32), 0), _t7_543);
  _t7_545 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_541, _t7_541, 32), _mm256_permute2f128_pd(_t7_541, _t7_541, 32), 15), _t7_543);
  _t7_546 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_541, _t7_541, 49), _mm256_permute2f128_pd(_t7_541, _t7_541, 49), 0), _t7_543);
  _t7_547 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_541, _t7_541, 49), _mm256_permute2f128_pd(_t7_541, _t7_541, 49), 15), _t7_543);

  // 4-BLAC: 4x4 + 4x4
  _t7_548 = _mm256_add_pd(_t7_537, _t7_544);
  _t7_549 = _mm256_add_pd(_t7_538, _t7_545);
  _t7_550 = _mm256_add_pd(_t7_539, _t7_546);
  _t7_551 = _mm256_add_pd(_t7_540, _t7_547);

  // 4-BLAC: 4x4 - 4x4
  _t7_552 = _mm256_sub_pd(_t7_530, _t7_548);
  _t7_553 = _mm256_sub_pd(_t7_531, _t7_549);
  _t7_554 = _mm256_sub_pd(_t7_532, _t7_550);
  _t7_555 = _mm256_sub_pd(_t7_533, _t7_551);

  // AVX Storer:

  // 4x4 -> 2x2 - LowSymm
  _t7_89 = _t7_552;
  _t7_90 = _t7_553;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 26)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_556 = _t7_89;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_557 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_558 = _t7_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_559 = _mm256_mul_pd(_t7_557, _t7_558);

  // 4-BLAC: 1x4 / 1x4
  _t7_560 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_556), _mm256_castpd256_pd128(_t7_559)));

  // AVX Storer:
  _t7_89 = _t7_560;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_561 = _mm256_blend_pd(_mm256_setzero_pd(), _t7_90, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_562 = _t7_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_563 = _t7_89;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_564 = _mm256_mul_pd(_t7_562, _t7_563);

  // 4-BLAC: 1x4 - 1x4
  _t7_565 = _mm256_sub_pd(_t7_561, _t7_564);

  // AVX Storer:
  _t7_91 = _t7_565;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_566 = _t7_91;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_567 = _t7_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_568 = _t7_7;

  // 4-BLAC: 1x4 + 1x4
  _t7_569 = _mm256_add_pd(_t7_567, _t7_568);

  // 4-BLAC: 1x4 / 1x4
  _t7_570 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_566), _mm256_castpd256_pd128(_t7_569)));

  // AVX Storer:
  _t7_91 = _t7_570;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 27)) - ( ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro T( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) + ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) Kro T( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_571 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t7_90, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_572 = _t7_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_573 = _t7_91;

  // 4-BLAC: (4x1)^T
  _t7_574 = _t7_573;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_575 = _mm256_mul_pd(_t7_572, _t7_574);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_576 = _t7_91;

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_577 = _t7_0;

  // 4-BLAC: (4x1)^T
  _t7_578 = _t7_577;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_579 = _mm256_mul_pd(_t7_576, _t7_578);

  // 4-BLAC: 1x4 + 1x4
  _t7_580 = _mm256_add_pd(_t7_575, _t7_579);

  // 4-BLAC: 1x4 - 1x4
  _t7_581 = _mm256_sub_pd(_t7_571, _t7_580);

  // AVX Storer:
  _t7_92 = _t7_581;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 27)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_582 = _t7_92;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t7_583 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t7_584 = _t7_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t7_585 = _mm256_mul_pd(_t7_583, _t7_584);

  // 4-BLAC: 1x4 / 1x4
  _t7_586 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t7_582), _mm256_castpd256_pd128(_t7_585)));

  // AVX Storer:
  _t7_92 = _t7_586;

  _mm_store_sd(&(C[580]), _mm256_castpd256_pd128(_t7_42));
  _mm_store_sd(&(C[608]), _mm256_castpd256_pd128(_t7_44));
  _mm_store_sd(&(C[636]), _mm256_castpd256_pd128(_t7_46));
  _mm_store_sd(&(C[664]), _mm256_castpd256_pd128(_t7_47));
  _mm_store_sd(C + 609, _mm256_castpd256_pd128(_t7_48));
  _mm_store_sd(&(C[637]), _mm256_castpd256_pd128(_t7_52));
  _mm_store_sd(&(C[665]), _mm256_castpd256_pd128(_t7_53));
  _mm_store_sd(C + 638, _mm256_castpd256_pd128(_t7_54));
  _mm_store_sd(&(C[666]), _mm256_castpd256_pd128(_t7_56));
  _mm_store_sd(&(C[667]), _mm256_castpd256_pd128(_t7_57));
  _mm_store_sd(&(C[692]), _mm256_castpd256_pd128(_t7_58));
  _mm_store_sd(&(C[693]), _mm256_castpd256_pd128(_t7_59));
  _mm_store_sd(&(C[694]), _mm256_castpd256_pd128(_t7_60));
  _mm_store_sd(&(C[695]), _mm256_castpd256_pd128(_t7_61));
  _mm_store_sd(&(C[720]), _mm256_castpd256_pd128(_t7_62));
  _mm_store_sd(&(C[721]), _mm256_castpd256_pd128(_t7_63));
  _mm_store_sd(&(C[722]), _mm256_castpd256_pd128(_t7_64));
  _mm_store_sd(&(C[723]), _mm256_castpd256_pd128(_t7_65));
  _mm_store_sd(&(C[748]), _mm256_castpd256_pd128(_t7_66));
  _mm_store_sd(&(C[749]), _mm256_castpd256_pd128(_t7_67));
  _mm_store_sd(&(C[750]), _mm256_castpd256_pd128(_t7_68));
  _mm_store_sd(&(C[751]), _mm256_castpd256_pd128(_t7_69));
  _mm_store_sd(&(C[776]), _mm256_castpd256_pd128(_t7_70));
  _mm_store_sd(&(C[777]), _mm256_castpd256_pd128(_t7_71));
  _mm_store_sd(&(C[778]), _mm256_castpd256_pd128(_t7_72));
  _mm_store_sd(&(C[779]), _mm256_castpd256_pd128(_t7_73));
  _mm_store_sd(C + 696, _mm256_castpd256_pd128(_t7_74));
  _mm_store_sd(&(C[724]), _mm256_castpd256_pd128(_t7_79));
  _mm_store_sd(&(C[752]), _mm256_castpd256_pd128(_t7_81));
  _mm_store_sd(&(C[780]), _mm256_castpd256_pd128(_t7_82));
  _mm_store_sd(C + 725, _mm256_castpd256_pd128(_t7_83));
  _mm_store_sd(&(C[753]), _mm256_castpd256_pd128(_t7_87));
  _mm_store_sd(&(C[781]), _mm256_castpd256_pd128(_t7_88));
  _mm_store_sd(C + 754, _mm256_castpd256_pd128(_t7_89));
  _mm_store_sd(&(C[782]), _mm256_castpd256_pd128(_t7_91));
  _mm_store_sd(&(C[783]), _mm256_castpd256_pd128(_t7_92));

}
