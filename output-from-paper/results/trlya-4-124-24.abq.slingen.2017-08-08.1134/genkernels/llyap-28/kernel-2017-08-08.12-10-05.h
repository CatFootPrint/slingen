/*
 * llyap_kernel.h
 *
Decl { {u'X': Symmetric[X, (28, 28), LSMatAccess], u'C': Symmetric[C, (28, 28), LSMatAccess], u'L': LowerTriangular[L, (28, 28), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt': {'m0': 'm03.ll'}, 'ftmpyozk_lwn_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 1, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt_m03_ftmpyozk_lwn_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), X[28,28],h(1, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), X[28,28],h(1, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), L[28,28],h(1, 28, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), L[28,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), L[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), L[28,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(3, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 4), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 4), C[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 4), L[28,28],h(4, 28, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 0), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
For_{fi661;0;19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi661 + 5), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi661 + 5), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 4), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi661 + 6), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi661 + 6), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 5)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 5), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 6), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi661 + 7), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi661 + 20, 28, fi661 + 8), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi661 + 20, 28, fi661 + 8), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi661 + 20, 28, fi661 + 8), L[28,28],h(4, 28, fi661 + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi661 + 4), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 25:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 56:
For_{fi409;4;23;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), C[28,28],h(4, 28, fi409)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), L[28,28],h(fi409, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), X[28,28],h(fi409, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), X[28,28],h(fi409, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), L[28,28],h(fi409, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 28, fi409), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409), X[28,28],h(1, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), L[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), X[28,28],h(1, 28, fi409 + 1)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 1)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(2, 28, fi409)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), X[28,28],h(1, 28, fi409 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(3, 28, fi409)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + 3), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), C[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), X[28,28],h(fi409, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), L[28,28],h(fi409, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), L[28,28],h(fi409, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), X[28,28],h(fi409, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi409 + 24, 28, fi409 + 4), L[28,28],h(4, 28, fi409)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi409), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 27:
For_{fi1034;0;-fi409 + 19;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), L[28,28],h(1, 28, fi1034 + fi409 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), L[28,28],h(1, 28, fi1034 + fi409 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), L[28,28],h(1, 28, fi1034 + fi409 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 4), L[28,28],h(1, 28, fi1034 + fi409 + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1034 + fi409 + 5), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1034 + fi409 + 5), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1034 + fi409 + 5), L[28,28],h(1, 28, fi1034 + fi409 + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), L[28,28],h(1, 28, fi1034 + fi409 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), L[28,28],h(1, 28, fi1034 + fi409 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), L[28,28],h(1, 28, fi1034 + fi409 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 5), L[28,28],h(1, 28, fi1034 + fi409 + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1034 + fi409 + 6), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1034 + fi409 + 6), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1034 + fi409 + 6), L[28,28],h(1, 28, fi1034 + fi409 + 5)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 5), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), L[28,28],h(1, 28, fi1034 + fi409 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), L[28,28],h(1, 28, fi1034 + fi409 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), L[28,28],h(1, 28, fi1034 + fi409 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 6), L[28,28],h(1, 28, fi1034 + fi409 + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), L[28,28],h(1, 28, fi1034 + fi409 + 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 6), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), L[28,28],h(1, 28, fi1034 + fi409 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), L[28,28],h(1, 28, fi1034 + fi409 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), L[28,28],h(1, 28, fi1034 + fi409 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1034 + fi409 + 7), L[28,28],h(1, 28, fi1034 + fi409 + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1034 - fi409 + 20, 28, fi1034 + fi409 + 8), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1034 - fi409 + 20, 28, fi1034 + fi409 + 8), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1034 - fi409 + 20, 28, fi1034 + fi409 + 8), L[28,28],h(4, 28, fi1034 + fi409 + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1034 + fi409 + 4), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
 )Entry 28:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 4)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi409 + Max(0, -fi409 + 20) + 5), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 4), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 5)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi409 + Max(0, -fi409 + 20) + 6), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 5)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 5), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 6)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(4, 28, fi409)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(4, 28, fi409)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 6), X[28,28],h(4, 28, fi409)) ) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409), L[28,28],h(1, 28, fi409)) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 1), L[28,28],h(1, 28, fi409 + 1)) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(2, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 2), L[28,28],h(2, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 56:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 2), L[28,28],h(1, 28, fi409 + 2)) ) ) )
Eq.ann: {}
Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(3, 28, fi409)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi409 + 3), L[28,28],h(3, 28, fi409)) ) ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), X[28,28],h(1, 28, fi409 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi409 + Max(0, -fi409 + 20) + 7), L[28,28],h(1, 28, fi409 + Max(0, -fi409 + 20) + 7)) ) + Tile( (1, 1), G(h(1, 28, fi409 + 3), L[28,28],h(1, 28, fi409 + 3)) ) ) )
Eq.ann: {}
 )Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(4, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), C[28,28],h(4, 28, 24)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), L[28,28],h(24, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(24, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(24, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), L[28,28],h(24, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 59:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 60:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 61:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 62:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 63:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 64:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 65:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 66:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 67:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 68:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 69:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) ) )
Eq.ann: {}
Entry 70:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 71:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) ) ) )
Eq.ann: {}
Entry 72:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 73:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 74:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ) )
Eq.ann: {}
Entry 75:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 76:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 77:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) ) )
Eq.ann: {}
Entry 78:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ) )
Eq.ann: {}
Entry 79:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(3, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(3, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 80:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-08
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 28

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84, _t2_85, _t2_86, _t2_87,
	_t2_88, _t2_89, _t2_90, _t2_91, _t2_92, _t2_93, _t2_94, _t2_95,
	_t2_96, _t2_97, _t2_98, _t2_99, _t2_100, _t2_101, _t2_102, _t2_103,
	_t2_104, _t2_105, _t2_106, _t2_107, _t2_108, _t2_109, _t2_110, _t2_111,
	_t2_112, _t2_113, _t2_114, _t2_115, _t2_116, _t2_117, _t2_118, _t2_119,
	_t2_120, _t2_121, _t2_122, _t2_123, _t2_124, _t2_125, _t2_126, _t2_127,
	_t2_128, _t2_129, _t2_130, _t2_131, _t2_132, _t2_133, _t2_134, _t2_135,
	_t2_136, _t2_137, _t2_138, _t2_139, _t2_140, _t2_141, _t2_142, _t2_143,
	_t2_144, _t2_145, _t2_146, _t2_147, _t2_148, _t2_149, _t2_150, _t2_151,
	_t2_152, _t2_153, _t2_154, _t2_155, _t2_156, _t2_157, _t2_158, _t2_159,
	_t2_160, _t2_161, _t2_162, _t2_163, _t2_164, _t2_165, _t2_166, _t2_167,
	_t2_168, _t2_169, _t2_170, _t2_171, _t2_172, _t2_173, _t2_174, _t2_175,
	_t2_176, _t2_177, _t2_178, _t2_179, _t2_180, _t2_181, _t2_182, _t2_183,
	_t2_184, _t2_185, _t2_186, _t2_187, _t2_188, _t2_189, _t2_190, _t2_191,
	_t2_192, _t2_193, _t2_194, _t2_195, _t2_196, _t2_197, _t2_198, _t2_199,
	_t2_200, _t2_201, _t2_202, _t2_203, _t2_204, _t2_205;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31,
	_t3_32, _t3_33, _t3_34, _t3_35, _t3_36, _t3_37, _t3_38, _t3_39;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143,
	_t4_144, _t4_145, _t4_146, _t4_147, _t4_148, _t4_149, _t4_150, _t4_151,
	_t4_152, _t4_153, _t4_154, _t4_155, _t4_156, _t4_157, _t4_158, _t4_159,
	_t4_160, _t4_161, _t4_162, _t4_163, _t4_164, _t4_165, _t4_166, _t4_167,
	_t4_168, _t4_169, _t4_170, _t4_171, _t4_172, _t4_173, _t4_174, _t4_175,
	_t4_176, _t4_177, _t4_178, _t4_179, _t4_180, _t4_181, _t4_182, _t4_183,
	_t4_184, _t4_185, _t4_186, _t4_187, _t4_188, _t4_189, _t4_190, _t4_191,
	_t4_192, _t4_193, _t4_194, _t4_195, _t4_196, _t4_197, _t4_198, _t4_199,
	_t4_200, _t4_201, _t4_202, _t4_203, _t4_204, _t4_205, _t4_206, _t4_207,
	_t4_208, _t4_209, _t4_210, _t4_211, _t4_212, _t4_213, _t4_214, _t4_215,
	_t4_216, _t4_217, _t4_218, _t4_219, _t4_220, _t4_221, _t4_222, _t4_223,
	_t4_224, _t4_225, _t4_226, _t4_227, _t4_228, _t4_229, _t4_230, _t4_231,
	_t4_232, _t4_233, _t4_234, _t4_235, _t4_236, _t4_237, _t4_238, _t4_239,
	_t4_240, _t4_241, _t4_242, _t4_243, _t4_244, _t4_245, _t4_246, _t4_247,
	_t4_248, _t4_249, _t4_250, _t4_251, _t4_252, _t4_253, _t4_254, _t4_255,
	_t4_256, _t4_257, _t4_258, _t4_259, _t4_260, _t4_261, _t4_262, _t4_263,
	_t4_264, _t4_265, _t4_266, _t4_267, _t4_268, _t4_269, _t4_270, _t4_271,
	_t4_272, _t4_273, _t4_274, _t4_275, _t4_276, _t4_277, _t4_278, _t4_279,
	_t4_280, _t4_281, _t4_282, _t4_283, _t4_284, _t4_285, _t4_286, _t4_287,
	_t4_288, _t4_289, _t4_290, _t4_291, _t4_292, _t4_293, _t4_294, _t4_295,
	_t4_296, _t4_297, _t4_298, _t4_299, _t4_300, _t4_301, _t4_302, _t4_303,
	_t4_304, _t4_305, _t4_306, _t4_307, _t4_308, _t4_309, _t4_310, _t4_311,
	_t4_312, _t4_313, _t4_314, _t4_315, _t4_316, _t4_317, _t4_318, _t4_319,
	_t4_320, _t4_321, _t4_322, _t4_323, _t4_324, _t4_325, _t4_326, _t4_327,
	_t4_328, _t4_329, _t4_330, _t4_331, _t4_332, _t4_333, _t4_334, _t4_335,
	_t4_336, _t4_337, _t4_338, _t4_339, _t4_340, _t4_341, _t4_342, _t4_343,
	_t4_344, _t4_345, _t4_346, _t4_347, _t4_348, _t4_349, _t4_350, _t4_351,
	_t4_352, _t4_353, _t4_354, _t4_355, _t4_356, _t4_357, _t4_358, _t4_359,
	_t4_360, _t4_361, _t4_362, _t4_363, _t4_364, _t4_365, _t4_366, _t4_367,
	_t4_368, _t4_369, _t4_370, _t4_371, _t4_372, _t4_373, _t4_374, _t4_375,
	_t4_376, _t4_377, _t4_378, _t4_379, _t4_380, _t4_381, _t4_382, _t4_383,
	_t4_384, _t4_385, _t4_386, _t4_387, _t4_388, _t4_389, _t4_390, _t4_391,
	_t4_392, _t4_393, _t4_394, _t4_395, _t4_396, _t4_397, _t4_398, _t4_399,
	_t4_400, _t4_401, _t4_402, _t4_403, _t4_404, _t4_405, _t4_406, _t4_407,
	_t4_408, _t4_409, _t4_410, _t4_411, _t4_412, _t4_413, _t4_414, _t4_415,
	_t4_416, _t4_417, _t4_418, _t4_419, _t4_420, _t4_421, _t4_422, _t4_423,
	_t4_424, _t4_425, _t4_426, _t4_427, _t4_428, _t4_429, _t4_430, _t4_431,
	_t4_432, _t4_433, _t4_434, _t4_435, _t4_436, _t4_437, _t4_438, _t4_439,
	_t4_440, _t4_441, _t4_442, _t4_443;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27, _t10_28, _t10_29, _t10_30, _t10_31,
	_t10_32, _t10_33, _t10_34, _t10_35, _t10_36, _t10_37, _t10_38, _t10_39,
	_t10_40, _t10_41, _t10_42, _t10_43, _t10_44, _t10_45, _t10_46, _t10_47,
	_t10_48, _t10_49, _t10_50, _t10_51, _t10_52, _t10_53, _t10_54, _t10_55,
	_t10_56, _t10_57, _t10_58, _t10_59, _t10_60, _t10_61, _t10_62, _t10_63,
	_t10_64, _t10_65, _t10_66, _t10_67, _t10_68, _t10_69, _t10_70, _t10_71,
	_t10_72, _t10_73, _t10_74, _t10_75, _t10_76, _t10_77, _t10_78, _t10_79,
	_t10_80, _t10_81, _t10_82, _t10_83, _t10_84, _t10_85, _t10_86, _t10_87,
	_t10_88, _t10_89, _t10_90, _t10_91, _t10_92, _t10_93, _t10_94, _t10_95,
	_t10_96, _t10_97, _t10_98, _t10_99, _t10_100, _t10_101, _t10_102, _t10_103,
	_t10_104, _t10_105, _t10_106, _t10_107, _t10_108, _t10_109, _t10_110, _t10_111,
	_t10_112, _t10_113, _t10_114, _t10_115, _t10_116, _t10_117, _t10_118, _t10_119,
	_t10_120, _t10_121, _t10_122, _t10_123, _t10_124, _t10_125, _t10_126, _t10_127,
	_t10_128, _t10_129, _t10_130, _t10_131, _t10_132, _t10_133, _t10_134, _t10_135,
	_t10_136, _t10_137, _t10_138, _t10_139, _t10_140, _t10_141, _t10_142, _t10_143,
	_t10_144, _t10_145, _t10_146, _t10_147, _t10_148, _t10_149, _t10_150, _t10_151,
	_t10_152, _t10_153, _t10_154, _t10_155, _t10_156, _t10_157, _t10_158, _t10_159,
	_t10_160, _t10_161, _t10_162, _t10_163, _t10_164, _t10_165, _t10_166, _t10_167,
	_t10_168, _t10_169, _t10_170, _t10_171, _t10_172, _t10_173, _t10_174, _t10_175,
	_t10_176, _t10_177, _t10_178, _t10_179, _t10_180, _t10_181, _t10_182, _t10_183,
	_t10_184, _t10_185, _t10_186, _t10_187, _t10_188, _t10_189, _t10_190, _t10_191,
	_t10_192, _t10_193, _t10_194, _t10_195, _t10_196, _t10_197, _t10_198, _t10_199,
	_t10_200, _t10_201, _t10_202, _t10_203, _t10_204, _t10_205;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35, _t11_36, _t11_37, _t11_38, _t11_39;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23,
	_t12_24, _t12_25, _t12_26, _t12_27, _t12_28, _t12_29, _t12_30, _t12_31,
	_t12_32, _t12_33, _t12_34, _t12_35, _t12_36, _t12_37, _t12_38, _t12_39,
	_t12_40, _t12_41, _t12_42, _t12_43, _t12_44, _t12_45, _t12_46, _t12_47,
	_t12_48, _t12_49, _t12_50, _t12_51, _t12_52, _t12_53, _t12_54, _t12_55,
	_t12_56, _t12_57, _t12_58, _t12_59, _t12_60, _t12_61, _t12_62, _t12_63,
	_t12_64, _t12_65, _t12_66, _t12_67, _t12_68, _t12_69, _t12_70, _t12_71,
	_t12_72, _t12_73, _t12_74, _t12_75, _t12_76, _t12_77, _t12_78, _t12_79,
	_t12_80, _t12_81, _t12_82, _t12_83, _t12_84, _t12_85, _t12_86, _t12_87,
	_t12_88, _t12_89, _t12_90, _t12_91, _t12_92, _t12_93, _t12_94, _t12_95,
	_t12_96, _t12_97, _t12_98, _t12_99, _t12_100, _t12_101, _t12_102, _t12_103,
	_t12_104, _t12_105, _t12_106, _t12_107, _t12_108, _t12_109, _t12_110, _t12_111,
	_t12_112, _t12_113, _t12_114, _t12_115, _t12_116, _t12_117, _t12_118, _t12_119,
	_t12_120, _t12_121, _t12_122, _t12_123, _t12_124, _t12_125, _t12_126, _t12_127,
	_t12_128, _t12_129, _t12_130, _t12_131, _t12_132, _t12_133, _t12_134, _t12_135,
	_t12_136, _t12_137, _t12_138, _t12_139, _t12_140, _t12_141, _t12_142, _t12_143,
	_t12_144, _t12_145, _t12_146, _t12_147, _t12_148, _t12_149, _t12_150, _t12_151,
	_t12_152, _t12_153, _t12_154, _t12_155, _t12_156, _t12_157, _t12_158, _t12_159,
	_t12_160, _t12_161, _t12_162, _t12_163, _t12_164, _t12_165, _t12_166, _t12_167,
	_t12_168, _t12_169, _t12_170, _t12_171, _t12_172, _t12_173, _t12_174, _t12_175,
	_t12_176, _t12_177, _t12_178, _t12_179, _t12_180, _t12_181, _t12_182, _t12_183,
	_t12_184, _t12_185, _t12_186, _t12_187, _t12_188, _t12_189, _t12_190, _t12_191,
	_t12_192, _t12_193, _t12_194, _t12_195, _t12_196, _t12_197, _t12_198;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27, _t13_28, _t13_29, _t13_30, _t13_31,
	_t13_32, _t13_33, _t13_34, _t13_35, _t13_36, _t13_37, _t13_38, _t13_39,
	_t13_40, _t13_41, _t13_42, _t13_43, _t13_44, _t13_45, _t13_46, _t13_47,
	_t13_48, _t13_49, _t13_50, _t13_51, _t13_52, _t13_53, _t13_54, _t13_55,
	_t13_56, _t13_57, _t13_58, _t13_59, _t13_60, _t13_61, _t13_62, _t13_63,
	_t13_64, _t13_65, _t13_66, _t13_67, _t13_68, _t13_69, _t13_70, _t13_71;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35, _t16_36, _t16_37, _t16_38, _t16_39,
	_t16_40, _t16_41, _t16_42, _t16_43, _t16_44, _t16_45, _t16_46, _t16_47,
	_t16_48, _t16_49, _t16_50, _t16_51, _t16_52, _t16_53, _t16_54, _t16_55,
	_t16_56, _t16_57, _t16_58, _t16_59, _t16_60, _t16_61, _t16_62, _t16_63,
	_t16_64, _t16_65, _t16_66, _t16_67, _t16_68, _t16_69, _t16_70, _t16_71,
	_t16_72, _t16_73, _t16_74, _t16_75, _t16_76, _t16_77, _t16_78, _t16_79,
	_t16_80, _t16_81, _t16_82, _t16_83, _t16_84, _t16_85, _t16_86, _t16_87,
	_t16_88, _t16_89, _t16_90, _t16_91, _t16_92, _t16_93, _t16_94, _t16_95,
	_t16_96, _t16_97, _t16_98, _t16_99, _t16_100, _t16_101, _t16_102, _t16_103,
	_t16_104, _t16_105, _t16_106, _t16_107, _t16_108, _t16_109, _t16_110, _t16_111,
	_t16_112, _t16_113, _t16_114, _t16_115, _t16_116, _t16_117, _t16_118, _t16_119,
	_t16_120, _t16_121, _t16_122, _t16_123, _t16_124, _t16_125, _t16_126, _t16_127,
	_t16_128, _t16_129, _t16_130, _t16_131, _t16_132, _t16_133, _t16_134, _t16_135,
	_t16_136, _t16_137, _t16_138, _t16_139, _t16_140, _t16_141, _t16_142, _t16_143,
	_t16_144, _t16_145, _t16_146, _t16_147, _t16_148, _t16_149, _t16_150, _t16_151,
	_t16_152, _t16_153, _t16_154, _t16_155, _t16_156, _t16_157, _t16_158, _t16_159,
	_t16_160, _t16_161, _t16_162, _t16_163, _t16_164, _t16_165;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7,
	_t21_8, _t21_9, _t21_10, _t21_11, _t21_12, _t21_13, _t21_14, _t21_15,
	_t21_16, _t21_17, _t21_18, _t21_19, _t21_20, _t21_21, _t21_22, _t21_23,
	_t21_24, _t21_25, _t21_26, _t21_27, _t21_28, _t21_29, _t21_30, _t21_31;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19, _t23_20, _t23_21, _t23_22, _t23_23,
	_t23_24, _t23_25, _t23_26, _t23_27;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19, _t24_20, _t24_21, _t24_22, _t24_23,
	_t24_24, _t24_25, _t24_26, _t24_27, _t24_28, _t24_29, _t24_30, _t24_31,
	_t24_32, _t24_33, _t24_34, _t24_35, _t24_36, _t24_37, _t24_38, _t24_39,
	_t24_40, _t24_41, _t24_42, _t24_43, _t24_44, _t24_45, _t24_46, _t24_47,
	_t24_48, _t24_49, _t24_50, _t24_51, _t24_52, _t24_53, _t24_54, _t24_55,
	_t24_56, _t24_57, _t24_58, _t24_59, _t24_60, _t24_61, _t24_62, _t24_63,
	_t24_64, _t24_65, _t24_66, _t24_67, _t24_68, _t24_69, _t24_70, _t24_71,
	_t24_72, _t24_73, _t24_74, _t24_75, _t24_76, _t24_77, _t24_78, _t24_79,
	_t24_80, _t24_81, _t24_82, _t24_83, _t24_84, _t24_85, _t24_86, _t24_87,
	_t24_88, _t24_89, _t24_90, _t24_91, _t24_92, _t24_93, _t24_94, _t24_95,
	_t24_96, _t24_97, _t24_98, _t24_99, _t24_100, _t24_101, _t24_102, _t24_103,
	_t24_104, _t24_105, _t24_106, _t24_107, _t24_108, _t24_109, _t24_110, _t24_111,
	_t24_112, _t24_113, _t24_114, _t24_115, _t24_116, _t24_117, _t24_118, _t24_119,
	_t24_120, _t24_121, _t24_122, _t24_123, _t24_124, _t24_125, _t24_126, _t24_127,
	_t24_128, _t24_129, _t24_130, _t24_131, _t24_132, _t24_133, _t24_134, _t24_135,
	_t24_136, _t24_137, _t24_138, _t24_139, _t24_140, _t24_141, _t24_142, _t24_143,
	_t24_144, _t24_145, _t24_146, _t24_147, _t24_148, _t24_149, _t24_150, _t24_151,
	_t24_152, _t24_153, _t24_154, _t24_155, _t24_156, _t24_157, _t24_158, _t24_159,
	_t24_160, _t24_161, _t24_162, _t24_163, _t24_164, _t24_165, _t24_166, _t24_167,
	_t24_168, _t24_169, _t24_170, _t24_171, _t24_172, _t24_173, _t24_174, _t24_175,
	_t24_176, _t24_177, _t24_178, _t24_179, _t24_180, _t24_181, _t24_182, _t24_183,
	_t24_184, _t24_185, _t24_186, _t24_187, _t24_188, _t24_189, _t24_190, _t24_191,
	_t24_192, _t24_193, _t24_194, _t24_195, _t24_196, _t24_197, _t24_198, _t24_199,
	_t24_200, _t24_201, _t24_202, _t24_203, _t24_204, _t24_205;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27, _t25_28, _t25_29, _t25_30, _t25_31,
	_t25_32, _t25_33, _t25_34, _t25_35, _t25_36, _t25_37, _t25_38, _t25_39;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27, _t26_28, _t26_29, _t26_30, _t26_31,
	_t26_32, _t26_33, _t26_34, _t26_35, _t26_36, _t26_37, _t26_38, _t26_39,
	_t26_40, _t26_41, _t26_42, _t26_43, _t26_44, _t26_45, _t26_46, _t26_47,
	_t26_48, _t26_49, _t26_50, _t26_51, _t26_52, _t26_53, _t26_54, _t26_55,
	_t26_56, _t26_57, _t26_58, _t26_59, _t26_60, _t26_61, _t26_62, _t26_63,
	_t26_64, _t26_65, _t26_66, _t26_67, _t26_68, _t26_69, _t26_70, _t26_71,
	_t26_72, _t26_73, _t26_74, _t26_75, _t26_76, _t26_77, _t26_78, _t26_79,
	_t26_80, _t26_81, _t26_82, _t26_83, _t26_84, _t26_85, _t26_86, _t26_87,
	_t26_88, _t26_89, _t26_90, _t26_91, _t26_92, _t26_93, _t26_94, _t26_95,
	_t26_96, _t26_97, _t26_98, _t26_99, _t26_100, _t26_101, _t26_102, _t26_103,
	_t26_104, _t26_105, _t26_106, _t26_107, _t26_108, _t26_109, _t26_110, _t26_111,
	_t26_112, _t26_113, _t26_114, _t26_115, _t26_116, _t26_117, _t26_118, _t26_119,
	_t26_120, _t26_121, _t26_122, _t26_123, _t26_124, _t26_125, _t26_126, _t26_127,
	_t26_128, _t26_129, _t26_130, _t26_131, _t26_132, _t26_133, _t26_134, _t26_135,
	_t26_136, _t26_137, _t26_138, _t26_139, _t26_140, _t26_141, _t26_142, _t26_143,
	_t26_144, _t26_145, _t26_146, _t26_147, _t26_148, _t26_149, _t26_150, _t26_151,
	_t26_152, _t26_153, _t26_154, _t26_155, _t26_156, _t26_157, _t26_158, _t26_159,
	_t26_160, _t26_161, _t26_162, _t26_163, _t26_164, _t26_165, _t26_166, _t26_167,
	_t26_168, _t26_169, _t26_170, _t26_171, _t26_172, _t26_173, _t26_174, _t26_175,
	_t26_176, _t26_177, _t26_178, _t26_179, _t26_180, _t26_181, _t26_182, _t26_183,
	_t26_184, _t26_185, _t26_186, _t26_187, _t26_188, _t26_189, _t26_190, _t26_191,
	_t26_192, _t26_193, _t26_194, _t26_195, _t26_196, _t26_197, _t26_198, _t26_199,
	_t26_200, _t26_201, _t26_202, _t26_203, _t26_204, _t26_205;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11, _t27_12, _t27_13, _t27_14, _t27_15,
	_t27_16, _t27_17, _t27_18, _t27_19, _t27_20, _t27_21, _t27_22, _t27_23,
	_t27_24, _t27_25, _t27_26, _t27_27, _t27_28, _t27_29, _t27_30, _t27_31,
	_t27_32, _t27_33, _t27_34, _t27_35, _t27_36, _t27_37, _t27_38, _t27_39,
	_t27_40, _t27_41, _t27_42, _t27_43, _t27_44, _t27_45, _t27_46, _t27_47,
	_t27_48, _t27_49, _t27_50, _t27_51, _t27_52, _t27_53, _t27_54, _t27_55,
	_t27_56, _t27_57, _t27_58, _t27_59, _t27_60, _t27_61, _t27_62, _t27_63,
	_t27_64, _t27_65, _t27_66, _t27_67, _t27_68, _t27_69, _t27_70, _t27_71;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7,
	_t28_8, _t28_9, _t28_10, _t28_11, _t28_12, _t28_13, _t28_14, _t28_15,
	_t28_16, _t28_17, _t28_18, _t28_19, _t28_20, _t28_21, _t28_22, _t28_23,
	_t28_24, _t28_25, _t28_26, _t28_27, _t28_28, _t28_29, _t28_30, _t28_31;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23,
	_t30_24, _t30_25, _t30_26, _t30_27, _t30_28, _t30_29, _t30_30, _t30_31,
	_t30_32, _t30_33, _t30_34, _t30_35, _t30_36, _t30_37, _t30_38, _t30_39,
	_t30_40, _t30_41, _t30_42, _t30_43, _t30_44, _t30_45, _t30_46, _t30_47,
	_t30_48, _t30_49, _t30_50, _t30_51, _t30_52, _t30_53, _t30_54, _t30_55,
	_t30_56, _t30_57, _t30_58, _t30_59, _t30_60, _t30_61, _t30_62, _t30_63,
	_t30_64, _t30_65, _t30_66, _t30_67, _t30_68, _t30_69, _t30_70, _t30_71,
	_t30_72, _t30_73, _t30_74, _t30_75, _t30_76, _t30_77, _t30_78, _t30_79,
	_t30_80, _t30_81, _t30_82, _t30_83, _t30_84, _t30_85, _t30_86, _t30_87,
	_t30_88, _t30_89, _t30_90, _t30_91, _t30_92, _t30_93, _t30_94, _t30_95,
	_t30_96, _t30_97, _t30_98, _t30_99, _t30_100, _t30_101, _t30_102, _t30_103,
	_t30_104, _t30_105, _t30_106, _t30_107, _t30_108, _t30_109, _t30_110, _t30_111,
	_t30_112, _t30_113, _t30_114, _t30_115, _t30_116, _t30_117, _t30_118, _t30_119,
	_t30_120, _t30_121, _t30_122, _t30_123, _t30_124, _t30_125, _t30_126, _t30_127,
	_t30_128, _t30_129, _t30_130, _t30_131, _t30_132, _t30_133, _t30_134, _t30_135,
	_t30_136, _t30_137, _t30_138, _t30_139, _t30_140, _t30_141, _t30_142, _t30_143,
	_t30_144, _t30_145, _t30_146, _t30_147, _t30_148, _t30_149, _t30_150, _t30_151,
	_t30_152, _t30_153, _t30_154, _t30_155, _t30_156, _t30_157, _t30_158, _t30_159,
	_t30_160, _t30_161, _t30_162, _t30_163, _t30_164, _t30_165, _t30_166, _t30_167,
	_t30_168, _t30_169, _t30_170, _t30_171, _t30_172, _t30_173;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23,
	_t31_24, _t31_25, _t31_26, _t31_27;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11, _t32_12, _t32_13, _t32_14, _t32_15,
	_t32_16, _t32_17, _t32_18, _t32_19, _t32_20, _t32_21, _t32_22, _t32_23;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23,
	_t33_24, _t33_25, _t33_26, _t33_27;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11, _t34_12, _t34_13, _t34_14, _t34_15,
	_t34_16, _t34_17, _t34_18, _t34_19, _t34_20, _t34_21, _t34_22, _t34_23,
	_t34_24, _t34_25, _t34_26, _t34_27, _t34_28, _t34_29, _t34_30, _t34_31,
	_t34_32, _t34_33, _t34_34, _t34_35, _t34_36, _t34_37, _t34_38, _t34_39,
	_t34_40, _t34_41, _t34_42, _t34_43, _t34_44, _t34_45, _t34_46, _t34_47,
	_t34_48, _t34_49, _t34_50, _t34_51, _t34_52, _t34_53, _t34_54, _t34_55,
	_t34_56, _t34_57, _t34_58, _t34_59, _t34_60, _t34_61, _t34_62, _t34_63,
	_t34_64, _t34_65, _t34_66, _t34_67, _t34_68, _t34_69, _t34_70, _t34_71,
	_t34_72, _t34_73, _t34_74, _t34_75, _t34_76, _t34_77, _t34_78, _t34_79,
	_t34_80, _t34_81, _t34_82, _t34_83, _t34_84, _t34_85, _t34_86, _t34_87,
	_t34_88, _t34_89, _t34_90, _t34_91, _t34_92, _t34_93, _t34_94, _t34_95,
	_t34_96, _t34_97, _t34_98, _t34_99, _t34_100, _t34_101, _t34_102, _t34_103,
	_t34_104, _t34_105, _t34_106, _t34_107, _t34_108, _t34_109, _t34_110, _t34_111,
	_t34_112, _t34_113, _t34_114, _t34_115, _t34_116, _t34_117, _t34_118, _t34_119,
	_t34_120, _t34_121, _t34_122, _t34_123, _t34_124, _t34_125, _t34_126, _t34_127,
	_t34_128, _t34_129, _t34_130, _t34_131, _t34_132, _t34_133, _t34_134, _t34_135,
	_t34_136, _t34_137, _t34_138, _t34_139, _t34_140, _t34_141, _t34_142, _t34_143,
	_t34_144, _t34_145, _t34_146, _t34_147, _t34_148, _t34_149, _t34_150, _t34_151,
	_t34_152, _t34_153, _t34_154, _t34_155, _t34_156, _t34_157, _t34_158, _t34_159,
	_t34_160, _t34_161, _t34_162, _t34_163, _t34_164, _t34_165, _t34_166, _t34_167,
	_t34_168, _t34_169, _t34_170, _t34_171, _t34_172, _t34_173, _t34_174, _t34_175,
	_t34_176, _t34_177, _t34_178, _t34_179, _t34_180, _t34_181, _t34_182, _t34_183,
	_t34_184, _t34_185, _t34_186, _t34_187, _t34_188, _t34_189, _t34_190, _t34_191,
	_t34_192, _t34_193, _t34_194, _t34_195, _t34_196, _t34_197, _t34_198, _t34_199,
	_t34_200, _t34_201, _t34_202, _t34_203, _t34_204, _t34_205, _t34_206, _t34_207,
	_t34_208, _t34_209, _t34_210, _t34_211, _t34_212, _t34_213, _t34_214, _t34_215,
	_t34_216, _t34_217, _t34_218, _t34_219, _t34_220, _t34_221, _t34_222, _t34_223,
	_t34_224, _t34_225, _t34_226, _t34_227, _t34_228, _t34_229, _t34_230, _t34_231,
	_t34_232, _t34_233, _t34_234, _t34_235, _t34_236, _t34_237, _t34_238, _t34_239,
	_t34_240, _t34_241, _t34_242, _t34_243, _t34_244, _t34_245, _t34_246, _t34_247,
	_t34_248, _t34_249, _t34_250, _t34_251, _t34_252, _t34_253, _t34_254, _t34_255;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27, _t35_28, _t35_29, _t35_30, _t35_31;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7,
	_t36_8, _t36_9, _t36_10, _t36_11, _t36_12, _t36_13, _t36_14, _t36_15,
	_t36_16, _t36_17, _t36_18, _t36_19, _t36_20, _t36_21, _t36_22, _t36_23,
	_t36_24, _t36_25, _t36_26, _t36_27, _t36_28, _t36_29, _t36_30, _t36_31;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19, _t37_20, _t37_21, _t37_22, _t37_23,
	_t37_24, _t37_25, _t37_26, _t37_27, _t37_28, _t37_29, _t37_30, _t37_31,
	_t37_32, _t37_33, _t37_34, _t37_35, _t37_36, _t37_37, _t37_38, _t37_39,
	_t37_40, _t37_41, _t37_42, _t37_43, _t37_44, _t37_45, _t37_46, _t37_47,
	_t37_48, _t37_49, _t37_50, _t37_51, _t37_52, _t37_53, _t37_54, _t37_55,
	_t37_56, _t37_57, _t37_58, _t37_59, _t37_60, _t37_61, _t37_62, _t37_63,
	_t37_64, _t37_65, _t37_66, _t37_67, _t37_68, _t37_69, _t37_70, _t37_71,
	_t37_72, _t37_73, _t37_74, _t37_75, _t37_76, _t37_77, _t37_78, _t37_79,
	_t37_80, _t37_81, _t37_82, _t37_83, _t37_84, _t37_85, _t37_86, _t37_87,
	_t37_88, _t37_89, _t37_90, _t37_91, _t37_92, _t37_93, _t37_94, _t37_95,
	_t37_96, _t37_97, _t37_98, _t37_99, _t37_100, _t37_101, _t37_102, _t37_103,
	_t37_104, _t37_105, _t37_106, _t37_107, _t37_108, _t37_109, _t37_110, _t37_111,
	_t37_112, _t37_113, _t37_114, _t37_115, _t37_116, _t37_117, _t37_118, _t37_119,
	_t37_120, _t37_121, _t37_122, _t37_123, _t37_124, _t37_125, _t37_126, _t37_127,
	_t37_128, _t37_129, _t37_130, _t37_131, _t37_132, _t37_133, _t37_134, _t37_135,
	_t37_136, _t37_137, _t37_138, _t37_139, _t37_140, _t37_141, _t37_142, _t37_143,
	_t37_144, _t37_145, _t37_146, _t37_147, _t37_148, _t37_149, _t37_150, _t37_151,
	_t37_152, _t37_153, _t37_154, _t37_155;

  _t0_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[0])));
  _t0_12 = _mm256_castpd128_pd256(_mm_load_sd(&(L[0])));
  _t0_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(C + 28)), _mm256_castpd128_pd256(_mm_load_sd(C + 56))), _mm256_castpd128_pd256(_mm_load_sd(C + 84)), 32);
  _t0_11 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 28)), _mm256_castpd128_pd256(_mm_load_sd(L + 56))), _mm256_castpd128_pd256(_mm_load_sd(L + 84)), 32);
  _t0_10 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29])));
  _t0_9 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 57)), _mm256_castpd128_pd256(_mm_load_sd(L + 85)), 0);
  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[58])));
  _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(L[86])));
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[87])));
  _t0_19 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29])));
  _t0_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[28])));
  _t0_20 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(C + 57)), _mm256_castpd128_pd256(_mm_load_sd(C + 85)), 0);
  _t0_4 = _mm256_broadcast_sd(&(L[28]));
  _t0_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 56)), _mm256_castpd128_pd256(_mm_load_sd(L + 84)), 0);
  _t0_23 = _mm256_castpd128_pd256(_mm_load_sd(&(C[58])));
  _t0_2 = _mm256_maskload_pd(L + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_24 = _mm256_castpd128_pd256(_mm_load_sd(&(C[86])));
  _t0_1 = _mm256_maskload_pd(L + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_25 = _mm256_castpd128_pd256(_mm_load_sd(&(C[87])));
  _t0_0 = _mm256_maskload_pd(L + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = S(h(1, 28, 0), ( G(h(1, 28, 0), X[28,28],h(1, 28, 0)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_26 = _t0_13;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_27 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_28 = _t0_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_29 = _mm256_mul_pd(_t0_27, _t0_28);

  // 4-BLAC: 1x4 / 1x4
  _t0_30 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_26), _mm256_castpd256_pd128(_t0_29)));

  // AVX Storer:
  _t0_13 = _t0_30;

  // Generating : X[28,28] = S(h(3, 28, 1), ( G(h(3, 28, 1), X[28,28],h(1, 28, 0)) - ( G(h(3, 28, 1), L[28,28],h(1, 28, 0)) Kro G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_31 = _t0_14;

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_32 = _t0_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_34 = _mm256_mul_pd(_t0_32, _t0_33);

  // 4-BLAC: 4x1 - 4x1
  _t0_35 = _mm256_sub_pd(_t0_31, _t0_34);

  // AVX Storer:
  _t0_14 = _t0_35;

  // Generating : X[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 1), L[28,28],h(1, 28, 1)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_36 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_37 = _t0_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_38 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t0_39 = _mm256_add_pd(_t0_37, _t0_38);

  // 4-BLAC: 1x4 / 1x4
  _t0_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_36), _mm256_castpd256_pd128(_t0_39)));

  // AVX Storer:
  _t0_15 = _t0_40;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 0)) - ( G(h(2, 28, 2), L[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 2), _mm256_permute2f128_pd(_t0_14, _t0_14, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_42 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_44 = _mm256_mul_pd(_t0_42, _t0_43);

  // 4-BLAC: 4x1 - 4x1
  _t0_45 = _mm256_sub_pd(_t0_41, _t0_44);

  // AVX Storer:
  _t0_16 = _t0_45;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 2), L[28,28],h(1, 28, 2)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_46 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_47 = _t0_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_48 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t0_49 = _mm256_add_pd(_t0_47, _t0_48);

  // 4-BLAC: 1x4 / 1x4
  _t0_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_46), _mm256_castpd256_pd128(_t0_49)));

  // AVX Storer:
  _t0_17 = _t0_50;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 0)) - ( G(h(1, 28, 3), L[28,28],h(1, 28, 2)) Kro G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_51 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_52 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_53 = _t0_17;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_54 = _mm256_mul_pd(_t0_52, _t0_53);

  // 4-BLAC: 1x4 - 1x4
  _t0_55 = _mm256_sub_pd(_t0_51, _t0_54);

  // AVX Storer:
  _t0_18 = _t0_55;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 3), L[28,28],h(1, 28, 3)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_56 = _t0_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_57 = _t0_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_58 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t0_59 = _mm256_add_pd(_t0_57, _t0_58);

  // 4-BLAC: 1x4 / 1x4
  _t0_60 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_56), _mm256_castpd256_pd128(_t0_59)));

  // AVX Storer:
  _t0_18 = _t0_60;

  // Generating : X[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), X[28,28],h(1, 28, 1)) - ( ( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) + ( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_61 = _t0_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_62 = _t0_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_63 = _t0_15;

  // 4-BLAC: (4x1)^T
  _t0_64 = _t0_63;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_65 = _mm256_mul_pd(_t0_62, _t0_64);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_66 = _t0_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_67 = _t0_5;

  // 4-BLAC: (4x1)^T
  _t0_68 = _t0_67;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_69 = _mm256_mul_pd(_t0_66, _t0_68);

  // 4-BLAC: 1x4 + 1x4
  _t0_70 = _mm256_add_pd(_t0_65, _t0_69);

  // 4-BLAC: 1x4 - 1x4
  _t0_71 = _mm256_sub_pd(_t0_61, _t0_70);

  // AVX Storer:
  _t0_19 = _t0_71;

  // Generating : X[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), X[28,28],h(1, 28, 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_72 = _t0_19;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_73 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_74 = _t0_10;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_75 = _mm256_mul_pd(_t0_73, _t0_74);

  // 4-BLAC: 1x4 / 1x4
  _t0_76 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_72), _mm256_castpd256_pd128(_t0_75)));

  // AVX Storer:
  _t0_19 = _t0_76;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 1)) - ( G(h(2, 28, 2), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_77 = _t0_20;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_78 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_17, _t0_18), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_79 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_80 = _t0_79;

  // 4-BLAC: 4x1 Kro 1x4
  _t0_81 = _mm256_mul_pd(_t0_78, _t0_80);

  // 4-BLAC: 4x1 - 4x1
  _t0_82 = _mm256_sub_pd(_t0_77, _t0_81);

  // AVX Storer:
  _t0_20 = _t0_82;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 1)) - ( G(h(2, 28, 2), L[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_83 = _t0_20;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_84 = _t0_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_85 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_86 = _t0_85;

  // 4-BLAC: 4x1 Kro 1x4
  _t0_87 = _mm256_mul_pd(_t0_84, _t0_86);

  // 4-BLAC: 4x1 - 4x1
  _t0_88 = _mm256_sub_pd(_t0_83, _t0_87);

  // AVX Storer:
  _t0_20 = _t0_88;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 1)) - ( G(h(2, 28, 2), L[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_89 = _t0_20;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_90 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_91 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_19, _t0_19, 32), _mm256_permute2f128_pd(_t0_19, _t0_19, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_92 = _mm256_mul_pd(_t0_90, _t0_91);

  // 4-BLAC: 4x1 - 4x1
  _t0_93 = _mm256_sub_pd(_t0_89, _t0_92);

  // AVX Storer:
  _t0_20 = _t0_93;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 2), L[28,28],h(1, 28, 2)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_94 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_20, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_95 = _t0_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_96 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t0_97 = _mm256_add_pd(_t0_95, _t0_96);

  // 4-BLAC: 1x4 / 1x4
  _t0_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_94), _mm256_castpd256_pd128(_t0_97)));

  // AVX Storer:
  _t0_21 = _t0_98;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 3), L[28,28],h(1, 28, 2)) Kro G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_99 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_20, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_100 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_101 = _t0_21;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_102 = _mm256_mul_pd(_t0_100, _t0_101);

  // 4-BLAC: 1x4 - 1x4
  _t0_103 = _mm256_sub_pd(_t0_99, _t0_102);

  // AVX Storer:
  _t0_22 = _t0_103;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 3), L[28,28],h(1, 28, 3)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_104 = _t0_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_105 = _t0_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_106 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t0_107 = _mm256_add_pd(_t0_105, _t0_106);

  // 4-BLAC: 1x4 / 1x4
  _t0_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_104), _mm256_castpd256_pd128(_t0_107)));

  // AVX Storer:
  _t0_22 = _t0_108;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 2)) - ( ( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) + ( G(h(1, 28, 2), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_109 = _t0_23;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_110 = _t0_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_111 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_112 = _t0_111;

  // 4-BLAC: 1x4 * 4x1
  _t0_113 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_110, _t0_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_110, _t0_112), _mm256_mul_pd(_t0_110, _t0_112), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_110, _t0_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_110, _t0_112), _mm256_mul_pd(_t0_110, _t0_112), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_110, _t0_112), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_110, _t0_112), _mm256_mul_pd(_t0_110, _t0_112), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_114 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_115 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t0_116 = _t0_115;

  // 4-BLAC: 1x4 * 4x1
  _t0_117 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_114, _t0_116), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_114, _t0_116), _mm256_mul_pd(_t0_114, _t0_116), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_114, _t0_116), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_114, _t0_116), _mm256_mul_pd(_t0_114, _t0_116), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_114, _t0_116), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_114, _t0_116), _mm256_mul_pd(_t0_114, _t0_116), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_118 = _mm256_add_pd(_t0_113, _t0_117);

  // 4-BLAC: 1x4 - 1x4
  _t0_119 = _mm256_sub_pd(_t0_109, _t0_118);

  // AVX Storer:
  _t0_23 = _t0_119;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_120 = _t0_23;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_121 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_122 = _t0_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_123 = _mm256_mul_pd(_t0_121, _t0_122);

  // 4-BLAC: 1x4 / 1x4
  _t0_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_120), _mm256_castpd256_pd128(_t0_123)));

  // AVX Storer:
  _t0_23 = _t0_124;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 3), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_125 = _t0_24;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_126 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_127 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t0_128 = _t0_127;

  // 4-BLAC: 1x4 * 4x1
  _t0_129 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_126, _t0_128), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_126, _t0_128), _mm256_mul_pd(_t0_126, _t0_128), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_126, _t0_128), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_126, _t0_128), _mm256_mul_pd(_t0_126, _t0_128), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_126, _t0_128), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_126, _t0_128), _mm256_mul_pd(_t0_126, _t0_128), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_130 = _mm256_sub_pd(_t0_125, _t0_129);

  // AVX Storer:
  _t0_24 = _t0_130;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 3), L[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_131 = _t0_24;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_132 = _t0_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_133 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_134 = _t0_133;

  // 4-BLAC: 1x4 * 4x1
  _t0_135 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_132, _t0_134), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_132, _t0_134), _mm256_mul_pd(_t0_132, _t0_134), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_132, _t0_134), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_132, _t0_134), _mm256_mul_pd(_t0_132, _t0_134), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_132, _t0_134), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_132, _t0_134), _mm256_mul_pd(_t0_132, _t0_134), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_136 = _mm256_sub_pd(_t0_131, _t0_135);

  // AVX Storer:
  _t0_24 = _t0_136;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 3), L[28,28],h(1, 28, 2)) Kro G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_137 = _t0_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_138 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_139 = _t0_23;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_140 = _mm256_mul_pd(_t0_138, _t0_139);

  // 4-BLAC: 1x4 - 1x4
  _t0_141 = _mm256_sub_pd(_t0_137, _t0_140);

  // AVX Storer:
  _t0_24 = _t0_141;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 3), L[28,28],h(1, 28, 3)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_142 = _t0_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_143 = _t0_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_144 = _t0_8;

  // 4-BLAC: 1x4 + 1x4
  _t0_145 = _mm256_add_pd(_t0_143, _t0_144);

  // 4-BLAC: 1x4 / 1x4
  _t0_146 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_142), _mm256_castpd256_pd128(_t0_145)));

  // AVX Storer:
  _t0_24 = _t0_146;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 3)) - ( ( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), X[28,28],h(3, 28, 0)) ) ) + ( G(h(1, 28, 3), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_147 = _t0_25;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_148 = _t0_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_149 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_unpacklo_pd(_t0_24, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_150 = _t0_149;

  // 4-BLAC: 1x4 * 4x1
  _t0_151 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_148, _t0_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_148, _t0_150), _mm256_mul_pd(_t0_148, _t0_150), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_148, _t0_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_148, _t0_150), _mm256_mul_pd(_t0_148, _t0_150), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_148, _t0_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_148, _t0_150), _mm256_mul_pd(_t0_148, _t0_150), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_152 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_unpacklo_pd(_t0_24, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_153 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t0_154 = _t0_153;

  // 4-BLAC: 1x4 * 4x1
  _t0_155 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_152, _t0_154), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_152, _t0_154), _mm256_mul_pd(_t0_152, _t0_154), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_152, _t0_154), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_152, _t0_154), _mm256_mul_pd(_t0_152, _t0_154), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_152, _t0_154), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_152, _t0_154), _mm256_mul_pd(_t0_152, _t0_154), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_156 = _mm256_add_pd(_t0_151, _t0_155);

  // 4-BLAC: 1x4 - 1x4
  _t0_157 = _mm256_sub_pd(_t0_147, _t0_156);

  // AVX Storer:
  _t0_25 = _t0_157;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_158 = _t0_25;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_159 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_160 = _t0_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_161 = _mm256_mul_pd(_t0_159, _t0_160);

  // 4-BLAC: 1x4 / 1x4
  _t0_162 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_158), _mm256_castpd256_pd128(_t0_161)));

  // AVX Storer:
  _t0_25 = _t0_162;

  // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, i100 + 4), ( G(h(4, 28, i100 + 4), C[28,28],h(4, 28, 0)) - ( G(h(4, 28, i100 + 4), L[28,28],h(4, 28, 0)) * G(h(4, 28, 0), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_163 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_13, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_15, _t0_19), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_unpacklo_pd(_t0_23, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_unpacklo_pd(_t0_24, _t0_25), 32), 0), 32);
  _t0_164 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_15, _t0_19), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_unpacklo_pd(_t0_23, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_unpacklo_pd(_t0_24, _t0_25), 32), 3), 32);
  _t0_165 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_unpacklo_pd(_t0_23, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_21), _mm256_unpacklo_pd(_t0_23, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_unpacklo_pd(_t0_24, _t0_25), 32), 3), 12);
  _t0_166 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_18, _t0_22), _mm256_unpacklo_pd(_t0_24, _t0_25), 32);


  for( int i100 = 0; i100 <= 23; i100+=4 ) {
    _t1_20 = _asm256_loadu_pd(C + 28*i100 + 112);
    _t1_21 = _asm256_loadu_pd(C + 28*i100 + 140);
    _t1_22 = _asm256_loadu_pd(C + 28*i100 + 168);
    _t1_23 = _asm256_loadu_pd(C + 28*i100 + 196);
    _t1_15 = _mm256_broadcast_sd(L + 28*i100 + 112);
    _t1_14 = _mm256_broadcast_sd(L + 28*i100 + 113);
    _t1_13 = _mm256_broadcast_sd(L + 28*i100 + 114);
    _t1_12 = _mm256_broadcast_sd(L + 28*i100 + 115);
    _t1_11 = _mm256_broadcast_sd(L + 28*i100 + 140);
    _t1_10 = _mm256_broadcast_sd(L + 28*i100 + 141);
    _t1_9 = _mm256_broadcast_sd(L + 28*i100 + 142);
    _t1_8 = _mm256_broadcast_sd(L + 28*i100 + 143);
    _t1_7 = _mm256_broadcast_sd(L + 28*i100 + 168);
    _t1_6 = _mm256_broadcast_sd(L + 28*i100 + 169);
    _t1_5 = _mm256_broadcast_sd(L + 28*i100 + 170);
    _t1_4 = _mm256_broadcast_sd(L + 28*i100 + 171);
    _t1_3 = _mm256_broadcast_sd(L + 28*i100 + 196);
    _t1_2 = _mm256_broadcast_sd(L + 28*i100 + 197);
    _t1_1 = _mm256_broadcast_sd(L + 28*i100 + 198);
    _t1_0 = _mm256_broadcast_sd(L + 28*i100 + 199);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t1_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_15, _t0_163), _mm256_mul_pd(_t1_14, _t0_164)), _mm256_add_pd(_mm256_mul_pd(_t1_13, _t0_165), _mm256_mul_pd(_t1_12, _t0_166)));
    _t1_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_11, _t0_163), _mm256_mul_pd(_t1_10, _t0_164)), _mm256_add_pd(_mm256_mul_pd(_t1_9, _t0_165), _mm256_mul_pd(_t1_8, _t0_166)));
    _t1_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_7, _t0_163), _mm256_mul_pd(_t1_6, _t0_164)), _mm256_add_pd(_mm256_mul_pd(_t1_5, _t0_165), _mm256_mul_pd(_t1_4, _t0_166)));
    _t1_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_3, _t0_163), _mm256_mul_pd(_t1_2, _t0_164)), _mm256_add_pd(_mm256_mul_pd(_t1_1, _t0_165), _mm256_mul_pd(_t1_0, _t0_166)));

    // 4-BLAC: 4x4 - 4x4
    _t1_20 = _mm256_sub_pd(_t1_20, _t1_16);
    _t1_21 = _mm256_sub_pd(_t1_21, _t1_17);
    _t1_22 = _mm256_sub_pd(_t1_22, _t1_18);
    _t1_23 = _mm256_sub_pd(_t1_23, _t1_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i100 + 112, _t1_20);
    _asm256_storeu_pd(C + 28*i100 + 140, _t1_21);
    _asm256_storeu_pd(C + 28*i100 + 168, _t1_22);
    _asm256_storeu_pd(C + 28*i100 + 196, _t1_23);
  }


  for( int fi661 = 0; fi661 <= 19; fi661+=4 ) {
    _t2_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 112])));
    _t2_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 116])));
    _t2_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 113])));
    _t2_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 114])));
    _t2_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 115])));
    _t2_11 = _asm256_loadu_pd(C + 28*fi661 + 140);
    _t2_12 = _asm256_loadu_pd(C + 28*fi661 + 168);
    _t2_13 = _asm256_loadu_pd(C + 28*fi661 + 196);
    _t2_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 200)), 32);
    _t2_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 145])));
    _t2_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 201)), 0);
    _t2_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 174])));
    _t2_1 = _mm256_broadcast_sd(&(L[29*fi661 + 202]));
    _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 203])));

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_27 = _t2_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_28 = _t2_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_29 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t2_30 = _mm256_add_pd(_t2_28, _t2_29);

    // 4-BLAC: 1x4 / 1x4
    _t2_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_27), _mm256_castpd256_pd128(_t2_30)));

    // AVX Storer:
    _t2_7 = _t2_31;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_32 = _t2_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_33 = _t2_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_34 = _t0_5;

    // 4-BLAC: (4x1)^T
    _t2_35 = _t2_34;

    // 4-BLAC: 1x4 Kro 1x4
    _t2_36 = _mm256_mul_pd(_t2_33, _t2_35);

    // 4-BLAC: 1x4 - 1x4
    _t2_37 = _mm256_sub_pd(_t2_32, _t2_36);

    // AVX Storer:
    _t2_8 = _t2_37;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_38 = _t2_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_39 = _t2_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_40 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t2_41 = _mm256_add_pd(_t2_39, _t2_40);

    // 4-BLAC: 1x4 / 1x4
    _t2_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_38), _mm256_castpd256_pd128(_t2_41)));

    // AVX Storer:
    _t2_8 = _t2_42;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi661 + 4), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_43 = _t2_9;

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_44 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_45 = _t0_2;

    // 4-BLAC: (1x4)^T
    _t2_46 = _t2_45;

    // 4-BLAC: 1x4 * 4x1
    _t2_47 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_mul_pd(_t2_44, _t2_46), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_mul_pd(_t2_44, _t2_46), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_mul_pd(_t2_44, _t2_46), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_48 = _mm256_sub_pd(_t2_43, _t2_47);

    // AVX Storer:
    _t2_9 = _t2_48;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_49 = _t2_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_50 = _t2_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_51 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t2_52 = _mm256_add_pd(_t2_50, _t2_51);

    // 4-BLAC: 1x4 / 1x4
    _t2_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_49), _mm256_castpd256_pd128(_t2_52)));

    // AVX Storer:
    _t2_9 = _t2_53;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi661 + 4), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_54 = _t2_10;

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_55 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_56 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t2_57 = _t2_56;

    // 4-BLAC: 1x4 * 4x1
    _t2_58 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_mul_pd(_t2_55, _t2_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_mul_pd(_t2_55, _t2_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_mul_pd(_t2_55, _t2_57), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_59 = _mm256_sub_pd(_t2_54, _t2_58);

    // AVX Storer:
    _t2_10 = _t2_59;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 4), ( G(h(1, 28, fi661 + 4), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi661 + 4), L[28,28],h(1, 28, fi661 + 4)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_60 = _t2_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_61 = _t2_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_62 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t2_63 = _mm256_add_pd(_t2_61, _t2_62);

    // 4-BLAC: 1x4 / 1x4
    _t2_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_60), _mm256_castpd256_pd128(_t2_63)));

    // AVX Storer:
    _t2_10 = _t2_64;

    // Generating : X[28,28] = S(h(3, 28, fi661 + 5), ( G(h(3, 28, fi661 + 5), X[28,28],h(4, 28, 0)) - ( G(h(3, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 4)) * G(h(1, 28, fi661 + 4), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

    // AVX Loader:

    // 3x4 -> 4x4
    _t2_65 = _t2_11;
    _t2_66 = _t2_12;
    _t2_67 = _t2_13;
    _t2_68 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t2_69 = _t2_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t2_70 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 32), _mm256_permute2f128_pd(_t2_69, _t2_69, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));
    _t2_71 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 32), _mm256_permute2f128_pd(_t2_69, _t2_69, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));
    _t2_72 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 49), _mm256_permute2f128_pd(_t2_69, _t2_69, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));
    _t2_73 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 49), _mm256_permute2f128_pd(_t2_69, _t2_69, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));

    // 4-BLAC: 4x4 - 4x4
    _t2_74 = _mm256_sub_pd(_t2_65, _t2_70);
    _t2_75 = _mm256_sub_pd(_t2_66, _t2_71);
    _t2_76 = _mm256_sub_pd(_t2_67, _t2_72);
    _t2_77 = _mm256_sub_pd(_t2_68, _t2_73);

    // AVX Storer:
    _t2_11 = _t2_74;
    _t2_12 = _t2_75;
    _t2_13 = _t2_76;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_78 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_79 = _t2_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_80 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t2_81 = _mm256_add_pd(_t2_79, _t2_80);

    // 4-BLAC: 1x4 / 1x4
    _t2_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_78), _mm256_castpd256_pd128(_t2_81)));

    // AVX Storer:
    _t2_14 = _t2_82;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_83 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_84 = _t2_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_85 = _t0_5;

    // 4-BLAC: (4x1)^T
    _t2_86 = _t2_85;

    // 4-BLAC: 1x4 Kro 1x4
    _t2_87 = _mm256_mul_pd(_t2_84, _t2_86);

    // 4-BLAC: 1x4 - 1x4
    _t2_88 = _mm256_sub_pd(_t2_83, _t2_87);

    // AVX Storer:
    _t2_15 = _t2_88;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_89 = _t2_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_90 = _t2_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_91 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t2_92 = _mm256_add_pd(_t2_90, _t2_91);

    // 4-BLAC: 1x4 / 1x4
    _t2_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_89), _mm256_castpd256_pd128(_t2_92)));

    // AVX Storer:
    _t2_15 = _t2_93;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi661 + 5), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_94 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_95 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_96 = _t0_2;

    // 4-BLAC: (1x4)^T
    _t2_97 = _t2_96;

    // 4-BLAC: 1x4 * 4x1
    _t2_98 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_mul_pd(_t2_95, _t2_97), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_mul_pd(_t2_95, _t2_97), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_mul_pd(_t2_95, _t2_97), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_99 = _mm256_sub_pd(_t2_94, _t2_98);

    // AVX Storer:
    _t2_16 = _t2_99;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_100 = _t2_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_101 = _t2_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_102 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t2_103 = _mm256_add_pd(_t2_101, _t2_102);

    // 4-BLAC: 1x4 / 1x4
    _t2_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_100), _mm256_castpd256_pd128(_t2_103)));

    // AVX Storer:
    _t2_16 = _t2_104;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi661 + 5), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_11, _t2_11, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_107 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t2_108 = _t2_107;

    // 4-BLAC: 1x4 * 4x1
    _t2_109 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_mul_pd(_t2_106, _t2_108), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_mul_pd(_t2_106, _t2_108), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_mul_pd(_t2_106, _t2_108), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_110 = _mm256_sub_pd(_t2_105, _t2_109);

    // AVX Storer:
    _t2_17 = _t2_110;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 5), ( G(h(1, 28, fi661 + 5), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi661 + 5), L[28,28],h(1, 28, fi661 + 5)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_111 = _t2_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_112 = _t2_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_113 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t2_114 = _mm256_add_pd(_t2_112, _t2_113);

    // 4-BLAC: 1x4 / 1x4
    _t2_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_111), _mm256_castpd256_pd128(_t2_114)));

    // AVX Storer:
    _t2_17 = _t2_115;

    // Generating : X[28,28] = S(h(2, 28, fi661 + 6), ( G(h(2, 28, fi661 + 6), X[28,28],h(4, 28, 0)) - ( G(h(2, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 5)) * G(h(1, 28, fi661 + 5), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

    // AVX Loader:

    // 2x4 -> 4x4
    _t2_116 = _t2_12;
    _t2_117 = _t2_13;
    _t2_118 = _mm256_setzero_pd();
    _t2_119 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t2_120 = _t2_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t2_121 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 32), _mm256_permute2f128_pd(_t2_120, _t2_120, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));
    _t2_122 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 32), _mm256_permute2f128_pd(_t2_120, _t2_120, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));
    _t2_123 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 49), _mm256_permute2f128_pd(_t2_120, _t2_120, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));
    _t2_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 49), _mm256_permute2f128_pd(_t2_120, _t2_120, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));

    // 4-BLAC: 4x4 - 4x4
    _t2_125 = _mm256_sub_pd(_t2_116, _t2_121);
    _t2_126 = _mm256_sub_pd(_t2_117, _t2_122);
    _t2_127 = _mm256_sub_pd(_t2_118, _t2_123);
    _t2_128 = _mm256_sub_pd(_t2_119, _t2_124);

    // AVX Storer:
    _t2_12 = _t2_125;
    _t2_13 = _t2_126;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_129 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_130 = _t2_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_131 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t2_132 = _mm256_add_pd(_t2_130, _t2_131);

    // 4-BLAC: 1x4 / 1x4
    _t2_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_129), _mm256_castpd256_pd128(_t2_132)));

    // AVX Storer:
    _t2_18 = _t2_133;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_134 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_135 = _t2_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_136 = _t0_5;

    // 4-BLAC: (4x1)^T
    _t2_137 = _t2_136;

    // 4-BLAC: 1x4 Kro 1x4
    _t2_138 = _mm256_mul_pd(_t2_135, _t2_137);

    // 4-BLAC: 1x4 - 1x4
    _t2_139 = _mm256_sub_pd(_t2_134, _t2_138);

    // AVX Storer:
    _t2_19 = _t2_139;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_140 = _t2_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_141 = _t2_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_142 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t2_143 = _mm256_add_pd(_t2_141, _t2_142);

    // 4-BLAC: 1x4 / 1x4
    _t2_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_140), _mm256_castpd256_pd128(_t2_143)));

    // AVX Storer:
    _t2_19 = _t2_144;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi661 + 6), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_145 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_146 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_147 = _t0_2;

    // 4-BLAC: (1x4)^T
    _t2_148 = _t2_147;

    // 4-BLAC: 1x4 * 4x1
    _t2_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_mul_pd(_t2_146, _t2_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_mul_pd(_t2_146, _t2_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_mul_pd(_t2_146, _t2_148), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_150 = _mm256_sub_pd(_t2_145, _t2_149);

    // AVX Storer:
    _t2_20 = _t2_150;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_151 = _t2_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_152 = _t2_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_153 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t2_154 = _mm256_add_pd(_t2_152, _t2_153);

    // 4-BLAC: 1x4 / 1x4
    _t2_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_151), _mm256_castpd256_pd128(_t2_154)));

    // AVX Storer:
    _t2_20 = _t2_155;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi661 + 6), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_156 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_12, _t2_12, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_158 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t2_159 = _t2_158;

    // 4-BLAC: 1x4 * 4x1
    _t2_160 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_mul_pd(_t2_157, _t2_159), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_mul_pd(_t2_157, _t2_159), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_mul_pd(_t2_157, _t2_159), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_161 = _mm256_sub_pd(_t2_156, _t2_160);

    // AVX Storer:
    _t2_21 = _t2_161;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 6), ( G(h(1, 28, fi661 + 6), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi661 + 6), L[28,28],h(1, 28, fi661 + 6)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_162 = _t2_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_163 = _t2_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_164 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t2_165 = _mm256_add_pd(_t2_163, _t2_164);

    // 4-BLAC: 1x4 / 1x4
    _t2_166 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_162), _mm256_castpd256_pd128(_t2_165)));

    // AVX Storer:
    _t2_21 = _t2_166;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(4, 28, 0)) - ( G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 6)) Kro G(h(1, 28, fi661 + 6), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_167 = _t2_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t2_26 = _mm256_mul_pd(_t2_167, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32));

    // 4-BLAC: 1x4 - 1x4
    _t2_13 = _mm256_sub_pd(_t2_13, _t2_26);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_168 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_169 = _t2_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_170 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t2_171 = _mm256_add_pd(_t2_169, _t2_170);

    // 4-BLAC: 1x4 / 1x4
    _t2_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_168), _mm256_castpd256_pd128(_t2_171)));

    // AVX Storer:
    _t2_22 = _t2_172;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_173 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_174 = _t2_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_175 = _t0_5;

    // 4-BLAC: (4x1)^T
    _t2_176 = _t2_175;

    // 4-BLAC: 1x4 Kro 1x4
    _t2_177 = _mm256_mul_pd(_t2_174, _t2_176);

    // 4-BLAC: 1x4 - 1x4
    _t2_178 = _mm256_sub_pd(_t2_173, _t2_177);

    // AVX Storer:
    _t2_23 = _t2_178;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_179 = _t2_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_180 = _t2_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_181 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t2_182 = _mm256_add_pd(_t2_180, _t2_181);

    // 4-BLAC: 1x4 / 1x4
    _t2_183 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_179), _mm256_castpd256_pd128(_t2_182)));

    // AVX Storer:
    _t2_23 = _t2_183;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi661 + 7), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_184 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_185 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t2_186 = _t0_2;

    // 4-BLAC: (1x4)^T
    _t2_187 = _t2_186;

    // 4-BLAC: 1x4 * 4x1
    _t2_188 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_mul_pd(_t2_185, _t2_187), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_mul_pd(_t2_185, _t2_187), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_mul_pd(_t2_185, _t2_187), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_189 = _mm256_sub_pd(_t2_184, _t2_188);

    // AVX Storer:
    _t2_24 = _t2_189;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_190 = _t2_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_191 = _t2_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_192 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t2_193 = _mm256_add_pd(_t2_191, _t2_192);

    // 4-BLAC: 1x4 / 1x4
    _t2_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_190), _mm256_castpd256_pd128(_t2_193)));

    // AVX Storer:
    _t2_24 = _t2_194;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi661 + 7), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_195 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_13, _t2_13, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_196 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t2_197 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t2_198 = _t2_197;

    // 4-BLAC: 1x4 * 4x1
    _t2_199 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_mul_pd(_t2_196, _t2_198), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_mul_pd(_t2_196, _t2_198), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_mul_pd(_t2_196, _t2_198), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t2_200 = _mm256_sub_pd(_t2_195, _t2_199);

    // AVX Storer:
    _t2_25 = _t2_200;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 7), ( G(h(1, 28, fi661 + 7), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi661 + 7), L[28,28],h(1, 28, fi661 + 7)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_201 = _t2_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_202 = _t2_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t2_203 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t2_204 = _mm256_add_pd(_t2_202, _t2_203);

    // 4-BLAC: 1x4 / 1x4
    _t2_205 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_201), _mm256_castpd256_pd128(_t2_204)));

    // AVX Storer:
    _t2_25 = _t2_205;

    // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, fi661 + i100 + 8), ( G(h(4, 28, fi661 + i100 + 8), X[28,28],h(4, 28, 0)) - ( G(h(4, 28, fi661 + i100 + 8), L[28,28],h(4, 28, fi661 + 4)) * G(h(4, 28, fi661 + 4), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0)) )

    // AVX Loader:
    _mm_store_sd(&(C[28*fi661 + 112]), _mm256_castpd256_pd128(_t2_7));
    _mm_store_sd(&(C[28*fi661 + 113]), _mm256_castpd256_pd128(_t2_8));
    _mm_store_sd(&(C[28*fi661 + 114]), _mm256_castpd256_pd128(_t2_9));
    _mm_store_sd(&(C[28*fi661 + 115]), _mm256_castpd256_pd128(_t2_10));
    _mm_store_sd(&(C[28*fi661 + 140]), _mm256_castpd256_pd128(_t2_14));
    _mm_store_sd(&(C[28*fi661 + 141]), _mm256_castpd256_pd128(_t2_15));
    _mm_store_sd(&(C[28*fi661 + 142]), _mm256_castpd256_pd128(_t2_16));
    _mm_store_sd(&(C[28*fi661 + 143]), _mm256_castpd256_pd128(_t2_17));
    _mm_store_sd(&(C[28*fi661 + 168]), _mm256_castpd256_pd128(_t2_18));
    _mm_store_sd(&(C[28*fi661 + 169]), _mm256_castpd256_pd128(_t2_19));
    _mm_store_sd(&(C[28*fi661 + 170]), _mm256_castpd256_pd128(_t2_20));
    _mm_store_sd(&(C[28*fi661 + 171]), _mm256_castpd256_pd128(_t2_21));
    _mm_store_sd(&(C[28*fi661 + 196]), _mm256_castpd256_pd128(_t2_22));
    _mm_store_sd(&(C[28*fi661 + 197]), _mm256_castpd256_pd128(_t2_23));
    _mm_store_sd(&(C[28*fi661 + 198]), _mm256_castpd256_pd128(_t2_24));
    _mm_store_sd(&(C[28*fi661 + 199]), _mm256_castpd256_pd128(_t2_25));

    for( int i100 = 0; i100 <= -fi661 + 19; i100+=4 ) {
      _t3_36 = _asm256_loadu_pd(C + 28*fi661 + 28*i100 + 224);
      _t3_37 = _asm256_loadu_pd(C + 28*fi661 + 28*i100 + 252);
      _t3_38 = _asm256_loadu_pd(C + 28*fi661 + 28*i100 + 280);
      _t3_39 = _asm256_loadu_pd(C + 28*fi661 + 28*i100 + 308);
      _t3_31 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 228);
      _t3_30 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 229);
      _t3_29 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 230);
      _t3_28 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 231);
      _t3_27 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 256);
      _t3_26 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 257);
      _t3_25 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 258);
      _t3_24 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 259);
      _t3_23 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 284);
      _t3_22 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 285);
      _t3_21 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 286);
      _t3_20 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 287);
      _t3_19 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 312);
      _t3_18 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 313);
      _t3_17 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 314);
      _t3_16 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 315);
      _t3_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 112])));
      _t3_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 113])));
      _t3_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 114])));
      _t3_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 115])));
      _t3_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 140])));
      _t3_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 141])));
      _t3_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 142])));
      _t3_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 143])));
      _t3_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 168])));
      _t3_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 169])));
      _t3_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 170])));
      _t3_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 171])));
      _t3_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 196])));
      _t3_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 197])));
      _t3_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 198])));
      _t3_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi661 + 199])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t3_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_31, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_30, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_29, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_28, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));
      _t3_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_27, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_26, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_25, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_24, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));
      _t3_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_23, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_22, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_21, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_20, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));
      _t3_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_19, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_14), _mm256_unpacklo_pd(_t3_13, _t3_12), 32)), _mm256_mul_pd(_t3_18, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_10), _mm256_unpacklo_pd(_t3_9, _t3_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_17, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_6), _mm256_unpacklo_pd(_t3_5, _t3_4), 32)), _mm256_mul_pd(_t3_16, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32))));

      // 4-BLAC: 4x4 - 4x4
      _t3_36 = _mm256_sub_pd(_t3_36, _t3_32);
      _t3_37 = _mm256_sub_pd(_t3_37, _t3_33);
      _t3_38 = _mm256_sub_pd(_t3_38, _t3_34);
      _t3_39 = _mm256_sub_pd(_t3_39, _t3_35);

      // AVX Storer:
      _asm256_storeu_pd(C + 28*fi661 + 28*i100 + 224, _t3_36);
      _asm256_storeu_pd(C + 28*fi661 + 28*i100 + 252, _t3_37);
      _asm256_storeu_pd(C + 28*fi661 + 28*i100 + 280, _t3_38);
      _asm256_storeu_pd(C + 28*fi661 + 28*i100 + 308, _t3_39);
    }
  }

  _t4_64 = _mm256_castpd128_pd256(_mm_load_sd(&(C[672])));
  _t4_59 = _mm256_castpd128_pd256(_mm_load_sd(&(L[696])));
  _t4_65 = _mm256_castpd128_pd256(_mm_load_sd(&(C[673])));
  _t4_66 = _mm256_castpd128_pd256(_mm_load_sd(&(C[674])));
  _t4_67 = _mm256_castpd128_pd256(_mm_load_sd(&(C[675])));
  _t4_68 = _asm256_loadu_pd(C + 700);
  _t4_69 = _asm256_loadu_pd(C + 728);
  _t4_70 = _asm256_loadu_pd(C + 756);
  _t4_58 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 724)), _mm256_castpd128_pd256(_mm_load_sd(L + 752))), _mm256_castpd128_pd256(_mm_load_sd(L + 780)), 32);
  _t4_57 = _mm256_castpd128_pd256(_mm_load_sd(&(L[725])));
  _t4_56 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 753)), _mm256_castpd128_pd256(_mm_load_sd(L + 781)), 0);
  _t4_55 = _mm256_castpd128_pd256(_mm_load_sd(&(L[754])));
  _t4_54 = _mm256_broadcast_sd(&(L[782]));
  _t4_53 = _mm256_castpd128_pd256(_mm_load_sd(&(L[783])));
  _t4_83 = _mm256_castpd128_pd256(_mm_load_sd(C + 116));
  _t4_84 = _mm256_maskload_pd(C + 144, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_85 = _mm256_maskload_pd(C + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_86 = _asm256_loadu_pd(C + 200);
  _t4_52 = _mm256_broadcast_sd(L + 112);
  _t4_51 = _mm256_broadcast_sd(L + 113);
  _t4_50 = _mm256_broadcast_sd(L + 114);
  _t4_49 = _mm256_broadcast_sd(L + 115);
  _t4_48 = _mm256_broadcast_sd(L + 140);
  _t4_47 = _mm256_broadcast_sd(L + 141);
  _t4_46 = _mm256_broadcast_sd(L + 142);
  _t4_45 = _mm256_broadcast_sd(L + 143);
  _t4_44 = _mm256_broadcast_sd(L + 168);
  _t4_43 = _mm256_broadcast_sd(L + 169);
  _t4_42 = _mm256_broadcast_sd(L + 170);
  _t4_41 = _mm256_broadcast_sd(L + 171);
  _t4_40 = _mm256_broadcast_sd(L + 196);
  _t4_39 = _mm256_broadcast_sd(L + 197);
  _t4_38 = _mm256_broadcast_sd(L + 198);
  _t4_37 = _mm256_broadcast_sd(L + 199);
  _t4_36 = _asm256_loadu_pd(C + 112);
  _t4_35 = _asm256_loadu_pd(C + 140);
  _t4_34 = _asm256_loadu_pd(C + 168);
  _t4_33 = _asm256_loadu_pd(C + 196);
  _t4_32 = _mm256_broadcast_sd(C + 112);
  _t4_31 = _mm256_broadcast_sd(C + 113);
  _t4_30 = _mm256_broadcast_sd(C + 114);
  _t4_29 = _mm256_broadcast_sd(C + 115);
  _t4_28 = _mm256_broadcast_sd(C + 140);
  _t4_27 = _mm256_broadcast_sd(C + 141);
  _t4_26 = _mm256_broadcast_sd(C + 142);
  _t4_25 = _mm256_broadcast_sd(C + 143);
  _t4_24 = _mm256_broadcast_sd(C + 168);
  _t4_23 = _mm256_broadcast_sd(C + 169);
  _t4_22 = _mm256_broadcast_sd(C + 170);
  _t4_21 = _mm256_broadcast_sd(C + 171);
  _t4_20 = _mm256_broadcast_sd(C + 196);
  _t4_19 = _mm256_broadcast_sd(C + 197);
  _t4_18 = _mm256_broadcast_sd(C + 198);
  _t4_17 = _mm256_broadcast_sd(C + 199);
  _t4_16 = _asm256_loadu_pd(L + 112);
  _t4_15 = _asm256_loadu_pd(L + 140);
  _t4_14 = _asm256_loadu_pd(L + 168);
  _t4_13 = _asm256_loadu_pd(L + 196);
  _t4_12 = _mm256_castpd128_pd256(_mm_load_sd(&(L[116])));
  _t4_11 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 200)), 32);
  _t4_10 = _mm256_castpd128_pd256(_mm_load_sd(&(L[145])));
  _t4_9 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 201)), 0);
  _t4_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[174])));
  _t4_7 = _mm256_castpd128_pd256(_mm_load_sd(&(L[202])));
  _t4_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[203])));
  _t4_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[144])));
  _t4_4 = _mm256_broadcast_sd(&(L[144]));
  _t4_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 172)), _mm256_castpd128_pd256(_mm_load_sd(L + 200)), 0);
  _t4_2 = _mm256_maskload_pd(L + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_1 = _mm256_maskload_pd(L + 200, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_0 = _mm256_maskload_pd(L + 200, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_315 = _t4_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_316 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_317 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_318 = _mm256_add_pd(_t4_316, _t4_317);

  // 4-BLAC: 1x4 / 1x4
  _t4_319 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_315), _mm256_castpd256_pd128(_t4_318)));

  // AVX Storer:
  _t4_64 = _t4_319;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_320 = _t4_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_321 = _t4_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_322 = _t0_5;

  // 4-BLAC: (4x1)^T
  _t4_323 = _t4_322;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_324 = _mm256_mul_pd(_t4_321, _t4_323);

  // 4-BLAC: 1x4 - 1x4
  _t4_325 = _mm256_sub_pd(_t4_320, _t4_324);

  // AVX Storer:
  _t4_65 = _t4_325;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_326 = _t4_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_327 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_328 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t4_329 = _mm256_add_pd(_t4_327, _t4_328);

  // 4-BLAC: 1x4 / 1x4
  _t4_330 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_326), _mm256_castpd256_pd128(_t4_329)));

  // AVX Storer:
  _t4_65 = _t4_330;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_331 = _t4_66;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_332 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_333 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t4_334 = _t4_333;

  // 4-BLAC: 1x4 * 4x1
  _t4_335 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_332, _t4_334), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_332, _t4_334), _mm256_mul_pd(_t4_332, _t4_334), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_332, _t4_334), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_332, _t4_334), _mm256_mul_pd(_t4_332, _t4_334), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_332, _t4_334), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_332, _t4_334), _mm256_mul_pd(_t4_332, _t4_334), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_336 = _mm256_sub_pd(_t4_331, _t4_335);

  // AVX Storer:
  _t4_66 = _t4_336;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_337 = _t4_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_338 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_339 = _t0_8;

  // 4-BLAC: 1x4 + 1x4
  _t4_340 = _mm256_add_pd(_t4_338, _t4_339);

  // 4-BLAC: 1x4 / 1x4
  _t4_341 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_337), _mm256_castpd256_pd128(_t4_340)));

  // AVX Storer:
  _t4_66 = _t4_341;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_342 = _t4_67;

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_343 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_344 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t4_345 = _t4_344;

  // 4-BLAC: 1x4 * 4x1
  _t4_346 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_343, _t4_345), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_343, _t4_345), _mm256_mul_pd(_t4_343, _t4_345), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_343, _t4_345), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_343, _t4_345), _mm256_mul_pd(_t4_343, _t4_345), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_343, _t4_345), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_343, _t4_345), _mm256_mul_pd(_t4_343, _t4_345), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_347 = _mm256_sub_pd(_t4_342, _t4_346);

  // AVX Storer:
  _t4_67 = _t4_347;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_348 = _t4_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_349 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_350 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_351 = _mm256_add_pd(_t4_349, _t4_350);

  // 4-BLAC: 1x4 / 1x4
  _t4_352 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_348), _mm256_castpd256_pd128(_t4_351)));

  // AVX Storer:
  _t4_67 = _t4_352;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, 0)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t4_353 = _t4_68;
  _t4_354 = _t4_69;
  _t4_355 = _t4_70;
  _t4_356 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_357 = _t4_58;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_358 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_357, _t4_357, 32), _mm256_permute2f128_pd(_t4_357, _t4_357, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32));
  _t4_359 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_357, _t4_357, 32), _mm256_permute2f128_pd(_t4_357, _t4_357, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32));
  _t4_360 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_357, _t4_357, 49), _mm256_permute2f128_pd(_t4_357, _t4_357, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32));
  _t4_361 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_357, _t4_357, 49), _mm256_permute2f128_pd(_t4_357, _t4_357, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_362 = _mm256_sub_pd(_t4_353, _t4_358);
  _t4_363 = _mm256_sub_pd(_t4_354, _t4_359);
  _t4_364 = _mm256_sub_pd(_t4_355, _t4_360);
  _t4_365 = _mm256_sub_pd(_t4_356, _t4_361);

  // AVX Storer:
  _t4_68 = _t4_362;
  _t4_69 = _t4_363;
  _t4_70 = _t4_364;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_366 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_68, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_367 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_368 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_369 = _mm256_add_pd(_t4_367, _t4_368);

  // 4-BLAC: 1x4 / 1x4
  _t4_370 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_366), _mm256_castpd256_pd128(_t4_369)));

  // AVX Storer:
  _t4_71 = _t4_370;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_371 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_68, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_372 = _t4_71;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_373 = _t0_5;

  // 4-BLAC: (4x1)^T
  _t4_374 = _t4_373;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_375 = _mm256_mul_pd(_t4_372, _t4_374);

  // 4-BLAC: 1x4 - 1x4
  _t4_376 = _mm256_sub_pd(_t4_371, _t4_375);

  // AVX Storer:
  _t4_72 = _t4_376;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_377 = _t4_72;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_378 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_379 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t4_380 = _mm256_add_pd(_t4_378, _t4_379);

  // 4-BLAC: 1x4 / 1x4
  _t4_381 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_377), _mm256_castpd256_pd128(_t4_380)));

  // AVX Storer:
  _t4_72 = _t4_381;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_382 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_68, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_68, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_383 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_384 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t4_385 = _t4_384;

  // 4-BLAC: 1x4 * 4x1
  _t4_386 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_383, _t4_385), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_383, _t4_385), _mm256_mul_pd(_t4_383, _t4_385), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_383, _t4_385), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_383, _t4_385), _mm256_mul_pd(_t4_383, _t4_385), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_383, _t4_385), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_383, _t4_385), _mm256_mul_pd(_t4_383, _t4_385), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_387 = _mm256_sub_pd(_t4_382, _t4_386);

  // AVX Storer:
  _t4_73 = _t4_387;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_388 = _t4_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_389 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_390 = _t0_8;

  // 4-BLAC: 1x4 + 1x4
  _t4_391 = _mm256_add_pd(_t4_389, _t4_390);

  // 4-BLAC: 1x4 / 1x4
  _t4_392 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_388), _mm256_castpd256_pd128(_t4_391)));

  // AVX Storer:
  _t4_73 = _t4_392;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_393 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_68, _t4_68, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_394 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_395 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t4_396 = _t4_395;

  // 4-BLAC: 1x4 * 4x1
  _t4_397 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_394, _t4_396), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_394, _t4_396), _mm256_mul_pd(_t4_394, _t4_396), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_394, _t4_396), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_394, _t4_396), _mm256_mul_pd(_t4_394, _t4_396), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_394, _t4_396), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_394, _t4_396), _mm256_mul_pd(_t4_394, _t4_396), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_398 = _mm256_sub_pd(_t4_393, _t4_397);

  // AVX Storer:
  _t4_74 = _t4_398;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_399 = _t4_74;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_400 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_401 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_402 = _mm256_add_pd(_t4_400, _t4_401);

  // 4-BLAC: 1x4 / 1x4
  _t4_403 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_399), _mm256_castpd256_pd128(_t4_402)));

  // AVX Storer:
  _t4_74 = _t4_403;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, 0)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t4_404 = _t4_69;
  _t4_405 = _t4_70;
  _t4_406 = _mm256_setzero_pd();
  _t4_407 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_408 = _t4_56;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t4_409 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_408, _t4_408, 32), _mm256_permute2f128_pd(_t4_408, _t4_408, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32));
  _t4_410 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_408, _t4_408, 32), _mm256_permute2f128_pd(_t4_408, _t4_408, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32));
  _t4_411 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_408, _t4_408, 49), _mm256_permute2f128_pd(_t4_408, _t4_408, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32));
  _t4_412 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_408, _t4_408, 49), _mm256_permute2f128_pd(_t4_408, _t4_408, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32));

  // 4-BLAC: 4x4 - 4x4
  _t4_413 = _mm256_sub_pd(_t4_404, _t4_409);
  _t4_414 = _mm256_sub_pd(_t4_405, _t4_410);
  _t4_415 = _mm256_sub_pd(_t4_406, _t4_411);
  _t4_416 = _mm256_sub_pd(_t4_407, _t4_412);

  // AVX Storer:
  _t4_69 = _t4_413;
  _t4_70 = _t4_414;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_417 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_69, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_418 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_419 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_420 = _mm256_add_pd(_t4_418, _t4_419);

  // 4-BLAC: 1x4 / 1x4
  _t4_421 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_417), _mm256_castpd256_pd128(_t4_420)));

  // AVX Storer:
  _t4_75 = _t4_421;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_422 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_69, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_423 = _t4_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_424 = _t0_5;

  // 4-BLAC: (4x1)^T
  _t4_425 = _t4_424;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_426 = _mm256_mul_pd(_t4_423, _t4_425);

  // 4-BLAC: 1x4 - 1x4
  _t4_427 = _mm256_sub_pd(_t4_422, _t4_426);

  // AVX Storer:
  _t4_76 = _t4_427;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_428 = _t4_76;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_429 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_430 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t4_431 = _mm256_add_pd(_t4_429, _t4_430);

  // 4-BLAC: 1x4 / 1x4
  _t4_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_428), _mm256_castpd256_pd128(_t4_431)));

  // AVX Storer:
  _t4_76 = _t4_112;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_113 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_69, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_69, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_114 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_115 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t4_116 = _t4_115;

  // 4-BLAC: 1x4 * 4x1
  _t4_117 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_114, _t4_116), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_114, _t4_116), _mm256_mul_pd(_t4_114, _t4_116), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_114, _t4_116), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_114, _t4_116), _mm256_mul_pd(_t4_114, _t4_116), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_114, _t4_116), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_114, _t4_116), _mm256_mul_pd(_t4_114, _t4_116), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_118 = _mm256_sub_pd(_t4_113, _t4_117);

  // AVX Storer:
  _t4_77 = _t4_118;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_119 = _t4_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_120 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_121 = _t0_8;

  // 4-BLAC: 1x4 + 1x4
  _t4_122 = _mm256_add_pd(_t4_120, _t4_121);

  // 4-BLAC: 1x4 / 1x4
  _t4_123 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_119), _mm256_castpd256_pd128(_t4_122)));

  // AVX Storer:
  _t4_77 = _t4_123;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_124 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_69, _t4_69, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_125 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_126 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t4_127 = _t4_126;

  // 4-BLAC: 1x4 * 4x1
  _t4_128 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_125, _t4_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_125, _t4_127), _mm256_mul_pd(_t4_125, _t4_127), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_125, _t4_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_125, _t4_127), _mm256_mul_pd(_t4_125, _t4_127), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_125, _t4_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_125, _t4_127), _mm256_mul_pd(_t4_125, _t4_127), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_129 = _mm256_sub_pd(_t4_124, _t4_128);

  // AVX Storer:
  _t4_78 = _t4_129;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_130 = _t4_78;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_131 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_132 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_133 = _mm256_add_pd(_t4_131, _t4_132);

  // 4-BLAC: 1x4 / 1x4
  _t4_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_130), _mm256_castpd256_pd128(_t4_133)));

  // AVX Storer:
  _t4_78 = _t4_134;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, 0)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_135 = _t4_54;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t4_99 = _mm256_mul_pd(_t4_135, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _t4_78), 32));

  // 4-BLAC: 1x4 - 1x4
  _t4_70 = _mm256_sub_pd(_t4_70, _t4_99);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_136 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_70, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_137 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_138 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_139 = _mm256_add_pd(_t4_137, _t4_138);

  // 4-BLAC: 1x4 / 1x4
  _t4_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_136), _mm256_castpd256_pd128(_t4_139)));

  // AVX Storer:
  _t4_79 = _t4_140;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_141 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_70, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_142 = _t4_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_143 = _t0_5;

  // 4-BLAC: (4x1)^T
  _t4_144 = _t4_143;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_145 = _mm256_mul_pd(_t4_142, _t4_144);

  // 4-BLAC: 1x4 - 1x4
  _t4_146 = _mm256_sub_pd(_t4_141, _t4_145);

  // AVX Storer:
  _t4_80 = _t4_146;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_147 = _t4_80;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_148 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_149 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t4_150 = _mm256_add_pd(_t4_148, _t4_149);

  // 4-BLAC: 1x4 / 1x4
  _t4_151 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_147), _mm256_castpd256_pd128(_t4_150)));

  // AVX Storer:
  _t4_80 = _t4_151;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_152 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_70, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_70, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_153 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_79, _t4_80), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_154 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t4_155 = _t4_154;

  // 4-BLAC: 1x4 * 4x1
  _t4_156 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_153, _t4_155), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_153, _t4_155), _mm256_mul_pd(_t4_153, _t4_155), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_153, _t4_155), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_153, _t4_155), _mm256_mul_pd(_t4_153, _t4_155), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_153, _t4_155), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_153, _t4_155), _mm256_mul_pd(_t4_153, _t4_155), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_157 = _mm256_sub_pd(_t4_152, _t4_156);

  // AVX Storer:
  _t4_81 = _t4_157;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_158 = _t4_81;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_159 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_160 = _t0_8;

  // 4-BLAC: 1x4 + 1x4
  _t4_161 = _mm256_add_pd(_t4_159, _t4_160);

  // 4-BLAC: 1x4 / 1x4
  _t4_162 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_158), _mm256_castpd256_pd128(_t4_161)));

  // AVX Storer:
  _t4_81 = _t4_162;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_163 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_70, _t4_70, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_164 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_79, _t4_80), _mm256_unpacklo_pd(_t4_81, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_165 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t4_166 = _t4_165;

  // 4-BLAC: 1x4 * 4x1
  _t4_167 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_mul_pd(_t4_164, _t4_166), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_mul_pd(_t4_164, _t4_166), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_164, _t4_166), _mm256_mul_pd(_t4_164, _t4_166), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_168 = _mm256_sub_pd(_t4_163, _t4_167);

  // AVX Storer:
  _t4_82 = _t4_168;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_169 = _t4_82;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_170 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_171 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_172 = _mm256_add_pd(_t4_170, _t4_171);

  // 4-BLAC: 1x4 / 1x4
  _t4_173 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_169), _mm256_castpd256_pd128(_t4_172)));

  // AVX Storer:
  _t4_82 = _t4_173;

  // Generating : X[28,28] = S(h(4, 28, 4), ( G(h(4, 28, 4), C[28,28],h(4, 28, 4)) - ( ( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, 4), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t4_174 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_83, _t4_84, 0), _mm256_shuffle_pd(_t4_85, _t4_86, 0), 32);
  _t4_175 = _mm256_permute2f128_pd(_t4_84, _mm256_shuffle_pd(_t4_85, _t4_86, 3), 32);
  _t4_176 = _mm256_blend_pd(_t4_85, _mm256_shuffle_pd(_t4_85, _t4_86, 3), 12);
  _t4_177 = _t4_86;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_432 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_36, _t4_35), _mm256_unpacklo_pd(_t4_34, _t4_33), 32);
  _t4_433 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_36, _t4_35), _mm256_unpackhi_pd(_t4_34, _t4_33), 32);
  _t4_434 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_36, _t4_35), _mm256_unpacklo_pd(_t4_34, _t4_33), 49);
  _t4_435 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_36, _t4_35), _mm256_unpackhi_pd(_t4_34, _t4_33), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_52, _t4_432), _mm256_mul_pd(_t4_51, _t4_433)), _mm256_add_pd(_mm256_mul_pd(_t4_50, _t4_434), _mm256_mul_pd(_t4_49, _t4_435)));
  _t4_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_48, _t4_432), _mm256_mul_pd(_t4_47, _t4_433)), _mm256_add_pd(_mm256_mul_pd(_t4_46, _t4_434), _mm256_mul_pd(_t4_45, _t4_435)));
  _t4_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_44, _t4_432), _mm256_mul_pd(_t4_43, _t4_433)), _mm256_add_pd(_mm256_mul_pd(_t4_42, _t4_434), _mm256_mul_pd(_t4_41, _t4_435)));
  _t4_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_40, _t4_432), _mm256_mul_pd(_t4_39, _t4_433)), _mm256_add_pd(_mm256_mul_pd(_t4_38, _t4_434), _mm256_mul_pd(_t4_37, _t4_435)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_436 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_16, _t4_15), _mm256_unpacklo_pd(_t4_14, _t4_13), 32);
  _t4_437 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_16, _t4_15), _mm256_unpackhi_pd(_t4_14, _t4_13), 32);
  _t4_438 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_16, _t4_15), _mm256_unpacklo_pd(_t4_14, _t4_13), 49);
  _t4_439 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_16, _t4_15), _mm256_unpackhi_pd(_t4_14, _t4_13), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_32, _t4_436), _mm256_mul_pd(_t4_31, _t4_437)), _mm256_add_pd(_mm256_mul_pd(_t4_30, _t4_438), _mm256_mul_pd(_t4_29, _t4_439)));
  _t4_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_28, _t4_436), _mm256_mul_pd(_t4_27, _t4_437)), _mm256_add_pd(_mm256_mul_pd(_t4_26, _t4_438), _mm256_mul_pd(_t4_25, _t4_439)));
  _t4_106 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_24, _t4_436), _mm256_mul_pd(_t4_23, _t4_437)), _mm256_add_pd(_mm256_mul_pd(_t4_22, _t4_438), _mm256_mul_pd(_t4_21, _t4_439)));
  _t4_107 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_20, _t4_436), _mm256_mul_pd(_t4_19, _t4_437)), _mm256_add_pd(_mm256_mul_pd(_t4_18, _t4_438), _mm256_mul_pd(_t4_17, _t4_439)));

  // 4-BLAC: 4x4 + 4x4
  _t4_60 = _mm256_add_pd(_t4_100, _t4_104);
  _t4_61 = _mm256_add_pd(_t4_101, _t4_105);
  _t4_62 = _mm256_add_pd(_t4_102, _t4_106);
  _t4_63 = _mm256_add_pd(_t4_103, _t4_107);

  // 4-BLAC: 4x4 - 4x4
  _t4_108 = _mm256_sub_pd(_t4_174, _t4_60);
  _t4_109 = _mm256_sub_pd(_t4_175, _t4_61);
  _t4_110 = _mm256_sub_pd(_t4_176, _t4_62);
  _t4_111 = _mm256_sub_pd(_t4_177, _t4_63);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t4_83 = _t4_108;
  _t4_84 = _t4_109;
  _t4_85 = _t4_110;
  _t4_86 = _t4_111;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 4)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_178 = _t4_83;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_179 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_180 = _t4_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_181 = _mm256_mul_pd(_t4_179, _t4_180);

  // 4-BLAC: 1x4 / 1x4
  _t4_182 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_178), _mm256_castpd256_pd128(_t4_181)));

  // AVX Storer:
  _t4_83 = _t4_182;

  // Generating : X[28,28] = S(h(3, 28, 5), ( G(h(3, 28, 5), X[28,28],h(1, 28, 4)) - ( G(h(3, 28, 5), L[28,28],h(1, 28, 4)) Kro G(h(1, 28, 4), X[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_183 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_84, _t4_85), _mm256_unpacklo_pd(_t4_86, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_184 = _t4_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_185 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_83, _t4_83, 32), _mm256_permute2f128_pd(_t4_83, _t4_83, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t4_186 = _mm256_mul_pd(_t4_184, _t4_185);

  // 4-BLAC: 4x1 - 4x1
  _t4_187 = _mm256_sub_pd(_t4_183, _t4_186);

  // AVX Storer:
  _t4_87 = _t4_187;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 5), L[28,28],h(1, 28, 5)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_188 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_87, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_189 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_190 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_191 = _mm256_add_pd(_t4_189, _t4_190);

  // 4-BLAC: 1x4 / 1x4
  _t4_192 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_188), _mm256_castpd256_pd128(_t4_191)));

  // AVX Storer:
  _t4_88 = _t4_192;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 4)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 5)) Kro G(h(1, 28, 5), X[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_193 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_87, 2), _mm256_permute2f128_pd(_t4_87, _t4_87, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_194 = _t4_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_195 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_88, _t4_88, 32), _mm256_permute2f128_pd(_t4_88, _t4_88, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t4_196 = _mm256_mul_pd(_t4_194, _t4_195);

  // 4-BLAC: 4x1 - 4x1
  _t4_197 = _mm256_sub_pd(_t4_193, _t4_196);

  // AVX Storer:
  _t4_89 = _t4_197;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_198 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_89, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_199 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_200 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_201 = _mm256_add_pd(_t4_199, _t4_200);

  // 4-BLAC: 1x4 / 1x4
  _t4_202 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_198), _mm256_castpd256_pd128(_t4_201)));

  // AVX Storer:
  _t4_90 = _t4_202;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 4)) - ( G(h(1, 28, 7), L[28,28],h(1, 28, 6)) Kro G(h(1, 28, 6), X[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_203 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_89, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_204 = _t4_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_205 = _t4_90;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_206 = _mm256_mul_pd(_t4_204, _t4_205);

  // 4-BLAC: 1x4 - 1x4
  _t4_207 = _mm256_sub_pd(_t4_203, _t4_206);

  // AVX Storer:
  _t4_91 = _t4_207;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_208 = _t4_91;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_209 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_210 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t4_211 = _mm256_add_pd(_t4_209, _t4_210);

  // 4-BLAC: 1x4 / 1x4
  _t4_212 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_208), _mm256_castpd256_pd128(_t4_211)));

  // AVX Storer:
  _t4_91 = _t4_212;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 5)) - ( ( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) ) ) + ( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_213 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_84, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_214 = _t4_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_215 = _t4_88;

  // 4-BLAC: (4x1)^T
  _t4_216 = _t4_215;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_217 = _mm256_mul_pd(_t4_214, _t4_216);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_218 = _t4_88;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_219 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t4_220 = _t4_219;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_221 = _mm256_mul_pd(_t4_218, _t4_220);

  // 4-BLAC: 1x4 + 1x4
  _t4_222 = _mm256_add_pd(_t4_217, _t4_221);

  // 4-BLAC: 1x4 - 1x4
  _t4_223 = _mm256_sub_pd(_t4_213, _t4_222);

  // AVX Storer:
  _t4_92 = _t4_223;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 5)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_224 = _t4_92;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_225 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_226 = _t4_10;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_227 = _mm256_mul_pd(_t4_225, _t4_226);

  // 4-BLAC: 1x4 / 1x4
  _t4_228 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_224), _mm256_castpd256_pd128(_t4_227)));

  // AVX Storer:
  _t4_92 = _t4_228;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 5)) - ( G(h(2, 28, 6), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_229 = _mm256_unpackhi_pd(_mm256_blend_pd(_t4_85, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t4_86, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_230 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_90, _t4_91), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_231 = _t4_4;

  // 4-BLAC: (4x1)^T
  _t4_232 = _t4_231;

  // 4-BLAC: 4x1 Kro 1x4
  _t4_233 = _mm256_mul_pd(_t4_230, _t4_232);

  // 4-BLAC: 4x1 - 4x1
  _t4_234 = _mm256_sub_pd(_t4_229, _t4_233);

  // AVX Storer:
  _t4_93 = _t4_234;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 5)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_235 = _t4_93;

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_236 = _t4_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_237 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_88, _t4_88, 32), _mm256_permute2f128_pd(_t4_88, _t4_88, 32), 0);

  // 4-BLAC: (4x1)^T
  _t4_238 = _t4_237;

  // 4-BLAC: 4x1 Kro 1x4
  _t4_239 = _mm256_mul_pd(_t4_236, _t4_238);

  // 4-BLAC: 4x1 - 4x1
  _t4_240 = _mm256_sub_pd(_t4_235, _t4_239);

  // AVX Storer:
  _t4_93 = _t4_240;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 5)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 5)) Kro G(h(1, 28, 5), X[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_241 = _t4_93;

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_242 = _t4_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_243 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_92, _t4_92, 32), _mm256_permute2f128_pd(_t4_92, _t4_92, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t4_244 = _mm256_mul_pd(_t4_242, _t4_243);

  // 4-BLAC: 4x1 - 4x1
  _t4_245 = _mm256_sub_pd(_t4_241, _t4_244);

  // AVX Storer:
  _t4_93 = _t4_245;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_246 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_93, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_247 = _t4_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_248 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t4_249 = _mm256_add_pd(_t4_247, _t4_248);

  // 4-BLAC: 1x4 / 1x4
  _t4_250 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_246), _mm256_castpd256_pd128(_t4_249)));

  // AVX Storer:
  _t4_94 = _t4_250;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 7), L[28,28],h(1, 28, 6)) Kro G(h(1, 28, 6), X[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_251 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_93, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_252 = _t4_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_253 = _t4_94;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_254 = _mm256_mul_pd(_t4_252, _t4_253);

  // 4-BLAC: 1x4 - 1x4
  _t4_255 = _mm256_sub_pd(_t4_251, _t4_254);

  // AVX Storer:
  _t4_95 = _t4_255;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_256 = _t4_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_257 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_258 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t4_259 = _mm256_add_pd(_t4_257, _t4_258);

  // 4-BLAC: 1x4 / 1x4
  _t4_260 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_256), _mm256_castpd256_pd128(_t4_259)));

  // AVX Storer:
  _t4_95 = _t4_260;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 6)) - ( ( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), X[28,28],h(2, 28, 4)) ) ) + ( G(h(1, 28, 6), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_261 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_85, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_85, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_262 = _t4_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_263 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t4_264 = _t4_263;

  // 4-BLAC: 1x4 * 4x1
  _t4_265 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_262, _t4_264), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_262, _t4_264), _mm256_mul_pd(_t4_262, _t4_264), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_262, _t4_264), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_262, _t4_264), _mm256_mul_pd(_t4_262, _t4_264), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_262, _t4_264), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_262, _t4_264), _mm256_mul_pd(_t4_262, _t4_264), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_266 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_267 = _t4_2;

  // 4-BLAC: (1x4)^T
  _t4_268 = _t4_267;

  // 4-BLAC: 1x4 * 4x1
  _t4_269 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_266, _t4_268), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_266, _t4_268), _mm256_mul_pd(_t4_266, _t4_268), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_266, _t4_268), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_266, _t4_268), _mm256_mul_pd(_t4_266, _t4_268), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_266, _t4_268), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_266, _t4_268), _mm256_mul_pd(_t4_266, _t4_268), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t4_270 = _mm256_add_pd(_t4_265, _t4_269);

  // 4-BLAC: 1x4 - 1x4
  _t4_271 = _mm256_sub_pd(_t4_261, _t4_270);

  // AVX Storer:
  _t4_96 = _t4_271;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 6)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_272 = _t4_96;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_273 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_274 = _t4_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_275 = _mm256_mul_pd(_t4_273, _t4_274);

  // 4-BLAC: 1x4 / 1x4
  _t4_276 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_272), _mm256_castpd256_pd128(_t4_275)));

  // AVX Storer:
  _t4_96 = _t4_276;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 7), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_277 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_86, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_86, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_278 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_279 = _t4_2;

  // 4-BLAC: (1x4)^T
  _t4_280 = _t4_279;

  // 4-BLAC: 1x4 * 4x1
  _t4_281 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_278, _t4_280), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_278, _t4_280), _mm256_mul_pd(_t4_278, _t4_280), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_278, _t4_280), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_278, _t4_280), _mm256_mul_pd(_t4_278, _t4_280), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_278, _t4_280), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_278, _t4_280), _mm256_mul_pd(_t4_278, _t4_280), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_282 = _mm256_sub_pd(_t4_277, _t4_281);

  // AVX Storer:
  _t4_97 = _t4_282;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 7), L[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), X[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_283 = _t4_97;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_284 = _t4_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_285 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t4_286 = _t4_285;

  // 4-BLAC: 1x4 * 4x1
  _t4_287 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_284, _t4_286), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_284, _t4_286), _mm256_mul_pd(_t4_284, _t4_286), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_284, _t4_286), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_284, _t4_286), _mm256_mul_pd(_t4_284, _t4_286), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_284, _t4_286), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_284, _t4_286), _mm256_mul_pd(_t4_284, _t4_286), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_288 = _mm256_sub_pd(_t4_283, _t4_287);

  // AVX Storer:
  _t4_97 = _t4_288;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 7), L[28,28],h(1, 28, 6)) Kro G(h(1, 28, 6), X[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_289 = _t4_97;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_290 = _t4_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_291 = _t4_96;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_292 = _mm256_mul_pd(_t4_290, _t4_291);

  // 4-BLAC: 1x4 - 1x4
  _t4_293 = _mm256_sub_pd(_t4_289, _t4_292);

  // AVX Storer:
  _t4_97 = _t4_293;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_294 = _t4_97;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_295 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_296 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t4_297 = _mm256_add_pd(_t4_295, _t4_296);

  // 4-BLAC: 1x4 / 1x4
  _t4_298 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_294), _mm256_castpd256_pd128(_t4_297)));

  // AVX Storer:
  _t4_97 = _t4_298;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 7)) - ( ( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), X[28,28],h(3, 28, 4)) ) ) + ( G(h(1, 28, 7), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_299 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_86, _t4_86, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_300 = _t4_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_301 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_unpacklo_pd(_t4_97, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t4_302 = _t4_301;

  // 4-BLAC: 1x4 * 4x1
  _t4_303 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_300, _t4_302), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_300, _t4_302), _mm256_mul_pd(_t4_300, _t4_302), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_300, _t4_302), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_300, _t4_302), _mm256_mul_pd(_t4_300, _t4_302), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_300, _t4_302), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_300, _t4_302), _mm256_mul_pd(_t4_300, _t4_302), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_304 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_unpacklo_pd(_t4_97, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_305 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t4_306 = _t4_305;

  // 4-BLAC: 1x4 * 4x1
  _t4_307 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_304, _t4_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_304, _t4_306), _mm256_mul_pd(_t4_304, _t4_306), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_304, _t4_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_304, _t4_306), _mm256_mul_pd(_t4_304, _t4_306), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_304, _t4_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_304, _t4_306), _mm256_mul_pd(_t4_304, _t4_306), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t4_308 = _mm256_add_pd(_t4_303, _t4_307);

  // 4-BLAC: 1x4 - 1x4
  _t4_309 = _mm256_sub_pd(_t4_299, _t4_308);

  // AVX Storer:
  _t4_98 = _t4_309;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 7)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_310 = _t4_98;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_311 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_312 = _t4_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_313 = _mm256_mul_pd(_t4_311, _t4_312);

  // 4-BLAC: 1x4 / 1x4
  _t4_314 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_310), _mm256_castpd256_pd128(_t4_313)));

  // AVX Storer:
  _t4_98 = _t4_314;

  // Generating : X[28,28] = Sum_{i217} ( S(h(4, 28, i217 + 8), ( G(h(4, 28, i217 + 8), C[28,28],h(4, 28, 4)) - ( G(h(4, 28, i217 + 8), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 4)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_440 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_16, _t4_15), _mm256_unpacklo_pd(_t4_14, _t4_13), 32);
  _t4_441 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_16, _t4_15), _mm256_unpackhi_pd(_t4_14, _t4_13), 32);
  _t4_442 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_16, _t4_15), _mm256_unpacklo_pd(_t4_14, _t4_13), 49);
  _t4_443 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_16, _t4_15), _mm256_unpackhi_pd(_t4_14, _t4_13), 49);

  _mm_store_sd(&(C[672]), _mm256_castpd256_pd128(_t4_64));
  _mm_store_sd(&(C[673]), _mm256_castpd256_pd128(_t4_65));
  _mm_store_sd(&(C[674]), _mm256_castpd256_pd128(_t4_66));
  _mm_store_sd(&(C[675]), _mm256_castpd256_pd128(_t4_67));
  _mm_store_sd(&(C[700]), _mm256_castpd256_pd128(_t4_71));
  _mm_store_sd(&(C[701]), _mm256_castpd256_pd128(_t4_72));
  _mm_store_sd(&(C[702]), _mm256_castpd256_pd128(_t4_73));
  _mm_store_sd(&(C[703]), _mm256_castpd256_pd128(_t4_74));
  _mm_store_sd(&(C[728]), _mm256_castpd256_pd128(_t4_75));
  _mm_store_sd(&(C[729]), _mm256_castpd256_pd128(_t4_76));
  _mm_store_sd(&(C[730]), _mm256_castpd256_pd128(_t4_77));
  _mm_store_sd(&(C[731]), _mm256_castpd256_pd128(_t4_78));
  _mm_store_sd(&(C[756]), _mm256_castpd256_pd128(_t4_79));
  _mm_store_sd(&(C[757]), _mm256_castpd256_pd128(_t4_80));
  _mm_store_sd(&(C[758]), _mm256_castpd256_pd128(_t4_81));
  _mm_store_sd(&(C[759]), _mm256_castpd256_pd128(_t4_82));

  for( int i217 = 0; i217 <= 19; i217+=4 ) {
    _t5_20 = _asm256_loadu_pd(C + 28*i217 + 228);
    _t5_21 = _asm256_loadu_pd(C + 28*i217 + 256);
    _t5_22 = _asm256_loadu_pd(C + 28*i217 + 284);
    _t5_23 = _asm256_loadu_pd(C + 28*i217 + 312);
    _t5_15 = _mm256_broadcast_sd(C + 28*i217 + 224);
    _t5_14 = _mm256_broadcast_sd(C + 28*i217 + 225);
    _t5_13 = _mm256_broadcast_sd(C + 28*i217 + 226);
    _t5_12 = _mm256_broadcast_sd(C + 28*i217 + 227);
    _t5_11 = _mm256_broadcast_sd(C + 28*i217 + 252);
    _t5_10 = _mm256_broadcast_sd(C + 28*i217 + 253);
    _t5_9 = _mm256_broadcast_sd(C + 28*i217 + 254);
    _t5_8 = _mm256_broadcast_sd(C + 28*i217 + 255);
    _t5_7 = _mm256_broadcast_sd(C + 28*i217 + 280);
    _t5_6 = _mm256_broadcast_sd(C + 28*i217 + 281);
    _t5_5 = _mm256_broadcast_sd(C + 28*i217 + 282);
    _t5_4 = _mm256_broadcast_sd(C + 28*i217 + 283);
    _t5_3 = _mm256_broadcast_sd(C + 28*i217 + 308);
    _t5_2 = _mm256_broadcast_sd(C + 28*i217 + 309);
    _t5_1 = _mm256_broadcast_sd(C + 28*i217 + 310);
    _t5_0 = _mm256_broadcast_sd(C + 28*i217 + 311);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t5_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t4_440), _mm256_mul_pd(_t5_14, _t4_441)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t4_442), _mm256_mul_pd(_t5_12, _t4_443)));
    _t5_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t4_440), _mm256_mul_pd(_t5_10, _t4_441)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t4_442), _mm256_mul_pd(_t5_8, _t4_443)));
    _t5_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t4_440), _mm256_mul_pd(_t5_6, _t4_441)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t4_442), _mm256_mul_pd(_t5_4, _t4_443)));
    _t5_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_3, _t4_440), _mm256_mul_pd(_t5_2, _t4_441)), _mm256_add_pd(_mm256_mul_pd(_t5_1, _t4_442), _mm256_mul_pd(_t5_0, _t4_443)));

    // 4-BLAC: 4x4 - 4x4
    _t5_20 = _mm256_sub_pd(_t5_20, _t5_16);
    _t5_21 = _mm256_sub_pd(_t5_21, _t5_17);
    _t5_22 = _mm256_sub_pd(_t5_22, _t5_18);
    _t5_23 = _mm256_sub_pd(_t5_23, _t5_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i217 + 228, _t5_20);
    _asm256_storeu_pd(C + 28*i217 + 256, _t5_21);
    _asm256_storeu_pd(C + 28*i217 + 284, _t5_22);
    _asm256_storeu_pd(C + 28*i217 + 312, _t5_23);
  }


  // Generating : X[28,28] = Sum_{i217} ( S(h(4, 28, i217 + 8), ( G(h(4, 28, i217 + 8), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, i217 + 8), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), X[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 4)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_36, _t4_35), _mm256_unpacklo_pd(_t4_34, _t4_33), 32);
  _t6_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_36, _t4_35), _mm256_unpackhi_pd(_t4_34, _t4_33), 32);
  _t6_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_36, _t4_35), _mm256_unpacklo_pd(_t4_34, _t4_33), 49);
  _t6_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_36, _t4_35), _mm256_unpackhi_pd(_t4_34, _t4_33), 49);


  for( int i217 = 0; i217 <= 19; i217+=4 ) {
    _t7_20 = _asm256_loadu_pd(C + 28*i217 + 228);
    _t7_21 = _asm256_loadu_pd(C + 28*i217 + 256);
    _t7_22 = _asm256_loadu_pd(C + 28*i217 + 284);
    _t7_23 = _asm256_loadu_pd(C + 28*i217 + 312);
    _t7_15 = _mm256_broadcast_sd(L + 28*i217 + 224);
    _t7_14 = _mm256_broadcast_sd(L + 28*i217 + 225);
    _t7_13 = _mm256_broadcast_sd(L + 28*i217 + 226);
    _t7_12 = _mm256_broadcast_sd(L + 28*i217 + 227);
    _t7_11 = _mm256_broadcast_sd(L + 28*i217 + 252);
    _t7_10 = _mm256_broadcast_sd(L + 28*i217 + 253);
    _t7_9 = _mm256_broadcast_sd(L + 28*i217 + 254);
    _t7_8 = _mm256_broadcast_sd(L + 28*i217 + 255);
    _t7_7 = _mm256_broadcast_sd(L + 28*i217 + 280);
    _t7_6 = _mm256_broadcast_sd(L + 28*i217 + 281);
    _t7_5 = _mm256_broadcast_sd(L + 28*i217 + 282);
    _t7_4 = _mm256_broadcast_sd(L + 28*i217 + 283);
    _t7_3 = _mm256_broadcast_sd(L + 28*i217 + 308);
    _t7_2 = _mm256_broadcast_sd(L + 28*i217 + 309);
    _t7_1 = _mm256_broadcast_sd(L + 28*i217 + 310);
    _t7_0 = _mm256_broadcast_sd(L + 28*i217 + 311);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t7_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t6_0), _mm256_mul_pd(_t7_14, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t6_2), _mm256_mul_pd(_t7_12, _t6_3)));
    _t7_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t6_0), _mm256_mul_pd(_t7_10, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t6_2), _mm256_mul_pd(_t7_8, _t6_3)));
    _t7_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t6_0), _mm256_mul_pd(_t7_6, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t6_2), _mm256_mul_pd(_t7_4, _t6_3)));
    _t7_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_3, _t6_0), _mm256_mul_pd(_t7_2, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_1, _t6_2), _mm256_mul_pd(_t7_0, _t6_3)));

    // 4-BLAC: 4x4 - 4x4
    _t7_20 = _mm256_sub_pd(_t7_20, _t7_16);
    _t7_21 = _mm256_sub_pd(_t7_21, _t7_17);
    _t7_22 = _mm256_sub_pd(_t7_22, _t7_18);
    _t7_23 = _mm256_sub_pd(_t7_23, _t7_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i217 + 228, _t7_20);
    _asm256_storeu_pd(C + 28*i217 + 256, _t7_21);
    _asm256_storeu_pd(C + 28*i217 + 284, _t7_22);
    _asm256_storeu_pd(C + 28*i217 + 312, _t7_23);
  }


  // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, i100 + 8), ( G(h(4, 28, i100 + 8), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, i100 + 8), L[28,28],h(4, 28, 4)) * G(h(4, 28, 4), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t8_0 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_83, _mm256_blend_pd(_mm256_unpacklo_pd(_t4_88, _t4_92), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_unpacklo_pd(_t4_96, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_unpacklo_pd(_t4_97, _t4_98), 32), 0), 32);
  _t8_1 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t4_88, _t4_92), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_unpacklo_pd(_t4_96, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_unpacklo_pd(_t4_97, _t4_98), 32), 3), 32);
  _t8_2 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_unpacklo_pd(_t4_96, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_90, _t4_94), _mm256_unpacklo_pd(_t4_96, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_unpacklo_pd(_t4_97, _t4_98), 32), 3), 12);
  _t8_3 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_91, _t4_95), _mm256_unpacklo_pd(_t4_97, _t4_98), 32);


  for( int i100 = 0; i100 <= 19; i100+=4 ) {
    _t9_20 = _asm256_loadu_pd(C + 28*i100 + 228);
    _t9_21 = _asm256_loadu_pd(C + 28*i100 + 256);
    _t9_22 = _asm256_loadu_pd(C + 28*i100 + 284);
    _t9_23 = _asm256_loadu_pd(C + 28*i100 + 312);
    _t9_15 = _mm256_broadcast_sd(L + 28*i100 + 228);
    _t9_14 = _mm256_broadcast_sd(L + 28*i100 + 229);
    _t9_13 = _mm256_broadcast_sd(L + 28*i100 + 230);
    _t9_12 = _mm256_broadcast_sd(L + 28*i100 + 231);
    _t9_11 = _mm256_broadcast_sd(L + 28*i100 + 256);
    _t9_10 = _mm256_broadcast_sd(L + 28*i100 + 257);
    _t9_9 = _mm256_broadcast_sd(L + 28*i100 + 258);
    _t9_8 = _mm256_broadcast_sd(L + 28*i100 + 259);
    _t9_7 = _mm256_broadcast_sd(L + 28*i100 + 284);
    _t9_6 = _mm256_broadcast_sd(L + 28*i100 + 285);
    _t9_5 = _mm256_broadcast_sd(L + 28*i100 + 286);
    _t9_4 = _mm256_broadcast_sd(L + 28*i100 + 287);
    _t9_3 = _mm256_broadcast_sd(L + 28*i100 + 312);
    _t9_2 = _mm256_broadcast_sd(L + 28*i100 + 313);
    _t9_1 = _mm256_broadcast_sd(L + 28*i100 + 314);
    _t9_0 = _mm256_broadcast_sd(L + 28*i100 + 315);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t9_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t8_0), _mm256_mul_pd(_t9_14, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t8_2), _mm256_mul_pd(_t9_12, _t8_3)));
    _t9_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t8_0), _mm256_mul_pd(_t9_10, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t8_2), _mm256_mul_pd(_t9_8, _t8_3)));
    _t9_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t8_0), _mm256_mul_pd(_t9_6, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t8_2), _mm256_mul_pd(_t9_4, _t8_3)));
    _t9_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_3, _t8_0), _mm256_mul_pd(_t9_2, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_1, _t8_2), _mm256_mul_pd(_t9_0, _t8_3)));

    // 4-BLAC: 4x4 - 4x4
    _t9_20 = _mm256_sub_pd(_t9_20, _t9_16);
    _t9_21 = _mm256_sub_pd(_t9_21, _t9_17);
    _t9_22 = _mm256_sub_pd(_t9_22, _t9_18);
    _t9_23 = _mm256_sub_pd(_t9_23, _t9_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i100 + 228, _t9_20);
    _asm256_storeu_pd(C + 28*i100 + 256, _t9_21);
    _asm256_storeu_pd(C + 28*i100 + 284, _t9_22);
    _asm256_storeu_pd(C + 28*i100 + 312, _t9_23);
  }


  for( int fi1034 = 0; fi1034 <= 15; fi1034+=4 ) {
    _t10_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 228])));
    _t10_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 232])));
    _t10_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 229])));
    _t10_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 230])));
    _t10_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 231])));
    _t10_11 = _asm256_loadu_pd(C + 28*fi1034 + 256);
    _t10_12 = _asm256_loadu_pd(C + 28*fi1034 + 284);
    _t10_13 = _asm256_loadu_pd(C + 28*fi1034 + 312);
    _t10_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 260)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 288))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 316)), 32);
    _t10_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 261])));
    _t10_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 289)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 317)), 0);
    _t10_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 290])));
    _t10_1 = _mm256_broadcast_sd(&(L[29*fi1034 + 318]));
    _t10_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 319])));

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1034 + 8), L[28,28],h(1, 28, fi1034 + 8)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_27 = _t10_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_28 = _t10_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_29 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t10_30 = _mm256_add_pd(_t10_28, _t10_29);

    // 4-BLAC: 1x4 / 1x4
    _t10_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_27), _mm256_castpd256_pd128(_t10_30)));

    // AVX Storer:
    _t10_7 = _t10_31;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_32 = _t10_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_33 = _t10_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_34 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t10_35 = _t10_34;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_36 = _mm256_mul_pd(_t10_33, _t10_35);

    // 4-BLAC: 1x4 - 1x4
    _t10_37 = _mm256_sub_pd(_t10_32, _t10_36);

    // AVX Storer:
    _t10_8 = _t10_37;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1034 + 8), L[28,28],h(1, 28, fi1034 + 8)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_38 = _t10_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_39 = _t10_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_40 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t10_41 = _mm256_add_pd(_t10_39, _t10_40);

    // 4-BLAC: 1x4 / 1x4
    _t10_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_38), _mm256_castpd256_pd128(_t10_41)));

    // AVX Storer:
    _t10_8 = _t10_42;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1034 + 8), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_43 = _t10_9;

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_44 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_7, _t10_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_45 = _t4_2;

    // 4-BLAC: (1x4)^T
    _t10_46 = _t10_45;

    // 4-BLAC: 1x4 * 4x1
    _t10_47 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_44, _t10_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_44, _t10_46), _mm256_mul_pd(_t10_44, _t10_46), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_44, _t10_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_44, _t10_46), _mm256_mul_pd(_t10_44, _t10_46), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_44, _t10_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_44, _t10_46), _mm256_mul_pd(_t10_44, _t10_46), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_48 = _mm256_sub_pd(_t10_43, _t10_47);

    // AVX Storer:
    _t10_9 = _t10_48;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1034 + 8), L[28,28],h(1, 28, fi1034 + 8)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_49 = _t10_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_50 = _t10_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_51 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t10_52 = _mm256_add_pd(_t10_50, _t10_51);

    // 4-BLAC: 1x4 / 1x4
    _t10_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_49), _mm256_castpd256_pd128(_t10_52)));

    // AVX Storer:
    _t10_9 = _t10_53;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1034 + 8), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_54 = _t10_10;

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_55 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_8), _mm256_unpacklo_pd(_t10_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_56 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t10_57 = _t10_56;

    // 4-BLAC: 1x4 * 4x1
    _t10_58 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_55, _t10_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_55, _t10_57), _mm256_mul_pd(_t10_55, _t10_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_55, _t10_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_55, _t10_57), _mm256_mul_pd(_t10_55, _t10_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_55, _t10_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_55, _t10_57), _mm256_mul_pd(_t10_55, _t10_57), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_59 = _mm256_sub_pd(_t10_54, _t10_58);

    // AVX Storer:
    _t10_10 = _t10_59;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 8), ( G(h(1, 28, fi1034 + 8), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1034 + 8), L[28,28],h(1, 28, fi1034 + 8)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_60 = _t10_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_61 = _t10_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_62 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t10_63 = _mm256_add_pd(_t10_61, _t10_62);

    // 4-BLAC: 1x4 / 1x4
    _t10_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_60), _mm256_castpd256_pd128(_t10_63)));

    // AVX Storer:
    _t10_10 = _t10_64;

    // Generating : X[28,28] = S(h(3, 28, fi1034 + 9), ( G(h(3, 28, fi1034 + 9), X[28,28],h(4, 28, 4)) - ( G(h(3, 28, fi1034 + 9), L[28,28],h(1, 28, fi1034 + 8)) * G(h(1, 28, fi1034 + 8), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t10_65 = _t10_11;
    _t10_66 = _t10_12;
    _t10_67 = _t10_13;
    _t10_68 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t10_69 = _t10_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t10_70 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_69, _t10_69, 32), _mm256_permute2f128_pd(_t10_69, _t10_69, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_8), _mm256_unpacklo_pd(_t10_9, _t10_10), 32));
    _t10_71 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_69, _t10_69, 32), _mm256_permute2f128_pd(_t10_69, _t10_69, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_8), _mm256_unpacklo_pd(_t10_9, _t10_10), 32));
    _t10_72 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_69, _t10_69, 49), _mm256_permute2f128_pd(_t10_69, _t10_69, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_8), _mm256_unpacklo_pd(_t10_9, _t10_10), 32));
    _t10_73 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_69, _t10_69, 49), _mm256_permute2f128_pd(_t10_69, _t10_69, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_8), _mm256_unpacklo_pd(_t10_9, _t10_10), 32));

    // 4-BLAC: 4x4 - 4x4
    _t10_74 = _mm256_sub_pd(_t10_65, _t10_70);
    _t10_75 = _mm256_sub_pd(_t10_66, _t10_71);
    _t10_76 = _mm256_sub_pd(_t10_67, _t10_72);
    _t10_77 = _mm256_sub_pd(_t10_68, _t10_73);

    // AVX Storer:
    _t10_11 = _t10_74;
    _t10_12 = _t10_75;
    _t10_13 = _t10_76;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1034 + 9), L[28,28],h(1, 28, fi1034 + 9)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_78 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_11, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_79 = _t10_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_80 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t10_81 = _mm256_add_pd(_t10_79, _t10_80);

    // 4-BLAC: 1x4 / 1x4
    _t10_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_78), _mm256_castpd256_pd128(_t10_81)));

    // AVX Storer:
    _t10_14 = _t10_82;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_83 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_11, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_84 = _t10_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_85 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t10_86 = _t10_85;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_87 = _mm256_mul_pd(_t10_84, _t10_86);

    // 4-BLAC: 1x4 - 1x4
    _t10_88 = _mm256_sub_pd(_t10_83, _t10_87);

    // AVX Storer:
    _t10_15 = _t10_88;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1034 + 9), L[28,28],h(1, 28, fi1034 + 9)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_89 = _t10_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_90 = _t10_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_91 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t10_92 = _mm256_add_pd(_t10_90, _t10_91);

    // 4-BLAC: 1x4 / 1x4
    _t10_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_89), _mm256_castpd256_pd128(_t10_92)));

    // AVX Storer:
    _t10_15 = _t10_93;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1034 + 9), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_94 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_11, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_95 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_14, _t10_15), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_96 = _t4_2;

    // 4-BLAC: (1x4)^T
    _t10_97 = _t10_96;

    // 4-BLAC: 1x4 * 4x1
    _t10_98 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_95, _t10_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_95, _t10_97), _mm256_mul_pd(_t10_95, _t10_97), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_95, _t10_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_95, _t10_97), _mm256_mul_pd(_t10_95, _t10_97), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_95, _t10_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_95, _t10_97), _mm256_mul_pd(_t10_95, _t10_97), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_99 = _mm256_sub_pd(_t10_94, _t10_98);

    // AVX Storer:
    _t10_16 = _t10_99;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1034 + 9), L[28,28],h(1, 28, fi1034 + 9)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_100 = _t10_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_101 = _t10_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_102 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t10_103 = _mm256_add_pd(_t10_101, _t10_102);

    // 4-BLAC: 1x4 / 1x4
    _t10_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_100), _mm256_castpd256_pd128(_t10_103)));

    // AVX Storer:
    _t10_16 = _t10_104;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1034 + 9), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_11, _t10_11, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_14, _t10_15), _mm256_unpacklo_pd(_t10_16, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_107 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t10_108 = _t10_107;

    // 4-BLAC: 1x4 * 4x1
    _t10_109 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_106, _t10_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_106, _t10_108), _mm256_mul_pd(_t10_106, _t10_108), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_106, _t10_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_106, _t10_108), _mm256_mul_pd(_t10_106, _t10_108), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_106, _t10_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_106, _t10_108), _mm256_mul_pd(_t10_106, _t10_108), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_110 = _mm256_sub_pd(_t10_105, _t10_109);

    // AVX Storer:
    _t10_17 = _t10_110;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 9), ( G(h(1, 28, fi1034 + 9), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1034 + 9), L[28,28],h(1, 28, fi1034 + 9)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_111 = _t10_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_112 = _t10_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_113 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t10_114 = _mm256_add_pd(_t10_112, _t10_113);

    // 4-BLAC: 1x4 / 1x4
    _t10_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_111), _mm256_castpd256_pd128(_t10_114)));

    // AVX Storer:
    _t10_17 = _t10_115;

    // Generating : X[28,28] = S(h(2, 28, fi1034 + 10), ( G(h(2, 28, fi1034 + 10), X[28,28],h(4, 28, 4)) - ( G(h(2, 28, fi1034 + 10), L[28,28],h(1, 28, fi1034 + 9)) * G(h(1, 28, fi1034 + 9), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t10_116 = _t10_12;
    _t10_117 = _t10_13;
    _t10_118 = _mm256_setzero_pd();
    _t10_119 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t10_120 = _t10_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t10_121 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_120, _t10_120, 32), _mm256_permute2f128_pd(_t10_120, _t10_120, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_14, _t10_15), _mm256_unpacklo_pd(_t10_16, _t10_17), 32));
    _t10_122 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_120, _t10_120, 32), _mm256_permute2f128_pd(_t10_120, _t10_120, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_14, _t10_15), _mm256_unpacklo_pd(_t10_16, _t10_17), 32));
    _t10_123 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_120, _t10_120, 49), _mm256_permute2f128_pd(_t10_120, _t10_120, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_14, _t10_15), _mm256_unpacklo_pd(_t10_16, _t10_17), 32));
    _t10_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_120, _t10_120, 49), _mm256_permute2f128_pd(_t10_120, _t10_120, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_14, _t10_15), _mm256_unpacklo_pd(_t10_16, _t10_17), 32));

    // 4-BLAC: 4x4 - 4x4
    _t10_125 = _mm256_sub_pd(_t10_116, _t10_121);
    _t10_126 = _mm256_sub_pd(_t10_117, _t10_122);
    _t10_127 = _mm256_sub_pd(_t10_118, _t10_123);
    _t10_128 = _mm256_sub_pd(_t10_119, _t10_124);

    // AVX Storer:
    _t10_12 = _t10_125;
    _t10_13 = _t10_126;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1034 + 10), L[28,28],h(1, 28, fi1034 + 10)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_129 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_12, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_130 = _t10_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_131 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t10_132 = _mm256_add_pd(_t10_130, _t10_131);

    // 4-BLAC: 1x4 / 1x4
    _t10_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_129), _mm256_castpd256_pd128(_t10_132)));

    // AVX Storer:
    _t10_18 = _t10_133;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_134 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_12, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_135 = _t10_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_136 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t10_137 = _t10_136;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_138 = _mm256_mul_pd(_t10_135, _t10_137);

    // 4-BLAC: 1x4 - 1x4
    _t10_139 = _mm256_sub_pd(_t10_134, _t10_138);

    // AVX Storer:
    _t10_19 = _t10_139;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1034 + 10), L[28,28],h(1, 28, fi1034 + 10)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_140 = _t10_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_141 = _t10_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_142 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t10_143 = _mm256_add_pd(_t10_141, _t10_142);

    // 4-BLAC: 1x4 / 1x4
    _t10_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_140), _mm256_castpd256_pd128(_t10_143)));

    // AVX Storer:
    _t10_19 = _t10_144;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1034 + 10), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_145 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_12, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_146 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_18, _t10_19), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_147 = _t4_2;

    // 4-BLAC: (1x4)^T
    _t10_148 = _t10_147;

    // 4-BLAC: 1x4 * 4x1
    _t10_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_146, _t10_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_146, _t10_148), _mm256_mul_pd(_t10_146, _t10_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_146, _t10_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_146, _t10_148), _mm256_mul_pd(_t10_146, _t10_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_146, _t10_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_146, _t10_148), _mm256_mul_pd(_t10_146, _t10_148), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_150 = _mm256_sub_pd(_t10_145, _t10_149);

    // AVX Storer:
    _t10_20 = _t10_150;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1034 + 10), L[28,28],h(1, 28, fi1034 + 10)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_151 = _t10_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_152 = _t10_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_153 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t10_154 = _mm256_add_pd(_t10_152, _t10_153);

    // 4-BLAC: 1x4 / 1x4
    _t10_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_151), _mm256_castpd256_pd128(_t10_154)));

    // AVX Storer:
    _t10_20 = _t10_155;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1034 + 10), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_156 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_12, _t10_12, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_18, _t10_19), _mm256_unpacklo_pd(_t10_20, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_158 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t10_159 = _t10_158;

    // 4-BLAC: 1x4 * 4x1
    _t10_160 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_157, _t10_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_157, _t10_159), _mm256_mul_pd(_t10_157, _t10_159), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_157, _t10_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_157, _t10_159), _mm256_mul_pd(_t10_157, _t10_159), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_157, _t10_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_157, _t10_159), _mm256_mul_pd(_t10_157, _t10_159), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_161 = _mm256_sub_pd(_t10_156, _t10_160);

    // AVX Storer:
    _t10_21 = _t10_161;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 10), ( G(h(1, 28, fi1034 + 10), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1034 + 10), L[28,28],h(1, 28, fi1034 + 10)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_162 = _t10_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_163 = _t10_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_164 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t10_165 = _mm256_add_pd(_t10_163, _t10_164);

    // 4-BLAC: 1x4 / 1x4
    _t10_166 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_162), _mm256_castpd256_pd128(_t10_165)));

    // AVX Storer:
    _t10_21 = _t10_166;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(4, 28, 4)) - ( G(h(1, 28, fi1034 + 11), L[28,28],h(1, 28, fi1034 + 10)) Kro G(h(1, 28, fi1034 + 10), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_167 = _t10_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t10_26 = _mm256_mul_pd(_t10_167, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_18, _t10_19), _mm256_unpacklo_pd(_t10_20, _t10_21), 32));

    // 4-BLAC: 1x4 - 1x4
    _t10_13 = _mm256_sub_pd(_t10_13, _t10_26);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1034 + 11), L[28,28],h(1, 28, fi1034 + 11)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_168 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_169 = _t10_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_170 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t10_171 = _mm256_add_pd(_t10_169, _t10_170);

    // 4-BLAC: 1x4 / 1x4
    _t10_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_168), _mm256_castpd256_pd128(_t10_171)));

    // AVX Storer:
    _t10_22 = _t10_172;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_173 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_13, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_174 = _t10_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_175 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t10_176 = _t10_175;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_177 = _mm256_mul_pd(_t10_174, _t10_176);

    // 4-BLAC: 1x4 - 1x4
    _t10_178 = _mm256_sub_pd(_t10_173, _t10_177);

    // AVX Storer:
    _t10_23 = _t10_178;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1034 + 11), L[28,28],h(1, 28, fi1034 + 11)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_179 = _t10_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_180 = _t10_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_181 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t10_182 = _mm256_add_pd(_t10_180, _t10_181);

    // 4-BLAC: 1x4 / 1x4
    _t10_183 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_179), _mm256_castpd256_pd128(_t10_182)));

    // AVX Storer:
    _t10_23 = _t10_183;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1034 + 11), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_184 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_13, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_185 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_22, _t10_23), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t10_186 = _t4_2;

    // 4-BLAC: (1x4)^T
    _t10_187 = _t10_186;

    // 4-BLAC: 1x4 * 4x1
    _t10_188 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_185, _t10_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_185, _t10_187), _mm256_mul_pd(_t10_185, _t10_187), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_185, _t10_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_185, _t10_187), _mm256_mul_pd(_t10_185, _t10_187), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_185, _t10_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_185, _t10_187), _mm256_mul_pd(_t10_185, _t10_187), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_189 = _mm256_sub_pd(_t10_184, _t10_188);

    // AVX Storer:
    _t10_24 = _t10_189;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1034 + 11), L[28,28],h(1, 28, fi1034 + 11)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_190 = _t10_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_191 = _t10_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_192 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t10_193 = _mm256_add_pd(_t10_191, _t10_192);

    // 4-BLAC: 1x4 / 1x4
    _t10_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_190), _mm256_castpd256_pd128(_t10_193)));

    // AVX Storer:
    _t10_24 = _t10_194;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1034 + 11), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_195 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_13, _t10_13, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_196 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_22, _t10_23), _mm256_unpacklo_pd(_t10_24, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t10_197 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t10_198 = _t10_197;

    // 4-BLAC: 1x4 * 4x1
    _t10_199 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_196, _t10_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_196, _t10_198), _mm256_mul_pd(_t10_196, _t10_198), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_196, _t10_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_196, _t10_198), _mm256_mul_pd(_t10_196, _t10_198), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_196, _t10_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_196, _t10_198), _mm256_mul_pd(_t10_196, _t10_198), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t10_200 = _mm256_sub_pd(_t10_195, _t10_199);

    // AVX Storer:
    _t10_25 = _t10_200;

    // Generating : X[28,28] = S(h(1, 28, fi1034 + 11), ( G(h(1, 28, fi1034 + 11), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1034 + 11), L[28,28],h(1, 28, fi1034 + 11)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_201 = _t10_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_202 = _t10_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t10_203 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t10_204 = _mm256_add_pd(_t10_202, _t10_203);

    // 4-BLAC: 1x4 / 1x4
    _t10_205 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_201), _mm256_castpd256_pd128(_t10_204)));

    // AVX Storer:
    _t10_25 = _t10_205;

    // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, fi1034 + i100 + 12), ( G(h(4, 28, fi1034 + i100 + 12), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, fi1034 + i100 + 12), L[28,28],h(4, 28, fi1034 + 8)) * G(h(4, 28, fi1034 + 8), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4)) )

    // AVX Loader:
    _mm_store_sd(&(C[28*fi1034 + 228]), _mm256_castpd256_pd128(_t10_7));
    _mm_store_sd(&(C[28*fi1034 + 229]), _mm256_castpd256_pd128(_t10_8));
    _mm_store_sd(&(C[28*fi1034 + 230]), _mm256_castpd256_pd128(_t10_9));
    _mm_store_sd(&(C[28*fi1034 + 231]), _mm256_castpd256_pd128(_t10_10));
    _mm_store_sd(&(C[28*fi1034 + 256]), _mm256_castpd256_pd128(_t10_14));
    _mm_store_sd(&(C[28*fi1034 + 257]), _mm256_castpd256_pd128(_t10_15));
    _mm_store_sd(&(C[28*fi1034 + 258]), _mm256_castpd256_pd128(_t10_16));
    _mm_store_sd(&(C[28*fi1034 + 259]), _mm256_castpd256_pd128(_t10_17));
    _mm_store_sd(&(C[28*fi1034 + 284]), _mm256_castpd256_pd128(_t10_18));
    _mm_store_sd(&(C[28*fi1034 + 285]), _mm256_castpd256_pd128(_t10_19));
    _mm_store_sd(&(C[28*fi1034 + 286]), _mm256_castpd256_pd128(_t10_20));
    _mm_store_sd(&(C[28*fi1034 + 287]), _mm256_castpd256_pd128(_t10_21));
    _mm_store_sd(&(C[28*fi1034 + 312]), _mm256_castpd256_pd128(_t10_22));
    _mm_store_sd(&(C[28*fi1034 + 313]), _mm256_castpd256_pd128(_t10_23));
    _mm_store_sd(&(C[28*fi1034 + 314]), _mm256_castpd256_pd128(_t10_24));
    _mm_store_sd(&(C[28*fi1034 + 315]), _mm256_castpd256_pd128(_t10_25));

    for( int i100 = 0; i100 <= -fi1034 + 15; i100+=4 ) {
      _t11_36 = _asm256_loadu_pd(C + 28*fi1034 + 28*i100 + 340);
      _t11_37 = _asm256_loadu_pd(C + 28*fi1034 + 28*i100 + 368);
      _t11_38 = _asm256_loadu_pd(C + 28*fi1034 + 28*i100 + 396);
      _t11_39 = _asm256_loadu_pd(C + 28*fi1034 + 28*i100 + 424);
      _t11_31 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 344);
      _t11_30 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 345);
      _t11_29 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 346);
      _t11_28 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 347);
      _t11_27 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 372);
      _t11_26 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 373);
      _t11_25 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 374);
      _t11_24 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 375);
      _t11_23 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 400);
      _t11_22 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 401);
      _t11_21 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 402);
      _t11_20 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 403);
      _t11_19 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 428);
      _t11_18 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 429);
      _t11_17 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 430);
      _t11_16 = _mm256_broadcast_sd(L + 29*fi1034 + 28*i100 + 431);
      _t11_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 228])));
      _t11_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 229])));
      _t11_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 230])));
      _t11_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 231])));
      _t11_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 256])));
      _t11_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 257])));
      _t11_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 258])));
      _t11_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 259])));
      _t11_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 284])));
      _t11_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 285])));
      _t11_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 286])));
      _t11_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 287])));
      _t11_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 312])));
      _t11_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 313])));
      _t11_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 314])));
      _t11_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 315])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t11_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_31, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_15, _t11_14), _mm256_unpacklo_pd(_t11_13, _t11_12), 32)), _mm256_mul_pd(_t11_30, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_10), _mm256_unpacklo_pd(_t11_9, _t11_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t11_29, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_6), _mm256_unpacklo_pd(_t11_5, _t11_4), 32)), _mm256_mul_pd(_t11_28, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32))));
      _t11_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_27, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_15, _t11_14), _mm256_unpacklo_pd(_t11_13, _t11_12), 32)), _mm256_mul_pd(_t11_26, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_10), _mm256_unpacklo_pd(_t11_9, _t11_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t11_25, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_6), _mm256_unpacklo_pd(_t11_5, _t11_4), 32)), _mm256_mul_pd(_t11_24, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32))));
      _t11_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_23, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_15, _t11_14), _mm256_unpacklo_pd(_t11_13, _t11_12), 32)), _mm256_mul_pd(_t11_22, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_10), _mm256_unpacklo_pd(_t11_9, _t11_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t11_21, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_6), _mm256_unpacklo_pd(_t11_5, _t11_4), 32)), _mm256_mul_pd(_t11_20, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32))));
      _t11_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_15, _t11_14), _mm256_unpacklo_pd(_t11_13, _t11_12), 32)), _mm256_mul_pd(_t11_18, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_10), _mm256_unpacklo_pd(_t11_9, _t11_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t11_17, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_6), _mm256_unpacklo_pd(_t11_5, _t11_4), 32)), _mm256_mul_pd(_t11_16, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32))));

      // 4-BLAC: 4x4 - 4x4
      _t11_36 = _mm256_sub_pd(_t11_36, _t11_32);
      _t11_37 = _mm256_sub_pd(_t11_37, _t11_33);
      _t11_38 = _mm256_sub_pd(_t11_38, _t11_34);
      _t11_39 = _mm256_sub_pd(_t11_39, _t11_35);

      // AVX Storer:
      _asm256_storeu_pd(C + 28*fi1034 + 28*i100 + 340, _t11_36);
      _asm256_storeu_pd(C + 28*fi1034 + 28*i100 + 368, _t11_37);
      _asm256_storeu_pd(C + 28*fi1034 + 28*i100 + 396, _t11_38);
      _asm256_storeu_pd(C + 28*fi1034 + 28*i100 + 424, _t11_39);
    }
  }

  _t12_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[676])));
  _t12_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[677])));
  _t12_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[678])));
  _t12_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[679])));
  _t12_4 = _asm256_loadu_pd(C + 704);
  _t12_5 = _asm256_loadu_pd(C + 732);
  _t12_6 = _asm256_loadu_pd(C + 760);

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_20 = _t12_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_21 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_22 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t12_23 = _mm256_add_pd(_t12_21, _t12_22);

  // 4-BLAC: 1x4 / 1x4
  _t12_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_20), _mm256_castpd256_pd128(_t12_23)));

  // AVX Storer:
  _t12_0 = _t12_24;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_25 = _t12_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_26 = _t12_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_27 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t12_28 = _t12_27;

  // 4-BLAC: 1x4 Kro 1x4
  _t12_29 = _mm256_mul_pd(_t12_26, _t12_28);

  // 4-BLAC: 1x4 - 1x4
  _t12_30 = _mm256_sub_pd(_t12_25, _t12_29);

  // AVX Storer:
  _t12_1 = _t12_30;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_31 = _t12_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_32 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_33 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t12_34 = _mm256_add_pd(_t12_32, _t12_33);

  // 4-BLAC: 1x4 / 1x4
  _t12_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_31), _mm256_castpd256_pd128(_t12_34)));

  // AVX Storer:
  _t12_1 = _t12_35;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_36 = _t12_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_37 = _mm256_blend_pd(_mm256_unpacklo_pd(_t12_0, _t12_1), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_38 = _t4_2;

  // 4-BLAC: (1x4)^T
  _t12_39 = _t12_38;

  // 4-BLAC: 1x4 * 4x1
  _t12_40 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_37, _t12_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_37, _t12_39), _mm256_mul_pd(_t12_37, _t12_39), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_37, _t12_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_37, _t12_39), _mm256_mul_pd(_t12_37, _t12_39), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_37, _t12_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_37, _t12_39), _mm256_mul_pd(_t12_37, _t12_39), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_41 = _mm256_sub_pd(_t12_36, _t12_40);

  // AVX Storer:
  _t12_2 = _t12_41;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_42 = _t12_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_43 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_44 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t12_45 = _mm256_add_pd(_t12_43, _t12_44);

  // 4-BLAC: 1x4 / 1x4
  _t12_46 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_42), _mm256_castpd256_pd128(_t12_45)));

  // AVX Storer:
  _t12_2 = _t12_46;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_47 = _t12_3;

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_0, _t12_1), _mm256_unpacklo_pd(_t12_2, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_49 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t12_50 = _t12_49;

  // 4-BLAC: 1x4 * 4x1
  _t12_51 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_48, _t12_50), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_48, _t12_50), _mm256_mul_pd(_t12_48, _t12_50), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_48, _t12_50), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_48, _t12_50), _mm256_mul_pd(_t12_48, _t12_50), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_48, _t12_50), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_48, _t12_50), _mm256_mul_pd(_t12_48, _t12_50), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_52 = _mm256_sub_pd(_t12_47, _t12_51);

  // AVX Storer:
  _t12_3 = _t12_52;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_53 = _t12_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_54 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_55 = _t4_6;

  // 4-BLAC: 1x4 + 1x4
  _t12_56 = _mm256_add_pd(_t12_54, _t12_55);

  // 4-BLAC: 1x4 / 1x4
  _t12_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_53), _mm256_castpd256_pd128(_t12_56)));

  // AVX Storer:
  _t12_3 = _t12_57;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, 4)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t12_58 = _t12_4;
  _t12_59 = _t12_5;
  _t12_60 = _t12_6;
  _t12_61 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t12_62 = _t4_58;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t12_63 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_62, _t12_62, 32), _mm256_permute2f128_pd(_t12_62, _t12_62, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_0, _t12_1), _mm256_unpacklo_pd(_t12_2, _t12_3), 32));
  _t12_64 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_62, _t12_62, 32), _mm256_permute2f128_pd(_t12_62, _t12_62, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_0, _t12_1), _mm256_unpacklo_pd(_t12_2, _t12_3), 32));
  _t12_65 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_62, _t12_62, 49), _mm256_permute2f128_pd(_t12_62, _t12_62, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_0, _t12_1), _mm256_unpacklo_pd(_t12_2, _t12_3), 32));
  _t12_66 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_62, _t12_62, 49), _mm256_permute2f128_pd(_t12_62, _t12_62, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_0, _t12_1), _mm256_unpacklo_pd(_t12_2, _t12_3), 32));

  // 4-BLAC: 4x4 - 4x4
  _t12_67 = _mm256_sub_pd(_t12_58, _t12_63);
  _t12_68 = _mm256_sub_pd(_t12_59, _t12_64);
  _t12_69 = _mm256_sub_pd(_t12_60, _t12_65);
  _t12_70 = _mm256_sub_pd(_t12_61, _t12_66);

  // AVX Storer:
  _t12_4 = _t12_67;
  _t12_5 = _t12_68;
  _t12_6 = _t12_69;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_71 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_72 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_73 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t12_74 = _mm256_add_pd(_t12_72, _t12_73);

  // 4-BLAC: 1x4 / 1x4
  _t12_75 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_71), _mm256_castpd256_pd128(_t12_74)));

  // AVX Storer:
  _t12_7 = _t12_75;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_76 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_77 = _t12_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_78 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t12_79 = _t12_78;

  // 4-BLAC: 1x4 Kro 1x4
  _t12_80 = _mm256_mul_pd(_t12_77, _t12_79);

  // 4-BLAC: 1x4 - 1x4
  _t12_81 = _mm256_sub_pd(_t12_76, _t12_80);

  // AVX Storer:
  _t12_8 = _t12_81;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_82 = _t12_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_83 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_84 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t12_85 = _mm256_add_pd(_t12_83, _t12_84);

  // 4-BLAC: 1x4 / 1x4
  _t12_86 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_82), _mm256_castpd256_pd128(_t12_85)));

  // AVX Storer:
  _t12_8 = _t12_86;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_87 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_88 = _mm256_blend_pd(_mm256_unpacklo_pd(_t12_7, _t12_8), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_89 = _t4_2;

  // 4-BLAC: (1x4)^T
  _t12_90 = _t12_89;

  // 4-BLAC: 1x4 * 4x1
  _t12_91 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_88, _t12_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_88, _t12_90), _mm256_mul_pd(_t12_88, _t12_90), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_88, _t12_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_88, _t12_90), _mm256_mul_pd(_t12_88, _t12_90), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_88, _t12_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_88, _t12_90), _mm256_mul_pd(_t12_88, _t12_90), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_92 = _mm256_sub_pd(_t12_87, _t12_91);

  // AVX Storer:
  _t12_9 = _t12_92;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_93 = _t12_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_94 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_95 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t12_96 = _mm256_add_pd(_t12_94, _t12_95);

  // 4-BLAC: 1x4 / 1x4
  _t12_97 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_93), _mm256_castpd256_pd128(_t12_96)));

  // AVX Storer:
  _t12_9 = _t12_97;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_98 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_4, _t12_4, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_99 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_8), _mm256_unpacklo_pd(_t12_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_100 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t12_101 = _t12_100;

  // 4-BLAC: 1x4 * 4x1
  _t12_102 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_99, _t12_101), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_99, _t12_101), _mm256_mul_pd(_t12_99, _t12_101), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_99, _t12_101), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_99, _t12_101), _mm256_mul_pd(_t12_99, _t12_101), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_99, _t12_101), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_99, _t12_101), _mm256_mul_pd(_t12_99, _t12_101), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_103 = _mm256_sub_pd(_t12_98, _t12_102);

  // AVX Storer:
  _t12_10 = _t12_103;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_104 = _t12_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_105 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_106 = _t4_6;

  // 4-BLAC: 1x4 + 1x4
  _t12_107 = _mm256_add_pd(_t12_105, _t12_106);

  // 4-BLAC: 1x4 / 1x4
  _t12_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_104), _mm256_castpd256_pd128(_t12_107)));

  // AVX Storer:
  _t12_10 = _t12_108;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, 4)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t12_109 = _t12_5;
  _t12_110 = _t12_6;
  _t12_111 = _mm256_setzero_pd();
  _t12_112 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t12_113 = _t4_56;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t12_114 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_113, _t12_113, 32), _mm256_permute2f128_pd(_t12_113, _t12_113, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_8), _mm256_unpacklo_pd(_t12_9, _t12_10), 32));
  _t12_115 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_113, _t12_113, 32), _mm256_permute2f128_pd(_t12_113, _t12_113, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_8), _mm256_unpacklo_pd(_t12_9, _t12_10), 32));
  _t12_116 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_113, _t12_113, 49), _mm256_permute2f128_pd(_t12_113, _t12_113, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_8), _mm256_unpacklo_pd(_t12_9, _t12_10), 32));
  _t12_117 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_113, _t12_113, 49), _mm256_permute2f128_pd(_t12_113, _t12_113, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_8), _mm256_unpacklo_pd(_t12_9, _t12_10), 32));

  // 4-BLAC: 4x4 - 4x4
  _t12_118 = _mm256_sub_pd(_t12_109, _t12_114);
  _t12_119 = _mm256_sub_pd(_t12_110, _t12_115);
  _t12_120 = _mm256_sub_pd(_t12_111, _t12_116);
  _t12_121 = _mm256_sub_pd(_t12_112, _t12_117);

  // AVX Storer:
  _t12_5 = _t12_118;
  _t12_6 = _t12_119;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_122 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_123 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_124 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t12_125 = _mm256_add_pd(_t12_123, _t12_124);

  // 4-BLAC: 1x4 / 1x4
  _t12_126 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_122), _mm256_castpd256_pd128(_t12_125)));

  // AVX Storer:
  _t12_11 = _t12_126;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_127 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_128 = _t12_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_129 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t12_130 = _t12_129;

  // 4-BLAC: 1x4 Kro 1x4
  _t12_131 = _mm256_mul_pd(_t12_128, _t12_130);

  // 4-BLAC: 1x4 - 1x4
  _t12_132 = _mm256_sub_pd(_t12_127, _t12_131);

  // AVX Storer:
  _t12_12 = _t12_132;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_133 = _t12_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_134 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_135 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t12_136 = _mm256_add_pd(_t12_134, _t12_135);

  // 4-BLAC: 1x4 / 1x4
  _t12_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_133), _mm256_castpd256_pd128(_t12_136)));

  // AVX Storer:
  _t12_12 = _t12_137;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t12_11, _t12_12), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_140 = _t4_2;

  // 4-BLAC: (1x4)^T
  _t12_141 = _t12_140;

  // 4-BLAC: 1x4 * 4x1
  _t12_142 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_139, _t12_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_139, _t12_141), _mm256_mul_pd(_t12_139, _t12_141), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_139, _t12_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_139, _t12_141), _mm256_mul_pd(_t12_139, _t12_141), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_139, _t12_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_139, _t12_141), _mm256_mul_pd(_t12_139, _t12_141), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_143 = _mm256_sub_pd(_t12_138, _t12_142);

  // AVX Storer:
  _t12_13 = _t12_143;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_144 = _t12_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_145 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_146 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t12_147 = _mm256_add_pd(_t12_145, _t12_146);

  // 4-BLAC: 1x4 / 1x4
  _t12_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_144), _mm256_castpd256_pd128(_t12_147)));

  // AVX Storer:
  _t12_13 = _t12_148;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_149 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_5, _t12_5, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_150 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_11, _t12_12), _mm256_unpacklo_pd(_t12_13, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_151 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t12_152 = _t12_151;

  // 4-BLAC: 1x4 * 4x1
  _t12_153 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_150, _t12_152), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_150, _t12_152), _mm256_mul_pd(_t12_150, _t12_152), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_150, _t12_152), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_150, _t12_152), _mm256_mul_pd(_t12_150, _t12_152), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_150, _t12_152), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_150, _t12_152), _mm256_mul_pd(_t12_150, _t12_152), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_154 = _mm256_sub_pd(_t12_149, _t12_153);

  // AVX Storer:
  _t12_14 = _t12_154;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_155 = _t12_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_156 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_157 = _t4_6;

  // 4-BLAC: 1x4 + 1x4
  _t12_158 = _mm256_add_pd(_t12_156, _t12_157);

  // 4-BLAC: 1x4 / 1x4
  _t12_159 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_155), _mm256_castpd256_pd128(_t12_158)));

  // AVX Storer:
  _t12_14 = _t12_159;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, 4)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_160 = _t4_54;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t12_19 = _mm256_mul_pd(_t12_160, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_11, _t12_12), _mm256_unpacklo_pd(_t12_13, _t12_14), 32));

  // 4-BLAC: 1x4 - 1x4
  _t12_6 = _mm256_sub_pd(_t12_6, _t12_19);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_161 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_162 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_163 = _t4_12;

  // 4-BLAC: 1x4 + 1x4
  _t12_164 = _mm256_add_pd(_t12_162, _t12_163);

  // 4-BLAC: 1x4 / 1x4
  _t12_165 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_161), _mm256_castpd256_pd128(_t12_164)));

  // AVX Storer:
  _t12_15 = _t12_165;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_166 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_167 = _t12_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_168 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t12_169 = _t12_168;

  // 4-BLAC: 1x4 Kro 1x4
  _t12_170 = _mm256_mul_pd(_t12_167, _t12_169);

  // 4-BLAC: 1x4 - 1x4
  _t12_171 = _mm256_sub_pd(_t12_166, _t12_170);

  // AVX Storer:
  _t12_16 = _t12_171;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_172 = _t12_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_173 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_174 = _t4_10;

  // 4-BLAC: 1x4 + 1x4
  _t12_175 = _mm256_add_pd(_t12_173, _t12_174);

  // 4-BLAC: 1x4 / 1x4
  _t12_176 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_172), _mm256_castpd256_pd128(_t12_175)));

  // AVX Storer:
  _t12_16 = _t12_176;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_177 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_178 = _mm256_blend_pd(_mm256_unpacklo_pd(_t12_15, _t12_16), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_179 = _t4_2;

  // 4-BLAC: (1x4)^T
  _t12_180 = _t12_179;

  // 4-BLAC: 1x4 * 4x1
  _t12_181 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_178, _t12_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_178, _t12_180), _mm256_mul_pd(_t12_178, _t12_180), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_178, _t12_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_178, _t12_180), _mm256_mul_pd(_t12_178, _t12_180), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_178, _t12_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_178, _t12_180), _mm256_mul_pd(_t12_178, _t12_180), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_182 = _mm256_sub_pd(_t12_177, _t12_181);

  // AVX Storer:
  _t12_17 = _t12_182;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_183 = _t12_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_184 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_185 = _t4_8;

  // 4-BLAC: 1x4 + 1x4
  _t12_186 = _mm256_add_pd(_t12_184, _t12_185);

  // 4-BLAC: 1x4 / 1x4
  _t12_187 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_183), _mm256_castpd256_pd128(_t12_186)));

  // AVX Storer:
  _t12_17 = _t12_187;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_188 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_6, _t12_6, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_189 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_15, _t12_16), _mm256_unpacklo_pd(_t12_17, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_190 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t12_191 = _t12_190;

  // 4-BLAC: 1x4 * 4x1
  _t12_192 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_189, _t12_191), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_189, _t12_191), _mm256_mul_pd(_t12_189, _t12_191), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_189, _t12_191), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_189, _t12_191), _mm256_mul_pd(_t12_189, _t12_191), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_189, _t12_191), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_189, _t12_191), _mm256_mul_pd(_t12_189, _t12_191), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_193 = _mm256_sub_pd(_t12_188, _t12_192);

  // AVX Storer:
  _t12_18 = _t12_193;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_194 = _t12_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_195 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_196 = _t4_6;

  // 4-BLAC: 1x4 + 1x4
  _t12_197 = _mm256_add_pd(_t12_195, _t12_196);

  // 4-BLAC: 1x4 / 1x4
  _t12_198 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_194), _mm256_castpd256_pd128(_t12_197)));

  // AVX Storer:
  _t12_18 = _t12_198;

  _mm_store_sd(&(C[676]), _mm256_castpd256_pd128(_t12_0));
  _mm_store_sd(&(C[677]), _mm256_castpd256_pd128(_t12_1));
  _mm_store_sd(&(C[678]), _mm256_castpd256_pd128(_t12_2));
  _mm_store_sd(&(C[679]), _mm256_castpd256_pd128(_t12_3));
  _mm_store_sd(&(C[704]), _mm256_castpd256_pd128(_t12_7));
  _mm_store_sd(&(C[705]), _mm256_castpd256_pd128(_t12_8));
  _mm_store_sd(&(C[706]), _mm256_castpd256_pd128(_t12_9));
  _mm_store_sd(&(C[707]), _mm256_castpd256_pd128(_t12_10));
  _mm_store_sd(&(C[732]), _mm256_castpd256_pd128(_t12_11));
  _mm_store_sd(&(C[733]), _mm256_castpd256_pd128(_t12_12));
  _mm_store_sd(&(C[734]), _mm256_castpd256_pd128(_t12_13));
  _mm_store_sd(&(C[735]), _mm256_castpd256_pd128(_t12_14));
  _mm_store_sd(&(C[760]), _mm256_castpd256_pd128(_t12_15));
  _mm_store_sd(&(C[761]), _mm256_castpd256_pd128(_t12_16));
  _mm_store_sd(&(C[762]), _mm256_castpd256_pd128(_t12_17));
  _mm_store_sd(&(C[763]), _mm256_castpd256_pd128(_t12_18));

  for( int fi661 = 8; fi661 <= 19; fi661+=4 ) {
    _t13_44 = _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi661));
    _t13_45 = _mm256_maskload_pd(C + 29*fi661 + 28, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t13_46 = _mm256_maskload_pd(C + 29*fi661 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t13_47 = _asm256_loadu_pd(C + 29*fi661 + 84);
    _t13_39 = _mm256_broadcast_sd(L + 28*fi661);
    _t13_38 = _mm256_broadcast_sd(L + 28*fi661 + 1);
    _t13_37 = _mm256_broadcast_sd(L + 28*fi661 + 2);
    _t13_36 = _mm256_broadcast_sd(L + 28*fi661 + 3);
    _t13_35 = _mm256_broadcast_sd(L + 28*fi661 + 28);
    _t13_34 = _mm256_broadcast_sd(L + 28*fi661 + 29);
    _t13_33 = _mm256_broadcast_sd(L + 28*fi661 + 30);
    _t13_32 = _mm256_broadcast_sd(L + 28*fi661 + 31);
    _t13_31 = _mm256_broadcast_sd(L + 28*fi661 + 56);
    _t13_30 = _mm256_broadcast_sd(L + 28*fi661 + 57);
    _t13_29 = _mm256_broadcast_sd(L + 28*fi661 + 58);
    _t13_28 = _mm256_broadcast_sd(L + 28*fi661 + 59);
    _t13_27 = _mm256_broadcast_sd(L + 28*fi661 + 84);
    _t13_26 = _mm256_broadcast_sd(L + 28*fi661 + 85);
    _t13_25 = _mm256_broadcast_sd(L + 28*fi661 + 86);
    _t13_24 = _mm256_broadcast_sd(L + 28*fi661 + 87);
    _t13_23 = _asm256_loadu_pd(C + 28*fi661);
    _t13_22 = _asm256_loadu_pd(C + 28*fi661 + 28);
    _t13_21 = _asm256_loadu_pd(C + 28*fi661 + 56);
    _t13_20 = _asm256_loadu_pd(C + 28*fi661 + 84);
    _t13_19 = _mm256_broadcast_sd(C + 28*fi661);
    _t13_18 = _mm256_broadcast_sd(C + 28*fi661 + 1);
    _t13_17 = _mm256_broadcast_sd(C + 28*fi661 + 2);
    _t13_16 = _mm256_broadcast_sd(C + 28*fi661 + 3);
    _t13_15 = _mm256_broadcast_sd(C + 28*fi661 + 28);
    _t13_14 = _mm256_broadcast_sd(C + 28*fi661 + 29);
    _t13_13 = _mm256_broadcast_sd(C + 28*fi661 + 30);
    _t13_12 = _mm256_broadcast_sd(C + 28*fi661 + 31);
    _t13_11 = _mm256_broadcast_sd(C + 28*fi661 + 56);
    _t13_10 = _mm256_broadcast_sd(C + 28*fi661 + 57);
    _t13_9 = _mm256_broadcast_sd(C + 28*fi661 + 58);
    _t13_8 = _mm256_broadcast_sd(C + 28*fi661 + 59);
    _t13_7 = _mm256_broadcast_sd(C + 28*fi661 + 84);
    _t13_6 = _mm256_broadcast_sd(C + 28*fi661 + 85);
    _t13_5 = _mm256_broadcast_sd(C + 28*fi661 + 86);
    _t13_4 = _mm256_broadcast_sd(C + 28*fi661 + 87);
    _t13_3 = _asm256_loadu_pd(L + 28*fi661);
    _t13_2 = _asm256_loadu_pd(L + 28*fi661 + 28);
    _t13_1 = _asm256_loadu_pd(L + 28*fi661 + 56);
    _t13_0 = _asm256_loadu_pd(L + 28*fi661 + 84);

    // Generating : X[28,28] = ( ( S(h(4, 28, fi661), ( G(h(4, 28, fi661), C[28,28],h(4, 28, fi661)) - ( ( G(h(4, 28, fi661), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi661), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, fi661), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi661), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, fi661)) + Sum_{i217} ( -$(h(4, 28, fi661), ( G(h(4, 28, fi661), L[28,28],h(4, 28, i217)) * T( G(h(4, 28, fi661), X[28,28],h(4, 28, i217)) ) ),h(4, 28, fi661)) ) ) + Sum_{i100} ( -$(h(4, 28, fi661), ( G(h(4, 28, fi661), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi661), L[28,28],h(4, 28, i100)) ) ),h(4, 28, fi661)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t13_60 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_44, _t13_45, 0), _mm256_shuffle_pd(_t13_46, _t13_47, 0), 32);
    _t13_61 = _mm256_permute2f128_pd(_t13_45, _mm256_shuffle_pd(_t13_46, _t13_47, 3), 32);
    _t13_62 = _mm256_blend_pd(_t13_46, _mm256_shuffle_pd(_t13_46, _t13_47, 3), 12);
    _t13_63 = _t13_47;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t13_64 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_23, _t13_22), _mm256_unpacklo_pd(_t13_21, _t13_20), 32);
    _t13_65 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_23, _t13_22), _mm256_unpackhi_pd(_t13_21, _t13_20), 32);
    _t13_66 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_23, _t13_22), _mm256_unpacklo_pd(_t13_21, _t13_20), 49);
    _t13_67 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_23, _t13_22), _mm256_unpackhi_pd(_t13_21, _t13_20), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_39, _t13_64), _mm256_mul_pd(_t13_38, _t13_65)), _mm256_add_pd(_mm256_mul_pd(_t13_37, _t13_66), _mm256_mul_pd(_t13_36, _t13_67)));
    _t13_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_35, _t13_64), _mm256_mul_pd(_t13_34, _t13_65)), _mm256_add_pd(_mm256_mul_pd(_t13_33, _t13_66), _mm256_mul_pd(_t13_32, _t13_67)));
    _t13_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_31, _t13_64), _mm256_mul_pd(_t13_30, _t13_65)), _mm256_add_pd(_mm256_mul_pd(_t13_29, _t13_66), _mm256_mul_pd(_t13_28, _t13_67)));
    _t13_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_27, _t13_64), _mm256_mul_pd(_t13_26, _t13_65)), _mm256_add_pd(_mm256_mul_pd(_t13_25, _t13_66), _mm256_mul_pd(_t13_24, _t13_67)));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t13_68 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 32);
    _t13_69 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 32);
    _t13_70 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 49);
    _t13_71 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_68), _mm256_mul_pd(_t13_18, _t13_69)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_70), _mm256_mul_pd(_t13_16, _t13_71)));
    _t13_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_68), _mm256_mul_pd(_t13_14, _t13_69)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_70), _mm256_mul_pd(_t13_12, _t13_71)));
    _t13_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_68), _mm256_mul_pd(_t13_10, _t13_69)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_70), _mm256_mul_pd(_t13_8, _t13_71)));
    _t13_55 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_68), _mm256_mul_pd(_t13_6, _t13_69)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_70), _mm256_mul_pd(_t13_4, _t13_71)));

    // 4-BLAC: 4x4 + 4x4
    _t13_40 = _mm256_add_pd(_t13_48, _t13_52);
    _t13_41 = _mm256_add_pd(_t13_49, _t13_53);
    _t13_42 = _mm256_add_pd(_t13_50, _t13_54);
    _t13_43 = _mm256_add_pd(_t13_51, _t13_55);

    // 4-BLAC: 4x4 - 4x4
    _t13_56 = _mm256_sub_pd(_t13_60, _t13_40);
    _t13_57 = _mm256_sub_pd(_t13_61, _t13_41);
    _t13_58 = _mm256_sub_pd(_t13_62, _t13_42);
    _t13_59 = _mm256_sub_pd(_t13_63, _t13_43);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t13_44 = _t13_56;
    _t13_45 = _t13_57;
    _t13_46 = _t13_58;
    _t13_47 = _t13_59;

    for( int i217 = 4; i217 <= fi661 - 1; i217+=4 ) {
      _t14_19 = _mm256_broadcast_sd(L + 28*fi661 + i217);
      _t14_18 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 1);
      _t14_17 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 2);
      _t14_16 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 3);
      _t14_15 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 28);
      _t14_14 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 29);
      _t14_13 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 30);
      _t14_12 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 31);
      _t14_11 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 56);
      _t14_10 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 57);
      _t14_9 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 58);
      _t14_8 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 59);
      _t14_7 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 84);
      _t14_6 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 85);
      _t14_5 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 86);
      _t14_4 = _mm256_broadcast_sd(L + 28*fi661 + i217 + 87);
      _t14_3 = _asm256_loadu_pd(C + 28*fi661 + i217);
      _t14_2 = _asm256_loadu_pd(C + 28*fi661 + i217 + 28);
      _t14_1 = _asm256_loadu_pd(C + 28*fi661 + i217 + 56);
      _t14_0 = _asm256_loadu_pd(C + 28*fi661 + i217 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t14_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_3, _t14_2), _mm256_unpacklo_pd(_t14_1, _t14_0), 32);
      _t14_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_3, _t14_2), _mm256_unpackhi_pd(_t14_1, _t14_0), 32);
      _t14_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_3, _t14_2), _mm256_unpacklo_pd(_t14_1, _t14_0), 49);
      _t14_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_3, _t14_2), _mm256_unpackhi_pd(_t14_1, _t14_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t14_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_19, _t14_28), _mm256_mul_pd(_t14_18, _t14_29)), _mm256_add_pd(_mm256_mul_pd(_t14_17, _t14_30), _mm256_mul_pd(_t14_16, _t14_31)));
      _t14_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t14_28), _mm256_mul_pd(_t14_14, _t14_29)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t14_30), _mm256_mul_pd(_t14_12, _t14_31)));
      _t14_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t14_28), _mm256_mul_pd(_t14_10, _t14_29)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t14_30), _mm256_mul_pd(_t14_8, _t14_31)));
      _t14_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_7, _t14_28), _mm256_mul_pd(_t14_6, _t14_29)), _mm256_add_pd(_mm256_mul_pd(_t14_5, _t14_30), _mm256_mul_pd(_t14_4, _t14_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t14_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_44, _t13_45, 0), _mm256_shuffle_pd(_t13_46, _t13_47, 0), 32);
      _t14_25 = _mm256_permute2f128_pd(_t13_45, _mm256_shuffle_pd(_t13_46, _t13_47, 3), 32);
      _t14_26 = _mm256_blend_pd(_t13_46, _mm256_shuffle_pd(_t13_46, _t13_47, 3), 12);
      _t14_27 = _t13_47;

      // 4-BLAC: 4x4 - 4x4
      _t14_24 = _mm256_sub_pd(_t14_24, _t14_20);
      _t14_25 = _mm256_sub_pd(_t14_25, _t14_21);
      _t14_26 = _mm256_sub_pd(_t14_26, _t14_22);
      _t14_27 = _mm256_sub_pd(_t14_27, _t14_23);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t13_44 = _t14_24;
      _t13_45 = _t14_25;
      _t13_46 = _t14_26;
      _t13_47 = _t14_27;
    }

    for( int i100 = 4; i100 <= fi661 - 1; i100+=4 ) {
      _t15_19 = _mm256_broadcast_sd(C + 28*fi661 + i100);
      _t15_18 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 1);
      _t15_17 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 2);
      _t15_16 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 3);
      _t15_15 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28);
      _t15_14 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 29);
      _t15_13 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 30);
      _t15_12 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 31);
      _t15_11 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 56);
      _t15_10 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 57);
      _t15_9 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 58);
      _t15_8 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 59);
      _t15_7 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 84);
      _t15_6 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 85);
      _t15_5 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 86);
      _t15_4 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 87);
      _t15_3 = _asm256_loadu_pd(L + 28*fi661 + i100);
      _t15_2 = _asm256_loadu_pd(L + 28*fi661 + i100 + 28);
      _t15_1 = _asm256_loadu_pd(L + 28*fi661 + i100 + 56);
      _t15_0 = _asm256_loadu_pd(L + 28*fi661 + i100 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t15_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 32);
      _t15_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 32);
      _t15_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 49);
      _t15_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t15_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_28), _mm256_mul_pd(_t15_18, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_30), _mm256_mul_pd(_t15_16, _t15_31)));
      _t15_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_28), _mm256_mul_pd(_t15_14, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_30), _mm256_mul_pd(_t15_12, _t15_31)));
      _t15_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_28), _mm256_mul_pd(_t15_10, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_30), _mm256_mul_pd(_t15_8, _t15_31)));
      _t15_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_7, _t15_28), _mm256_mul_pd(_t15_6, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_5, _t15_30), _mm256_mul_pd(_t15_4, _t15_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t15_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_44, _t13_45, 0), _mm256_shuffle_pd(_t13_46, _t13_47, 0), 32);
      _t15_25 = _mm256_permute2f128_pd(_t13_45, _mm256_shuffle_pd(_t13_46, _t13_47, 3), 32);
      _t15_26 = _mm256_blend_pd(_t13_46, _mm256_shuffle_pd(_t13_46, _t13_47, 3), 12);
      _t15_27 = _t13_47;

      // 4-BLAC: 4x4 - 4x4
      _t15_24 = _mm256_sub_pd(_t15_24, _t15_20);
      _t15_25 = _mm256_sub_pd(_t15_25, _t15_21);
      _t15_26 = _mm256_sub_pd(_t15_26, _t15_22);
      _t15_27 = _mm256_sub_pd(_t15_27, _t15_23);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t13_44 = _t15_24;
      _t13_45 = _t15_25;
      _t13_46 = _t15_26;
      _t13_47 = _t15_27;
    }
    _t16_12 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661])));
    _t16_11 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 28)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 56))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 84)), 32);
    _t16_10 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 29])));
    _t16_9 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 57)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 85)), 0);
    _t16_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 58])));
    _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 86])));
    _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 87])));
    _t16_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 28])));
    _t16_4 = _mm256_broadcast_sd(&(L[29*fi661 + 28]));
    _t16_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 56)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 84)), 0);
    _t16_2 = _mm256_maskload_pd(L + 29*fi661 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t16_1 = _mm256_maskload_pd(L + 29*fi661 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t16_0 = _mm256_maskload_pd(L + 29*fi661 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : X[28,28] = S(h(1, 28, fi661), ( G(h(1, 28, fi661), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_25 = _t13_44;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t16_26 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_27 = _t16_12;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_28 = _mm256_mul_pd(_t16_26, _t16_27);

    // 4-BLAC: 1x4 / 1x4
    _t16_29 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_25), _mm256_castpd256_pd128(_t16_28)));

    // AVX Storer:
    _t13_44 = _t16_29;

    // Generating : X[28,28] = S(h(3, 28, fi661 + 1), ( G(h(3, 28, fi661 + 1), X[28,28],h(1, 28, fi661)) - ( G(h(3, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) Kro G(h(1, 28, fi661), X[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_45, _t13_46), _mm256_unpacklo_pd(_t13_47, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_31 = _t16_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_32 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t13_44, _t13_44, 32), _mm256_permute2f128_pd(_t13_44, _t13_44, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_33 = _mm256_mul_pd(_t16_31, _t16_32);

    // 4-BLAC: 4x1 - 4x1
    _t16_34 = _mm256_sub_pd(_t16_30, _t16_33);

    // AVX Storer:
    _t16_13 = _t16_34;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 1), ( G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_35 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_36 = _t16_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_37 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_38 = _mm256_add_pd(_t16_36, _t16_37);

    // 4-BLAC: 1x4 / 1x4
    _t16_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_35), _mm256_castpd256_pd128(_t16_38)));

    // AVX Storer:
    _t16_14 = _t16_39;

    // Generating : X[28,28] = S(h(2, 28, fi661 + 2), ( G(h(2, 28, fi661 + 2), X[28,28],h(1, 28, fi661)) - ( G(h(2, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 1)) Kro G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_40 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_13, 2), _mm256_permute2f128_pd(_t16_13, _t16_13, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_41 = _t16_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_14, _t16_14, 32), _mm256_permute2f128_pd(_t16_14, _t16_14, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_43 = _mm256_mul_pd(_t16_41, _t16_42);

    // 4-BLAC: 4x1 - 4x1
    _t16_44 = _mm256_sub_pd(_t16_40, _t16_43);

    // AVX Storer:
    _t16_15 = _t16_44;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 2), ( G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_45 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_46 = _t16_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_47 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_48 = _mm256_add_pd(_t16_46, _t16_47);

    // 4-BLAC: 1x4 / 1x4
    _t16_49 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_45), _mm256_castpd256_pd128(_t16_48)));

    // AVX Storer:
    _t16_16 = _t16_49;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661)) - ( G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 2)) Kro G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_50 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_15, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_51 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_52 = _t16_16;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_53 = _mm256_mul_pd(_t16_51, _t16_52);

    // 4-BLAC: 1x4 - 1x4
    _t16_54 = _mm256_sub_pd(_t16_50, _t16_53);

    // AVX Storer:
    _t16_17 = _t16_54;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_55 = _t16_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_56 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_57 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_58 = _mm256_add_pd(_t16_56, _t16_57);

    // 4-BLAC: 1x4 / 1x4
    _t16_59 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_55), _mm256_castpd256_pd128(_t16_58)));

    // AVX Storer:
    _t16_17 = _t16_59;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 1), ( G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661 + 1)) - ( ( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661)) ) ) + ( G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_60 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t13_45, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_61 = _t16_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_62 = _t16_14;

    // 4-BLAC: (4x1)^T
    _t16_63 = _t16_62;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_64 = _mm256_mul_pd(_t16_61, _t16_63);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_65 = _t16_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_66 = _t16_5;

    // 4-BLAC: (4x1)^T
    _t16_67 = _t16_66;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_68 = _mm256_mul_pd(_t16_65, _t16_67);

    // 4-BLAC: 1x4 + 1x4
    _t16_69 = _mm256_add_pd(_t16_64, _t16_68);

    // 4-BLAC: 1x4 - 1x4
    _t16_70 = _mm256_sub_pd(_t16_60, _t16_69);

    // AVX Storer:
    _t16_18 = _t16_70;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 1), ( G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_71 = _t16_18;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t16_72 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_73 = _t16_10;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_74 = _mm256_mul_pd(_t16_72, _t16_73);

    // 4-BLAC: 1x4 / 1x4
    _t16_75 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_71), _mm256_castpd256_pd128(_t16_74)));

    // AVX Storer:
    _t16_18 = _t16_75;

    // Generating : X[28,28] = S(h(2, 28, fi661 + 2), ( G(h(2, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(2, 28, fi661 + 2), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_76 = _mm256_unpackhi_pd(_mm256_blend_pd(_t13_46, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t13_47, _mm256_setzero_pd(), 12));

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_77 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_16, _t16_17), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_78 = _t16_4;

    // 4-BLAC: (4x1)^T
    _t16_79 = _t16_78;

    // 4-BLAC: 4x1 Kro 1x4
    _t16_80 = _mm256_mul_pd(_t16_77, _t16_79);

    // 4-BLAC: 4x1 - 4x1
    _t16_81 = _mm256_sub_pd(_t16_76, _t16_80);

    // AVX Storer:
    _t16_19 = _t16_81;

    // Generating : X[28,28] = S(h(2, 28, fi661 + 2), ( G(h(2, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(2, 28, fi661 + 2), L[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_82 = _t16_19;

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_83 = _t16_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_84 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_14, _t16_14, 32), _mm256_permute2f128_pd(_t16_14, _t16_14, 32), 0);

    // 4-BLAC: (4x1)^T
    _t16_85 = _t16_84;

    // 4-BLAC: 4x1 Kro 1x4
    _t16_86 = _mm256_mul_pd(_t16_83, _t16_85);

    // 4-BLAC: 4x1 - 4x1
    _t16_87 = _mm256_sub_pd(_t16_82, _t16_86);

    // AVX Storer:
    _t16_19 = _t16_87;

    // Generating : X[28,28] = S(h(2, 28, fi661 + 2), ( G(h(2, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(2, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 1)) Kro G(h(1, 28, fi661 + 1), X[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_88 = _t16_19;

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_89 = _t16_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_90 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_18, _t16_18, 32), _mm256_permute2f128_pd(_t16_18, _t16_18, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_91 = _mm256_mul_pd(_t16_89, _t16_90);

    // 4-BLAC: 4x1 - 4x1
    _t16_92 = _mm256_sub_pd(_t16_88, _t16_91);

    // AVX Storer:
    _t16_19 = _t16_92;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 2), ( G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_94 = _t16_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_95 = _t16_10;

    // 4-BLAC: 1x4 + 1x4
    _t16_96 = _mm256_add_pd(_t16_94, _t16_95);

    // 4-BLAC: 1x4 / 1x4
    _t16_97 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_93), _mm256_castpd256_pd128(_t16_96)));

    // AVX Storer:
    _t16_20 = _t16_97;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 2)) Kro G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_98 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_19, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_99 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_100 = _t16_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_101 = _mm256_mul_pd(_t16_99, _t16_100);

    // 4-BLAC: 1x4 - 1x4
    _t16_102 = _mm256_sub_pd(_t16_98, _t16_101);

    // AVX Storer:
    _t16_21 = _t16_102;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_103 = _t16_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_104 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_105 = _t16_10;

    // 4-BLAC: 1x4 + 1x4
    _t16_106 = _mm256_add_pd(_t16_104, _t16_105);

    // 4-BLAC: 1x4 / 1x4
    _t16_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_103), _mm256_castpd256_pd128(_t16_106)));

    // AVX Storer:
    _t16_21 = _t16_107;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 2), ( G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 2)) - ( ( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), X[28,28],h(2, 28, fi661)) ) ) + ( G(h(1, 28, fi661 + 2), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_108 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t13_46, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t13_46, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_109 = _t16_2;

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_110 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_setzero_pd(), 12);

    // 4-BLAC: (1x4)^T
    _t16_111 = _t16_110;

    // 4-BLAC: 1x4 * 4x1
    _t16_112 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_109, _t16_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_109, _t16_111), _mm256_mul_pd(_t16_109, _t16_111), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_109, _t16_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_109, _t16_111), _mm256_mul_pd(_t16_109, _t16_111), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_109, _t16_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_109, _t16_111), _mm256_mul_pd(_t16_109, _t16_111), 129)), 1));

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_113 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_114 = _t16_2;

    // 4-BLAC: (1x4)^T
    _t16_115 = _t16_114;

    // 4-BLAC: 1x4 * 4x1
    _t16_116 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_113, _t16_115), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_113, _t16_115), _mm256_mul_pd(_t16_113, _t16_115), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_113, _t16_115), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_113, _t16_115), _mm256_mul_pd(_t16_113, _t16_115), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_113, _t16_115), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_113, _t16_115), _mm256_mul_pd(_t16_113, _t16_115), 129)), 1));

    // 4-BLAC: 1x4 + 1x4
    _t16_117 = _mm256_add_pd(_t16_112, _t16_116);

    // 4-BLAC: 1x4 - 1x4
    _t16_118 = _mm256_sub_pd(_t16_108, _t16_117);

    // AVX Storer:
    _t16_22 = _t16_118;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 2), ( G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_119 = _t16_22;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t16_120 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_121 = _t16_8;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_122 = _mm256_mul_pd(_t16_120, _t16_121);

    // 4-BLAC: 1x4 / 1x4
    _t16_123 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_119), _mm256_castpd256_pd128(_t16_122)));

    // AVX Storer:
    _t16_22 = _t16_123;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + 3), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_124 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t13_47, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t13_47, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_125 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_126 = _t16_2;

    // 4-BLAC: (1x4)^T
    _t16_127 = _t16_126;

    // 4-BLAC: 1x4 * 4x1
    _t16_128 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_125, _t16_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_125, _t16_127), _mm256_mul_pd(_t16_125, _t16_127), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_125, _t16_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_125, _t16_127), _mm256_mul_pd(_t16_125, _t16_127), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_125, _t16_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_125, _t16_127), _mm256_mul_pd(_t16_125, _t16_127), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_129 = _mm256_sub_pd(_t16_124, _t16_128);

    // AVX Storer:
    _t16_23 = _t16_129;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + 3), L[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), X[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_130 = _t16_23;

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_131 = _t16_1;

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_132 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_setzero_pd(), 12);

    // 4-BLAC: (1x4)^T
    _t16_133 = _t16_132;

    // 4-BLAC: 1x4 * 4x1
    _t16_134 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_mul_pd(_t16_131, _t16_133), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_mul_pd(_t16_131, _t16_133), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_mul_pd(_t16_131, _t16_133), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_135 = _mm256_sub_pd(_t16_130, _t16_134);

    // AVX Storer:
    _t16_23 = _t16_135;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 2)) Kro G(h(1, 28, fi661 + 2), X[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_136 = _t16_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_137 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_138 = _t16_22;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_139 = _mm256_mul_pd(_t16_137, _t16_138);

    // 4-BLAC: 1x4 - 1x4
    _t16_140 = _mm256_sub_pd(_t16_136, _t16_139);

    // AVX Storer:
    _t16_23 = _t16_140;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_141 = _t16_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_142 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_143 = _t16_8;

    // 4-BLAC: 1x4 + 1x4
    _t16_144 = _mm256_add_pd(_t16_142, _t16_143);

    // 4-BLAC: 1x4 / 1x4
    _t16_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_141), _mm256_castpd256_pd128(_t16_144)));

    // AVX Storer:
    _t16_23 = _t16_145;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 3)) - ( ( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), X[28,28],h(3, 28, fi661)) ) ) + ( G(h(1, 28, fi661 + 3), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_146 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t13_47, _t13_47, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_147 = _t16_0;

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_148 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _mm256_setzero_pd()), 32);

    // 4-BLAC: (1x4)^T
    _t16_149 = _t16_148;

    // 4-BLAC: 1x4 * 4x1
    _t16_150 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_147, _t16_149), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_147, _t16_149), _mm256_mul_pd(_t16_147, _t16_149), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_147, _t16_149), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_147, _t16_149), _mm256_mul_pd(_t16_147, _t16_149), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_147, _t16_149), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_147, _t16_149), _mm256_mul_pd(_t16_147, _t16_149), 129)), 1));

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_151 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_152 = _t16_0;

    // 4-BLAC: (1x4)^T
    _t16_153 = _t16_152;

    // 4-BLAC: 1x4 * 4x1
    _t16_154 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_151, _t16_153), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_151, _t16_153), _mm256_mul_pd(_t16_151, _t16_153), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_151, _t16_153), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_151, _t16_153), _mm256_mul_pd(_t16_151, _t16_153), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_151, _t16_153), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_151, _t16_153), _mm256_mul_pd(_t16_151, _t16_153), 129)), 1));

    // 4-BLAC: 1x4 + 1x4
    _t16_155 = _mm256_add_pd(_t16_150, _t16_154);

    // 4-BLAC: 1x4 - 1x4
    _t16_156 = _mm256_sub_pd(_t16_146, _t16_155);

    // AVX Storer:
    _t16_24 = _t16_156;

    // Generating : X[28,28] = S(h(1, 28, fi661 + 3), ( G(h(1, 28, fi661 + 3), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_157 = _t16_24;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t16_158 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_159 = _t16_6;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_160 = _mm256_mul_pd(_t16_158, _t16_159);

    // 4-BLAC: 1x4 / 1x4
    _t16_161 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_157), _mm256_castpd256_pd128(_t16_160)));

    // AVX Storer:
    _t16_24 = _t16_161;

    // Generating : X[28,28] = ( Sum_{i217} ( S(h(4, 28, fi661 + i217 + 4), ( G(h(4, 28, fi661 + i217 + 4), C[28,28],h(4, 28, fi661)) - ( G(h(4, 28, fi661 + i217 + 4), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi661), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, fi661)) ) + Sum_{i100} ( Sum_{i217} ( -$(h(4, 28, fi661 + i217 + 4), ( G(h(4, 28, fi661 + i217 + 4), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi661), L[28,28],h(4, 28, i100)) ) ),h(4, 28, fi661)) ) ) )

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t16_162 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 32);
    _t16_163 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 32);
    _t16_164 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 49);
    _t16_165 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 49);

    for( int i217 = 0; i217 <= -fi661 + 23; i217+=4 ) {
      _t17_20 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 112);
      _t17_21 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 140);
      _t17_22 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 168);
      _t17_23 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 196);
      _t17_15 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 112);
      _t17_14 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 113);
      _t17_13 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 114);
      _t17_12 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 115);
      _t17_11 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 140);
      _t17_10 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 141);
      _t17_9 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 142);
      _t17_8 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 143);
      _t17_7 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 168);
      _t17_6 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 169);
      _t17_5 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 170);
      _t17_4 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 171);
      _t17_3 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 196);
      _t17_2 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 197);
      _t17_1 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 198);
      _t17_0 = _mm256_broadcast_sd(C + 28*fi661 + 28*i217 + 199);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_162 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 32);
      _t16_163 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 32);
      _t16_164 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 49);
      _t16_165 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t17_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_15, _t16_162), _mm256_mul_pd(_t17_14, _t16_163)), _mm256_add_pd(_mm256_mul_pd(_t17_13, _t16_164), _mm256_mul_pd(_t17_12, _t16_165)));
      _t17_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_11, _t16_162), _mm256_mul_pd(_t17_10, _t16_163)), _mm256_add_pd(_mm256_mul_pd(_t17_9, _t16_164), _mm256_mul_pd(_t17_8, _t16_165)));
      _t17_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_7, _t16_162), _mm256_mul_pd(_t17_6, _t16_163)), _mm256_add_pd(_mm256_mul_pd(_t17_5, _t16_164), _mm256_mul_pd(_t17_4, _t16_165)));
      _t17_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_3, _t16_162), _mm256_mul_pd(_t17_2, _t16_163)), _mm256_add_pd(_mm256_mul_pd(_t17_1, _t16_164), _mm256_mul_pd(_t17_0, _t16_165)));

      // 4-BLAC: 4x4 - 4x4
      _t17_20 = _mm256_sub_pd(_t17_20, _t17_16);
      _t17_21 = _mm256_sub_pd(_t17_21, _t17_17);
      _t17_22 = _mm256_sub_pd(_t17_22, _t17_18);
      _t17_23 = _mm256_sub_pd(_t17_23, _t17_19);

      // AVX Storer:
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 112, _t17_20);
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 140, _t17_21);
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 168, _t17_22);
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 196, _t17_23);
    }

    for( int i100 = 4; i100 <= fi661 - 1; i100+=4 ) {

      for( int i217 = 0; i217 <= -fi661 + 23; i217+=4 ) {
        _t18_19 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 112);
        _t18_18 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 113);
        _t18_17 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 114);
        _t18_16 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 115);
        _t18_15 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 140);
        _t18_14 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 141);
        _t18_13 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 142);
        _t18_12 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 143);
        _t18_11 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 168);
        _t18_10 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 169);
        _t18_9 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 170);
        _t18_8 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 171);
        _t18_7 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 196);
        _t18_6 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 197);
        _t18_5 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 198);
        _t18_4 = _mm256_broadcast_sd(C + 28*fi661 + i100 + 28*i217 + 199);
        _t18_3 = _asm256_loadu_pd(L + 28*fi661 + i100);
        _t18_2 = _asm256_loadu_pd(L + 28*fi661 + i100 + 28);
        _t18_1 = _asm256_loadu_pd(L + 28*fi661 + i100 + 56);
        _t18_0 = _asm256_loadu_pd(L + 28*fi661 + i100 + 84);
        _t18_20 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 112);
        _t18_21 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 140);
        _t18_22 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 168);
        _t18_23 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 196);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t18_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
        _t18_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
        _t18_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
        _t18_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t18_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_28), _mm256_mul_pd(_t18_18, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_30), _mm256_mul_pd(_t18_16, _t18_31)));
        _t18_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_28), _mm256_mul_pd(_t18_14, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_30), _mm256_mul_pd(_t18_12, _t18_31)));
        _t18_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_28), _mm256_mul_pd(_t18_10, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_30), _mm256_mul_pd(_t18_8, _t18_31)));
        _t18_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_28), _mm256_mul_pd(_t18_6, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_30), _mm256_mul_pd(_t18_4, _t18_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t18_20 = _mm256_sub_pd(_t18_20, _t18_24);
        _t18_21 = _mm256_sub_pd(_t18_21, _t18_25);
        _t18_22 = _mm256_sub_pd(_t18_22, _t18_26);
        _t18_23 = _mm256_sub_pd(_t18_23, _t18_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 112, _t18_20);
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 140, _t18_21);
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 168, _t18_22);
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 196, _t18_23);
      }
    }

    // Generating : X[28,28] = ( Sum_{i217} ( S(h(4, 28, fi661 + i217 + 4), ( G(h(4, 28, fi661 + i217 + 4), X[28,28],h(4, 28, fi661)) - ( G(h(4, 28, fi661 + i217 + 4), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi661), X[28,28],h(4, 28, 0)) ) ) ),h(4, 28, fi661)) ) + Sum_{i100} ( Sum_{i217} ( -$(h(4, 28, fi661 + i217 + 4), ( G(h(4, 28, fi661 + i217 + 4), L[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi661), X[28,28],h(4, 28, i100)) ) ),h(4, 28, fi661)) ) ) )

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t19_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_23, _t13_22), _mm256_unpacklo_pd(_t13_21, _t13_20), 32);
    _t19_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_23, _t13_22), _mm256_unpackhi_pd(_t13_21, _t13_20), 32);
    _t19_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_23, _t13_22), _mm256_unpacklo_pd(_t13_21, _t13_20), 49);
    _t19_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_23, _t13_22), _mm256_unpackhi_pd(_t13_21, _t13_20), 49);

    for( int i217 = 0; i217 <= -fi661 + 23; i217+=4 ) {
      _t20_20 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 112);
      _t20_21 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 140);
      _t20_22 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 168);
      _t20_23 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 196);
      _t20_15 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 112);
      _t20_14 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 113);
      _t20_13 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 114);
      _t20_12 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 115);
      _t20_11 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 140);
      _t20_10 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 141);
      _t20_9 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 142);
      _t20_8 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 143);
      _t20_7 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 168);
      _t20_6 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 169);
      _t20_5 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 170);
      _t20_4 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 171);
      _t20_3 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 196);
      _t20_2 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 197);
      _t20_1 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 198);
      _t20_0 = _mm256_broadcast_sd(L + 28*fi661 + 28*i217 + 199);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t19_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_23, _t13_22), _mm256_unpacklo_pd(_t13_21, _t13_20), 32);
      _t19_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_23, _t13_22), _mm256_unpackhi_pd(_t13_21, _t13_20), 32);
      _t19_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_23, _t13_22), _mm256_unpacklo_pd(_t13_21, _t13_20), 49);
      _t19_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_23, _t13_22), _mm256_unpackhi_pd(_t13_21, _t13_20), 49);

      // 4-BLAC: 4x4 * 4x4
      _t20_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t19_0), _mm256_mul_pd(_t20_14, _t19_1)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t19_2), _mm256_mul_pd(_t20_12, _t19_3)));
      _t20_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t19_0), _mm256_mul_pd(_t20_10, _t19_1)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t19_2), _mm256_mul_pd(_t20_8, _t19_3)));
      _t20_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t19_0), _mm256_mul_pd(_t20_6, _t19_1)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t19_2), _mm256_mul_pd(_t20_4, _t19_3)));
      _t20_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t19_0), _mm256_mul_pd(_t20_2, _t19_1)), _mm256_add_pd(_mm256_mul_pd(_t20_1, _t19_2), _mm256_mul_pd(_t20_0, _t19_3)));

      // 4-BLAC: 4x4 - 4x4
      _t20_20 = _mm256_sub_pd(_t20_20, _t20_16);
      _t20_21 = _mm256_sub_pd(_t20_21, _t20_17);
      _t20_22 = _mm256_sub_pd(_t20_22, _t20_18);
      _t20_23 = _mm256_sub_pd(_t20_23, _t20_19);

      // AVX Storer:
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 112, _t20_20);
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 140, _t20_21);
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 168, _t20_22);
      _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 196, _t20_23);
    }

    for( int i100 = 4; i100 <= fi661 - 1; i100+=4 ) {

      for( int i217 = 0; i217 <= -fi661 + 23; i217+=4 ) {
        _t21_19 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 112);
        _t21_18 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 113);
        _t21_17 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 114);
        _t21_16 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 115);
        _t21_15 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 140);
        _t21_14 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 141);
        _t21_13 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 142);
        _t21_12 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 143);
        _t21_11 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 168);
        _t21_10 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 169);
        _t21_9 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 170);
        _t21_8 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 171);
        _t21_7 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 196);
        _t21_6 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 197);
        _t21_5 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 198);
        _t21_4 = _mm256_broadcast_sd(L + 28*fi661 + i100 + 28*i217 + 199);
        _t21_3 = _asm256_loadu_pd(C + 28*fi661 + i100);
        _t21_2 = _asm256_loadu_pd(C + 28*fi661 + i100 + 28);
        _t21_1 = _asm256_loadu_pd(C + 28*fi661 + i100 + 56);
        _t21_0 = _asm256_loadu_pd(C + 28*fi661 + i100 + 84);
        _t21_20 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 112);
        _t21_21 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 140);
        _t21_22 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 168);
        _t21_23 = _asm256_loadu_pd(C + 29*fi661 + 28*i217 + 196);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t21_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_3, _t21_2), _mm256_unpacklo_pd(_t21_1, _t21_0), 32);
        _t21_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t21_3, _t21_2), _mm256_unpackhi_pd(_t21_1, _t21_0), 32);
        _t21_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_3, _t21_2), _mm256_unpacklo_pd(_t21_1, _t21_0), 49);
        _t21_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t21_3, _t21_2), _mm256_unpackhi_pd(_t21_1, _t21_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t21_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_19, _t21_28), _mm256_mul_pd(_t21_18, _t21_29)), _mm256_add_pd(_mm256_mul_pd(_t21_17, _t21_30), _mm256_mul_pd(_t21_16, _t21_31)));
        _t21_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_15, _t21_28), _mm256_mul_pd(_t21_14, _t21_29)), _mm256_add_pd(_mm256_mul_pd(_t21_13, _t21_30), _mm256_mul_pd(_t21_12, _t21_31)));
        _t21_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_11, _t21_28), _mm256_mul_pd(_t21_10, _t21_29)), _mm256_add_pd(_mm256_mul_pd(_t21_9, _t21_30), _mm256_mul_pd(_t21_8, _t21_31)));
        _t21_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_7, _t21_28), _mm256_mul_pd(_t21_6, _t21_29)), _mm256_add_pd(_mm256_mul_pd(_t21_5, _t21_30), _mm256_mul_pd(_t21_4, _t21_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t21_20 = _mm256_sub_pd(_t21_20, _t21_24);
        _t21_21 = _mm256_sub_pd(_t21_21, _t21_25);
        _t21_22 = _mm256_sub_pd(_t21_22, _t21_26);
        _t21_23 = _mm256_sub_pd(_t21_23, _t21_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 112, _t21_20);
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 140, _t21_21);
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 168, _t21_22);
        _asm256_storeu_pd(C + 29*fi661 + 28*i217 + 196, _t21_23);
      }
    }

    // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, fi661 + i100 + 4), ( G(h(4, 28, fi661 + i100 + 4), X[28,28],h(4, 28, fi661)) - ( G(h(4, 28, fi661 + i100 + 4), L[28,28],h(4, 28, fi661)) * G(h(4, 28, fi661), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661)) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t22_0 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_44, _mm256_blend_pd(_mm256_unpacklo_pd(_t16_14, _t16_18), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32), 0), 32);
    _t22_1 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t16_14, _t16_18), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32), 3), 32);
    _t22_2 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32), 3), 12);
    _t22_3 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32);
    _mm256_maskstore_pd(C + 29*fi661 + 28, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t13_45);
    _mm256_maskstore_pd(C + 29*fi661 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t13_46);
    _asm256_storeu_pd(C + 29*fi661 + 84, _t13_47);
    _mm256_maskstore_pd(C + 29*fi661 + 28, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t16_13);
    _mm256_maskstore_pd(C + 29*fi661 + 56, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t16_13, _t16_13, 1));
    _mm256_maskstore_pd(C + 29*fi661 + 84, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t16_13, _t16_13, 129));
    _mm256_maskstore_pd(C + 29*fi661 + 56, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t16_15);
    _mm256_maskstore_pd(C + 29*fi661 + 84, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t16_15, _t16_15, 1));
    _mm256_maskstore_pd(C + 29*fi661 + 57, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t16_19);
    _mm256_maskstore_pd(C + 29*fi661 + 85, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t16_19, _t16_19, 1));

    for( int i100 = 0; i100 <= -fi661 + 23; i100+=4 ) {
      _t23_20 = _asm256_loadu_pd(C + 29*fi661 + 28*i100 + 112);
      _t23_21 = _asm256_loadu_pd(C + 29*fi661 + 28*i100 + 140);
      _t23_22 = _asm256_loadu_pd(C + 29*fi661 + 28*i100 + 168);
      _t23_23 = _asm256_loadu_pd(C + 29*fi661 + 28*i100 + 196);
      _t23_15 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 112);
      _t23_14 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 113);
      _t23_13 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 114);
      _t23_12 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 115);
      _t23_11 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 140);
      _t23_10 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 141);
      _t23_9 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 142);
      _t23_8 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 143);
      _t23_7 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 168);
      _t23_6 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 169);
      _t23_5 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 170);
      _t23_4 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 171);
      _t23_3 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 196);
      _t23_2 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 197);
      _t23_1 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 198);
      _t23_0 = _mm256_broadcast_sd(L + 29*fi661 + 28*i100 + 199);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t23_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_44, _mm256_blend_pd(_mm256_unpacklo_pd(_t16_14, _t16_18), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32), 0), 32);
      _t23_25 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t16_14, _t16_18), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32), 3), 32);
      _t23_26 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_16, _t16_20), _mm256_unpacklo_pd(_t16_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32), 3), 12);
      _t23_27 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_17, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32);

      // 4-BLAC: 4x4 * 4x4
      _t23_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_15, _t23_24), _mm256_mul_pd(_t23_14, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_13, _t23_26), _mm256_mul_pd(_t23_12, _t23_27)));
      _t23_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_11, _t23_24), _mm256_mul_pd(_t23_10, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_9, _t23_26), _mm256_mul_pd(_t23_8, _t23_27)));
      _t23_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_7, _t23_24), _mm256_mul_pd(_t23_6, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_5, _t23_26), _mm256_mul_pd(_t23_4, _t23_27)));
      _t23_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_3, _t23_24), _mm256_mul_pd(_t23_2, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_1, _t23_26), _mm256_mul_pd(_t23_0, _t23_27)));

      // 4-BLAC: 4x4 - 4x4
      _t23_20 = _mm256_sub_pd(_t23_20, _t23_16);
      _t23_21 = _mm256_sub_pd(_t23_21, _t23_17);
      _t23_22 = _mm256_sub_pd(_t23_22, _t23_18);
      _t23_23 = _mm256_sub_pd(_t23_23, _t23_19);

      // AVX Storer:
      _asm256_storeu_pd(C + 29*fi661 + 28*i100 + 112, _t23_20);
      _asm256_storeu_pd(C + 29*fi661 + 28*i100 + 140, _t23_21);
      _asm256_storeu_pd(C + 29*fi661 + 28*i100 + 168, _t23_22);
      _asm256_storeu_pd(C + 29*fi661 + 28*i100 + 196, _t23_23);
    }

    for( int fi1034 = 0; fi1034 <= -fi661 + 19; fi1034+=4 ) {
      _t24_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 112])));
      _t24_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 29*fi661 + 116])));
      _t24_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 113])));
      _t24_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 114])));
      _t24_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 115])));
      _t24_11 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 140);
      _t24_12 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 168);
      _t24_13 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 196);
      _t24_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 29*fi661 + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 29*fi661 + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 29*fi661 + 200)), 32);
      _t24_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 29*fi661 + 145])));
      _t24_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 29*fi661 + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1034 + 29*fi661 + 201)), 0);
      _t24_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 29*fi661 + 174])));
      _t24_1 = _mm256_broadcast_sd(&(L[29*fi1034 + 29*fi661 + 202]));
      _t24_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1034 + 29*fi661 + 203])));

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi1034 + fi661 + 4), L[28,28],h(1, 28, fi1034 + fi661 + 4)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_27 = _t24_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_28 = _t24_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_29 = _t16_12;

      // 4-BLAC: 1x4 + 1x4
      _t24_30 = _mm256_add_pd(_t24_28, _t24_29);

      // 4-BLAC: 1x4 / 1x4
      _t24_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_27), _mm256_castpd256_pd128(_t24_30)));

      // AVX Storer:
      _t24_7 = _t24_31;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_32 = _t24_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_33 = _t24_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_34 = _t16_5;

      // 4-BLAC: (4x1)^T
      _t24_35 = _t24_34;

      // 4-BLAC: 1x4 Kro 1x4
      _t24_36 = _mm256_mul_pd(_t24_33, _t24_35);

      // 4-BLAC: 1x4 - 1x4
      _t24_37 = _mm256_sub_pd(_t24_32, _t24_36);

      // AVX Storer:
      _t24_8 = _t24_37;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi1034 + fi661 + 4), L[28,28],h(1, 28, fi1034 + fi661 + 4)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_38 = _t24_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_39 = _t24_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_40 = _t16_10;

      // 4-BLAC: 1x4 + 1x4
      _t24_41 = _mm256_add_pd(_t24_39, _t24_40);

      // 4-BLAC: 1x4 / 1x4
      _t24_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_38), _mm256_castpd256_pd128(_t24_41)));

      // AVX Storer:
      _t24_8 = _t24_42;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_43 = _t24_9;

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_44 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_7, _t24_8), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_45 = _t16_2;

      // 4-BLAC: (1x4)^T
      _t24_46 = _t24_45;

      // 4-BLAC: 1x4 * 4x1
      _t24_47 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_44, _t24_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_44, _t24_46), _mm256_mul_pd(_t24_44, _t24_46), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_44, _t24_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_44, _t24_46), _mm256_mul_pd(_t24_44, _t24_46), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_44, _t24_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_44, _t24_46), _mm256_mul_pd(_t24_44, _t24_46), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_48 = _mm256_sub_pd(_t24_43, _t24_47);

      // AVX Storer:
      _t24_9 = _t24_48;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi1034 + fi661 + 4), L[28,28],h(1, 28, fi1034 + fi661 + 4)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_49 = _t24_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_50 = _t24_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_51 = _t16_8;

      // 4-BLAC: 1x4 + 1x4
      _t24_52 = _mm256_add_pd(_t24_50, _t24_51);

      // 4-BLAC: 1x4 / 1x4
      _t24_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_49), _mm256_castpd256_pd128(_t24_52)));

      // AVX Storer:
      _t24_9 = _t24_53;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_54 = _t24_10;

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_55 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_7, _t24_8), _mm256_unpacklo_pd(_t24_9, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_56 = _t16_0;

      // 4-BLAC: (1x4)^T
      _t24_57 = _t24_56;

      // 4-BLAC: 1x4 * 4x1
      _t24_58 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_55, _t24_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_55, _t24_57), _mm256_mul_pd(_t24_55, _t24_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_55, _t24_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_55, _t24_57), _mm256_mul_pd(_t24_55, _t24_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_55, _t24_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_55, _t24_57), _mm256_mul_pd(_t24_55, _t24_57), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_59 = _mm256_sub_pd(_t24_54, _t24_58);

      // AVX Storer:
      _t24_10 = _t24_59;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 4), ( G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi1034 + fi661 + 4), L[28,28],h(1, 28, fi1034 + fi661 + 4)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_60 = _t24_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_61 = _t24_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_62 = _t16_6;

      // 4-BLAC: 1x4 + 1x4
      _t24_63 = _mm256_add_pd(_t24_61, _t24_62);

      // 4-BLAC: 1x4 / 1x4
      _t24_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_60), _mm256_castpd256_pd128(_t24_63)));

      // AVX Storer:
      _t24_10 = _t24_64;

      // Generating : X[28,28] = S(h(3, 28, fi1034 + fi661 + 5), ( G(h(3, 28, fi1034 + fi661 + 5), X[28,28],h(4, 28, fi661)) - ( G(h(3, 28, fi1034 + fi661 + 5), L[28,28],h(1, 28, fi1034 + fi661 + 4)) * G(h(1, 28, fi1034 + fi661 + 4), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661))

      // AVX Loader:

      // 3x4 -> 4x4
      _t24_65 = _t24_11;
      _t24_66 = _t24_12;
      _t24_67 = _t24_13;
      _t24_68 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t24_69 = _t24_5;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t24_70 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_69, _t24_69, 32), _mm256_permute2f128_pd(_t24_69, _t24_69, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_7, _t24_8), _mm256_unpacklo_pd(_t24_9, _t24_10), 32));
      _t24_71 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_69, _t24_69, 32), _mm256_permute2f128_pd(_t24_69, _t24_69, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_7, _t24_8), _mm256_unpacklo_pd(_t24_9, _t24_10), 32));
      _t24_72 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_69, _t24_69, 49), _mm256_permute2f128_pd(_t24_69, _t24_69, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_7, _t24_8), _mm256_unpacklo_pd(_t24_9, _t24_10), 32));
      _t24_73 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_69, _t24_69, 49), _mm256_permute2f128_pd(_t24_69, _t24_69, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_7, _t24_8), _mm256_unpacklo_pd(_t24_9, _t24_10), 32));

      // 4-BLAC: 4x4 - 4x4
      _t24_74 = _mm256_sub_pd(_t24_65, _t24_70);
      _t24_75 = _mm256_sub_pd(_t24_66, _t24_71);
      _t24_76 = _mm256_sub_pd(_t24_67, _t24_72);
      _t24_77 = _mm256_sub_pd(_t24_68, _t24_73);

      // AVX Storer:
      _t24_11 = _t24_74;
      _t24_12 = _t24_75;
      _t24_13 = _t24_76;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi1034 + fi661 + 5), L[28,28],h(1, 28, fi1034 + fi661 + 5)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_78 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_11, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_79 = _t24_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_80 = _t16_12;

      // 4-BLAC: 1x4 + 1x4
      _t24_81 = _mm256_add_pd(_t24_79, _t24_80);

      // 4-BLAC: 1x4 / 1x4
      _t24_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_78), _mm256_castpd256_pd128(_t24_81)));

      // AVX Storer:
      _t24_14 = _t24_82;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_83 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_11, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_84 = _t24_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_85 = _t16_5;

      // 4-BLAC: (4x1)^T
      _t24_86 = _t24_85;

      // 4-BLAC: 1x4 Kro 1x4
      _t24_87 = _mm256_mul_pd(_t24_84, _t24_86);

      // 4-BLAC: 1x4 - 1x4
      _t24_88 = _mm256_sub_pd(_t24_83, _t24_87);

      // AVX Storer:
      _t24_15 = _t24_88;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi1034 + fi661 + 5), L[28,28],h(1, 28, fi1034 + fi661 + 5)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_89 = _t24_15;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_90 = _t24_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_91 = _t16_10;

      // 4-BLAC: 1x4 + 1x4
      _t24_92 = _mm256_add_pd(_t24_90, _t24_91);

      // 4-BLAC: 1x4 / 1x4
      _t24_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_89), _mm256_castpd256_pd128(_t24_92)));

      // AVX Storer:
      _t24_15 = _t24_93;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_94 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t24_11, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_95 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_14, _t24_15), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_96 = _t16_2;

      // 4-BLAC: (1x4)^T
      _t24_97 = _t24_96;

      // 4-BLAC: 1x4 * 4x1
      _t24_98 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_95, _t24_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_95, _t24_97), _mm256_mul_pd(_t24_95, _t24_97), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_95, _t24_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_95, _t24_97), _mm256_mul_pd(_t24_95, _t24_97), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_95, _t24_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_95, _t24_97), _mm256_mul_pd(_t24_95, _t24_97), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_99 = _mm256_sub_pd(_t24_94, _t24_98);

      // AVX Storer:
      _t24_16 = _t24_99;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi1034 + fi661 + 5), L[28,28],h(1, 28, fi1034 + fi661 + 5)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_100 = _t24_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_101 = _t24_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_102 = _t16_8;

      // 4-BLAC: 1x4 + 1x4
      _t24_103 = _mm256_add_pd(_t24_101, _t24_102);

      // 4-BLAC: 1x4 / 1x4
      _t24_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_100), _mm256_castpd256_pd128(_t24_103)));

      // AVX Storer:
      _t24_16 = _t24_104;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t24_11, _t24_11, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_14, _t24_15), _mm256_unpacklo_pd(_t24_16, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_107 = _t16_0;

      // 4-BLAC: (1x4)^T
      _t24_108 = _t24_107;

      // 4-BLAC: 1x4 * 4x1
      _t24_109 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_106, _t24_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_106, _t24_108), _mm256_mul_pd(_t24_106, _t24_108), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_106, _t24_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_106, _t24_108), _mm256_mul_pd(_t24_106, _t24_108), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_106, _t24_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_106, _t24_108), _mm256_mul_pd(_t24_106, _t24_108), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_110 = _mm256_sub_pd(_t24_105, _t24_109);

      // AVX Storer:
      _t24_17 = _t24_110;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 5), ( G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi1034 + fi661 + 5), L[28,28],h(1, 28, fi1034 + fi661 + 5)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_111 = _t24_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_112 = _t24_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_113 = _t16_6;

      // 4-BLAC: 1x4 + 1x4
      _t24_114 = _mm256_add_pd(_t24_112, _t24_113);

      // 4-BLAC: 1x4 / 1x4
      _t24_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_111), _mm256_castpd256_pd128(_t24_114)));

      // AVX Storer:
      _t24_17 = _t24_115;

      // Generating : X[28,28] = S(h(2, 28, fi1034 + fi661 + 6), ( G(h(2, 28, fi1034 + fi661 + 6), X[28,28],h(4, 28, fi661)) - ( G(h(2, 28, fi1034 + fi661 + 6), L[28,28],h(1, 28, fi1034 + fi661 + 5)) * G(h(1, 28, fi1034 + fi661 + 5), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661))

      // AVX Loader:

      // 2x4 -> 4x4
      _t24_116 = _t24_12;
      _t24_117 = _t24_13;
      _t24_118 = _mm256_setzero_pd();
      _t24_119 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t24_120 = _t24_3;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t24_121 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_120, _t24_120, 32), _mm256_permute2f128_pd(_t24_120, _t24_120, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_14, _t24_15), _mm256_unpacklo_pd(_t24_16, _t24_17), 32));
      _t24_122 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_120, _t24_120, 32), _mm256_permute2f128_pd(_t24_120, _t24_120, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_14, _t24_15), _mm256_unpacklo_pd(_t24_16, _t24_17), 32));
      _t24_123 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_120, _t24_120, 49), _mm256_permute2f128_pd(_t24_120, _t24_120, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_14, _t24_15), _mm256_unpacklo_pd(_t24_16, _t24_17), 32));
      _t24_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_120, _t24_120, 49), _mm256_permute2f128_pd(_t24_120, _t24_120, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_14, _t24_15), _mm256_unpacklo_pd(_t24_16, _t24_17), 32));

      // 4-BLAC: 4x4 - 4x4
      _t24_125 = _mm256_sub_pd(_t24_116, _t24_121);
      _t24_126 = _mm256_sub_pd(_t24_117, _t24_122);
      _t24_127 = _mm256_sub_pd(_t24_118, _t24_123);
      _t24_128 = _mm256_sub_pd(_t24_119, _t24_124);

      // AVX Storer:
      _t24_12 = _t24_125;
      _t24_13 = _t24_126;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi1034 + fi661 + 6), L[28,28],h(1, 28, fi1034 + fi661 + 6)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_129 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_12, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_130 = _t24_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_131 = _t16_12;

      // 4-BLAC: 1x4 + 1x4
      _t24_132 = _mm256_add_pd(_t24_130, _t24_131);

      // 4-BLAC: 1x4 / 1x4
      _t24_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_129), _mm256_castpd256_pd128(_t24_132)));

      // AVX Storer:
      _t24_18 = _t24_133;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_134 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_12, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_135 = _t24_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_136 = _t16_5;

      // 4-BLAC: (4x1)^T
      _t24_137 = _t24_136;

      // 4-BLAC: 1x4 Kro 1x4
      _t24_138 = _mm256_mul_pd(_t24_135, _t24_137);

      // 4-BLAC: 1x4 - 1x4
      _t24_139 = _mm256_sub_pd(_t24_134, _t24_138);

      // AVX Storer:
      _t24_19 = _t24_139;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi1034 + fi661 + 6), L[28,28],h(1, 28, fi1034 + fi661 + 6)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_140 = _t24_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_141 = _t24_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_142 = _t16_10;

      // 4-BLAC: 1x4 + 1x4
      _t24_143 = _mm256_add_pd(_t24_141, _t24_142);

      // 4-BLAC: 1x4 / 1x4
      _t24_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_140), _mm256_castpd256_pd128(_t24_143)));

      // AVX Storer:
      _t24_19 = _t24_144;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_145 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t24_12, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_146 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_18, _t24_19), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_147 = _t16_2;

      // 4-BLAC: (1x4)^T
      _t24_148 = _t24_147;

      // 4-BLAC: 1x4 * 4x1
      _t24_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_mul_pd(_t24_146, _t24_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_mul_pd(_t24_146, _t24_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_mul_pd(_t24_146, _t24_148), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_150 = _mm256_sub_pd(_t24_145, _t24_149);

      // AVX Storer:
      _t24_20 = _t24_150;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi1034 + fi661 + 6), L[28,28],h(1, 28, fi1034 + fi661 + 6)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_151 = _t24_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_152 = _t24_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_153 = _t16_8;

      // 4-BLAC: 1x4 + 1x4
      _t24_154 = _mm256_add_pd(_t24_152, _t24_153);

      // 4-BLAC: 1x4 / 1x4
      _t24_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_151), _mm256_castpd256_pd128(_t24_154)));

      // AVX Storer:
      _t24_20 = _t24_155;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_156 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t24_12, _t24_12, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_18, _t24_19), _mm256_unpacklo_pd(_t24_20, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_158 = _t16_0;

      // 4-BLAC: (1x4)^T
      _t24_159 = _t24_158;

      // 4-BLAC: 1x4 * 4x1
      _t24_160 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_157, _t24_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_157, _t24_159), _mm256_mul_pd(_t24_157, _t24_159), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_157, _t24_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_157, _t24_159), _mm256_mul_pd(_t24_157, _t24_159), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_157, _t24_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_157, _t24_159), _mm256_mul_pd(_t24_157, _t24_159), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_161 = _mm256_sub_pd(_t24_156, _t24_160);

      // AVX Storer:
      _t24_21 = _t24_161;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 6), ( G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi1034 + fi661 + 6), L[28,28],h(1, 28, fi1034 + fi661 + 6)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_162 = _t24_21;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_163 = _t24_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_164 = _t16_6;

      // 4-BLAC: 1x4 + 1x4
      _t24_165 = _mm256_add_pd(_t24_163, _t24_164);

      // 4-BLAC: 1x4 / 1x4
      _t24_166 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_162), _mm256_castpd256_pd128(_t24_165)));

      // AVX Storer:
      _t24_21 = _t24_166;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(4, 28, fi661)) - ( G(h(1, 28, fi1034 + fi661 + 7), L[28,28],h(1, 28, fi1034 + fi661 + 6)) Kro G(h(1, 28, fi1034 + fi661 + 6), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_167 = _t24_1;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t24_26 = _mm256_mul_pd(_t24_167, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_18, _t24_19), _mm256_unpacklo_pd(_t24_20, _t24_21), 32));

      // 4-BLAC: 1x4 - 1x4
      _t24_13 = _mm256_sub_pd(_t24_13, _t24_26);

      // AVX Storer:

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi1034 + fi661 + 7), L[28,28],h(1, 28, fi1034 + fi661 + 7)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_168 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_13, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_169 = _t24_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_170 = _t16_12;

      // 4-BLAC: 1x4 + 1x4
      _t24_171 = _mm256_add_pd(_t24_169, _t24_170);

      // 4-BLAC: 1x4 / 1x4
      _t24_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_168), _mm256_castpd256_pd128(_t24_171)));

      // AVX Storer:
      _t24_22 = _t24_172;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_173 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_13, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_174 = _t24_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_175 = _t16_5;

      // 4-BLAC: (4x1)^T
      _t24_176 = _t24_175;

      // 4-BLAC: 1x4 Kro 1x4
      _t24_177 = _mm256_mul_pd(_t24_174, _t24_176);

      // 4-BLAC: 1x4 - 1x4
      _t24_178 = _mm256_sub_pd(_t24_173, _t24_177);

      // AVX Storer:
      _t24_23 = _t24_178;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi1034 + fi661 + 7), L[28,28],h(1, 28, fi1034 + fi661 + 7)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_179 = _t24_23;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_180 = _t24_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_181 = _t16_10;

      // 4-BLAC: 1x4 + 1x4
      _t24_182 = _mm256_add_pd(_t24_180, _t24_181);

      // 4-BLAC: 1x4 / 1x4
      _t24_183 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_179), _mm256_castpd256_pd128(_t24_182)));

      // AVX Storer:
      _t24_23 = _t24_183;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_184 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t24_13, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_185 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_22, _t24_23), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t24_186 = _t16_2;

      // 4-BLAC: (1x4)^T
      _t24_187 = _t24_186;

      // 4-BLAC: 1x4 * 4x1
      _t24_188 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_185, _t24_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_185, _t24_187), _mm256_mul_pd(_t24_185, _t24_187), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_185, _t24_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_185, _t24_187), _mm256_mul_pd(_t24_185, _t24_187), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_185, _t24_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_185, _t24_187), _mm256_mul_pd(_t24_185, _t24_187), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_189 = _mm256_sub_pd(_t24_184, _t24_188);

      // AVX Storer:
      _t24_24 = _t24_189;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi1034 + fi661 + 7), L[28,28],h(1, 28, fi1034 + fi661 + 7)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_190 = _t24_24;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_191 = _t24_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_192 = _t16_8;

      // 4-BLAC: 1x4 + 1x4
      _t24_193 = _mm256_add_pd(_t24_191, _t24_192);

      // 4-BLAC: 1x4 / 1x4
      _t24_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_190), _mm256_castpd256_pd128(_t24_193)));

      // AVX Storer:
      _t24_24 = _t24_194;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_195 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t24_13, _t24_13, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_196 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_22, _t24_23), _mm256_unpacklo_pd(_t24_24, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t24_197 = _t16_0;

      // 4-BLAC: (1x4)^T
      _t24_198 = _t24_197;

      // 4-BLAC: 1x4 * 4x1
      _t24_199 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_196, _t24_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_196, _t24_198), _mm256_mul_pd(_t24_196, _t24_198), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_196, _t24_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_196, _t24_198), _mm256_mul_pd(_t24_196, _t24_198), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_196, _t24_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_196, _t24_198), _mm256_mul_pd(_t24_196, _t24_198), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t24_200 = _mm256_sub_pd(_t24_195, _t24_199);

      // AVX Storer:
      _t24_25 = _t24_200;

      // Generating : X[28,28] = S(h(1, 28, fi1034 + fi661 + 7), ( G(h(1, 28, fi1034 + fi661 + 7), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi1034 + fi661 + 7), L[28,28],h(1, 28, fi1034 + fi661 + 7)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_201 = _t24_25;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_202 = _t24_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t24_203 = _t16_6;

      // 4-BLAC: 1x4 + 1x4
      _t24_204 = _mm256_add_pd(_t24_202, _t24_203);

      // 4-BLAC: 1x4 / 1x4
      _t24_205 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_201), _mm256_castpd256_pd128(_t24_204)));

      // AVX Storer:
      _t24_25 = _t24_205;

      // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, fi1034 + fi661 + i100 + 8), ( G(h(4, 28, fi1034 + fi661 + i100 + 8), X[28,28],h(4, 28, fi661)) - ( G(h(4, 28, fi1034 + fi661 + i100 + 8), L[28,28],h(4, 28, fi1034 + fi661 + 4)) * G(h(4, 28, fi1034 + fi661 + 4), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661)) )
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 112]), _mm256_castpd256_pd128(_t24_7));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 113]), _mm256_castpd256_pd128(_t24_8));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 114]), _mm256_castpd256_pd128(_t24_9));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 115]), _mm256_castpd256_pd128(_t24_10));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 140]), _mm256_castpd256_pd128(_t24_14));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 141]), _mm256_castpd256_pd128(_t24_15));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 142]), _mm256_castpd256_pd128(_t24_16));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 143]), _mm256_castpd256_pd128(_t24_17));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 168]), _mm256_castpd256_pd128(_t24_18));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 169]), _mm256_castpd256_pd128(_t24_19));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 170]), _mm256_castpd256_pd128(_t24_20));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 171]), _mm256_castpd256_pd128(_t24_21));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 196]), _mm256_castpd256_pd128(_t24_22));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 197]), _mm256_castpd256_pd128(_t24_23));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 198]), _mm256_castpd256_pd128(_t24_24));
      _mm_store_sd(&(C[28*fi1034 + 29*fi661 + 199]), _mm256_castpd256_pd128(_t24_25));

      for( int i100 = 0; i100 <= -fi1034 - fi661 + 19; i100+=4 ) {
        _t25_36 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 224);
        _t25_37 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 252);
        _t25_38 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 280);
        _t25_39 = _asm256_loadu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 308);
        _t25_31 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 228);
        _t25_30 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 229);
        _t25_29 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 230);
        _t25_28 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 231);
        _t25_27 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 256);
        _t25_26 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 257);
        _t25_25 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 258);
        _t25_24 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 259);
        _t25_23 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 284);
        _t25_22 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 285);
        _t25_21 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 286);
        _t25_20 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 287);
        _t25_19 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 312);
        _t25_18 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 313);
        _t25_17 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 314);
        _t25_16 = _mm256_broadcast_sd(L + 29*fi1034 + 29*fi661 + 28*i100 + 315);
        _t25_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 112])));
        _t25_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 113])));
        _t25_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 114])));
        _t25_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 115])));
        _t25_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 140])));
        _t25_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 141])));
        _t25_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 142])));
        _t25_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 143])));
        _t25_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 168])));
        _t25_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 169])));
        _t25_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 170])));
        _t25_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 171])));
        _t25_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 196])));
        _t25_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 197])));
        _t25_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 198])));
        _t25_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1034 + 29*fi661 + 199])));

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t25_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_31, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_15, _t25_14), _mm256_unpacklo_pd(_t25_13, _t25_12), 32)), _mm256_mul_pd(_t25_30, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_11, _t25_10), _mm256_unpacklo_pd(_t25_9, _t25_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t25_29, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_7, _t25_6), _mm256_unpacklo_pd(_t25_5, _t25_4), 32)), _mm256_mul_pd(_t25_28, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32))));
        _t25_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_27, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_15, _t25_14), _mm256_unpacklo_pd(_t25_13, _t25_12), 32)), _mm256_mul_pd(_t25_26, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_11, _t25_10), _mm256_unpacklo_pd(_t25_9, _t25_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t25_25, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_7, _t25_6), _mm256_unpacklo_pd(_t25_5, _t25_4), 32)), _mm256_mul_pd(_t25_24, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32))));
        _t25_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_23, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_15, _t25_14), _mm256_unpacklo_pd(_t25_13, _t25_12), 32)), _mm256_mul_pd(_t25_22, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_11, _t25_10), _mm256_unpacklo_pd(_t25_9, _t25_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t25_21, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_7, _t25_6), _mm256_unpacklo_pd(_t25_5, _t25_4), 32)), _mm256_mul_pd(_t25_20, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32))));
        _t25_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_15, _t25_14), _mm256_unpacklo_pd(_t25_13, _t25_12), 32)), _mm256_mul_pd(_t25_18, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_11, _t25_10), _mm256_unpacklo_pd(_t25_9, _t25_8), 32))), _mm256_add_pd(_mm256_mul_pd(_t25_17, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_7, _t25_6), _mm256_unpacklo_pd(_t25_5, _t25_4), 32)), _mm256_mul_pd(_t25_16, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32))));

        // 4-BLAC: 4x4 - 4x4
        _t25_36 = _mm256_sub_pd(_t25_36, _t25_32);
        _t25_37 = _mm256_sub_pd(_t25_37, _t25_33);
        _t25_38 = _mm256_sub_pd(_t25_38, _t25_34);
        _t25_39 = _mm256_sub_pd(_t25_39, _t25_35);

        // AVX Storer:
        _asm256_storeu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 224, _t25_36);
        _asm256_storeu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 252, _t25_37);
        _asm256_storeu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 280, _t25_38);
        _asm256_storeu_pd(C + 28*fi1034 + 29*fi661 + 28*i100 + 308, _t25_39);
      }
    }
    _t26_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 112])));
    _t26_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 29*Max(0, -fi661 + 20) + 116])));
    _t26_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 113])));
    _t26_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 114])));
    _t26_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 115])));
    _t26_11 = _asm256_loadu_pd(C + 29*fi661 + 28*Max(0, -fi661 + 20) + 140);
    _t26_12 = _asm256_loadu_pd(C + 29*fi661 + 28*Max(0, -fi661 + 20) + 168);
    _t26_13 = _asm256_loadu_pd(C + 29*fi661 + 28*Max(0, -fi661 + 20) + 196);
    _t26_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 29*Max(0, -fi661 + 20) + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 29*Max(0, -fi661 + 20) + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 29*Max(0, -fi661 + 20) + 200)), 32);
    _t26_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 29*Max(0, -fi661 + 20) + 145])));
    _t26_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 29*Max(0, -fi661 + 20) + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi661 + 29*Max(0, -fi661 + 20) + 201)), 0);
    _t26_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 29*Max(0, -fi661 + 20) + 174])));
    _t26_1 = _mm256_broadcast_sd(&(L[29*fi661 + 29*Max(0, -fi661 + 20) + 202]));
    _t26_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi661 + 29*Max(0, -fi661 + 20) + 203])));

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 4)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_27 = _t26_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_28 = _t26_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_29 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t26_30 = _mm256_add_pd(_t26_28, _t26_29);

    // 4-BLAC: 1x4 / 1x4
    _t26_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_27), _mm256_castpd256_pd128(_t26_30)));

    // AVX Storer:
    _t26_7 = _t26_31;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_32 = _t26_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_33 = _t26_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_34 = _t16_5;

    // 4-BLAC: (4x1)^T
    _t26_35 = _t26_34;

    // 4-BLAC: 1x4 Kro 1x4
    _t26_36 = _mm256_mul_pd(_t26_33, _t26_35);

    // 4-BLAC: 1x4 - 1x4
    _t26_37 = _mm256_sub_pd(_t26_32, _t26_36);

    // AVX Storer:
    _t26_8 = _t26_37;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 4)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_38 = _t26_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_39 = _t26_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_40 = _t16_10;

    // 4-BLAC: 1x4 + 1x4
    _t26_41 = _mm256_add_pd(_t26_39, _t26_40);

    // 4-BLAC: 1x4 / 1x4
    _t26_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_38), _mm256_castpd256_pd128(_t26_41)));

    // AVX Storer:
    _t26_8 = _t26_42;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_43 = _t26_9;

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_44 = _mm256_blend_pd(_mm256_unpacklo_pd(_t26_7, _t26_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_45 = _t16_2;

    // 4-BLAC: (1x4)^T
    _t26_46 = _t26_45;

    // 4-BLAC: 1x4 * 4x1
    _t26_47 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_44, _t26_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_44, _t26_46), _mm256_mul_pd(_t26_44, _t26_46), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_44, _t26_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_44, _t26_46), _mm256_mul_pd(_t26_44, _t26_46), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_44, _t26_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_44, _t26_46), _mm256_mul_pd(_t26_44, _t26_46), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_48 = _mm256_sub_pd(_t26_43, _t26_47);

    // AVX Storer:
    _t26_9 = _t26_48;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 4)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_49 = _t26_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_50 = _t26_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_51 = _t16_8;

    // 4-BLAC: 1x4 + 1x4
    _t26_52 = _mm256_add_pd(_t26_50, _t26_51);

    // 4-BLAC: 1x4 / 1x4
    _t26_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_49), _mm256_castpd256_pd128(_t26_52)));

    // AVX Storer:
    _t26_9 = _t26_53;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_54 = _t26_10;

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_55 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_7, _t26_8), _mm256_unpacklo_pd(_t26_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_56 = _t16_0;

    // 4-BLAC: (1x4)^T
    _t26_57 = _t26_56;

    // 4-BLAC: 1x4 * 4x1
    _t26_58 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_55, _t26_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_55, _t26_57), _mm256_mul_pd(_t26_55, _t26_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_55, _t26_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_55, _t26_57), _mm256_mul_pd(_t26_55, _t26_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_55, _t26_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_55, _t26_57), _mm256_mul_pd(_t26_55, _t26_57), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_59 = _mm256_sub_pd(_t26_54, _t26_58);

    // AVX Storer:
    _t26_10 = _t26_59;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 4)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_60 = _t26_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_61 = _t26_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_62 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t26_63 = _mm256_add_pd(_t26_61, _t26_62);

    // 4-BLAC: 1x4 / 1x4
    _t26_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_60), _mm256_castpd256_pd128(_t26_63)));

    // AVX Storer:
    _t26_10 = _t26_64;

    // Generating : X[28,28] = S(h(3, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(3, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(4, 28, fi661)) - ( G(h(3, 28, fi661 + Max(0, -fi661 + 20) + 5), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 4)) * G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 4), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661))

    // AVX Loader:

    // 3x4 -> 4x4
    _t26_65 = _t26_11;
    _t26_66 = _t26_12;
    _t26_67 = _t26_13;
    _t26_68 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t26_69 = _t26_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t26_70 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_69, _t26_69, 32), _mm256_permute2f128_pd(_t26_69, _t26_69, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_7, _t26_8), _mm256_unpacklo_pd(_t26_9, _t26_10), 32));
    _t26_71 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_69, _t26_69, 32), _mm256_permute2f128_pd(_t26_69, _t26_69, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_7, _t26_8), _mm256_unpacklo_pd(_t26_9, _t26_10), 32));
    _t26_72 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_69, _t26_69, 49), _mm256_permute2f128_pd(_t26_69, _t26_69, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_7, _t26_8), _mm256_unpacklo_pd(_t26_9, _t26_10), 32));
    _t26_73 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_69, _t26_69, 49), _mm256_permute2f128_pd(_t26_69, _t26_69, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_7, _t26_8), _mm256_unpacklo_pd(_t26_9, _t26_10), 32));

    // 4-BLAC: 4x4 - 4x4
    _t26_74 = _mm256_sub_pd(_t26_65, _t26_70);
    _t26_75 = _mm256_sub_pd(_t26_66, _t26_71);
    _t26_76 = _mm256_sub_pd(_t26_67, _t26_72);
    _t26_77 = _mm256_sub_pd(_t26_68, _t26_73);

    // AVX Storer:
    _t26_11 = _t26_74;
    _t26_12 = _t26_75;
    _t26_13 = _t26_76;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 5)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_78 = _mm256_blend_pd(_mm256_setzero_pd(), _t26_11, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_79 = _t26_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_80 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t26_81 = _mm256_add_pd(_t26_79, _t26_80);

    // 4-BLAC: 1x4 / 1x4
    _t26_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_78), _mm256_castpd256_pd128(_t26_81)));

    // AVX Storer:
    _t26_14 = _t26_82;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_83 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_11, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_84 = _t26_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_85 = _t16_5;

    // 4-BLAC: (4x1)^T
    _t26_86 = _t26_85;

    // 4-BLAC: 1x4 Kro 1x4
    _t26_87 = _mm256_mul_pd(_t26_84, _t26_86);

    // 4-BLAC: 1x4 - 1x4
    _t26_88 = _mm256_sub_pd(_t26_83, _t26_87);

    // AVX Storer:
    _t26_15 = _t26_88;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 5)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_89 = _t26_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_90 = _t26_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_91 = _t16_10;

    // 4-BLAC: 1x4 + 1x4
    _t26_92 = _mm256_add_pd(_t26_90, _t26_91);

    // 4-BLAC: 1x4 / 1x4
    _t26_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_89), _mm256_castpd256_pd128(_t26_92)));

    // AVX Storer:
    _t26_15 = _t26_93;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_94 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t26_11, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_95 = _mm256_blend_pd(_mm256_unpacklo_pd(_t26_14, _t26_15), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_96 = _t16_2;

    // 4-BLAC: (1x4)^T
    _t26_97 = _t26_96;

    // 4-BLAC: 1x4 * 4x1
    _t26_98 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_95, _t26_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_95, _t26_97), _mm256_mul_pd(_t26_95, _t26_97), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_95, _t26_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_95, _t26_97), _mm256_mul_pd(_t26_95, _t26_97), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_95, _t26_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_95, _t26_97), _mm256_mul_pd(_t26_95, _t26_97), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_99 = _mm256_sub_pd(_t26_94, _t26_98);

    // AVX Storer:
    _t26_16 = _t26_99;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 5)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_100 = _t26_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_101 = _t26_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_102 = _t16_8;

    // 4-BLAC: 1x4 + 1x4
    _t26_103 = _mm256_add_pd(_t26_101, _t26_102);

    // 4-BLAC: 1x4 / 1x4
    _t26_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_100), _mm256_castpd256_pd128(_t26_103)));

    // AVX Storer:
    _t26_16 = _t26_104;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t26_11, _t26_11, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_14, _t26_15), _mm256_unpacklo_pd(_t26_16, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_107 = _t16_0;

    // 4-BLAC: (1x4)^T
    _t26_108 = _t26_107;

    // 4-BLAC: 1x4 * 4x1
    _t26_109 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_106, _t26_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_106, _t26_108), _mm256_mul_pd(_t26_106, _t26_108), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_106, _t26_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_106, _t26_108), _mm256_mul_pd(_t26_106, _t26_108), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_106, _t26_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_106, _t26_108), _mm256_mul_pd(_t26_106, _t26_108), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_110 = _mm256_sub_pd(_t26_105, _t26_109);

    // AVX Storer:
    _t26_17 = _t26_110;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 5)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_111 = _t26_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_112 = _t26_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_113 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t26_114 = _mm256_add_pd(_t26_112, _t26_113);

    // 4-BLAC: 1x4 / 1x4
    _t26_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_111), _mm256_castpd256_pd128(_t26_114)));

    // AVX Storer:
    _t26_17 = _t26_115;

    // Generating : X[28,28] = S(h(2, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(2, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(4, 28, fi661)) - ( G(h(2, 28, fi661 + Max(0, -fi661 + 20) + 6), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 5)) * G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 5), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661))

    // AVX Loader:

    // 2x4 -> 4x4
    _t26_116 = _t26_12;
    _t26_117 = _t26_13;
    _t26_118 = _mm256_setzero_pd();
    _t26_119 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t26_120 = _t26_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t26_121 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_120, _t26_120, 32), _mm256_permute2f128_pd(_t26_120, _t26_120, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_14, _t26_15), _mm256_unpacklo_pd(_t26_16, _t26_17), 32));
    _t26_122 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_120, _t26_120, 32), _mm256_permute2f128_pd(_t26_120, _t26_120, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_14, _t26_15), _mm256_unpacklo_pd(_t26_16, _t26_17), 32));
    _t26_123 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_120, _t26_120, 49), _mm256_permute2f128_pd(_t26_120, _t26_120, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_14, _t26_15), _mm256_unpacklo_pd(_t26_16, _t26_17), 32));
    _t26_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_120, _t26_120, 49), _mm256_permute2f128_pd(_t26_120, _t26_120, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_14, _t26_15), _mm256_unpacklo_pd(_t26_16, _t26_17), 32));

    // 4-BLAC: 4x4 - 4x4
    _t26_125 = _mm256_sub_pd(_t26_116, _t26_121);
    _t26_126 = _mm256_sub_pd(_t26_117, _t26_122);
    _t26_127 = _mm256_sub_pd(_t26_118, _t26_123);
    _t26_128 = _mm256_sub_pd(_t26_119, _t26_124);

    // AVX Storer:
    _t26_12 = _t26_125;
    _t26_13 = _t26_126;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 6)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_129 = _mm256_blend_pd(_mm256_setzero_pd(), _t26_12, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_130 = _t26_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_131 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t26_132 = _mm256_add_pd(_t26_130, _t26_131);

    // 4-BLAC: 1x4 / 1x4
    _t26_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_129), _mm256_castpd256_pd128(_t26_132)));

    // AVX Storer:
    _t26_18 = _t26_133;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_134 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_12, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_135 = _t26_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_136 = _t16_5;

    // 4-BLAC: (4x1)^T
    _t26_137 = _t26_136;

    // 4-BLAC: 1x4 Kro 1x4
    _t26_138 = _mm256_mul_pd(_t26_135, _t26_137);

    // 4-BLAC: 1x4 - 1x4
    _t26_139 = _mm256_sub_pd(_t26_134, _t26_138);

    // AVX Storer:
    _t26_19 = _t26_139;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 6)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_140 = _t26_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_141 = _t26_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_142 = _t16_10;

    // 4-BLAC: 1x4 + 1x4
    _t26_143 = _mm256_add_pd(_t26_141, _t26_142);

    // 4-BLAC: 1x4 / 1x4
    _t26_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_140), _mm256_castpd256_pd128(_t26_143)));

    // AVX Storer:
    _t26_19 = _t26_144;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_145 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t26_12, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_146 = _mm256_blend_pd(_mm256_unpacklo_pd(_t26_18, _t26_19), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_147 = _t16_2;

    // 4-BLAC: (1x4)^T
    _t26_148 = _t26_147;

    // 4-BLAC: 1x4 * 4x1
    _t26_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_146, _t26_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_146, _t26_148), _mm256_mul_pd(_t26_146, _t26_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_146, _t26_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_146, _t26_148), _mm256_mul_pd(_t26_146, _t26_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_146, _t26_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_146, _t26_148), _mm256_mul_pd(_t26_146, _t26_148), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_150 = _mm256_sub_pd(_t26_145, _t26_149);

    // AVX Storer:
    _t26_20 = _t26_150;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 6)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_151 = _t26_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_152 = _t26_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_153 = _t16_8;

    // 4-BLAC: 1x4 + 1x4
    _t26_154 = _mm256_add_pd(_t26_152, _t26_153);

    // 4-BLAC: 1x4 / 1x4
    _t26_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_151), _mm256_castpd256_pd128(_t26_154)));

    // AVX Storer:
    _t26_20 = _t26_155;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_156 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t26_12, _t26_12, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_18, _t26_19), _mm256_unpacklo_pd(_t26_20, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_158 = _t16_0;

    // 4-BLAC: (1x4)^T
    _t26_159 = _t26_158;

    // 4-BLAC: 1x4 * 4x1
    _t26_160 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_157, _t26_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_157, _t26_159), _mm256_mul_pd(_t26_157, _t26_159), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_157, _t26_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_157, _t26_159), _mm256_mul_pd(_t26_157, _t26_159), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_157, _t26_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_157, _t26_159), _mm256_mul_pd(_t26_157, _t26_159), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_161 = _mm256_sub_pd(_t26_156, _t26_160);

    // AVX Storer:
    _t26_21 = _t26_161;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 6)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_162 = _t26_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_163 = _t26_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_164 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t26_165 = _mm256_add_pd(_t26_163, _t26_164);

    // 4-BLAC: 1x4 / 1x4
    _t26_166 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_162), _mm256_castpd256_pd128(_t26_165)));

    // AVX Storer:
    _t26_21 = _t26_166;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(4, 28, fi661)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 6)) Kro G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 6), X[28,28],h(4, 28, fi661)) ) ),h(4, 28, fi661))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_167 = _t26_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t26_26 = _mm256_mul_pd(_t26_167, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_18, _t26_19), _mm256_unpacklo_pd(_t26_20, _t26_21), 32));

    // 4-BLAC: 1x4 - 1x4
    _t26_13 = _mm256_sub_pd(_t26_13, _t26_26);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 7)) + G(h(1, 28, fi661), L[28,28],h(1, 28, fi661)) ) ),h(1, 28, fi661))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_168 = _mm256_blend_pd(_mm256_setzero_pd(), _t26_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_169 = _t26_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_170 = _t16_12;

    // 4-BLAC: 1x4 + 1x4
    _t26_171 = _mm256_add_pd(_t26_169, _t26_170);

    // 4-BLAC: 1x4 / 1x4
    _t26_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_168), _mm256_castpd256_pd128(_t26_171)));

    // AVX Storer:
    _t26_22 = _t26_172;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661 + 1)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661)) Kro T( G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661)) ) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_173 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_13, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_174 = _t26_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_175 = _t16_5;

    // 4-BLAC: (4x1)^T
    _t26_176 = _t26_175;

    // 4-BLAC: 1x4 Kro 1x4
    _t26_177 = _mm256_mul_pd(_t26_174, _t26_176);

    // 4-BLAC: 1x4 - 1x4
    _t26_178 = _mm256_sub_pd(_t26_173, _t26_177);

    // AVX Storer:
    _t26_23 = _t26_178;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661 + 1)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 7)) + G(h(1, 28, fi661 + 1), L[28,28],h(1, 28, fi661 + 1)) ) ),h(1, 28, fi661 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_179 = _t26_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_180 = _t26_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_181 = _t16_10;

    // 4-BLAC: 1x4 + 1x4
    _t26_182 = _mm256_add_pd(_t26_180, _t26_181);

    // 4-BLAC: 1x4 / 1x4
    _t26_183 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_179), _mm256_castpd256_pd128(_t26_182)));

    // AVX Storer:
    _t26_23 = _t26_183;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661 + 2)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(2, 28, fi661)) * T( G(h(1, 28, fi661 + 2), L[28,28],h(2, 28, fi661)) ) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_184 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t26_13, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_185 = _mm256_blend_pd(_mm256_unpacklo_pd(_t26_22, _t26_23), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t26_186 = _t16_2;

    // 4-BLAC: (1x4)^T
    _t26_187 = _t26_186;

    // 4-BLAC: 1x4 * 4x1
    _t26_188 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_185, _t26_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_185, _t26_187), _mm256_mul_pd(_t26_185, _t26_187), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_185, _t26_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_185, _t26_187), _mm256_mul_pd(_t26_185, _t26_187), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_185, _t26_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_185, _t26_187), _mm256_mul_pd(_t26_185, _t26_187), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_189 = _mm256_sub_pd(_t26_184, _t26_188);

    // AVX Storer:
    _t26_24 = _t26_189;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661 + 2)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 7)) + G(h(1, 28, fi661 + 2), L[28,28],h(1, 28, fi661 + 2)) ) ),h(1, 28, fi661 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_190 = _t26_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_191 = _t26_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_192 = _t16_8;

    // 4-BLAC: 1x4 + 1x4
    _t26_193 = _mm256_add_pd(_t26_191, _t26_192);

    // 4-BLAC: 1x4 / 1x4
    _t26_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_190), _mm256_castpd256_pd128(_t26_193)));

    // AVX Storer:
    _t26_24 = _t26_194;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661 + 3)) - ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(3, 28, fi661)) * T( G(h(1, 28, fi661 + 3), L[28,28],h(3, 28, fi661)) ) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_195 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t26_13, _t26_13, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_196 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_22, _t26_23), _mm256_unpacklo_pd(_t26_24, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t26_197 = _t16_0;

    // 4-BLAC: (1x4)^T
    _t26_198 = _t26_197;

    // 4-BLAC: 1x4 * 4x1
    _t26_199 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_196, _t26_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_196, _t26_198), _mm256_mul_pd(_t26_196, _t26_198), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_196, _t26_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_196, _t26_198), _mm256_mul_pd(_t26_196, _t26_198), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_196, _t26_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_196, _t26_198), _mm256_mul_pd(_t26_196, _t26_198), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t26_200 = _mm256_sub_pd(_t26_195, _t26_199);

    // AVX Storer:
    _t26_25 = _t26_200;

    // Generating : X[28,28] = S(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), X[28,28],h(1, 28, fi661 + 3)) Div ( G(h(1, 28, fi661 + Max(0, -fi661 + 20) + 7), L[28,28],h(1, 28, fi661 + Max(0, -fi661 + 20) + 7)) + G(h(1, 28, fi661 + 3), L[28,28],h(1, 28, fi661 + 3)) ) ),h(1, 28, fi661 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_201 = _t26_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_202 = _t26_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_203 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t26_204 = _mm256_add_pd(_t26_202, _t26_203);

    // 4-BLAC: 1x4 / 1x4
    _t26_205 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_201), _mm256_castpd256_pd128(_t26_204)));

    // AVX Storer:
    _t26_25 = _t26_205;
    _mm_store_sd(C + 29*fi661, _mm256_castpd256_pd128(_t13_44));
    _mm_store_sd(&(C[29*fi661 + 28]), _mm256_castpd256_pd128(_t16_14));
    _mm_store_sd(&(C[29*fi661 + 56]), _mm256_castpd256_pd128(_t16_16));
    _mm_store_sd(&(C[29*fi661 + 84]), _mm256_castpd256_pd128(_t16_17));
    _mm_store_sd(&(C[29*fi661 + 29]), _mm256_castpd256_pd128(_t16_18));
    _mm_store_sd(&(C[29*fi661 + 57]), _mm256_castpd256_pd128(_t16_20));
    _mm_store_sd(&(C[29*fi661 + 85]), _mm256_castpd256_pd128(_t16_21));
    _mm_store_sd(&(C[29*fi661 + 58]), _mm256_castpd256_pd128(_t16_22));
    _mm_store_sd(&(C[29*fi661 + 86]), _mm256_castpd256_pd128(_t16_23));
    _mm_store_sd(&(C[29*fi661 + 87]), _mm256_castpd256_pd128(_t16_24));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 112]), _mm256_castpd256_pd128(_t26_7));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 113]), _mm256_castpd256_pd128(_t26_8));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 114]), _mm256_castpd256_pd128(_t26_9));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 115]), _mm256_castpd256_pd128(_t26_10));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 140]), _mm256_castpd256_pd128(_t26_14));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 141]), _mm256_castpd256_pd128(_t26_15));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 142]), _mm256_castpd256_pd128(_t26_16));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 143]), _mm256_castpd256_pd128(_t26_17));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 168]), _mm256_castpd256_pd128(_t26_18));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 169]), _mm256_castpd256_pd128(_t26_19));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 170]), _mm256_castpd256_pd128(_t26_20));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 171]), _mm256_castpd256_pd128(_t26_21));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 196]), _mm256_castpd256_pd128(_t26_22));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 197]), _mm256_castpd256_pd128(_t26_23));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 198]), _mm256_castpd256_pd128(_t26_24));
    _mm_store_sd(&(C[29*fi661 + 28*Max(0, -fi661 + 20) + 199]), _mm256_castpd256_pd128(_t26_25));
  }

  _t27_44 = _mm256_castpd128_pd256(_mm_load_sd(C + 580));
  _t27_45 = _mm256_maskload_pd(C + 608, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t27_46 = _mm256_maskload_pd(C + 636, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t27_47 = _asm256_loadu_pd(C + 664);
  _t27_39 = _mm256_broadcast_sd(L + 560);
  _t27_38 = _mm256_broadcast_sd(L + 561);
  _t27_37 = _mm256_broadcast_sd(L + 562);
  _t27_36 = _mm256_broadcast_sd(L + 563);
  _t27_35 = _mm256_broadcast_sd(L + 588);
  _t27_34 = _mm256_broadcast_sd(L + 589);
  _t27_33 = _mm256_broadcast_sd(L + 590);
  _t27_32 = _mm256_broadcast_sd(L + 591);
  _t27_31 = _mm256_broadcast_sd(L + 616);
  _t27_30 = _mm256_broadcast_sd(L + 617);
  _t27_29 = _mm256_broadcast_sd(L + 618);
  _t27_28 = _mm256_broadcast_sd(L + 619);
  _t27_27 = _mm256_broadcast_sd(L + 644);
  _t27_26 = _mm256_broadcast_sd(L + 645);
  _t27_25 = _mm256_broadcast_sd(L + 646);
  _t27_24 = _mm256_broadcast_sd(L + 647);
  _t27_23 = _asm256_loadu_pd(C + 560);
  _t27_22 = _asm256_loadu_pd(C + 588);
  _t27_21 = _asm256_loadu_pd(C + 616);
  _t27_20 = _asm256_loadu_pd(C + 644);
  _t27_19 = _mm256_broadcast_sd(C + 560);
  _t27_18 = _mm256_broadcast_sd(C + 561);
  _t27_17 = _mm256_broadcast_sd(C + 562);
  _t27_16 = _mm256_broadcast_sd(C + 563);
  _t27_15 = _mm256_broadcast_sd(C + 588);
  _t27_14 = _mm256_broadcast_sd(C + 589);
  _t27_13 = _mm256_broadcast_sd(C + 590);
  _t27_12 = _mm256_broadcast_sd(C + 591);
  _t27_11 = _mm256_broadcast_sd(C + 616);
  _t27_10 = _mm256_broadcast_sd(C + 617);
  _t27_9 = _mm256_broadcast_sd(C + 618);
  _t27_8 = _mm256_broadcast_sd(C + 619);
  _t27_7 = _mm256_broadcast_sd(C + 644);
  _t27_6 = _mm256_broadcast_sd(C + 645);
  _t27_5 = _mm256_broadcast_sd(C + 646);
  _t27_4 = _mm256_broadcast_sd(C + 647);
  _t27_3 = _asm256_loadu_pd(L + 560);
  _t27_2 = _asm256_loadu_pd(L + 588);
  _t27_1 = _asm256_loadu_pd(L + 616);
  _t27_0 = _asm256_loadu_pd(L + 644);

  // Generating : X[28,28] = ( ( S(h(4, 28, 20), ( G(h(4, 28, 20), C[28,28],h(4, 28, 20)) - ( ( G(h(4, 28, 20), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 20), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, 20), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 20), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, 20)) + Sum_{i217} ( -$(h(4, 28, 20), ( G(h(4, 28, 20), L[28,28],h(4, 28, i217)) * T( G(h(4, 28, 20), X[28,28],h(4, 28, i217)) ) ),h(4, 28, 20)) ) ) + Sum_{i100} ( -$(h(4, 28, 20), ( G(h(4, 28, 20), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, 20), L[28,28],h(4, 28, i100)) ) ),h(4, 28, 20)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t27_60 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t27_44, _t27_45, 0), _mm256_shuffle_pd(_t27_46, _t27_47, 0), 32);
  _t27_61 = _mm256_permute2f128_pd(_t27_45, _mm256_shuffle_pd(_t27_46, _t27_47, 3), 32);
  _t27_62 = _mm256_blend_pd(_t27_46, _mm256_shuffle_pd(_t27_46, _t27_47, 3), 12);
  _t27_63 = _t27_47;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t27_64 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_23, _t27_22), _mm256_unpacklo_pd(_t27_21, _t27_20), 32);
  _t27_65 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_23, _t27_22), _mm256_unpackhi_pd(_t27_21, _t27_20), 32);
  _t27_66 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_23, _t27_22), _mm256_unpacklo_pd(_t27_21, _t27_20), 49);
  _t27_67 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_23, _t27_22), _mm256_unpackhi_pd(_t27_21, _t27_20), 49);

  // 4-BLAC: 4x4 * 4x4
  _t27_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_39, _t27_64), _mm256_mul_pd(_t27_38, _t27_65)), _mm256_add_pd(_mm256_mul_pd(_t27_37, _t27_66), _mm256_mul_pd(_t27_36, _t27_67)));
  _t27_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_35, _t27_64), _mm256_mul_pd(_t27_34, _t27_65)), _mm256_add_pd(_mm256_mul_pd(_t27_33, _t27_66), _mm256_mul_pd(_t27_32, _t27_67)));
  _t27_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_31, _t27_64), _mm256_mul_pd(_t27_30, _t27_65)), _mm256_add_pd(_mm256_mul_pd(_t27_29, _t27_66), _mm256_mul_pd(_t27_28, _t27_67)));
  _t27_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_27, _t27_64), _mm256_mul_pd(_t27_26, _t27_65)), _mm256_add_pd(_mm256_mul_pd(_t27_25, _t27_66), _mm256_mul_pd(_t27_24, _t27_67)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t27_68 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 32);
  _t27_69 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 32);
  _t27_70 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 49);
  _t27_71 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t27_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_19, _t27_68), _mm256_mul_pd(_t27_18, _t27_69)), _mm256_add_pd(_mm256_mul_pd(_t27_17, _t27_70), _mm256_mul_pd(_t27_16, _t27_71)));
  _t27_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_15, _t27_68), _mm256_mul_pd(_t27_14, _t27_69)), _mm256_add_pd(_mm256_mul_pd(_t27_13, _t27_70), _mm256_mul_pd(_t27_12, _t27_71)));
  _t27_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_11, _t27_68), _mm256_mul_pd(_t27_10, _t27_69)), _mm256_add_pd(_mm256_mul_pd(_t27_9, _t27_70), _mm256_mul_pd(_t27_8, _t27_71)));
  _t27_55 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_7, _t27_68), _mm256_mul_pd(_t27_6, _t27_69)), _mm256_add_pd(_mm256_mul_pd(_t27_5, _t27_70), _mm256_mul_pd(_t27_4, _t27_71)));

  // 4-BLAC: 4x4 + 4x4
  _t27_40 = _mm256_add_pd(_t27_48, _t27_52);
  _t27_41 = _mm256_add_pd(_t27_49, _t27_53);
  _t27_42 = _mm256_add_pd(_t27_50, _t27_54);
  _t27_43 = _mm256_add_pd(_t27_51, _t27_55);

  // 4-BLAC: 4x4 - 4x4
  _t27_56 = _mm256_sub_pd(_t27_60, _t27_40);
  _t27_57 = _mm256_sub_pd(_t27_61, _t27_41);
  _t27_58 = _mm256_sub_pd(_t27_62, _t27_42);
  _t27_59 = _mm256_sub_pd(_t27_63, _t27_43);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t27_44 = _t27_56;
  _t27_45 = _t27_57;
  _t27_46 = _t27_58;
  _t27_47 = _t27_59;


  for( int i217 = 4; i217 <= 19; i217+=4 ) {
    _t28_19 = _mm256_broadcast_sd(L + i217 + 560);
    _t28_18 = _mm256_broadcast_sd(L + i217 + 561);
    _t28_17 = _mm256_broadcast_sd(L + i217 + 562);
    _t28_16 = _mm256_broadcast_sd(L + i217 + 563);
    _t28_15 = _mm256_broadcast_sd(L + i217 + 588);
    _t28_14 = _mm256_broadcast_sd(L + i217 + 589);
    _t28_13 = _mm256_broadcast_sd(L + i217 + 590);
    _t28_12 = _mm256_broadcast_sd(L + i217 + 591);
    _t28_11 = _mm256_broadcast_sd(L + i217 + 616);
    _t28_10 = _mm256_broadcast_sd(L + i217 + 617);
    _t28_9 = _mm256_broadcast_sd(L + i217 + 618);
    _t28_8 = _mm256_broadcast_sd(L + i217 + 619);
    _t28_7 = _mm256_broadcast_sd(L + i217 + 644);
    _t28_6 = _mm256_broadcast_sd(L + i217 + 645);
    _t28_5 = _mm256_broadcast_sd(L + i217 + 646);
    _t28_4 = _mm256_broadcast_sd(L + i217 + 647);
    _t28_3 = _asm256_loadu_pd(C + i217 + 560);
    _t28_2 = _asm256_loadu_pd(C + i217 + 588);
    _t28_1 = _asm256_loadu_pd(C + i217 + 616);
    _t28_0 = _asm256_loadu_pd(C + i217 + 644);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t28_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32);
    _t28_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t28_3, _t28_2), _mm256_unpackhi_pd(_t28_1, _t28_0), 32);
    _t28_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 49);
    _t28_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t28_3, _t28_2), _mm256_unpackhi_pd(_t28_1, _t28_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t28_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_19, _t28_28), _mm256_mul_pd(_t28_18, _t28_29)), _mm256_add_pd(_mm256_mul_pd(_t28_17, _t28_30), _mm256_mul_pd(_t28_16, _t28_31)));
    _t28_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_15, _t28_28), _mm256_mul_pd(_t28_14, _t28_29)), _mm256_add_pd(_mm256_mul_pd(_t28_13, _t28_30), _mm256_mul_pd(_t28_12, _t28_31)));
    _t28_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_11, _t28_28), _mm256_mul_pd(_t28_10, _t28_29)), _mm256_add_pd(_mm256_mul_pd(_t28_9, _t28_30), _mm256_mul_pd(_t28_8, _t28_31)));
    _t28_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_7, _t28_28), _mm256_mul_pd(_t28_6, _t28_29)), _mm256_add_pd(_mm256_mul_pd(_t28_5, _t28_30), _mm256_mul_pd(_t28_4, _t28_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t28_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t27_44, _t27_45, 0), _mm256_shuffle_pd(_t27_46, _t27_47, 0), 32);
    _t28_25 = _mm256_permute2f128_pd(_t27_45, _mm256_shuffle_pd(_t27_46, _t27_47, 3), 32);
    _t28_26 = _mm256_blend_pd(_t27_46, _mm256_shuffle_pd(_t27_46, _t27_47, 3), 12);
    _t28_27 = _t27_47;

    // 4-BLAC: 4x4 - 4x4
    _t28_24 = _mm256_sub_pd(_t28_24, _t28_20);
    _t28_25 = _mm256_sub_pd(_t28_25, _t28_21);
    _t28_26 = _mm256_sub_pd(_t28_26, _t28_22);
    _t28_27 = _mm256_sub_pd(_t28_27, _t28_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t27_44 = _t28_24;
    _t27_45 = _t28_25;
    _t27_46 = _t28_26;
    _t27_47 = _t28_27;
  }


  for( int i100 = 4; i100 <= 19; i100+=4 ) {
    _t29_19 = _mm256_broadcast_sd(C + i100 + 560);
    _t29_18 = _mm256_broadcast_sd(C + i100 + 561);
    _t29_17 = _mm256_broadcast_sd(C + i100 + 562);
    _t29_16 = _mm256_broadcast_sd(C + i100 + 563);
    _t29_15 = _mm256_broadcast_sd(C + i100 + 588);
    _t29_14 = _mm256_broadcast_sd(C + i100 + 589);
    _t29_13 = _mm256_broadcast_sd(C + i100 + 590);
    _t29_12 = _mm256_broadcast_sd(C + i100 + 591);
    _t29_11 = _mm256_broadcast_sd(C + i100 + 616);
    _t29_10 = _mm256_broadcast_sd(C + i100 + 617);
    _t29_9 = _mm256_broadcast_sd(C + i100 + 618);
    _t29_8 = _mm256_broadcast_sd(C + i100 + 619);
    _t29_7 = _mm256_broadcast_sd(C + i100 + 644);
    _t29_6 = _mm256_broadcast_sd(C + i100 + 645);
    _t29_5 = _mm256_broadcast_sd(C + i100 + 646);
    _t29_4 = _mm256_broadcast_sd(C + i100 + 647);
    _t29_3 = _asm256_loadu_pd(L + i100 + 560);
    _t29_2 = _asm256_loadu_pd(L + i100 + 588);
    _t29_1 = _asm256_loadu_pd(L + i100 + 616);
    _t29_0 = _asm256_loadu_pd(L + i100 + 644);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t29_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
    _t29_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
    _t29_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
    _t29_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t29_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_28), _mm256_mul_pd(_t29_18, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_30), _mm256_mul_pd(_t29_16, _t29_31)));
    _t29_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_28), _mm256_mul_pd(_t29_14, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_30), _mm256_mul_pd(_t29_12, _t29_31)));
    _t29_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_28), _mm256_mul_pd(_t29_10, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_30), _mm256_mul_pd(_t29_8, _t29_31)));
    _t29_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_28), _mm256_mul_pd(_t29_6, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_30), _mm256_mul_pd(_t29_4, _t29_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t29_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t27_44, _t27_45, 0), _mm256_shuffle_pd(_t27_46, _t27_47, 0), 32);
    _t29_25 = _mm256_permute2f128_pd(_t27_45, _mm256_shuffle_pd(_t27_46, _t27_47, 3), 32);
    _t29_26 = _mm256_blend_pd(_t27_46, _mm256_shuffle_pd(_t27_46, _t27_47, 3), 12);
    _t29_27 = _t27_47;

    // 4-BLAC: 4x4 - 4x4
    _t29_24 = _mm256_sub_pd(_t29_24, _t29_20);
    _t29_25 = _mm256_sub_pd(_t29_25, _t29_21);
    _t29_26 = _mm256_sub_pd(_t29_26, _t29_22);
    _t29_27 = _mm256_sub_pd(_t29_27, _t29_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t27_44 = _t29_24;
    _t27_45 = _t29_25;
    _t27_46 = _t29_26;
    _t27_47 = _t29_27;
  }

  _t4_73 = _mm256_castpd128_pd256(_mm_load_sd(&(C[702])));
  _t4_77 = _mm256_castpd128_pd256(_mm_load_sd(&(C[730])));
  _t4_76 = _mm256_castpd128_pd256(_mm_load_sd(&(C[729])));
  _t4_71 = _mm256_castpd128_pd256(_mm_load_sd(&(C[700])));
  _t4_82 = _mm256_castpd128_pd256(_mm_load_sd(&(C[759])));
  _t4_79 = _mm256_castpd128_pd256(_mm_load_sd(&(C[756])));
  _t4_66 = _mm256_castpd128_pd256(_mm_load_sd(&(C[674])));
  _t4_74 = _mm256_castpd128_pd256(_mm_load_sd(&(C[703])));
  _t4_72 = _mm256_castpd128_pd256(_mm_load_sd(&(C[701])));
  _t4_80 = _mm256_castpd128_pd256(_mm_load_sd(&(C[757])));
  _t4_67 = _mm256_castpd128_pd256(_mm_load_sd(&(C[675])));
  _t4_78 = _mm256_castpd128_pd256(_mm_load_sd(&(C[731])));
  _t4_75 = _mm256_castpd128_pd256(_mm_load_sd(&(C[728])));
  _t4_81 = _mm256_castpd128_pd256(_mm_load_sd(&(C[758])));
  _t4_64 = _mm256_castpd128_pd256(_mm_load_sd(&(C[672])));
  _t4_65 = _mm256_castpd128_pd256(_mm_load_sd(&(C[673])));
  _t30_12 = _mm256_castpd128_pd256(_mm_load_sd(&(L[580])));
  _t30_11 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 608)), _mm256_castpd128_pd256(_mm_load_sd(L + 636))), _mm256_castpd128_pd256(_mm_load_sd(L + 664)), 32);
  _t30_10 = _mm256_castpd128_pd256(_mm_load_sd(&(L[609])));
  _t30_9 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 637)), _mm256_castpd128_pd256(_mm_load_sd(L + 665)), 0);
  _t30_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[638])));
  _t30_7 = _mm256_castpd128_pd256(_mm_load_sd(&(L[666])));
  _t30_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[667])));
  _t30_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[608])));
  _t30_4 = _mm256_broadcast_sd(&(L[608]));
  _t30_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 636)), _mm256_castpd128_pd256(_mm_load_sd(L + 664)), 0);
  _t30_2 = _mm256_maskload_pd(L + 636, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t30_1 = _mm256_maskload_pd(L + 664, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t30_0 = _mm256_maskload_pd(L + 664, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t30_29 = _asm256_loadu_pd(C + 692);
  _t30_30 = _asm256_loadu_pd(C + 720);
  _t30_31 = _asm256_loadu_pd(C + 748);
  _t30_32 = _asm256_loadu_pd(C + 776);

  // Generating : X[28,28] = S(h(1, 28, 20), ( G(h(1, 28, 20), X[28,28],h(1, 28, 20)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_33 = _t27_44;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t30_34 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_35 = _t30_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_36 = _mm256_mul_pd(_t30_34, _t30_35);

  // 4-BLAC: 1x4 / 1x4
  _t30_37 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_33), _mm256_castpd256_pd128(_t30_36)));

  // AVX Storer:
  _t27_44 = _t30_37;

  // Generating : X[28,28] = S(h(3, 28, 21), ( G(h(3, 28, 21), X[28,28],h(1, 28, 20)) - ( G(h(3, 28, 21), L[28,28],h(1, 28, 20)) Kro G(h(1, 28, 20), X[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 3x1 -> 4x1
  _t30_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_45, _t27_46), _mm256_unpacklo_pd(_t27_47, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t30_39 = _t30_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_40 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_44, _t27_44, 32), _mm256_permute2f128_pd(_t27_44, _t27_44, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t30_41 = _mm256_mul_pd(_t30_39, _t30_40);

  // 4-BLAC: 4x1 - 4x1
  _t30_42 = _mm256_sub_pd(_t30_38, _t30_41);

  // AVX Storer:
  _t30_13 = _t30_42;

  // Generating : X[28,28] = S(h(1, 28, 21), ( G(h(1, 28, 21), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 21), L[28,28],h(1, 28, 21)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_43 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_44 = _t30_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_45 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t30_46 = _mm256_add_pd(_t30_44, _t30_45);

  // 4-BLAC: 1x4 / 1x4
  _t30_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_43), _mm256_castpd256_pd128(_t30_46)));

  // AVX Storer:
  _t30_14 = _t30_47;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(1, 28, 20)) - ( G(h(2, 28, 22), L[28,28],h(1, 28, 21)) Kro G(h(1, 28, 21), X[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_48 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_13, 2), _mm256_permute2f128_pd(_t30_13, _t30_13, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_49 = _t30_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t30_14, _t30_14, 32), _mm256_permute2f128_pd(_t30_14, _t30_14, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t30_51 = _mm256_mul_pd(_t30_49, _t30_50);

  // 4-BLAC: 4x1 - 4x1
  _t30_52 = _mm256_sub_pd(_t30_48, _t30_51);

  // AVX Storer:
  _t30_15 = _t30_52;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 22), L[28,28],h(1, 28, 22)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_53 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_54 = _t30_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_55 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t30_56 = _mm256_add_pd(_t30_54, _t30_55);

  // 4-BLAC: 1x4 / 1x4
  _t30_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_53), _mm256_castpd256_pd128(_t30_56)));

  // AVX Storer:
  _t30_16 = _t30_57;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 20)) - ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro G(h(1, 28, 22), X[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_58 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_15, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_59 = _t30_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_60 = _t30_16;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_61 = _mm256_mul_pd(_t30_59, _t30_60);

  // 4-BLAC: 1x4 - 1x4
  _t30_62 = _mm256_sub_pd(_t30_58, _t30_61);

  // AVX Storer:
  _t30_17 = _t30_62;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 23), L[28,28],h(1, 28, 23)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_63 = _t30_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_64 = _t30_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_65 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t30_66 = _mm256_add_pd(_t30_64, _t30_65);

  // 4-BLAC: 1x4 / 1x4
  _t30_67 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_63), _mm256_castpd256_pd128(_t30_66)));

  // AVX Storer:
  _t30_17 = _t30_67;

  // Generating : X[28,28] = S(h(1, 28, 21), ( G(h(1, 28, 21), X[28,28],h(1, 28, 21)) - ( ( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), X[28,28],h(1, 28, 20)) ) ) + ( G(h(1, 28, 21), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_68 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t27_45, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_69 = _t30_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_70 = _t30_14;

  // 4-BLAC: (4x1)^T
  _t30_71 = _t30_70;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_72 = _mm256_mul_pd(_t30_69, _t30_71);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_73 = _t30_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_74 = _t30_5;

  // 4-BLAC: (4x1)^T
  _t30_75 = _t30_74;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_76 = _mm256_mul_pd(_t30_73, _t30_75);

  // 4-BLAC: 1x4 + 1x4
  _t30_77 = _mm256_add_pd(_t30_72, _t30_76);

  // 4-BLAC: 1x4 - 1x4
  _t30_78 = _mm256_sub_pd(_t30_68, _t30_77);

  // AVX Storer:
  _t30_18 = _t30_78;

  // Generating : X[28,28] = S(h(1, 28, 21), ( G(h(1, 28, 21), X[28,28],h(1, 28, 21)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_79 = _t30_18;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t30_80 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_81 = _t30_10;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_82 = _mm256_mul_pd(_t30_80, _t30_81);

  // 4-BLAC: 1x4 / 1x4
  _t30_83 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_79), _mm256_castpd256_pd128(_t30_82)));

  // AVX Storer:
  _t30_18 = _t30_83;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(1, 28, 21)) - ( G(h(2, 28, 22), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_84 = _mm256_unpackhi_pd(_mm256_blend_pd(_t27_46, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t27_47, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_85 = _mm256_blend_pd(_mm256_unpacklo_pd(_t30_16, _t30_17), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_86 = _t30_4;

  // 4-BLAC: (4x1)^T
  _t30_87 = _t30_86;

  // 4-BLAC: 4x1 Kro 1x4
  _t30_88 = _mm256_mul_pd(_t30_85, _t30_87);

  // 4-BLAC: 4x1 - 4x1
  _t30_89 = _mm256_sub_pd(_t30_84, _t30_88);

  // AVX Storer:
  _t30_19 = _t30_89;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(1, 28, 21)) - ( G(h(2, 28, 22), L[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), X[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_90 = _t30_19;

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_91 = _t30_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_92 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t30_14, _t30_14, 32), _mm256_permute2f128_pd(_t30_14, _t30_14, 32), 0);

  // 4-BLAC: (4x1)^T
  _t30_93 = _t30_92;

  // 4-BLAC: 4x1 Kro 1x4
  _t30_94 = _mm256_mul_pd(_t30_91, _t30_93);

  // 4-BLAC: 4x1 - 4x1
  _t30_95 = _mm256_sub_pd(_t30_90, _t30_94);

  // AVX Storer:
  _t30_19 = _t30_95;

  // Generating : X[28,28] = S(h(2, 28, 22), ( G(h(2, 28, 22), X[28,28],h(1, 28, 21)) - ( G(h(2, 28, 22), L[28,28],h(1, 28, 21)) Kro G(h(1, 28, 21), X[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_96 = _t30_19;

  // AVX Loader:

  // 2x1 -> 4x1
  _t30_97 = _t30_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_98 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t30_18, _t30_18, 32), _mm256_permute2f128_pd(_t30_18, _t30_18, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t30_99 = _mm256_mul_pd(_t30_97, _t30_98);

  // 4-BLAC: 4x1 - 4x1
  _t30_100 = _mm256_sub_pd(_t30_96, _t30_99);

  // AVX Storer:
  _t30_19 = _t30_100;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 22), L[28,28],h(1, 28, 22)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_101 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_102 = _t30_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_103 = _t30_10;

  // 4-BLAC: 1x4 + 1x4
  _t30_104 = _mm256_add_pd(_t30_102, _t30_103);

  // 4-BLAC: 1x4 / 1x4
  _t30_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_101), _mm256_castpd256_pd128(_t30_104)));

  // AVX Storer:
  _t30_20 = _t30_105;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro G(h(1, 28, 22), X[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_106 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_19, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_107 = _t30_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_108 = _t30_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_109 = _mm256_mul_pd(_t30_107, _t30_108);

  // 4-BLAC: 1x4 - 1x4
  _t30_110 = _mm256_sub_pd(_t30_106, _t30_109);

  // AVX Storer:
  _t30_21 = _t30_110;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 23), L[28,28],h(1, 28, 23)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_111 = _t30_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_112 = _t30_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_113 = _t30_10;

  // 4-BLAC: 1x4 + 1x4
  _t30_114 = _mm256_add_pd(_t30_112, _t30_113);

  // 4-BLAC: 1x4 / 1x4
  _t30_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_111), _mm256_castpd256_pd128(_t30_114)));

  // AVX Storer:
  _t30_21 = _t30_115;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 22)) - ( ( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), X[28,28],h(2, 28, 20)) ) ) + ( G(h(1, 28, 22), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_116 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t27_46, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t27_46, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_117 = _t30_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_118 = _mm256_blend_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t30_119 = _t30_118;

  // 4-BLAC: 1x4 * 4x1
  _t30_120 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_117, _t30_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_117, _t30_119), _mm256_mul_pd(_t30_117, _t30_119), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_117, _t30_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_117, _t30_119), _mm256_mul_pd(_t30_117, _t30_119), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_117, _t30_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_117, _t30_119), _mm256_mul_pd(_t30_117, _t30_119), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_121 = _mm256_blend_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_122 = _t30_2;

  // 4-BLAC: (1x4)^T
  _t30_123 = _t30_122;

  // 4-BLAC: 1x4 * 4x1
  _t30_124 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_121, _t30_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_121, _t30_123), _mm256_mul_pd(_t30_121, _t30_123), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_121, _t30_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_121, _t30_123), _mm256_mul_pd(_t30_121, _t30_123), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_121, _t30_123), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_121, _t30_123), _mm256_mul_pd(_t30_121, _t30_123), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t30_125 = _mm256_add_pd(_t30_120, _t30_124);

  // 4-BLAC: 1x4 - 1x4
  _t30_126 = _mm256_sub_pd(_t30_116, _t30_125);

  // AVX Storer:
  _t30_22 = _t30_126;

  // Generating : X[28,28] = S(h(1, 28, 22), ( G(h(1, 28, 22), X[28,28],h(1, 28, 22)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_127 = _t30_22;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t30_128 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_129 = _t30_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_130 = _mm256_mul_pd(_t30_128, _t30_129);

  // 4-BLAC: 1x4 / 1x4
  _t30_131 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_127), _mm256_castpd256_pd128(_t30_130)));

  // AVX Storer:
  _t30_22 = _t30_131;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 23), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_132 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t27_47, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t27_47, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_133 = _mm256_blend_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_134 = _t30_2;

  // 4-BLAC: (1x4)^T
  _t30_135 = _t30_134;

  // 4-BLAC: 1x4 * 4x1
  _t30_136 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_133, _t30_135), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_133, _t30_135), _mm256_mul_pd(_t30_133, _t30_135), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_133, _t30_135), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_133, _t30_135), _mm256_mul_pd(_t30_133, _t30_135), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_133, _t30_135), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_133, _t30_135), _mm256_mul_pd(_t30_133, _t30_135), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t30_137 = _mm256_sub_pd(_t30_132, _t30_136);

  // AVX Storer:
  _t30_23 = _t30_137;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 23), L[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), X[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_138 = _t30_23;

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_139 = _t30_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t30_140 = _mm256_blend_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t30_141 = _t30_140;

  // 4-BLAC: 1x4 * 4x1
  _t30_142 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_139, _t30_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_139, _t30_141), _mm256_mul_pd(_t30_139, _t30_141), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_139, _t30_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_139, _t30_141), _mm256_mul_pd(_t30_139, _t30_141), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_139, _t30_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_139, _t30_141), _mm256_mul_pd(_t30_139, _t30_141), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t30_143 = _mm256_sub_pd(_t30_138, _t30_142);

  // AVX Storer:
  _t30_23 = _t30_143;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 23), L[28,28],h(1, 28, 22)) Kro G(h(1, 28, 22), X[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_144 = _t30_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_145 = _t30_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_146 = _t30_22;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_147 = _mm256_mul_pd(_t30_145, _t30_146);

  // 4-BLAC: 1x4 - 1x4
  _t30_148 = _mm256_sub_pd(_t30_144, _t30_147);

  // AVX Storer:
  _t30_23 = _t30_148;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 23), L[28,28],h(1, 28, 23)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_149 = _t30_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_150 = _t30_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_151 = _t30_8;

  // 4-BLAC: 1x4 + 1x4
  _t30_152 = _mm256_add_pd(_t30_150, _t30_151);

  // 4-BLAC: 1x4 / 1x4
  _t30_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_149), _mm256_castpd256_pd128(_t30_152)));

  // AVX Storer:
  _t30_23 = _t30_153;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 23)) - ( ( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), X[28,28],h(3, 28, 20)) ) ) + ( G(h(1, 28, 23), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_154 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t27_47, _t27_47, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t30_155 = _t30_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t30_156 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_unpacklo_pd(_t30_23, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t30_157 = _t30_156;

  // 4-BLAC: 1x4 * 4x1
  _t30_158 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_155, _t30_157), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_155, _t30_157), _mm256_mul_pd(_t30_155, _t30_157), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_155, _t30_157), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_155, _t30_157), _mm256_mul_pd(_t30_155, _t30_157), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_155, _t30_157), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_155, _t30_157), _mm256_mul_pd(_t30_155, _t30_157), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t30_159 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_unpacklo_pd(_t30_23, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t30_160 = _t30_0;

  // 4-BLAC: (1x4)^T
  _t30_161 = _t30_160;

  // 4-BLAC: 1x4 * 4x1
  _t30_162 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_159, _t30_161), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_159, _t30_161), _mm256_mul_pd(_t30_159, _t30_161), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_159, _t30_161), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_159, _t30_161), _mm256_mul_pd(_t30_159, _t30_161), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_159, _t30_161), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_159, _t30_161), _mm256_mul_pd(_t30_159, _t30_161), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t30_163 = _mm256_add_pd(_t30_158, _t30_162);

  // 4-BLAC: 1x4 - 1x4
  _t30_164 = _mm256_sub_pd(_t30_154, _t30_163);

  // AVX Storer:
  _t30_24 = _t30_164;

  // Generating : X[28,28] = S(h(1, 28, 23), ( G(h(1, 28, 23), X[28,28],h(1, 28, 23)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_165 = _t30_24;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t30_166 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t30_167 = _t30_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t30_168 = _mm256_mul_pd(_t30_166, _t30_167);

  // 4-BLAC: 1x4 / 1x4
  _t30_169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t30_165), _mm256_castpd256_pd128(_t30_168)));

  // AVX Storer:
  _t30_24 = _t30_169;

  // Generating : X[28,28] = ( S(h(4, 28, 24), ( G(h(4, 28, 24), C[28,28],h(4, 28, 20)) - ( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 20), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 20)) + Sum_{i100} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, 20), L[28,28],h(4, 28, i100)) ) ),h(4, 28, 20)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t30_170 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 32);
  _t30_171 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 32);
  _t30_172 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 49);
  _t30_173 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t30_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_64, _t4_64, 32), _mm256_permute2f128_pd(_t4_64, _t4_64, 32), 0), _t30_170), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_65, _t4_65, 32), _mm256_permute2f128_pd(_t4_65, _t4_65, 32), 0), _t30_171)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_66, _t4_66, 32), _mm256_permute2f128_pd(_t4_66, _t4_66, 32), 0), _t30_172), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_67, _t4_67, 32), _mm256_permute2f128_pd(_t4_67, _t4_67, 32), 0), _t30_173)));
  _t30_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_71, _t4_71, 32), _mm256_permute2f128_pd(_t4_71, _t4_71, 32), 0), _t30_170), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_72, _t4_72, 32), _mm256_permute2f128_pd(_t4_72, _t4_72, 32), 0), _t30_171)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_73, _t4_73, 32), _mm256_permute2f128_pd(_t4_73, _t4_73, 32), 0), _t30_172), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_74, _t4_74, 32), _mm256_permute2f128_pd(_t4_74, _t4_74, 32), 0), _t30_173)));
  _t30_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_75, _t4_75, 32), _mm256_permute2f128_pd(_t4_75, _t4_75, 32), 0), _t30_170), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_76, _t4_76, 32), _mm256_permute2f128_pd(_t4_76, _t4_76, 32), 0), _t30_171)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_77, _t4_77, 32), _mm256_permute2f128_pd(_t4_77, _t4_77, 32), 0), _t30_172), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_78, _t4_78, 32), _mm256_permute2f128_pd(_t4_78, _t4_78, 32), 0), _t30_173)));
  _t30_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_79, _t4_79, 32), _mm256_permute2f128_pd(_t4_79, _t4_79, 32), 0), _t30_170), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_80, _t4_80, 32), _mm256_permute2f128_pd(_t4_80, _t4_80, 32), 0), _t30_171)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_81, _t4_81, 32), _mm256_permute2f128_pd(_t4_81, _t4_81, 32), 0), _t30_172), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_82, _t4_82, 32), _mm256_permute2f128_pd(_t4_82, _t4_82, 32), 0), _t30_173)));

  // 4-BLAC: 4x4 - 4x4
  _t30_29 = _mm256_sub_pd(_t30_29, _t30_25);
  _t30_30 = _mm256_sub_pd(_t30_30, _t30_26);
  _t30_31 = _mm256_sub_pd(_t30_31, _t30_27);
  _t30_32 = _mm256_sub_pd(_t30_32, _t30_28);

  // AVX Storer:


  for( int i100 = 4; i100 <= 19; i100+=4 ) {
    _t31_19 = _mm256_broadcast_sd(C + i100 + 672);
    _t31_18 = _mm256_broadcast_sd(C + i100 + 673);
    _t31_17 = _mm256_broadcast_sd(C + i100 + 674);
    _t31_16 = _mm256_broadcast_sd(C + i100 + 675);
    _t31_15 = _mm256_broadcast_sd(C + i100 + 700);
    _t31_14 = _mm256_broadcast_sd(C + i100 + 701);
    _t31_13 = _mm256_broadcast_sd(C + i100 + 702);
    _t31_12 = _mm256_broadcast_sd(C + i100 + 703);
    _t31_11 = _mm256_broadcast_sd(C + i100 + 728);
    _t31_10 = _mm256_broadcast_sd(C + i100 + 729);
    _t31_9 = _mm256_broadcast_sd(C + i100 + 730);
    _t31_8 = _mm256_broadcast_sd(C + i100 + 731);
    _t31_7 = _mm256_broadcast_sd(C + i100 + 756);
    _t31_6 = _mm256_broadcast_sd(C + i100 + 757);
    _t31_5 = _mm256_broadcast_sd(C + i100 + 758);
    _t31_4 = _mm256_broadcast_sd(C + i100 + 759);
    _t31_3 = _asm256_loadu_pd(L + i100 + 560);
    _t31_2 = _asm256_loadu_pd(L + i100 + 588);
    _t31_1 = _asm256_loadu_pd(L + i100 + 616);
    _t31_0 = _asm256_loadu_pd(L + i100 + 644);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t31_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 32);
    _t31_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 32);
    _t31_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 49);
    _t31_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t31_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t31_24), _mm256_mul_pd(_t31_18, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t31_26), _mm256_mul_pd(_t31_16, _t31_27)));
    _t31_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t31_24), _mm256_mul_pd(_t31_14, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t31_26), _mm256_mul_pd(_t31_12, _t31_27)));
    _t31_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t31_24), _mm256_mul_pd(_t31_10, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t31_26), _mm256_mul_pd(_t31_8, _t31_27)));
    _t31_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t31_24), _mm256_mul_pd(_t31_6, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t31_26), _mm256_mul_pd(_t31_4, _t31_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t30_29 = _mm256_sub_pd(_t30_29, _t31_20);
    _t30_30 = _mm256_sub_pd(_t30_30, _t31_21);
    _t30_31 = _mm256_sub_pd(_t30_31, _t31_22);
    _t30_32 = _mm256_sub_pd(_t30_32, _t31_23);

    // AVX Storer:
  }

  _t32_15 = _mm256_broadcast_sd(L + 672);
  _t32_14 = _mm256_broadcast_sd(L + 673);
  _t32_13 = _mm256_broadcast_sd(L + 674);
  _t32_12 = _mm256_broadcast_sd(L + 675);
  _t32_11 = _mm256_broadcast_sd(L + 700);
  _t32_10 = _mm256_broadcast_sd(L + 701);
  _t32_9 = _mm256_broadcast_sd(L + 702);
  _t32_8 = _mm256_broadcast_sd(L + 703);
  _t32_7 = _mm256_broadcast_sd(L + 728);
  _t32_6 = _mm256_broadcast_sd(L + 729);
  _t32_5 = _mm256_broadcast_sd(L + 730);
  _t32_4 = _mm256_broadcast_sd(L + 731);
  _t32_3 = _mm256_broadcast_sd(L + 756);
  _t32_2 = _mm256_broadcast_sd(L + 757);
  _t32_1 = _mm256_broadcast_sd(L + 758);
  _t32_0 = _mm256_broadcast_sd(L + 759);

  // Generating : X[28,28] = ( S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, 20)) - ( G(h(4, 28, 24), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 20), X[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 20)) + Sum_{i100} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), L[28,28],h(4, 28, i100)) * T( G(h(4, 28, 20), X[28,28],h(4, 28, i100)) ) ),h(4, 28, 20)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t32_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_23, _t27_22), _mm256_unpacklo_pd(_t27_21, _t27_20), 32);
  _t32_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_23, _t27_22), _mm256_unpackhi_pd(_t27_21, _t27_20), 32);
  _t32_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_23, _t27_22), _mm256_unpacklo_pd(_t27_21, _t27_20), 49);
  _t32_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_23, _t27_22), _mm256_unpackhi_pd(_t27_21, _t27_20), 49);

  // 4-BLAC: 4x4 * 4x4
  _t32_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_15, _t32_20), _mm256_mul_pd(_t32_14, _t32_21)), _mm256_add_pd(_mm256_mul_pd(_t32_13, _t32_22), _mm256_mul_pd(_t32_12, _t32_23)));
  _t32_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_11, _t32_20), _mm256_mul_pd(_t32_10, _t32_21)), _mm256_add_pd(_mm256_mul_pd(_t32_9, _t32_22), _mm256_mul_pd(_t32_8, _t32_23)));
  _t32_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_7, _t32_20), _mm256_mul_pd(_t32_6, _t32_21)), _mm256_add_pd(_mm256_mul_pd(_t32_5, _t32_22), _mm256_mul_pd(_t32_4, _t32_23)));
  _t32_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_3, _t32_20), _mm256_mul_pd(_t32_2, _t32_21)), _mm256_add_pd(_mm256_mul_pd(_t32_1, _t32_22), _mm256_mul_pd(_t32_0, _t32_23)));

  // 4-BLAC: 4x4 - 4x4
  _t30_29 = _mm256_sub_pd(_t30_29, _t32_16);
  _t30_30 = _mm256_sub_pd(_t30_30, _t32_17);
  _t30_31 = _mm256_sub_pd(_t30_31, _t32_18);
  _t30_32 = _mm256_sub_pd(_t30_32, _t32_19);

  // AVX Storer:


  for( int i100 = 4; i100 <= 19; i100+=4 ) {
    _t33_19 = _mm256_broadcast_sd(L + i100 + 672);
    _t33_18 = _mm256_broadcast_sd(L + i100 + 673);
    _t33_17 = _mm256_broadcast_sd(L + i100 + 674);
    _t33_16 = _mm256_broadcast_sd(L + i100 + 675);
    _t33_15 = _mm256_broadcast_sd(L + i100 + 700);
    _t33_14 = _mm256_broadcast_sd(L + i100 + 701);
    _t33_13 = _mm256_broadcast_sd(L + i100 + 702);
    _t33_12 = _mm256_broadcast_sd(L + i100 + 703);
    _t33_11 = _mm256_broadcast_sd(L + i100 + 728);
    _t33_10 = _mm256_broadcast_sd(L + i100 + 729);
    _t33_9 = _mm256_broadcast_sd(L + i100 + 730);
    _t33_8 = _mm256_broadcast_sd(L + i100 + 731);
    _t33_7 = _mm256_broadcast_sd(L + i100 + 756);
    _t33_6 = _mm256_broadcast_sd(L + i100 + 757);
    _t33_5 = _mm256_broadcast_sd(L + i100 + 758);
    _t33_4 = _mm256_broadcast_sd(L + i100 + 759);
    _t33_3 = _asm256_loadu_pd(C + i100 + 560);
    _t33_2 = _asm256_loadu_pd(C + i100 + 588);
    _t33_1 = _asm256_loadu_pd(C + i100 + 616);
    _t33_0 = _asm256_loadu_pd(C + i100 + 644);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t33_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_19, _t33_24), _mm256_mul_pd(_t33_18, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_17, _t33_26), _mm256_mul_pd(_t33_16, _t33_27)));
    _t33_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_15, _t33_24), _mm256_mul_pd(_t33_14, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_13, _t33_26), _mm256_mul_pd(_t33_12, _t33_27)));
    _t33_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_11, _t33_24), _mm256_mul_pd(_t33_10, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_9, _t33_26), _mm256_mul_pd(_t33_8, _t33_27)));
    _t33_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_7, _t33_24), _mm256_mul_pd(_t33_6, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_5, _t33_26), _mm256_mul_pd(_t33_4, _t33_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t30_29 = _mm256_sub_pd(_t30_29, _t33_20);
    _t30_30 = _mm256_sub_pd(_t30_30, _t33_21);
    _t30_31 = _mm256_sub_pd(_t30_31, _t33_22);
    _t30_32 = _mm256_sub_pd(_t30_32, _t33_23);

    // AVX Storer:
  }

  _t34_19 = _mm256_broadcast_sd(L + 692);
  _t34_18 = _mm256_broadcast_sd(L + 693);
  _t34_17 = _mm256_broadcast_sd(L + 694);
  _t34_16 = _mm256_broadcast_sd(L + 695);
  _t34_15 = _mm256_broadcast_sd(L + 720);
  _t34_14 = _mm256_broadcast_sd(L + 721);
  _t34_13 = _mm256_broadcast_sd(L + 722);
  _t34_12 = _mm256_broadcast_sd(L + 723);
  _t34_11 = _mm256_broadcast_sd(L + 748);
  _t34_10 = _mm256_broadcast_sd(L + 749);
  _t34_9 = _mm256_broadcast_sd(L + 750);
  _t34_8 = _mm256_broadcast_sd(L + 751);
  _t34_7 = _mm256_broadcast_sd(L + 776);
  _t34_6 = _mm256_broadcast_sd(L + 777);
  _t34_5 = _mm256_broadcast_sd(L + 778);
  _t34_4 = _mm256_broadcast_sd(L + 779);
  _t34_40 = _mm256_castpd128_pd256(_mm_load_sd(C + 696));
  _t34_41 = _mm256_maskload_pd(C + 724, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t34_42 = _mm256_maskload_pd(C + 752, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t34_43 = _asm256_loadu_pd(C + 780);
  _t34_3 = _asm256_loadu_pd(L + 672);
  _t34_2 = _asm256_loadu_pd(L + 700);
  _t34_1 = _asm256_loadu_pd(L + 728);
  _t34_0 = _asm256_loadu_pd(L + 756);

  // Generating : X[28,28] = S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, 20)) - ( G(h(4, 28, 24), L[28,28],h(4, 28, 20)) * G(h(4, 28, 20), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t34_61 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t27_44, _mm256_blend_pd(_mm256_unpacklo_pd(_t30_14, _t30_18), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_unpacklo_pd(_t30_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_unpacklo_pd(_t30_23, _t30_24), 32), 0), 32);
  _t34_62 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t30_14, _t30_18), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_unpacklo_pd(_t30_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_unpacklo_pd(_t30_23, _t30_24), 32), 3), 32);
  _t34_63 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_unpacklo_pd(_t30_22, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_16, _t30_20), _mm256_unpacklo_pd(_t30_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_unpacklo_pd(_t30_23, _t30_24), 32), 3), 12);
  _t34_64 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_17, _t30_21), _mm256_unpacklo_pd(_t30_23, _t30_24), 32);

  // 4-BLAC: 4x4 * 4x4
  _t34_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_19, _t34_61), _mm256_mul_pd(_t34_18, _t34_62)), _mm256_add_pd(_mm256_mul_pd(_t34_17, _t34_63), _mm256_mul_pd(_t34_16, _t34_64)));
  _t34_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_15, _t34_61), _mm256_mul_pd(_t34_14, _t34_62)), _mm256_add_pd(_mm256_mul_pd(_t34_13, _t34_63), _mm256_mul_pd(_t34_12, _t34_64)));
  _t34_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_11, _t34_61), _mm256_mul_pd(_t34_10, _t34_62)), _mm256_add_pd(_mm256_mul_pd(_t34_9, _t34_63), _mm256_mul_pd(_t34_8, _t34_64)));
  _t34_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_7, _t34_61), _mm256_mul_pd(_t34_6, _t34_62)), _mm256_add_pd(_mm256_mul_pd(_t34_5, _t34_63), _mm256_mul_pd(_t34_4, _t34_64)));

  // 4-BLAC: 4x4 - 4x4
  _t30_29 = _mm256_sub_pd(_t30_29, _t34_45);
  _t30_30 = _mm256_sub_pd(_t30_30, _t34_46);
  _t30_31 = _mm256_sub_pd(_t30_31, _t34_47);
  _t30_32 = _mm256_sub_pd(_t30_32, _t34_48);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_65 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_29, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_66 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_67 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t34_68 = _mm256_add_pd(_t34_66, _t34_67);

  // 4-BLAC: 1x4 / 1x4
  _t34_69 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_65), _mm256_castpd256_pd128(_t34_68)));

  // AVX Storer:
  _t34_24 = _t34_69;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_70 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_71 = _t34_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_72 = _t30_5;

  // 4-BLAC: (4x1)^T
  _t34_73 = _t34_72;

  // 4-BLAC: 1x4 Kro 1x4
  _t34_74 = _mm256_mul_pd(_t34_71, _t34_73);

  // 4-BLAC: 1x4 - 1x4
  _t34_75 = _mm256_sub_pd(_t34_70, _t34_74);

  // AVX Storer:
  _t34_25 = _t34_75;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_76 = _t34_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_77 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_78 = _t30_10;

  // 4-BLAC: 1x4 + 1x4
  _t34_79 = _mm256_add_pd(_t34_77, _t34_78);

  // 4-BLAC: 1x4 / 1x4
  _t34_80 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_76), _mm256_castpd256_pd128(_t34_79)));

  // AVX Storer:
  _t34_25 = _t34_80;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_81 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_29, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_29, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_82 = _mm256_blend_pd(_mm256_unpacklo_pd(_t34_24, _t34_25), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_83 = _t30_2;

  // 4-BLAC: (1x4)^T
  _t34_84 = _t34_83;

  // 4-BLAC: 1x4 * 4x1
  _t34_85 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_82, _t34_84), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_82, _t34_84), _mm256_mul_pd(_t34_82, _t34_84), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_82, _t34_84), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_82, _t34_84), _mm256_mul_pd(_t34_82, _t34_84), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_82, _t34_84), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_82, _t34_84), _mm256_mul_pd(_t34_82, _t34_84), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_86 = _mm256_sub_pd(_t34_81, _t34_85);

  // AVX Storer:
  _t34_26 = _t34_86;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_87 = _t34_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_88 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_89 = _t30_8;

  // 4-BLAC: 1x4 + 1x4
  _t34_90 = _mm256_add_pd(_t34_88, _t34_89);

  // 4-BLAC: 1x4 / 1x4
  _t34_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_87), _mm256_castpd256_pd128(_t34_90)));

  // AVX Storer:
  _t34_26 = _t34_91;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_92 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_29, _t30_29, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_93 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_24, _t34_25), _mm256_unpacklo_pd(_t34_26, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_94 = _t30_0;

  // 4-BLAC: (1x4)^T
  _t34_95 = _t34_94;

  // 4-BLAC: 1x4 * 4x1
  _t34_96 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_93, _t34_95), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_93, _t34_95), _mm256_mul_pd(_t34_93, _t34_95), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_93, _t34_95), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_93, _t34_95), _mm256_mul_pd(_t34_93, _t34_95), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_93, _t34_95), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_93, _t34_95), _mm256_mul_pd(_t34_93, _t34_95), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_97 = _mm256_sub_pd(_t34_92, _t34_96);

  // AVX Storer:
  _t34_27 = _t34_97;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_98 = _t34_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_99 = _t4_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_100 = _t30_6;

  // 4-BLAC: 1x4 + 1x4
  _t34_101 = _mm256_add_pd(_t34_99, _t34_100);

  // 4-BLAC: 1x4 / 1x4
  _t34_102 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_98), _mm256_castpd256_pd128(_t34_101)));

  // AVX Storer:
  _t34_27 = _t34_102;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, 20)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // 3x4 -> 4x4
  _t34_103 = _t30_30;
  _t34_104 = _t30_31;
  _t34_105 = _t30_32;
  _t34_106 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t34_107 = _t4_58;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t34_108 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_107, _t34_107, 32), _mm256_permute2f128_pd(_t34_107, _t34_107, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_24, _t34_25), _mm256_unpacklo_pd(_t34_26, _t34_27), 32));
  _t34_109 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_107, _t34_107, 32), _mm256_permute2f128_pd(_t34_107, _t34_107, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_24, _t34_25), _mm256_unpacklo_pd(_t34_26, _t34_27), 32));
  _t34_110 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_107, _t34_107, 49), _mm256_permute2f128_pd(_t34_107, _t34_107, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_24, _t34_25), _mm256_unpacklo_pd(_t34_26, _t34_27), 32));
  _t34_111 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_107, _t34_107, 49), _mm256_permute2f128_pd(_t34_107, _t34_107, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_24, _t34_25), _mm256_unpacklo_pd(_t34_26, _t34_27), 32));

  // 4-BLAC: 4x4 - 4x4
  _t34_112 = _mm256_sub_pd(_t34_103, _t34_108);
  _t34_113 = _mm256_sub_pd(_t34_104, _t34_109);
  _t34_114 = _mm256_sub_pd(_t34_105, _t34_110);
  _t34_115 = _mm256_sub_pd(_t34_106, _t34_111);

  // AVX Storer:
  _t30_30 = _t34_112;
  _t30_31 = _t34_113;
  _t30_32 = _t34_114;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_116 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_30, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_117 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_118 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t34_119 = _mm256_add_pd(_t34_117, _t34_118);

  // 4-BLAC: 1x4 / 1x4
  _t34_120 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_116), _mm256_castpd256_pd128(_t34_119)));

  // AVX Storer:
  _t34_28 = _t34_120;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_121 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_30, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_122 = _t34_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_123 = _t30_5;

  // 4-BLAC: (4x1)^T
  _t34_124 = _t34_123;

  // 4-BLAC: 1x4 Kro 1x4
  _t34_125 = _mm256_mul_pd(_t34_122, _t34_124);

  // 4-BLAC: 1x4 - 1x4
  _t34_126 = _mm256_sub_pd(_t34_121, _t34_125);

  // AVX Storer:
  _t34_29 = _t34_126;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_127 = _t34_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_128 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_129 = _t30_10;

  // 4-BLAC: 1x4 + 1x4
  _t34_130 = _mm256_add_pd(_t34_128, _t34_129);

  // 4-BLAC: 1x4 / 1x4
  _t34_131 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_127), _mm256_castpd256_pd128(_t34_130)));

  // AVX Storer:
  _t34_29 = _t34_131;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_132 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_30, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_30, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_133 = _mm256_blend_pd(_mm256_unpacklo_pd(_t34_28, _t34_29), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_134 = _t30_2;

  // 4-BLAC: (1x4)^T
  _t34_135 = _t34_134;

  // 4-BLAC: 1x4 * 4x1
  _t34_136 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_133, _t34_135), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_133, _t34_135), _mm256_mul_pd(_t34_133, _t34_135), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_133, _t34_135), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_133, _t34_135), _mm256_mul_pd(_t34_133, _t34_135), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_133, _t34_135), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_133, _t34_135), _mm256_mul_pd(_t34_133, _t34_135), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_137 = _mm256_sub_pd(_t34_132, _t34_136);

  // AVX Storer:
  _t34_30 = _t34_137;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_138 = _t34_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_139 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_140 = _t30_8;

  // 4-BLAC: 1x4 + 1x4
  _t34_141 = _mm256_add_pd(_t34_139, _t34_140);

  // 4-BLAC: 1x4 / 1x4
  _t34_142 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_138), _mm256_castpd256_pd128(_t34_141)));

  // AVX Storer:
  _t34_30 = _t34_142;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_143 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_30, _t30_30, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_144 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_28, _t34_29), _mm256_unpacklo_pd(_t34_30, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_145 = _t30_0;

  // 4-BLAC: (1x4)^T
  _t34_146 = _t34_145;

  // 4-BLAC: 1x4 * 4x1
  _t34_147 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_144, _t34_146), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_144, _t34_146), _mm256_mul_pd(_t34_144, _t34_146), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_144, _t34_146), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_144, _t34_146), _mm256_mul_pd(_t34_144, _t34_146), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_144, _t34_146), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_144, _t34_146), _mm256_mul_pd(_t34_144, _t34_146), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_148 = _mm256_sub_pd(_t34_143, _t34_147);

  // AVX Storer:
  _t34_31 = _t34_148;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_149 = _t34_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_150 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_151 = _t30_6;

  // 4-BLAC: 1x4 + 1x4
  _t34_152 = _mm256_add_pd(_t34_150, _t34_151);

  // 4-BLAC: 1x4 / 1x4
  _t34_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_149), _mm256_castpd256_pd128(_t34_152)));

  // AVX Storer:
  _t34_31 = _t34_153;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, 20)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // 2x4 -> 4x4
  _t34_154 = _t30_31;
  _t34_155 = _t30_32;
  _t34_156 = _mm256_setzero_pd();
  _t34_157 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t34_158 = _t4_56;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t34_159 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_158, _t34_158, 32), _mm256_permute2f128_pd(_t34_158, _t34_158, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_28, _t34_29), _mm256_unpacklo_pd(_t34_30, _t34_31), 32));
  _t34_160 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_158, _t34_158, 32), _mm256_permute2f128_pd(_t34_158, _t34_158, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_28, _t34_29), _mm256_unpacklo_pd(_t34_30, _t34_31), 32));
  _t34_161 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_158, _t34_158, 49), _mm256_permute2f128_pd(_t34_158, _t34_158, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_28, _t34_29), _mm256_unpacklo_pd(_t34_30, _t34_31), 32));
  _t34_162 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_158, _t34_158, 49), _mm256_permute2f128_pd(_t34_158, _t34_158, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_28, _t34_29), _mm256_unpacklo_pd(_t34_30, _t34_31), 32));

  // 4-BLAC: 4x4 - 4x4
  _t34_163 = _mm256_sub_pd(_t34_154, _t34_159);
  _t34_164 = _mm256_sub_pd(_t34_155, _t34_160);
  _t34_165 = _mm256_sub_pd(_t34_156, _t34_161);
  _t34_166 = _mm256_sub_pd(_t34_157, _t34_162);

  // AVX Storer:
  _t30_31 = _t34_163;
  _t30_32 = _t34_164;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_167 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_31, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_168 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_169 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t34_170 = _mm256_add_pd(_t34_168, _t34_169);

  // 4-BLAC: 1x4 / 1x4
  _t34_171 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_167), _mm256_castpd256_pd128(_t34_170)));

  // AVX Storer:
  _t34_32 = _t34_171;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_172 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_31, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_173 = _t34_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_174 = _t30_5;

  // 4-BLAC: (4x1)^T
  _t34_175 = _t34_174;

  // 4-BLAC: 1x4 Kro 1x4
  _t34_176 = _mm256_mul_pd(_t34_173, _t34_175);

  // 4-BLAC: 1x4 - 1x4
  _t34_177 = _mm256_sub_pd(_t34_172, _t34_176);

  // AVX Storer:
  _t34_33 = _t34_177;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_178 = _t34_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_179 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_180 = _t30_10;

  // 4-BLAC: 1x4 + 1x4
  _t34_181 = _mm256_add_pd(_t34_179, _t34_180);

  // 4-BLAC: 1x4 / 1x4
  _t34_182 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_178), _mm256_castpd256_pd128(_t34_181)));

  // AVX Storer:
  _t34_33 = _t34_182;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_183 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_31, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_31, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_184 = _mm256_blend_pd(_mm256_unpacklo_pd(_t34_32, _t34_33), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_185 = _t30_2;

  // 4-BLAC: (1x4)^T
  _t34_186 = _t34_185;

  // 4-BLAC: 1x4 * 4x1
  _t34_187 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_184, _t34_186), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_184, _t34_186), _mm256_mul_pd(_t34_184, _t34_186), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_184, _t34_186), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_184, _t34_186), _mm256_mul_pd(_t34_184, _t34_186), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_184, _t34_186), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_184, _t34_186), _mm256_mul_pd(_t34_184, _t34_186), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_188 = _mm256_sub_pd(_t34_183, _t34_187);

  // AVX Storer:
  _t34_34 = _t34_188;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_189 = _t34_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_190 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_191 = _t30_8;

  // 4-BLAC: 1x4 + 1x4
  _t34_192 = _mm256_add_pd(_t34_190, _t34_191);

  // 4-BLAC: 1x4 / 1x4
  _t34_193 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_189), _mm256_castpd256_pd128(_t34_192)));

  // AVX Storer:
  _t34_34 = _t34_193;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_194 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_31, _t30_31, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_195 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_32, _t34_33), _mm256_unpacklo_pd(_t34_34, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_196 = _t30_0;

  // 4-BLAC: (1x4)^T
  _t34_197 = _t34_196;

  // 4-BLAC: 1x4 * 4x1
  _t34_198 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_195, _t34_197), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_195, _t34_197), _mm256_mul_pd(_t34_195, _t34_197), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_195, _t34_197), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_195, _t34_197), _mm256_mul_pd(_t34_195, _t34_197), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_195, _t34_197), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_195, _t34_197), _mm256_mul_pd(_t34_195, _t34_197), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_199 = _mm256_sub_pd(_t34_194, _t34_198);

  // AVX Storer:
  _t34_35 = _t34_199;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_200 = _t34_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_201 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_202 = _t30_6;

  // 4-BLAC: 1x4 + 1x4
  _t34_203 = _mm256_add_pd(_t34_201, _t34_202);

  // 4-BLAC: 1x4 / 1x4
  _t34_204 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_200), _mm256_castpd256_pd128(_t34_203)));

  // AVX Storer:
  _t34_35 = _t34_204;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, 20)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, 20)) ) ),h(4, 28, 20))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_205 = _t4_54;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t34_44 = _mm256_mul_pd(_t34_205, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_32, _t34_33), _mm256_unpacklo_pd(_t34_34, _t34_35), 32));

  // 4-BLAC: 1x4 - 1x4
  _t30_32 = _mm256_sub_pd(_t30_32, _t34_44);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 20)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 20), L[28,28],h(1, 28, 20)) ) ),h(1, 28, 20))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_206 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_32, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_207 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_208 = _t30_12;

  // 4-BLAC: 1x4 + 1x4
  _t34_209 = _mm256_add_pd(_t34_207, _t34_208);

  // 4-BLAC: 1x4 / 1x4
  _t34_210 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_206), _mm256_castpd256_pd128(_t34_209)));

  // AVX Storer:
  _t34_36 = _t34_210;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 21)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 20)) Kro T( G(h(1, 28, 21), L[28,28],h(1, 28, 20)) ) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_211 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_32, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_212 = _t34_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_213 = _t30_5;

  // 4-BLAC: (4x1)^T
  _t34_214 = _t34_213;

  // 4-BLAC: 1x4 Kro 1x4
  _t34_215 = _mm256_mul_pd(_t34_212, _t34_214);

  // 4-BLAC: 1x4 - 1x4
  _t34_216 = _mm256_sub_pd(_t34_211, _t34_215);

  // AVX Storer:
  _t34_37 = _t34_216;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 21)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 21), L[28,28],h(1, 28, 21)) ) ),h(1, 28, 21))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_217 = _t34_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_218 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_219 = _t30_10;

  // 4-BLAC: 1x4 + 1x4
  _t34_220 = _mm256_add_pd(_t34_218, _t34_219);

  // 4-BLAC: 1x4 / 1x4
  _t34_221 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_217), _mm256_castpd256_pd128(_t34_220)));

  // AVX Storer:
  _t34_37 = _t34_221;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 22)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 20)) * T( G(h(1, 28, 22), L[28,28],h(2, 28, 20)) ) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_222 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_32, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_32, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_223 = _mm256_blend_pd(_mm256_unpacklo_pd(_t34_36, _t34_37), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t34_224 = _t30_2;

  // 4-BLAC: (1x4)^T
  _t34_225 = _t34_224;

  // 4-BLAC: 1x4 * 4x1
  _t34_226 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_223, _t34_225), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_223, _t34_225), _mm256_mul_pd(_t34_223, _t34_225), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_223, _t34_225), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_223, _t34_225), _mm256_mul_pd(_t34_223, _t34_225), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_223, _t34_225), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_223, _t34_225), _mm256_mul_pd(_t34_223, _t34_225), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_227 = _mm256_sub_pd(_t34_222, _t34_226);

  // AVX Storer:
  _t34_38 = _t34_227;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 22)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 22), L[28,28],h(1, 28, 22)) ) ),h(1, 28, 22))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_228 = _t34_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_229 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_230 = _t30_8;

  // 4-BLAC: 1x4 + 1x4
  _t34_231 = _mm256_add_pd(_t34_229, _t34_230);

  // 4-BLAC: 1x4 / 1x4
  _t34_232 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_228), _mm256_castpd256_pd128(_t34_231)));

  // AVX Storer:
  _t34_38 = _t34_232;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 23)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, 20)) * T( G(h(1, 28, 23), L[28,28],h(3, 28, 20)) ) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_233 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_32, _t30_32, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_234 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_36, _t34_37), _mm256_unpacklo_pd(_t34_38, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t34_235 = _t30_0;

  // 4-BLAC: (1x4)^T
  _t34_236 = _t34_235;

  // 4-BLAC: 1x4 * 4x1
  _t34_237 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_234, _t34_236), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_234, _t34_236), _mm256_mul_pd(_t34_234, _t34_236), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_234, _t34_236), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_234, _t34_236), _mm256_mul_pd(_t34_234, _t34_236), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_234, _t34_236), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_234, _t34_236), _mm256_mul_pd(_t34_234, _t34_236), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t34_238 = _mm256_sub_pd(_t34_233, _t34_237);

  // AVX Storer:
  _t34_39 = _t34_238;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 23)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 23), L[28,28],h(1, 28, 23)) ) ),h(1, 28, 23))

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_239 = _t34_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_240 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t34_241 = _t30_6;

  // 4-BLAC: 1x4 + 1x4
  _t34_242 = _mm256_add_pd(_t34_240, _t34_241);

  // 4-BLAC: 1x4 / 1x4
  _t34_243 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t34_239), _mm256_castpd256_pd128(_t34_242)));

  // AVX Storer:
  _t34_39 = _t34_243;

  // Generating : X[28,28] = ( ( S(h(4, 28, 24), ( G(h(4, 28, 24), C[28,28],h(4, 28, 24)) - ( ( G(h(4, 28, 24), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, 24)) + Sum_{i217} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), L[28,28],h(4, 28, i217)) * T( G(h(4, 28, 24), X[28,28],h(4, 28, i217)) ) ),h(4, 28, 24)) ) ) + Sum_{i100} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, 24), L[28,28],h(4, 28, i100)) ) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t34_244 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t34_40, _t34_41, 0), _mm256_shuffle_pd(_t34_42, _t34_43, 0), 32);
  _t34_245 = _mm256_permute2f128_pd(_t34_41, _mm256_shuffle_pd(_t34_42, _t34_43, 3), 32);
  _t34_246 = _mm256_blend_pd(_t34_42, _mm256_shuffle_pd(_t34_42, _t34_43, 3), 12);
  _t34_247 = _t34_43;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t34_248 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _t4_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_79, _t4_80), _mm256_unpacklo_pd(_t4_81, _t4_82), 32)), 32);
  _t34_249 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _t4_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_79, _t4_80), _mm256_unpacklo_pd(_t4_81, _t4_82), 32)), 32);
  _t34_250 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _t4_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_79, _t4_80), _mm256_unpacklo_pd(_t4_81, _t4_82), 32)), 49);
  _t34_251 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_64, _t4_65), _mm256_unpacklo_pd(_t4_66, _t4_67), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_71, _t4_72), _mm256_unpacklo_pd(_t4_73, _t4_74), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_75, _t4_76), _mm256_unpacklo_pd(_t4_77, _t4_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_79, _t4_80), _mm256_unpacklo_pd(_t4_81, _t4_82), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t34_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_15, _t34_248), _mm256_mul_pd(_t32_14, _t34_249)), _mm256_add_pd(_mm256_mul_pd(_t32_13, _t34_250), _mm256_mul_pd(_t32_12, _t34_251)));
  _t34_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_11, _t34_248), _mm256_mul_pd(_t32_10, _t34_249)), _mm256_add_pd(_mm256_mul_pd(_t32_9, _t34_250), _mm256_mul_pd(_t32_8, _t34_251)));
  _t34_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_7, _t34_248), _mm256_mul_pd(_t32_6, _t34_249)), _mm256_add_pd(_mm256_mul_pd(_t32_5, _t34_250), _mm256_mul_pd(_t32_4, _t34_251)));
  _t34_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_3, _t34_248), _mm256_mul_pd(_t32_2, _t34_249)), _mm256_add_pd(_mm256_mul_pd(_t32_1, _t34_250), _mm256_mul_pd(_t32_0, _t34_251)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t34_252 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
  _t34_253 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
  _t34_254 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
  _t34_255 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t34_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_64, _t4_64, 32), _mm256_permute2f128_pd(_t4_64, _t4_64, 32), 0), _t34_252), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_65, _t4_65, 32), _mm256_permute2f128_pd(_t4_65, _t4_65, 32), 0), _t34_253)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_66, _t4_66, 32), _mm256_permute2f128_pd(_t4_66, _t4_66, 32), 0), _t34_254), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_67, _t4_67, 32), _mm256_permute2f128_pd(_t4_67, _t4_67, 32), 0), _t34_255)));
  _t34_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_71, _t4_71, 32), _mm256_permute2f128_pd(_t4_71, _t4_71, 32), 0), _t34_252), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_72, _t4_72, 32), _mm256_permute2f128_pd(_t4_72, _t4_72, 32), 0), _t34_253)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_73, _t4_73, 32), _mm256_permute2f128_pd(_t4_73, _t4_73, 32), 0), _t34_254), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_74, _t4_74, 32), _mm256_permute2f128_pd(_t4_74, _t4_74, 32), 0), _t34_255)));
  _t34_55 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_75, _t4_75, 32), _mm256_permute2f128_pd(_t4_75, _t4_75, 32), 0), _t34_252), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_76, _t4_76, 32), _mm256_permute2f128_pd(_t4_76, _t4_76, 32), 0), _t34_253)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_77, _t4_77, 32), _mm256_permute2f128_pd(_t4_77, _t4_77, 32), 0), _t34_254), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_78, _t4_78, 32), _mm256_permute2f128_pd(_t4_78, _t4_78, 32), 0), _t34_255)));
  _t34_56 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_79, _t4_79, 32), _mm256_permute2f128_pd(_t4_79, _t4_79, 32), 0), _t34_252), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_80, _t4_80, 32), _mm256_permute2f128_pd(_t4_80, _t4_80, 32), 0), _t34_253)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_81, _t4_81, 32), _mm256_permute2f128_pd(_t4_81, _t4_81, 32), 0), _t34_254), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_82, _t4_82, 32), _mm256_permute2f128_pd(_t4_82, _t4_82, 32), 0), _t34_255)));

  // 4-BLAC: 4x4 + 4x4
  _t34_20 = _mm256_add_pd(_t34_49, _t34_53);
  _t34_21 = _mm256_add_pd(_t34_50, _t34_54);
  _t34_22 = _mm256_add_pd(_t34_51, _t34_55);
  _t34_23 = _mm256_add_pd(_t34_52, _t34_56);

  // 4-BLAC: 4x4 - 4x4
  _t34_57 = _mm256_sub_pd(_t34_244, _t34_20);
  _t34_58 = _mm256_sub_pd(_t34_245, _t34_21);
  _t34_59 = _mm256_sub_pd(_t34_246, _t34_22);
  _t34_60 = _mm256_sub_pd(_t34_247, _t34_23);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t34_40 = _t34_57;
  _t34_41 = _t34_58;
  _t34_42 = _t34_59;
  _t34_43 = _t34_60;

  _mm_store_sd(&(C[692]), _mm256_castpd256_pd128(_t34_24));
  _mm_store_sd(&(C[693]), _mm256_castpd256_pd128(_t34_25));
  _mm_store_sd(&(C[694]), _mm256_castpd256_pd128(_t34_26));
  _mm_store_sd(&(C[695]), _mm256_castpd256_pd128(_t34_27));
  _mm_store_sd(&(C[720]), _mm256_castpd256_pd128(_t34_28));
  _mm_store_sd(&(C[721]), _mm256_castpd256_pd128(_t34_29));
  _mm_store_sd(&(C[722]), _mm256_castpd256_pd128(_t34_30));
  _mm_store_sd(&(C[723]), _mm256_castpd256_pd128(_t34_31));
  _mm_store_sd(&(C[748]), _mm256_castpd256_pd128(_t34_32));
  _mm_store_sd(&(C[749]), _mm256_castpd256_pd128(_t34_33));
  _mm_store_sd(&(C[750]), _mm256_castpd256_pd128(_t34_34));
  _mm_store_sd(&(C[751]), _mm256_castpd256_pd128(_t34_35));
  _mm_store_sd(&(C[776]), _mm256_castpd256_pd128(_t34_36));
  _mm_store_sd(&(C[777]), _mm256_castpd256_pd128(_t34_37));
  _mm_store_sd(&(C[778]), _mm256_castpd256_pd128(_t34_38));
  _mm_store_sd(&(C[779]), _mm256_castpd256_pd128(_t34_39));

  for( int i217 = 4; i217 <= 23; i217+=4 ) {
    _t35_19 = _mm256_broadcast_sd(L + i217 + 672);
    _t35_18 = _mm256_broadcast_sd(L + i217 + 673);
    _t35_17 = _mm256_broadcast_sd(L + i217 + 674);
    _t35_16 = _mm256_broadcast_sd(L + i217 + 675);
    _t35_15 = _mm256_broadcast_sd(L + i217 + 700);
    _t35_14 = _mm256_broadcast_sd(L + i217 + 701);
    _t35_13 = _mm256_broadcast_sd(L + i217 + 702);
    _t35_12 = _mm256_broadcast_sd(L + i217 + 703);
    _t35_11 = _mm256_broadcast_sd(L + i217 + 728);
    _t35_10 = _mm256_broadcast_sd(L + i217 + 729);
    _t35_9 = _mm256_broadcast_sd(L + i217 + 730);
    _t35_8 = _mm256_broadcast_sd(L + i217 + 731);
    _t35_7 = _mm256_broadcast_sd(L + i217 + 756);
    _t35_6 = _mm256_broadcast_sd(L + i217 + 757);
    _t35_5 = _mm256_broadcast_sd(L + i217 + 758);
    _t35_4 = _mm256_broadcast_sd(L + i217 + 759);
    _t35_3 = _asm256_loadu_pd(C + i217 + 672);
    _t35_2 = _asm256_loadu_pd(C + i217 + 700);
    _t35_1 = _asm256_loadu_pd(C + i217 + 728);
    _t35_0 = _asm256_loadu_pd(C + i217 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t35_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
    _t35_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
    _t35_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
    _t35_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t35_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_28), _mm256_mul_pd(_t35_18, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_30), _mm256_mul_pd(_t35_16, _t35_31)));
    _t35_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_28), _mm256_mul_pd(_t35_14, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_30), _mm256_mul_pd(_t35_12, _t35_31)));
    _t35_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_28), _mm256_mul_pd(_t35_10, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_30), _mm256_mul_pd(_t35_8, _t35_31)));
    _t35_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_28), _mm256_mul_pd(_t35_6, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_30), _mm256_mul_pd(_t35_4, _t35_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t35_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t34_40, _t34_41, 0), _mm256_shuffle_pd(_t34_42, _t34_43, 0), 32);
    _t35_25 = _mm256_permute2f128_pd(_t34_41, _mm256_shuffle_pd(_t34_42, _t34_43, 3), 32);
    _t35_26 = _mm256_blend_pd(_t34_42, _mm256_shuffle_pd(_t34_42, _t34_43, 3), 12);
    _t35_27 = _t34_43;

    // 4-BLAC: 4x4 - 4x4
    _t35_24 = _mm256_sub_pd(_t35_24, _t35_20);
    _t35_25 = _mm256_sub_pd(_t35_25, _t35_21);
    _t35_26 = _mm256_sub_pd(_t35_26, _t35_22);
    _t35_27 = _mm256_sub_pd(_t35_27, _t35_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t34_40 = _t35_24;
    _t34_41 = _t35_25;
    _t34_42 = _t35_26;
    _t34_43 = _t35_27;
  }


  for( int i100 = 4; i100 <= 23; i100+=4 ) {
    _t36_19 = _mm256_broadcast_sd(C + i100 + 672);
    _t36_18 = _mm256_broadcast_sd(C + i100 + 673);
    _t36_17 = _mm256_broadcast_sd(C + i100 + 674);
    _t36_16 = _mm256_broadcast_sd(C + i100 + 675);
    _t36_15 = _mm256_broadcast_sd(C + i100 + 700);
    _t36_14 = _mm256_broadcast_sd(C + i100 + 701);
    _t36_13 = _mm256_broadcast_sd(C + i100 + 702);
    _t36_12 = _mm256_broadcast_sd(C + i100 + 703);
    _t36_11 = _mm256_broadcast_sd(C + i100 + 728);
    _t36_10 = _mm256_broadcast_sd(C + i100 + 729);
    _t36_9 = _mm256_broadcast_sd(C + i100 + 730);
    _t36_8 = _mm256_broadcast_sd(C + i100 + 731);
    _t36_7 = _mm256_broadcast_sd(C + i100 + 756);
    _t36_6 = _mm256_broadcast_sd(C + i100 + 757);
    _t36_5 = _mm256_broadcast_sd(C + i100 + 758);
    _t36_4 = _mm256_broadcast_sd(C + i100 + 759);
    _t36_3 = _asm256_loadu_pd(L + i100 + 672);
    _t36_2 = _asm256_loadu_pd(L + i100 + 700);
    _t36_1 = _asm256_loadu_pd(L + i100 + 728);
    _t36_0 = _asm256_loadu_pd(L + i100 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t36_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_3, _t36_2), _mm256_unpacklo_pd(_t36_1, _t36_0), 32);
    _t36_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_3, _t36_2), _mm256_unpackhi_pd(_t36_1, _t36_0), 32);
    _t36_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_3, _t36_2), _mm256_unpacklo_pd(_t36_1, _t36_0), 49);
    _t36_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_3, _t36_2), _mm256_unpackhi_pd(_t36_1, _t36_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t36_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_19, _t36_28), _mm256_mul_pd(_t36_18, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_17, _t36_30), _mm256_mul_pd(_t36_16, _t36_31)));
    _t36_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_15, _t36_28), _mm256_mul_pd(_t36_14, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_13, _t36_30), _mm256_mul_pd(_t36_12, _t36_31)));
    _t36_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_11, _t36_28), _mm256_mul_pd(_t36_10, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_9, _t36_30), _mm256_mul_pd(_t36_8, _t36_31)));
    _t36_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_7, _t36_28), _mm256_mul_pd(_t36_6, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_5, _t36_30), _mm256_mul_pd(_t36_4, _t36_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t36_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t34_40, _t34_41, 0), _mm256_shuffle_pd(_t34_42, _t34_43, 0), 32);
    _t36_25 = _mm256_permute2f128_pd(_t34_41, _mm256_shuffle_pd(_t34_42, _t34_43, 3), 32);
    _t36_26 = _mm256_blend_pd(_t34_42, _mm256_shuffle_pd(_t34_42, _t34_43, 3), 12);
    _t36_27 = _t34_43;

    // 4-BLAC: 4x4 - 4x4
    _t36_24 = _mm256_sub_pd(_t36_24, _t36_20);
    _t36_25 = _mm256_sub_pd(_t36_25, _t36_21);
    _t36_26 = _mm256_sub_pd(_t36_26, _t36_22);
    _t36_27 = _mm256_sub_pd(_t36_27, _t36_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t34_40 = _t36_24;
    _t34_41 = _t36_25;
    _t34_42 = _t36_26;
    _t34_43 = _t36_27;
  }

  _t37_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[782])));
  _t37_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[724])));
  _t37_4 = _mm256_broadcast_sd(&(L[724]));
  _t37_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 752)), _mm256_castpd128_pd256(_mm_load_sd(L + 780)), 0);
  _t37_2 = _mm256_maskload_pd(L + 752, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t37_1 = _mm256_maskload_pd(L + 780, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t37_0 = _mm256_maskload_pd(L + 780, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 24)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_19 = _t34_40;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t37_20 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_21 = _t4_59;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_22 = _mm256_mul_pd(_t37_20, _t37_21);

  // 4-BLAC: 1x4 / 1x4
  _t37_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_19), _mm256_castpd256_pd128(_t37_22)));

  // AVX Storer:
  _t34_40 = _t37_23;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(1, 28, 24)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) Kro G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 3x1 -> 4x1
  _t37_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_41, _t34_42), _mm256_unpacklo_pd(_t34_43, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t37_25 = _t4_58;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_40, _t34_40, 32), _mm256_permute2f128_pd(_t34_40, _t34_40, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t37_27 = _mm256_mul_pd(_t37_25, _t37_26);

  // 4-BLAC: 4x1 - 4x1
  _t37_28 = _mm256_sub_pd(_t37_24, _t37_27);

  // AVX Storer:
  _t37_7 = _t37_28;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t37_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_30 = _t4_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_31 = _t4_59;

  // 4-BLAC: 1x4 + 1x4
  _t37_32 = _mm256_add_pd(_t37_30, _t37_31);

  // 4-BLAC: 1x4 / 1x4
  _t37_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_29), _mm256_castpd256_pd128(_t37_32)));

  // AVX Storer:
  _t37_8 = _t37_33;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 24)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) Kro G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t37_7, 2), _mm256_permute2f128_pd(_t37_7, _t37_7, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_35 = _t4_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_8, _t37_8, 32), _mm256_permute2f128_pd(_t37_8, _t37_8, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t37_37 = _mm256_mul_pd(_t37_35, _t37_36);

  // 4-BLAC: 4x1 - 4x1
  _t37_38 = _mm256_sub_pd(_t37_34, _t37_37);

  // AVX Storer:
  _t37_9 = _t37_38;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_39 = _mm256_blend_pd(_mm256_setzero_pd(), _t37_9, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_40 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_41 = _t4_59;

  // 4-BLAC: 1x4 + 1x4
  _t37_42 = _mm256_add_pd(_t37_40, _t37_41);

  // 4-BLAC: 1x4 / 1x4
  _t37_43 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_39), _mm256_castpd256_pd128(_t37_42)));

  // AVX Storer:
  _t37_10 = _t37_43;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 24)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_44 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t37_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_45 = _t37_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_46 = _t37_10;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_47 = _mm256_mul_pd(_t37_45, _t37_46);

  // 4-BLAC: 1x4 - 1x4
  _t37_48 = _mm256_sub_pd(_t37_44, _t37_47);

  // AVX Storer:
  _t37_11 = _t37_48;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_49 = _t37_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_50 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_51 = _t4_59;

  // 4-BLAC: 1x4 + 1x4
  _t37_52 = _mm256_add_pd(_t37_50, _t37_51);

  // 4-BLAC: 1x4 / 1x4
  _t37_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_49), _mm256_castpd256_pd128(_t37_52)));

  // AVX Storer:
  _t37_11 = _t37_53;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 25)) - ( ( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) + ( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_54 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t34_41, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_55 = _t37_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_56 = _t37_8;

  // 4-BLAC: (4x1)^T
  _t37_57 = _t37_56;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_58 = _mm256_mul_pd(_t37_55, _t37_57);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_59 = _t37_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_60 = _t37_5;

  // 4-BLAC: (4x1)^T
  _t37_61 = _t37_60;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_62 = _mm256_mul_pd(_t37_59, _t37_61);

  // 4-BLAC: 1x4 + 1x4
  _t37_63 = _mm256_add_pd(_t37_58, _t37_62);

  // 4-BLAC: 1x4 - 1x4
  _t37_64 = _mm256_sub_pd(_t37_54, _t37_63);

  // AVX Storer:
  _t37_12 = _t37_64;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 25)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_65 = _t37_12;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t37_66 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_67 = _t4_57;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_68 = _mm256_mul_pd(_t37_66, _t37_67);

  // 4-BLAC: 1x4 / 1x4
  _t37_69 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_65), _mm256_castpd256_pd128(_t37_68)));

  // AVX Storer:
  _t37_12 = _t37_69;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(2, 28, 26), X[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_70 = _mm256_unpackhi_pd(_mm256_blend_pd(_t34_42, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t34_43, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_71 = _mm256_blend_pd(_mm256_unpacklo_pd(_t37_10, _t37_11), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_72 = _t37_4;

  // 4-BLAC: (4x1)^T
  _t37_73 = _t37_72;

  // 4-BLAC: 4x1 Kro 1x4
  _t37_74 = _mm256_mul_pd(_t37_71, _t37_73);

  // 4-BLAC: 4x1 - 4x1
  _t37_75 = _mm256_sub_pd(_t37_70, _t37_74);

  // AVX Storer:
  _t37_13 = _t37_75;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_76 = _t37_13;

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_77 = _t37_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_78 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_8, _t37_8, 32), _mm256_permute2f128_pd(_t37_8, _t37_8, 32), 0);

  // 4-BLAC: (4x1)^T
  _t37_79 = _t37_78;

  // 4-BLAC: 4x1 Kro 1x4
  _t37_80 = _mm256_mul_pd(_t37_77, _t37_79);

  // 4-BLAC: 4x1 - 4x1
  _t37_81 = _mm256_sub_pd(_t37_76, _t37_80);

  // AVX Storer:
  _t37_13 = _t37_81;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) Kro G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_82 = _t37_13;

  // AVX Loader:

  // 2x1 -> 4x1
  _t37_83 = _t4_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_84 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t37_85 = _mm256_mul_pd(_t37_83, _t37_84);

  // 4-BLAC: 4x1 - 4x1
  _t37_86 = _mm256_sub_pd(_t37_82, _t37_85);

  // AVX Storer:
  _t37_13 = _t37_86;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 25)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_87 = _mm256_blend_pd(_mm256_setzero_pd(), _t37_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_88 = _t4_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_89 = _t4_57;

  // 4-BLAC: 1x4 + 1x4
  _t37_90 = _mm256_add_pd(_t37_88, _t37_89);

  // 4-BLAC: 1x4 / 1x4
  _t37_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_87), _mm256_castpd256_pd128(_t37_90)));

  // AVX Storer:
  _t37_14 = _t37_91;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 25)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_92 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t37_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_93 = _t37_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_94 = _t37_14;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_95 = _mm256_mul_pd(_t37_93, _t37_94);

  // 4-BLAC: 1x4 - 1x4
  _t37_96 = _mm256_sub_pd(_t37_92, _t37_95);

  // AVX Storer:
  _t37_15 = _t37_96;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 25)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_97 = _t37_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_98 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_99 = _t4_57;

  // 4-BLAC: 1x4 + 1x4
  _t37_100 = _mm256_add_pd(_t37_98, _t37_99);

  // 4-BLAC: 1x4 / 1x4
  _t37_101 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_97), _mm256_castpd256_pd128(_t37_100)));

  // AVX Storer:
  _t37_15 = _t37_101;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 26)) - ( ( G(h(1, 28, 26), L[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) + ( G(h(1, 28, 26), X[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_102 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t34_42, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t34_42, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_103 = _t37_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_104 = _mm256_blend_pd(_mm256_unpacklo_pd(_t37_10, _t37_14), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t37_105 = _t37_104;

  // 4-BLAC: 1x4 * 4x1
  _t37_106 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_103, _t37_105), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_103, _t37_105), _mm256_mul_pd(_t37_103, _t37_105), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_103, _t37_105), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_103, _t37_105), _mm256_mul_pd(_t37_103, _t37_105), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_103, _t37_105), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_103, _t37_105), _mm256_mul_pd(_t37_103, _t37_105), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_107 = _mm256_blend_pd(_mm256_unpacklo_pd(_t37_10, _t37_14), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_108 = _t37_2;

  // 4-BLAC: (1x4)^T
  _t37_109 = _t37_108;

  // 4-BLAC: 1x4 * 4x1
  _t37_110 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_107, _t37_109), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_107, _t37_109), _mm256_mul_pd(_t37_107, _t37_109), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_107, _t37_109), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_107, _t37_109), _mm256_mul_pd(_t37_107, _t37_109), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_107, _t37_109), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_107, _t37_109), _mm256_mul_pd(_t37_107, _t37_109), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t37_111 = _mm256_add_pd(_t37_106, _t37_110);

  // 4-BLAC: 1x4 - 1x4
  _t37_112 = _mm256_sub_pd(_t37_102, _t37_111);

  // AVX Storer:
  _t37_16 = _t37_112;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 26)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_113 = _t37_16;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t37_114 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_115 = _t4_55;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_116 = _mm256_mul_pd(_t37_114, _t37_115);

  // 4-BLAC: 1x4 / 1x4
  _t37_117 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_113), _mm256_castpd256_pd128(_t37_116)));

  // AVX Storer:
  _t37_16 = _t37_117;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_118 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t34_43, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t34_43, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_119 = _mm256_blend_pd(_mm256_unpacklo_pd(_t37_11, _t37_15), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_120 = _t37_2;

  // 4-BLAC: (1x4)^T
  _t37_121 = _t37_120;

  // 4-BLAC: 1x4 * 4x1
  _t37_122 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_119, _t37_121), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_119, _t37_121), _mm256_mul_pd(_t37_119, _t37_121), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_119, _t37_121), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_119, _t37_121), _mm256_mul_pd(_t37_119, _t37_121), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_119, _t37_121), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_119, _t37_121), _mm256_mul_pd(_t37_119, _t37_121), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t37_123 = _mm256_sub_pd(_t37_118, _t37_122);

  // AVX Storer:
  _t37_17 = _t37_123;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), L[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_124 = _t37_17;

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_125 = _t37_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t37_126 = _mm256_blend_pd(_mm256_unpacklo_pd(_t37_10, _t37_14), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t37_127 = _t37_126;

  // 4-BLAC: 1x4 * 4x1
  _t37_128 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_125, _t37_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_125, _t37_127), _mm256_mul_pd(_t37_125, _t37_127), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_125, _t37_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_125, _t37_127), _mm256_mul_pd(_t37_125, _t37_127), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_125, _t37_127), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_125, _t37_127), _mm256_mul_pd(_t37_125, _t37_127), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t37_129 = _mm256_sub_pd(_t37_124, _t37_128);

  // AVX Storer:
  _t37_17 = _t37_129;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_130 = _t37_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_131 = _t37_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_132 = _t37_16;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_133 = _mm256_mul_pd(_t37_131, _t37_132);

  // 4-BLAC: 1x4 - 1x4
  _t37_134 = _mm256_sub_pd(_t37_130, _t37_133);

  // AVX Storer:
  _t37_17 = _t37_134;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_135 = _t37_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_136 = _t4_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_137 = _t4_55;

  // 4-BLAC: 1x4 + 1x4
  _t37_138 = _mm256_add_pd(_t37_136, _t37_137);

  // 4-BLAC: 1x4 / 1x4
  _t37_139 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_135), _mm256_castpd256_pd128(_t37_138)));

  // AVX Storer:
  _t37_17 = _t37_139;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 27)) - ( ( G(h(1, 28, 27), L[28,28],h(3, 28, 24)) * T( G(h(1, 28, 27), X[28,28],h(3, 28, 24)) ) ) + ( G(h(1, 28, 27), X[28,28],h(3, 28, 24)) * T( G(h(1, 28, 27), L[28,28],h(3, 28, 24)) ) ) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_140 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t34_43, _t34_43, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t37_141 = _t37_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t37_142 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_11, _t37_15), _mm256_unpacklo_pd(_t37_17, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t37_143 = _t37_142;

  // 4-BLAC: 1x4 * 4x1
  _t37_144 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_141, _t37_143), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_141, _t37_143), _mm256_mul_pd(_t37_141, _t37_143), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_141, _t37_143), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_141, _t37_143), _mm256_mul_pd(_t37_141, _t37_143), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_141, _t37_143), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_141, _t37_143), _mm256_mul_pd(_t37_141, _t37_143), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t37_145 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_11, _t37_15), _mm256_unpacklo_pd(_t37_17, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t37_146 = _t37_0;

  // 4-BLAC: (1x4)^T
  _t37_147 = _t37_146;

  // 4-BLAC: 1x4 * 4x1
  _t37_148 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_145, _t37_147), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_145, _t37_147), _mm256_mul_pd(_t37_145, _t37_147), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_145, _t37_147), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_145, _t37_147), _mm256_mul_pd(_t37_145, _t37_147), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_145, _t37_147), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_145, _t37_147), _mm256_mul_pd(_t37_145, _t37_147), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t37_149 = _mm256_add_pd(_t37_144, _t37_148);

  // 4-BLAC: 1x4 - 1x4
  _t37_150 = _mm256_sub_pd(_t37_140, _t37_149);

  // AVX Storer:
  _t37_18 = _t37_150;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 27)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_151 = _t37_18;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t37_152 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t37_153 = _t4_53;

  // 4-BLAC: 1x4 Kro 1x4
  _t37_154 = _mm256_mul_pd(_t37_152, _t37_153);

  // 4-BLAC: 1x4 / 1x4
  _t37_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t37_151), _mm256_castpd256_pd128(_t37_154)));

  // AVX Storer:
  _t37_18 = _t37_155;

  _mm_store_sd(&(C[0]), _mm256_castpd256_pd128(_t0_13));
  _mm_store_sd(&(C[28]), _mm256_castpd256_pd128(_t0_15));
  _mm_store_sd(&(C[56]), _mm256_castpd256_pd128(_t0_17));
  _mm_store_sd(&(C[84]), _mm256_castpd256_pd128(_t0_18));
  _mm_store_sd(&(C[29]), _mm256_castpd256_pd128(_t0_19));
  _mm_store_sd(&(C[57]), _mm256_castpd256_pd128(_t0_21));
  _mm_store_sd(&(C[85]), _mm256_castpd256_pd128(_t0_22));
  _mm_store_sd(&(C[58]), _mm256_castpd256_pd128(_t0_23));
  _mm_store_sd(&(C[86]), _mm256_castpd256_pd128(_t0_24));
  _mm_store_sd(&(C[87]), _mm256_castpd256_pd128(_t0_25));
  _mm_store_sd(C + 116, _mm256_castpd256_pd128(_t4_83));
  _mm_store_sd(&(C[144]), _mm256_castpd256_pd128(_t4_88));
  _mm_store_sd(&(C[172]), _mm256_castpd256_pd128(_t4_90));
  _mm_store_sd(&(C[200]), _mm256_castpd256_pd128(_t4_91));
  _mm_store_sd(&(C[145]), _mm256_castpd256_pd128(_t4_92));
  _mm_store_sd(&(C[173]), _mm256_castpd256_pd128(_t4_94));
  _mm_store_sd(&(C[201]), _mm256_castpd256_pd128(_t4_95));
  _mm_store_sd(&(C[174]), _mm256_castpd256_pd128(_t4_96));
  _mm_store_sd(&(C[202]), _mm256_castpd256_pd128(_t4_97));
  _mm_store_sd(&(C[203]), _mm256_castpd256_pd128(_t4_98));
  _mm_store_sd(C + 580, _mm256_castpd256_pd128(_t27_44));
  _mm_store_sd(&(C[608]), _mm256_castpd256_pd128(_t30_14));
  _mm_store_sd(&(C[636]), _mm256_castpd256_pd128(_t30_16));
  _mm_store_sd(&(C[664]), _mm256_castpd256_pd128(_t30_17));
  _mm_store_sd(&(C[609]), _mm256_castpd256_pd128(_t30_18));
  _mm_store_sd(&(C[637]), _mm256_castpd256_pd128(_t30_20));
  _mm_store_sd(&(C[665]), _mm256_castpd256_pd128(_t30_21));
  _mm_store_sd(&(C[638]), _mm256_castpd256_pd128(_t30_22));
  _mm_store_sd(&(C[666]), _mm256_castpd256_pd128(_t30_23));
  _mm_store_sd(&(C[667]), _mm256_castpd256_pd128(_t30_24));
  _mm_store_sd(C + 696, _mm256_castpd256_pd128(_t34_40));
  _mm_store_sd(&(C[724]), _mm256_castpd256_pd128(_t37_8));
  _mm_store_sd(&(C[752]), _mm256_castpd256_pd128(_t37_10));
  _mm_store_sd(&(C[780]), _mm256_castpd256_pd128(_t37_11));
  _mm_store_sd(&(C[725]), _mm256_castpd256_pd128(_t37_12));
  _mm_store_sd(&(C[753]), _mm256_castpd256_pd128(_t37_14));
  _mm_store_sd(&(C[781]), _mm256_castpd256_pd128(_t37_15));
  _mm_store_sd(&(C[754]), _mm256_castpd256_pd128(_t37_16));
  _mm_store_sd(&(C[782]), _mm256_castpd256_pd128(_t37_17));
  _mm_store_sd(&(C[783]), _mm256_castpd256_pd128(_t37_18));

}
