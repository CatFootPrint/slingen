/*
 * llyap_kernel.h
 *
Decl { {u'X': Symmetric[X, (28, 28), LSMatAccess], u'C': Symmetric[C, (28, 28), LSMatAccess], u'L': LowerTriangular[L, (28, 28), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt': {'m0': 'm02.ll'}, 'ftmpyozk_lwn_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 3, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_Symmetric_Mul_Symmetric_T_LowerTriangular_Symmetric_opt_m02_ftmpyozk_lwn_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), X[28,28],h(1, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), X[28,28],h(1, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), L[28,28],h(1, 28, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), L[28,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), L[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), L[28,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(2, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(2, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(3, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 3), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 4), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 4), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 4), L[28,28],h(4, 28, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 0), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
For_{fi1812;4;23;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 31:
For_{fi2064;4;fi1812 - 4;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(4, 28, fi2064)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(4, 28, fi2064)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(fi2064, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi2064), L[28,28],h(fi2064, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(2, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(3, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, fi2064)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, fi2064)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(4, 28, fi2064)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(2, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(3, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, fi2064)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, fi2064)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(4, 28, fi2064)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(3, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, fi2064)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, fi2064)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(4, 28, fi2064)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, fi2064)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ) )
Eq.ann: {}
 )Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), X[28,28],h(fi1812, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), X[28,28],h(fi1812, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), L[28,28],h(4, 28, fi1812)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(fi1812, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(4, 28, fi1812)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), C[28,28],h(4, 28, fi1812)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), L[28,28],h(fi1812, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(fi1812, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(fi1812, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), L[28,28],h(fi1812, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi1812)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi1812)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812), X[28,28],h(1, 28, fi1812)) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) + Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) ) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) + Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi1812)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi1812)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(2, 28, fi1812)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) ) ) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi1812)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) + Tile( (1, 1), G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 3)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, fi1812)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, fi1812)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ) ) ) )
Eq.ann: {}
Entry 56:
Eq: Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 3)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ) )
Eq.ann: {}
Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), X[28,28],h(4, 28, fi1812)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), C[28,28],h(4, 28, fi1812)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), L[28,28],h(fi1812, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(fi1812, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), X[28,28],h(4, 28, fi1812)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), X[28,28],h(4, 28, fi1812)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1812 + 24, 28, fi1812 + 4), L[28,28],h(4, 28, fi1812)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1812), X[28,28],h(4, 28, fi1812)) ) ) ) )
Eq.ann: {}
 )Entry 25:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(4, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(4, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(4, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 0)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ) )
Eq.ann: {}
Entry 56:
For_{fi2437;4;20;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(4, 28, fi2437)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(4, 28, fi2437)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(fi2437, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi2437), L[28,28],h(fi2437, 28, 0)) ) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, fi2437), L[28,28],h(1, 28, fi2437)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(2, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 2), L[28,28],h(2, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 2), L[28,28],h(1, 28, fi2437 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(3, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 3), L[28,28],h(3, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, fi2437 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 3), L[28,28],h(1, 28, fi2437 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(4, 28, fi2437)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(4, 28, fi2437)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(4, 28, fi2437)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, fi2437), L[28,28],h(1, 28, fi2437)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(2, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 2), L[28,28],h(2, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 2), L[28,28],h(1, 28, fi2437 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(3, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 3), L[28,28],h(3, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, fi2437 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 3), L[28,28],h(1, 28, fi2437 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(4, 28, fi2437)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(4, 28, fi2437)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(4, 28, fi2437)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, fi2437), L[28,28],h(1, 28, fi2437)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 2), L[28,28],h(2, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 2), L[28,28],h(1, 28, fi2437 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(3, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 3), L[28,28],h(3, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, fi2437 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 3), L[28,28],h(1, 28, fi2437 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(4, 28, fi2437)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(4, 28, fi2437)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(4, 28, fi2437)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, fi2437), L[28,28],h(1, 28, fi2437)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 1)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 1)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 1), L[28,28],h(1, 28, fi2437 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 2), L[28,28],h(2, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 2)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 2)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 2), L[28,28],h(1, 28, fi2437 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, fi2437)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi2437 + 3), L[28,28],h(3, 28, fi2437)) ) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, fi2437 + 3)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, fi2437 + 3), L[28,28],h(1, 28, fi2437 + 3)) ) ) )
Eq.ann: {}
 )Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(4, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), C[28,28],h(4, 28, 24)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), L[28,28],h(24, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(24, 28, 0)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), X[28,28],h(24, 28, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), L[28,28],h(24, 28, 0)) ) ) ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 59:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), L[28,28],h(1, 28, 24)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 60:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 61:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 62:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 63:
Eq: Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 64:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 65:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), L[28,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ) ) )
Eq.ann: {}
Entry 66:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 67:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 68:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) + Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 69:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, 24)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 70:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 71:
Eq: Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) = ( Tile( (1, 1), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ) )
Eq.ann: {}
Entry 72:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 73:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ) ) )
Eq.ann: {}
Entry 74:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ) )
Eq.ann: {}
Entry 75:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 24)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 76:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 25)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ) )
Eq.ann: {}
Entry 77:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(2, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ) ) )
Eq.ann: {}
Entry 78:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 26)) ) Div ( Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) + Tile( (1, 1), G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ) )
Eq.ann: {}
Entry 79:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) ) - ( ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(3, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, 24)) ) ) ) ) + ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), X[28,28],h(3, 28, 24)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), L[28,28],h(3, 28, 24)) ) ) ) ) ) )
Eq.ann: {}
Entry 80:
Eq: Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) = ( Tile( (1, 1), G(h(1, 28, 27), X[28,28],h(1, 28, 27)) ) Div ( Tile( (1, 1), 2[1,1] ) Kro Tile( (1, 1), G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-08
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 28

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84, _t2_85, _t2_86, _t2_87,
	_t2_88, _t2_89, _t2_90, _t2_91, _t2_92, _t2_93, _t2_94, _t2_95,
	_t2_96, _t2_97, _t2_98, _t2_99, _t2_100, _t2_101, _t2_102, _t2_103,
	_t2_104, _t2_105, _t2_106, _t2_107, _t2_108, _t2_109, _t2_110, _t2_111,
	_t2_112, _t2_113, _t2_114, _t2_115, _t2_116, _t2_117, _t2_118, _t2_119,
	_t2_120, _t2_121, _t2_122, _t2_123, _t2_124, _t2_125, _t2_126, _t2_127,
	_t2_128, _t2_129, _t2_130, _t2_131, _t2_132, _t2_133, _t2_134, _t2_135,
	_t2_136, _t2_137, _t2_138, _t2_139, _t2_140, _t2_141, _t2_142, _t2_143,
	_t2_144, _t2_145, _t2_146, _t2_147, _t2_148, _t2_149, _t2_150, _t2_151,
	_t2_152, _t2_153, _t2_154, _t2_155, _t2_156, _t2_157, _t2_158, _t2_159,
	_t2_160, _t2_161, _t2_162, _t2_163, _t2_164, _t2_165, _t2_166, _t2_167,
	_t2_168, _t2_169, _t2_170, _t2_171, _t2_172, _t2_173, _t2_174, _t2_175,
	_t2_176, _t2_177, _t2_178, _t2_179, _t2_180, _t2_181, _t2_182, _t2_183,
	_t2_184, _t2_185, _t2_186, _t2_187, _t2_188, _t2_189, _t2_190, _t2_191,
	_t2_192, _t2_193, _t2_194, _t2_195, _t2_196, _t2_197, _t2_198, _t2_199,
	_t2_200, _t2_201, _t2_202, _t2_203, _t2_204, _t2_205;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143,
	_t4_144, _t4_145, _t4_146, _t4_147, _t4_148, _t4_149, _t4_150, _t4_151,
	_t4_152, _t4_153, _t4_154, _t4_155, _t4_156, _t4_157, _t4_158, _t4_159,
	_t4_160, _t4_161, _t4_162, _t4_163, _t4_164, _t4_165, _t4_166, _t4_167,
	_t4_168, _t4_169, _t4_170, _t4_171, _t4_172, _t4_173, _t4_174, _t4_175,
	_t4_176, _t4_177, _t4_178, _t4_179, _t4_180, _t4_181, _t4_182, _t4_183,
	_t4_184, _t4_185, _t4_186, _t4_187, _t4_188, _t4_189, _t4_190, _t4_191,
	_t4_192, _t4_193, _t4_194, _t4_195, _t4_196, _t4_197, _t4_198, _t4_199,
	_t4_200, _t4_201, _t4_202, _t4_203, _t4_204, _t4_205, _t4_206, _t4_207,
	_t4_208, _t4_209, _t4_210, _t4_211, _t4_212;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27, _t8_28, _t8_29, _t8_30, _t8_31,
	_t8_32, _t8_33, _t8_34, _t8_35, _t8_36, _t8_37, _t8_38, _t8_39,
	_t8_40, _t8_41, _t8_42, _t8_43, _t8_44, _t8_45, _t8_46, _t8_47,
	_t8_48, _t8_49, _t8_50, _t8_51, _t8_52, _t8_53, _t8_54, _t8_55,
	_t8_56, _t8_57, _t8_58, _t8_59, _t8_60, _t8_61, _t8_62, _t8_63,
	_t8_64, _t8_65, _t8_66, _t8_67, _t8_68, _t8_69, _t8_70, _t8_71,
	_t8_72, _t8_73, _t8_74, _t8_75, _t8_76, _t8_77, _t8_78, _t8_79,
	_t8_80, _t8_81, _t8_82, _t8_83, _t8_84, _t8_85, _t8_86, _t8_87,
	_t8_88, _t8_89, _t8_90, _t8_91, _t8_92, _t8_93, _t8_94, _t8_95,
	_t8_96, _t8_97, _t8_98, _t8_99, _t8_100, _t8_101, _t8_102, _t8_103,
	_t8_104, _t8_105, _t8_106, _t8_107, _t8_108, _t8_109, _t8_110, _t8_111,
	_t8_112, _t8_113, _t8_114, _t8_115, _t8_116, _t8_117, _t8_118, _t8_119,
	_t8_120, _t8_121, _t8_122, _t8_123, _t8_124, _t8_125, _t8_126, _t8_127,
	_t8_128, _t8_129, _t8_130, _t8_131, _t8_132, _t8_133, _t8_134, _t8_135,
	_t8_136, _t8_137, _t8_138, _t8_139, _t8_140, _t8_141, _t8_142, _t8_143,
	_t8_144, _t8_145, _t8_146, _t8_147, _t8_148, _t8_149, _t8_150, _t8_151,
	_t8_152, _t8_153, _t8_154, _t8_155, _t8_156, _t8_157, _t8_158, _t8_159,
	_t8_160, _t8_161, _t8_162, _t8_163, _t8_164, _t8_165, _t8_166, _t8_167,
	_t8_168, _t8_169, _t8_170, _t8_171, _t8_172, _t8_173, _t8_174, _t8_175,
	_t8_176, _t8_177, _t8_178, _t8_179, _t8_180, _t8_181, _t8_182, _t8_183,
	_t8_184, _t8_185, _t8_186, _t8_187, _t8_188, _t8_189, _t8_190, _t8_191,
	_t8_192, _t8_193, _t8_194, _t8_195, _t8_196, _t8_197, _t8_198, _t8_199,
	_t8_200, _t8_201, _t8_202, _t8_203, _t8_204, _t8_205, _t8_206, _t8_207,
	_t8_208, _t8_209, _t8_210, _t8_211, _t8_212, _t8_213, _t8_214, _t8_215,
	_t8_216, _t8_217, _t8_218, _t8_219, _t8_220, _t8_221, _t8_222, _t8_223,
	_t8_224, _t8_225, _t8_226, _t8_227, _t8_228, _t8_229, _t8_230, _t8_231,
	_t8_232, _t8_233, _t8_234, _t8_235, _t8_236, _t8_237, _t8_238, _t8_239,
	_t8_240, _t8_241, _t8_242, _t8_243, _t8_244, _t8_245, _t8_246, _t8_247,
	_t8_248, _t8_249, _t8_250, _t8_251, _t8_252, _t8_253, _t8_254, _t8_255,
	_t8_256, _t8_257, _t8_258, _t8_259, _t8_260, _t8_261, _t8_262, _t8_263,
	_t8_264, _t8_265, _t8_266, _t8_267, _t8_268, _t8_269, _t8_270, _t8_271,
	_t8_272, _t8_273, _t8_274, _t8_275, _t8_276, _t8_277, _t8_278, _t8_279,
	_t8_280, _t8_281, _t8_282, _t8_283, _t8_284, _t8_285, _t8_286, _t8_287,
	_t8_288, _t8_289, _t8_290, _t8_291, _t8_292, _t8_293, _t8_294, _t8_295,
	_t8_296, _t8_297, _t8_298, _t8_299, _t8_300, _t8_301, _t8_302, _t8_303,
	_t8_304, _t8_305, _t8_306, _t8_307, _t8_308, _t8_309, _t8_310, _t8_311,
	_t8_312, _t8_313, _t8_314, _t8_315, _t8_316, _t8_317, _t8_318, _t8_319,
	_t8_320, _t8_321, _t8_322, _t8_323, _t8_324, _t8_325, _t8_326, _t8_327,
	_t8_328, _t8_329, _t8_330, _t8_331, _t8_332, _t8_333, _t8_334, _t8_335,
	_t8_336, _t8_337, _t8_338, _t8_339, _t8_340, _t8_341, _t8_342, _t8_343,
	_t8_344, _t8_345, _t8_346, _t8_347, _t8_348, _t8_349, _t8_350, _t8_351,
	_t8_352, _t8_353, _t8_354, _t8_355, _t8_356, _t8_357, _t8_358, _t8_359,
	_t8_360, _t8_361, _t8_362, _t8_363, _t8_364, _t8_365, _t8_366, _t8_367,
	_t8_368, _t8_369, _t8_370, _t8_371, _t8_372, _t8_373, _t8_374, _t8_375,
	_t8_376, _t8_377, _t8_378, _t8_379, _t8_380, _t8_381, _t8_382, _t8_383,
	_t8_384, _t8_385, _t8_386, _t8_387, _t8_388, _t8_389, _t8_390, _t8_391,
	_t8_392, _t8_393, _t8_394, _t8_395, _t8_396, _t8_397, _t8_398, _t8_399,
	_t8_400, _t8_401, _t8_402, _t8_403, _t8_404, _t8_405, _t8_406, _t8_407,
	_t8_408, _t8_409, _t8_410, _t8_411, _t8_412, _t8_413;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27, _t10_28, _t10_29, _t10_30, _t10_31,
	_t10_32, _t10_33, _t10_34, _t10_35, _t10_36, _t10_37, _t10_38, _t10_39,
	_t10_40, _t10_41, _t10_42, _t10_43, _t10_44, _t10_45, _t10_46, _t10_47,
	_t10_48, _t10_49, _t10_50, _t10_51, _t10_52, _t10_53, _t10_54, _t10_55,
	_t10_56, _t10_57, _t10_58, _t10_59, _t10_60, _t10_61, _t10_62, _t10_63,
	_t10_64, _t10_65, _t10_66, _t10_67, _t10_68, _t10_69, _t10_70, _t10_71,
	_t10_72, _t10_73, _t10_74, _t10_75, _t10_76, _t10_77, _t10_78, _t10_79,
	_t10_80, _t10_81, _t10_82, _t10_83, _t10_84, _t10_85, _t10_86, _t10_87,
	_t10_88, _t10_89, _t10_90, _t10_91, _t10_92, _t10_93, _t10_94, _t10_95,
	_t10_96, _t10_97, _t10_98, _t10_99, _t10_100, _t10_101, _t10_102, _t10_103,
	_t10_104, _t10_105, _t10_106, _t10_107, _t10_108, _t10_109, _t10_110, _t10_111,
	_t10_112, _t10_113, _t10_114, _t10_115, _t10_116, _t10_117, _t10_118, _t10_119,
	_t10_120, _t10_121, _t10_122, _t10_123, _t10_124, _t10_125, _t10_126, _t10_127,
	_t10_128, _t10_129, _t10_130, _t10_131, _t10_132, _t10_133, _t10_134, _t10_135,
	_t10_136, _t10_137, _t10_138, _t10_139, _t10_140, _t10_141, _t10_142, _t10_143,
	_t10_144, _t10_145, _t10_146, _t10_147, _t10_148, _t10_149, _t10_150, _t10_151,
	_t10_152, _t10_153, _t10_154, _t10_155, _t10_156, _t10_157, _t10_158, _t10_159,
	_t10_160, _t10_161, _t10_162, _t10_163, _t10_164, _t10_165, _t10_166, _t10_167,
	_t10_168, _t10_169, _t10_170, _t10_171, _t10_172, _t10_173, _t10_174, _t10_175,
	_t10_176, _t10_177, _t10_178, _t10_179, _t10_180, _t10_181, _t10_182, _t10_183,
	_t10_184, _t10_185, _t10_186, _t10_187, _t10_188, _t10_189, _t10_190, _t10_191,
	_t10_192, _t10_193, _t10_194, _t10_195, _t10_196, _t10_197, _t10_198, _t10_199,
	_t10_200, _t10_201, _t10_202, _t10_203, _t10_204, _t10_205, _t10_206, _t10_207,
	_t10_208, _t10_209, _t10_210, _t10_211, _t10_212, _t10_213, _t10_214, _t10_215,
	_t10_216, _t10_217, _t10_218, _t10_219, _t10_220, _t10_221, _t10_222, _t10_223,
	_t10_224, _t10_225, _t10_226, _t10_227, _t10_228, _t10_229, _t10_230, _t10_231,
	_t10_232, _t10_233, _t10_234, _t10_235, _t10_236, _t10_237, _t10_238, _t10_239,
	_t10_240, _t10_241, _t10_242, _t10_243, _t10_244, _t10_245, _t10_246, _t10_247,
	_t10_248, _t10_249, _t10_250, _t10_251, _t10_252, _t10_253, _t10_254, _t10_255,
	_t10_256;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35, _t16_36, _t16_37, _t16_38, _t16_39,
	_t16_40, _t16_41, _t16_42, _t16_43, _t16_44, _t16_45, _t16_46, _t16_47,
	_t16_48, _t16_49, _t16_50, _t16_51, _t16_52, _t16_53, _t16_54, _t16_55,
	_t16_56, _t16_57, _t16_58, _t16_59, _t16_60, _t16_61, _t16_62, _t16_63,
	_t16_64, _t16_65, _t16_66, _t16_67, _t16_68, _t16_69, _t16_70, _t16_71,
	_t16_72, _t16_73, _t16_74, _t16_75, _t16_76, _t16_77, _t16_78, _t16_79,
	_t16_80, _t16_81, _t16_82, _t16_83, _t16_84, _t16_85, _t16_86, _t16_87,
	_t16_88, _t16_89, _t16_90, _t16_91, _t16_92, _t16_93, _t16_94, _t16_95,
	_t16_96, _t16_97, _t16_98, _t16_99, _t16_100, _t16_101, _t16_102, _t16_103,
	_t16_104, _t16_105, _t16_106, _t16_107, _t16_108, _t16_109, _t16_110, _t16_111,
	_t16_112, _t16_113, _t16_114, _t16_115, _t16_116, _t16_117, _t16_118, _t16_119,
	_t16_120, _t16_121, _t16_122, _t16_123, _t16_124, _t16_125, _t16_126, _t16_127,
	_t16_128, _t16_129, _t16_130, _t16_131, _t16_132, _t16_133, _t16_134, _t16_135,
	_t16_136, _t16_137, _t16_138, _t16_139, _t16_140, _t16_141, _t16_142, _t16_143,
	_t16_144, _t16_145, _t16_146, _t16_147, _t16_148, _t16_149, _t16_150, _t16_151,
	_t16_152, _t16_153, _t16_154, _t16_155, _t16_156, _t16_157, _t16_158, _t16_159,
	_t16_160, _t16_161, _t16_162, _t16_163, _t16_164, _t16_165, _t16_166, _t16_167,
	_t16_168, _t16_169, _t16_170, _t16_171, _t16_172, _t16_173, _t16_174, _t16_175,
	_t16_176, _t16_177, _t16_178, _t16_179, _t16_180, _t16_181, _t16_182, _t16_183,
	_t16_184, _t16_185, _t16_186, _t16_187, _t16_188, _t16_189, _t16_190, _t16_191,
	_t16_192, _t16_193, _t16_194, _t16_195, _t16_196, _t16_197, _t16_198, _t16_199,
	_t16_200, _t16_201, _t16_202, _t16_203, _t16_204, _t16_205, _t16_206, _t16_207,
	_t16_208, _t16_209, _t16_210, _t16_211, _t16_212, _t16_213, _t16_214, _t16_215,
	_t16_216, _t16_217, _t16_218, _t16_219, _t16_220, _t16_221, _t16_222, _t16_223,
	_t16_224, _t16_225, _t16_226, _t16_227, _t16_228, _t16_229, _t16_230, _t16_231,
	_t16_232, _t16_233, _t16_234, _t16_235, _t16_236, _t16_237, _t16_238, _t16_239,
	_t16_240, _t16_241, _t16_242, _t16_243, _t16_244, _t16_245, _t16_246, _t16_247,
	_t16_248, _t16_249, _t16_250, _t16_251, _t16_252, _t16_253, _t16_254, _t16_255,
	_t16_256, _t16_257, _t16_258, _t16_259, _t16_260, _t16_261, _t16_262, _t16_263,
	_t16_264, _t16_265, _t16_266, _t16_267, _t16_268, _t16_269, _t16_270, _t16_271,
	_t16_272, _t16_273, _t16_274, _t16_275, _t16_276, _t16_277, _t16_278, _t16_279,
	_t16_280, _t16_281, _t16_282, _t16_283, _t16_284, _t16_285, _t16_286, _t16_287,
	_t16_288, _t16_289, _t16_290, _t16_291, _t16_292, _t16_293, _t16_294, _t16_295,
	_t16_296, _t16_297, _t16_298, _t16_299, _t16_300, _t16_301, _t16_302, _t16_303,
	_t16_304, _t16_305, _t16_306, _t16_307, _t16_308, _t16_309, _t16_310, _t16_311,
	_t16_312, _t16_313, _t16_314, _t16_315, _t16_316, _t16_317, _t16_318, _t16_319,
	_t16_320, _t16_321, _t16_322, _t16_323, _t16_324, _t16_325, _t16_326, _t16_327,
	_t16_328, _t16_329, _t16_330, _t16_331, _t16_332, _t16_333, _t16_334, _t16_335,
	_t16_336, _t16_337, _t16_338, _t16_339, _t16_340, _t16_341, _t16_342, _t16_343,
	_t16_344, _t16_345, _t16_346, _t16_347, _t16_348, _t16_349, _t16_350, _t16_351,
	_t16_352, _t16_353, _t16_354, _t16_355, _t16_356, _t16_357, _t16_358, _t16_359,
	_t16_360, _t16_361, _t16_362, _t16_363, _t16_364, _t16_365, _t16_366, _t16_367,
	_t16_368, _t16_369, _t16_370, _t16_371, _t16_372, _t16_373, _t16_374, _t16_375,
	_t16_376, _t16_377, _t16_378, _t16_379, _t16_380, _t16_381, _t16_382, _t16_383,
	_t16_384, _t16_385, _t16_386, _t16_387, _t16_388, _t16_389, _t16_390, _t16_391,
	_t16_392, _t16_393, _t16_394, _t16_395, _t16_396, _t16_397, _t16_398, _t16_399,
	_t16_400, _t16_401, _t16_402, _t16_403, _t16_404, _t16_405, _t16_406, _t16_407,
	_t16_408, _t16_409, _t16_410, _t16_411, _t16_412, _t16_413;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31,
	_t19_32, _t19_33, _t19_34, _t19_35, _t19_36, _t19_37, _t19_38, _t19_39,
	_t19_40, _t19_41, _t19_42, _t19_43, _t19_44, _t19_45, _t19_46, _t19_47,
	_t19_48, _t19_49, _t19_50, _t19_51, _t19_52, _t19_53, _t19_54, _t19_55,
	_t19_56, _t19_57, _t19_58, _t19_59, _t19_60, _t19_61, _t19_62, _t19_63,
	_t19_64, _t19_65, _t19_66, _t19_67, _t19_68, _t19_69, _t19_70, _t19_71,
	_t19_72, _t19_73, _t19_74, _t19_75, _t19_76, _t19_77, _t19_78, _t19_79,
	_t19_80, _t19_81, _t19_82, _t19_83, _t19_84, _t19_85, _t19_86, _t19_87,
	_t19_88, _t19_89, _t19_90, _t19_91, _t19_92, _t19_93, _t19_94, _t19_95,
	_t19_96, _t19_97, _t19_98, _t19_99, _t19_100, _t19_101, _t19_102, _t19_103,
	_t19_104, _t19_105, _t19_106, _t19_107, _t19_108, _t19_109, _t19_110, _t19_111,
	_t19_112, _t19_113, _t19_114, _t19_115, _t19_116, _t19_117, _t19_118, _t19_119,
	_t19_120, _t19_121, _t19_122, _t19_123, _t19_124, _t19_125, _t19_126, _t19_127,
	_t19_128, _t19_129, _t19_130, _t19_131, _t19_132, _t19_133, _t19_134, _t19_135,
	_t19_136, _t19_137, _t19_138, _t19_139, _t19_140, _t19_141, _t19_142, _t19_143,
	_t19_144, _t19_145, _t19_146, _t19_147, _t19_148, _t19_149, _t19_150, _t19_151,
	_t19_152, _t19_153, _t19_154, _t19_155, _t19_156, _t19_157, _t19_158, _t19_159,
	_t19_160, _t19_161, _t19_162, _t19_163, _t19_164, _t19_165, _t19_166, _t19_167,
	_t19_168, _t19_169, _t19_170, _t19_171, _t19_172, _t19_173, _t19_174, _t19_175,
	_t19_176, _t19_177, _t19_178, _t19_179, _t19_180, _t19_181, _t19_182, _t19_183,
	_t19_184, _t19_185, _t19_186, _t19_187, _t19_188, _t19_189, _t19_190, _t19_191,
	_t19_192, _t19_193, _t19_194, _t19_195, _t19_196, _t19_197, _t19_198, _t19_199,
	_t19_200, _t19_201, _t19_202;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7,
	_t21_8, _t21_9, _t21_10, _t21_11, _t21_12, _t21_13, _t21_14, _t21_15,
	_t21_16, _t21_17, _t21_18, _t21_19, _t21_20, _t21_21, _t21_22, _t21_23,
	_t21_24, _t21_25, _t21_26, _t21_27, _t21_28, _t21_29, _t21_30, _t21_31,
	_t21_32, _t21_33, _t21_34, _t21_35, _t21_36, _t21_37, _t21_38, _t21_39,
	_t21_40, _t21_41, _t21_42, _t21_43, _t21_44, _t21_45, _t21_46, _t21_47,
	_t21_48, _t21_49, _t21_50, _t21_51;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19, _t22_20, _t22_21, _t22_22, _t22_23,
	_t22_24, _t22_25, _t22_26, _t22_27, _t22_28, _t22_29, _t22_30, _t22_31;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19, _t23_20, _t23_21, _t23_22, _t23_23,
	_t23_24, _t23_25, _t23_26, _t23_27, _t23_28, _t23_29, _t23_30, _t23_31;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19, _t24_20, _t24_21, _t24_22, _t24_23,
	_t24_24, _t24_25, _t24_26, _t24_27, _t24_28, _t24_29, _t24_30, _t24_31,
	_t24_32, _t24_33, _t24_34, _t24_35, _t24_36, _t24_37, _t24_38, _t24_39,
	_t24_40, _t24_41, _t24_42, _t24_43, _t24_44, _t24_45, _t24_46, _t24_47,
	_t24_48, _t24_49, _t24_50, _t24_51, _t24_52, _t24_53, _t24_54, _t24_55,
	_t24_56, _t24_57, _t24_58, _t24_59, _t24_60, _t24_61, _t24_62, _t24_63,
	_t24_64, _t24_65, _t24_66, _t24_67, _t24_68, _t24_69, _t24_70, _t24_71,
	_t24_72, _t24_73, _t24_74, _t24_75, _t24_76, _t24_77, _t24_78, _t24_79,
	_t24_80, _t24_81, _t24_82, _t24_83, _t24_84, _t24_85, _t24_86, _t24_87,
	_t24_88, _t24_89, _t24_90, _t24_91, _t24_92, _t24_93, _t24_94, _t24_95,
	_t24_96, _t24_97, _t24_98, _t24_99, _t24_100, _t24_101, _t24_102, _t24_103,
	_t24_104, _t24_105, _t24_106, _t24_107, _t24_108, _t24_109, _t24_110, _t24_111,
	_t24_112, _t24_113, _t24_114, _t24_115, _t24_116, _t24_117, _t24_118, _t24_119,
	_t24_120, _t24_121, _t24_122, _t24_123, _t24_124, _t24_125, _t24_126, _t24_127,
	_t24_128, _t24_129, _t24_130, _t24_131, _t24_132, _t24_133, _t24_134, _t24_135,
	_t24_136, _t24_137, _t24_138, _t24_139, _t24_140, _t24_141, _t24_142, _t24_143,
	_t24_144, _t24_145, _t24_146, _t24_147, _t24_148, _t24_149, _t24_150, _t24_151,
	_t24_152, _t24_153, _t24_154, _t24_155, _t24_156, _t24_157, _t24_158, _t24_159,
	_t24_160;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27, _t26_28, _t26_29, _t26_30, _t26_31;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7,
	_t28_8, _t28_9, _t28_10, _t28_11, _t28_12, _t28_13, _t28_14, _t28_15,
	_t28_16, _t28_17, _t28_18, _t28_19, _t28_20, _t28_21, _t28_22, _t28_23,
	_t28_24, _t28_25, _t28_26, _t28_27;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31,
	_t29_32, _t29_33, _t29_34, _t29_35, _t29_36, _t29_37, _t29_38, _t29_39,
	_t29_40, _t29_41, _t29_42, _t29_43, _t29_44, _t29_45, _t29_46, _t29_47,
	_t29_48, _t29_49, _t29_50, _t29_51, _t29_52, _t29_53, _t29_54, _t29_55,
	_t29_56, _t29_57, _t29_58, _t29_59, _t29_60, _t29_61, _t29_62, _t29_63,
	_t29_64, _t29_65, _t29_66, _t29_67, _t29_68, _t29_69, _t29_70, _t29_71,
	_t29_72, _t29_73, _t29_74, _t29_75, _t29_76, _t29_77, _t29_78, _t29_79,
	_t29_80, _t29_81, _t29_82, _t29_83, _t29_84, _t29_85, _t29_86, _t29_87,
	_t29_88, _t29_89, _t29_90, _t29_91, _t29_92, _t29_93, _t29_94, _t29_95,
	_t29_96, _t29_97, _t29_98, _t29_99, _t29_100, _t29_101, _t29_102, _t29_103,
	_t29_104, _t29_105, _t29_106, _t29_107, _t29_108, _t29_109, _t29_110, _t29_111,
	_t29_112, _t29_113, _t29_114, _t29_115, _t29_116, _t29_117, _t29_118, _t29_119,
	_t29_120, _t29_121, _t29_122, _t29_123, _t29_124, _t29_125, _t29_126, _t29_127,
	_t29_128, _t29_129, _t29_130, _t29_131, _t29_132, _t29_133, _t29_134, _t29_135,
	_t29_136, _t29_137, _t29_138, _t29_139, _t29_140, _t29_141, _t29_142, _t29_143,
	_t29_144, _t29_145, _t29_146, _t29_147, _t29_148, _t29_149, _t29_150, _t29_151,
	_t29_152, _t29_153, _t29_154, _t29_155, _t29_156, _t29_157, _t29_158, _t29_159,
	_t29_160, _t29_161, _t29_162, _t29_163, _t29_164, _t29_165, _t29_166, _t29_167,
	_t29_168, _t29_169, _t29_170, _t29_171, _t29_172, _t29_173, _t29_174, _t29_175,
	_t29_176, _t29_177, _t29_178, _t29_179, _t29_180, _t29_181, _t29_182, _t29_183,
	_t29_184, _t29_185, _t29_186, _t29_187, _t29_188, _t29_189, _t29_190, _t29_191,
	_t29_192, _t29_193, _t29_194, _t29_195, _t29_196, _t29_197, _t29_198, _t29_199,
	_t29_200, _t29_201, _t29_202, _t29_203, _t29_204, _t29_205, _t29_206, _t29_207,
	_t29_208, _t29_209, _t29_210, _t29_211, _t29_212, _t29_213, _t29_214, _t29_215,
	_t29_216, _t29_217, _t29_218, _t29_219, _t29_220, _t29_221, _t29_222, _t29_223,
	_t29_224, _t29_225, _t29_226, _t29_227, _t29_228, _t29_229, _t29_230, _t29_231,
	_t29_232, _t29_233, _t29_234, _t29_235, _t29_236, _t29_237, _t29_238, _t29_239,
	_t29_240, _t29_241, _t29_242, _t29_243, _t29_244, _t29_245, _t29_246, _t29_247,
	_t29_248, _t29_249, _t29_250, _t29_251, _t29_252, _t29_253, _t29_254, _t29_255,
	_t29_256, _t29_257, _t29_258, _t29_259, _t29_260, _t29_261, _t29_262, _t29_263,
	_t29_264, _t29_265, _t29_266, _t29_267, _t29_268, _t29_269, _t29_270, _t29_271,
	_t29_272, _t29_273, _t29_274, _t29_275, _t29_276, _t29_277, _t29_278, _t29_279,
	_t29_280, _t29_281, _t29_282, _t29_283, _t29_284, _t29_285, _t29_286, _t29_287,
	_t29_288, _t29_289, _t29_290, _t29_291, _t29_292, _t29_293, _t29_294, _t29_295,
	_t29_296, _t29_297, _t29_298, _t29_299, _t29_300, _t29_301, _t29_302, _t29_303,
	_t29_304, _t29_305, _t29_306, _t29_307, _t29_308, _t29_309, _t29_310, _t29_311,
	_t29_312, _t29_313, _t29_314, _t29_315, _t29_316, _t29_317, _t29_318, _t29_319,
	_t29_320, _t29_321, _t29_322, _t29_323, _t29_324, _t29_325, _t29_326, _t29_327,
	_t29_328, _t29_329, _t29_330, _t29_331, _t29_332, _t29_333, _t29_334, _t29_335,
	_t29_336, _t29_337, _t29_338, _t29_339, _t29_340, _t29_341, _t29_342, _t29_343,
	_t29_344, _t29_345, _t29_346, _t29_347, _t29_348, _t29_349, _t29_350, _t29_351,
	_t29_352, _t29_353, _t29_354, _t29_355, _t29_356, _t29_357, _t29_358, _t29_359,
	_t29_360, _t29_361, _t29_362, _t29_363, _t29_364, _t29_365, _t29_366, _t29_367,
	_t29_368, _t29_369, _t29_370, _t29_371, _t29_372, _t29_373, _t29_374, _t29_375,
	_t29_376, _t29_377, _t29_378, _t29_379, _t29_380, _t29_381, _t29_382, _t29_383,
	_t29_384, _t29_385, _t29_386, _t29_387, _t29_388, _t29_389, _t29_390, _t29_391,
	_t29_392, _t29_393, _t29_394, _t29_395, _t29_396, _t29_397, _t29_398, _t29_399,
	_t29_400, _t29_401, _t29_402, _t29_403, _t29_404, _t29_405, _t29_406, _t29_407,
	_t29_408, _t29_409, _t29_410, _t29_411, _t29_412, _t29_413, _t29_414, _t29_415,
	_t29_416, _t29_417, _t29_418, _t29_419, _t29_420, _t29_421, _t29_422, _t29_423,
	_t29_424, _t29_425, _t29_426, _t29_427, _t29_428, _t29_429, _t29_430, _t29_431,
	_t29_432, _t29_433, _t29_434, _t29_435, _t29_436, _t29_437, _t29_438, _t29_439,
	_t29_440, _t29_441, _t29_442, _t29_443, _t29_444, _t29_445, _t29_446, _t29_447,
	_t29_448, _t29_449, _t29_450, _t29_451, _t29_452, _t29_453, _t29_454, _t29_455,
	_t29_456, _t29_457, _t29_458, _t29_459, _t29_460, _t29_461, _t29_462, _t29_463,
	_t29_464, _t29_465, _t29_466, _t29_467, _t29_468, _t29_469, _t29_470, _t29_471,
	_t29_472, _t29_473, _t29_474, _t29_475, _t29_476, _t29_477, _t29_478, _t29_479,
	_t29_480, _t29_481, _t29_482, _t29_483, _t29_484, _t29_485, _t29_486, _t29_487,
	_t29_488, _t29_489, _t29_490, _t29_491, _t29_492, _t29_493, _t29_494, _t29_495,
	_t29_496, _t29_497, _t29_498, _t29_499, _t29_500, _t29_501, _t29_502, _t29_503,
	_t29_504, _t29_505, _t29_506, _t29_507, _t29_508, _t29_509, _t29_510, _t29_511,
	_t29_512, _t29_513, _t29_514, _t29_515, _t29_516, _t29_517, _t29_518, _t29_519,
	_t29_520, _t29_521, _t29_522, _t29_523, _t29_524, _t29_525, _t29_526, _t29_527,
	_t29_528, _t29_529, _t29_530, _t29_531, _t29_532, _t29_533, _t29_534, _t29_535,
	_t29_536, _t29_537, _t29_538, _t29_539, _t29_540, _t29_541, _t29_542, _t29_543,
	_t29_544, _t29_545, _t29_546, _t29_547, _t29_548, _t29_549, _t29_550, _t29_551,
	_t29_552, _t29_553, _t29_554, _t29_555, _t29_556, _t29_557, _t29_558, _t29_559,
	_t29_560, _t29_561, _t29_562, _t29_563, _t29_564, _t29_565, _t29_566, _t29_567,
	_t29_568, _t29_569, _t29_570, _t29_571, _t29_572, _t29_573, _t29_574, _t29_575,
	_t29_576, _t29_577, _t29_578, _t29_579, _t29_580, _t29_581, _t29_582, _t29_583,
	_t29_584, _t29_585, _t29_586, _t29_587, _t29_588, _t29_589, _t29_590, _t29_591,
	_t29_592, _t29_593, _t29_594, _t29_595, _t29_596, _t29_597, _t29_598, _t29_599,
	_t29_600, _t29_601, _t29_602, _t29_603, _t29_604, _t29_605, _t29_606, _t29_607,
	_t29_608, _t29_609, _t29_610, _t29_611, _t29_612, _t29_613, _t29_614, _t29_615,
	_t29_616, _t29_617, _t29_618, _t29_619, _t29_620, _t29_621, _t29_622, _t29_623,
	_t29_624, _t29_625, _t29_626, _t29_627, _t29_628, _t29_629;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11, _t32_12, _t32_13, _t32_14, _t32_15,
	_t32_16, _t32_17, _t32_18, _t32_19, _t32_20, _t32_21, _t32_22, _t32_23,
	_t32_24, _t32_25, _t32_26, _t32_27, _t32_28, _t32_29, _t32_30, _t32_31,
	_t32_32, _t32_33, _t32_34, _t32_35, _t32_36, _t32_37, _t32_38, _t32_39,
	_t32_40, _t32_41, _t32_42, _t32_43, _t32_44, _t32_45, _t32_46, _t32_47,
	_t32_48, _t32_49, _t32_50, _t32_51, _t32_52, _t32_53, _t32_54, _t32_55,
	_t32_56, _t32_57, _t32_58, _t32_59, _t32_60, _t32_61, _t32_62, _t32_63,
	_t32_64, _t32_65, _t32_66, _t32_67, _t32_68, _t32_69, _t32_70, _t32_71,
	_t32_72, _t32_73, _t32_74, _t32_75, _t32_76, _t32_77, _t32_78, _t32_79,
	_t32_80, _t32_81, _t32_82, _t32_83, _t32_84, _t32_85, _t32_86, _t32_87,
	_t32_88, _t32_89, _t32_90, _t32_91, _t32_92, _t32_93, _t32_94, _t32_95,
	_t32_96, _t32_97, _t32_98, _t32_99, _t32_100, _t32_101, _t32_102, _t32_103,
	_t32_104, _t32_105, _t32_106, _t32_107, _t32_108, _t32_109, _t32_110, _t32_111,
	_t32_112, _t32_113, _t32_114, _t32_115, _t32_116, _t32_117, _t32_118, _t32_119,
	_t32_120, _t32_121, _t32_122, _t32_123, _t32_124, _t32_125, _t32_126, _t32_127,
	_t32_128, _t32_129, _t32_130, _t32_131, _t32_132, _t32_133, _t32_134, _t32_135,
	_t32_136, _t32_137, _t32_138, _t32_139, _t32_140, _t32_141, _t32_142, _t32_143,
	_t32_144, _t32_145, _t32_146, _t32_147, _t32_148, _t32_149, _t32_150, _t32_151,
	_t32_152, _t32_153;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23,
	_t33_24, _t33_25, _t33_26, _t33_27, _t33_28, _t33_29, _t33_30, _t33_31,
	_t33_32, _t33_33, _t33_34, _t33_35, _t33_36, _t33_37, _t33_38, _t33_39,
	_t33_40, _t33_41, _t33_42, _t33_43, _t33_44, _t33_45, _t33_46, _t33_47,
	_t33_48, _t33_49, _t33_50, _t33_51;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11, _t34_12, _t34_13, _t34_14, _t34_15,
	_t34_16, _t34_17, _t34_18, _t34_19, _t34_20, _t34_21, _t34_22, _t34_23,
	_t34_24, _t34_25, _t34_26, _t34_27, _t34_28, _t34_29, _t34_30, _t34_31;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27, _t35_28, _t35_29, _t35_30, _t35_31;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7,
	_t36_8, _t36_9, _t36_10, _t36_11, _t36_12, _t36_13, _t36_14, _t36_15,
	_t36_16, _t36_17, _t36_18, _t36_19, _t36_20, _t36_21, _t36_22, _t36_23,
	_t36_24, _t36_25, _t36_26, _t36_27, _t36_28, _t36_29, _t36_30, _t36_31,
	_t36_32, _t36_33, _t36_34, _t36_35, _t36_36, _t36_37, _t36_38, _t36_39,
	_t36_40, _t36_41, _t36_42, _t36_43, _t36_44, _t36_45, _t36_46, _t36_47,
	_t36_48, _t36_49, _t36_50, _t36_51, _t36_52, _t36_53, _t36_54, _t36_55,
	_t36_56, _t36_57, _t36_58, _t36_59, _t36_60, _t36_61, _t36_62, _t36_63,
	_t36_64, _t36_65, _t36_66, _t36_67, _t36_68, _t36_69, _t36_70, _t36_71,
	_t36_72, _t36_73, _t36_74, _t36_75, _t36_76, _t36_77, _t36_78, _t36_79,
	_t36_80, _t36_81, _t36_82, _t36_83, _t36_84, _t36_85, _t36_86, _t36_87,
	_t36_88, _t36_89, _t36_90, _t36_91, _t36_92, _t36_93, _t36_94, _t36_95,
	_t36_96, _t36_97, _t36_98, _t36_99, _t36_100, _t36_101, _t36_102, _t36_103,
	_t36_104, _t36_105, _t36_106, _t36_107, _t36_108, _t36_109, _t36_110, _t36_111,
	_t36_112, _t36_113, _t36_114, _t36_115, _t36_116, _t36_117, _t36_118, _t36_119,
	_t36_120, _t36_121, _t36_122, _t36_123, _t36_124, _t36_125, _t36_126, _t36_127,
	_t36_128, _t36_129, _t36_130, _t36_131, _t36_132, _t36_133, _t36_134, _t36_135,
	_t36_136, _t36_137, _t36_138, _t36_139, _t36_140, _t36_141, _t36_142, _t36_143,
	_t36_144, _t36_145, _t36_146, _t36_147, _t36_148, _t36_149, _t36_150, _t36_151,
	_t36_152, _t36_153, _t36_154, _t36_155, _t36_156;

  _t0_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[0])));
  _t0_12 = _mm256_castpd128_pd256(_mm_load_sd(&(L[0])));
  _t0_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(C + 28)), _mm256_castpd128_pd256(_mm_load_sd(C + 56))), _mm256_castpd128_pd256(_mm_load_sd(C + 84)), 32);
  _t0_11 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 28)), _mm256_castpd128_pd256(_mm_load_sd(L + 56))), _mm256_castpd128_pd256(_mm_load_sd(L + 84)), 32);
  _t0_10 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29])));
  _t0_9 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 57)), _mm256_castpd128_pd256(_mm_load_sd(L + 85)), 0);
  _t0_17 = _mm256_castpd128_pd256(_mm_load_sd(&(C[29])));
  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(L[28])));
  _t0_18 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(C + 57)), _mm256_castpd128_pd256(_mm_load_sd(C + 85)), 0);
  _t0_7 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 56)), _mm256_castpd128_pd256(_mm_load_sd(L + 84)), 0);
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[58])));
  _t0_5 = _mm256_broadcast_sd(&(L[86]));
  _t0_22 = _mm256_castpd128_pd256(_mm_load_sd(&(C[58])));
  _t0_4 = _mm256_maskload_pd(L + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_23 = _mm256_castpd128_pd256(_mm_load_sd(&(C[86])));
  _t0_3 = _mm256_maskload_pd(L + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[86])));
  _t0_1 = _mm256_castpd128_pd256(_mm_load_sd(&(L[87])));
  _t0_26 = _mm256_castpd128_pd256(_mm_load_sd(&(C[87])));
  _t0_0 = _mm256_maskload_pd(L + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = S(h(1, 28, 0), ( G(h(1, 28, 0), X[28,28],h(1, 28, 0)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_27 = _t0_13;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_28 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_29 = _t0_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_30 = _mm256_mul_pd(_t0_28, _t0_29);

  // 4-BLAC: 1x4 / 1x4
  _t0_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_27), _mm256_castpd256_pd128(_t0_30)));

  // AVX Storer:
  _t0_13 = _t0_31;

  // Generating : X[28,28] = S(h(3, 28, 1), ( G(h(3, 28, 1), X[28,28],h(1, 28, 0)) - ( G(h(3, 28, 1), L[28,28],h(1, 28, 0)) Kro G(h(1, 28, 0), X[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_32 = _t0_14;

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_33 = _t0_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_34 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_35 = _mm256_mul_pd(_t0_33, _t0_34);

  // 4-BLAC: 4x1 - 4x1
  _t0_36 = _mm256_sub_pd(_t0_32, _t0_35);

  // AVX Storer:
  _t0_14 = _t0_36;

  // Generating : X[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 1), L[28,28],h(1, 28, 1)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_37 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_38 = _t0_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_39 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t0_40 = _mm256_add_pd(_t0_38, _t0_39);

  // 4-BLAC: 1x4 / 1x4
  _t0_41 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_37), _mm256_castpd256_pd128(_t0_40)));

  // AVX Storer:
  _t0_15 = _t0_41;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 0)) - ( G(h(2, 28, 2), L[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_42 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 2), _mm256_permute2f128_pd(_t0_14, _t0_14, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_43 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_45 = _mm256_mul_pd(_t0_43, _t0_44);

  // 4-BLAC: 4x1 - 4x1
  _t0_46 = _mm256_sub_pd(_t0_42, _t0_45);

  // AVX Storer:
  _t0_16 = _t0_46;

  // Generating : X[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), X[28,28],h(1, 28, 1)) - ( ( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) + ( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_47 = _t0_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_48 = _t0_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_49 = _t0_15;

  // 4-BLAC: (4x1)^T
  _t0_50 = _t0_49;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_51 = _mm256_mul_pd(_t0_48, _t0_50);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_52 = _t0_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_53 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t0_54 = _t0_53;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_55 = _mm256_mul_pd(_t0_52, _t0_54);

  // 4-BLAC: 1x4 + 1x4
  _t0_56 = _mm256_add_pd(_t0_51, _t0_55);

  // 4-BLAC: 1x4 - 1x4
  _t0_57 = _mm256_sub_pd(_t0_47, _t0_56);

  // AVX Storer:
  _t0_17 = _t0_57;

  // Generating : X[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), X[28,28],h(1, 28, 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_58 = _t0_17;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_59 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_60 = _t0_10;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_61 = _mm256_mul_pd(_t0_59, _t0_60);

  // 4-BLAC: 1x4 / 1x4
  _t0_62 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_58), _mm256_castpd256_pd128(_t0_61)));

  // AVX Storer:
  _t0_17 = _t0_62;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 1)) - ( G(h(2, 28, 2), L[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), X[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_63 = _t0_18;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_64 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_65 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_66 = _t0_65;

  // 4-BLAC: 4x1 Kro 1x4
  _t0_67 = _mm256_mul_pd(_t0_64, _t0_66);

  // 4-BLAC: 4x1 - 4x1
  _t0_68 = _mm256_sub_pd(_t0_63, _t0_67);

  // AVX Storer:
  _t0_18 = _t0_68;

  // Generating : X[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), X[28,28],h(1, 28, 1)) - ( G(h(2, 28, 2), L[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), X[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_69 = _t0_18;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_70 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_71 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_72 = _mm256_mul_pd(_t0_70, _t0_71);

  // 4-BLAC: 4x1 - 4x1
  _t0_73 = _mm256_sub_pd(_t0_69, _t0_72);

  // AVX Storer:
  _t0_18 = _t0_73;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 2), L[28,28],h(1, 28, 2)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_74 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_75 = _t0_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_76 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t0_77 = _mm256_add_pd(_t0_75, _t0_76);

  // 4-BLAC: 1x4 / 1x4
  _t0_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_74), _mm256_castpd256_pd128(_t0_77)));

  // AVX Storer:
  _t0_19 = _t0_78;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 2), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_80 = _t0_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_81 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t0_82 = _t0_81;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_83 = _mm256_mul_pd(_t0_80, _t0_82);

  // 4-BLAC: 1x4 - 1x4
  _t0_84 = _mm256_sub_pd(_t0_79, _t0_83);

  // AVX Storer:
  _t0_20 = _t0_84;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 2), L[28,28],h(1, 28, 2)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_85 = _t0_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_86 = _t0_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_87 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t0_88 = _mm256_add_pd(_t0_86, _t0_87);

  // 4-BLAC: 1x4 / 1x4
  _t0_89 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_85), _mm256_castpd256_pd128(_t0_88)));

  // AVX Storer:
  _t0_20 = _t0_89;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(2, 28, 0)) - ( G(h(1, 28, 3), L[28,28],h(1, 28, 2)) Kro G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ),h(2, 28, 0))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_90 = _mm256_unpackhi_pd(_mm256_blend_pd(_t0_16, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_18, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_91 = _t0_5;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_92 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_93 = _mm256_mul_pd(_t0_91, _t0_92);

  // 4-BLAC: 1x4 - 1x4
  _t0_94 = _mm256_sub_pd(_t0_90, _t0_93);

  // AVX Storer:
  _t0_21 = _t0_94;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 2)) - ( ( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) + ( G(h(1, 28, 2), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_95 = _t0_22;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_96 = _t0_4;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_97 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_98 = _t0_97;

  // 4-BLAC: 1x4 * 4x1
  _t0_99 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_96, _t0_98), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_96, _t0_98), _mm256_mul_pd(_t0_96, _t0_98), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_96, _t0_98), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_96, _t0_98), _mm256_mul_pd(_t0_96, _t0_98), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_96, _t0_98), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_96, _t0_98), _mm256_mul_pd(_t0_96, _t0_98), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_100 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_101 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t0_102 = _t0_101;

  // 4-BLAC: 1x4 * 4x1
  _t0_103 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_100, _t0_102), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_100, _t0_102), _mm256_mul_pd(_t0_100, _t0_102), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_100, _t0_102), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_100, _t0_102), _mm256_mul_pd(_t0_100, _t0_102), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_100, _t0_102), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_100, _t0_102), _mm256_mul_pd(_t0_100, _t0_102), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_104 = _mm256_add_pd(_t0_99, _t0_103);

  // 4-BLAC: 1x4 - 1x4
  _t0_105 = _mm256_sub_pd(_t0_95, _t0_104);

  // AVX Storer:
  _t0_22 = _t0_105;

  // Generating : X[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), X[28,28],h(1, 28, 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_106 = _t0_22;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_107 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_108 = _t0_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_109 = _mm256_mul_pd(_t0_107, _t0_108);

  // 4-BLAC: 1x4 / 1x4
  _t0_110 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_106), _mm256_castpd256_pd128(_t0_109)));

  // AVX Storer:
  _t0_22 = _t0_110;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 3), L[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), X[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_111 = _t0_23;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_112 = _t0_3;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_113 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_114 = _t0_113;

  // 4-BLAC: 1x4 * 4x1
  _t0_115 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_112, _t0_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_112, _t0_114), _mm256_mul_pd(_t0_112, _t0_114), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_112, _t0_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_112, _t0_114), _mm256_mul_pd(_t0_112, _t0_114), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_112, _t0_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_112, _t0_114), _mm256_mul_pd(_t0_112, _t0_114), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_116 = _mm256_sub_pd(_t0_111, _t0_115);

  // AVX Storer:
  _t0_23 = _t0_116;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 3), L[28,28],h(1, 28, 2)) Kro G(h(1, 28, 2), X[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_117 = _t0_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_118 = _t0_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_119 = _t0_22;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_120 = _mm256_mul_pd(_t0_118, _t0_119);

  // 4-BLAC: 1x4 - 1x4
  _t0_121 = _mm256_sub_pd(_t0_117, _t0_120);

  // AVX Storer:
  _t0_23 = _t0_121;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 3), L[28,28],h(1, 28, 3)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_122 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_123 = _t0_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_124 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t0_125 = _mm256_add_pd(_t0_123, _t0_124);

  // 4-BLAC: 1x4 / 1x4
  _t0_126 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_122), _mm256_castpd256_pd128(_t0_125)));

  // AVX Storer:
  _t0_24 = _t0_126;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 3), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_127 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_128 = _t0_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_129 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t0_130 = _t0_129;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_131 = _mm256_mul_pd(_t0_128, _t0_130);

  // 4-BLAC: 1x4 - 1x4
  _t0_132 = _mm256_sub_pd(_t0_127, _t0_131);

  // AVX Storer:
  _t0_25 = _t0_132;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 3), L[28,28],h(1, 28, 3)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_133 = _t0_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_134 = _t0_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_135 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t0_136 = _mm256_add_pd(_t0_134, _t0_135);

  // 4-BLAC: 1x4 / 1x4
  _t0_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_133), _mm256_castpd256_pd128(_t0_136)));

  // AVX Storer:
  _t0_25 = _t0_137;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 3), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_138 = _t0_23;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_140 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t0_141 = _t0_140;

  // 4-BLAC: 1x4 * 4x1
  _t0_142 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_139, _t0_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_139, _t0_141), _mm256_mul_pd(_t0_139, _t0_141), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_139, _t0_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_139, _t0_141), _mm256_mul_pd(_t0_139, _t0_141), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_139, _t0_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_139, _t0_141), _mm256_mul_pd(_t0_139, _t0_141), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_143 = _mm256_sub_pd(_t0_138, _t0_142);

  // AVX Storer:
  _t0_23 = _t0_143;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 3), L[28,28],h(1, 28, 3)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_144 = _t0_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_145 = _t0_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_146 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t0_147 = _mm256_add_pd(_t0_145, _t0_146);

  // 4-BLAC: 1x4 / 1x4
  _t0_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_144), _mm256_castpd256_pd128(_t0_147)));

  // AVX Storer:
  _t0_23 = _t0_148;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 3)) - ( ( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), X[28,28],h(3, 28, 0)) ) ) + ( G(h(1, 28, 3), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_149 = _t0_26;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_150 = _t0_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_151 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_unpacklo_pd(_t0_23, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_152 = _t0_151;

  // 4-BLAC: 1x4 * 4x1
  _t0_153 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_150, _t0_152), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_150, _t0_152), _mm256_mul_pd(_t0_150, _t0_152), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_150, _t0_152), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_150, _t0_152), _mm256_mul_pd(_t0_150, _t0_152), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_150, _t0_152), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_150, _t0_152), _mm256_mul_pd(_t0_150, _t0_152), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_154 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_unpacklo_pd(_t0_23, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_155 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t0_156 = _t0_155;

  // 4-BLAC: 1x4 * 4x1
  _t0_157 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_154, _t0_156), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_154, _t0_156), _mm256_mul_pd(_t0_154, _t0_156), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_154, _t0_156), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_154, _t0_156), _mm256_mul_pd(_t0_154, _t0_156), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_154, _t0_156), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_154, _t0_156), _mm256_mul_pd(_t0_154, _t0_156), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t0_158 = _mm256_add_pd(_t0_153, _t0_157);

  // 4-BLAC: 1x4 - 1x4
  _t0_159 = _mm256_sub_pd(_t0_149, _t0_158);

  // AVX Storer:
  _t0_26 = _t0_159;

  // Generating : X[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), X[28,28],h(1, 28, 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_160 = _t0_26;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_161 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_162 = _t0_1;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_163 = _mm256_mul_pd(_t0_161, _t0_162);

  // 4-BLAC: 1x4 / 1x4
  _t0_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_160), _mm256_castpd256_pd128(_t0_163)));

  // AVX Storer:
  _t0_26 = _t0_164;

  // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, i100 + 4), ( G(h(4, 28, i100 + 4), X[28,28],h(4, 28, 0)) - ( G(h(4, 28, i100 + 4), L[28,28],h(4, 28, 0)) * G(h(4, 28, 0), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_165 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_13, _mm256_blend_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_unpacklo_pd(_t0_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_unpacklo_pd(_t0_23, _t0_26), 32), 0), 32);
  _t0_166 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_unpacklo_pd(_t0_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_unpacklo_pd(_t0_23, _t0_26), 32), 3), 32);
  _t0_167 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_unpacklo_pd(_t0_22, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_19, _t0_20), _mm256_unpacklo_pd(_t0_22, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_unpacklo_pd(_t0_23, _t0_26), 32), 3), 12);
  _t0_168 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_24, _t0_25), _mm256_unpacklo_pd(_t0_23, _t0_26), 32);


  for( int i100 = 0; i100 <= 23; i100+=4 ) {
    _t1_20 = _asm256_loadu_pd(C + 28*i100 + 112);
    _t1_21 = _asm256_loadu_pd(C + 28*i100 + 140);
    _t1_22 = _asm256_loadu_pd(C + 28*i100 + 168);
    _t1_23 = _asm256_loadu_pd(C + 28*i100 + 196);
    _t1_15 = _mm256_broadcast_sd(L + 28*i100 + 112);
    _t1_14 = _mm256_broadcast_sd(L + 28*i100 + 113);
    _t1_13 = _mm256_broadcast_sd(L + 28*i100 + 114);
    _t1_12 = _mm256_broadcast_sd(L + 28*i100 + 115);
    _t1_11 = _mm256_broadcast_sd(L + 28*i100 + 140);
    _t1_10 = _mm256_broadcast_sd(L + 28*i100 + 141);
    _t1_9 = _mm256_broadcast_sd(L + 28*i100 + 142);
    _t1_8 = _mm256_broadcast_sd(L + 28*i100 + 143);
    _t1_7 = _mm256_broadcast_sd(L + 28*i100 + 168);
    _t1_6 = _mm256_broadcast_sd(L + 28*i100 + 169);
    _t1_5 = _mm256_broadcast_sd(L + 28*i100 + 170);
    _t1_4 = _mm256_broadcast_sd(L + 28*i100 + 171);
    _t1_3 = _mm256_broadcast_sd(L + 28*i100 + 196);
    _t1_2 = _mm256_broadcast_sd(L + 28*i100 + 197);
    _t1_1 = _mm256_broadcast_sd(L + 28*i100 + 198);
    _t1_0 = _mm256_broadcast_sd(L + 28*i100 + 199);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t1_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_15, _t0_165), _mm256_mul_pd(_t1_14, _t0_166)), _mm256_add_pd(_mm256_mul_pd(_t1_13, _t0_167), _mm256_mul_pd(_t1_12, _t0_168)));
    _t1_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_11, _t0_165), _mm256_mul_pd(_t1_10, _t0_166)), _mm256_add_pd(_mm256_mul_pd(_t1_9, _t0_167), _mm256_mul_pd(_t1_8, _t0_168)));
    _t1_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_7, _t0_165), _mm256_mul_pd(_t1_6, _t0_166)), _mm256_add_pd(_mm256_mul_pd(_t1_5, _t0_167), _mm256_mul_pd(_t1_4, _t0_168)));
    _t1_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t1_3, _t0_165), _mm256_mul_pd(_t1_2, _t0_166)), _mm256_add_pd(_mm256_mul_pd(_t1_1, _t0_167), _mm256_mul_pd(_t1_0, _t0_168)));

    // 4-BLAC: 4x4 - 4x4
    _t1_20 = _mm256_sub_pd(_t1_20, _t1_16);
    _t1_21 = _mm256_sub_pd(_t1_21, _t1_17);
    _t1_22 = _mm256_sub_pd(_t1_22, _t1_18);
    _t1_23 = _mm256_sub_pd(_t1_23, _t1_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i100 + 112, _t1_20);
    _asm256_storeu_pd(C + 28*i100 + 140, _t1_21);
    _asm256_storeu_pd(C + 28*i100 + 168, _t1_22);
    _asm256_storeu_pd(C + 28*i100 + 196, _t1_23);
  }

  _t2_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[112])));
  _t2_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[116])));
  _t2_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[113])));
  _t2_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[114])));
  _t2_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[115])));
  _t2_11 = _asm256_loadu_pd(C + 140);
  _t2_12 = _asm256_loadu_pd(C + 168);
  _t2_13 = _asm256_loadu_pd(C + 196);
  _t2_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 144)), _mm256_castpd128_pd256(_mm_load_sd(L + 172))), _mm256_castpd128_pd256(_mm_load_sd(L + 200)), 32);
  _t2_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[145])));
  _t2_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 173)), _mm256_castpd128_pd256(_mm_load_sd(L + 201)), 0);
  _t2_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[174])));
  _t2_1 = _mm256_broadcast_sd(&(L[202]));
  _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[203])));

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 4), L[28,28],h(1, 28, 4)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_27 = _t2_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_28 = _t2_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_29 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_30 = _mm256_add_pd(_t2_28, _t2_29);

  // 4-BLAC: 1x4 / 1x4
  _t2_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_27), _mm256_castpd256_pd128(_t2_30)));

  // AVX Storer:
  _t2_7 = _t2_31;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 4), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_32 = _t2_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_33 = _t2_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_34 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t2_35 = _t2_34;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_36 = _mm256_mul_pd(_t2_33, _t2_35);

  // 4-BLAC: 1x4 - 1x4
  _t2_37 = _mm256_sub_pd(_t2_32, _t2_36);

  // AVX Storer:
  _t2_8 = _t2_37;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 4), L[28,28],h(1, 28, 4)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_38 = _t2_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_39 = _t2_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_40 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_41 = _mm256_add_pd(_t2_39, _t2_40);

  // 4-BLAC: 1x4 / 1x4
  _t2_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_38), _mm256_castpd256_pd128(_t2_41)));

  // AVX Storer:
  _t2_8 = _t2_42;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 4), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_43 = _t2_9;

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_44 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_45 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t2_46 = _t2_45;

  // 4-BLAC: 1x4 * 4x1
  _t2_47 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_mul_pd(_t2_44, _t2_46), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_mul_pd(_t2_44, _t2_46), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_44, _t2_46), _mm256_mul_pd(_t2_44, _t2_46), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_48 = _mm256_sub_pd(_t2_43, _t2_47);

  // AVX Storer:
  _t2_9 = _t2_48;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 4), L[28,28],h(1, 28, 4)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_49 = _t2_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_50 = _t2_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_51 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_52 = _mm256_add_pd(_t2_50, _t2_51);

  // 4-BLAC: 1x4 / 1x4
  _t2_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_49), _mm256_castpd256_pd128(_t2_52)));

  // AVX Storer:
  _t2_9 = _t2_53;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 4), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_54 = _t2_10;

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_55 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_56 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t2_57 = _t2_56;

  // 4-BLAC: 1x4 * 4x1
  _t2_58 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_mul_pd(_t2_55, _t2_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_mul_pd(_t2_55, _t2_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_57), _mm256_mul_pd(_t2_55, _t2_57), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_59 = _mm256_sub_pd(_t2_54, _t2_58);

  // AVX Storer:
  _t2_10 = _t2_59;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 4), L[28,28],h(1, 28, 4)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_60 = _t2_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_61 = _t2_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_62 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t2_63 = _mm256_add_pd(_t2_61, _t2_62);

  // 4-BLAC: 1x4 / 1x4
  _t2_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_60), _mm256_castpd256_pd128(_t2_63)));

  // AVX Storer:
  _t2_10 = _t2_64;

  // Generating : X[28,28] = S(h(3, 28, 5), ( G(h(3, 28, 5), X[28,28],h(4, 28, 0)) - ( G(h(3, 28, 5), L[28,28],h(1, 28, 4)) * G(h(1, 28, 4), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t2_65 = _t2_11;
  _t2_66 = _t2_12;
  _t2_67 = _t2_13;
  _t2_68 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t2_69 = _t2_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t2_70 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 32), _mm256_permute2f128_pd(_t2_69, _t2_69, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));
  _t2_71 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 32), _mm256_permute2f128_pd(_t2_69, _t2_69, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));
  _t2_72 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 49), _mm256_permute2f128_pd(_t2_69, _t2_69, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));
  _t2_73 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_69, _t2_69, 49), _mm256_permute2f128_pd(_t2_69, _t2_69, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32));

  // 4-BLAC: 4x4 - 4x4
  _t2_74 = _mm256_sub_pd(_t2_65, _t2_70);
  _t2_75 = _mm256_sub_pd(_t2_66, _t2_71);
  _t2_76 = _mm256_sub_pd(_t2_67, _t2_72);
  _t2_77 = _mm256_sub_pd(_t2_68, _t2_73);

  // AVX Storer:
  _t2_11 = _t2_74;
  _t2_12 = _t2_75;
  _t2_13 = _t2_76;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 5), L[28,28],h(1, 28, 5)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_78 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_79 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_80 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_81 = _mm256_add_pd(_t2_79, _t2_80);

  // 4-BLAC: 1x4 / 1x4
  _t2_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_78), _mm256_castpd256_pd128(_t2_81)));

  // AVX Storer:
  _t2_14 = _t2_82;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 5), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_83 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_84 = _t2_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_85 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t2_86 = _t2_85;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_87 = _mm256_mul_pd(_t2_84, _t2_86);

  // 4-BLAC: 1x4 - 1x4
  _t2_88 = _mm256_sub_pd(_t2_83, _t2_87);

  // AVX Storer:
  _t2_15 = _t2_88;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 5), L[28,28],h(1, 28, 5)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_89 = _t2_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_90 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_91 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_92 = _mm256_add_pd(_t2_90, _t2_91);

  // 4-BLAC: 1x4 / 1x4
  _t2_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_89), _mm256_castpd256_pd128(_t2_92)));

  // AVX Storer:
  _t2_15 = _t2_93;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 5), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_94 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_11, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_95 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_96 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t2_97 = _t2_96;

  // 4-BLAC: 1x4 * 4x1
  _t2_98 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_mul_pd(_t2_95, _t2_97), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_mul_pd(_t2_95, _t2_97), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_95, _t2_97), _mm256_mul_pd(_t2_95, _t2_97), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_99 = _mm256_sub_pd(_t2_94, _t2_98);

  // AVX Storer:
  _t2_16 = _t2_99;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 5), L[28,28],h(1, 28, 5)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_100 = _t2_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_101 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_102 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_103 = _mm256_add_pd(_t2_101, _t2_102);

  // 4-BLAC: 1x4 / 1x4
  _t2_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_100), _mm256_castpd256_pd128(_t2_103)));

  // AVX Storer:
  _t2_16 = _t2_104;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 5), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_11, _t2_11, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_107 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t2_108 = _t2_107;

  // 4-BLAC: 1x4 * 4x1
  _t2_109 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_mul_pd(_t2_106, _t2_108), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_mul_pd(_t2_106, _t2_108), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_106, _t2_108), _mm256_mul_pd(_t2_106, _t2_108), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_110 = _mm256_sub_pd(_t2_105, _t2_109);

  // AVX Storer:
  _t2_17 = _t2_110;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 5), L[28,28],h(1, 28, 5)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_111 = _t2_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_112 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_113 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t2_114 = _mm256_add_pd(_t2_112, _t2_113);

  // 4-BLAC: 1x4 / 1x4
  _t2_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_111), _mm256_castpd256_pd128(_t2_114)));

  // AVX Storer:
  _t2_17 = _t2_115;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(4, 28, 0)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 5)) * G(h(1, 28, 5), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t2_116 = _t2_12;
  _t2_117 = _t2_13;
  _t2_118 = _mm256_setzero_pd();
  _t2_119 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_120 = _t2_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t2_121 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 32), _mm256_permute2f128_pd(_t2_120, _t2_120, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));
  _t2_122 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 32), _mm256_permute2f128_pd(_t2_120, _t2_120, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));
  _t2_123 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 49), _mm256_permute2f128_pd(_t2_120, _t2_120, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));
  _t2_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_120, _t2_120, 49), _mm256_permute2f128_pd(_t2_120, _t2_120, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32));

  // 4-BLAC: 4x4 - 4x4
  _t2_125 = _mm256_sub_pd(_t2_116, _t2_121);
  _t2_126 = _mm256_sub_pd(_t2_117, _t2_122);
  _t2_127 = _mm256_sub_pd(_t2_118, _t2_123);
  _t2_128 = _mm256_sub_pd(_t2_119, _t2_124);

  // AVX Storer:
  _t2_12 = _t2_125;
  _t2_13 = _t2_126;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_129 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_130 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_131 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_132 = _mm256_add_pd(_t2_130, _t2_131);

  // 4-BLAC: 1x4 / 1x4
  _t2_133 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_129), _mm256_castpd256_pd128(_t2_132)));

  // AVX Storer:
  _t2_18 = _t2_133;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 6), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_134 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_135 = _t2_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_136 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t2_137 = _t2_136;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_138 = _mm256_mul_pd(_t2_135, _t2_137);

  // 4-BLAC: 1x4 - 1x4
  _t2_139 = _mm256_sub_pd(_t2_134, _t2_138);

  // AVX Storer:
  _t2_19 = _t2_139;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_140 = _t2_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_141 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_142 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_143 = _mm256_add_pd(_t2_141, _t2_142);

  // 4-BLAC: 1x4 / 1x4
  _t2_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_140), _mm256_castpd256_pd128(_t2_143)));

  // AVX Storer:
  _t2_19 = _t2_144;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 6), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_145 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_12, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_146 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_147 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t2_148 = _t2_147;

  // 4-BLAC: 1x4 * 4x1
  _t2_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_mul_pd(_t2_146, _t2_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_mul_pd(_t2_146, _t2_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_146, _t2_148), _mm256_mul_pd(_t2_146, _t2_148), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_150 = _mm256_sub_pd(_t2_145, _t2_149);

  // AVX Storer:
  _t2_20 = _t2_150;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_151 = _t2_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_152 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_153 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_154 = _mm256_add_pd(_t2_152, _t2_153);

  // 4-BLAC: 1x4 / 1x4
  _t2_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_151), _mm256_castpd256_pd128(_t2_154)));

  // AVX Storer:
  _t2_20 = _t2_155;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 6), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_156 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_12, _t2_12, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_158 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t2_159 = _t2_158;

  // 4-BLAC: 1x4 * 4x1
  _t2_160 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_mul_pd(_t2_157, _t2_159), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_mul_pd(_t2_157, _t2_159), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_157, _t2_159), _mm256_mul_pd(_t2_157, _t2_159), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_161 = _mm256_sub_pd(_t2_156, _t2_160);

  // AVX Storer:
  _t2_21 = _t2_161;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_162 = _t2_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_163 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_164 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t2_165 = _mm256_add_pd(_t2_163, _t2_164);

  // 4-BLAC: 1x4 / 1x4
  _t2_166 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_162), _mm256_castpd256_pd128(_t2_165)));

  // AVX Storer:
  _t2_21 = _t2_166;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(4, 28, 0)) - ( G(h(1, 28, 7), L[28,28],h(1, 28, 6)) Kro G(h(1, 28, 6), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_167 = _t2_1;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t2_26 = _mm256_mul_pd(_t2_167, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32));

  // 4-BLAC: 1x4 - 1x4
  _t2_13 = _mm256_sub_pd(_t2_13, _t2_26);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_168 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_169 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_170 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_171 = _mm256_add_pd(_t2_169, _t2_170);

  // 4-BLAC: 1x4 / 1x4
  _t2_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_168), _mm256_castpd256_pd128(_t2_171)));

  // AVX Storer:
  _t2_22 = _t2_172;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 7), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_173 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_174 = _t2_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_175 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t2_176 = _t2_175;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_177 = _mm256_mul_pd(_t2_174, _t2_176);

  // 4-BLAC: 1x4 - 1x4
  _t2_178 = _mm256_sub_pd(_t2_173, _t2_177);

  // AVX Storer:
  _t2_23 = _t2_178;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_179 = _t2_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_180 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_181 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_182 = _mm256_add_pd(_t2_180, _t2_181);

  // 4-BLAC: 1x4 / 1x4
  _t2_183 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_179), _mm256_castpd256_pd128(_t2_182)));

  // AVX Storer:
  _t2_23 = _t2_183;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 7), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_184 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_13, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_185 = _mm256_blend_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_186 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t2_187 = _t2_186;

  // 4-BLAC: 1x4 * 4x1
  _t2_188 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_mul_pd(_t2_185, _t2_187), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_mul_pd(_t2_185, _t2_187), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_185, _t2_187), _mm256_mul_pd(_t2_185, _t2_187), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_189 = _mm256_sub_pd(_t2_184, _t2_188);

  // AVX Storer:
  _t2_24 = _t2_189;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_190 = _t2_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_191 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_192 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_193 = _mm256_add_pd(_t2_191, _t2_192);

  // 4-BLAC: 1x4 / 1x4
  _t2_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_190), _mm256_castpd256_pd128(_t2_193)));

  // AVX Storer:
  _t2_24 = _t2_194;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 7), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_195 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_13, _t2_13, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_196 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_197 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t2_198 = _t2_197;

  // 4-BLAC: 1x4 * 4x1
  _t2_199 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_mul_pd(_t2_196, _t2_198), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_mul_pd(_t2_196, _t2_198), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_196, _t2_198), _mm256_mul_pd(_t2_196, _t2_198), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_200 = _mm256_sub_pd(_t2_195, _t2_199);

  // AVX Storer:
  _t2_25 = _t2_200;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_201 = _t2_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_202 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_203 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t2_204 = _mm256_add_pd(_t2_202, _t2_203);

  // 4-BLAC: 1x4 / 1x4
  _t2_205 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_201), _mm256_castpd256_pd128(_t2_204)));

  // AVX Storer:
  _t2_25 = _t2_205;

  // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, i100 + 8), ( G(h(4, 28, i100 + 8), X[28,28],h(4, 28, 0)) - ( G(h(4, 28, i100 + 8), L[28,28],h(4, 28, 4)) * G(h(4, 28, 4), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0)) )

  // AVX Loader:

  _asm256_storeu_pd(C + 140, _t2_11);
  _asm256_storeu_pd(C + 168, _t2_12);
  _asm256_storeu_pd(C + 196, _t2_13);

  for( int i100 = 0; i100 <= 19; i100+=4 ) {
    _t3_20 = _asm256_loadu_pd(C + 28*i100 + 224);
    _t3_21 = _asm256_loadu_pd(C + 28*i100 + 252);
    _t3_22 = _asm256_loadu_pd(C + 28*i100 + 280);
    _t3_23 = _asm256_loadu_pd(C + 28*i100 + 308);
    _t3_15 = _mm256_broadcast_sd(L + 28*i100 + 228);
    _t3_14 = _mm256_broadcast_sd(L + 28*i100 + 229);
    _t3_13 = _mm256_broadcast_sd(L + 28*i100 + 230);
    _t3_12 = _mm256_broadcast_sd(L + 28*i100 + 231);
    _t3_11 = _mm256_broadcast_sd(L + 28*i100 + 256);
    _t3_10 = _mm256_broadcast_sd(L + 28*i100 + 257);
    _t3_9 = _mm256_broadcast_sd(L + 28*i100 + 258);
    _t3_8 = _mm256_broadcast_sd(L + 28*i100 + 259);
    _t3_7 = _mm256_broadcast_sd(L + 28*i100 + 284);
    _t3_6 = _mm256_broadcast_sd(L + 28*i100 + 285);
    _t3_5 = _mm256_broadcast_sd(L + 28*i100 + 286);
    _t3_4 = _mm256_broadcast_sd(L + 28*i100 + 287);
    _t3_3 = _mm256_broadcast_sd(L + 28*i100 + 312);
    _t3_2 = _mm256_broadcast_sd(L + 28*i100 + 313);
    _t3_1 = _mm256_broadcast_sd(L + 28*i100 + 314);
    _t3_0 = _mm256_broadcast_sd(L + 28*i100 + 315);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t3_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32)), _mm256_mul_pd(_t3_14, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32)), _mm256_mul_pd(_t3_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32))));
    _t3_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32)), _mm256_mul_pd(_t3_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32)), _mm256_mul_pd(_t3_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32))));
    _t3_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32)), _mm256_mul_pd(_t3_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32)), _mm256_mul_pd(_t3_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32))));
    _t3_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32)), _mm256_mul_pd(_t3_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32))), _mm256_add_pd(_mm256_mul_pd(_t3_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32)), _mm256_mul_pd(_t3_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32))));

    // 4-BLAC: 4x4 - 4x4
    _t3_20 = _mm256_sub_pd(_t3_20, _t3_16);
    _t3_21 = _mm256_sub_pd(_t3_21, _t3_17);
    _t3_22 = _mm256_sub_pd(_t3_22, _t3_18);
    _t3_23 = _mm256_sub_pd(_t3_23, _t3_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i100 + 224, _t3_20);
    _asm256_storeu_pd(C + 28*i100 + 252, _t3_21);
    _asm256_storeu_pd(C + 28*i100 + 280, _t3_22);
    _asm256_storeu_pd(C + 28*i100 + 308, _t3_23);
  }

  _t4_30 = _mm256_castpd128_pd256(_mm_load_sd(C + 116));
  _t4_31 = _mm256_maskload_pd(C + 144, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_32 = _mm256_maskload_pd(C + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_33 = _asm256_loadu_pd(C + 200);
  _t4_25 = _mm256_broadcast_sd(L + 112);
  _t4_24 = _mm256_broadcast_sd(L + 113);
  _t4_23 = _mm256_broadcast_sd(L + 114);
  _t4_22 = _mm256_broadcast_sd(L + 115);
  _t4_21 = _mm256_broadcast_sd(L + 140);
  _t4_20 = _mm256_broadcast_sd(L + 141);
  _t4_19 = _mm256_broadcast_sd(L + 142);
  _t4_18 = _mm256_broadcast_sd(L + 143);
  _t4_17 = _mm256_broadcast_sd(L + 168);
  _t4_16 = _mm256_broadcast_sd(L + 169);
  _t4_15 = _mm256_broadcast_sd(L + 170);
  _t4_14 = _mm256_broadcast_sd(L + 171);
  _t4_13 = _mm256_broadcast_sd(L + 196);
  _t4_12 = _mm256_broadcast_sd(L + 197);
  _t4_11 = _mm256_broadcast_sd(L + 198);
  _t4_10 = _mm256_broadcast_sd(L + 199);
  _t4_9 = _asm256_loadu_pd(L + 112);
  _t4_8 = _asm256_loadu_pd(L + 140);
  _t4_7 = _asm256_loadu_pd(L + 168);
  _t4_6 = _asm256_loadu_pd(L + 196);
  _t4_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[144])));
  _t4_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 172)), _mm256_castpd128_pd256(_mm_load_sd(L + 200)), 0);
  _t4_3 = _mm256_maskload_pd(L + 172, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_2 = _mm256_maskload_pd(L + 200, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t4_1 = _mm256_castpd128_pd256(_mm_load_sd(&(L[202])));
  _t4_0 = _mm256_maskload_pd(L + 200, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = S(h(4, 28, 4), ( G(h(4, 28, 4), C[28,28],h(4, 28, 4)) - ( ( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, 4), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t4_109 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_30, _t4_31, 0), _mm256_shuffle_pd(_t4_32, _t4_33, 0), 32);
  _t4_110 = _mm256_permute2f128_pd(_t4_31, _mm256_shuffle_pd(_t4_32, _t4_33, 3), 32);
  _t4_111 = _mm256_blend_pd(_t4_32, _mm256_shuffle_pd(_t4_32, _t4_33, 3), 12);
  _t4_112 = _t4_33;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_201 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 32);
  _t4_202 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 32);
  _t4_203 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 49);
  _t4_204 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_25, _t4_201), _mm256_mul_pd(_t4_24, _t4_202)), _mm256_add_pd(_mm256_mul_pd(_t4_23, _t4_203), _mm256_mul_pd(_t4_22, _t4_204)));
  _t4_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_21, _t4_201), _mm256_mul_pd(_t4_20, _t4_202)), _mm256_add_pd(_mm256_mul_pd(_t4_19, _t4_203), _mm256_mul_pd(_t4_18, _t4_204)));
  _t4_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_17, _t4_201), _mm256_mul_pd(_t4_16, _t4_202)), _mm256_add_pd(_mm256_mul_pd(_t4_15, _t4_203), _mm256_mul_pd(_t4_14, _t4_204)));
  _t4_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_13, _t4_201), _mm256_mul_pd(_t4_12, _t4_202)), _mm256_add_pd(_mm256_mul_pd(_t4_11, _t4_203), _mm256_mul_pd(_t4_10, _t4_204)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_205 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 32);
  _t4_206 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 32);
  _t4_207 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 49);
  _t4_208 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 49);

  // 4-BLAC: 4x4 * 4x4
  _t4_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_7, _t2_7, 32), _mm256_permute2f128_pd(_t2_7, _t2_7, 32), 0), _t4_205), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_8, _t2_8, 32), _mm256_permute2f128_pd(_t2_8, _t2_8, 32), 0), _t4_206)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_9, _t2_9, 32), _mm256_permute2f128_pd(_t2_9, _t2_9, 32), 0), _t4_207), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_10, _t2_10, 32), _mm256_permute2f128_pd(_t2_10, _t2_10, 32), 0), _t4_208)));
  _t4_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_14, _t2_14, 32), _mm256_permute2f128_pd(_t2_14, _t2_14, 32), 0), _t4_205), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_15, _t2_15, 32), _mm256_permute2f128_pd(_t2_15, _t2_15, 32), 0), _t4_206)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_16, _t2_16, 32), _mm256_permute2f128_pd(_t2_16, _t2_16, 32), 0), _t4_207), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_17, _t2_17, 32), _mm256_permute2f128_pd(_t2_17, _t2_17, 32), 0), _t4_208)));
  _t4_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_18, _t2_18, 32), _mm256_permute2f128_pd(_t2_18, _t2_18, 32), 0), _t4_205), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_19, _t2_19, 32), _mm256_permute2f128_pd(_t2_19, _t2_19, 32), 0), _t4_206)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_20, _t2_20, 32), _mm256_permute2f128_pd(_t2_20, _t2_20, 32), 0), _t4_207), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_21, _t2_21, 32), _mm256_permute2f128_pd(_t2_21, _t2_21, 32), 0), _t4_208)));
  _t4_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_22, _t2_22, 32), _mm256_permute2f128_pd(_t2_22, _t2_22, 32), 0), _t4_205), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_23, _t2_23, 32), _mm256_permute2f128_pd(_t2_23, _t2_23, 32), 0), _t4_206)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_24, _t2_24, 32), _mm256_permute2f128_pd(_t2_24, _t2_24, 32), 0), _t4_207), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_25, _t2_25, 32), _mm256_permute2f128_pd(_t2_25, _t2_25, 32), 0), _t4_208)));

  // 4-BLAC: 4x4 + 4x4
  _t4_26 = _mm256_add_pd(_t4_47, _t4_51);
  _t4_27 = _mm256_add_pd(_t4_48, _t4_52);
  _t4_28 = _mm256_add_pd(_t4_49, _t4_53);
  _t4_29 = _mm256_add_pd(_t4_50, _t4_54);

  // 4-BLAC: 4x4 - 4x4
  _t4_55 = _mm256_sub_pd(_t4_109, _t4_26);
  _t4_56 = _mm256_sub_pd(_t4_110, _t4_27);
  _t4_57 = _mm256_sub_pd(_t4_111, _t4_28);
  _t4_58 = _mm256_sub_pd(_t4_112, _t4_29);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t4_30 = _t4_55;
  _t4_31 = _t4_56;
  _t4_32 = _t4_57;
  _t4_33 = _t4_58;

  // Generating : X[28,28] = S(h(1, 28, 4), ( G(h(1, 28, 4), X[28,28],h(1, 28, 4)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_113 = _t4_30;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_114 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_115 = _t2_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_116 = _mm256_mul_pd(_t4_114, _t4_115);

  // 4-BLAC: 1x4 / 1x4
  _t4_117 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_113), _mm256_castpd256_pd128(_t4_116)));

  // AVX Storer:
  _t4_30 = _t4_117;

  // Generating : X[28,28] = S(h(3, 28, 5), ( G(h(3, 28, 5), X[28,28],h(1, 28, 4)) - ( G(h(3, 28, 5), L[28,28],h(1, 28, 4)) Kro G(h(1, 28, 4), X[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_118 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_31, _t4_32), _mm256_unpacklo_pd(_t4_33, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_119 = _t2_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_120 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_30, _t4_30, 32), _mm256_permute2f128_pd(_t4_30, _t4_30, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t4_121 = _mm256_mul_pd(_t4_119, _t4_120);

  // 4-BLAC: 4x1 - 4x1
  _t4_122 = _mm256_sub_pd(_t4_118, _t4_121);

  // AVX Storer:
  _t4_34 = _t4_122;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 5), L[28,28],h(1, 28, 5)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_34, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_124 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_125 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_126 = _mm256_add_pd(_t4_124, _t4_125);

  // 4-BLAC: 1x4 / 1x4
  _t4_127 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_123), _mm256_castpd256_pd128(_t4_126)));

  // AVX Storer:
  _t4_35 = _t4_127;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 4)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 5)) Kro G(h(1, 28, 5), X[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_128 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_34, 2), _mm256_permute2f128_pd(_t4_34, _t4_34, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_129 = _t2_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_130 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_35, _t4_35, 32), _mm256_permute2f128_pd(_t4_35, _t4_35, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t4_131 = _mm256_mul_pd(_t4_129, _t4_130);

  // 4-BLAC: 4x1 - 4x1
  _t4_132 = _mm256_sub_pd(_t4_128, _t4_131);

  // AVX Storer:
  _t4_36 = _t4_132;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 5)) - ( ( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) ) ) + ( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_133 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_31, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_134 = _t4_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_135 = _t4_35;

  // 4-BLAC: (4x1)^T
  _t4_136 = _t4_135;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_137 = _mm256_mul_pd(_t4_134, _t4_136);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_138 = _t4_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_139 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t4_140 = _t4_139;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_141 = _mm256_mul_pd(_t4_138, _t4_140);

  // 4-BLAC: 1x4 + 1x4
  _t4_142 = _mm256_add_pd(_t4_137, _t4_141);

  // 4-BLAC: 1x4 - 1x4
  _t4_143 = _mm256_sub_pd(_t4_133, _t4_142);

  // AVX Storer:
  _t4_37 = _t4_143;

  // Generating : X[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), X[28,28],h(1, 28, 5)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_144 = _t4_37;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_145 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_146 = _t2_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_147 = _mm256_mul_pd(_t4_145, _t4_146);

  // 4-BLAC: 1x4 / 1x4
  _t4_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_144), _mm256_castpd256_pd128(_t4_147)));

  // AVX Storer:
  _t4_37 = _t4_148;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 5)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), X[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_149 = _mm256_unpackhi_pd(_mm256_blend_pd(_t4_32, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t4_33, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_150 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_151 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_35, _t4_35, 32), _mm256_permute2f128_pd(_t4_35, _t4_35, 32), 0);

  // 4-BLAC: (4x1)^T
  _t4_152 = _t4_151;

  // 4-BLAC: 4x1 Kro 1x4
  _t4_153 = _mm256_mul_pd(_t4_150, _t4_152);

  // 4-BLAC: 4x1 - 4x1
  _t4_154 = _mm256_sub_pd(_t4_149, _t4_153);

  // AVX Storer:
  _t4_38 = _t4_154;

  // Generating : X[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), X[28,28],h(1, 28, 5)) - ( G(h(2, 28, 6), L[28,28],h(1, 28, 5)) Kro G(h(1, 28, 5), X[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_155 = _t4_38;

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_156 = _t2_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_157 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_37, _t4_37, 32), _mm256_permute2f128_pd(_t4_37, _t4_37, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t4_158 = _mm256_mul_pd(_t4_156, _t4_157);

  // 4-BLAC: 4x1 - 4x1
  _t4_159 = _mm256_sub_pd(_t4_155, _t4_158);

  // AVX Storer:
  _t4_38 = _t4_159;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_160 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_36, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_161 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_162 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_163 = _mm256_add_pd(_t4_161, _t4_162);

  // 4-BLAC: 1x4 / 1x4
  _t4_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_160), _mm256_castpd256_pd128(_t4_163)));

  // AVX Storer:
  _t4_39 = _t4_164;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 6), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_165 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_38, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_166 = _t4_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_167 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t4_168 = _t4_167;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_169 = _mm256_mul_pd(_t4_166, _t4_168);

  // 4-BLAC: 1x4 - 1x4
  _t4_170 = _mm256_sub_pd(_t4_165, _t4_169);

  // AVX Storer:
  _t4_40 = _t4_170;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 6), L[28,28],h(1, 28, 6)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_171 = _t4_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_172 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_173 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t4_174 = _mm256_add_pd(_t4_172, _t4_173);

  // 4-BLAC: 1x4 / 1x4
  _t4_175 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_171), _mm256_castpd256_pd128(_t4_174)));

  // AVX Storer:
  _t4_40 = _t4_175;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(2, 28, 4)) - ( G(h(1, 28, 7), L[28,28],h(1, 28, 6)) Kro G(h(1, 28, 6), X[28,28],h(2, 28, 4)) ) ),h(2, 28, 4))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_176 = _mm256_unpackhi_pd(_mm256_blend_pd(_t4_36, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t4_38, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_177 = _t2_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_178 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 1x4
  _t4_179 = _mm256_mul_pd(_t4_177, _t4_178);

  // 4-BLAC: 1x4 - 1x4
  _t4_180 = _mm256_sub_pd(_t4_176, _t4_179);

  // AVX Storer:
  _t4_41 = _t4_180;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 6)) - ( ( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), X[28,28],h(2, 28, 4)) ) ) + ( G(h(1, 28, 6), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_181 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_32, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_32, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_182 = _t4_3;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_183 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t4_184 = _t4_183;

  // 4-BLAC: 1x4 * 4x1
  _t4_185 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_182, _t4_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_182, _t4_184), _mm256_mul_pd(_t4_182, _t4_184), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_182, _t4_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_182, _t4_184), _mm256_mul_pd(_t4_182, _t4_184), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_182, _t4_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_182, _t4_184), _mm256_mul_pd(_t4_182, _t4_184), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_186 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_187 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t4_188 = _t4_187;

  // 4-BLAC: 1x4 * 4x1
  _t4_189 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_mul_pd(_t4_186, _t4_188), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_mul_pd(_t4_186, _t4_188), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_186, _t4_188), _mm256_mul_pd(_t4_186, _t4_188), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t4_190 = _mm256_add_pd(_t4_185, _t4_189);

  // 4-BLAC: 1x4 - 1x4
  _t4_191 = _mm256_sub_pd(_t4_181, _t4_190);

  // AVX Storer:
  _t4_42 = _t4_191;

  // Generating : X[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), X[28,28],h(1, 28, 6)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_192 = _t4_42;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_193 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_194 = _t2_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_195 = _mm256_mul_pd(_t4_193, _t4_194);

  // 4-BLAC: 1x4 / 1x4
  _t4_196 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_192), _mm256_castpd256_pd128(_t4_195)));

  // AVX Storer:
  _t4_42 = _t4_196;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 7), L[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), X[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_197 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_33, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t4_33, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_198 = _t4_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_199 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t4_200 = _t4_199;

  // 4-BLAC: 1x4 * 4x1
  _t4_59 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_198, _t4_200), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_198, _t4_200), _mm256_mul_pd(_t4_198, _t4_200), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_198, _t4_200), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_198, _t4_200), _mm256_mul_pd(_t4_198, _t4_200), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_198, _t4_200), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_198, _t4_200), _mm256_mul_pd(_t4_198, _t4_200), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_60 = _mm256_sub_pd(_t4_197, _t4_59);

  // AVX Storer:
  _t4_43 = _t4_60;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 7), L[28,28],h(1, 28, 6)) Kro G(h(1, 28, 6), X[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_61 = _t4_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_62 = _t4_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_63 = _t4_42;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_64 = _mm256_mul_pd(_t4_62, _t4_63);

  // 4-BLAC: 1x4 - 1x4
  _t4_65 = _mm256_sub_pd(_t4_61, _t4_64);

  // AVX Storer:
  _t4_43 = _t4_65;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_66 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_41, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_67 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_68 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t4_69 = _mm256_add_pd(_t4_67, _t4_68);

  // 4-BLAC: 1x4 / 1x4
  _t4_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_66), _mm256_castpd256_pd128(_t4_69)));

  // AVX Storer:
  _t4_44 = _t4_70;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 7), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_71 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_41, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_72 = _t4_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_73 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t4_74 = _t4_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_75 = _mm256_mul_pd(_t4_72, _t4_74);

  // 4-BLAC: 1x4 - 1x4
  _t4_76 = _mm256_sub_pd(_t4_71, _t4_75);

  // AVX Storer:
  _t4_45 = _t4_76;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_77 = _t4_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_78 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_79 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t4_80 = _mm256_add_pd(_t4_78, _t4_79);

  // 4-BLAC: 1x4 / 1x4
  _t4_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_77), _mm256_castpd256_pd128(_t4_80)));

  // AVX Storer:
  _t4_45 = _t4_81;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 7), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_82 = _t4_43;

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_83 = _mm256_blend_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_84 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t4_85 = _t4_84;

  // 4-BLAC: 1x4 * 4x1
  _t4_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_83, _t4_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_83, _t4_85), _mm256_mul_pd(_t4_83, _t4_85), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_83, _t4_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_83, _t4_85), _mm256_mul_pd(_t4_83, _t4_85), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_83, _t4_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_83, _t4_85), _mm256_mul_pd(_t4_83, _t4_85), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t4_87 = _mm256_sub_pd(_t4_82, _t4_86);

  // AVX Storer:
  _t4_43 = _t4_87;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 7), L[28,28],h(1, 28, 7)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_88 = _t4_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_89 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_90 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t4_91 = _mm256_add_pd(_t4_89, _t4_90);

  // 4-BLAC: 1x4 / 1x4
  _t4_92 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_88), _mm256_castpd256_pd128(_t4_91)));

  // AVX Storer:
  _t4_43 = _t4_92;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 7)) - ( ( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), X[28,28],h(3, 28, 4)) ) ) + ( G(h(1, 28, 7), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_93 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t4_33, _t4_33, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_94 = _t4_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_95 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_unpacklo_pd(_t4_43, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t4_96 = _t4_95;

  // 4-BLAC: 1x4 * 4x1
  _t4_97 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_94, _t4_96), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_94, _t4_96), _mm256_mul_pd(_t4_94, _t4_96), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_94, _t4_96), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_94, _t4_96), _mm256_mul_pd(_t4_94, _t4_96), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_94, _t4_96), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_94, _t4_96), _mm256_mul_pd(_t4_94, _t4_96), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_unpacklo_pd(_t4_43, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_99 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t4_100 = _t4_99;

  // 4-BLAC: 1x4 * 4x1
  _t4_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t4_98, _t4_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_98, _t4_100), _mm256_mul_pd(_t4_98, _t4_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t4_98, _t4_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_98, _t4_100), _mm256_mul_pd(_t4_98, _t4_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t4_98, _t4_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t4_98, _t4_100), _mm256_mul_pd(_t4_98, _t4_100), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t4_102 = _mm256_add_pd(_t4_97, _t4_101);

  // 4-BLAC: 1x4 - 1x4
  _t4_103 = _mm256_sub_pd(_t4_93, _t4_102);

  // AVX Storer:
  _t4_46 = _t4_103;

  // Generating : X[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), X[28,28],h(1, 28, 7)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_104 = _t4_46;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t4_105 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_106 = _t2_0;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_107 = _mm256_mul_pd(_t4_105, _t4_106);

  // 4-BLAC: 1x4 / 1x4
  _t4_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_104), _mm256_castpd256_pd128(_t4_107)));

  // AVX Storer:
  _t4_46 = _t4_108;

  // Generating : X[28,28] = Sum_{k219} ( S(h(4, 28, k219 + 8), ( G(h(4, 28, k219 + 8), C[28,28],h(4, 28, 4)) - ( G(h(4, 28, k219 + 8), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), X[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 4)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t4_209 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 32);
  _t4_210 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 32);
  _t4_211 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 49);
  _t4_212 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_8), _mm256_unpacklo_pd(_t2_9, _t2_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_15), _mm256_unpacklo_pd(_t2_16, _t2_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_18, _t2_19), _mm256_unpacklo_pd(_t2_20, _t2_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_22, _t2_23), _mm256_unpacklo_pd(_t2_24, _t2_25), 32)), 49);


  for( int k219 = 0; k219 <= 19; k219+=4 ) {
    _t5_20 = _asm256_loadu_pd(C + 28*k219 + 228);
    _t5_21 = _asm256_loadu_pd(C + 28*k219 + 256);
    _t5_22 = _asm256_loadu_pd(C + 28*k219 + 284);
    _t5_23 = _asm256_loadu_pd(C + 28*k219 + 312);
    _t5_15 = _mm256_broadcast_sd(L + 28*k219 + 224);
    _t5_14 = _mm256_broadcast_sd(L + 28*k219 + 225);
    _t5_13 = _mm256_broadcast_sd(L + 28*k219 + 226);
    _t5_12 = _mm256_broadcast_sd(L + 28*k219 + 227);
    _t5_11 = _mm256_broadcast_sd(L + 28*k219 + 252);
    _t5_10 = _mm256_broadcast_sd(L + 28*k219 + 253);
    _t5_9 = _mm256_broadcast_sd(L + 28*k219 + 254);
    _t5_8 = _mm256_broadcast_sd(L + 28*k219 + 255);
    _t5_7 = _mm256_broadcast_sd(L + 28*k219 + 280);
    _t5_6 = _mm256_broadcast_sd(L + 28*k219 + 281);
    _t5_5 = _mm256_broadcast_sd(L + 28*k219 + 282);
    _t5_4 = _mm256_broadcast_sd(L + 28*k219 + 283);
    _t5_3 = _mm256_broadcast_sd(L + 28*k219 + 308);
    _t5_2 = _mm256_broadcast_sd(L + 28*k219 + 309);
    _t5_1 = _mm256_broadcast_sd(L + 28*k219 + 310);
    _t5_0 = _mm256_broadcast_sd(L + 28*k219 + 311);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t5_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t4_209), _mm256_mul_pd(_t5_14, _t4_210)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t4_211), _mm256_mul_pd(_t5_12, _t4_212)));
    _t5_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t4_209), _mm256_mul_pd(_t5_10, _t4_210)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t4_211), _mm256_mul_pd(_t5_8, _t4_212)));
    _t5_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t4_209), _mm256_mul_pd(_t5_6, _t4_210)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t4_211), _mm256_mul_pd(_t5_4, _t4_212)));
    _t5_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_3, _t4_209), _mm256_mul_pd(_t5_2, _t4_210)), _mm256_add_pd(_mm256_mul_pd(_t5_1, _t4_211), _mm256_mul_pd(_t5_0, _t4_212)));

    // 4-BLAC: 4x4 - 4x4
    _t5_20 = _mm256_sub_pd(_t5_20, _t5_16);
    _t5_21 = _mm256_sub_pd(_t5_21, _t5_17);
    _t5_22 = _mm256_sub_pd(_t5_22, _t5_18);
    _t5_23 = _mm256_sub_pd(_t5_23, _t5_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*k219 + 228, _t5_20);
    _asm256_storeu_pd(C + 28*k219 + 256, _t5_21);
    _asm256_storeu_pd(C + 28*k219 + 284, _t5_22);
    _asm256_storeu_pd(C + 28*k219 + 312, _t5_23);
  }


  // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, i100 + 8), ( G(h(4, 28, i100 + 8), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, i100 + 8), L[28,28],h(4, 28, 4)) * G(h(4, 28, 4), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t6_0 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t4_30, _mm256_blend_pd(_mm256_unpacklo_pd(_t4_35, _t4_37), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_42, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_unpacklo_pd(_t4_43, _t4_46), 32), 0), 32);
  _t6_1 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t4_35, _t4_37), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_42, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_unpacklo_pd(_t4_43, _t4_46), 32), 3), 32);
  _t6_2 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_42, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_39, _t4_40), _mm256_unpacklo_pd(_t4_42, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_unpacklo_pd(_t4_43, _t4_46), 32), 3), 12);
  _t6_3 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_44, _t4_45), _mm256_unpacklo_pd(_t4_43, _t4_46), 32);


  for( int i100 = 0; i100 <= 19; i100+=4 ) {
    _t7_20 = _asm256_loadu_pd(C + 28*i100 + 228);
    _t7_21 = _asm256_loadu_pd(C + 28*i100 + 256);
    _t7_22 = _asm256_loadu_pd(C + 28*i100 + 284);
    _t7_23 = _asm256_loadu_pd(C + 28*i100 + 312);
    _t7_15 = _mm256_broadcast_sd(L + 28*i100 + 228);
    _t7_14 = _mm256_broadcast_sd(L + 28*i100 + 229);
    _t7_13 = _mm256_broadcast_sd(L + 28*i100 + 230);
    _t7_12 = _mm256_broadcast_sd(L + 28*i100 + 231);
    _t7_11 = _mm256_broadcast_sd(L + 28*i100 + 256);
    _t7_10 = _mm256_broadcast_sd(L + 28*i100 + 257);
    _t7_9 = _mm256_broadcast_sd(L + 28*i100 + 258);
    _t7_8 = _mm256_broadcast_sd(L + 28*i100 + 259);
    _t7_7 = _mm256_broadcast_sd(L + 28*i100 + 284);
    _t7_6 = _mm256_broadcast_sd(L + 28*i100 + 285);
    _t7_5 = _mm256_broadcast_sd(L + 28*i100 + 286);
    _t7_4 = _mm256_broadcast_sd(L + 28*i100 + 287);
    _t7_3 = _mm256_broadcast_sd(L + 28*i100 + 312);
    _t7_2 = _mm256_broadcast_sd(L + 28*i100 + 313);
    _t7_1 = _mm256_broadcast_sd(L + 28*i100 + 314);
    _t7_0 = _mm256_broadcast_sd(L + 28*i100 + 315);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t7_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t6_0), _mm256_mul_pd(_t7_14, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t6_2), _mm256_mul_pd(_t7_12, _t6_3)));
    _t7_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t6_0), _mm256_mul_pd(_t7_10, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t6_2), _mm256_mul_pd(_t7_8, _t6_3)));
    _t7_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t6_0), _mm256_mul_pd(_t7_6, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t6_2), _mm256_mul_pd(_t7_4, _t6_3)));
    _t7_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_3, _t6_0), _mm256_mul_pd(_t7_2, _t6_1)), _mm256_add_pd(_mm256_mul_pd(_t7_1, _t6_2), _mm256_mul_pd(_t7_0, _t6_3)));

    // 4-BLAC: 4x4 - 4x4
    _t7_20 = _mm256_sub_pd(_t7_20, _t7_16);
    _t7_21 = _mm256_sub_pd(_t7_21, _t7_17);
    _t7_22 = _mm256_sub_pd(_t7_22, _t7_18);
    _t7_23 = _mm256_sub_pd(_t7_23, _t7_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i100 + 228, _t7_20);
    _asm256_storeu_pd(C + 28*i100 + 256, _t7_21);
    _asm256_storeu_pd(C + 28*i100 + 284, _t7_22);
    _asm256_storeu_pd(C + 28*i100 + 312, _t7_23);
  }

  _t8_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[224])));
  _t8_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[232])));
  _t8_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[225])));
  _t8_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[226])));
  _t8_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[227])));
  _t8_11 = _asm256_loadu_pd(C + 252);
  _t8_12 = _asm256_loadu_pd(C + 280);
  _t8_13 = _asm256_loadu_pd(C + 308);
  _t8_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 260)), _mm256_castpd128_pd256(_mm_load_sd(L + 288))), _mm256_castpd128_pd256(_mm_load_sd(L + 316)), 32);
  _t8_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[261])));
  _t8_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 289)), _mm256_castpd128_pd256(_mm_load_sd(L + 317)), 0);
  _t8_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[290])));
  _t8_1 = _mm256_broadcast_sd(&(L[318]));
  _t8_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[319])));
  _t8_48 = _asm256_loadu_pd(C + 228);
  _t8_49 = _asm256_loadu_pd(C + 256);
  _t8_50 = _asm256_loadu_pd(C + 284);
  _t8_51 = _asm256_loadu_pd(C + 312);

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_52 = _t8_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_53 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_54 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t8_55 = _mm256_add_pd(_t8_53, _t8_54);

  // 4-BLAC: 1x4 / 1x4
  _t8_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_52), _mm256_castpd256_pd128(_t8_55)));

  // AVX Storer:
  _t8_7 = _t8_56;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 8), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_57 = _t8_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_58 = _t8_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_59 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t8_60 = _t8_59;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_61 = _mm256_mul_pd(_t8_58, _t8_60);

  // 4-BLAC: 1x4 - 1x4
  _t8_62 = _mm256_sub_pd(_t8_57, _t8_61);

  // AVX Storer:
  _t8_8 = _t8_62;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_63 = _t8_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_64 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_65 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t8_66 = _mm256_add_pd(_t8_64, _t8_65);

  // 4-BLAC: 1x4 / 1x4
  _t8_67 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_63), _mm256_castpd256_pd128(_t8_66)));

  // AVX Storer:
  _t8_8 = _t8_67;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 8), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_68 = _t8_9;

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_69 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_70 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t8_71 = _t8_70;

  // 4-BLAC: 1x4 * 4x1
  _t8_72 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_69, _t8_71), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_69, _t8_71), _mm256_mul_pd(_t8_69, _t8_71), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_69, _t8_71), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_69, _t8_71), _mm256_mul_pd(_t8_69, _t8_71), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_69, _t8_71), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_69, _t8_71), _mm256_mul_pd(_t8_69, _t8_71), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_73 = _mm256_sub_pd(_t8_68, _t8_72);

  // AVX Storer:
  _t8_9 = _t8_73;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_74 = _t8_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_75 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_76 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_77 = _mm256_add_pd(_t8_75, _t8_76);

  // 4-BLAC: 1x4 / 1x4
  _t8_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_74), _mm256_castpd256_pd128(_t8_77)));

  // AVX Storer:
  _t8_9 = _t8_78;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 8), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_79 = _t8_10;

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_80 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_81 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t8_82 = _t8_81;

  // 4-BLAC: 1x4 * 4x1
  _t8_83 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_80, _t8_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_80, _t8_82), _mm256_mul_pd(_t8_80, _t8_82), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_80, _t8_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_80, _t8_82), _mm256_mul_pd(_t8_80, _t8_82), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_80, _t8_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_80, _t8_82), _mm256_mul_pd(_t8_80, _t8_82), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_84 = _mm256_sub_pd(_t8_79, _t8_83);

  // AVX Storer:
  _t8_10 = _t8_84;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_85 = _t8_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_86 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_87 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t8_88 = _mm256_add_pd(_t8_86, _t8_87);

  // 4-BLAC: 1x4 / 1x4
  _t8_89 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_85), _mm256_castpd256_pd128(_t8_88)));

  // AVX Storer:
  _t8_10 = _t8_89;

  // Generating : X[28,28] = S(h(3, 28, 9), ( G(h(3, 28, 9), X[28,28],h(4, 28, 0)) - ( G(h(3, 28, 9), L[28,28],h(1, 28, 8)) * G(h(1, 28, 8), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t8_90 = _t8_11;
  _t8_91 = _t8_12;
  _t8_92 = _t8_13;
  _t8_93 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t8_94 = _t8_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t8_95 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_94, _t8_94, 32), _mm256_permute2f128_pd(_t8_94, _t8_94, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));
  _t8_96 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_94, _t8_94, 32), _mm256_permute2f128_pd(_t8_94, _t8_94, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));
  _t8_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_94, _t8_94, 49), _mm256_permute2f128_pd(_t8_94, _t8_94, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));
  _t8_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_94, _t8_94, 49), _mm256_permute2f128_pd(_t8_94, _t8_94, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));

  // 4-BLAC: 4x4 - 4x4
  _t8_99 = _mm256_sub_pd(_t8_90, _t8_95);
  _t8_100 = _mm256_sub_pd(_t8_91, _t8_96);
  _t8_101 = _mm256_sub_pd(_t8_92, _t8_97);
  _t8_102 = _mm256_sub_pd(_t8_93, _t8_98);

  // AVX Storer:
  _t8_11 = _t8_99;
  _t8_12 = _t8_100;
  _t8_13 = _t8_101;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_11, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_104 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_105 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t8_106 = _mm256_add_pd(_t8_104, _t8_105);

  // 4-BLAC: 1x4 / 1x4
  _t8_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_103), _mm256_castpd256_pd128(_t8_106)));

  // AVX Storer:
  _t8_14 = _t8_107;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 9), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_108 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_11, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_109 = _t8_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_110 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t8_111 = _t8_110;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_112 = _mm256_mul_pd(_t8_109, _t8_111);

  // 4-BLAC: 1x4 - 1x4
  _t8_113 = _mm256_sub_pd(_t8_108, _t8_112);

  // AVX Storer:
  _t8_15 = _t8_113;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_114 = _t8_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_115 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_116 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t8_117 = _mm256_add_pd(_t8_115, _t8_116);

  // 4-BLAC: 1x4 / 1x4
  _t8_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_114), _mm256_castpd256_pd128(_t8_117)));

  // AVX Storer:
  _t8_15 = _t8_118;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 9), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_119 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_11, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_120 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_121 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t8_122 = _t8_121;

  // 4-BLAC: 1x4 * 4x1
  _t8_123 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_120, _t8_122), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_120, _t8_122), _mm256_mul_pd(_t8_120, _t8_122), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_120, _t8_122), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_120, _t8_122), _mm256_mul_pd(_t8_120, _t8_122), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_120, _t8_122), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_120, _t8_122), _mm256_mul_pd(_t8_120, _t8_122), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_124 = _mm256_sub_pd(_t8_119, _t8_123);

  // AVX Storer:
  _t8_16 = _t8_124;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_125 = _t8_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_126 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_127 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_128 = _mm256_add_pd(_t8_126, _t8_127);

  // 4-BLAC: 1x4 / 1x4
  _t8_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_125), _mm256_castpd256_pd128(_t8_128)));

  // AVX Storer:
  _t8_16 = _t8_129;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 9), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_130 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_11, _t8_11, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_131 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_132 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t8_133 = _t8_132;

  // 4-BLAC: 1x4 * 4x1
  _t8_134 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_131, _t8_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_131, _t8_133), _mm256_mul_pd(_t8_131, _t8_133), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_131, _t8_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_131, _t8_133), _mm256_mul_pd(_t8_131, _t8_133), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_131, _t8_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_131, _t8_133), _mm256_mul_pd(_t8_131, _t8_133), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_135 = _mm256_sub_pd(_t8_130, _t8_134);

  // AVX Storer:
  _t8_17 = _t8_135;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_136 = _t8_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_137 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_138 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t8_139 = _mm256_add_pd(_t8_137, _t8_138);

  // 4-BLAC: 1x4 / 1x4
  _t8_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_136), _mm256_castpd256_pd128(_t8_139)));

  // AVX Storer:
  _t8_17 = _t8_140;

  // Generating : X[28,28] = S(h(2, 28, 10), ( G(h(2, 28, 10), X[28,28],h(4, 28, 0)) - ( G(h(2, 28, 10), L[28,28],h(1, 28, 9)) * G(h(1, 28, 9), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t8_141 = _t8_12;
  _t8_142 = _t8_13;
  _t8_143 = _mm256_setzero_pd();
  _t8_144 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t8_145 = _t8_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t8_146 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_145, _t8_145, 32), _mm256_permute2f128_pd(_t8_145, _t8_145, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32));
  _t8_147 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_145, _t8_145, 32), _mm256_permute2f128_pd(_t8_145, _t8_145, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32));
  _t8_148 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_145, _t8_145, 49), _mm256_permute2f128_pd(_t8_145, _t8_145, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32));
  _t8_149 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_145, _t8_145, 49), _mm256_permute2f128_pd(_t8_145, _t8_145, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32));

  // 4-BLAC: 4x4 - 4x4
  _t8_150 = _mm256_sub_pd(_t8_141, _t8_146);
  _t8_151 = _mm256_sub_pd(_t8_142, _t8_147);
  _t8_152 = _mm256_sub_pd(_t8_143, _t8_148);
  _t8_153 = _mm256_sub_pd(_t8_144, _t8_149);

  // AVX Storer:
  _t8_12 = _t8_150;
  _t8_13 = _t8_151;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_154 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_12, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_155 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_156 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t8_157 = _mm256_add_pd(_t8_155, _t8_156);

  // 4-BLAC: 1x4 / 1x4
  _t8_158 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_154), _mm256_castpd256_pd128(_t8_157)));

  // AVX Storer:
  _t8_18 = _t8_158;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 10), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_159 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_12, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_160 = _t8_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_161 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t8_162 = _t8_161;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_163 = _mm256_mul_pd(_t8_160, _t8_162);

  // 4-BLAC: 1x4 - 1x4
  _t8_164 = _mm256_sub_pd(_t8_159, _t8_163);

  // AVX Storer:
  _t8_19 = _t8_164;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_165 = _t8_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_166 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_167 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t8_168 = _mm256_add_pd(_t8_166, _t8_167);

  // 4-BLAC: 1x4 / 1x4
  _t8_169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_165), _mm256_castpd256_pd128(_t8_168)));

  // AVX Storer:
  _t8_19 = _t8_169;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 10), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_170 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_12, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_171 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_172 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t8_173 = _t8_172;

  // 4-BLAC: 1x4 * 4x1
  _t8_174 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_171, _t8_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_171, _t8_173), _mm256_mul_pd(_t8_171, _t8_173), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_171, _t8_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_171, _t8_173), _mm256_mul_pd(_t8_171, _t8_173), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_171, _t8_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_171, _t8_173), _mm256_mul_pd(_t8_171, _t8_173), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_175 = _mm256_sub_pd(_t8_170, _t8_174);

  // AVX Storer:
  _t8_20 = _t8_175;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_176 = _t8_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_177 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_178 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_179 = _mm256_add_pd(_t8_177, _t8_178);

  // 4-BLAC: 1x4 / 1x4
  _t8_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_176), _mm256_castpd256_pd128(_t8_179)));

  // AVX Storer:
  _t8_20 = _t8_180;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 10), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_181 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_12, _t8_12, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_182 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_183 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t8_184 = _t8_183;

  // 4-BLAC: 1x4 * 4x1
  _t8_185 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_182, _t8_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_182, _t8_184), _mm256_mul_pd(_t8_182, _t8_184), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_182, _t8_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_182, _t8_184), _mm256_mul_pd(_t8_182, _t8_184), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_182, _t8_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_182, _t8_184), _mm256_mul_pd(_t8_182, _t8_184), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_186 = _mm256_sub_pd(_t8_181, _t8_185);

  // AVX Storer:
  _t8_21 = _t8_186;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_187 = _t8_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_188 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_189 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t8_190 = _mm256_add_pd(_t8_188, _t8_189);

  // 4-BLAC: 1x4 / 1x4
  _t8_191 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_187), _mm256_castpd256_pd128(_t8_190)));

  // AVX Storer:
  _t8_21 = _t8_191;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(4, 28, 0)) - ( G(h(1, 28, 11), L[28,28],h(1, 28, 10)) Kro G(h(1, 28, 10), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_192 = _t8_1;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t8_42 = _mm256_mul_pd(_t8_192, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32));

  // 4-BLAC: 1x4 - 1x4
  _t8_13 = _mm256_sub_pd(_t8_13, _t8_42);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_193 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_194 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_195 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t8_196 = _mm256_add_pd(_t8_194, _t8_195);

  // 4-BLAC: 1x4 / 1x4
  _t8_197 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_193), _mm256_castpd256_pd128(_t8_196)));

  // AVX Storer:
  _t8_22 = _t8_197;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 11), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_198 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_199 = _t8_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_200 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t8_201 = _t8_200;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_202 = _mm256_mul_pd(_t8_199, _t8_201);

  // 4-BLAC: 1x4 - 1x4
  _t8_203 = _mm256_sub_pd(_t8_198, _t8_202);

  // AVX Storer:
  _t8_23 = _t8_203;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_204 = _t8_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_205 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_206 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t8_207 = _mm256_add_pd(_t8_205, _t8_206);

  // 4-BLAC: 1x4 / 1x4
  _t8_208 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_204), _mm256_castpd256_pd128(_t8_207)));

  // AVX Storer:
  _t8_23 = _t8_208;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 11), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_209 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_13, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_210 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_211 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t8_212 = _t8_211;

  // 4-BLAC: 1x4 * 4x1
  _t8_213 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_210, _t8_212), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_210, _t8_212), _mm256_mul_pd(_t8_210, _t8_212), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_210, _t8_212), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_210, _t8_212), _mm256_mul_pd(_t8_210, _t8_212), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_210, _t8_212), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_210, _t8_212), _mm256_mul_pd(_t8_210, _t8_212), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_214 = _mm256_sub_pd(_t8_209, _t8_213);

  // AVX Storer:
  _t8_24 = _t8_214;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_215 = _t8_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_216 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_217 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_218 = _mm256_add_pd(_t8_216, _t8_217);

  // 4-BLAC: 1x4 / 1x4
  _t8_219 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_215), _mm256_castpd256_pd128(_t8_218)));

  // AVX Storer:
  _t8_24 = _t8_219;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 11), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_220 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_13, _t8_13, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_221 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_222 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t8_223 = _t8_222;

  // 4-BLAC: 1x4 * 4x1
  _t8_224 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_221, _t8_223), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_221, _t8_223), _mm256_mul_pd(_t8_221, _t8_223), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_221, _t8_223), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_221, _t8_223), _mm256_mul_pd(_t8_221, _t8_223), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_221, _t8_223), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_221, _t8_223), _mm256_mul_pd(_t8_221, _t8_223), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_225 = _mm256_sub_pd(_t8_220, _t8_224);

  // AVX Storer:
  _t8_25 = _t8_225;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_226 = _t8_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_227 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_228 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t8_229 = _mm256_add_pd(_t8_227, _t8_228);

  // 4-BLAC: 1x4 / 1x4
  _t8_230 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_226), _mm256_castpd256_pd128(_t8_229)));

  // AVX Storer:
  _t8_25 = _t8_230;

  // Generating : X[28,28] = S(h(4, 28, 8), ( G(h(4, 28, 8), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, 8), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t8_410 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 32);
  _t8_411 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 32);
  _t8_412 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 49);
  _t8_413 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 49);

  // 4-BLAC: 4x4 * 4x4
  _t8_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_7, _t8_7, 32), _mm256_permute2f128_pd(_t8_7, _t8_7, 32), 0), _t8_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_8, _t8_8, 32), _mm256_permute2f128_pd(_t8_8, _t8_8, 32), 0), _t8_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_9, _t8_9, 32), _mm256_permute2f128_pd(_t8_9, _t8_9, 32), 0), _t8_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_10, _t8_10, 32), _mm256_permute2f128_pd(_t8_10, _t8_10, 32), 0), _t8_413)));
  _t8_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_14, _t8_14, 32), _mm256_permute2f128_pd(_t8_14, _t8_14, 32), 0), _t8_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_15, _t8_15, 32), _mm256_permute2f128_pd(_t8_15, _t8_15, 32), 0), _t8_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_16, _t8_16, 32), _mm256_permute2f128_pd(_t8_16, _t8_16, 32), 0), _t8_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_17, _t8_17, 32), _mm256_permute2f128_pd(_t8_17, _t8_17, 32), 0), _t8_413)));
  _t8_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_18, _t8_18, 32), _mm256_permute2f128_pd(_t8_18, _t8_18, 32), 0), _t8_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 0), _t8_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_20, _t8_20, 32), _mm256_permute2f128_pd(_t8_20, _t8_20, 32), 0), _t8_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_21, _t8_21, 32), _mm256_permute2f128_pd(_t8_21, _t8_21, 32), 0), _t8_413)));
  _t8_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_22, _t8_22, 32), _mm256_permute2f128_pd(_t8_22, _t8_22, 32), 0), _t8_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_23, _t8_23, 32), _mm256_permute2f128_pd(_t8_23, _t8_23, 32), 0), _t8_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_24, _t8_24, 32), _mm256_permute2f128_pd(_t8_24, _t8_24, 32), 0), _t8_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_25, _t8_25, 32), _mm256_permute2f128_pd(_t8_25, _t8_25, 32), 0), _t8_413)));

  // 4-BLAC: 4x4 - 4x4
  _t8_48 = _mm256_sub_pd(_t8_48, _t8_44);
  _t8_49 = _mm256_sub_pd(_t8_49, _t8_45);
  _t8_50 = _mm256_sub_pd(_t8_50, _t8_46);
  _t8_51 = _mm256_sub_pd(_t8_51, _t8_47);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_231 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_48, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_232 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_233 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_234 = _mm256_add_pd(_t8_232, _t8_233);

  // 4-BLAC: 1x4 / 1x4
  _t8_235 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_231), _mm256_castpd256_pd128(_t8_234)));

  // AVX Storer:
  _t8_26 = _t8_235;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 8), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_236 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_48, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_237 = _t8_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_238 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t8_239 = _t8_238;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_240 = _mm256_mul_pd(_t8_237, _t8_239);

  // 4-BLAC: 1x4 - 1x4
  _t8_241 = _mm256_sub_pd(_t8_236, _t8_240);

  // AVX Storer:
  _t8_27 = _t8_241;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_242 = _t8_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_243 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_244 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t8_245 = _mm256_add_pd(_t8_243, _t8_244);

  // 4-BLAC: 1x4 / 1x4
  _t8_246 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_242), _mm256_castpd256_pd128(_t8_245)));

  // AVX Storer:
  _t8_27 = _t8_246;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 8), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_247 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_48, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_48, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_248 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_249 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t8_250 = _t8_249;

  // 4-BLAC: 1x4 * 4x1
  _t8_251 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_248, _t8_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_248, _t8_250), _mm256_mul_pd(_t8_248, _t8_250), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_248, _t8_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_248, _t8_250), _mm256_mul_pd(_t8_248, _t8_250), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_248, _t8_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_248, _t8_250), _mm256_mul_pd(_t8_248, _t8_250), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_252 = _mm256_sub_pd(_t8_247, _t8_251);

  // AVX Storer:
  _t8_28 = _t8_252;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_253 = _t8_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_254 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_255 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t8_256 = _mm256_add_pd(_t8_254, _t8_255);

  // 4-BLAC: 1x4 / 1x4
  _t8_257 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_253), _mm256_castpd256_pd128(_t8_256)));

  // AVX Storer:
  _t8_28 = _t8_257;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 8), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_258 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_48, _t8_48, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_259 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_260 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t8_261 = _t8_260;

  // 4-BLAC: 1x4 * 4x1
  _t8_262 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_259, _t8_261), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_259, _t8_261), _mm256_mul_pd(_t8_259, _t8_261), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_259, _t8_261), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_259, _t8_261), _mm256_mul_pd(_t8_259, _t8_261), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_259, _t8_261), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_259, _t8_261), _mm256_mul_pd(_t8_259, _t8_261), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_263 = _mm256_sub_pd(_t8_258, _t8_262);

  // AVX Storer:
  _t8_29 = _t8_263;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 8), L[28,28],h(1, 28, 8)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_264 = _t8_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_265 = _t8_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_266 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t8_267 = _mm256_add_pd(_t8_265, _t8_266);

  // 4-BLAC: 1x4 / 1x4
  _t8_268 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_264), _mm256_castpd256_pd128(_t8_267)));

  // AVX Storer:
  _t8_29 = _t8_268;

  // Generating : X[28,28] = S(h(3, 28, 9), ( G(h(3, 28, 9), X[28,28],h(4, 28, 4)) - ( G(h(3, 28, 9), L[28,28],h(1, 28, 8)) * G(h(1, 28, 8), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t8_269 = _t8_49;
  _t8_270 = _t8_50;
  _t8_271 = _t8_51;
  _t8_272 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t8_273 = _t8_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t8_274 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_273, _t8_273, 32), _mm256_permute2f128_pd(_t8_273, _t8_273, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32));
  _t8_275 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_273, _t8_273, 32), _mm256_permute2f128_pd(_t8_273, _t8_273, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32));
  _t8_276 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_273, _t8_273, 49), _mm256_permute2f128_pd(_t8_273, _t8_273, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32));
  _t8_277 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_273, _t8_273, 49), _mm256_permute2f128_pd(_t8_273, _t8_273, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32));

  // 4-BLAC: 4x4 - 4x4
  _t8_278 = _mm256_sub_pd(_t8_269, _t8_274);
  _t8_279 = _mm256_sub_pd(_t8_270, _t8_275);
  _t8_280 = _mm256_sub_pd(_t8_271, _t8_276);
  _t8_281 = _mm256_sub_pd(_t8_272, _t8_277);

  // AVX Storer:
  _t8_49 = _t8_278;
  _t8_50 = _t8_279;
  _t8_51 = _t8_280;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_282 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_49, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_283 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_284 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_285 = _mm256_add_pd(_t8_283, _t8_284);

  // 4-BLAC: 1x4 / 1x4
  _t8_286 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_282), _mm256_castpd256_pd128(_t8_285)));

  // AVX Storer:
  _t8_30 = _t8_286;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 9), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_287 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_49, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_288 = _t8_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_289 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t8_290 = _t8_289;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_291 = _mm256_mul_pd(_t8_288, _t8_290);

  // 4-BLAC: 1x4 - 1x4
  _t8_292 = _mm256_sub_pd(_t8_287, _t8_291);

  // AVX Storer:
  _t8_31 = _t8_292;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_293 = _t8_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_294 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_295 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t8_296 = _mm256_add_pd(_t8_294, _t8_295);

  // 4-BLAC: 1x4 / 1x4
  _t8_297 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_293), _mm256_castpd256_pd128(_t8_296)));

  // AVX Storer:
  _t8_31 = _t8_297;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 9), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_298 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_49, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_49, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_299 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_300 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t8_301 = _t8_300;

  // 4-BLAC: 1x4 * 4x1
  _t8_302 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_299, _t8_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_299, _t8_301), _mm256_mul_pd(_t8_299, _t8_301), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_299, _t8_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_299, _t8_301), _mm256_mul_pd(_t8_299, _t8_301), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_299, _t8_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_299, _t8_301), _mm256_mul_pd(_t8_299, _t8_301), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_303 = _mm256_sub_pd(_t8_298, _t8_302);

  // AVX Storer:
  _t8_32 = _t8_303;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_304 = _t8_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_305 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_306 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t8_307 = _mm256_add_pd(_t8_305, _t8_306);

  // 4-BLAC: 1x4 / 1x4
  _t8_308 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_304), _mm256_castpd256_pd128(_t8_307)));

  // AVX Storer:
  _t8_32 = _t8_308;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 9), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_309 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_49, _t8_49, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_310 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_311 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t8_312 = _t8_311;

  // 4-BLAC: 1x4 * 4x1
  _t8_313 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_310, _t8_312), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_310, _t8_312), _mm256_mul_pd(_t8_310, _t8_312), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_310, _t8_312), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_310, _t8_312), _mm256_mul_pd(_t8_310, _t8_312), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_310, _t8_312), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_310, _t8_312), _mm256_mul_pd(_t8_310, _t8_312), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_314 = _mm256_sub_pd(_t8_309, _t8_313);

  // AVX Storer:
  _t8_33 = _t8_314;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_315 = _t8_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_316 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_317 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t8_318 = _mm256_add_pd(_t8_316, _t8_317);

  // 4-BLAC: 1x4 / 1x4
  _t8_319 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_315), _mm256_castpd256_pd128(_t8_318)));

  // AVX Storer:
  _t8_33 = _t8_319;

  // Generating : X[28,28] = S(h(2, 28, 10), ( G(h(2, 28, 10), X[28,28],h(4, 28, 4)) - ( G(h(2, 28, 10), L[28,28],h(1, 28, 9)) * G(h(1, 28, 9), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t8_320 = _t8_50;
  _t8_321 = _t8_51;
  _t8_322 = _mm256_setzero_pd();
  _t8_323 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t8_324 = _t8_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t8_325 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_324, _t8_324, 32), _mm256_permute2f128_pd(_t8_324, _t8_324, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32));
  _t8_326 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_324, _t8_324, 32), _mm256_permute2f128_pd(_t8_324, _t8_324, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32));
  _t8_327 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_324, _t8_324, 49), _mm256_permute2f128_pd(_t8_324, _t8_324, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32));
  _t8_328 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_324, _t8_324, 49), _mm256_permute2f128_pd(_t8_324, _t8_324, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32));

  // 4-BLAC: 4x4 - 4x4
  _t8_329 = _mm256_sub_pd(_t8_320, _t8_325);
  _t8_330 = _mm256_sub_pd(_t8_321, _t8_326);
  _t8_331 = _mm256_sub_pd(_t8_322, _t8_327);
  _t8_332 = _mm256_sub_pd(_t8_323, _t8_328);

  // AVX Storer:
  _t8_50 = _t8_329;
  _t8_51 = _t8_330;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_333 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_50, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_334 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_335 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_336 = _mm256_add_pd(_t8_334, _t8_335);

  // 4-BLAC: 1x4 / 1x4
  _t8_337 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_333), _mm256_castpd256_pd128(_t8_336)));

  // AVX Storer:
  _t8_34 = _t8_337;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 10), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_338 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_50, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_339 = _t8_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_340 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t8_341 = _t8_340;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_342 = _mm256_mul_pd(_t8_339, _t8_341);

  // 4-BLAC: 1x4 - 1x4
  _t8_343 = _mm256_sub_pd(_t8_338, _t8_342);

  // AVX Storer:
  _t8_35 = _t8_343;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_344 = _t8_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_345 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_346 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t8_347 = _mm256_add_pd(_t8_345, _t8_346);

  // 4-BLAC: 1x4 / 1x4
  _t8_348 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_344), _mm256_castpd256_pd128(_t8_347)));

  // AVX Storer:
  _t8_35 = _t8_348;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 10), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_349 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_50, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_50, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_350 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_351 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t8_352 = _t8_351;

  // 4-BLAC: 1x4 * 4x1
  _t8_353 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_350, _t8_352), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_350, _t8_352), _mm256_mul_pd(_t8_350, _t8_352), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_350, _t8_352), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_350, _t8_352), _mm256_mul_pd(_t8_350, _t8_352), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_350, _t8_352), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_350, _t8_352), _mm256_mul_pd(_t8_350, _t8_352), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_354 = _mm256_sub_pd(_t8_349, _t8_353);

  // AVX Storer:
  _t8_36 = _t8_354;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_355 = _t8_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_356 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_357 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t8_358 = _mm256_add_pd(_t8_356, _t8_357);

  // 4-BLAC: 1x4 / 1x4
  _t8_359 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_355), _mm256_castpd256_pd128(_t8_358)));

  // AVX Storer:
  _t8_36 = _t8_359;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 10), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_360 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_50, _t8_50, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_361 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_362 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t8_363 = _t8_362;

  // 4-BLAC: 1x4 * 4x1
  _t8_364 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_361, _t8_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_361, _t8_363), _mm256_mul_pd(_t8_361, _t8_363), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_361, _t8_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_361, _t8_363), _mm256_mul_pd(_t8_361, _t8_363), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_361, _t8_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_361, _t8_363), _mm256_mul_pd(_t8_361, _t8_363), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_365 = _mm256_sub_pd(_t8_360, _t8_364);

  // AVX Storer:
  _t8_37 = _t8_365;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_366 = _t8_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_367 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_368 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t8_369 = _mm256_add_pd(_t8_367, _t8_368);

  // 4-BLAC: 1x4 / 1x4
  _t8_370 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_366), _mm256_castpd256_pd128(_t8_369)));

  // AVX Storer:
  _t8_37 = _t8_370;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(4, 28, 4)) - ( G(h(1, 28, 11), L[28,28],h(1, 28, 10)) Kro G(h(1, 28, 10), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_371 = _t8_1;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t8_43 = _mm256_mul_pd(_t8_371, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32));

  // 4-BLAC: 1x4 - 1x4
  _t8_51 = _mm256_sub_pd(_t8_51, _t8_43);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_372 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_51, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_373 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_374 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t8_375 = _mm256_add_pd(_t8_373, _t8_374);

  // 4-BLAC: 1x4 / 1x4
  _t8_376 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_372), _mm256_castpd256_pd128(_t8_375)));

  // AVX Storer:
  _t8_38 = _t8_376;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 11), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_377 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_51, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_378 = _t8_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_379 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t8_380 = _t8_379;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_381 = _mm256_mul_pd(_t8_378, _t8_380);

  // 4-BLAC: 1x4 - 1x4
  _t8_382 = _mm256_sub_pd(_t8_377, _t8_381);

  // AVX Storer:
  _t8_39 = _t8_382;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_383 = _t8_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_384 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_385 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t8_386 = _mm256_add_pd(_t8_384, _t8_385);

  // 4-BLAC: 1x4 / 1x4
  _t8_387 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_383), _mm256_castpd256_pd128(_t8_386)));

  // AVX Storer:
  _t8_39 = _t8_387;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 11), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_388 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_51, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t8_51, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_389 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t8_390 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t8_391 = _t8_390;

  // 4-BLAC: 1x4 * 4x1
  _t8_392 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_389, _t8_391), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_389, _t8_391), _mm256_mul_pd(_t8_389, _t8_391), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_389, _t8_391), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_389, _t8_391), _mm256_mul_pd(_t8_389, _t8_391), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_389, _t8_391), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_389, _t8_391), _mm256_mul_pd(_t8_389, _t8_391), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_393 = _mm256_sub_pd(_t8_388, _t8_392);

  // AVX Storer:
  _t8_40 = _t8_393;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_394 = _t8_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_395 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_396 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t8_397 = _mm256_add_pd(_t8_395, _t8_396);

  // 4-BLAC: 1x4 / 1x4
  _t8_398 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_394), _mm256_castpd256_pd128(_t8_397)));

  // AVX Storer:
  _t8_40 = _t8_398;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 11), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_399 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t8_51, _t8_51, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_400 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t8_401 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t8_402 = _t8_401;

  // 4-BLAC: 1x4 * 4x1
  _t8_403 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_400, _t8_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_400, _t8_402), _mm256_mul_pd(_t8_400, _t8_402), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_400, _t8_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_400, _t8_402), _mm256_mul_pd(_t8_400, _t8_402), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_400, _t8_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_400, _t8_402), _mm256_mul_pd(_t8_400, _t8_402), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t8_404 = _mm256_sub_pd(_t8_399, _t8_403);

  // AVX Storer:
  _t8_41 = _t8_404;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_405 = _t8_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_406 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_407 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t8_408 = _mm256_add_pd(_t8_406, _t8_407);

  // 4-BLAC: 1x4 / 1x4
  _t8_409 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_405), _mm256_castpd256_pd128(_t8_408)));

  // AVX Storer:
  _t8_41 = _t8_409;

  // Generating : X[28,28] = Sum_{i100} ( Sum_{k219} ( S(h(4, 28, i100 + 12), ( G(h(4, 28, i100 + 12), X[28,28],h(4, 28, k219)) - ( G(h(4, 28, i100 + 12), L[28,28],h(4, 28, 8)) * G(h(4, 28, 8), X[28,28],h(4, 28, k219)) ) ),h(4, 28, k219)) ) )

  _mm_store_sd(&(C[224]), _mm256_castpd256_pd128(_t8_7));
  _mm_store_sd(&(C[225]), _mm256_castpd256_pd128(_t8_8));
  _mm_store_sd(&(C[226]), _mm256_castpd256_pd128(_t8_9));
  _mm_store_sd(&(C[227]), _mm256_castpd256_pd128(_t8_10));
  _mm_store_sd(&(C[252]), _mm256_castpd256_pd128(_t8_14));
  _mm_store_sd(&(C[253]), _mm256_castpd256_pd128(_t8_15));
  _mm_store_sd(&(C[254]), _mm256_castpd256_pd128(_t8_16));
  _mm_store_sd(&(C[255]), _mm256_castpd256_pd128(_t8_17));
  _mm_store_sd(&(C[280]), _mm256_castpd256_pd128(_t8_18));
  _mm_store_sd(&(C[281]), _mm256_castpd256_pd128(_t8_19));
  _mm_store_sd(&(C[282]), _mm256_castpd256_pd128(_t8_20));
  _mm_store_sd(&(C[283]), _mm256_castpd256_pd128(_t8_21));
  _mm_store_sd(&(C[308]), _mm256_castpd256_pd128(_t8_22));
  _mm_store_sd(&(C[309]), _mm256_castpd256_pd128(_t8_23));
  _mm_store_sd(&(C[310]), _mm256_castpd256_pd128(_t8_24));
  _mm_store_sd(&(C[311]), _mm256_castpd256_pd128(_t8_25));
  _mm_store_sd(&(C[228]), _mm256_castpd256_pd128(_t8_26));
  _mm_store_sd(&(C[229]), _mm256_castpd256_pd128(_t8_27));
  _mm_store_sd(&(C[230]), _mm256_castpd256_pd128(_t8_28));
  _mm_store_sd(&(C[231]), _mm256_castpd256_pd128(_t8_29));
  _mm_store_sd(&(C[256]), _mm256_castpd256_pd128(_t8_30));
  _mm_store_sd(&(C[257]), _mm256_castpd256_pd128(_t8_31));
  _mm_store_sd(&(C[258]), _mm256_castpd256_pd128(_t8_32));
  _mm_store_sd(&(C[259]), _mm256_castpd256_pd128(_t8_33));
  _mm_store_sd(&(C[284]), _mm256_castpd256_pd128(_t8_34));
  _mm_store_sd(&(C[285]), _mm256_castpd256_pd128(_t8_35));
  _mm_store_sd(&(C[286]), _mm256_castpd256_pd128(_t8_36));
  _mm_store_sd(&(C[287]), _mm256_castpd256_pd128(_t8_37));
  _mm_store_sd(&(C[312]), _mm256_castpd256_pd128(_t8_38));
  _mm_store_sd(&(C[313]), _mm256_castpd256_pd128(_t8_39));
  _mm_store_sd(&(C[314]), _mm256_castpd256_pd128(_t8_40));
  _mm_store_sd(&(C[315]), _mm256_castpd256_pd128(_t8_41));

  for( int i100 = 0; i100 <= 15; i100+=4 ) {

    // AVX Loader:

    for( int k219 = 0; k219 <= 7; k219+=4 ) {
      _t9_24 = _asm256_loadu_pd(C + 28*i100 + k219 + 336);
      _t9_25 = _asm256_loadu_pd(C + 28*i100 + k219 + 364);
      _t9_26 = _asm256_loadu_pd(C + 28*i100 + k219 + 392);
      _t9_27 = _asm256_loadu_pd(C + 28*i100 + k219 + 420);
      _t9_19 = _mm256_broadcast_sd(L + 28*i100 + 344);
      _t9_18 = _mm256_broadcast_sd(L + 28*i100 + 345);
      _t9_17 = _mm256_broadcast_sd(L + 28*i100 + 346);
      _t9_16 = _mm256_broadcast_sd(L + 28*i100 + 347);
      _t9_15 = _mm256_broadcast_sd(L + 28*i100 + 372);
      _t9_14 = _mm256_broadcast_sd(L + 28*i100 + 373);
      _t9_13 = _mm256_broadcast_sd(L + 28*i100 + 374);
      _t9_12 = _mm256_broadcast_sd(L + 28*i100 + 375);
      _t9_11 = _mm256_broadcast_sd(L + 28*i100 + 400);
      _t9_10 = _mm256_broadcast_sd(L + 28*i100 + 401);
      _t9_9 = _mm256_broadcast_sd(L + 28*i100 + 402);
      _t9_8 = _mm256_broadcast_sd(L + 28*i100 + 403);
      _t9_7 = _mm256_broadcast_sd(L + 28*i100 + 428);
      _t9_6 = _mm256_broadcast_sd(L + 28*i100 + 429);
      _t9_5 = _mm256_broadcast_sd(L + 28*i100 + 430);
      _t9_4 = _mm256_broadcast_sd(L + 28*i100 + 431);
      _t9_3 = _asm256_loadu_pd(C + k219 + 224);
      _t9_2 = _asm256_loadu_pd(C + k219 + 252);
      _t9_1 = _asm256_loadu_pd(C + k219 + 280);
      _t9_0 = _asm256_loadu_pd(C + k219 + 308);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_3), _mm256_mul_pd(_t9_18, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_1), _mm256_mul_pd(_t9_16, _t9_0)));
      _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_3), _mm256_mul_pd(_t9_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_1), _mm256_mul_pd(_t9_12, _t9_0)));
      _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_3), _mm256_mul_pd(_t9_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_1), _mm256_mul_pd(_t9_8, _t9_0)));
      _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_3), _mm256_mul_pd(_t9_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_1), _mm256_mul_pd(_t9_4, _t9_0)));

      // 4-BLAC: 4x4 - 4x4
      _t9_24 = _mm256_sub_pd(_t9_24, _t9_20);
      _t9_25 = _mm256_sub_pd(_t9_25, _t9_21);
      _t9_26 = _mm256_sub_pd(_t9_26, _t9_22);
      _t9_27 = _mm256_sub_pd(_t9_27, _t9_23);

      // AVX Storer:
      _asm256_storeu_pd(C + 28*i100 + k219 + 336, _t9_24);
      _asm256_storeu_pd(C + 28*i100 + k219 + 364, _t9_25);
      _asm256_storeu_pd(C + 28*i100 + k219 + 392, _t9_26);
      _asm256_storeu_pd(C + 28*i100 + k219 + 420, _t9_27);
    }
  }

  _t8_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[225])));
  _t8_23 = _mm256_castpd128_pd256(_mm_load_sd(&(C[309])));
  _t8_41 = _mm256_castpd128_pd256(_mm_load_sd(&(C[315])));
  _t8_35 = _mm256_castpd128_pd256(_mm_load_sd(&(C[285])));
  _t8_18 = _mm256_castpd128_pd256(_mm_load_sd(&(C[280])));
  _t8_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[224])));
  _t8_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[253])));
  _t8_26 = _mm256_castpd128_pd256(_mm_load_sd(&(C[228])));
  _t8_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[226])));
  _t8_16 = _mm256_castpd128_pd256(_mm_load_sd(&(C[254])));
  _t8_27 = _mm256_castpd128_pd256(_mm_load_sd(&(C[229])));
  _t8_38 = _mm256_castpd128_pd256(_mm_load_sd(&(C[312])));
  _t8_20 = _mm256_castpd128_pd256(_mm_load_sd(&(C[282])));
  _t8_22 = _mm256_castpd128_pd256(_mm_load_sd(&(C[308])));
  _t8_28 = _mm256_castpd128_pd256(_mm_load_sd(&(C[230])));
  _t8_33 = _mm256_castpd128_pd256(_mm_load_sd(&(C[259])));
  _t8_31 = _mm256_castpd128_pd256(_mm_load_sd(&(C[257])));
  _t8_37 = _mm256_castpd128_pd256(_mm_load_sd(&(C[287])));
  _t8_32 = _mm256_castpd128_pd256(_mm_load_sd(&(C[258])));
  _t8_25 = _mm256_castpd128_pd256(_mm_load_sd(&(C[311])));
  _t8_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[227])));
  _t8_19 = _mm256_castpd128_pd256(_mm_load_sd(&(C[281])));
  _t8_21 = _mm256_castpd128_pd256(_mm_load_sd(&(C[283])));
  _t8_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[252])));
  _t8_24 = _mm256_castpd128_pd256(_mm_load_sd(&(C[310])));
  _t8_30 = _mm256_castpd128_pd256(_mm_load_sd(&(C[256])));
  _t8_29 = _mm256_castpd128_pd256(_mm_load_sd(&(C[231])));
  _t8_36 = _mm256_castpd128_pd256(_mm_load_sd(&(C[286])));
  _t8_17 = _mm256_castpd128_pd256(_mm_load_sd(&(C[255])));
  _t8_34 = _mm256_castpd128_pd256(_mm_load_sd(&(C[284])));
  _t8_40 = _mm256_castpd128_pd256(_mm_load_sd(&(C[314])));
  _t8_39 = _mm256_castpd128_pd256(_mm_load_sd(&(C[313])));
  _t10_50 = _mm256_castpd128_pd256(_mm_load_sd(C + 232));
  _t10_51 = _mm256_maskload_pd(C + 260, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t10_52 = _mm256_maskload_pd(C + 288, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_53 = _asm256_loadu_pd(C + 316);
  _t10_45 = _mm256_broadcast_sd(L + 224);
  _t10_44 = _mm256_broadcast_sd(L + 225);
  _t10_43 = _mm256_broadcast_sd(L + 226);
  _t10_42 = _mm256_broadcast_sd(L + 227);
  _t10_41 = _mm256_broadcast_sd(L + 252);
  _t10_40 = _mm256_broadcast_sd(L + 253);
  _t10_39 = _mm256_broadcast_sd(L + 254);
  _t10_38 = _mm256_broadcast_sd(L + 255);
  _t10_37 = _mm256_broadcast_sd(L + 280);
  _t10_36 = _mm256_broadcast_sd(L + 281);
  _t10_35 = _mm256_broadcast_sd(L + 282);
  _t10_34 = _mm256_broadcast_sd(L + 283);
  _t10_33 = _mm256_broadcast_sd(L + 308);
  _t10_32 = _mm256_broadcast_sd(L + 309);
  _t10_31 = _mm256_broadcast_sd(L + 310);
  _t10_30 = _mm256_broadcast_sd(L + 311);
  _t10_29 = _asm256_loadu_pd(L + 224);
  _t10_28 = _asm256_loadu_pd(L + 252);
  _t10_27 = _asm256_loadu_pd(L + 280);
  _t10_26 = _asm256_loadu_pd(L + 308);
  _t10_25 = _asm256_loadu_pd(L + 228);
  _t10_24 = _asm256_loadu_pd(L + 256);
  _t10_23 = _asm256_loadu_pd(L + 284);
  _t10_22 = _asm256_loadu_pd(L + 312);
  _t10_21 = _mm256_broadcast_sd(L + 228);
  _t10_20 = _mm256_broadcast_sd(L + 229);
  _t10_19 = _mm256_broadcast_sd(L + 230);
  _t10_18 = _mm256_broadcast_sd(L + 231);
  _t10_17 = _mm256_broadcast_sd(L + 256);
  _t10_16 = _mm256_broadcast_sd(L + 257);
  _t10_15 = _mm256_broadcast_sd(L + 258);
  _t10_14 = _mm256_broadcast_sd(L + 259);
  _t10_13 = _mm256_broadcast_sd(L + 284);
  _t10_12 = _mm256_broadcast_sd(L + 285);
  _t10_11 = _mm256_broadcast_sd(L + 286);
  _t10_10 = _mm256_broadcast_sd(L + 287);
  _t10_9 = _mm256_broadcast_sd(L + 312);
  _t10_8 = _mm256_broadcast_sd(L + 313);
  _t10_7 = _mm256_broadcast_sd(L + 314);
  _t10_6 = _mm256_broadcast_sd(L + 315);
  _t10_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[260])));
  _t10_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 288)), _mm256_castpd128_pd256(_mm_load_sd(L + 316)), 0);
  _t10_3 = _mm256_maskload_pd(L + 288, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t10_2 = _mm256_maskload_pd(L + 316, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t10_1 = _mm256_castpd128_pd256(_mm_load_sd(&(L[318])));
  _t10_0 = _mm256_maskload_pd(L + 316, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = ( ( S(h(4, 28, 8), ( G(h(4, 28, 8), C[28,28],h(4, 28, 8)) - ( ( G(h(4, 28, 8), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 8), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, 8), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 8), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, 8)) + -$(h(4, 28, 8), ( G(h(4, 28, 8), X[28,28],h(4, 28, 4)) * T( G(h(4, 28, 8), L[28,28],h(4, 28, 4)) ) ),h(4, 28, 8)) ) + -$(h(4, 28, 8), ( G(h(4, 28, 8), L[28,28],h(4, 28, 4)) * T( G(h(4, 28, 8), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 8)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t10_87 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_50, _t10_51, 0), _mm256_shuffle_pd(_t10_52, _t10_53, 0), 32);
  _t10_88 = _mm256_permute2f128_pd(_t10_51, _mm256_shuffle_pd(_t10_52, _t10_53, 3), 32);
  _t10_89 = _mm256_blend_pd(_t10_52, _mm256_shuffle_pd(_t10_52, _t10_53, 3), 12);
  _t10_90 = _t10_53;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t10_237 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 32);
  _t10_238 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 32);
  _t10_239 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 49);
  _t10_240 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t10_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_45, _t10_237), _mm256_mul_pd(_t10_44, _t10_238)), _mm256_add_pd(_mm256_mul_pd(_t10_43, _t10_239), _mm256_mul_pd(_t10_42, _t10_240)));
  _t10_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_41, _t10_237), _mm256_mul_pd(_t10_40, _t10_238)), _mm256_add_pd(_mm256_mul_pd(_t10_39, _t10_239), _mm256_mul_pd(_t10_38, _t10_240)));
  _t10_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_37, _t10_237), _mm256_mul_pd(_t10_36, _t10_238)), _mm256_add_pd(_mm256_mul_pd(_t10_35, _t10_239), _mm256_mul_pd(_t10_34, _t10_240)));
  _t10_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_33, _t10_237), _mm256_mul_pd(_t10_32, _t10_238)), _mm256_add_pd(_mm256_mul_pd(_t10_31, _t10_239), _mm256_mul_pd(_t10_30, _t10_240)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t10_241 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_29, _t10_28), _mm256_unpacklo_pd(_t10_27, _t10_26), 32);
  _t10_242 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_29, _t10_28), _mm256_unpackhi_pd(_t10_27, _t10_26), 32);
  _t10_243 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_29, _t10_28), _mm256_unpacklo_pd(_t10_27, _t10_26), 49);
  _t10_244 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_29, _t10_28), _mm256_unpackhi_pd(_t10_27, _t10_26), 49);

  // 4-BLAC: 4x4 * 4x4
  _t10_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_7, _t8_7, 32), _mm256_permute2f128_pd(_t8_7, _t8_7, 32), 0), _t10_241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_8, _t8_8, 32), _mm256_permute2f128_pd(_t8_8, _t8_8, 32), 0), _t10_242)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_9, _t8_9, 32), _mm256_permute2f128_pd(_t8_9, _t8_9, 32), 0), _t10_243), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_10, _t8_10, 32), _mm256_permute2f128_pd(_t8_10, _t8_10, 32), 0), _t10_244)));
  _t10_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_14, _t8_14, 32), _mm256_permute2f128_pd(_t8_14, _t8_14, 32), 0), _t10_241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_15, _t8_15, 32), _mm256_permute2f128_pd(_t8_15, _t8_15, 32), 0), _t10_242)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_16, _t8_16, 32), _mm256_permute2f128_pd(_t8_16, _t8_16, 32), 0), _t10_243), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_17, _t8_17, 32), _mm256_permute2f128_pd(_t8_17, _t8_17, 32), 0), _t10_244)));
  _t10_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_18, _t8_18, 32), _mm256_permute2f128_pd(_t8_18, _t8_18, 32), 0), _t10_241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 0), _t10_242)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_20, _t8_20, 32), _mm256_permute2f128_pd(_t8_20, _t8_20, 32), 0), _t10_243), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_21, _t8_21, 32), _mm256_permute2f128_pd(_t8_21, _t8_21, 32), 0), _t10_244)));
  _t10_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_22, _t8_22, 32), _mm256_permute2f128_pd(_t8_22, _t8_22, 32), 0), _t10_241), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_23, _t8_23, 32), _mm256_permute2f128_pd(_t8_23, _t8_23, 32), 0), _t10_242)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_24, _t8_24, 32), _mm256_permute2f128_pd(_t8_24, _t8_24, 32), 0), _t10_243), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_25, _t8_25, 32), _mm256_permute2f128_pd(_t8_25, _t8_25, 32), 0), _t10_244)));

  // 4-BLAC: 4x4 + 4x4
  _t10_46 = _mm256_add_pd(_t10_67, _t10_71);
  _t10_47 = _mm256_add_pd(_t10_68, _t10_72);
  _t10_48 = _mm256_add_pd(_t10_69, _t10_73);
  _t10_49 = _mm256_add_pd(_t10_70, _t10_74);

  // 4-BLAC: 4x4 - 4x4
  _t10_83 = _mm256_sub_pd(_t10_87, _t10_46);
  _t10_84 = _mm256_sub_pd(_t10_88, _t10_47);
  _t10_85 = _mm256_sub_pd(_t10_89, _t10_48);
  _t10_86 = _mm256_sub_pd(_t10_90, _t10_49);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t10_50 = _t10_83;
  _t10_51 = _t10_84;
  _t10_52 = _t10_85;
  _t10_53 = _t10_86;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t10_245 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_25, _t10_24), _mm256_unpacklo_pd(_t10_23, _t10_22), 32);
  _t10_246 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_25, _t10_24), _mm256_unpackhi_pd(_t10_23, _t10_22), 32);
  _t10_247 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_25, _t10_24), _mm256_unpacklo_pd(_t10_23, _t10_22), 49);
  _t10_248 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_25, _t10_24), _mm256_unpackhi_pd(_t10_23, _t10_22), 49);

  // 4-BLAC: 4x4 * 4x4
  _t10_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_26, _t8_26, 32), _mm256_permute2f128_pd(_t8_26, _t8_26, 32), 0), _t10_245), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_27, _t8_27, 32), _mm256_permute2f128_pd(_t8_27, _t8_27, 32), 0), _t10_246)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_28, _t8_28, 32), _mm256_permute2f128_pd(_t8_28, _t8_28, 32), 0), _t10_247), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_29, _t8_29, 32), _mm256_permute2f128_pd(_t8_29, _t8_29, 32), 0), _t10_248)));
  _t10_76 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_30, _t8_30, 32), _mm256_permute2f128_pd(_t8_30, _t8_30, 32), 0), _t10_245), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_31, _t8_31, 32), _mm256_permute2f128_pd(_t8_31, _t8_31, 32), 0), _t10_246)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_32, _t8_32, 32), _mm256_permute2f128_pd(_t8_32, _t8_32, 32), 0), _t10_247), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_33, _t8_33, 32), _mm256_permute2f128_pd(_t8_33, _t8_33, 32), 0), _t10_248)));
  _t10_77 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 0), _t10_245), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_35, _t8_35, 32), _mm256_permute2f128_pd(_t8_35, _t8_35, 32), 0), _t10_246)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_36, _t8_36, 32), _mm256_permute2f128_pd(_t8_36, _t8_36, 32), 0), _t10_247), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_37, _t8_37, 32), _mm256_permute2f128_pd(_t8_37, _t8_37, 32), 0), _t10_248)));
  _t10_78 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_38, _t8_38, 32), _mm256_permute2f128_pd(_t8_38, _t8_38, 32), 0), _t10_245), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_39, _t8_39, 32), _mm256_permute2f128_pd(_t8_39, _t8_39, 32), 0), _t10_246)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_40, _t8_40, 32), _mm256_permute2f128_pd(_t8_40, _t8_40, 32), 0), _t10_247), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_41, _t8_41, 32), _mm256_permute2f128_pd(_t8_41, _t8_41, 32), 0), _t10_248)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t10_91 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_50, _t10_51, 0), _mm256_shuffle_pd(_t10_52, _t10_53, 0), 32);
  _t10_92 = _mm256_permute2f128_pd(_t10_51, _mm256_shuffle_pd(_t10_52, _t10_53, 3), 32);
  _t10_93 = _mm256_blend_pd(_t10_52, _mm256_shuffle_pd(_t10_52, _t10_53, 3), 12);
  _t10_94 = _t10_53;

  // 4-BLAC: 4x4 - 4x4
  _t10_91 = _mm256_sub_pd(_t10_91, _t10_75);
  _t10_92 = _mm256_sub_pd(_t10_92, _t10_76);
  _t10_93 = _mm256_sub_pd(_t10_93, _t10_77);
  _t10_94 = _mm256_sub_pd(_t10_94, _t10_78);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t10_50 = _t10_91;
  _t10_51 = _t10_92;
  _t10_52 = _t10_93;
  _t10_53 = _t10_94;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t10_249 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 32);
  _t10_250 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 32);
  _t10_251 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 49);
  _t10_252 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t10_79 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_21, _t10_249), _mm256_mul_pd(_t10_20, _t10_250)), _mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_251), _mm256_mul_pd(_t10_18, _t10_252)));
  _t10_80 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_249), _mm256_mul_pd(_t10_16, _t10_250)), _mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_251), _mm256_mul_pd(_t10_14, _t10_252)));
  _t10_81 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_249), _mm256_mul_pd(_t10_12, _t10_250)), _mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_251), _mm256_mul_pd(_t10_10, _t10_252)));
  _t10_82 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_249), _mm256_mul_pd(_t10_8, _t10_250)), _mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_251), _mm256_mul_pd(_t10_6, _t10_252)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t10_95 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_50, _t10_51, 0), _mm256_shuffle_pd(_t10_52, _t10_53, 0), 32);
  _t10_96 = _mm256_permute2f128_pd(_t10_51, _mm256_shuffle_pd(_t10_52, _t10_53, 3), 32);
  _t10_97 = _mm256_blend_pd(_t10_52, _mm256_shuffle_pd(_t10_52, _t10_53, 3), 12);
  _t10_98 = _t10_53;

  // 4-BLAC: 4x4 - 4x4
  _t10_95 = _mm256_sub_pd(_t10_95, _t10_79);
  _t10_96 = _mm256_sub_pd(_t10_96, _t10_80);
  _t10_97 = _mm256_sub_pd(_t10_97, _t10_81);
  _t10_98 = _mm256_sub_pd(_t10_98, _t10_82);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t10_50 = _t10_95;
  _t10_51 = _t10_96;
  _t10_52 = _t10_97;
  _t10_53 = _t10_98;

  // Generating : X[28,28] = S(h(1, 28, 8), ( G(h(1, 28, 8), X[28,28],h(1, 28, 8)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 8), L[28,28],h(1, 28, 8)) ) ),h(1, 28, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_99 = _t10_50;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t10_100 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_101 = _t8_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_102 = _mm256_mul_pd(_t10_100, _t10_101);

  // 4-BLAC: 1x4 / 1x4
  _t10_103 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_99), _mm256_castpd256_pd128(_t10_102)));

  // AVX Storer:
  _t10_50 = _t10_103;

  // Generating : X[28,28] = S(h(3, 28, 9), ( G(h(3, 28, 9), X[28,28],h(1, 28, 8)) - ( G(h(3, 28, 9), L[28,28],h(1, 28, 8)) Kro G(h(1, 28, 8), X[28,28],h(1, 28, 8)) ) ),h(1, 28, 8))

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_104 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_51, _t10_52), _mm256_unpacklo_pd(_t10_53, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_105 = _t8_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_106 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_50, _t10_50, 32), _mm256_permute2f128_pd(_t10_50, _t10_50, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t10_107 = _mm256_mul_pd(_t10_105, _t10_106);

  // 4-BLAC: 4x1 - 4x1
  _t10_108 = _mm256_sub_pd(_t10_104, _t10_107);

  // AVX Storer:
  _t10_54 = _t10_108;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 8)) Div ( G(h(1, 28, 9), L[28,28],h(1, 28, 9)) + G(h(1, 28, 8), L[28,28],h(1, 28, 8)) ) ),h(1, 28, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_109 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_54, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_110 = _t8_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_111 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t10_112 = _mm256_add_pd(_t10_110, _t10_111);

  // 4-BLAC: 1x4 / 1x4
  _t10_113 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_109), _mm256_castpd256_pd128(_t10_112)));

  // AVX Storer:
  _t10_55 = _t10_113;

  // Generating : X[28,28] = S(h(2, 28, 10), ( G(h(2, 28, 10), X[28,28],h(1, 28, 8)) - ( G(h(2, 28, 10), L[28,28],h(1, 28, 9)) Kro G(h(1, 28, 9), X[28,28],h(1, 28, 8)) ) ),h(1, 28, 8))

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_114 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_54, 2), _mm256_permute2f128_pd(_t10_54, _t10_54, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_115 = _t8_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_116 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_55, _t10_55, 32), _mm256_permute2f128_pd(_t10_55, _t10_55, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t10_117 = _mm256_mul_pd(_t10_115, _t10_116);

  // 4-BLAC: 4x1 - 4x1
  _t10_118 = _mm256_sub_pd(_t10_114, _t10_117);

  // AVX Storer:
  _t10_56 = _t10_118;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 9)) - ( ( G(h(1, 28, 9), L[28,28],h(1, 28, 8)) Kro T( G(h(1, 28, 9), X[28,28],h(1, 28, 8)) ) ) + ( G(h(1, 28, 9), X[28,28],h(1, 28, 8)) Kro T( G(h(1, 28, 9), L[28,28],h(1, 28, 8)) ) ) ) ),h(1, 28, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_119 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_51, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_120 = _t10_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_121 = _t10_55;

  // 4-BLAC: (4x1)^T
  _t10_122 = _t10_121;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_123 = _mm256_mul_pd(_t10_120, _t10_122);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_124 = _t10_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_125 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t10_126 = _t10_125;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_127 = _mm256_mul_pd(_t10_124, _t10_126);

  // 4-BLAC: 1x4 + 1x4
  _t10_128 = _mm256_add_pd(_t10_123, _t10_127);

  // 4-BLAC: 1x4 - 1x4
  _t10_129 = _mm256_sub_pd(_t10_119, _t10_128);

  // AVX Storer:
  _t10_57 = _t10_129;

  // Generating : X[28,28] = S(h(1, 28, 9), ( G(h(1, 28, 9), X[28,28],h(1, 28, 9)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 9), L[28,28],h(1, 28, 9)) ) ),h(1, 28, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_130 = _t10_57;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t10_131 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_132 = _t8_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_133 = _mm256_mul_pd(_t10_131, _t10_132);

  // 4-BLAC: 1x4 / 1x4
  _t10_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_130), _mm256_castpd256_pd128(_t10_133)));

  // AVX Storer:
  _t10_57 = _t10_134;

  // Generating : X[28,28] = S(h(2, 28, 10), ( G(h(2, 28, 10), X[28,28],h(1, 28, 9)) - ( G(h(2, 28, 10), L[28,28],h(1, 28, 8)) Kro T( G(h(1, 28, 9), X[28,28],h(1, 28, 8)) ) ) ),h(1, 28, 9))

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_135 = _mm256_unpackhi_pd(_mm256_blend_pd(_t10_52, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t10_53, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_136 = _t10_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_137 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_55, _t10_55, 32), _mm256_permute2f128_pd(_t10_55, _t10_55, 32), 0);

  // 4-BLAC: (4x1)^T
  _t10_138 = _t10_137;

  // 4-BLAC: 4x1 Kro 1x4
  _t10_139 = _mm256_mul_pd(_t10_136, _t10_138);

  // 4-BLAC: 4x1 - 4x1
  _t10_140 = _mm256_sub_pd(_t10_135, _t10_139);

  // AVX Storer:
  _t10_58 = _t10_140;

  // Generating : X[28,28] = S(h(2, 28, 10), ( G(h(2, 28, 10), X[28,28],h(1, 28, 9)) - ( G(h(2, 28, 10), L[28,28],h(1, 28, 9)) Kro G(h(1, 28, 9), X[28,28],h(1, 28, 9)) ) ),h(1, 28, 9))

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_141 = _t10_58;

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_142 = _t8_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_143 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_57, _t10_57, 32), _mm256_permute2f128_pd(_t10_57, _t10_57, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t10_144 = _mm256_mul_pd(_t10_142, _t10_143);

  // 4-BLAC: 4x1 - 4x1
  _t10_145 = _mm256_sub_pd(_t10_141, _t10_144);

  // AVX Storer:
  _t10_58 = _t10_145;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 8)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 8), L[28,28],h(1, 28, 8)) ) ),h(1, 28, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_146 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_56, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_147 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_148 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t10_149 = _mm256_add_pd(_t10_147, _t10_148);

  // 4-BLAC: 1x4 / 1x4
  _t10_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_146), _mm256_castpd256_pd128(_t10_149)));

  // AVX Storer:
  _t10_59 = _t10_150;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 9)) - ( G(h(1, 28, 10), X[28,28],h(1, 28, 8)) Kro T( G(h(1, 28, 9), L[28,28],h(1, 28, 8)) ) ) ),h(1, 28, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_151 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_58, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_152 = _t10_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_153 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t10_154 = _t10_153;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_155 = _mm256_mul_pd(_t10_152, _t10_154);

  // 4-BLAC: 1x4 - 1x4
  _t10_156 = _mm256_sub_pd(_t10_151, _t10_155);

  // AVX Storer:
  _t10_60 = _t10_156;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 9)) Div ( G(h(1, 28, 10), L[28,28],h(1, 28, 10)) + G(h(1, 28, 9), L[28,28],h(1, 28, 9)) ) ),h(1, 28, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_157 = _t10_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_158 = _t8_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_159 = _t8_4;

  // 4-BLAC: 1x4 + 1x4
  _t10_160 = _mm256_add_pd(_t10_158, _t10_159);

  // 4-BLAC: 1x4 / 1x4
  _t10_161 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_157), _mm256_castpd256_pd128(_t10_160)));

  // AVX Storer:
  _t10_60 = _t10_161;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(2, 28, 8)) - ( G(h(1, 28, 11), L[28,28],h(1, 28, 10)) Kro G(h(1, 28, 10), X[28,28],h(2, 28, 8)) ) ),h(2, 28, 8))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_162 = _mm256_unpackhi_pd(_mm256_blend_pd(_t10_56, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t10_58, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_163 = _t8_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_164 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 1x4
  _t10_165 = _mm256_mul_pd(_t10_163, _t10_164);

  // 4-BLAC: 1x4 - 1x4
  _t10_166 = _mm256_sub_pd(_t10_162, _t10_165);

  // AVX Storer:
  _t10_61 = _t10_166;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 10)) - ( ( G(h(1, 28, 10), L[28,28],h(2, 28, 8)) * T( G(h(1, 28, 10), X[28,28],h(2, 28, 8)) ) ) + ( G(h(1, 28, 10), X[28,28],h(2, 28, 8)) * T( G(h(1, 28, 10), L[28,28],h(2, 28, 8)) ) ) ) ),h(1, 28, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_167 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_52, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_52, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_168 = _t10_3;

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_169 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t10_170 = _t10_169;

  // 4-BLAC: 1x4 * 4x1
  _t10_171 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_168, _t10_170), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_168, _t10_170), _mm256_mul_pd(_t10_168, _t10_170), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_168, _t10_170), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_168, _t10_170), _mm256_mul_pd(_t10_168, _t10_170), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_168, _t10_170), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_168, _t10_170), _mm256_mul_pd(_t10_168, _t10_170), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_172 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_173 = _t10_3;

  // 4-BLAC: (1x4)^T
  _t10_174 = _t10_173;

  // 4-BLAC: 1x4 * 4x1
  _t10_175 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_172, _t10_174), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_172, _t10_174), _mm256_mul_pd(_t10_172, _t10_174), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_172, _t10_174), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_172, _t10_174), _mm256_mul_pd(_t10_172, _t10_174), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_172, _t10_174), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_172, _t10_174), _mm256_mul_pd(_t10_172, _t10_174), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t10_176 = _mm256_add_pd(_t10_171, _t10_175);

  // 4-BLAC: 1x4 - 1x4
  _t10_177 = _mm256_sub_pd(_t10_167, _t10_176);

  // AVX Storer:
  _t10_62 = _t10_177;

  // Generating : X[28,28] = S(h(1, 28, 10), ( G(h(1, 28, 10), X[28,28],h(1, 28, 10)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 10), L[28,28],h(1, 28, 10)) ) ),h(1, 28, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_178 = _t10_62;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t10_179 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_180 = _t8_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_181 = _mm256_mul_pd(_t10_179, _t10_180);

  // 4-BLAC: 1x4 / 1x4
  _t10_182 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_178), _mm256_castpd256_pd128(_t10_181)));

  // AVX Storer:
  _t10_62 = _t10_182;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 10)) - ( G(h(1, 28, 11), L[28,28],h(2, 28, 8)) * T( G(h(1, 28, 10), X[28,28],h(2, 28, 8)) ) ) ),h(1, 28, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_183 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_53, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_53, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_184 = _t10_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_185 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t10_186 = _t10_185;

  // 4-BLAC: 1x4 * 4x1
  _t10_187 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_184, _t10_186), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_184, _t10_186), _mm256_mul_pd(_t10_184, _t10_186), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_184, _t10_186), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_184, _t10_186), _mm256_mul_pd(_t10_184, _t10_186), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_184, _t10_186), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_184, _t10_186), _mm256_mul_pd(_t10_184, _t10_186), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t10_188 = _mm256_sub_pd(_t10_183, _t10_187);

  // AVX Storer:
  _t10_63 = _t10_188;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 10)) - ( G(h(1, 28, 11), L[28,28],h(1, 28, 10)) Kro G(h(1, 28, 10), X[28,28],h(1, 28, 10)) ) ),h(1, 28, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_189 = _t10_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_190 = _t10_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_191 = _t10_62;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_192 = _mm256_mul_pd(_t10_190, _t10_191);

  // 4-BLAC: 1x4 - 1x4
  _t10_193 = _mm256_sub_pd(_t10_189, _t10_192);

  // AVX Storer:
  _t10_63 = _t10_193;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 8)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 8), L[28,28],h(1, 28, 8)) ) ),h(1, 28, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_194 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_61, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_195 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_196 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t10_197 = _mm256_add_pd(_t10_195, _t10_196);

  // 4-BLAC: 1x4 / 1x4
  _t10_198 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_194), _mm256_castpd256_pd128(_t10_197)));

  // AVX Storer:
  _t10_64 = _t10_198;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 9)) - ( G(h(1, 28, 11), X[28,28],h(1, 28, 8)) Kro T( G(h(1, 28, 9), L[28,28],h(1, 28, 8)) ) ) ),h(1, 28, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_199 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_61, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_200 = _t10_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_201 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t10_202 = _t10_201;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_203 = _mm256_mul_pd(_t10_200, _t10_202);

  // 4-BLAC: 1x4 - 1x4
  _t10_204 = _mm256_sub_pd(_t10_199, _t10_203);

  // AVX Storer:
  _t10_65 = _t10_204;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 9)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 9), L[28,28],h(1, 28, 9)) ) ),h(1, 28, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_205 = _t10_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_206 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_207 = _t8_4;

  // 4-BLAC: 1x4 + 1x4
  _t10_208 = _mm256_add_pd(_t10_206, _t10_207);

  // 4-BLAC: 1x4 / 1x4
  _t10_209 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_205), _mm256_castpd256_pd128(_t10_208)));

  // AVX Storer:
  _t10_65 = _t10_209;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 10)) - ( G(h(1, 28, 11), X[28,28],h(2, 28, 8)) * T( G(h(1, 28, 10), L[28,28],h(2, 28, 8)) ) ) ),h(1, 28, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_210 = _t10_63;

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_211 = _mm256_blend_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_212 = _t10_3;

  // 4-BLAC: (1x4)^T
  _t10_213 = _t10_212;

  // 4-BLAC: 1x4 * 4x1
  _t10_214 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_211, _t10_213), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_211, _t10_213), _mm256_mul_pd(_t10_211, _t10_213), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_211, _t10_213), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_211, _t10_213), _mm256_mul_pd(_t10_211, _t10_213), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_211, _t10_213), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_211, _t10_213), _mm256_mul_pd(_t10_211, _t10_213), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t10_215 = _mm256_sub_pd(_t10_210, _t10_214);

  // AVX Storer:
  _t10_63 = _t10_215;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 10)) Div ( G(h(1, 28, 11), L[28,28],h(1, 28, 11)) + G(h(1, 28, 10), L[28,28],h(1, 28, 10)) ) ),h(1, 28, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_216 = _t10_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_217 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_218 = _t8_2;

  // 4-BLAC: 1x4 + 1x4
  _t10_219 = _mm256_add_pd(_t10_217, _t10_218);

  // 4-BLAC: 1x4 / 1x4
  _t10_220 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_216), _mm256_castpd256_pd128(_t10_219)));

  // AVX Storer:
  _t10_63 = _t10_220;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 11)) - ( ( G(h(1, 28, 11), L[28,28],h(3, 28, 8)) * T( G(h(1, 28, 11), X[28,28],h(3, 28, 8)) ) ) + ( G(h(1, 28, 11), X[28,28],h(3, 28, 8)) * T( G(h(1, 28, 11), L[28,28],h(3, 28, 8)) ) ) ) ),h(1, 28, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_221 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_53, _t10_53, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_222 = _t10_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_223 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_unpacklo_pd(_t10_63, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t10_224 = _t10_223;

  // 4-BLAC: 1x4 * 4x1
  _t10_225 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_222, _t10_224), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_222, _t10_224), _mm256_mul_pd(_t10_222, _t10_224), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_222, _t10_224), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_222, _t10_224), _mm256_mul_pd(_t10_222, _t10_224), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_222, _t10_224), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_222, _t10_224), _mm256_mul_pd(_t10_222, _t10_224), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_226 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_unpacklo_pd(_t10_63, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_227 = _t10_0;

  // 4-BLAC: (1x4)^T
  _t10_228 = _t10_227;

  // 4-BLAC: 1x4 * 4x1
  _t10_229 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t10_226, _t10_228), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_226, _t10_228), _mm256_mul_pd(_t10_226, _t10_228), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t10_226, _t10_228), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_226, _t10_228), _mm256_mul_pd(_t10_226, _t10_228), 129)), _mm256_add_pd(_mm256_mul_pd(_t10_226, _t10_228), _mm256_permute2f128_pd(_mm256_mul_pd(_t10_226, _t10_228), _mm256_mul_pd(_t10_226, _t10_228), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t10_230 = _mm256_add_pd(_t10_225, _t10_229);

  // 4-BLAC: 1x4 - 1x4
  _t10_231 = _mm256_sub_pd(_t10_221, _t10_230);

  // AVX Storer:
  _t10_66 = _t10_231;

  // Generating : X[28,28] = S(h(1, 28, 11), ( G(h(1, 28, 11), X[28,28],h(1, 28, 11)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 11), L[28,28],h(1, 28, 11)) ) ),h(1, 28, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_232 = _t10_66;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t10_233 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_234 = _t8_0;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_235 = _mm256_mul_pd(_t10_233, _t10_234);

  // 4-BLAC: 1x4 / 1x4
  _t10_236 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_232), _mm256_castpd256_pd128(_t10_235)));

  // AVX Storer:
  _t10_66 = _t10_236;

  // Generating : X[28,28] = ( Sum_{k219} ( S(h(4, 28, k219 + 12), ( G(h(4, 28, k219 + 12), C[28,28],h(4, 28, 8)) - ( G(h(4, 28, k219 + 12), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 8), X[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 8)) ) + Sum_{k219} ( -$(h(4, 28, k219 + 12), ( G(h(4, 28, k219 + 12), L[28,28],h(4, 28, 4)) * T( G(h(4, 28, 8), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 8)) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t10_253 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 32);
  _t10_254 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 32);
  _t10_255 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 49);
  _t10_256 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_14, _t8_15), _mm256_unpacklo_pd(_t8_16, _t8_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_18, _t8_19), _mm256_unpacklo_pd(_t8_20, _t8_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_22, _t8_23), _mm256_unpacklo_pd(_t8_24, _t8_25), 32)), 49);


  for( int k219 = 0; k219 <= 15; k219+=4 ) {
    _t11_20 = _asm256_loadu_pd(C + 28*k219 + 344);
    _t11_21 = _asm256_loadu_pd(C + 28*k219 + 372);
    _t11_22 = _asm256_loadu_pd(C + 28*k219 + 400);
    _t11_23 = _asm256_loadu_pd(C + 28*k219 + 428);
    _t11_15 = _mm256_broadcast_sd(L + 28*k219 + 336);
    _t11_14 = _mm256_broadcast_sd(L + 28*k219 + 337);
    _t11_13 = _mm256_broadcast_sd(L + 28*k219 + 338);
    _t11_12 = _mm256_broadcast_sd(L + 28*k219 + 339);
    _t11_11 = _mm256_broadcast_sd(L + 28*k219 + 364);
    _t11_10 = _mm256_broadcast_sd(L + 28*k219 + 365);
    _t11_9 = _mm256_broadcast_sd(L + 28*k219 + 366);
    _t11_8 = _mm256_broadcast_sd(L + 28*k219 + 367);
    _t11_7 = _mm256_broadcast_sd(L + 28*k219 + 392);
    _t11_6 = _mm256_broadcast_sd(L + 28*k219 + 393);
    _t11_5 = _mm256_broadcast_sd(L + 28*k219 + 394);
    _t11_4 = _mm256_broadcast_sd(L + 28*k219 + 395);
    _t11_3 = _mm256_broadcast_sd(L + 28*k219 + 420);
    _t11_2 = _mm256_broadcast_sd(L + 28*k219 + 421);
    _t11_1 = _mm256_broadcast_sd(L + 28*k219 + 422);
    _t11_0 = _mm256_broadcast_sd(L + 28*k219 + 423);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t11_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t10_253), _mm256_mul_pd(_t11_14, _t10_254)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t10_255), _mm256_mul_pd(_t11_12, _t10_256)));
    _t11_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t10_253), _mm256_mul_pd(_t11_10, _t10_254)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t10_255), _mm256_mul_pd(_t11_8, _t10_256)));
    _t11_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t10_253), _mm256_mul_pd(_t11_6, _t10_254)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t10_255), _mm256_mul_pd(_t11_4, _t10_256)));
    _t11_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_3, _t10_253), _mm256_mul_pd(_t11_2, _t10_254)), _mm256_add_pd(_mm256_mul_pd(_t11_1, _t10_255), _mm256_mul_pd(_t11_0, _t10_256)));

    // 4-BLAC: 4x4 - 4x4
    _t11_20 = _mm256_sub_pd(_t11_20, _t11_16);
    _t11_21 = _mm256_sub_pd(_t11_21, _t11_17);
    _t11_22 = _mm256_sub_pd(_t11_22, _t11_18);
    _t11_23 = _mm256_sub_pd(_t11_23, _t11_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*k219 + 344, _t11_20);
    _asm256_storeu_pd(C + 28*k219 + 372, _t11_21);
    _asm256_storeu_pd(C + 28*k219 + 400, _t11_22);
    _asm256_storeu_pd(C + 28*k219 + 428, _t11_23);
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t12_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 32);
  _t12_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 32);
  _t12_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 49);
  _t12_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_26, _t8_27), _mm256_unpacklo_pd(_t8_28, _t8_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_30, _t8_31), _mm256_unpacklo_pd(_t8_32, _t8_33), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_34, _t8_35), _mm256_unpacklo_pd(_t8_36, _t8_37), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_38, _t8_39), _mm256_unpacklo_pd(_t8_40, _t8_41), 32)), 49);


  for( int k219 = 0; k219 <= 15; k219+=4 ) {
    _t13_15 = _mm256_broadcast_sd(L + 28*k219 + 340);
    _t13_14 = _mm256_broadcast_sd(L + 28*k219 + 341);
    _t13_13 = _mm256_broadcast_sd(L + 28*k219 + 342);
    _t13_12 = _mm256_broadcast_sd(L + 28*k219 + 343);
    _t13_11 = _mm256_broadcast_sd(L + 28*k219 + 368);
    _t13_10 = _mm256_broadcast_sd(L + 28*k219 + 369);
    _t13_9 = _mm256_broadcast_sd(L + 28*k219 + 370);
    _t13_8 = _mm256_broadcast_sd(L + 28*k219 + 371);
    _t13_7 = _mm256_broadcast_sd(L + 28*k219 + 396);
    _t13_6 = _mm256_broadcast_sd(L + 28*k219 + 397);
    _t13_5 = _mm256_broadcast_sd(L + 28*k219 + 398);
    _t13_4 = _mm256_broadcast_sd(L + 28*k219 + 399);
    _t13_3 = _mm256_broadcast_sd(L + 28*k219 + 424);
    _t13_2 = _mm256_broadcast_sd(L + 28*k219 + 425);
    _t13_1 = _mm256_broadcast_sd(L + 28*k219 + 426);
    _t13_0 = _mm256_broadcast_sd(L + 28*k219 + 427);
    _t13_16 = _asm256_loadu_pd(C + 28*k219 + 344);
    _t13_17 = _asm256_loadu_pd(C + 28*k219 + 372);
    _t13_18 = _asm256_loadu_pd(C + 28*k219 + 400);
    _t13_19 = _asm256_loadu_pd(C + 28*k219 + 428);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t13_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t12_0), _mm256_mul_pd(_t13_14, _t12_1)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t12_2), _mm256_mul_pd(_t13_12, _t12_3)));
    _t13_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t12_0), _mm256_mul_pd(_t13_10, _t12_1)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t12_2), _mm256_mul_pd(_t13_8, _t12_3)));
    _t13_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t12_0), _mm256_mul_pd(_t13_6, _t12_1)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t12_2), _mm256_mul_pd(_t13_4, _t12_3)));
    _t13_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_3, _t12_0), _mm256_mul_pd(_t13_2, _t12_1)), _mm256_add_pd(_mm256_mul_pd(_t13_1, _t12_2), _mm256_mul_pd(_t13_0, _t12_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t13_16 = _mm256_sub_pd(_t13_16, _t13_20);
    _t13_17 = _mm256_sub_pd(_t13_17, _t13_21);
    _t13_18 = _mm256_sub_pd(_t13_18, _t13_22);
    _t13_19 = _mm256_sub_pd(_t13_19, _t13_23);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*k219 + 344, _t13_16);
    _asm256_storeu_pd(C + 28*k219 + 372, _t13_17);
    _asm256_storeu_pd(C + 28*k219 + 400, _t13_18);
    _asm256_storeu_pd(C + 28*k219 + 428, _t13_19);
  }


  // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, i100 + 12), ( G(h(4, 28, i100 + 12), X[28,28],h(4, 28, 8)) - ( G(h(4, 28, i100 + 12), L[28,28],h(4, 28, 8)) * G(h(4, 28, 8), X[28,28],h(4, 28, 8)) ) ),h(4, 28, 8)) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t14_0 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_50, _mm256_blend_pd(_mm256_unpacklo_pd(_t10_55, _t10_57), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_unpacklo_pd(_t10_62, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_unpacklo_pd(_t10_63, _t10_66), 32), 0), 32);
  _t14_1 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t10_55, _t10_57), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_unpacklo_pd(_t10_62, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_unpacklo_pd(_t10_63, _t10_66), 32), 3), 32);
  _t14_2 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_unpacklo_pd(_t10_62, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_59, _t10_60), _mm256_unpacklo_pd(_t10_62, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_unpacklo_pd(_t10_63, _t10_66), 32), 3), 12);
  _t14_3 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_64, _t10_65), _mm256_unpacklo_pd(_t10_63, _t10_66), 32);


  for( int i100 = 0; i100 <= 15; i100+=4 ) {
    _t15_20 = _asm256_loadu_pd(C + 28*i100 + 344);
    _t15_21 = _asm256_loadu_pd(C + 28*i100 + 372);
    _t15_22 = _asm256_loadu_pd(C + 28*i100 + 400);
    _t15_23 = _asm256_loadu_pd(C + 28*i100 + 428);
    _t15_15 = _mm256_broadcast_sd(L + 28*i100 + 344);
    _t15_14 = _mm256_broadcast_sd(L + 28*i100 + 345);
    _t15_13 = _mm256_broadcast_sd(L + 28*i100 + 346);
    _t15_12 = _mm256_broadcast_sd(L + 28*i100 + 347);
    _t15_11 = _mm256_broadcast_sd(L + 28*i100 + 372);
    _t15_10 = _mm256_broadcast_sd(L + 28*i100 + 373);
    _t15_9 = _mm256_broadcast_sd(L + 28*i100 + 374);
    _t15_8 = _mm256_broadcast_sd(L + 28*i100 + 375);
    _t15_7 = _mm256_broadcast_sd(L + 28*i100 + 400);
    _t15_6 = _mm256_broadcast_sd(L + 28*i100 + 401);
    _t15_5 = _mm256_broadcast_sd(L + 28*i100 + 402);
    _t15_4 = _mm256_broadcast_sd(L + 28*i100 + 403);
    _t15_3 = _mm256_broadcast_sd(L + 28*i100 + 428);
    _t15_2 = _mm256_broadcast_sd(L + 28*i100 + 429);
    _t15_1 = _mm256_broadcast_sd(L + 28*i100 + 430);
    _t15_0 = _mm256_broadcast_sd(L + 28*i100 + 431);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t15_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t14_0), _mm256_mul_pd(_t15_14, _t14_1)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t14_2), _mm256_mul_pd(_t15_12, _t14_3)));
    _t15_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t14_0), _mm256_mul_pd(_t15_10, _t14_1)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t14_2), _mm256_mul_pd(_t15_8, _t14_3)));
    _t15_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_7, _t14_0), _mm256_mul_pd(_t15_6, _t14_1)), _mm256_add_pd(_mm256_mul_pd(_t15_5, _t14_2), _mm256_mul_pd(_t15_4, _t14_3)));
    _t15_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_3, _t14_0), _mm256_mul_pd(_t15_2, _t14_1)), _mm256_add_pd(_mm256_mul_pd(_t15_1, _t14_2), _mm256_mul_pd(_t15_0, _t14_3)));

    // 4-BLAC: 4x4 - 4x4
    _t15_20 = _mm256_sub_pd(_t15_20, _t15_16);
    _t15_21 = _mm256_sub_pd(_t15_21, _t15_17);
    _t15_22 = _mm256_sub_pd(_t15_22, _t15_18);
    _t15_23 = _mm256_sub_pd(_t15_23, _t15_19);

    // AVX Storer:
    _asm256_storeu_pd(C + 28*i100 + 344, _t15_20);
    _asm256_storeu_pd(C + 28*i100 + 372, _t15_21);
    _asm256_storeu_pd(C + 28*i100 + 400, _t15_22);
    _asm256_storeu_pd(C + 28*i100 + 428, _t15_23);
  }


  for( int fi1812 = 12; fi1812 <= 23; fi1812+=4 ) {
    _t16_39 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812])));
    _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812])));
    _t16_40 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 1])));
    _t16_41 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 2])));
    _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 3])));
    _t16_8 = _asm256_loadu_pd(C + 28*fi1812 + 28);
    _t16_9 = _asm256_loadu_pd(C + 28*fi1812 + 56);
    _t16_10 = _asm256_loadu_pd(C + 28*fi1812 + 84);
    _t16_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 28)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 56))), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 84)), 32);
    _t16_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 29])));
    _t16_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 57)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 85)), 0);
    _t16_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 58])));
    _t16_1 = _mm256_broadcast_sd(&(L[29*fi1812 + 86]));
    _t16_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 87])));
    _t16_48 = _asm256_loadu_pd(C + 28*fi1812 + 4);
    _t16_49 = _asm256_loadu_pd(C + 28*fi1812 + 32);
    _t16_50 = _asm256_loadu_pd(C + 28*fi1812 + 60);
    _t16_51 = _asm256_loadu_pd(C + 28*fi1812 + 88);

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_52 = _t16_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_53 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_54 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_55 = _mm256_add_pd(_t16_53, _t16_54);

    // 4-BLAC: 1x4 / 1x4
    _t16_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_52), _mm256_castpd256_pd128(_t16_55)));

    // AVX Storer:
    _t16_39 = _t16_56;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_57 = _t16_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_58 = _t16_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_59 = _t0_8;

    // 4-BLAC: (4x1)^T
    _t16_60 = _t16_59;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_61 = _mm256_mul_pd(_t16_58, _t16_60);

    // 4-BLAC: 1x4 - 1x4
    _t16_62 = _mm256_sub_pd(_t16_57, _t16_61);

    // AVX Storer:
    _t16_40 = _t16_62;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_63 = _t16_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_64 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_65 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t16_66 = _mm256_add_pd(_t16_64, _t16_65);

    // 4-BLAC: 1x4 / 1x4
    _t16_67 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_63), _mm256_castpd256_pd128(_t16_66)));

    // AVX Storer:
    _t16_40 = _t16_67;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi1812), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_68 = _t16_41;

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_69 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_70 = _t0_4;

    // 4-BLAC: (1x4)^T
    _t16_71 = _t16_70;

    // 4-BLAC: 1x4 * 4x1
    _t16_72 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_69, _t16_71), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_69, _t16_71), _mm256_mul_pd(_t16_69, _t16_71), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_69, _t16_71), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_69, _t16_71), _mm256_mul_pd(_t16_69, _t16_71), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_69, _t16_71), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_69, _t16_71), _mm256_mul_pd(_t16_69, _t16_71), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_73 = _mm256_sub_pd(_t16_68, _t16_72);

    // AVX Storer:
    _t16_41 = _t16_73;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_74 = _t16_41;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_75 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_76 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_77 = _mm256_add_pd(_t16_75, _t16_76);

    // 4-BLAC: 1x4 / 1x4
    _t16_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_74), _mm256_castpd256_pd128(_t16_77)));

    // AVX Storer:
    _t16_41 = _t16_78;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi1812), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_79 = _t16_7;

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_80 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_81 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t16_82 = _t16_81;

    // 4-BLAC: 1x4 * 4x1
    _t16_83 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_80, _t16_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_80, _t16_82), _mm256_mul_pd(_t16_80, _t16_82), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_80, _t16_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_80, _t16_82), _mm256_mul_pd(_t16_80, _t16_82), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_80, _t16_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_80, _t16_82), _mm256_mul_pd(_t16_80, _t16_82), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_84 = _mm256_sub_pd(_t16_79, _t16_83);

    // AVX Storer:
    _t16_7 = _t16_84;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_85 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_86 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_87 = _t0_1;

    // 4-BLAC: 1x4 + 1x4
    _t16_88 = _mm256_add_pd(_t16_86, _t16_87);

    // 4-BLAC: 1x4 / 1x4
    _t16_89 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_85), _mm256_castpd256_pd128(_t16_88)));

    // AVX Storer:
    _t16_7 = _t16_89;

    // Generating : X[28,28] = S(h(3, 28, fi1812 + 1), ( G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, 0)) - ( G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) * G(h(1, 28, fi1812), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

    // AVX Loader:

    // 3x4 -> 4x4
    _t16_90 = _t16_8;
    _t16_91 = _t16_9;
    _t16_92 = _t16_10;
    _t16_93 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_94 = _t16_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t16_95 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_94, _t16_94, 32), _mm256_permute2f128_pd(_t16_94, _t16_94, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32));
    _t16_96 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_94, _t16_94, 32), _mm256_permute2f128_pd(_t16_94, _t16_94, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32));
    _t16_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_94, _t16_94, 49), _mm256_permute2f128_pd(_t16_94, _t16_94, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32));
    _t16_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_94, _t16_94, 49), _mm256_permute2f128_pd(_t16_94, _t16_94, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32));

    // 4-BLAC: 4x4 - 4x4
    _t16_99 = _mm256_sub_pd(_t16_90, _t16_95);
    _t16_100 = _mm256_sub_pd(_t16_91, _t16_96);
    _t16_101 = _mm256_sub_pd(_t16_92, _t16_97);
    _t16_102 = _mm256_sub_pd(_t16_93, _t16_98);

    // AVX Storer:
    _t16_8 = _t16_99;
    _t16_9 = _t16_100;
    _t16_10 = _t16_101;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_104 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_105 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_106 = _mm256_add_pd(_t16_104, _t16_105);

    // 4-BLAC: 1x4 / 1x4
    _t16_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_103), _mm256_castpd256_pd128(_t16_106)));

    // AVX Storer:
    _t16_11 = _t16_107;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_108 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_109 = _t16_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_110 = _t0_8;

    // 4-BLAC: (4x1)^T
    _t16_111 = _t16_110;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_112 = _mm256_mul_pd(_t16_109, _t16_111);

    // 4-BLAC: 1x4 - 1x4
    _t16_113 = _mm256_sub_pd(_t16_108, _t16_112);

    // AVX Storer:
    _t16_12 = _t16_113;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_114 = _t16_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_115 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_116 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t16_117 = _mm256_add_pd(_t16_115, _t16_116);

    // 4-BLAC: 1x4 / 1x4
    _t16_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_114), _mm256_castpd256_pd128(_t16_117)));

    // AVX Storer:
    _t16_12 = _t16_118;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_119 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_120 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_121 = _t0_4;

    // 4-BLAC: (1x4)^T
    _t16_122 = _t16_121;

    // 4-BLAC: 1x4 * 4x1
    _t16_123 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_120, _t16_122), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_120, _t16_122), _mm256_mul_pd(_t16_120, _t16_122), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_120, _t16_122), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_120, _t16_122), _mm256_mul_pd(_t16_120, _t16_122), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_120, _t16_122), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_120, _t16_122), _mm256_mul_pd(_t16_120, _t16_122), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_124 = _mm256_sub_pd(_t16_119, _t16_123);

    // AVX Storer:
    _t16_13 = _t16_124;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_125 = _t16_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_126 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_127 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_128 = _mm256_add_pd(_t16_126, _t16_127);

    // 4-BLAC: 1x4 / 1x4
    _t16_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_125), _mm256_castpd256_pd128(_t16_128)));

    // AVX Storer:
    _t16_13 = _t16_129;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_130 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_8, _t16_8, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_131 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_132 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t16_133 = _t16_132;

    // 4-BLAC: 1x4 * 4x1
    _t16_134 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_mul_pd(_t16_131, _t16_133), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_mul_pd(_t16_131, _t16_133), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_131, _t16_133), _mm256_mul_pd(_t16_131, _t16_133), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_135 = _mm256_sub_pd(_t16_130, _t16_134);

    // AVX Storer:
    _t16_14 = _t16_135;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_136 = _t16_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_137 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_138 = _t0_1;

    // 4-BLAC: 1x4 + 1x4
    _t16_139 = _mm256_add_pd(_t16_137, _t16_138);

    // 4-BLAC: 1x4 / 1x4
    _t16_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_136), _mm256_castpd256_pd128(_t16_139)));

    // AVX Storer:
    _t16_14 = _t16_140;

    // Generating : X[28,28] = S(h(2, 28, fi1812 + 2), ( G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, 0)) - ( G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) * G(h(1, 28, fi1812 + 1), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

    // AVX Loader:

    // 2x4 -> 4x4
    _t16_141 = _t16_9;
    _t16_142 = _t16_10;
    _t16_143 = _mm256_setzero_pd();
    _t16_144 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_145 = _t16_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t16_146 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_145, _t16_145, 32), _mm256_permute2f128_pd(_t16_145, _t16_145, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32));
    _t16_147 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_145, _t16_145, 32), _mm256_permute2f128_pd(_t16_145, _t16_145, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32));
    _t16_148 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_145, _t16_145, 49), _mm256_permute2f128_pd(_t16_145, _t16_145, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32));
    _t16_149 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_145, _t16_145, 49), _mm256_permute2f128_pd(_t16_145, _t16_145, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32));

    // 4-BLAC: 4x4 - 4x4
    _t16_150 = _mm256_sub_pd(_t16_141, _t16_146);
    _t16_151 = _mm256_sub_pd(_t16_142, _t16_147);
    _t16_152 = _mm256_sub_pd(_t16_143, _t16_148);
    _t16_153 = _mm256_sub_pd(_t16_144, _t16_149);

    // AVX Storer:
    _t16_9 = _t16_150;
    _t16_10 = _t16_151;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_154 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_155 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_156 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_157 = _mm256_add_pd(_t16_155, _t16_156);

    // 4-BLAC: 1x4 / 1x4
    _t16_158 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_154), _mm256_castpd256_pd128(_t16_157)));

    // AVX Storer:
    _t16_15 = _t16_158;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_159 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_160 = _t16_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_161 = _t0_8;

    // 4-BLAC: (4x1)^T
    _t16_162 = _t16_161;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_163 = _mm256_mul_pd(_t16_160, _t16_162);

    // 4-BLAC: 1x4 - 1x4
    _t16_164 = _mm256_sub_pd(_t16_159, _t16_163);

    // AVX Storer:
    _t16_16 = _t16_164;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_165 = _t16_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_166 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_167 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t16_168 = _mm256_add_pd(_t16_166, _t16_167);

    // 4-BLAC: 1x4 / 1x4
    _t16_169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_165), _mm256_castpd256_pd128(_t16_168)));

    // AVX Storer:
    _t16_16 = _t16_169;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_170 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_9, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_9, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_171 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_172 = _t0_4;

    // 4-BLAC: (1x4)^T
    _t16_173 = _t16_172;

    // 4-BLAC: 1x4 * 4x1
    _t16_174 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_171, _t16_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_171, _t16_173), _mm256_mul_pd(_t16_171, _t16_173), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_171, _t16_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_171, _t16_173), _mm256_mul_pd(_t16_171, _t16_173), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_171, _t16_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_171, _t16_173), _mm256_mul_pd(_t16_171, _t16_173), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_175 = _mm256_sub_pd(_t16_170, _t16_174);

    // AVX Storer:
    _t16_17 = _t16_175;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_176 = _t16_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_177 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_178 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_179 = _mm256_add_pd(_t16_177, _t16_178);

    // 4-BLAC: 1x4 / 1x4
    _t16_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_176), _mm256_castpd256_pd128(_t16_179)));

    // AVX Storer:
    _t16_17 = _t16_180;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_181 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_9, _t16_9, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_182 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_183 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t16_184 = _t16_183;

    // 4-BLAC: 1x4 * 4x1
    _t16_185 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_182, _t16_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_182, _t16_184), _mm256_mul_pd(_t16_182, _t16_184), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_182, _t16_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_182, _t16_184), _mm256_mul_pd(_t16_182, _t16_184), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_182, _t16_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_182, _t16_184), _mm256_mul_pd(_t16_182, _t16_184), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_186 = _mm256_sub_pd(_t16_181, _t16_185);

    // AVX Storer:
    _t16_18 = _t16_186;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_187 = _t16_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_188 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_189 = _t0_1;

    // 4-BLAC: 1x4 + 1x4
    _t16_190 = _mm256_add_pd(_t16_188, _t16_189);

    // 4-BLAC: 1x4 / 1x4
    _t16_191 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_187), _mm256_castpd256_pd128(_t16_190)));

    // AVX Storer:
    _t16_18 = _t16_191;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, 0)) - ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) Kro G(h(1, 28, fi1812 + 2), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_192 = _t16_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t16_42 = _mm256_mul_pd(_t16_192, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32));

    // 4-BLAC: 1x4 - 1x4
    _t16_10 = _mm256_sub_pd(_t16_10, _t16_42);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_193 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_194 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_195 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t16_196 = _mm256_add_pd(_t16_194, _t16_195);

    // 4-BLAC: 1x4 / 1x4
    _t16_197 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_193), _mm256_castpd256_pd128(_t16_196)));

    // AVX Storer:
    _t16_19 = _t16_197;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_198 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_199 = _t16_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_200 = _t0_8;

    // 4-BLAC: (4x1)^T
    _t16_201 = _t16_200;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_202 = _mm256_mul_pd(_t16_199, _t16_201);

    // 4-BLAC: 1x4 - 1x4
    _t16_203 = _mm256_sub_pd(_t16_198, _t16_202);

    // AVX Storer:
    _t16_20 = _t16_203;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_204 = _t16_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_205 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_206 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t16_207 = _mm256_add_pd(_t16_205, _t16_206);

    // 4-BLAC: 1x4 / 1x4
    _t16_208 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_204), _mm256_castpd256_pd128(_t16_207)));

    // AVX Storer:
    _t16_20 = _t16_208;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_209 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_210 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_211 = _t0_4;

    // 4-BLAC: (1x4)^T
    _t16_212 = _t16_211;

    // 4-BLAC: 1x4 * 4x1
    _t16_213 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_210, _t16_212), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_210, _t16_212), _mm256_mul_pd(_t16_210, _t16_212), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_210, _t16_212), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_210, _t16_212), _mm256_mul_pd(_t16_210, _t16_212), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_210, _t16_212), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_210, _t16_212), _mm256_mul_pd(_t16_210, _t16_212), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_214 = _mm256_sub_pd(_t16_209, _t16_213);

    // AVX Storer:
    _t16_21 = _t16_214;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_215 = _t16_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_216 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_217 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_218 = _mm256_add_pd(_t16_216, _t16_217);

    // 4-BLAC: 1x4 / 1x4
    _t16_219 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_215), _mm256_castpd256_pd128(_t16_218)));

    // AVX Storer:
    _t16_21 = _t16_219;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_220 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_10, _t16_10, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_221 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_222 = _t0_0;

    // 4-BLAC: (1x4)^T
    _t16_223 = _t16_222;

    // 4-BLAC: 1x4 * 4x1
    _t16_224 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_221, _t16_223), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_221, _t16_223), _mm256_mul_pd(_t16_221, _t16_223), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_221, _t16_223), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_221, _t16_223), _mm256_mul_pd(_t16_221, _t16_223), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_221, _t16_223), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_221, _t16_223), _mm256_mul_pd(_t16_221, _t16_223), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_225 = _mm256_sub_pd(_t16_220, _t16_224);

    // AVX Storer:
    _t16_22 = _t16_225;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_226 = _t16_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_227 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_228 = _t0_1;

    // 4-BLAC: 1x4 + 1x4
    _t16_229 = _mm256_add_pd(_t16_227, _t16_228);

    // 4-BLAC: 1x4 / 1x4
    _t16_230 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_226), _mm256_castpd256_pd128(_t16_229)));

    // AVX Storer:
    _t16_22 = _t16_230;

    // Generating : X[28,28] = S(h(4, 28, fi1812), ( G(h(4, 28, fi1812), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, fi1812), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 4))

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t16_410 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 32);
    _t16_411 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 32);
    _t16_412 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 49);
    _t16_413 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 49);

    // 4-BLAC: 4x4 * 4x4
    _t16_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_39, _t16_39, 32), _mm256_permute2f128_pd(_t16_39, _t16_39, 32), 0), _t16_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_40, _t16_40, 32), _mm256_permute2f128_pd(_t16_40, _t16_40, 32), 0), _t16_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_41, _t16_41, 32), _mm256_permute2f128_pd(_t16_41, _t16_41, 32), 0), _t16_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_7, _t16_7, 32), _mm256_permute2f128_pd(_t16_7, _t16_7, 32), 0), _t16_413)));
    _t16_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_11, _t16_11, 32), _mm256_permute2f128_pd(_t16_11, _t16_11, 32), 0), _t16_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_12, _t16_12, 32), _mm256_permute2f128_pd(_t16_12, _t16_12, 32), 0), _t16_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_13, _t16_13, 32), _mm256_permute2f128_pd(_t16_13, _t16_13, 32), 0), _t16_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_14, _t16_14, 32), _mm256_permute2f128_pd(_t16_14, _t16_14, 32), 0), _t16_413)));
    _t16_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_15, _t16_15, 32), _mm256_permute2f128_pd(_t16_15, _t16_15, 32), 0), _t16_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_16, _t16_16, 32), _mm256_permute2f128_pd(_t16_16, _t16_16, 32), 0), _t16_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_17, _t16_17, 32), _mm256_permute2f128_pd(_t16_17, _t16_17, 32), 0), _t16_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_18, _t16_18, 32), _mm256_permute2f128_pd(_t16_18, _t16_18, 32), 0), _t16_413)));
    _t16_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_19, _t16_19, 32), _mm256_permute2f128_pd(_t16_19, _t16_19, 32), 0), _t16_410), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_20, _t16_20, 32), _mm256_permute2f128_pd(_t16_20, _t16_20, 32), 0), _t16_411)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_21, _t16_21, 32), _mm256_permute2f128_pd(_t16_21, _t16_21, 32), 0), _t16_412), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_22, _t16_22, 32), _mm256_permute2f128_pd(_t16_22, _t16_22, 32), 0), _t16_413)));

    // 4-BLAC: 4x4 - 4x4
    _t16_48 = _mm256_sub_pd(_t16_48, _t16_44);
    _t16_49 = _mm256_sub_pd(_t16_49, _t16_45);
    _t16_50 = _mm256_sub_pd(_t16_50, _t16_46);
    _t16_51 = _mm256_sub_pd(_t16_51, _t16_47);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_231 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_48, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_232 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_233 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_234 = _mm256_add_pd(_t16_232, _t16_233);

    // 4-BLAC: 1x4 / 1x4
    _t16_235 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_231), _mm256_castpd256_pd128(_t16_234)));

    // AVX Storer:
    _t16_23 = _t16_235;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_236 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_48, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_237 = _t16_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_238 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t16_239 = _t16_238;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_240 = _mm256_mul_pd(_t16_237, _t16_239);

    // 4-BLAC: 1x4 - 1x4
    _t16_241 = _mm256_sub_pd(_t16_236, _t16_240);

    // AVX Storer:
    _t16_24 = _t16_241;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_242 = _t16_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_243 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_244 = _t2_4;

    // 4-BLAC: 1x4 + 1x4
    _t16_245 = _mm256_add_pd(_t16_243, _t16_244);

    // 4-BLAC: 1x4 / 1x4
    _t16_246 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_242), _mm256_castpd256_pd128(_t16_245)));

    // AVX Storer:
    _t16_24 = _t16_246;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1812), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_247 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_48, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_48, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_248 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_23, _t16_24), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_249 = _t4_3;

    // 4-BLAC: (1x4)^T
    _t16_250 = _t16_249;

    // 4-BLAC: 1x4 * 4x1
    _t16_251 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_248, _t16_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_248, _t16_250), _mm256_mul_pd(_t16_248, _t16_250), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_248, _t16_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_248, _t16_250), _mm256_mul_pd(_t16_248, _t16_250), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_248, _t16_250), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_248, _t16_250), _mm256_mul_pd(_t16_248, _t16_250), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_252 = _mm256_sub_pd(_t16_247, _t16_251);

    // AVX Storer:
    _t16_25 = _t16_252;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_253 = _t16_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_254 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_255 = _t2_2;

    // 4-BLAC: 1x4 + 1x4
    _t16_256 = _mm256_add_pd(_t16_254, _t16_255);

    // 4-BLAC: 1x4 / 1x4
    _t16_257 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_253), _mm256_castpd256_pd128(_t16_256)));

    // AVX Storer:
    _t16_25 = _t16_257;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1812), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_258 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_48, _t16_48, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_259 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_23, _t16_24), _mm256_unpacklo_pd(_t16_25, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_260 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t16_261 = _t16_260;

    // 4-BLAC: 1x4 * 4x1
    _t16_262 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_259, _t16_261), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_259, _t16_261), _mm256_mul_pd(_t16_259, _t16_261), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_259, _t16_261), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_259, _t16_261), _mm256_mul_pd(_t16_259, _t16_261), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_259, _t16_261), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_259, _t16_261), _mm256_mul_pd(_t16_259, _t16_261), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_263 = _mm256_sub_pd(_t16_258, _t16_262);

    // AVX Storer:
    _t16_26 = _t16_263;

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_264 = _t16_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_265 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_266 = _t2_0;

    // 4-BLAC: 1x4 + 1x4
    _t16_267 = _mm256_add_pd(_t16_265, _t16_266);

    // 4-BLAC: 1x4 / 1x4
    _t16_268 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_264), _mm256_castpd256_pd128(_t16_267)));

    // AVX Storer:
    _t16_26 = _t16_268;

    // Generating : X[28,28] = S(h(3, 28, fi1812 + 1), ( G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, 4)) - ( G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) * G(h(1, 28, fi1812), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t16_269 = _t16_49;
    _t16_270 = _t16_50;
    _t16_271 = _t16_51;
    _t16_272 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_273 = _t16_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t16_274 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_273, _t16_273, 32), _mm256_permute2f128_pd(_t16_273, _t16_273, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_23, _t16_24), _mm256_unpacklo_pd(_t16_25, _t16_26), 32));
    _t16_275 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_273, _t16_273, 32), _mm256_permute2f128_pd(_t16_273, _t16_273, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_23, _t16_24), _mm256_unpacklo_pd(_t16_25, _t16_26), 32));
    _t16_276 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_273, _t16_273, 49), _mm256_permute2f128_pd(_t16_273, _t16_273, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_23, _t16_24), _mm256_unpacklo_pd(_t16_25, _t16_26), 32));
    _t16_277 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_273, _t16_273, 49), _mm256_permute2f128_pd(_t16_273, _t16_273, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_23, _t16_24), _mm256_unpacklo_pd(_t16_25, _t16_26), 32));

    // 4-BLAC: 4x4 - 4x4
    _t16_278 = _mm256_sub_pd(_t16_269, _t16_274);
    _t16_279 = _mm256_sub_pd(_t16_270, _t16_275);
    _t16_280 = _mm256_sub_pd(_t16_271, _t16_276);
    _t16_281 = _mm256_sub_pd(_t16_272, _t16_277);

    // AVX Storer:
    _t16_49 = _t16_278;
    _t16_50 = _t16_279;
    _t16_51 = _t16_280;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_282 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_49, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_283 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_284 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_285 = _mm256_add_pd(_t16_283, _t16_284);

    // 4-BLAC: 1x4 / 1x4
    _t16_286 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_282), _mm256_castpd256_pd128(_t16_285)));

    // AVX Storer:
    _t16_27 = _t16_286;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_287 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_49, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_288 = _t16_27;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_289 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t16_290 = _t16_289;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_291 = _mm256_mul_pd(_t16_288, _t16_290);

    // 4-BLAC: 1x4 - 1x4
    _t16_292 = _mm256_sub_pd(_t16_287, _t16_291);

    // AVX Storer:
    _t16_28 = _t16_292;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_293 = _t16_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_294 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_295 = _t2_4;

    // 4-BLAC: 1x4 + 1x4
    _t16_296 = _mm256_add_pd(_t16_294, _t16_295);

    // 4-BLAC: 1x4 / 1x4
    _t16_297 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_293), _mm256_castpd256_pd128(_t16_296)));

    // AVX Storer:
    _t16_28 = _t16_297;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_298 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_49, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_49, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_299 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_27, _t16_28), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_300 = _t4_3;

    // 4-BLAC: (1x4)^T
    _t16_301 = _t16_300;

    // 4-BLAC: 1x4 * 4x1
    _t16_302 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_299, _t16_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_299, _t16_301), _mm256_mul_pd(_t16_299, _t16_301), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_299, _t16_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_299, _t16_301), _mm256_mul_pd(_t16_299, _t16_301), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_299, _t16_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_299, _t16_301), _mm256_mul_pd(_t16_299, _t16_301), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_303 = _mm256_sub_pd(_t16_298, _t16_302);

    // AVX Storer:
    _t16_29 = _t16_303;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_304 = _t16_29;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_305 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_306 = _t2_2;

    // 4-BLAC: 1x4 + 1x4
    _t16_307 = _mm256_add_pd(_t16_305, _t16_306);

    // 4-BLAC: 1x4 / 1x4
    _t16_308 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_304), _mm256_castpd256_pd128(_t16_307)));

    // AVX Storer:
    _t16_29 = _t16_308;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_309 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_49, _t16_49, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_310 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_27, _t16_28), _mm256_unpacklo_pd(_t16_29, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_311 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t16_312 = _t16_311;

    // 4-BLAC: 1x4 * 4x1
    _t16_313 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_310, _t16_312), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_310, _t16_312), _mm256_mul_pd(_t16_310, _t16_312), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_310, _t16_312), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_310, _t16_312), _mm256_mul_pd(_t16_310, _t16_312), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_310, _t16_312), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_310, _t16_312), _mm256_mul_pd(_t16_310, _t16_312), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_314 = _mm256_sub_pd(_t16_309, _t16_313);

    // AVX Storer:
    _t16_30 = _t16_314;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_315 = _t16_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_316 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_317 = _t2_0;

    // 4-BLAC: 1x4 + 1x4
    _t16_318 = _mm256_add_pd(_t16_316, _t16_317);

    // 4-BLAC: 1x4 / 1x4
    _t16_319 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_315), _mm256_castpd256_pd128(_t16_318)));

    // AVX Storer:
    _t16_30 = _t16_319;

    // Generating : X[28,28] = S(h(2, 28, fi1812 + 2), ( G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, 4)) - ( G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) * G(h(1, 28, fi1812 + 1), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t16_320 = _t16_50;
    _t16_321 = _t16_51;
    _t16_322 = _mm256_setzero_pd();
    _t16_323 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_324 = _t16_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t16_325 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_324, _t16_324, 32), _mm256_permute2f128_pd(_t16_324, _t16_324, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_27, _t16_28), _mm256_unpacklo_pd(_t16_29, _t16_30), 32));
    _t16_326 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_324, _t16_324, 32), _mm256_permute2f128_pd(_t16_324, _t16_324, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_27, _t16_28), _mm256_unpacklo_pd(_t16_29, _t16_30), 32));
    _t16_327 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_324, _t16_324, 49), _mm256_permute2f128_pd(_t16_324, _t16_324, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_27, _t16_28), _mm256_unpacklo_pd(_t16_29, _t16_30), 32));
    _t16_328 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_324, _t16_324, 49), _mm256_permute2f128_pd(_t16_324, _t16_324, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_27, _t16_28), _mm256_unpacklo_pd(_t16_29, _t16_30), 32));

    // 4-BLAC: 4x4 - 4x4
    _t16_329 = _mm256_sub_pd(_t16_320, _t16_325);
    _t16_330 = _mm256_sub_pd(_t16_321, _t16_326);
    _t16_331 = _mm256_sub_pd(_t16_322, _t16_327);
    _t16_332 = _mm256_sub_pd(_t16_323, _t16_328);

    // AVX Storer:
    _t16_50 = _t16_329;
    _t16_51 = _t16_330;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_333 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_50, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_334 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_335 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_336 = _mm256_add_pd(_t16_334, _t16_335);

    // 4-BLAC: 1x4 / 1x4
    _t16_337 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_333), _mm256_castpd256_pd128(_t16_336)));

    // AVX Storer:
    _t16_31 = _t16_337;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_338 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_50, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_339 = _t16_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_340 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t16_341 = _t16_340;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_342 = _mm256_mul_pd(_t16_339, _t16_341);

    // 4-BLAC: 1x4 - 1x4
    _t16_343 = _mm256_sub_pd(_t16_338, _t16_342);

    // AVX Storer:
    _t16_32 = _t16_343;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_344 = _t16_32;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_345 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_346 = _t2_4;

    // 4-BLAC: 1x4 + 1x4
    _t16_347 = _mm256_add_pd(_t16_345, _t16_346);

    // 4-BLAC: 1x4 / 1x4
    _t16_348 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_344), _mm256_castpd256_pd128(_t16_347)));

    // AVX Storer:
    _t16_32 = _t16_348;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_349 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_50, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_50, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_350 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_31, _t16_32), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_351 = _t4_3;

    // 4-BLAC: (1x4)^T
    _t16_352 = _t16_351;

    // 4-BLAC: 1x4 * 4x1
    _t16_353 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_350, _t16_352), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_350, _t16_352), _mm256_mul_pd(_t16_350, _t16_352), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_350, _t16_352), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_350, _t16_352), _mm256_mul_pd(_t16_350, _t16_352), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_350, _t16_352), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_350, _t16_352), _mm256_mul_pd(_t16_350, _t16_352), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_354 = _mm256_sub_pd(_t16_349, _t16_353);

    // AVX Storer:
    _t16_33 = _t16_354;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_355 = _t16_33;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_356 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_357 = _t2_2;

    // 4-BLAC: 1x4 + 1x4
    _t16_358 = _mm256_add_pd(_t16_356, _t16_357);

    // 4-BLAC: 1x4 / 1x4
    _t16_359 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_355), _mm256_castpd256_pd128(_t16_358)));

    // AVX Storer:
    _t16_33 = _t16_359;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_360 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_50, _t16_50, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_361 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_31, _t16_32), _mm256_unpacklo_pd(_t16_33, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_362 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t16_363 = _t16_362;

    // 4-BLAC: 1x4 * 4x1
    _t16_364 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_361, _t16_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_361, _t16_363), _mm256_mul_pd(_t16_361, _t16_363), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_361, _t16_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_361, _t16_363), _mm256_mul_pd(_t16_361, _t16_363), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_361, _t16_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_361, _t16_363), _mm256_mul_pd(_t16_361, _t16_363), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_365 = _mm256_sub_pd(_t16_360, _t16_364);

    // AVX Storer:
    _t16_34 = _t16_365;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_366 = _t16_34;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_367 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_368 = _t2_0;

    // 4-BLAC: 1x4 + 1x4
    _t16_369 = _mm256_add_pd(_t16_367, _t16_368);

    // 4-BLAC: 1x4 / 1x4
    _t16_370 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_366), _mm256_castpd256_pd128(_t16_369)));

    // AVX Storer:
    _t16_34 = _t16_370;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, 4)) - ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) Kro G(h(1, 28, fi1812 + 2), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_371 = _t16_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t16_43 = _mm256_mul_pd(_t16_371, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_31, _t16_32), _mm256_unpacklo_pd(_t16_33, _t16_34), 32));

    // 4-BLAC: 1x4 - 1x4
    _t16_51 = _mm256_sub_pd(_t16_51, _t16_43);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_372 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_51, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_373 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_374 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t16_375 = _mm256_add_pd(_t16_373, _t16_374);

    // 4-BLAC: 1x4 / 1x4
    _t16_376 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_372), _mm256_castpd256_pd128(_t16_375)));

    // AVX Storer:
    _t16_35 = _t16_376;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_377 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_51, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_378 = _t16_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_379 = _t4_5;

    // 4-BLAC: (4x1)^T
    _t16_380 = _t16_379;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_381 = _mm256_mul_pd(_t16_378, _t16_380);

    // 4-BLAC: 1x4 - 1x4
    _t16_382 = _mm256_sub_pd(_t16_377, _t16_381);

    // AVX Storer:
    _t16_36 = _t16_382;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_383 = _t16_36;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_384 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_385 = _t2_4;

    // 4-BLAC: 1x4 + 1x4
    _t16_386 = _mm256_add_pd(_t16_384, _t16_385);

    // 4-BLAC: 1x4 / 1x4
    _t16_387 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_383), _mm256_castpd256_pd128(_t16_386)));

    // AVX Storer:
    _t16_36 = _t16_387;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_388 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_51, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t16_51, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_389 = _mm256_blend_pd(_mm256_unpacklo_pd(_t16_35, _t16_36), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_390 = _t4_3;

    // 4-BLAC: (1x4)^T
    _t16_391 = _t16_390;

    // 4-BLAC: 1x4 * 4x1
    _t16_392 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_389, _t16_391), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_389, _t16_391), _mm256_mul_pd(_t16_389, _t16_391), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_389, _t16_391), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_389, _t16_391), _mm256_mul_pd(_t16_389, _t16_391), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_389, _t16_391), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_389, _t16_391), _mm256_mul_pd(_t16_389, _t16_391), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_393 = _mm256_sub_pd(_t16_388, _t16_392);

    // AVX Storer:
    _t16_37 = _t16_393;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_394 = _t16_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_395 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_396 = _t2_2;

    // 4-BLAC: 1x4 + 1x4
    _t16_397 = _mm256_add_pd(_t16_395, _t16_396);

    // 4-BLAC: 1x4 / 1x4
    _t16_398 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_394), _mm256_castpd256_pd128(_t16_397)));

    // AVX Storer:
    _t16_37 = _t16_398;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_399 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t16_51, _t16_51, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_400 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_35, _t16_36), _mm256_unpacklo_pd(_t16_37, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_401 = _t4_0;

    // 4-BLAC: (1x4)^T
    _t16_402 = _t16_401;

    // 4-BLAC: 1x4 * 4x1
    _t16_403 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_400, _t16_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_400, _t16_402), _mm256_mul_pd(_t16_400, _t16_402), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_400, _t16_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_400, _t16_402), _mm256_mul_pd(_t16_400, _t16_402), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_400, _t16_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_400, _t16_402), _mm256_mul_pd(_t16_400, _t16_402), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t16_404 = _mm256_sub_pd(_t16_399, _t16_403);

    // AVX Storer:
    _t16_38 = _t16_404;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_405 = _t16_38;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_406 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_407 = _t2_0;

    // 4-BLAC: 1x4 + 1x4
    _t16_408 = _mm256_add_pd(_t16_406, _t16_407);

    // 4-BLAC: 1x4 / 1x4
    _t16_409 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_405), _mm256_castpd256_pd128(_t16_408)));

    // AVX Storer:
    _t16_38 = _t16_409;
    _asm256_storeu_pd(C + 28*fi1812 + 28, _t16_8);
    _asm256_storeu_pd(C + 28*fi1812 + 56, _t16_9);
    _asm256_storeu_pd(C + 28*fi1812 + 84, _t16_10);
    _mm_store_sd(&(C[28*fi1812 + 4]), _mm256_castpd256_pd128(_t16_23));
    _mm_store_sd(&(C[28*fi1812 + 5]), _mm256_castpd256_pd128(_t16_24));
    _mm_store_sd(&(C[28*fi1812 + 6]), _mm256_castpd256_pd128(_t16_25));
    _mm_store_sd(&(C[28*fi1812 + 7]), _mm256_castpd256_pd128(_t16_26));
    _mm_store_sd(&(C[28*fi1812 + 32]), _mm256_castpd256_pd128(_t16_27));
    _mm_store_sd(&(C[28*fi1812 + 33]), _mm256_castpd256_pd128(_t16_28));
    _mm_store_sd(&(C[28*fi1812 + 34]), _mm256_castpd256_pd128(_t16_29));
    _mm_store_sd(&(C[28*fi1812 + 35]), _mm256_castpd256_pd128(_t16_30));
    _mm_store_sd(&(C[28*fi1812 + 60]), _mm256_castpd256_pd128(_t16_31));
    _mm_store_sd(&(C[28*fi1812 + 61]), _mm256_castpd256_pd128(_t16_32));
    _mm_store_sd(&(C[28*fi1812 + 62]), _mm256_castpd256_pd128(_t16_33));
    _mm_store_sd(&(C[28*fi1812 + 63]), _mm256_castpd256_pd128(_t16_34));
    _mm_store_sd(&(C[28*fi1812 + 88]), _mm256_castpd256_pd128(_t16_35));
    _mm_store_sd(&(C[28*fi1812 + 89]), _mm256_castpd256_pd128(_t16_36));
    _mm_store_sd(&(C[28*fi1812 + 90]), _mm256_castpd256_pd128(_t16_37));
    _mm_store_sd(&(C[28*fi1812 + 91]), _mm256_castpd256_pd128(_t16_38));

    for( int fi2064 = 8; fi2064 <= fi1812 - 4; fi2064+=4 ) {
      _t17_8 = _asm256_loadu_pd(C + 28*fi1812 + fi2064);
      _t17_9 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 28);
      _t17_10 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 56);
      _t17_11 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 84);
      _t17_3 = _asm256_loadu_pd(L + 28*fi2064);
      _t17_2 = _asm256_loadu_pd(L + 28*fi2064 + 28);
      _t17_1 = _asm256_loadu_pd(L + 28*fi2064 + 56);
      _t17_0 = _asm256_loadu_pd(L + 28*fi2064 + 84);

      // Generating : X[28,28] = ( S(h(4, 28, fi1812), ( G(h(4, 28, fi1812), X[28,28],h(4, 28, fi2064)) - ( G(h(4, 28, fi1812), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi2064), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, fi2064)) + Sum_{i100} ( -$(h(4, 28, fi1812), ( G(h(4, 28, fi1812), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi2064), L[28,28],h(4, 28, i100)) ) ),h(4, 28, fi2064)) ) )

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t17_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
      _t17_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
      _t17_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
      _t17_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t17_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_39, _t16_39, 32), _mm256_permute2f128_pd(_t16_39, _t16_39, 32), 0), _t17_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_40, _t16_40, 32), _mm256_permute2f128_pd(_t16_40, _t16_40, 32), 0), _t17_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_41, _t16_41, 32), _mm256_permute2f128_pd(_t16_41, _t16_41, 32), 0), _t17_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_7, _t16_7, 32), _mm256_permute2f128_pd(_t16_7, _t16_7, 32), 0), _t17_15)));
      _t17_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_11, _t16_11, 32), _mm256_permute2f128_pd(_t16_11, _t16_11, 32), 0), _t17_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_12, _t16_12, 32), _mm256_permute2f128_pd(_t16_12, _t16_12, 32), 0), _t17_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_13, _t16_13, 32), _mm256_permute2f128_pd(_t16_13, _t16_13, 32), 0), _t17_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_14, _t16_14, 32), _mm256_permute2f128_pd(_t16_14, _t16_14, 32), 0), _t17_15)));
      _t17_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_15, _t16_15, 32), _mm256_permute2f128_pd(_t16_15, _t16_15, 32), 0), _t17_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_16, _t16_16, 32), _mm256_permute2f128_pd(_t16_16, _t16_16, 32), 0), _t17_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_17, _t16_17, 32), _mm256_permute2f128_pd(_t16_17, _t16_17, 32), 0), _t17_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_18, _t16_18, 32), _mm256_permute2f128_pd(_t16_18, _t16_18, 32), 0), _t17_15)));
      _t17_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_19, _t16_19, 32), _mm256_permute2f128_pd(_t16_19, _t16_19, 32), 0), _t17_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_20, _t16_20, 32), _mm256_permute2f128_pd(_t16_20, _t16_20, 32), 0), _t17_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_21, _t16_21, 32), _mm256_permute2f128_pd(_t16_21, _t16_21, 32), 0), _t17_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_22, _t16_22, 32), _mm256_permute2f128_pd(_t16_22, _t16_22, 32), 0), _t17_15)));

      // 4-BLAC: 4x4 - 4x4
      _t17_8 = _mm256_sub_pd(_t17_8, _t17_4);
      _t17_9 = _mm256_sub_pd(_t17_9, _t17_5);
      _t17_10 = _mm256_sub_pd(_t17_10, _t17_6);
      _t17_11 = _mm256_sub_pd(_t17_11, _t17_7);

      // AVX Storer:
      _asm256_storeu_pd(C + 28*fi1812 + fi2064, _t17_8);
      _asm256_storeu_pd(C + 28*fi1812 + fi2064 + 28, _t17_9);
      _asm256_storeu_pd(C + 28*fi1812 + fi2064 + 56, _t17_10);
      _asm256_storeu_pd(C + 28*fi1812 + fi2064 + 84, _t17_11);

      for( int i100 = 4; i100 <= fi2064 - 1; i100+=4 ) {
        _t18_20 = _asm256_loadu_pd(C + 28*fi1812 + fi2064);
        _t18_21 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 28);
        _t18_22 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 56);
        _t18_23 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 84);
        _t18_19 = _mm256_broadcast_sd(C + 28*fi1812 + i100);
        _t18_18 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 1);
        _t18_17 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 2);
        _t18_16 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 3);
        _t18_15 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 28);
        _t18_14 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 29);
        _t18_13 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 30);
        _t18_12 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 31);
        _t18_11 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 56);
        _t18_10 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 57);
        _t18_9 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 58);
        _t18_8 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 59);
        _t18_7 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 84);
        _t18_6 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 85);
        _t18_5 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 86);
        _t18_4 = _mm256_broadcast_sd(C + 28*fi1812 + i100 + 87);
        _t18_3 = _asm256_loadu_pd(L + 28*fi2064 + i100);
        _t18_2 = _asm256_loadu_pd(L + 28*fi2064 + i100 + 28);
        _t18_1 = _asm256_loadu_pd(L + 28*fi2064 + i100 + 56);
        _t18_0 = _asm256_loadu_pd(L + 28*fi2064 + i100 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t18_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
        _t18_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
        _t18_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
        _t18_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t18_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_28), _mm256_mul_pd(_t18_18, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_30), _mm256_mul_pd(_t18_16, _t18_31)));
        _t18_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_28), _mm256_mul_pd(_t18_14, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_30), _mm256_mul_pd(_t18_12, _t18_31)));
        _t18_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_28), _mm256_mul_pd(_t18_10, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_30), _mm256_mul_pd(_t18_8, _t18_31)));
        _t18_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_28), _mm256_mul_pd(_t18_6, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_30), _mm256_mul_pd(_t18_4, _t18_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t18_20 = _mm256_sub_pd(_t18_20, _t18_24);
        _t18_21 = _mm256_sub_pd(_t18_21, _t18_25);
        _t18_22 = _mm256_sub_pd(_t18_22, _t18_26);
        _t18_23 = _mm256_sub_pd(_t18_23, _t18_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 28*fi1812 + fi2064, _t18_20);
        _asm256_storeu_pd(C + 28*fi1812 + fi2064 + 28, _t18_21);
        _asm256_storeu_pd(C + 28*fi1812 + fi2064 + 56, _t18_22);
        _asm256_storeu_pd(C + 28*fi1812 + fi2064 + 84, _t18_23);
      }
      _t17_9 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 28);
      _t17_8 = _asm256_loadu_pd(C + 28*fi1812 + fi2064);
      _t17_10 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 56);
      _t17_11 = _asm256_loadu_pd(C + 28*fi1812 + fi2064 + 84);
      _t19_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi2064])));
      _t19_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi2064 + 28])));
      _t19_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi2064 + 29])));
      _t19_3 = _mm256_maskload_pd(L + 29*fi2064 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t19_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi2064 + 58])));
      _t19_1 = _mm256_maskload_pd(L + 29*fi2064 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t19_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi2064 + 87])));

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ),h(1, 28, fi2064))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_8, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_25 = _t16_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_26 = _t19_6;

      // 4-BLAC: 1x4 + 1x4
      _t19_27 = _mm256_add_pd(_t19_25, _t19_26);

      // 4-BLAC: 1x4 / 1x4
      _t19_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_24), _mm256_castpd256_pd128(_t19_27)));

      // AVX Storer:
      _t19_7 = _t19_28;

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 1)) - ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064)) Kro T( G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_8, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_30 = _t19_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_31 = _t19_5;

      // 4-BLAC: (4x1)^T
      _t19_32 = _t19_31;

      // 4-BLAC: 1x4 Kro 1x4
      _t19_33 = _mm256_mul_pd(_t19_30, _t19_32);

      // 4-BLAC: 1x4 - 1x4
      _t19_34 = _mm256_sub_pd(_t19_29, _t19_33);

      // AVX Storer:
      _t19_8 = _t19_34;

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 1)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_35 = _t19_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_36 = _t16_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_37 = _t19_4;

      // 4-BLAC: 1x4 + 1x4
      _t19_38 = _mm256_add_pd(_t19_36, _t19_37);

      // 4-BLAC: 1x4 / 1x4
      _t19_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_35), _mm256_castpd256_pd128(_t19_38)));

      // AVX Storer:
      _t19_8 = _t19_39;

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 2)) - ( G(h(1, 28, fi1812), X[28,28],h(2, 28, fi2064)) * T( G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_40 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_8, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_41 = _mm256_blend_pd(_mm256_unpacklo_pd(_t19_7, _t19_8), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_42 = _t19_3;

      // 4-BLAC: (1x4)^T
      _t19_43 = _t19_42;

      // 4-BLAC: 1x4 * 4x1
      _t19_44 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_41, _t19_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_41, _t19_43), _mm256_mul_pd(_t19_41, _t19_43), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_41, _t19_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_41, _t19_43), _mm256_mul_pd(_t19_41, _t19_43), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_41, _t19_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_41, _t19_43), _mm256_mul_pd(_t19_41, _t19_43), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_45 = _mm256_sub_pd(_t19_40, _t19_44);

      // AVX Storer:
      _t19_9 = _t19_45;

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 2)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_46 = _t19_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_47 = _t16_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_48 = _t19_2;

      // 4-BLAC: 1x4 + 1x4
      _t19_49 = _mm256_add_pd(_t19_47, _t19_48);

      // 4-BLAC: 1x4 / 1x4
      _t19_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_46), _mm256_castpd256_pd128(_t19_49)));

      // AVX Storer:
      _t19_9 = _t19_50;

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 3)) - ( G(h(1, 28, fi1812), X[28,28],h(3, 28, fi2064)) * T( G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_51 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_8, _t17_8, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_52 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_8), _mm256_unpacklo_pd(_t19_9, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_53 = _t19_1;

      // 4-BLAC: (1x4)^T
      _t19_54 = _t19_53;

      // 4-BLAC: 1x4 * 4x1
      _t19_55 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_52, _t19_54), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_52, _t19_54), _mm256_mul_pd(_t19_52, _t19_54), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_52, _t19_54), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_52, _t19_54), _mm256_mul_pd(_t19_52, _t19_54), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_52, _t19_54), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_52, _t19_54), _mm256_mul_pd(_t19_52, _t19_54), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_56 = _mm256_sub_pd(_t19_51, _t19_55);

      // AVX Storer:
      _t19_10 = _t19_56;

      // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi2064 + 3)) Div ( G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) + G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_57 = _t19_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_58 = _t16_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_59 = _t19_0;

      // 4-BLAC: 1x4 + 1x4
      _t19_60 = _mm256_add_pd(_t19_58, _t19_59);

      // 4-BLAC: 1x4 / 1x4
      _t19_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_57), _mm256_castpd256_pd128(_t19_60)));

      // AVX Storer:
      _t19_10 = _t19_61;

      // Generating : X[28,28] = S(h(3, 28, fi1812 + 1), ( G(h(3, 28, fi1812 + 1), X[28,28],h(4, 28, fi2064)) - ( G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) * G(h(1, 28, fi1812), X[28,28],h(4, 28, fi2064)) ) ),h(4, 28, fi2064))

      // AVX Loader:

      // 3x4 -> 4x4
      _t19_62 = _t17_9;
      _t19_63 = _t17_10;
      _t19_64 = _t17_11;
      _t19_65 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t19_66 = _t16_5;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t19_67 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_66, _t19_66, 32), _mm256_permute2f128_pd(_t19_66, _t19_66, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_8), _mm256_unpacklo_pd(_t19_9, _t19_10), 32));
      _t19_68 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_66, _t19_66, 32), _mm256_permute2f128_pd(_t19_66, _t19_66, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_8), _mm256_unpacklo_pd(_t19_9, _t19_10), 32));
      _t19_69 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_66, _t19_66, 49), _mm256_permute2f128_pd(_t19_66, _t19_66, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_8), _mm256_unpacklo_pd(_t19_9, _t19_10), 32));
      _t19_70 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_66, _t19_66, 49), _mm256_permute2f128_pd(_t19_66, _t19_66, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_8), _mm256_unpacklo_pd(_t19_9, _t19_10), 32));

      // 4-BLAC: 4x4 - 4x4
      _t19_71 = _mm256_sub_pd(_t19_62, _t19_67);
      _t19_72 = _mm256_sub_pd(_t19_63, _t19_68);
      _t19_73 = _mm256_sub_pd(_t19_64, _t19_69);
      _t19_74 = _mm256_sub_pd(_t19_65, _t19_70);

      // AVX Storer:
      _t17_9 = _t19_71;
      _t17_10 = _t19_72;
      _t17_11 = _t19_73;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ),h(1, 28, fi2064))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_75 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_9, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_76 = _t16_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_77 = _t19_6;

      // 4-BLAC: 1x4 + 1x4
      _t19_78 = _mm256_add_pd(_t19_76, _t19_77);

      // 4-BLAC: 1x4 / 1x4
      _t19_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_75), _mm256_castpd256_pd128(_t19_78)));

      // AVX Storer:
      _t19_11 = _t19_79;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 1)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064)) Kro T( G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_80 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_9, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_81 = _t19_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_82 = _t19_5;

      // 4-BLAC: (4x1)^T
      _t19_83 = _t19_82;

      // 4-BLAC: 1x4 Kro 1x4
      _t19_84 = _mm256_mul_pd(_t19_81, _t19_83);

      // 4-BLAC: 1x4 - 1x4
      _t19_85 = _mm256_sub_pd(_t19_80, _t19_84);

      // AVX Storer:
      _t19_12 = _t19_85;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 1)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_86 = _t19_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_87 = _t16_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_88 = _t19_4;

      // 4-BLAC: 1x4 + 1x4
      _t19_89 = _mm256_add_pd(_t19_87, _t19_88);

      // 4-BLAC: 1x4 / 1x4
      _t19_90 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_86), _mm256_castpd256_pd128(_t19_89)));

      // AVX Storer:
      _t19_12 = _t19_90;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 2)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(2, 28, fi2064)) * T( G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_91 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_9, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_9, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_92 = _mm256_blend_pd(_mm256_unpacklo_pd(_t19_11, _t19_12), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_93 = _t19_3;

      // 4-BLAC: (1x4)^T
      _t19_94 = _t19_93;

      // 4-BLAC: 1x4 * 4x1
      _t19_95 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_92, _t19_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_92, _t19_94), _mm256_mul_pd(_t19_92, _t19_94), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_92, _t19_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_92, _t19_94), _mm256_mul_pd(_t19_92, _t19_94), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_92, _t19_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_92, _t19_94), _mm256_mul_pd(_t19_92, _t19_94), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_96 = _mm256_sub_pd(_t19_91, _t19_95);

      // AVX Storer:
      _t19_13 = _t19_96;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 2)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_97 = _t19_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_98 = _t16_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_99 = _t19_2;

      // 4-BLAC: 1x4 + 1x4
      _t19_100 = _mm256_add_pd(_t19_98, _t19_99);

      // 4-BLAC: 1x4 / 1x4
      _t19_101 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_97), _mm256_castpd256_pd128(_t19_100)));

      // AVX Storer:
      _t19_13 = _t19_101;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 3)) - ( G(h(1, 28, fi1812 + 1), X[28,28],h(3, 28, fi2064)) * T( G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_102 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_9, _t17_9, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_103 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_11, _t19_12), _mm256_unpacklo_pd(_t19_13, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_104 = _t19_1;

      // 4-BLAC: (1x4)^T
      _t19_105 = _t19_104;

      // 4-BLAC: 1x4 * 4x1
      _t19_106 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_103, _t19_105), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_103, _t19_105), _mm256_mul_pd(_t19_103, _t19_105), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_103, _t19_105), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_103, _t19_105), _mm256_mul_pd(_t19_103, _t19_105), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_103, _t19_105), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_103, _t19_105), _mm256_mul_pd(_t19_103, _t19_105), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_107 = _mm256_sub_pd(_t19_102, _t19_106);

      // AVX Storer:
      _t19_14 = _t19_107;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi2064 + 3)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_108 = _t19_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_109 = _t16_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_110 = _t19_0;

      // 4-BLAC: 1x4 + 1x4
      _t19_111 = _mm256_add_pd(_t19_109, _t19_110);

      // 4-BLAC: 1x4 / 1x4
      _t19_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_108), _mm256_castpd256_pd128(_t19_111)));

      // AVX Storer:
      _t19_14 = _t19_112;

      // Generating : X[28,28] = S(h(2, 28, fi1812 + 2), ( G(h(2, 28, fi1812 + 2), X[28,28],h(4, 28, fi2064)) - ( G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) * G(h(1, 28, fi1812 + 1), X[28,28],h(4, 28, fi2064)) ) ),h(4, 28, fi2064))

      // AVX Loader:

      // 2x4 -> 4x4
      _t19_113 = _t17_10;
      _t19_114 = _t17_11;
      _t19_115 = _mm256_setzero_pd();
      _t19_116 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t19_117 = _t16_3;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t19_118 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_117, _t19_117, 32), _mm256_permute2f128_pd(_t19_117, _t19_117, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_11, _t19_12), _mm256_unpacklo_pd(_t19_13, _t19_14), 32));
      _t19_119 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_117, _t19_117, 32), _mm256_permute2f128_pd(_t19_117, _t19_117, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_11, _t19_12), _mm256_unpacklo_pd(_t19_13, _t19_14), 32));
      _t19_120 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_117, _t19_117, 49), _mm256_permute2f128_pd(_t19_117, _t19_117, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_11, _t19_12), _mm256_unpacklo_pd(_t19_13, _t19_14), 32));
      _t19_121 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_117, _t19_117, 49), _mm256_permute2f128_pd(_t19_117, _t19_117, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_11, _t19_12), _mm256_unpacklo_pd(_t19_13, _t19_14), 32));

      // 4-BLAC: 4x4 - 4x4
      _t19_122 = _mm256_sub_pd(_t19_113, _t19_118);
      _t19_123 = _mm256_sub_pd(_t19_114, _t19_119);
      _t19_124 = _mm256_sub_pd(_t19_115, _t19_120);
      _t19_125 = _mm256_sub_pd(_t19_116, _t19_121);

      // AVX Storer:
      _t17_10 = _t19_122;
      _t17_11 = _t19_123;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ),h(1, 28, fi2064))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_10, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_127 = _t16_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_128 = _t19_6;

      // 4-BLAC: 1x4 + 1x4
      _t19_129 = _mm256_add_pd(_t19_127, _t19_128);

      // 4-BLAC: 1x4 / 1x4
      _t19_130 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_126), _mm256_castpd256_pd128(_t19_129)));

      // AVX Storer:
      _t19_15 = _t19_130;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 1)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064)) Kro T( G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_131 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_10, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_132 = _t19_15;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_133 = _t19_5;

      // 4-BLAC: (4x1)^T
      _t19_134 = _t19_133;

      // 4-BLAC: 1x4 Kro 1x4
      _t19_135 = _mm256_mul_pd(_t19_132, _t19_134);

      // 4-BLAC: 1x4 - 1x4
      _t19_136 = _mm256_sub_pd(_t19_131, _t19_135);

      // AVX Storer:
      _t19_16 = _t19_136;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 1)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_137 = _t19_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_138 = _t16_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_139 = _t19_4;

      // 4-BLAC: 1x4 + 1x4
      _t19_140 = _mm256_add_pd(_t19_138, _t19_139);

      // 4-BLAC: 1x4 / 1x4
      _t19_141 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_137), _mm256_castpd256_pd128(_t19_140)));

      // AVX Storer:
      _t19_16 = _t19_141;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 2)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi2064)) * T( G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_142 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_10, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_10, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_143 = _mm256_blend_pd(_mm256_unpacklo_pd(_t19_15, _t19_16), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_144 = _t19_3;

      // 4-BLAC: (1x4)^T
      _t19_145 = _t19_144;

      // 4-BLAC: 1x4 * 4x1
      _t19_146 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_143, _t19_145), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_143, _t19_145), _mm256_mul_pd(_t19_143, _t19_145), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_143, _t19_145), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_143, _t19_145), _mm256_mul_pd(_t19_143, _t19_145), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_143, _t19_145), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_143, _t19_145), _mm256_mul_pd(_t19_143, _t19_145), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_147 = _mm256_sub_pd(_t19_142, _t19_146);

      // AVX Storer:
      _t19_17 = _t19_147;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 2)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_148 = _t19_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_149 = _t16_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_150 = _t19_2;

      // 4-BLAC: 1x4 + 1x4
      _t19_151 = _mm256_add_pd(_t19_149, _t19_150);

      // 4-BLAC: 1x4 / 1x4
      _t19_152 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_148), _mm256_castpd256_pd128(_t19_151)));

      // AVX Storer:
      _t19_17 = _t19_152;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 3)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(3, 28, fi2064)) * T( G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_153 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_10, _t17_10, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_154 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_15, _t19_16), _mm256_unpacklo_pd(_t19_17, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_155 = _t19_1;

      // 4-BLAC: (1x4)^T
      _t19_156 = _t19_155;

      // 4-BLAC: 1x4 * 4x1
      _t19_157 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_154, _t19_156), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_154, _t19_156), _mm256_mul_pd(_t19_154, _t19_156), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_154, _t19_156), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_154, _t19_156), _mm256_mul_pd(_t19_154, _t19_156), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_154, _t19_156), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_154, _t19_156), _mm256_mul_pd(_t19_154, _t19_156), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_158 = _mm256_sub_pd(_t19_153, _t19_157);

      // AVX Storer:
      _t19_18 = _t19_158;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi2064 + 3)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_159 = _t19_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_160 = _t16_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_161 = _t19_0;

      // 4-BLAC: 1x4 + 1x4
      _t19_162 = _mm256_add_pd(_t19_160, _t19_161);

      // 4-BLAC: 1x4 / 1x4
      _t19_163 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_159), _mm256_castpd256_pd128(_t19_162)));

      // AVX Storer:
      _t19_18 = _t19_163;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(4, 28, fi2064)) - ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) Kro G(h(1, 28, fi1812 + 2), X[28,28],h(4, 28, fi2064)) ) ),h(4, 28, fi2064))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_164 = _t16_1;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t19_23 = _mm256_mul_pd(_t19_164, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_15, _t19_16), _mm256_unpacklo_pd(_t19_17, _t19_18), 32));

      // 4-BLAC: 1x4 - 1x4
      _t17_11 = _mm256_sub_pd(_t17_11, _t19_23);

      // AVX Storer:

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi2064), L[28,28],h(1, 28, fi2064)) ) ),h(1, 28, fi2064))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_165 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_11, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_166 = _t16_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_167 = _t19_6;

      // 4-BLAC: 1x4 + 1x4
      _t19_168 = _mm256_add_pd(_t19_166, _t19_167);

      // 4-BLAC: 1x4 / 1x4
      _t19_169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_165), _mm256_castpd256_pd128(_t19_168)));

      // AVX Storer:
      _t19_19 = _t19_169;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 1)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064)) Kro T( G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_170 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_11, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_171 = _t19_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_172 = _t19_5;

      // 4-BLAC: (4x1)^T
      _t19_173 = _t19_172;

      // 4-BLAC: 1x4 Kro 1x4
      _t19_174 = _mm256_mul_pd(_t19_171, _t19_173);

      // 4-BLAC: 1x4 - 1x4
      _t19_175 = _mm256_sub_pd(_t19_170, _t19_174);

      // AVX Storer:
      _t19_20 = _t19_175;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 1)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi2064 + 1), L[28,28],h(1, 28, fi2064 + 1)) ) ),h(1, 28, fi2064 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_176 = _t19_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_177 = _t16_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_178 = _t19_4;

      // 4-BLAC: 1x4 + 1x4
      _t19_179 = _mm256_add_pd(_t19_177, _t19_178);

      // 4-BLAC: 1x4 / 1x4
      _t19_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_176), _mm256_castpd256_pd128(_t19_179)));

      // AVX Storer:
      _t19_20 = _t19_180;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 2)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi2064)) * T( G(h(1, 28, fi2064 + 2), L[28,28],h(2, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_181 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_11, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_182 = _mm256_blend_pd(_mm256_unpacklo_pd(_t19_19, _t19_20), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 1x2 -> 1x4
      _t19_183 = _t19_3;

      // 4-BLAC: (1x4)^T
      _t19_184 = _t19_183;

      // 4-BLAC: 1x4 * 4x1
      _t19_185 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_182, _t19_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_182, _t19_184), _mm256_mul_pd(_t19_182, _t19_184), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_182, _t19_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_182, _t19_184), _mm256_mul_pd(_t19_182, _t19_184), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_182, _t19_184), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_182, _t19_184), _mm256_mul_pd(_t19_182, _t19_184), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_186 = _mm256_sub_pd(_t19_181, _t19_185);

      // AVX Storer:
      _t19_21 = _t19_186;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 2)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi2064 + 2), L[28,28],h(1, 28, fi2064 + 2)) ) ),h(1, 28, fi2064 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_187 = _t19_21;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_188 = _t16_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_189 = _t19_2;

      // 4-BLAC: 1x4 + 1x4
      _t19_190 = _mm256_add_pd(_t19_188, _t19_189);

      // 4-BLAC: 1x4 / 1x4
      _t19_191 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_187), _mm256_castpd256_pd128(_t19_190)));

      // AVX Storer:
      _t19_21 = _t19_191;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 3)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, fi2064)) * T( G(h(1, 28, fi2064 + 3), L[28,28],h(3, 28, fi2064)) ) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_192 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_11, _t17_11, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_193 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_19, _t19_20), _mm256_unpacklo_pd(_t19_21, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 1x3 -> 1x4
      _t19_194 = _t19_1;

      // 4-BLAC: (1x4)^T
      _t19_195 = _t19_194;

      // 4-BLAC: 1x4 * 4x1
      _t19_196 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_193, _t19_195), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_193, _t19_195), _mm256_mul_pd(_t19_193, _t19_195), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_193, _t19_195), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_193, _t19_195), _mm256_mul_pd(_t19_193, _t19_195), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_193, _t19_195), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_193, _t19_195), _mm256_mul_pd(_t19_193, _t19_195), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t19_197 = _mm256_sub_pd(_t19_192, _t19_196);

      // AVX Storer:
      _t19_22 = _t19_197;

      // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi2064 + 3)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi2064 + 3), L[28,28],h(1, 28, fi2064 + 3)) ) ),h(1, 28, fi2064 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_198 = _t19_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_199 = _t16_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t19_200 = _t19_0;

      // 4-BLAC: 1x4 + 1x4
      _t19_201 = _mm256_add_pd(_t19_199, _t19_200);

      // 4-BLAC: 1x4 / 1x4
      _t19_202 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_198), _mm256_castpd256_pd128(_t19_201)));

      // AVX Storer:
      _t19_22 = _t19_202;
      _mm_store_sd(&(C[28*fi1812 + fi2064]), _mm256_castpd256_pd128(_t19_7));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 1]), _mm256_castpd256_pd128(_t19_8));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 2]), _mm256_castpd256_pd128(_t19_9));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 3]), _mm256_castpd256_pd128(_t19_10));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 28]), _mm256_castpd256_pd128(_t19_11));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 29]), _mm256_castpd256_pd128(_t19_12));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 30]), _mm256_castpd256_pd128(_t19_13));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 31]), _mm256_castpd256_pd128(_t19_14));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 56]), _mm256_castpd256_pd128(_t19_15));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 57]), _mm256_castpd256_pd128(_t19_16));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 58]), _mm256_castpd256_pd128(_t19_17));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 59]), _mm256_castpd256_pd128(_t19_18));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 84]), _mm256_castpd256_pd128(_t19_19));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 85]), _mm256_castpd256_pd128(_t19_20));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 86]), _mm256_castpd256_pd128(_t19_21));
      _mm_store_sd(&(C[28*fi1812 + fi2064 + 87]), _mm256_castpd256_pd128(_t19_22));
    }

    // Generating : X[28,28] = Sum_{i100} ( Sum_{k219} ( S(h(4, 28, fi1812 + i100 + 4), ( G(h(4, 28, fi1812 + i100 + 4), X[28,28],h(4, 28, k219)) - ( G(h(4, 28, fi1812 + i100 + 4), L[28,28],h(4, 28, fi1812)) * G(h(4, 28, fi1812), X[28,28],h(4, 28, k219)) ) ),h(4, 28, k219)) ) )
    _mm_store_sd(&(C[28*fi1812]), _mm256_castpd256_pd128(_t16_39));
    _mm_store_sd(&(C[28*fi1812 + 1]), _mm256_castpd256_pd128(_t16_40));
    _mm_store_sd(&(C[28*fi1812 + 2]), _mm256_castpd256_pd128(_t16_41));
    _mm_store_sd(&(C[28*fi1812 + 3]), _mm256_castpd256_pd128(_t16_7));
    _mm_store_sd(&(C[28*fi1812 + 28]), _mm256_castpd256_pd128(_t16_11));
    _mm_store_sd(&(C[28*fi1812 + 29]), _mm256_castpd256_pd128(_t16_12));
    _mm_store_sd(&(C[28*fi1812 + 30]), _mm256_castpd256_pd128(_t16_13));
    _mm_store_sd(&(C[28*fi1812 + 31]), _mm256_castpd256_pd128(_t16_14));
    _mm_store_sd(&(C[28*fi1812 + 56]), _mm256_castpd256_pd128(_t16_15));
    _mm_store_sd(&(C[28*fi1812 + 57]), _mm256_castpd256_pd128(_t16_16));
    _mm_store_sd(&(C[28*fi1812 + 58]), _mm256_castpd256_pd128(_t16_17));
    _mm_store_sd(&(C[28*fi1812 + 59]), _mm256_castpd256_pd128(_t16_18));
    _mm_store_sd(&(C[28*fi1812 + 84]), _mm256_castpd256_pd128(_t16_19));
    _mm_store_sd(&(C[28*fi1812 + 85]), _mm256_castpd256_pd128(_t16_20));
    _mm_store_sd(&(C[28*fi1812 + 86]), _mm256_castpd256_pd128(_t16_21));
    _mm_store_sd(&(C[28*fi1812 + 87]), _mm256_castpd256_pd128(_t16_22));

    for( int i100 = 0; i100 <= -fi1812 + 23; i100+=4 ) {

      for( int k219 = 0; k219 <= fi1812 - 1; k219+=4 ) {
        _t20_24 = _asm256_loadu_pd(C + 28*fi1812 + 28*i100 + k219 + 112);
        _t20_25 = _asm256_loadu_pd(C + 28*fi1812 + 28*i100 + k219 + 140);
        _t20_26 = _asm256_loadu_pd(C + 28*fi1812 + 28*i100 + k219 + 168);
        _t20_27 = _asm256_loadu_pd(C + 28*fi1812 + 28*i100 + k219 + 196);
        _t20_19 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 112);
        _t20_18 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 113);
        _t20_17 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 114);
        _t20_16 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 115);
        _t20_15 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 140);
        _t20_14 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 141);
        _t20_13 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 142);
        _t20_12 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 143);
        _t20_11 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 168);
        _t20_10 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 169);
        _t20_9 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 170);
        _t20_8 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 171);
        _t20_7 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 196);
        _t20_6 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 197);
        _t20_5 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 198);
        _t20_4 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 199);
        _t20_3 = _asm256_loadu_pd(C + 28*fi1812 + k219);
        _t20_2 = _asm256_loadu_pd(C + 28*fi1812 + k219 + 28);
        _t20_1 = _asm256_loadu_pd(C + 28*fi1812 + k219 + 56);
        _t20_0 = _asm256_loadu_pd(C + 28*fi1812 + k219 + 84);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t20_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_19, _t20_3), _mm256_mul_pd(_t20_18, _t20_2)), _mm256_add_pd(_mm256_mul_pd(_t20_17, _t20_1), _mm256_mul_pd(_t20_16, _t20_0)));
        _t20_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t20_3), _mm256_mul_pd(_t20_14, _t20_2)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t20_1), _mm256_mul_pd(_t20_12, _t20_0)));
        _t20_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t20_3), _mm256_mul_pd(_t20_10, _t20_2)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t20_1), _mm256_mul_pd(_t20_8, _t20_0)));
        _t20_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t20_3), _mm256_mul_pd(_t20_6, _t20_2)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t20_1), _mm256_mul_pd(_t20_4, _t20_0)));

        // 4-BLAC: 4x4 - 4x4
        _t20_24 = _mm256_sub_pd(_t20_24, _t20_20);
        _t20_25 = _mm256_sub_pd(_t20_25, _t20_21);
        _t20_26 = _mm256_sub_pd(_t20_26, _t20_22);
        _t20_27 = _mm256_sub_pd(_t20_27, _t20_23);

        // AVX Storer:
        _asm256_storeu_pd(C + 28*fi1812 + 28*i100 + k219 + 112, _t20_24);
        _asm256_storeu_pd(C + 28*fi1812 + 28*i100 + k219 + 140, _t20_25);
        _asm256_storeu_pd(C + 28*fi1812 + 28*i100 + k219 + 168, _t20_26);
        _asm256_storeu_pd(C + 28*fi1812 + 28*i100 + k219 + 196, _t20_27);
      }
    }
    _t16_16 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 57])));
    _t16_22 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 87])));
    _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 3])));
    _t16_18 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 59])));
    _t16_40 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 1])));
    _t16_21 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 86])));
    _t16_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 31])));
    _t16_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 28])));
    _t16_17 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 58])));
    _t16_39 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812])));
    _t16_20 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 85])));
    _t16_19 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 84])));
    _t16_41 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 2])));
    _t16_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 56])));
    _t16_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 29])));
    _t16_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[28*fi1812 + 30])));
    _t21_24 = _mm256_castpd128_pd256(_mm_load_sd(C + 29*fi1812));
    _t21_25 = _mm256_maskload_pd(C + 29*fi1812 + 28, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t21_26 = _mm256_maskload_pd(C + 29*fi1812 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t21_27 = _asm256_loadu_pd(C + 29*fi1812 + 84);
    _t21_19 = _mm256_broadcast_sd(L + 28*fi1812);
    _t21_18 = _mm256_broadcast_sd(L + 28*fi1812 + 1);
    _t21_17 = _mm256_broadcast_sd(L + 28*fi1812 + 2);
    _t21_16 = _mm256_broadcast_sd(L + 28*fi1812 + 3);
    _t21_15 = _mm256_broadcast_sd(L + 28*fi1812 + 28);
    _t21_14 = _mm256_broadcast_sd(L + 28*fi1812 + 29);
    _t21_13 = _mm256_broadcast_sd(L + 28*fi1812 + 30);
    _t21_12 = _mm256_broadcast_sd(L + 28*fi1812 + 31);
    _t21_11 = _mm256_broadcast_sd(L + 28*fi1812 + 56);
    _t21_10 = _mm256_broadcast_sd(L + 28*fi1812 + 57);
    _t21_9 = _mm256_broadcast_sd(L + 28*fi1812 + 58);
    _t21_8 = _mm256_broadcast_sd(L + 28*fi1812 + 59);
    _t21_7 = _mm256_broadcast_sd(L + 28*fi1812 + 84);
    _t21_6 = _mm256_broadcast_sd(L + 28*fi1812 + 85);
    _t21_5 = _mm256_broadcast_sd(L + 28*fi1812 + 86);
    _t21_4 = _mm256_broadcast_sd(L + 28*fi1812 + 87);
    _t21_3 = _asm256_loadu_pd(L + 28*fi1812);
    _t21_2 = _asm256_loadu_pd(L + 28*fi1812 + 28);
    _t21_1 = _asm256_loadu_pd(L + 28*fi1812 + 56);
    _t21_0 = _asm256_loadu_pd(L + 28*fi1812 + 84);

    // Generating : X[28,28] = ( ( S(h(4, 28, fi1812), ( G(h(4, 28, fi1812), C[28,28],h(4, 28, fi1812)) - ( ( G(h(4, 28, fi1812), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi1812), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, fi1812), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi1812), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, fi1812)) + Sum_{k219} ( -$(h(4, 28, fi1812), ( G(h(4, 28, fi1812), X[28,28],h(4, 28, k219)) * T( G(h(4, 28, fi1812), L[28,28],h(4, 28, k219)) ) ),h(4, 28, fi1812)) ) ) + Sum_{i100} ( -$(h(4, 28, fi1812), ( G(h(4, 28, fi1812), L[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi1812), X[28,28],h(4, 28, i100)) ) ),h(4, 28, fi1812)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t21_40 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_24, _t21_25, 0), _mm256_shuffle_pd(_t21_26, _t21_27, 0), 32);
    _t21_41 = _mm256_permute2f128_pd(_t21_25, _mm256_shuffle_pd(_t21_26, _t21_27, 3), 32);
    _t21_42 = _mm256_blend_pd(_t21_26, _mm256_shuffle_pd(_t21_26, _t21_27, 3), 12);
    _t21_43 = _t21_27;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t21_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 32);
    _t21_45 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 32);
    _t21_46 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 49);
    _t21_47 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 49);

    // 4-BLAC: 4x4 * 4x4
    _t21_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_19, _t21_44), _mm256_mul_pd(_t21_18, _t21_45)), _mm256_add_pd(_mm256_mul_pd(_t21_17, _t21_46), _mm256_mul_pd(_t21_16, _t21_47)));
    _t21_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_15, _t21_44), _mm256_mul_pd(_t21_14, _t21_45)), _mm256_add_pd(_mm256_mul_pd(_t21_13, _t21_46), _mm256_mul_pd(_t21_12, _t21_47)));
    _t21_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_11, _t21_44), _mm256_mul_pd(_t21_10, _t21_45)), _mm256_add_pd(_mm256_mul_pd(_t21_9, _t21_46), _mm256_mul_pd(_t21_8, _t21_47)));
    _t21_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t21_7, _t21_44), _mm256_mul_pd(_t21_6, _t21_45)), _mm256_add_pd(_mm256_mul_pd(_t21_5, _t21_46), _mm256_mul_pd(_t21_4, _t21_47)));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t21_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_3, _t21_2), _mm256_unpacklo_pd(_t21_1, _t21_0), 32);
    _t21_49 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t21_3, _t21_2), _mm256_unpackhi_pd(_t21_1, _t21_0), 32);
    _t21_50 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_3, _t21_2), _mm256_unpacklo_pd(_t21_1, _t21_0), 49);
    _t21_51 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t21_3, _t21_2), _mm256_unpackhi_pd(_t21_1, _t21_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t21_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_39, _t16_39, 32), _mm256_permute2f128_pd(_t16_39, _t16_39, 32), 0), _t21_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_40, _t16_40, 32), _mm256_permute2f128_pd(_t16_40, _t16_40, 32), 0), _t21_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_41, _t16_41, 32), _mm256_permute2f128_pd(_t16_41, _t16_41, 32), 0), _t21_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_7, _t16_7, 32), _mm256_permute2f128_pd(_t16_7, _t16_7, 32), 0), _t21_51)));
    _t21_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_11, _t16_11, 32), _mm256_permute2f128_pd(_t16_11, _t16_11, 32), 0), _t21_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_12, _t16_12, 32), _mm256_permute2f128_pd(_t16_12, _t16_12, 32), 0), _t21_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_13, _t16_13, 32), _mm256_permute2f128_pd(_t16_13, _t16_13, 32), 0), _t21_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_14, _t16_14, 32), _mm256_permute2f128_pd(_t16_14, _t16_14, 32), 0), _t21_51)));
    _t21_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_15, _t16_15, 32), _mm256_permute2f128_pd(_t16_15, _t16_15, 32), 0), _t21_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_16, _t16_16, 32), _mm256_permute2f128_pd(_t16_16, _t16_16, 32), 0), _t21_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_17, _t16_17, 32), _mm256_permute2f128_pd(_t16_17, _t16_17, 32), 0), _t21_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_18, _t16_18, 32), _mm256_permute2f128_pd(_t16_18, _t16_18, 32), 0), _t21_51)));
    _t21_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_19, _t16_19, 32), _mm256_permute2f128_pd(_t16_19, _t16_19, 32), 0), _t21_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_20, _t16_20, 32), _mm256_permute2f128_pd(_t16_20, _t16_20, 32), 0), _t21_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_21, _t16_21, 32), _mm256_permute2f128_pd(_t16_21, _t16_21, 32), 0), _t21_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_22, _t16_22, 32), _mm256_permute2f128_pd(_t16_22, _t16_22, 32), 0), _t21_51)));

    // 4-BLAC: 4x4 + 4x4
    _t21_20 = _mm256_add_pd(_t21_28, _t21_32);
    _t21_21 = _mm256_add_pd(_t21_29, _t21_33);
    _t21_22 = _mm256_add_pd(_t21_30, _t21_34);
    _t21_23 = _mm256_add_pd(_t21_31, _t21_35);

    // 4-BLAC: 4x4 - 4x4
    _t21_36 = _mm256_sub_pd(_t21_40, _t21_20);
    _t21_37 = _mm256_sub_pd(_t21_41, _t21_21);
    _t21_38 = _mm256_sub_pd(_t21_42, _t21_22);
    _t21_39 = _mm256_sub_pd(_t21_43, _t21_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t21_24 = _t21_36;
    _t21_25 = _t21_37;
    _t21_26 = _t21_38;
    _t21_27 = _t21_39;

    for( int k219 = 4; k219 <= fi1812 - 1; k219+=4 ) {
      _t22_19 = _mm256_broadcast_sd(C + 28*fi1812 + k219);
      _t22_18 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 1);
      _t22_17 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 2);
      _t22_16 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 3);
      _t22_15 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 28);
      _t22_14 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 29);
      _t22_13 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 30);
      _t22_12 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 31);
      _t22_11 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 56);
      _t22_10 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 57);
      _t22_9 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 58);
      _t22_8 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 59);
      _t22_7 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 84);
      _t22_6 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 85);
      _t22_5 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 86);
      _t22_4 = _mm256_broadcast_sd(C + 28*fi1812 + k219 + 87);
      _t22_3 = _asm256_loadu_pd(L + 28*fi1812 + k219);
      _t22_2 = _asm256_loadu_pd(L + 28*fi1812 + k219 + 28);
      _t22_1 = _asm256_loadu_pd(L + 28*fi1812 + k219 + 56);
      _t22_0 = _asm256_loadu_pd(L + 28*fi1812 + k219 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t22_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32);
      _t22_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t22_3, _t22_2), _mm256_unpackhi_pd(_t22_1, _t22_0), 32);
      _t22_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 49);
      _t22_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t22_3, _t22_2), _mm256_unpackhi_pd(_t22_1, _t22_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t22_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_19, _t22_28), _mm256_mul_pd(_t22_18, _t22_29)), _mm256_add_pd(_mm256_mul_pd(_t22_17, _t22_30), _mm256_mul_pd(_t22_16, _t22_31)));
      _t22_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t22_28), _mm256_mul_pd(_t22_14, _t22_29)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t22_30), _mm256_mul_pd(_t22_12, _t22_31)));
      _t22_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t22_28), _mm256_mul_pd(_t22_10, _t22_29)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t22_30), _mm256_mul_pd(_t22_8, _t22_31)));
      _t22_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t22_28), _mm256_mul_pd(_t22_6, _t22_29)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t22_30), _mm256_mul_pd(_t22_4, _t22_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t22_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_24, _t21_25, 0), _mm256_shuffle_pd(_t21_26, _t21_27, 0), 32);
      _t22_25 = _mm256_permute2f128_pd(_t21_25, _mm256_shuffle_pd(_t21_26, _t21_27, 3), 32);
      _t22_26 = _mm256_blend_pd(_t21_26, _mm256_shuffle_pd(_t21_26, _t21_27, 3), 12);
      _t22_27 = _t21_27;

      // 4-BLAC: 4x4 - 4x4
      _t22_24 = _mm256_sub_pd(_t22_24, _t22_20);
      _t22_25 = _mm256_sub_pd(_t22_25, _t22_21);
      _t22_26 = _mm256_sub_pd(_t22_26, _t22_22);
      _t22_27 = _mm256_sub_pd(_t22_27, _t22_23);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t21_24 = _t22_24;
      _t21_25 = _t22_25;
      _t21_26 = _t22_26;
      _t21_27 = _t22_27;
    }

    for( int i100 = 4; i100 <= fi1812 - 1; i100+=4 ) {
      _t23_19 = _mm256_broadcast_sd(L + 28*fi1812 + i100);
      _t23_18 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 1);
      _t23_17 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 2);
      _t23_16 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 3);
      _t23_15 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28);
      _t23_14 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 29);
      _t23_13 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 30);
      _t23_12 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 31);
      _t23_11 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 56);
      _t23_10 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 57);
      _t23_9 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 58);
      _t23_8 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 59);
      _t23_7 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 84);
      _t23_6 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 85);
      _t23_5 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 86);
      _t23_4 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 87);
      _t23_3 = _asm256_loadu_pd(C + 28*fi1812 + i100);
      _t23_2 = _asm256_loadu_pd(C + 28*fi1812 + i100 + 28);
      _t23_1 = _asm256_loadu_pd(C + 28*fi1812 + i100 + 56);
      _t23_0 = _asm256_loadu_pd(C + 28*fi1812 + i100 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t23_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t23_3, _t23_2), _mm256_unpacklo_pd(_t23_1, _t23_0), 32);
      _t23_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t23_3, _t23_2), _mm256_unpackhi_pd(_t23_1, _t23_0), 32);
      _t23_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t23_3, _t23_2), _mm256_unpacklo_pd(_t23_1, _t23_0), 49);
      _t23_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t23_3, _t23_2), _mm256_unpackhi_pd(_t23_1, _t23_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t23_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_19, _t23_28), _mm256_mul_pd(_t23_18, _t23_29)), _mm256_add_pd(_mm256_mul_pd(_t23_17, _t23_30), _mm256_mul_pd(_t23_16, _t23_31)));
      _t23_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_15, _t23_28), _mm256_mul_pd(_t23_14, _t23_29)), _mm256_add_pd(_mm256_mul_pd(_t23_13, _t23_30), _mm256_mul_pd(_t23_12, _t23_31)));
      _t23_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_11, _t23_28), _mm256_mul_pd(_t23_10, _t23_29)), _mm256_add_pd(_mm256_mul_pd(_t23_9, _t23_30), _mm256_mul_pd(_t23_8, _t23_31)));
      _t23_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_7, _t23_28), _mm256_mul_pd(_t23_6, _t23_29)), _mm256_add_pd(_mm256_mul_pd(_t23_5, _t23_30), _mm256_mul_pd(_t23_4, _t23_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t23_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_24, _t21_25, 0), _mm256_shuffle_pd(_t21_26, _t21_27, 0), 32);
      _t23_25 = _mm256_permute2f128_pd(_t21_25, _mm256_shuffle_pd(_t21_26, _t21_27, 3), 32);
      _t23_26 = _mm256_blend_pd(_t21_26, _mm256_shuffle_pd(_t21_26, _t21_27, 3), 12);
      _t23_27 = _t21_27;

      // 4-BLAC: 4x4 - 4x4
      _t23_24 = _mm256_sub_pd(_t23_24, _t23_20);
      _t23_25 = _mm256_sub_pd(_t23_25, _t23_21);
      _t23_26 = _mm256_sub_pd(_t23_26, _t23_22);
      _t23_27 = _mm256_sub_pd(_t23_27, _t23_23);

      // AVX Storer:

      // 4x4 -> 4x4 - LowSymm
      _t21_24 = _t23_24;
      _t21_25 = _t23_25;
      _t21_26 = _t23_26;
      _t21_27 = _t23_27;
    }
    _t24_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 28])));
    _t24_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 56)), _mm256_castpd128_pd256(_mm_load_sd(L + 29*fi1812 + 84)), 0);
    _t24_3 = _mm256_maskload_pd(L + 29*fi1812 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t24_2 = _mm256_maskload_pd(L + 29*fi1812 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t24_1 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 86])));
    _t24_0 = _mm256_maskload_pd(L + 29*fi1812 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : X[28,28] = S(h(1, 28, fi1812), ( G(h(1, 28, fi1812), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_19 = _t21_24;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t24_20 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_21 = _t16_6;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_22 = _mm256_mul_pd(_t24_20, _t24_21);

    // 4-BLAC: 1x4 / 1x4
    _t24_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_19), _mm256_castpd256_pd128(_t24_22)));

    // AVX Storer:
    _t21_24 = _t24_23;

    // Generating : X[28,28] = S(h(3, 28, fi1812 + 1), ( G(h(3, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) - ( G(h(3, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) Kro G(h(1, 28, fi1812), X[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 3x1 -> 4x1
    _t24_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_25, _t21_26), _mm256_unpacklo_pd(_t21_27, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t24_25 = _t16_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_24, _t21_24, 32), _mm256_permute2f128_pd(_t21_24, _t21_24, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t24_27 = _mm256_mul_pd(_t24_25, _t24_26);

    // 4-BLAC: 4x1 - 4x1
    _t24_28 = _mm256_sub_pd(_t24_24, _t24_27);

    // AVX Storer:
    _t24_6 = _t24_28;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_30 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_31 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t24_32 = _mm256_add_pd(_t24_30, _t24_31);

    // 4-BLAC: 1x4 / 1x4
    _t24_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_29), _mm256_castpd256_pd128(_t24_32)));

    // AVX Storer:
    _t24_7 = _t24_33;

    // Generating : X[28,28] = S(h(2, 28, fi1812 + 2), ( G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) - ( G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) Kro G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_6, 2), _mm256_permute2f128_pd(_t24_6, _t24_6, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_35 = _t16_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_7, _t24_7, 32), _mm256_permute2f128_pd(_t24_7, _t24_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t24_37 = _mm256_mul_pd(_t24_35, _t24_36);

    // 4-BLAC: 4x1 - 4x1
    _t24_38 = _mm256_sub_pd(_t24_34, _t24_37);

    // AVX Storer:
    _t24_8 = _t24_38;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) - ( ( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) + ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_39 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_25, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_40 = _t24_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_41 = _t24_7;

    // 4-BLAC: (4x1)^T
    _t24_42 = _t24_41;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_43 = _mm256_mul_pd(_t24_40, _t24_42);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_44 = _t24_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_45 = _t24_5;

    // 4-BLAC: (4x1)^T
    _t24_46 = _t24_45;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_47 = _mm256_mul_pd(_t24_44, _t24_46);

    // 4-BLAC: 1x4 + 1x4
    _t24_48 = _mm256_add_pd(_t24_43, _t24_47);

    // 4-BLAC: 1x4 - 1x4
    _t24_49 = _mm256_sub_pd(_t24_39, _t24_48);

    // AVX Storer:
    _t24_9 = _t24_49;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 1), ( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_50 = _t24_9;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t24_51 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_52 = _t16_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_53 = _mm256_mul_pd(_t24_51, _t24_52);

    // 4-BLAC: 1x4 / 1x4
    _t24_54 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_50), _mm256_castpd256_pd128(_t24_53)));

    // AVX Storer:
    _t24_9 = _t24_54;

    // Generating : X[28,28] = S(h(2, 28, fi1812 + 2), ( G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_55 = _mm256_unpackhi_pd(_mm256_blend_pd(_t21_26, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t21_27, _mm256_setzero_pd(), 12));

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_56 = _t24_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_7, _t24_7, 32), _mm256_permute2f128_pd(_t24_7, _t24_7, 32), 0);

    // 4-BLAC: (4x1)^T
    _t24_58 = _t24_57;

    // 4-BLAC: 4x1 Kro 1x4
    _t24_59 = _mm256_mul_pd(_t24_56, _t24_58);

    // 4-BLAC: 4x1 - 4x1
    _t24_60 = _mm256_sub_pd(_t24_55, _t24_59);

    // AVX Storer:
    _t24_10 = _t24_60;

    // Generating : X[28,28] = S(h(2, 28, fi1812 + 2), ( G(h(2, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(2, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 1)) Kro G(h(1, 28, fi1812 + 1), X[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_61 = _t24_10;

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_62 = _t16_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_63 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_9, _t24_9, 32), _mm256_permute2f128_pd(_t24_9, _t24_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t24_64 = _mm256_mul_pd(_t24_62, _t24_63);

    // 4-BLAC: 4x1 - 4x1
    _t24_65 = _mm256_sub_pd(_t24_61, _t24_64);

    // AVX Storer:
    _t24_10 = _t24_65;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_66 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_67 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_68 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t24_69 = _mm256_add_pd(_t24_67, _t24_68);

    // 4-BLAC: 1x4 / 1x4
    _t24_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_66), _mm256_castpd256_pd128(_t24_69)));

    // AVX Storer:
    _t24_11 = _t24_70;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_71 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_72 = _t24_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_73 = _t24_5;

    // 4-BLAC: (4x1)^T
    _t24_74 = _t24_73;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_75 = _mm256_mul_pd(_t24_72, _t24_74);

    // 4-BLAC: 1x4 - 1x4
    _t24_76 = _mm256_sub_pd(_t24_71, _t24_75);

    // AVX Storer:
    _t24_12 = _t24_76;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_77 = _t24_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_78 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_79 = _t16_4;

    // 4-BLAC: 1x4 + 1x4
    _t24_80 = _mm256_add_pd(_t24_78, _t24_79);

    // 4-BLAC: 1x4 / 1x4
    _t24_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_77), _mm256_castpd256_pd128(_t24_80)));

    // AVX Storer:
    _t24_12 = _t24_81;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi1812)) - ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) Kro G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ),h(2, 28, fi1812))

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_82 = _mm256_unpackhi_pd(_mm256_blend_pd(_t24_8, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t24_10, _mm256_setzero_pd(), 12));

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_83 = _t16_1;

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_84 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_setzero_pd(), 12);

    // 4-BLAC: 1x4 Kro 1x4
    _t24_85 = _mm256_mul_pd(_t24_83, _t24_84);

    // 4-BLAC: 1x4 - 1x4
    _t24_86 = _mm256_sub_pd(_t24_82, _t24_85);

    // AVX Storer:
    _t24_13 = _t24_86;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) - ( ( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ) + ( G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_87 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_26, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t21_26, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_88 = _t24_3;

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_89 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_setzero_pd(), 12);

    // 4-BLAC: (1x4)^T
    _t24_90 = _t24_89;

    // 4-BLAC: 1x4 * 4x1
    _t24_91 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_88, _t24_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_88, _t24_90), _mm256_mul_pd(_t24_88, _t24_90), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_88, _t24_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_88, _t24_90), _mm256_mul_pd(_t24_88, _t24_90), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_88, _t24_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_88, _t24_90), _mm256_mul_pd(_t24_88, _t24_90), 129)), 1));

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_92 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_93 = _t24_3;

    // 4-BLAC: (1x4)^T
    _t24_94 = _t24_93;

    // 4-BLAC: 1x4 * 4x1
    _t24_95 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_92, _t24_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_92, _t24_94), _mm256_mul_pd(_t24_92, _t24_94), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_92, _t24_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_92, _t24_94), _mm256_mul_pd(_t24_92, _t24_94), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_92, _t24_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_92, _t24_94), _mm256_mul_pd(_t24_92, _t24_94), 129)), 1));

    // 4-BLAC: 1x4 + 1x4
    _t24_96 = _mm256_add_pd(_t24_91, _t24_95);

    // 4-BLAC: 1x4 - 1x4
    _t24_97 = _mm256_sub_pd(_t24_87, _t24_96);

    // AVX Storer:
    _t24_14 = _t24_97;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 2), ( G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_98 = _t24_14;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t24_99 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_100 = _t16_2;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_101 = _mm256_mul_pd(_t24_99, _t24_100);

    // 4-BLAC: 1x4 / 1x4
    _t24_102 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_98), _mm256_castpd256_pd128(_t24_101)));

    // AVX Storer:
    _t24_14 = _t24_102;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, fi1812 + 3), L[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), X[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_103 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_27, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t21_27, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_104 = _t24_2;

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_105 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_setzero_pd(), 12);

    // 4-BLAC: (1x4)^T
    _t24_106 = _t24_105;

    // 4-BLAC: 1x4 * 4x1
    _t24_107 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_104, _t24_106), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_104, _t24_106), _mm256_mul_pd(_t24_104, _t24_106), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_104, _t24_106), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_104, _t24_106), _mm256_mul_pd(_t24_104, _t24_106), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_104, _t24_106), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_104, _t24_106), _mm256_mul_pd(_t24_104, _t24_106), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t24_108 = _mm256_sub_pd(_t24_103, _t24_107);

    // AVX Storer:
    _t24_15 = _t24_108;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 2)) Kro G(h(1, 28, fi1812 + 2), X[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_109 = _t24_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_110 = _t24_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_111 = _t24_14;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_112 = _mm256_mul_pd(_t24_110, _t24_111);

    // 4-BLAC: 1x4 - 1x4
    _t24_113 = _mm256_sub_pd(_t24_109, _t24_112);

    // AVX Storer:
    _t24_15 = _t24_113;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_114 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_115 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_116 = _t16_6;

    // 4-BLAC: 1x4 + 1x4
    _t24_117 = _mm256_add_pd(_t24_115, _t24_116);

    // 4-BLAC: 1x4 / 1x4
    _t24_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_114), _mm256_castpd256_pd128(_t24_117)));

    // AVX Storer:
    _t24_16 = _t24_118;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_119 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_13, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_120 = _t24_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_121 = _t24_5;

    // 4-BLAC: (4x1)^T
    _t24_122 = _t24_121;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_123 = _mm256_mul_pd(_t24_120, _t24_122);

    // 4-BLAC: 1x4 - 1x4
    _t24_124 = _mm256_sub_pd(_t24_119, _t24_123);

    // AVX Storer:
    _t24_17 = _t24_124;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_125 = _t24_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_126 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_127 = _t16_4;

    // 4-BLAC: 1x4 + 1x4
    _t24_128 = _mm256_add_pd(_t24_126, _t24_127);

    // 4-BLAC: 1x4 / 1x4
    _t24_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_125), _mm256_castpd256_pd128(_t24_128)));

    // AVX Storer:
    _t24_17 = _t24_129;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, fi1812 + 3), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_130 = _t24_15;

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_131 = _mm256_blend_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_132 = _t24_3;

    // 4-BLAC: (1x4)^T
    _t24_133 = _t24_132;

    // 4-BLAC: 1x4 * 4x1
    _t24_134 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_131, _t24_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_131, _t24_133), _mm256_mul_pd(_t24_131, _t24_133), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_131, _t24_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_131, _t24_133), _mm256_mul_pd(_t24_131, _t24_133), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_131, _t24_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_131, _t24_133), _mm256_mul_pd(_t24_131, _t24_133), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t24_135 = _mm256_sub_pd(_t24_130, _t24_134);

    // AVX Storer:
    _t24_15 = _t24_135;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_136 = _t24_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_137 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_138 = _t16_2;

    // 4-BLAC: 1x4 + 1x4
    _t24_139 = _mm256_add_pd(_t24_137, _t24_138);

    // 4-BLAC: 1x4 / 1x4
    _t24_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_136), _mm256_castpd256_pd128(_t24_139)));

    // AVX Storer:
    _t24_15 = _t24_140;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 3)) - ( ( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, fi1812)) ) ) + ( G(h(1, 28, fi1812 + 3), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_141 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t21_27, _t21_27, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t24_142 = _t24_0;

    // AVX Loader:

    // 1x3 -> 1x4
    _t24_143 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _mm256_setzero_pd()), 32);

    // 4-BLAC: (1x4)^T
    _t24_144 = _t24_143;

    // 4-BLAC: 1x4 * 4x1
    _t24_145 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_142, _t24_144), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_142, _t24_144), _mm256_mul_pd(_t24_142, _t24_144), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_142, _t24_144), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_142, _t24_144), _mm256_mul_pd(_t24_142, _t24_144), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_142, _t24_144), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_142, _t24_144), _mm256_mul_pd(_t24_142, _t24_144), 129)), 1));

    // AVX Loader:

    // 1x3 -> 1x4
    _t24_146 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t24_147 = _t24_0;

    // 4-BLAC: (1x4)^T
    _t24_148 = _t24_147;

    // 4-BLAC: 1x4 * 4x1
    _t24_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_mul_pd(_t24_146, _t24_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_mul_pd(_t24_146, _t24_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t24_146, _t24_148), _mm256_mul_pd(_t24_146, _t24_148), 129)), 1));

    // 4-BLAC: 1x4 + 1x4
    _t24_150 = _mm256_add_pd(_t24_145, _t24_149);

    // 4-BLAC: 1x4 - 1x4
    _t24_151 = _mm256_sub_pd(_t24_141, _t24_150);

    // AVX Storer:
    _t24_18 = _t24_151;

    // Generating : X[28,28] = S(h(1, 28, fi1812 + 3), ( G(h(1, 28, fi1812 + 3), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_152 = _t24_18;

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t24_153 = _mm256_set_pd(0, 0, 0, 2);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_154 = _t16_0;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_155 = _mm256_mul_pd(_t24_153, _t24_154);

    // 4-BLAC: 1x4 / 1x4
    _t24_156 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_152), _mm256_castpd256_pd128(_t24_155)));

    // AVX Storer:
    _t24_18 = _t24_156;

    // Generating : X[28,28] = ( Sum_{k219} ( S(h(4, 28, fi1812 + k219 + 4), ( G(h(4, 28, fi1812 + k219 + 4), C[28,28],h(4, 28, fi1812)) - ( G(h(4, 28, fi1812 + k219 + 4), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi1812), X[28,28],h(4, 28, 0)) ) ) ),h(4, 28, fi1812)) ) + Sum_{i100} ( Sum_{k219} ( -$(h(4, 28, fi1812 + k219 + 4), ( G(h(4, 28, fi1812 + k219 + 4), L[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi1812), X[28,28],h(4, 28, i100)) ) ),h(4, 28, fi1812)) ) ) )

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t24_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 32);
    _t24_158 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 32);
    _t24_159 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 49);
    _t24_160 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 49);

    for( int k219 = 0; k219 <= -fi1812 + 23; k219+=4 ) {
      _t25_20 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 112);
      _t25_21 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 140);
      _t25_22 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 168);
      _t25_23 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 196);
      _t25_15 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 112);
      _t25_14 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 113);
      _t25_13 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 114);
      _t25_12 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 115);
      _t25_11 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 140);
      _t25_10 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 141);
      _t25_9 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 142);
      _t25_8 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 143);
      _t25_7 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 168);
      _t25_6 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 169);
      _t25_5 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 170);
      _t25_4 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 171);
      _t25_3 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 196);
      _t25_2 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 197);
      _t25_1 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 198);
      _t25_0 = _mm256_broadcast_sd(L + 28*fi1812 + 28*k219 + 199);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t24_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 32);
      _t24_158 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 32);
      _t24_159 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 49);
      _t24_160 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_39, _t16_40), _mm256_unpacklo_pd(_t16_41, _t16_7), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_11, _t16_12), _mm256_unpacklo_pd(_t16_13, _t16_14), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_15, _t16_16), _mm256_unpacklo_pd(_t16_17, _t16_18), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_20), _mm256_unpacklo_pd(_t16_21, _t16_22), 32)), 49);

      // 4-BLAC: 4x4 * 4x4
      _t25_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t24_157), _mm256_mul_pd(_t25_14, _t24_158)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t24_159), _mm256_mul_pd(_t25_12, _t24_160)));
      _t25_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t24_157), _mm256_mul_pd(_t25_10, _t24_158)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t24_159), _mm256_mul_pd(_t25_8, _t24_160)));
      _t25_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t24_157), _mm256_mul_pd(_t25_6, _t24_158)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t24_159), _mm256_mul_pd(_t25_4, _t24_160)));
      _t25_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_3, _t24_157), _mm256_mul_pd(_t25_2, _t24_158)), _mm256_add_pd(_mm256_mul_pd(_t25_1, _t24_159), _mm256_mul_pd(_t25_0, _t24_160)));

      // 4-BLAC: 4x4 - 4x4
      _t25_20 = _mm256_sub_pd(_t25_20, _t25_16);
      _t25_21 = _mm256_sub_pd(_t25_21, _t25_17);
      _t25_22 = _mm256_sub_pd(_t25_22, _t25_18);
      _t25_23 = _mm256_sub_pd(_t25_23, _t25_19);

      // AVX Storer:
      _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 112, _t25_20);
      _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 140, _t25_21);
      _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 168, _t25_22);
      _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 196, _t25_23);
    }

    for( int i100 = 4; i100 <= fi1812 - 1; i100+=4 ) {

      for( int k219 = 0; k219 <= -fi1812 + 23; k219+=4 ) {
        _t26_20 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 112);
        _t26_21 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 140);
        _t26_22 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 168);
        _t26_23 = _asm256_loadu_pd(C + 29*fi1812 + 28*k219 + 196);
        _t26_19 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 112);
        _t26_18 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 113);
        _t26_17 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 114);
        _t26_16 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 115);
        _t26_15 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 140);
        _t26_14 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 141);
        _t26_13 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 142);
        _t26_12 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 143);
        _t26_11 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 168);
        _t26_10 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 169);
        _t26_9 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 170);
        _t26_8 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 171);
        _t26_7 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 196);
        _t26_6 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 197);
        _t26_5 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 198);
        _t26_4 = _mm256_broadcast_sd(L + 28*fi1812 + i100 + 28*k219 + 199);
        _t26_3 = _asm256_loadu_pd(C + 28*fi1812 + i100);
        _t26_2 = _asm256_loadu_pd(C + 28*fi1812 + i100 + 28);
        _t26_1 = _asm256_loadu_pd(C + 28*fi1812 + i100 + 56);
        _t26_0 = _asm256_loadu_pd(C + 28*fi1812 + i100 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t26_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_3, _t26_2), _mm256_unpacklo_pd(_t26_1, _t26_0), 32);
        _t26_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t26_3, _t26_2), _mm256_unpackhi_pd(_t26_1, _t26_0), 32);
        _t26_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_3, _t26_2), _mm256_unpacklo_pd(_t26_1, _t26_0), 49);
        _t26_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t26_3, _t26_2), _mm256_unpackhi_pd(_t26_1, _t26_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t26_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_19, _t26_28), _mm256_mul_pd(_t26_18, _t26_29)), _mm256_add_pd(_mm256_mul_pd(_t26_17, _t26_30), _mm256_mul_pd(_t26_16, _t26_31)));
        _t26_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t26_28), _mm256_mul_pd(_t26_14, _t26_29)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t26_30), _mm256_mul_pd(_t26_12, _t26_31)));
        _t26_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t26_28), _mm256_mul_pd(_t26_10, _t26_29)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t26_30), _mm256_mul_pd(_t26_8, _t26_31)));
        _t26_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t26_28), _mm256_mul_pd(_t26_6, _t26_29)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t26_30), _mm256_mul_pd(_t26_4, _t26_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t26_20 = _mm256_sub_pd(_t26_20, _t26_24);
        _t26_21 = _mm256_sub_pd(_t26_21, _t26_25);
        _t26_22 = _mm256_sub_pd(_t26_22, _t26_26);
        _t26_23 = _mm256_sub_pd(_t26_23, _t26_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 112, _t26_20);
        _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 140, _t26_21);
        _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 168, _t26_22);
        _asm256_storeu_pd(C + 29*fi1812 + 28*k219 + 196, _t26_23);
      }
    }

    // Generating : X[28,28] = Sum_{i100} ( S(h(4, 28, fi1812 + i100 + 4), ( G(h(4, 28, fi1812 + i100 + 4), X[28,28],h(4, 28, fi1812)) - ( G(h(4, 28, fi1812 + i100 + 4), L[28,28],h(4, 28, fi1812)) * G(h(4, 28, fi1812), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812)) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t27_0 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_24, _mm256_blend_pd(_mm256_unpacklo_pd(_t24_7, _t24_9), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32), 0), 32);
    _t27_1 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t24_7, _t24_9), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32), 3), 32);
    _t27_2 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32), 3), 12);
    _t27_3 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32);
    _mm256_maskstore_pd(C + 29*fi1812 + 28, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t21_25);
    _mm256_maskstore_pd(C + 29*fi1812 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t21_26);
    _asm256_storeu_pd(C + 29*fi1812 + 84, _t21_27);
    _mm256_maskstore_pd(C + 29*fi1812 + 28, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t24_6);
    _mm256_maskstore_pd(C + 29*fi1812 + 56, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t24_6, _t24_6, 1));
    _mm256_maskstore_pd(C + 29*fi1812 + 84, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t24_6, _t24_6, 129));
    _mm256_maskstore_pd(C + 29*fi1812 + 56, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t24_8);
    _mm256_maskstore_pd(C + 29*fi1812 + 84, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t24_8, _t24_8, 1));
    _mm256_maskstore_pd(C + 29*fi1812 + 57, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t24_10);
    _mm256_maskstore_pd(C + 29*fi1812 + 85, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t24_10, _t24_10, 1));
    _mm256_maskstore_pd(C + 29*fi1812 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t24_13);

    for( int i100 = 0; i100 <= -fi1812 + 23; i100+=4 ) {
      _t28_20 = _asm256_loadu_pd(C + 29*fi1812 + 28*i100 + 112);
      _t28_21 = _asm256_loadu_pd(C + 29*fi1812 + 28*i100 + 140);
      _t28_22 = _asm256_loadu_pd(C + 29*fi1812 + 28*i100 + 168);
      _t28_23 = _asm256_loadu_pd(C + 29*fi1812 + 28*i100 + 196);
      _t28_15 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 112);
      _t28_14 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 113);
      _t28_13 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 114);
      _t28_12 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 115);
      _t28_11 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 140);
      _t28_10 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 141);
      _t28_9 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 142);
      _t28_8 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 143);
      _t28_7 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 168);
      _t28_6 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 169);
      _t28_5 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 170);
      _t28_4 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 171);
      _t28_3 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 196);
      _t28_2 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 197);
      _t28_1 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 198);
      _t28_0 = _mm256_broadcast_sd(L + 29*fi1812 + 28*i100 + 199);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - LowSymm
      _t28_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_24, _mm256_blend_pd(_mm256_unpacklo_pd(_t24_7, _t24_9), _mm256_setzero_pd(), 12), 0), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32), 0), 32);
      _t28_25 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_unpacklo_pd(_t24_7, _t24_9), _mm256_setzero_pd(), 12), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32), 3), 32);
      _t28_26 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_shuffle_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_11, _t24_12), _mm256_unpacklo_pd(_t24_14, _mm256_setzero_pd()), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32), 3), 12);
      _t28_27 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_16, _t24_17), _mm256_unpacklo_pd(_t24_15, _t24_18), 32);

      // 4-BLAC: 4x4 * 4x4
      _t28_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_15, _t28_24), _mm256_mul_pd(_t28_14, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_13, _t28_26), _mm256_mul_pd(_t28_12, _t28_27)));
      _t28_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_11, _t28_24), _mm256_mul_pd(_t28_10, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_9, _t28_26), _mm256_mul_pd(_t28_8, _t28_27)));
      _t28_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_7, _t28_24), _mm256_mul_pd(_t28_6, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_5, _t28_26), _mm256_mul_pd(_t28_4, _t28_27)));
      _t28_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_3, _t28_24), _mm256_mul_pd(_t28_2, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_1, _t28_26), _mm256_mul_pd(_t28_0, _t28_27)));

      // 4-BLAC: 4x4 - 4x4
      _t28_20 = _mm256_sub_pd(_t28_20, _t28_16);
      _t28_21 = _mm256_sub_pd(_t28_21, _t28_17);
      _t28_22 = _mm256_sub_pd(_t28_22, _t28_18);
      _t28_23 = _mm256_sub_pd(_t28_23, _t28_19);

      // AVX Storer:
      _asm256_storeu_pd(C + 29*fi1812 + 28*i100 + 112, _t28_20);
      _asm256_storeu_pd(C + 29*fi1812 + 28*i100 + 140, _t28_21);
      _asm256_storeu_pd(C + 29*fi1812 + 28*i100 + 168, _t28_22);
      _asm256_storeu_pd(C + 29*fi1812 + 28*i100 + 196, _t28_23);
    }
    _mm_store_sd(C + 29*fi1812, _mm256_castpd256_pd128(_t21_24));
    _mm_store_sd(&(C[29*fi1812 + 28]), _mm256_castpd256_pd128(_t24_7));
    _mm_store_sd(&(C[29*fi1812 + 29]), _mm256_castpd256_pd128(_t24_9));
    _mm_store_sd(&(C[29*fi1812 + 56]), _mm256_castpd256_pd128(_t24_11));
    _mm_store_sd(&(C[29*fi1812 + 57]), _mm256_castpd256_pd128(_t24_12));
    _mm_store_sd(&(C[29*fi1812 + 58]), _mm256_castpd256_pd128(_t24_14));
    _mm_store_sd(&(C[29*fi1812 + 84]), _mm256_castpd256_pd128(_t24_16));
    _mm_store_sd(&(C[29*fi1812 + 85]), _mm256_castpd256_pd128(_t24_17));
    _mm_store_sd(&(C[29*fi1812 + 86]), _mm256_castpd256_pd128(_t24_15));
    _mm_store_sd(&(C[29*fi1812 + 87]), _mm256_castpd256_pd128(_t24_18));
  }

  _t29_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[672])));
  _t29_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[696])));
  _t29_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[673])));
  _t29_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[674])));
  _t29_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[675])));
  _t29_11 = _asm256_loadu_pd(C + 700);
  _t29_12 = _asm256_loadu_pd(C + 728);
  _t29_13 = _asm256_loadu_pd(C + 756);
  _t29_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 724)), _mm256_castpd128_pd256(_mm_load_sd(L + 752))), _mm256_castpd128_pd256(_mm_load_sd(L + 780)), 32);
  _t29_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[725])));
  _t29_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 753)), _mm256_castpd128_pd256(_mm_load_sd(L + 781)), 0);
  _t29_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[754])));
  _t29_1 = _mm256_broadcast_sd(&(L[782]));
  _t29_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[783])));
  _t29_73 = _asm256_loadu_pd(C + 676);
  _t29_74 = _asm256_loadu_pd(C + 704);
  _t29_75 = _asm256_loadu_pd(C + 732);
  _t29_76 = _asm256_loadu_pd(C + 760);
  _t29_77 = _asm256_loadu_pd(C + 680);
  _t29_78 = _asm256_loadu_pd(C + 708);
  _t29_79 = _asm256_loadu_pd(C + 736);
  _t29_80 = _asm256_loadu_pd(C + 764);

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_81 = _t29_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_82 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_83 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t29_84 = _mm256_add_pd(_t29_82, _t29_83);

  // 4-BLAC: 1x4 / 1x4
  _t29_85 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_81), _mm256_castpd256_pd128(_t29_84)));

  // AVX Storer:
  _t29_7 = _t29_85;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_86 = _t29_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_87 = _t29_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_88 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t29_89 = _t29_88;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_90 = _mm256_mul_pd(_t29_87, _t29_89);

  // 4-BLAC: 1x4 - 1x4
  _t29_91 = _mm256_sub_pd(_t29_86, _t29_90);

  // AVX Storer:
  _t29_8 = _t29_91;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_92 = _t29_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_93 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_94 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t29_95 = _mm256_add_pd(_t29_93, _t29_94);

  // 4-BLAC: 1x4 / 1x4
  _t29_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_92), _mm256_castpd256_pd128(_t29_95)));

  // AVX Storer:
  _t29_8 = _t29_96;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_97 = _t29_9;

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_98 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_99 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t29_100 = _t29_99;

  // 4-BLAC: 1x4 * 4x1
  _t29_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_98, _t29_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_98, _t29_100), _mm256_mul_pd(_t29_98, _t29_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_98, _t29_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_98, _t29_100), _mm256_mul_pd(_t29_98, _t29_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_98, _t29_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_98, _t29_100), _mm256_mul_pd(_t29_98, _t29_100), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_102 = _mm256_sub_pd(_t29_97, _t29_101);

  // AVX Storer:
  _t29_9 = _t29_102;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_103 = _t29_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_104 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_105 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_106 = _mm256_add_pd(_t29_104, _t29_105);

  // 4-BLAC: 1x4 / 1x4
  _t29_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_103), _mm256_castpd256_pd128(_t29_106)));

  // AVX Storer:
  _t29_9 = _t29_107;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_108 = _t29_10;

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_110 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t29_111 = _t29_110;

  // 4-BLAC: 1x4 * 4x1
  _t29_112 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_109, _t29_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_109, _t29_111), _mm256_mul_pd(_t29_109, _t29_111), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_109, _t29_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_109, _t29_111), _mm256_mul_pd(_t29_109, _t29_111), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_109, _t29_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_109, _t29_111), _mm256_mul_pd(_t29_109, _t29_111), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_113 = _mm256_sub_pd(_t29_108, _t29_112);

  // AVX Storer:
  _t29_10 = _t29_113;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_114 = _t29_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_115 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_116 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t29_117 = _mm256_add_pd(_t29_115, _t29_116);

  // 4-BLAC: 1x4 / 1x4
  _t29_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_114), _mm256_castpd256_pd128(_t29_117)));

  // AVX Storer:
  _t29_10 = _t29_118;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, 0)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t29_119 = _t29_11;
  _t29_120 = _t29_12;
  _t29_121 = _t29_13;
  _t29_122 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t29_123 = _t29_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t29_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_123, _t29_123, 32), _mm256_permute2f128_pd(_t29_123, _t29_123, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32));
  _t29_125 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_123, _t29_123, 32), _mm256_permute2f128_pd(_t29_123, _t29_123, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32));
  _t29_126 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_123, _t29_123, 49), _mm256_permute2f128_pd(_t29_123, _t29_123, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32));
  _t29_127 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_123, _t29_123, 49), _mm256_permute2f128_pd(_t29_123, _t29_123, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32));

  // 4-BLAC: 4x4 - 4x4
  _t29_128 = _mm256_sub_pd(_t29_119, _t29_124);
  _t29_129 = _mm256_sub_pd(_t29_120, _t29_125);
  _t29_130 = _mm256_sub_pd(_t29_121, _t29_126);
  _t29_131 = _mm256_sub_pd(_t29_122, _t29_127);

  // AVX Storer:
  _t29_11 = _t29_128;
  _t29_12 = _t29_129;
  _t29_13 = _t29_130;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_132 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_11, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_133 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_134 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t29_135 = _mm256_add_pd(_t29_133, _t29_134);

  // 4-BLAC: 1x4 / 1x4
  _t29_136 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_132), _mm256_castpd256_pd128(_t29_135)));

  // AVX Storer:
  _t29_14 = _t29_136;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_137 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_11, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_138 = _t29_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_139 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t29_140 = _t29_139;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_141 = _mm256_mul_pd(_t29_138, _t29_140);

  // 4-BLAC: 1x4 - 1x4
  _t29_142 = _mm256_sub_pd(_t29_137, _t29_141);

  // AVX Storer:
  _t29_15 = _t29_142;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_143 = _t29_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_144 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_145 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t29_146 = _mm256_add_pd(_t29_144, _t29_145);

  // 4-BLAC: 1x4 / 1x4
  _t29_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_143), _mm256_castpd256_pd128(_t29_146)));

  // AVX Storer:
  _t29_15 = _t29_147;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_148 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_11, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_11, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_149 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_150 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t29_151 = _t29_150;

  // 4-BLAC: 1x4 * 4x1
  _t29_152 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_149, _t29_151), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_149, _t29_151), _mm256_mul_pd(_t29_149, _t29_151), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_149, _t29_151), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_149, _t29_151), _mm256_mul_pd(_t29_149, _t29_151), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_149, _t29_151), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_149, _t29_151), _mm256_mul_pd(_t29_149, _t29_151), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_153 = _mm256_sub_pd(_t29_148, _t29_152);

  // AVX Storer:
  _t29_16 = _t29_153;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_154 = _t29_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_155 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_156 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_157 = _mm256_add_pd(_t29_155, _t29_156);

  // 4-BLAC: 1x4 / 1x4
  _t29_158 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_154), _mm256_castpd256_pd128(_t29_157)));

  // AVX Storer:
  _t29_16 = _t29_158;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_159 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_11, _t29_11, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_160 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_161 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t29_162 = _t29_161;

  // 4-BLAC: 1x4 * 4x1
  _t29_163 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_160, _t29_162), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_160, _t29_162), _mm256_mul_pd(_t29_160, _t29_162), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_160, _t29_162), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_160, _t29_162), _mm256_mul_pd(_t29_160, _t29_162), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_160, _t29_162), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_160, _t29_162), _mm256_mul_pd(_t29_160, _t29_162), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_164 = _mm256_sub_pd(_t29_159, _t29_163);

  // AVX Storer:
  _t29_17 = _t29_164;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_165 = _t29_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_166 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_167 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t29_168 = _mm256_add_pd(_t29_166, _t29_167);

  // 4-BLAC: 1x4 / 1x4
  _t29_169 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_165), _mm256_castpd256_pd128(_t29_168)));

  // AVX Storer:
  _t29_17 = _t29_169;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, 0)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t29_170 = _t29_12;
  _t29_171 = _t29_13;
  _t29_172 = _mm256_setzero_pd();
  _t29_173 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t29_174 = _t29_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t29_175 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_174, _t29_174, 32), _mm256_permute2f128_pd(_t29_174, _t29_174, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32));
  _t29_176 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_174, _t29_174, 32), _mm256_permute2f128_pd(_t29_174, _t29_174, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32));
  _t29_177 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_174, _t29_174, 49), _mm256_permute2f128_pd(_t29_174, _t29_174, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32));
  _t29_178 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_174, _t29_174, 49), _mm256_permute2f128_pd(_t29_174, _t29_174, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32));

  // 4-BLAC: 4x4 - 4x4
  _t29_179 = _mm256_sub_pd(_t29_170, _t29_175);
  _t29_180 = _mm256_sub_pd(_t29_171, _t29_176);
  _t29_181 = _mm256_sub_pd(_t29_172, _t29_177);
  _t29_182 = _mm256_sub_pd(_t29_173, _t29_178);

  // AVX Storer:
  _t29_12 = _t29_179;
  _t29_13 = _t29_180;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_183 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_12, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_184 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_185 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t29_186 = _mm256_add_pd(_t29_184, _t29_185);

  // 4-BLAC: 1x4 / 1x4
  _t29_187 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_183), _mm256_castpd256_pd128(_t29_186)));

  // AVX Storer:
  _t29_18 = _t29_187;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_188 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_12, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_189 = _t29_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_190 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t29_191 = _t29_190;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_192 = _mm256_mul_pd(_t29_189, _t29_191);

  // 4-BLAC: 1x4 - 1x4
  _t29_193 = _mm256_sub_pd(_t29_188, _t29_192);

  // AVX Storer:
  _t29_19 = _t29_193;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_194 = _t29_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_195 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_196 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t29_197 = _mm256_add_pd(_t29_195, _t29_196);

  // 4-BLAC: 1x4 / 1x4
  _t29_198 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_194), _mm256_castpd256_pd128(_t29_197)));

  // AVX Storer:
  _t29_19 = _t29_198;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_199 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_12, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_12, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_200 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_201 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t29_202 = _t29_201;

  // 4-BLAC: 1x4 * 4x1
  _t29_203 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_200, _t29_202), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_200, _t29_202), _mm256_mul_pd(_t29_200, _t29_202), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_200, _t29_202), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_200, _t29_202), _mm256_mul_pd(_t29_200, _t29_202), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_200, _t29_202), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_200, _t29_202), _mm256_mul_pd(_t29_200, _t29_202), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_204 = _mm256_sub_pd(_t29_199, _t29_203);

  // AVX Storer:
  _t29_20 = _t29_204;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_205 = _t29_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_206 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_207 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_208 = _mm256_add_pd(_t29_206, _t29_207);

  // 4-BLAC: 1x4 / 1x4
  _t29_209 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_205), _mm256_castpd256_pd128(_t29_208)));

  // AVX Storer:
  _t29_20 = _t29_209;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_210 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_12, _t29_12, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_211 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_unpacklo_pd(_t29_20, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_212 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t29_213 = _t29_212;

  // 4-BLAC: 1x4 * 4x1
  _t29_214 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_211, _t29_213), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_211, _t29_213), _mm256_mul_pd(_t29_211, _t29_213), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_211, _t29_213), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_211, _t29_213), _mm256_mul_pd(_t29_211, _t29_213), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_211, _t29_213), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_211, _t29_213), _mm256_mul_pd(_t29_211, _t29_213), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_215 = _mm256_sub_pd(_t29_210, _t29_214);

  // AVX Storer:
  _t29_21 = _t29_215;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_216 = _t29_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_217 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_218 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t29_219 = _mm256_add_pd(_t29_217, _t29_218);

  // 4-BLAC: 1x4 / 1x4
  _t29_220 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_216), _mm256_castpd256_pd128(_t29_219)));

  // AVX Storer:
  _t29_21 = _t29_220;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, 0)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, 0)) ) ),h(4, 28, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_221 = _t29_1;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t29_58 = _mm256_mul_pd(_t29_221, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_unpacklo_pd(_t29_20, _t29_21), 32));

  // 4-BLAC: 1x4 - 1x4
  _t29_13 = _mm256_sub_pd(_t29_13, _t29_58);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 0)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 0), L[28,28],h(1, 28, 0)) ) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_222 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_223 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_224 = _t0_12;

  // 4-BLAC: 1x4 + 1x4
  _t29_225 = _mm256_add_pd(_t29_223, _t29_224);

  // 4-BLAC: 1x4 / 1x4
  _t29_226 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_222), _mm256_castpd256_pd128(_t29_225)));

  // AVX Storer:
  _t29_22 = _t29_226;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 1)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 0)) Kro T( G(h(1, 28, 1), L[28,28],h(1, 28, 0)) ) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_227 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_228 = _t29_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_229 = _t0_8;

  // 4-BLAC: (4x1)^T
  _t29_230 = _t29_229;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_231 = _mm256_mul_pd(_t29_228, _t29_230);

  // 4-BLAC: 1x4 - 1x4
  _t29_232 = _mm256_sub_pd(_t29_227, _t29_231);

  // AVX Storer:
  _t29_23 = _t29_232;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 1)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 1), L[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_233 = _t29_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_234 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_235 = _t0_10;

  // 4-BLAC: 1x4 + 1x4
  _t29_236 = _mm256_add_pd(_t29_234, _t29_235);

  // 4-BLAC: 1x4 / 1x4
  _t29_237 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_233), _mm256_castpd256_pd128(_t29_236)));

  // AVX Storer:
  _t29_23 = _t29_237;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 2)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 0)) * T( G(h(1, 28, 2), L[28,28],h(2, 28, 0)) ) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_238 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_13, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_13, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_239 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_22, _t29_23), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_240 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t29_241 = _t29_240;

  // 4-BLAC: 1x4 * 4x1
  _t29_242 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_239, _t29_241), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_239, _t29_241), _mm256_mul_pd(_t29_239, _t29_241), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_239, _t29_241), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_239, _t29_241), _mm256_mul_pd(_t29_239, _t29_241), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_239, _t29_241), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_239, _t29_241), _mm256_mul_pd(_t29_239, _t29_241), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_243 = _mm256_sub_pd(_t29_238, _t29_242);

  // AVX Storer:
  _t29_24 = _t29_243;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 2)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 2), L[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_244 = _t29_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_245 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_246 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_247 = _mm256_add_pd(_t29_245, _t29_246);

  // 4-BLAC: 1x4 / 1x4
  _t29_248 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_244), _mm256_castpd256_pd128(_t29_247)));

  // AVX Storer:
  _t29_24 = _t29_248;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 3)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, 0)) * T( G(h(1, 28, 3), L[28,28],h(3, 28, 0)) ) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_249 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_13, _t29_13, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_250 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_22, _t29_23), _mm256_unpacklo_pd(_t29_24, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_251 = _t0_0;

  // 4-BLAC: (1x4)^T
  _t29_252 = _t29_251;

  // 4-BLAC: 1x4 * 4x1
  _t29_253 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_250, _t29_252), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_250, _t29_252), _mm256_mul_pd(_t29_250, _t29_252), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_250, _t29_252), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_250, _t29_252), _mm256_mul_pd(_t29_250, _t29_252), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_250, _t29_252), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_250, _t29_252), _mm256_mul_pd(_t29_250, _t29_252), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_254 = _mm256_sub_pd(_t29_249, _t29_253);

  // AVX Storer:
  _t29_25 = _t29_254;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 3)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 3), L[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_255 = _t29_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_256 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_257 = _t0_1;

  // 4-BLAC: 1x4 + 1x4
  _t29_258 = _mm256_add_pd(_t29_256, _t29_257);

  // 4-BLAC: 1x4 / 1x4
  _t29_259 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_255), _mm256_castpd256_pd128(_t29_258)));

  // AVX Storer:
  _t29_25 = _t29_259;

  // Generating : X[28,28] = S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, 4)) - ( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 4), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t29_618 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 32);
  _t29_619 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 32);
  _t29_620 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_9, _t4_8), _mm256_unpacklo_pd(_t4_7, _t4_6), 49);
  _t29_621 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_9, _t4_8), _mm256_unpackhi_pd(_t4_7, _t4_6), 49);

  // 4-BLAC: 4x4 * 4x4
  _t29_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 32), _mm256_permute2f128_pd(_t29_7, _t29_7, 32), 0), _t29_618), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_8, _t29_8, 32), _mm256_permute2f128_pd(_t29_8, _t29_8, 32), 0), _t29_619)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_9, _t29_9, 32), _mm256_permute2f128_pd(_t29_9, _t29_9, 32), 0), _t29_620), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_10, _t29_10, 32), _mm256_permute2f128_pd(_t29_10, _t29_10, 32), 0), _t29_621)));
  _t29_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_14, _t29_14, 32), _mm256_permute2f128_pd(_t29_14, _t29_14, 32), 0), _t29_618), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_15, _t29_15, 32), _mm256_permute2f128_pd(_t29_15, _t29_15, 32), 0), _t29_619)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_16, _t29_16, 32), _mm256_permute2f128_pd(_t29_16, _t29_16, 32), 0), _t29_620), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_17, _t29_17, 32), _mm256_permute2f128_pd(_t29_17, _t29_17, 32), 0), _t29_621)));
  _t29_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_18, _t29_18, 32), _mm256_permute2f128_pd(_t29_18, _t29_18, 32), 0), _t29_618), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_19, _t29_19, 32), _mm256_permute2f128_pd(_t29_19, _t29_19, 32), 0), _t29_619)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_20, _t29_20, 32), _mm256_permute2f128_pd(_t29_20, _t29_20, 32), 0), _t29_620), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_21, _t29_21, 32), _mm256_permute2f128_pd(_t29_21, _t29_21, 32), 0), _t29_621)));
  _t29_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_22, _t29_22, 32), _mm256_permute2f128_pd(_t29_22, _t29_22, 32), 0), _t29_618), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_23, _t29_23, 32), _mm256_permute2f128_pd(_t29_23, _t29_23, 32), 0), _t29_619)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_24, _t29_24, 32), _mm256_permute2f128_pd(_t29_24, _t29_24, 32), 0), _t29_620), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_25, _t29_25, 32), _mm256_permute2f128_pd(_t29_25, _t29_25, 32), 0), _t29_621)));

  // 4-BLAC: 4x4 - 4x4
  _t29_73 = _mm256_sub_pd(_t29_73, _t29_61);
  _t29_74 = _mm256_sub_pd(_t29_74, _t29_62);
  _t29_75 = _mm256_sub_pd(_t29_75, _t29_63);
  _t29_76 = _mm256_sub_pd(_t29_76, _t29_64);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_260 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_73, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_261 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_262 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_263 = _mm256_add_pd(_t29_261, _t29_262);

  // 4-BLAC: 1x4 / 1x4
  _t29_264 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_260), _mm256_castpd256_pd128(_t29_263)));

  // AVX Storer:
  _t29_26 = _t29_264;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_265 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_73, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_266 = _t29_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_267 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t29_268 = _t29_267;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_269 = _mm256_mul_pd(_t29_266, _t29_268);

  // 4-BLAC: 1x4 - 1x4
  _t29_270 = _mm256_sub_pd(_t29_265, _t29_269);

  // AVX Storer:
  _t29_27 = _t29_270;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_271 = _t29_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_272 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_273 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_274 = _mm256_add_pd(_t29_272, _t29_273);

  // 4-BLAC: 1x4 / 1x4
  _t29_275 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_271), _mm256_castpd256_pd128(_t29_274)));

  // AVX Storer:
  _t29_27 = _t29_275;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_276 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_73, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_73, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_277 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_26, _t29_27), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_278 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t29_279 = _t29_278;

  // 4-BLAC: 1x4 * 4x1
  _t29_280 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_277, _t29_279), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_277, _t29_279), _mm256_mul_pd(_t29_277, _t29_279), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_277, _t29_279), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_277, _t29_279), _mm256_mul_pd(_t29_277, _t29_279), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_277, _t29_279), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_277, _t29_279), _mm256_mul_pd(_t29_277, _t29_279), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_281 = _mm256_sub_pd(_t29_276, _t29_280);

  // AVX Storer:
  _t29_28 = _t29_281;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_282 = _t29_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_283 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_284 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_285 = _mm256_add_pd(_t29_283, _t29_284);

  // 4-BLAC: 1x4 / 1x4
  _t29_286 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_282), _mm256_castpd256_pd128(_t29_285)));

  // AVX Storer:
  _t29_28 = _t29_286;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_287 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_73, _t29_73, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_288 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_26, _t29_27), _mm256_unpacklo_pd(_t29_28, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_289 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t29_290 = _t29_289;

  // 4-BLAC: 1x4 * 4x1
  _t29_291 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_288, _t29_290), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_288, _t29_290), _mm256_mul_pd(_t29_288, _t29_290), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_288, _t29_290), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_288, _t29_290), _mm256_mul_pd(_t29_288, _t29_290), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_288, _t29_290), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_288, _t29_290), _mm256_mul_pd(_t29_288, _t29_290), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_292 = _mm256_sub_pd(_t29_287, _t29_291);

  // AVX Storer:
  _t29_29 = _t29_292;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_293 = _t29_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_294 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_295 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_296 = _mm256_add_pd(_t29_294, _t29_295);

  // 4-BLAC: 1x4 / 1x4
  _t29_297 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_293), _mm256_castpd256_pd128(_t29_296)));

  // AVX Storer:
  _t29_29 = _t29_297;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, 4)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t29_298 = _t29_74;
  _t29_299 = _t29_75;
  _t29_300 = _t29_76;
  _t29_301 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t29_302 = _t29_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t29_303 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_302, _t29_302, 32), _mm256_permute2f128_pd(_t29_302, _t29_302, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_26, _t29_27), _mm256_unpacklo_pd(_t29_28, _t29_29), 32));
  _t29_304 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_302, _t29_302, 32), _mm256_permute2f128_pd(_t29_302, _t29_302, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_26, _t29_27), _mm256_unpacklo_pd(_t29_28, _t29_29), 32));
  _t29_305 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_302, _t29_302, 49), _mm256_permute2f128_pd(_t29_302, _t29_302, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_26, _t29_27), _mm256_unpacklo_pd(_t29_28, _t29_29), 32));
  _t29_306 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_302, _t29_302, 49), _mm256_permute2f128_pd(_t29_302, _t29_302, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_26, _t29_27), _mm256_unpacklo_pd(_t29_28, _t29_29), 32));

  // 4-BLAC: 4x4 - 4x4
  _t29_307 = _mm256_sub_pd(_t29_298, _t29_303);
  _t29_308 = _mm256_sub_pd(_t29_299, _t29_304);
  _t29_309 = _mm256_sub_pd(_t29_300, _t29_305);
  _t29_310 = _mm256_sub_pd(_t29_301, _t29_306);

  // AVX Storer:
  _t29_74 = _t29_307;
  _t29_75 = _t29_308;
  _t29_76 = _t29_309;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_311 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_74, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_312 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_313 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_314 = _mm256_add_pd(_t29_312, _t29_313);

  // 4-BLAC: 1x4 / 1x4
  _t29_315 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_311), _mm256_castpd256_pd128(_t29_314)));

  // AVX Storer:
  _t29_30 = _t29_315;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_316 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_74, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_317 = _t29_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_318 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t29_319 = _t29_318;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_320 = _mm256_mul_pd(_t29_317, _t29_319);

  // 4-BLAC: 1x4 - 1x4
  _t29_321 = _mm256_sub_pd(_t29_316, _t29_320);

  // AVX Storer:
  _t29_31 = _t29_321;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_322 = _t29_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_323 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_324 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_325 = _mm256_add_pd(_t29_323, _t29_324);

  // 4-BLAC: 1x4 / 1x4
  _t29_326 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_322), _mm256_castpd256_pd128(_t29_325)));

  // AVX Storer:
  _t29_31 = _t29_326;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_327 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_74, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_74, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_328 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_30, _t29_31), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_329 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t29_330 = _t29_329;

  // 4-BLAC: 1x4 * 4x1
  _t29_331 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_328, _t29_330), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_328, _t29_330), _mm256_mul_pd(_t29_328, _t29_330), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_328, _t29_330), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_328, _t29_330), _mm256_mul_pd(_t29_328, _t29_330), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_328, _t29_330), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_328, _t29_330), _mm256_mul_pd(_t29_328, _t29_330), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_332 = _mm256_sub_pd(_t29_327, _t29_331);

  // AVX Storer:
  _t29_32 = _t29_332;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_333 = _t29_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_334 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_335 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_336 = _mm256_add_pd(_t29_334, _t29_335);

  // 4-BLAC: 1x4 / 1x4
  _t29_337 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_333), _mm256_castpd256_pd128(_t29_336)));

  // AVX Storer:
  _t29_32 = _t29_337;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_338 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_74, _t29_74, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_339 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_30, _t29_31), _mm256_unpacklo_pd(_t29_32, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_340 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t29_341 = _t29_340;

  // 4-BLAC: 1x4 * 4x1
  _t29_342 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_339, _t29_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_339, _t29_341), _mm256_mul_pd(_t29_339, _t29_341), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_339, _t29_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_339, _t29_341), _mm256_mul_pd(_t29_339, _t29_341), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_339, _t29_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_339, _t29_341), _mm256_mul_pd(_t29_339, _t29_341), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_343 = _mm256_sub_pd(_t29_338, _t29_342);

  // AVX Storer:
  _t29_33 = _t29_343;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_344 = _t29_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_345 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_346 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_347 = _mm256_add_pd(_t29_345, _t29_346);

  // 4-BLAC: 1x4 / 1x4
  _t29_348 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_344), _mm256_castpd256_pd128(_t29_347)));

  // AVX Storer:
  _t29_33 = _t29_348;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, 4)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t29_349 = _t29_75;
  _t29_350 = _t29_76;
  _t29_351 = _mm256_setzero_pd();
  _t29_352 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t29_353 = _t29_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t29_354 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_353, _t29_353, 32), _mm256_permute2f128_pd(_t29_353, _t29_353, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_30, _t29_31), _mm256_unpacklo_pd(_t29_32, _t29_33), 32));
  _t29_355 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_353, _t29_353, 32), _mm256_permute2f128_pd(_t29_353, _t29_353, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_30, _t29_31), _mm256_unpacklo_pd(_t29_32, _t29_33), 32));
  _t29_356 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_353, _t29_353, 49), _mm256_permute2f128_pd(_t29_353, _t29_353, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_30, _t29_31), _mm256_unpacklo_pd(_t29_32, _t29_33), 32));
  _t29_357 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_353, _t29_353, 49), _mm256_permute2f128_pd(_t29_353, _t29_353, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_30, _t29_31), _mm256_unpacklo_pd(_t29_32, _t29_33), 32));

  // 4-BLAC: 4x4 - 4x4
  _t29_358 = _mm256_sub_pd(_t29_349, _t29_354);
  _t29_359 = _mm256_sub_pd(_t29_350, _t29_355);
  _t29_360 = _mm256_sub_pd(_t29_351, _t29_356);
  _t29_361 = _mm256_sub_pd(_t29_352, _t29_357);

  // AVX Storer:
  _t29_75 = _t29_358;
  _t29_76 = _t29_359;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_362 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_75, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_363 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_364 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_365 = _mm256_add_pd(_t29_363, _t29_364);

  // 4-BLAC: 1x4 / 1x4
  _t29_366 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_362), _mm256_castpd256_pd128(_t29_365)));

  // AVX Storer:
  _t29_34 = _t29_366;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_367 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_75, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_368 = _t29_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_369 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t29_370 = _t29_369;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_371 = _mm256_mul_pd(_t29_368, _t29_370);

  // 4-BLAC: 1x4 - 1x4
  _t29_372 = _mm256_sub_pd(_t29_367, _t29_371);

  // AVX Storer:
  _t29_35 = _t29_372;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_373 = _t29_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_374 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_375 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_376 = _mm256_add_pd(_t29_374, _t29_375);

  // 4-BLAC: 1x4 / 1x4
  _t29_377 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_373), _mm256_castpd256_pd128(_t29_376)));

  // AVX Storer:
  _t29_35 = _t29_377;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_378 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_75, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_75, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_379 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_34, _t29_35), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_380 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t29_381 = _t29_380;

  // 4-BLAC: 1x4 * 4x1
  _t29_382 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_379, _t29_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_379, _t29_381), _mm256_mul_pd(_t29_379, _t29_381), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_379, _t29_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_379, _t29_381), _mm256_mul_pd(_t29_379, _t29_381), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_379, _t29_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_379, _t29_381), _mm256_mul_pd(_t29_379, _t29_381), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_383 = _mm256_sub_pd(_t29_378, _t29_382);

  // AVX Storer:
  _t29_36 = _t29_383;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_384 = _t29_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_385 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_386 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_387 = _mm256_add_pd(_t29_385, _t29_386);

  // 4-BLAC: 1x4 / 1x4
  _t29_388 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_384), _mm256_castpd256_pd128(_t29_387)));

  // AVX Storer:
  _t29_36 = _t29_388;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_389 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_75, _t29_75, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_390 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_34, _t29_35), _mm256_unpacklo_pd(_t29_36, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_391 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t29_392 = _t29_391;

  // 4-BLAC: 1x4 * 4x1
  _t29_393 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_390, _t29_392), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_390, _t29_392), _mm256_mul_pd(_t29_390, _t29_392), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_390, _t29_392), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_390, _t29_392), _mm256_mul_pd(_t29_390, _t29_392), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_390, _t29_392), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_390, _t29_392), _mm256_mul_pd(_t29_390, _t29_392), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_394 = _mm256_sub_pd(_t29_389, _t29_393);

  // AVX Storer:
  _t29_37 = _t29_394;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_395 = _t29_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_396 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_397 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_398 = _mm256_add_pd(_t29_396, _t29_397);

  // 4-BLAC: 1x4 / 1x4
  _t29_399 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_395), _mm256_castpd256_pd128(_t29_398)));

  // AVX Storer:
  _t29_37 = _t29_399;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, 4)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_400 = _t29_1;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t29_59 = _mm256_mul_pd(_t29_400, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_34, _t29_35), _mm256_unpacklo_pd(_t29_36, _t29_37), 32));

  // 4-BLAC: 1x4 - 1x4
  _t29_76 = _mm256_sub_pd(_t29_76, _t29_59);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 4)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 4), L[28,28],h(1, 28, 4)) ) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_401 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_76, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_402 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_403 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_404 = _mm256_add_pd(_t29_402, _t29_403);

  // 4-BLAC: 1x4 / 1x4
  _t29_405 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_401), _mm256_castpd256_pd128(_t29_404)));

  // AVX Storer:
  _t29_38 = _t29_405;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 5)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 4)) Kro T( G(h(1, 28, 5), L[28,28],h(1, 28, 4)) ) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_406 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_76, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_407 = _t29_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_408 = _t4_5;

  // 4-BLAC: (4x1)^T
  _t29_409 = _t29_408;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_410 = _mm256_mul_pd(_t29_407, _t29_409);

  // 4-BLAC: 1x4 - 1x4
  _t29_411 = _mm256_sub_pd(_t29_406, _t29_410);

  // AVX Storer:
  _t29_39 = _t29_411;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 5)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 5), L[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_412 = _t29_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_413 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_414 = _t2_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_415 = _mm256_add_pd(_t29_413, _t29_414);

  // 4-BLAC: 1x4 / 1x4
  _t29_416 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_412), _mm256_castpd256_pd128(_t29_415)));

  // AVX Storer:
  _t29_39 = _t29_416;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 6)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 4)) * T( G(h(1, 28, 6), L[28,28],h(2, 28, 4)) ) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_417 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_76, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_76, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_418 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_38, _t29_39), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_419 = _t4_3;

  // 4-BLAC: (1x4)^T
  _t29_420 = _t29_419;

  // 4-BLAC: 1x4 * 4x1
  _t29_421 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_418, _t29_420), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_418, _t29_420), _mm256_mul_pd(_t29_418, _t29_420), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_418, _t29_420), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_418, _t29_420), _mm256_mul_pd(_t29_418, _t29_420), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_418, _t29_420), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_418, _t29_420), _mm256_mul_pd(_t29_418, _t29_420), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_422 = _mm256_sub_pd(_t29_417, _t29_421);

  // AVX Storer:
  _t29_40 = _t29_422;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 6)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 6), L[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_423 = _t29_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_424 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_425 = _t2_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_426 = _mm256_add_pd(_t29_424, _t29_425);

  // 4-BLAC: 1x4 / 1x4
  _t29_427 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_423), _mm256_castpd256_pd128(_t29_426)));

  // AVX Storer:
  _t29_40 = _t29_427;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 7)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, 4)) * T( G(h(1, 28, 7), L[28,28],h(3, 28, 4)) ) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_428 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_76, _t29_76, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_429 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_38, _t29_39), _mm256_unpacklo_pd(_t29_40, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_430 = _t4_0;

  // 4-BLAC: (1x4)^T
  _t29_431 = _t29_430;

  // 4-BLAC: 1x4 * 4x1
  _t29_432 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_429, _t29_431), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_429, _t29_431), _mm256_mul_pd(_t29_429, _t29_431), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_429, _t29_431), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_429, _t29_431), _mm256_mul_pd(_t29_429, _t29_431), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_429, _t29_431), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_429, _t29_431), _mm256_mul_pd(_t29_429, _t29_431), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_433 = _mm256_sub_pd(_t29_428, _t29_432);

  // AVX Storer:
  _t29_41 = _t29_433;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 7)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 7), L[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_434 = _t29_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_435 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_436 = _t2_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_437 = _mm256_add_pd(_t29_435, _t29_436);

  // 4-BLAC: 1x4 / 1x4
  _t29_438 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_434), _mm256_castpd256_pd128(_t29_437)));

  // AVX Storer:
  _t29_41 = _t29_438;

  // Generating : X[28,28] = ( S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, fi1812)) - ( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi1812), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, fi1812)) + Sum_{i100} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi1812), L[28,28],h(4, 28, i100)) ) ),h(4, 28, fi1812)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t29_622 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_29, _t10_28), _mm256_unpacklo_pd(_t10_27, _t10_26), 32);
  _t29_623 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_29, _t10_28), _mm256_unpackhi_pd(_t10_27, _t10_26), 32);
  _t29_624 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_29, _t10_28), _mm256_unpacklo_pd(_t10_27, _t10_26), 49);
  _t29_625 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_29, _t10_28), _mm256_unpackhi_pd(_t10_27, _t10_26), 49);

  // 4-BLAC: 4x4 * 4x4
  _t29_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 32), _mm256_permute2f128_pd(_t29_7, _t29_7, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_8, _t29_8, 32), _mm256_permute2f128_pd(_t29_8, _t29_8, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_9, _t29_9, 32), _mm256_permute2f128_pd(_t29_9, _t29_9, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_10, _t29_10, 32), _mm256_permute2f128_pd(_t29_10, _t29_10, 32), 0), _t29_625)));
  _t29_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_14, _t29_14, 32), _mm256_permute2f128_pd(_t29_14, _t29_14, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_15, _t29_15, 32), _mm256_permute2f128_pd(_t29_15, _t29_15, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_16, _t29_16, 32), _mm256_permute2f128_pd(_t29_16, _t29_16, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_17, _t29_17, 32), _mm256_permute2f128_pd(_t29_17, _t29_17, 32), 0), _t29_625)));
  _t29_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_18, _t29_18, 32), _mm256_permute2f128_pd(_t29_18, _t29_18, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_19, _t29_19, 32), _mm256_permute2f128_pd(_t29_19, _t29_19, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_20, _t29_20, 32), _mm256_permute2f128_pd(_t29_20, _t29_20, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_21, _t29_21, 32), _mm256_permute2f128_pd(_t29_21, _t29_21, 32), 0), _t29_625)));
  _t29_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_22, _t29_22, 32), _mm256_permute2f128_pd(_t29_22, _t29_22, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_23, _t29_23, 32), _mm256_permute2f128_pd(_t29_23, _t29_23, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_24, _t29_24, 32), _mm256_permute2f128_pd(_t29_24, _t29_24, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_25, _t29_25, 32), _mm256_permute2f128_pd(_t29_25, _t29_25, 32), 0), _t29_625)));

  // 4-BLAC: 4x4 - 4x4
  _t29_77 = _mm256_sub_pd(_t29_77, _t29_65);
  _t29_78 = _mm256_sub_pd(_t29_78, _t29_66);
  _t29_79 = _mm256_sub_pd(_t29_79, _t29_67);
  _t29_80 = _mm256_sub_pd(_t29_80, _t29_68);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t29_626 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_25, _t10_24), _mm256_unpacklo_pd(_t10_23, _t10_22), 32);
  _t29_627 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_25, _t10_24), _mm256_unpackhi_pd(_t10_23, _t10_22), 32);
  _t29_628 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_25, _t10_24), _mm256_unpacklo_pd(_t10_23, _t10_22), 49);
  _t29_629 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_25, _t10_24), _mm256_unpackhi_pd(_t10_23, _t10_22), 49);

  // 4-BLAC: 4x4 * 4x4
  _t29_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_26, _t29_26, 32), _mm256_permute2f128_pd(_t29_26, _t29_26, 32), 0), _t29_626), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_27, _t29_27, 32), _mm256_permute2f128_pd(_t29_27, _t29_27, 32), 0), _t29_627)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_28, _t29_28, 32), _mm256_permute2f128_pd(_t29_28, _t29_28, 32), 0), _t29_628), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_29, _t29_29, 32), _mm256_permute2f128_pd(_t29_29, _t29_29, 32), 0), _t29_629)));
  _t29_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_30, _t29_30, 32), _mm256_permute2f128_pd(_t29_30, _t29_30, 32), 0), _t29_626), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_31, _t29_31, 32), _mm256_permute2f128_pd(_t29_31, _t29_31, 32), 0), _t29_627)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_32, _t29_32, 32), _mm256_permute2f128_pd(_t29_32, _t29_32, 32), 0), _t29_628), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_33, _t29_33, 32), _mm256_permute2f128_pd(_t29_33, _t29_33, 32), 0), _t29_629)));
  _t29_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_34, _t29_34, 32), _mm256_permute2f128_pd(_t29_34, _t29_34, 32), 0), _t29_626), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_35, _t29_35, 32), _mm256_permute2f128_pd(_t29_35, _t29_35, 32), 0), _t29_627)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_36, _t29_36, 32), _mm256_permute2f128_pd(_t29_36, _t29_36, 32), 0), _t29_628), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_37, _t29_37, 32), _mm256_permute2f128_pd(_t29_37, _t29_37, 32), 0), _t29_629)));
  _t29_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_38, _t29_38, 32), _mm256_permute2f128_pd(_t29_38, _t29_38, 32), 0), _t29_626), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_39, _t29_39, 32), _mm256_permute2f128_pd(_t29_39, _t29_39, 32), 0), _t29_627)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_40, _t29_40, 32), _mm256_permute2f128_pd(_t29_40, _t29_40, 32), 0), _t29_628), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_41, _t29_41, 32), _mm256_permute2f128_pd(_t29_41, _t29_41, 32), 0), _t29_629)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t29_77 = _mm256_sub_pd(_t29_77, _t29_69);
  _t29_78 = _mm256_sub_pd(_t29_78, _t29_70);
  _t29_79 = _mm256_sub_pd(_t29_79, _t29_71);
  _t29_80 = _mm256_sub_pd(_t29_80, _t29_72);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_439 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_77, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_440 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_441 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_442 = _mm256_add_pd(_t29_440, _t29_441);

  // 4-BLAC: 1x4 / 1x4
  _t29_443 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_439), _mm256_castpd256_pd128(_t29_442)));

  // AVX Storer:
  _t29_42 = _t29_443;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_444 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_77, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_445 = _t29_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_446 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t29_447 = _t29_446;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_448 = _mm256_mul_pd(_t29_445, _t29_447);

  // 4-BLAC: 1x4 - 1x4
  _t29_449 = _mm256_sub_pd(_t29_444, _t29_448);

  // AVX Storer:
  _t29_43 = _t29_449;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_450 = _t29_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_451 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_452 = _t8_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_453 = _mm256_add_pd(_t29_451, _t29_452);

  // 4-BLAC: 1x4 / 1x4
  _t29_454 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_450), _mm256_castpd256_pd128(_t29_453)));

  // AVX Storer:
  _t29_43 = _t29_454;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_455 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_77, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_77, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_456 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_42, _t29_43), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_457 = _t10_3;

  // 4-BLAC: (1x4)^T
  _t29_458 = _t29_457;

  // 4-BLAC: 1x4 * 4x1
  _t29_459 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_456, _t29_458), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_456, _t29_458), _mm256_mul_pd(_t29_456, _t29_458), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_456, _t29_458), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_456, _t29_458), _mm256_mul_pd(_t29_456, _t29_458), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_456, _t29_458), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_456, _t29_458), _mm256_mul_pd(_t29_456, _t29_458), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_460 = _mm256_sub_pd(_t29_455, _t29_459);

  // AVX Storer:
  _t29_44 = _t29_460;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_461 = _t29_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_462 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_463 = _t8_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_464 = _mm256_add_pd(_t29_462, _t29_463);

  // 4-BLAC: 1x4 / 1x4
  _t29_465 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_461), _mm256_castpd256_pd128(_t29_464)));

  // AVX Storer:
  _t29_44 = _t29_465;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_466 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_77, _t29_77, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_467 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_42, _t29_43), _mm256_unpacklo_pd(_t29_44, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_468 = _t10_0;

  // 4-BLAC: (1x4)^T
  _t29_469 = _t29_468;

  // 4-BLAC: 1x4 * 4x1
  _t29_470 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_467, _t29_469), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_467, _t29_469), _mm256_mul_pd(_t29_467, _t29_469), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_467, _t29_469), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_467, _t29_469), _mm256_mul_pd(_t29_467, _t29_469), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_467, _t29_469), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_467, _t29_469), _mm256_mul_pd(_t29_467, _t29_469), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_471 = _mm256_sub_pd(_t29_466, _t29_470);

  // AVX Storer:
  _t29_45 = _t29_471;

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_472 = _t29_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_473 = _t29_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_474 = _t8_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_475 = _mm256_add_pd(_t29_473, _t29_474);

  // 4-BLAC: 1x4 / 1x4
  _t29_476 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_472), _mm256_castpd256_pd128(_t29_475)));

  // AVX Storer:
  _t29_45 = _t29_476;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, fi1812)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812))

  // AVX Loader:

  // 3x4 -> 4x4
  _t29_477 = _t29_78;
  _t29_478 = _t29_79;
  _t29_479 = _t29_80;
  _t29_480 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t29_481 = _t29_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t29_482 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_481, _t29_481, 32), _mm256_permute2f128_pd(_t29_481, _t29_481, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_42, _t29_43), _mm256_unpacklo_pd(_t29_44, _t29_45), 32));
  _t29_483 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_481, _t29_481, 32), _mm256_permute2f128_pd(_t29_481, _t29_481, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_42, _t29_43), _mm256_unpacklo_pd(_t29_44, _t29_45), 32));
  _t29_484 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_481, _t29_481, 49), _mm256_permute2f128_pd(_t29_481, _t29_481, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_42, _t29_43), _mm256_unpacklo_pd(_t29_44, _t29_45), 32));
  _t29_485 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_481, _t29_481, 49), _mm256_permute2f128_pd(_t29_481, _t29_481, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_42, _t29_43), _mm256_unpacklo_pd(_t29_44, _t29_45), 32));

  // 4-BLAC: 4x4 - 4x4
  _t29_486 = _mm256_sub_pd(_t29_477, _t29_482);
  _t29_487 = _mm256_sub_pd(_t29_478, _t29_483);
  _t29_488 = _mm256_sub_pd(_t29_479, _t29_484);
  _t29_489 = _mm256_sub_pd(_t29_480, _t29_485);

  // AVX Storer:
  _t29_78 = _t29_486;
  _t29_79 = _t29_487;
  _t29_80 = _t29_488;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_490 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_78, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_491 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_492 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_493 = _mm256_add_pd(_t29_491, _t29_492);

  // 4-BLAC: 1x4 / 1x4
  _t29_494 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_490), _mm256_castpd256_pd128(_t29_493)));

  // AVX Storer:
  _t29_46 = _t29_494;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_495 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_78, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_496 = _t29_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_497 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t29_498 = _t29_497;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_499 = _mm256_mul_pd(_t29_496, _t29_498);

  // 4-BLAC: 1x4 - 1x4
  _t29_500 = _mm256_sub_pd(_t29_495, _t29_499);

  // AVX Storer:
  _t29_47 = _t29_500;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_501 = _t29_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_502 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_503 = _t8_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_504 = _mm256_add_pd(_t29_502, _t29_503);

  // 4-BLAC: 1x4 / 1x4
  _t29_505 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_501), _mm256_castpd256_pd128(_t29_504)));

  // AVX Storer:
  _t29_47 = _t29_505;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_506 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_78, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_78, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_507 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_46, _t29_47), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_508 = _t10_3;

  // 4-BLAC: (1x4)^T
  _t29_509 = _t29_508;

  // 4-BLAC: 1x4 * 4x1
  _t29_510 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_507, _t29_509), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_507, _t29_509), _mm256_mul_pd(_t29_507, _t29_509), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_507, _t29_509), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_507, _t29_509), _mm256_mul_pd(_t29_507, _t29_509), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_507, _t29_509), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_507, _t29_509), _mm256_mul_pd(_t29_507, _t29_509), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_511 = _mm256_sub_pd(_t29_506, _t29_510);

  // AVX Storer:
  _t29_48 = _t29_511;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_512 = _t29_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_513 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_514 = _t8_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_515 = _mm256_add_pd(_t29_513, _t29_514);

  // 4-BLAC: 1x4 / 1x4
  _t29_516 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_512), _mm256_castpd256_pd128(_t29_515)));

  // AVX Storer:
  _t29_48 = _t29_516;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_517 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_78, _t29_78, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_518 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_46, _t29_47), _mm256_unpacklo_pd(_t29_48, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_519 = _t10_0;

  // 4-BLAC: (1x4)^T
  _t29_520 = _t29_519;

  // 4-BLAC: 1x4 * 4x1
  _t29_521 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_518, _t29_520), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_518, _t29_520), _mm256_mul_pd(_t29_518, _t29_520), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_518, _t29_520), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_518, _t29_520), _mm256_mul_pd(_t29_518, _t29_520), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_518, _t29_520), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_518, _t29_520), _mm256_mul_pd(_t29_518, _t29_520), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_522 = _mm256_sub_pd(_t29_517, _t29_521);

  // AVX Storer:
  _t29_49 = _t29_522;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_523 = _t29_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_524 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_525 = _t8_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_526 = _mm256_add_pd(_t29_524, _t29_525);

  // 4-BLAC: 1x4 / 1x4
  _t29_527 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_523), _mm256_castpd256_pd128(_t29_526)));

  // AVX Storer:
  _t29_49 = _t29_527;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, fi1812)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812))

  // AVX Loader:

  // 2x4 -> 4x4
  _t29_528 = _t29_79;
  _t29_529 = _t29_80;
  _t29_530 = _mm256_setzero_pd();
  _t29_531 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t29_532 = _t29_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t29_533 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_532, _t29_532, 32), _mm256_permute2f128_pd(_t29_532, _t29_532, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_46, _t29_47), _mm256_unpacklo_pd(_t29_48, _t29_49), 32));
  _t29_534 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_532, _t29_532, 32), _mm256_permute2f128_pd(_t29_532, _t29_532, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_46, _t29_47), _mm256_unpacklo_pd(_t29_48, _t29_49), 32));
  _t29_535 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_532, _t29_532, 49), _mm256_permute2f128_pd(_t29_532, _t29_532, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_46, _t29_47), _mm256_unpacklo_pd(_t29_48, _t29_49), 32));
  _t29_536 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_532, _t29_532, 49), _mm256_permute2f128_pd(_t29_532, _t29_532, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_46, _t29_47), _mm256_unpacklo_pd(_t29_48, _t29_49), 32));

  // 4-BLAC: 4x4 - 4x4
  _t29_537 = _mm256_sub_pd(_t29_528, _t29_533);
  _t29_538 = _mm256_sub_pd(_t29_529, _t29_534);
  _t29_539 = _mm256_sub_pd(_t29_530, _t29_535);
  _t29_540 = _mm256_sub_pd(_t29_531, _t29_536);

  // AVX Storer:
  _t29_79 = _t29_537;
  _t29_80 = _t29_538;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_541 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_79, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_542 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_543 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_544 = _mm256_add_pd(_t29_542, _t29_543);

  // 4-BLAC: 1x4 / 1x4
  _t29_545 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_541), _mm256_castpd256_pd128(_t29_544)));

  // AVX Storer:
  _t29_50 = _t29_545;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_546 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_79, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_547 = _t29_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_548 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t29_549 = _t29_548;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_550 = _mm256_mul_pd(_t29_547, _t29_549);

  // 4-BLAC: 1x4 - 1x4
  _t29_551 = _mm256_sub_pd(_t29_546, _t29_550);

  // AVX Storer:
  _t29_51 = _t29_551;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_552 = _t29_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_553 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_554 = _t8_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_555 = _mm256_add_pd(_t29_553, _t29_554);

  // 4-BLAC: 1x4 / 1x4
  _t29_556 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_552), _mm256_castpd256_pd128(_t29_555)));

  // AVX Storer:
  _t29_51 = _t29_556;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_557 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_79, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_79, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_558 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_50, _t29_51), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_559 = _t10_3;

  // 4-BLAC: (1x4)^T
  _t29_560 = _t29_559;

  // 4-BLAC: 1x4 * 4x1
  _t29_561 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_558, _t29_560), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_558, _t29_560), _mm256_mul_pd(_t29_558, _t29_560), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_558, _t29_560), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_558, _t29_560), _mm256_mul_pd(_t29_558, _t29_560), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_558, _t29_560), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_558, _t29_560), _mm256_mul_pd(_t29_558, _t29_560), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_562 = _mm256_sub_pd(_t29_557, _t29_561);

  // AVX Storer:
  _t29_52 = _t29_562;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_563 = _t29_52;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_564 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_565 = _t8_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_566 = _mm256_add_pd(_t29_564, _t29_565);

  // 4-BLAC: 1x4 / 1x4
  _t29_567 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_563), _mm256_castpd256_pd128(_t29_566)));

  // AVX Storer:
  _t29_52 = _t29_567;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_568 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_79, _t29_79, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_569 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_50, _t29_51), _mm256_unpacklo_pd(_t29_52, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_570 = _t10_0;

  // 4-BLAC: (1x4)^T
  _t29_571 = _t29_570;

  // 4-BLAC: 1x4 * 4x1
  _t29_572 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_569, _t29_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_569, _t29_571), _mm256_mul_pd(_t29_569, _t29_571), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_569, _t29_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_569, _t29_571), _mm256_mul_pd(_t29_569, _t29_571), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_569, _t29_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_569, _t29_571), _mm256_mul_pd(_t29_569, _t29_571), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_573 = _mm256_sub_pd(_t29_568, _t29_572);

  // AVX Storer:
  _t29_53 = _t29_573;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_574 = _t29_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_575 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_576 = _t8_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_577 = _mm256_add_pd(_t29_575, _t29_576);

  // 4-BLAC: 1x4 / 1x4
  _t29_578 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_574), _mm256_castpd256_pd128(_t29_577)));

  // AVX Storer:
  _t29_53 = _t29_578;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, fi1812)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_579 = _t29_1;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t29_60 = _mm256_mul_pd(_t29_579, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_50, _t29_51), _mm256_unpacklo_pd(_t29_52, _t29_53), 32));

  // 4-BLAC: 1x4 - 1x4
  _t29_80 = _mm256_sub_pd(_t29_80, _t29_60);

  // AVX Storer:

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_580 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_80, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_581 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_582 = _t8_6;

  // 4-BLAC: 1x4 + 1x4
  _t29_583 = _mm256_add_pd(_t29_581, _t29_582);

  // 4-BLAC: 1x4 / 1x4
  _t29_584 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_580), _mm256_castpd256_pd128(_t29_583)));

  // AVX Storer:
  _t29_54 = _t29_584;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_585 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_80, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_586 = _t29_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_587 = _t10_5;

  // 4-BLAC: (4x1)^T
  _t29_588 = _t29_587;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_589 = _mm256_mul_pd(_t29_586, _t29_588);

  // 4-BLAC: 1x4 - 1x4
  _t29_590 = _mm256_sub_pd(_t29_585, _t29_589);

  // AVX Storer:
  _t29_55 = _t29_590;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_591 = _t29_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_592 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_593 = _t8_4;

  // 4-BLAC: 1x4 + 1x4
  _t29_594 = _mm256_add_pd(_t29_592, _t29_593);

  // 4-BLAC: 1x4 / 1x4
  _t29_595 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_591), _mm256_castpd256_pd128(_t29_594)));

  // AVX Storer:
  _t29_55 = _t29_595;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_596 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_80, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t29_80, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_597 = _mm256_blend_pd(_mm256_unpacklo_pd(_t29_54, _t29_55), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t29_598 = _t10_3;

  // 4-BLAC: (1x4)^T
  _t29_599 = _t29_598;

  // 4-BLAC: 1x4 * 4x1
  _t29_600 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_597, _t29_599), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_597, _t29_599), _mm256_mul_pd(_t29_597, _t29_599), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_597, _t29_599), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_597, _t29_599), _mm256_mul_pd(_t29_597, _t29_599), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_597, _t29_599), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_597, _t29_599), _mm256_mul_pd(_t29_597, _t29_599), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_601 = _mm256_sub_pd(_t29_596, _t29_600);

  // AVX Storer:
  _t29_56 = _t29_601;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_602 = _t29_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_603 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_604 = _t8_2;

  // 4-BLAC: 1x4 + 1x4
  _t29_605 = _mm256_add_pd(_t29_603, _t29_604);

  // 4-BLAC: 1x4 / 1x4
  _t29_606 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_602), _mm256_castpd256_pd128(_t29_605)));

  // AVX Storer:
  _t29_56 = _t29_606;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_607 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t29_80, _t29_80, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_608 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_54, _t29_55), _mm256_unpacklo_pd(_t29_56, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t29_609 = _t10_0;

  // 4-BLAC: (1x4)^T
  _t29_610 = _t29_609;

  // 4-BLAC: 1x4 * 4x1
  _t29_611 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_608, _t29_610), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_608, _t29_610), _mm256_mul_pd(_t29_608, _t29_610), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_608, _t29_610), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_608, _t29_610), _mm256_mul_pd(_t29_608, _t29_610), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_608, _t29_610), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_608, _t29_610), _mm256_mul_pd(_t29_608, _t29_610), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_612 = _mm256_sub_pd(_t29_607, _t29_611);

  // AVX Storer:
  _t29_57 = _t29_612;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_613 = _t29_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_614 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_615 = _t8_0;

  // 4-BLAC: 1x4 + 1x4
  _t29_616 = _mm256_add_pd(_t29_614, _t29_615);

  // 4-BLAC: 1x4 / 1x4
  _t29_617 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_613), _mm256_castpd256_pd128(_t29_616)));

  // AVX Storer:
  _t29_57 = _t29_617;

  _asm256_storeu_pd(C + 700, _t29_11);
  _asm256_storeu_pd(C + 728, _t29_12);
  _asm256_storeu_pd(C + 756, _t29_13);
  _mm_store_sd(&(C[676]), _mm256_castpd256_pd128(_t29_26));
  _mm_store_sd(&(C[677]), _mm256_castpd256_pd128(_t29_27));
  _mm_store_sd(&(C[678]), _mm256_castpd256_pd128(_t29_28));
  _mm_store_sd(&(C[679]), _mm256_castpd256_pd128(_t29_29));
  _mm_store_sd(&(C[704]), _mm256_castpd256_pd128(_t29_30));
  _mm_store_sd(&(C[705]), _mm256_castpd256_pd128(_t29_31));
  _mm_store_sd(&(C[706]), _mm256_castpd256_pd128(_t29_32));
  _mm_store_sd(&(C[707]), _mm256_castpd256_pd128(_t29_33));
  _mm_store_sd(&(C[732]), _mm256_castpd256_pd128(_t29_34));
  _mm_store_sd(&(C[733]), _mm256_castpd256_pd128(_t29_35));
  _mm_store_sd(&(C[734]), _mm256_castpd256_pd128(_t29_36));
  _mm_store_sd(&(C[735]), _mm256_castpd256_pd128(_t29_37));
  _mm_store_sd(&(C[760]), _mm256_castpd256_pd128(_t29_38));
  _mm_store_sd(&(C[761]), _mm256_castpd256_pd128(_t29_39));
  _mm_store_sd(&(C[762]), _mm256_castpd256_pd128(_t29_40));
  _mm_store_sd(&(C[763]), _mm256_castpd256_pd128(_t29_41));
  _mm_store_sd(&(C[680]), _mm256_castpd256_pd128(_t29_42));
  _mm_store_sd(&(C[681]), _mm256_castpd256_pd128(_t29_43));
  _mm_store_sd(&(C[682]), _mm256_castpd256_pd128(_t29_44));
  _mm_store_sd(&(C[683]), _mm256_castpd256_pd128(_t29_45));
  _mm_store_sd(&(C[708]), _mm256_castpd256_pd128(_t29_46));
  _mm_store_sd(&(C[709]), _mm256_castpd256_pd128(_t29_47));
  _mm_store_sd(&(C[710]), _mm256_castpd256_pd128(_t29_48));
  _mm_store_sd(&(C[711]), _mm256_castpd256_pd128(_t29_49));
  _mm_store_sd(&(C[736]), _mm256_castpd256_pd128(_t29_50));
  _mm_store_sd(&(C[737]), _mm256_castpd256_pd128(_t29_51));
  _mm_store_sd(&(C[738]), _mm256_castpd256_pd128(_t29_52));
  _mm_store_sd(&(C[739]), _mm256_castpd256_pd128(_t29_53));
  _mm_store_sd(&(C[764]), _mm256_castpd256_pd128(_t29_54));
  _mm_store_sd(&(C[765]), _mm256_castpd256_pd128(_t29_55));
  _mm_store_sd(&(C[766]), _mm256_castpd256_pd128(_t29_56));
  _mm_store_sd(&(C[767]), _mm256_castpd256_pd128(_t29_57));

  for( int fi1812 = 12; fi1812 <= 20; fi1812+=4 ) {
    _t30_4 = _asm256_loadu_pd(C + fi1812 + 672);
    _t30_5 = _asm256_loadu_pd(C + fi1812 + 700);
    _t30_6 = _asm256_loadu_pd(C + fi1812 + 728);
    _t30_7 = _asm256_loadu_pd(C + fi1812 + 756);
    _t30_3 = _asm256_loadu_pd(L + 28*fi1812);
    _t30_2 = _asm256_loadu_pd(L + 28*fi1812 + 28);
    _t30_1 = _asm256_loadu_pd(L + 28*fi1812 + 56);
    _t30_0 = _asm256_loadu_pd(L + 28*fi1812 + 84);

    // Generating : X[28,28] = ( S(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, fi1812)) - ( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, fi1812), L[28,28],h(4, 28, 0)) ) ) ),h(4, 28, fi1812)) + Sum_{i100} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, fi1812), L[28,28],h(4, 28, i100)) ) ),h(4, 28, fi1812)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t29_622 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_3, _t30_2), _mm256_unpacklo_pd(_t30_1, _t30_0), 32);
    _t29_623 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t30_3, _t30_2), _mm256_unpackhi_pd(_t30_1, _t30_0), 32);
    _t29_624 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_3, _t30_2), _mm256_unpacklo_pd(_t30_1, _t30_0), 49);
    _t29_625 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t30_3, _t30_2), _mm256_unpackhi_pd(_t30_1, _t30_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t29_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 32), _mm256_permute2f128_pd(_t29_7, _t29_7, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_8, _t29_8, 32), _mm256_permute2f128_pd(_t29_8, _t29_8, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_9, _t29_9, 32), _mm256_permute2f128_pd(_t29_9, _t29_9, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_10, _t29_10, 32), _mm256_permute2f128_pd(_t29_10, _t29_10, 32), 0), _t29_625)));
    _t29_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_14, _t29_14, 32), _mm256_permute2f128_pd(_t29_14, _t29_14, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_15, _t29_15, 32), _mm256_permute2f128_pd(_t29_15, _t29_15, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_16, _t29_16, 32), _mm256_permute2f128_pd(_t29_16, _t29_16, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_17, _t29_17, 32), _mm256_permute2f128_pd(_t29_17, _t29_17, 32), 0), _t29_625)));
    _t29_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_18, _t29_18, 32), _mm256_permute2f128_pd(_t29_18, _t29_18, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_19, _t29_19, 32), _mm256_permute2f128_pd(_t29_19, _t29_19, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_20, _t29_20, 32), _mm256_permute2f128_pd(_t29_20, _t29_20, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_21, _t29_21, 32), _mm256_permute2f128_pd(_t29_21, _t29_21, 32), 0), _t29_625)));
    _t29_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_22, _t29_22, 32), _mm256_permute2f128_pd(_t29_22, _t29_22, 32), 0), _t29_622), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_23, _t29_23, 32), _mm256_permute2f128_pd(_t29_23, _t29_23, 32), 0), _t29_623)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_24, _t29_24, 32), _mm256_permute2f128_pd(_t29_24, _t29_24, 32), 0), _t29_624), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_25, _t29_25, 32), _mm256_permute2f128_pd(_t29_25, _t29_25, 32), 0), _t29_625)));

    // 4-BLAC: 4x4 - 4x4
    _t30_4 = _mm256_sub_pd(_t30_4, _t29_65);
    _t30_5 = _mm256_sub_pd(_t30_5, _t29_66);
    _t30_6 = _mm256_sub_pd(_t30_6, _t29_67);
    _t30_7 = _mm256_sub_pd(_t30_7, _t29_68);

    // AVX Storer:
    _asm256_storeu_pd(C + fi1812 + 672, _t30_4);
    _asm256_storeu_pd(C + fi1812 + 700, _t30_5);
    _asm256_storeu_pd(C + fi1812 + 728, _t30_6);
    _asm256_storeu_pd(C + fi1812 + 756, _t30_7);

    for( int i100 = 4; i100 <= fi1812 - 1; i100+=4 ) {
      _t31_20 = _asm256_loadu_pd(C + fi1812 + 672);
      _t31_21 = _asm256_loadu_pd(C + fi1812 + 700);
      _t31_22 = _asm256_loadu_pd(C + fi1812 + 728);
      _t31_23 = _asm256_loadu_pd(C + fi1812 + 756);
      _t31_19 = _mm256_broadcast_sd(C + i100 + 672);
      _t31_18 = _mm256_broadcast_sd(C + i100 + 673);
      _t31_17 = _mm256_broadcast_sd(C + i100 + 674);
      _t31_16 = _mm256_broadcast_sd(C + i100 + 675);
      _t31_15 = _mm256_broadcast_sd(C + i100 + 700);
      _t31_14 = _mm256_broadcast_sd(C + i100 + 701);
      _t31_13 = _mm256_broadcast_sd(C + i100 + 702);
      _t31_12 = _mm256_broadcast_sd(C + i100 + 703);
      _t31_11 = _mm256_broadcast_sd(C + i100 + 728);
      _t31_10 = _mm256_broadcast_sd(C + i100 + 729);
      _t31_9 = _mm256_broadcast_sd(C + i100 + 730);
      _t31_8 = _mm256_broadcast_sd(C + i100 + 731);
      _t31_7 = _mm256_broadcast_sd(C + i100 + 756);
      _t31_6 = _mm256_broadcast_sd(C + i100 + 757);
      _t31_5 = _mm256_broadcast_sd(C + i100 + 758);
      _t31_4 = _mm256_broadcast_sd(C + i100 + 759);
      _t31_3 = _asm256_loadu_pd(L + 28*fi1812 + i100);
      _t31_2 = _asm256_loadu_pd(L + 28*fi1812 + i100 + 28);
      _t31_1 = _asm256_loadu_pd(L + 28*fi1812 + i100 + 56);
      _t31_0 = _asm256_loadu_pd(L + 28*fi1812 + i100 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_626 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 32);
      _t29_627 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 32);
      _t29_628 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 49);
      _t29_629 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t29_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t29_626), _mm256_mul_pd(_t31_18, _t29_627)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t29_628), _mm256_mul_pd(_t31_16, _t29_629)));
      _t29_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t29_626), _mm256_mul_pd(_t31_14, _t29_627)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t29_628), _mm256_mul_pd(_t31_12, _t29_629)));
      _t29_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t29_626), _mm256_mul_pd(_t31_10, _t29_627)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t29_628), _mm256_mul_pd(_t31_8, _t29_629)));
      _t29_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t29_626), _mm256_mul_pd(_t31_6, _t29_627)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t29_628), _mm256_mul_pd(_t31_4, _t29_629)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t31_20 = _mm256_sub_pd(_t31_20, _t29_69);
      _t31_21 = _mm256_sub_pd(_t31_21, _t29_70);
      _t31_22 = _mm256_sub_pd(_t31_22, _t29_71);
      _t31_23 = _mm256_sub_pd(_t31_23, _t29_72);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1812 + 672, _t31_20);
      _asm256_storeu_pd(C + fi1812 + 700, _t31_21);
      _asm256_storeu_pd(C + fi1812 + 728, _t31_22);
      _asm256_storeu_pd(C + fi1812 + 756, _t31_23);
    }
    _t30_4 = _asm256_loadu_pd(C + fi1812 + 672);
    _t30_7 = _asm256_loadu_pd(C + fi1812 + 756);
    _t30_5 = _asm256_loadu_pd(C + fi1812 + 700);
    _t30_6 = _asm256_loadu_pd(C + fi1812 + 728);
    _t32_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812])));
    _t32_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 28])));
    _t32_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 29])));
    _t32_3 = _mm256_maskload_pd(L + 29*fi1812 + 56, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t32_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 58])));
    _t32_1 = _mm256_maskload_pd(L + 29*fi1812 + 84, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t32_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[29*fi1812 + 87])));

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_4, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_24 = _t29_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_25 = _t32_6;

    // 4-BLAC: 1x4 + 1x4
    _t29_442 = _mm256_add_pd(_t32_24, _t32_25);

    // 4-BLAC: 1x4 / 1x4
    _t32_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_23), _mm256_castpd256_pd128(_t29_442)));

    // AVX Storer:
    _t32_7 = _t32_26;

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_4, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_28 = _t32_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_29 = _t32_5;

    // 4-BLAC: (4x1)^T
    _t29_447 = _t32_29;

    // 4-BLAC: 1x4 Kro 1x4
    _t29_448 = _mm256_mul_pd(_t32_28, _t29_447);

    // 4-BLAC: 1x4 - 1x4
    _t32_30 = _mm256_sub_pd(_t32_27, _t29_448);

    // AVX Storer:
    _t32_8 = _t32_30;

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_31 = _t32_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_32 = _t29_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_33 = _t32_4;

    // 4-BLAC: 1x4 + 1x4
    _t29_453 = _mm256_add_pd(_t32_32, _t32_33);

    // 4-BLAC: 1x4 / 1x4
    _t32_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_31), _mm256_castpd256_pd128(_t29_453)));

    // AVX Storer:
    _t32_8 = _t32_34;

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 24), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_4, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t32_7, _t32_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_37 = _t32_3;

    // 4-BLAC: (1x4)^T
    _t29_458 = _t32_37;

    // 4-BLAC: 1x4 * 4x1
    _t29_459 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_36, _t29_458), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_36, _t29_458), _mm256_mul_pd(_t32_36, _t29_458), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_36, _t29_458), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_36, _t29_458), _mm256_mul_pd(_t32_36, _t29_458), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_36, _t29_458), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_36, _t29_458), _mm256_mul_pd(_t32_36, _t29_458), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_38 = _mm256_sub_pd(_t32_35, _t29_459);

    // AVX Storer:
    _t32_9 = _t32_38;

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_39 = _t32_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_40 = _t29_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_41 = _t32_2;

    // 4-BLAC: 1x4 + 1x4
    _t29_464 = _mm256_add_pd(_t32_40, _t32_41);

    // 4-BLAC: 1x4 / 1x4
    _t32_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_39), _mm256_castpd256_pd128(_t29_464)));

    // AVX Storer:
    _t32_9 = _t32_42;

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 24), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_4, _t30_4, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_7, _t32_8), _mm256_unpacklo_pd(_t32_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_45 = _t32_1;

    // 4-BLAC: (1x4)^T
    _t29_469 = _t32_45;

    // 4-BLAC: 1x4 * 4x1
    _t29_470 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_44, _t29_469), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_44, _t29_469), _mm256_mul_pd(_t32_44, _t29_469), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_44, _t29_469), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_44, _t29_469), _mm256_mul_pd(_t32_44, _t29_469), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_44, _t29_469), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_44, _t29_469), _mm256_mul_pd(_t32_44, _t29_469), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_46 = _mm256_sub_pd(_t32_43, _t29_470);

    // AVX Storer:
    _t32_10 = _t32_46;

    // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 24), L[28,28],h(1, 28, 24)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_47 = _t32_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_48 = _t29_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_49 = _t32_0;

    // 4-BLAC: 1x4 + 1x4
    _t29_475 = _mm256_add_pd(_t32_48, _t32_49);

    // 4-BLAC: 1x4 / 1x4
    _t32_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_47), _mm256_castpd256_pd128(_t29_475)));

    // AVX Storer:
    _t32_10 = _t32_50;

    // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(4, 28, fi1812)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) * G(h(1, 28, 24), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812))

    // AVX Loader:

    // 3x4 -> 4x4
    _t32_51 = _t30_5;
    _t32_52 = _t30_6;
    _t32_53 = _t30_7;
    _t32_54 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t32_55 = _t29_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t29_482 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_55, _t32_55, 32), _mm256_permute2f128_pd(_t32_55, _t32_55, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_7, _t32_8), _mm256_unpacklo_pd(_t32_9, _t32_10), 32));
    _t29_483 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_55, _t32_55, 32), _mm256_permute2f128_pd(_t32_55, _t32_55, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_7, _t32_8), _mm256_unpacklo_pd(_t32_9, _t32_10), 32));
    _t29_484 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_55, _t32_55, 49), _mm256_permute2f128_pd(_t32_55, _t32_55, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_7, _t32_8), _mm256_unpacklo_pd(_t32_9, _t32_10), 32));
    _t29_485 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_55, _t32_55, 49), _mm256_permute2f128_pd(_t32_55, _t32_55, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_7, _t32_8), _mm256_unpacklo_pd(_t32_9, _t32_10), 32));

    // 4-BLAC: 4x4 - 4x4
    _t32_56 = _mm256_sub_pd(_t32_51, _t29_482);
    _t32_57 = _mm256_sub_pd(_t32_52, _t29_483);
    _t32_58 = _mm256_sub_pd(_t32_53, _t29_484);
    _t32_59 = _mm256_sub_pd(_t32_54, _t29_485);

    // AVX Storer:
    _t30_5 = _t32_56;
    _t30_6 = _t32_57;
    _t30_7 = _t32_58;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_60 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_61 = _t29_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_62 = _t32_6;

    // 4-BLAC: 1x4 + 1x4
    _t29_493 = _mm256_add_pd(_t32_61, _t32_62);

    // 4-BLAC: 1x4 / 1x4
    _t32_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_60), _mm256_castpd256_pd128(_t29_493)));

    // AVX Storer:
    _t32_11 = _t32_63;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_64 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_65 = _t32_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_66 = _t32_5;

    // 4-BLAC: (4x1)^T
    _t29_498 = _t32_66;

    // 4-BLAC: 1x4 Kro 1x4
    _t29_499 = _mm256_mul_pd(_t32_65, _t29_498);

    // 4-BLAC: 1x4 - 1x4
    _t32_67 = _mm256_sub_pd(_t32_64, _t29_499);

    // AVX Storer:
    _t32_12 = _t32_67;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_68 = _t32_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_69 = _t29_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_70 = _t32_4;

    // 4-BLAC: 1x4 + 1x4
    _t29_504 = _mm256_add_pd(_t32_69, _t32_70);

    // 4-BLAC: 1x4 / 1x4
    _t32_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_68), _mm256_castpd256_pd128(_t29_504)));

    // AVX Storer:
    _t32_12 = _t32_71;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 25), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_72 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_5, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_73 = _mm256_blend_pd(_mm256_unpacklo_pd(_t32_11, _t32_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_74 = _t32_3;

    // 4-BLAC: (1x4)^T
    _t29_509 = _t32_74;

    // 4-BLAC: 1x4 * 4x1
    _t29_510 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_73, _t29_509), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_73, _t29_509), _mm256_mul_pd(_t32_73, _t29_509), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_73, _t29_509), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_73, _t29_509), _mm256_mul_pd(_t32_73, _t29_509), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_73, _t29_509), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_73, _t29_509), _mm256_mul_pd(_t32_73, _t29_509), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_75 = _mm256_sub_pd(_t32_72, _t29_510);

    // AVX Storer:
    _t32_13 = _t32_75;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_76 = _t32_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_77 = _t29_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_78 = _t32_2;

    // 4-BLAC: 1x4 + 1x4
    _t29_515 = _mm256_add_pd(_t32_77, _t32_78);

    // 4-BLAC: 1x4 / 1x4
    _t32_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_76), _mm256_castpd256_pd128(_t29_515)));

    // AVX Storer:
    _t32_13 = _t32_79;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 25), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_80 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_5, _t30_5, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_11, _t32_12), _mm256_unpacklo_pd(_t32_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_82 = _t32_1;

    // 4-BLAC: (1x4)^T
    _t29_520 = _t32_82;

    // 4-BLAC: 1x4 * 4x1
    _t29_521 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_81, _t29_520), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_81, _t29_520), _mm256_mul_pd(_t32_81, _t29_520), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_81, _t29_520), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_81, _t29_520), _mm256_mul_pd(_t32_81, _t29_520), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_81, _t29_520), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_81, _t29_520), _mm256_mul_pd(_t32_81, _t29_520), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_83 = _mm256_sub_pd(_t32_80, _t29_521);

    // AVX Storer:
    _t32_14 = _t32_83;

    // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_84 = _t32_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_85 = _t29_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_86 = _t32_0;

    // 4-BLAC: 1x4 + 1x4
    _t29_526 = _mm256_add_pd(_t32_85, _t32_86);

    // 4-BLAC: 1x4 / 1x4
    _t32_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_84), _mm256_castpd256_pd128(_t29_526)));

    // AVX Storer:
    _t32_14 = _t32_87;

    // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(4, 28, fi1812)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) * G(h(1, 28, 25), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812))

    // AVX Loader:

    // 2x4 -> 4x4
    _t32_88 = _t30_6;
    _t32_89 = _t30_7;
    _t32_90 = _mm256_setzero_pd();
    _t32_91 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t32_92 = _t29_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t29_533 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_92, _t32_92, 32), _mm256_permute2f128_pd(_t32_92, _t32_92, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_11, _t32_12), _mm256_unpacklo_pd(_t32_13, _t32_14), 32));
    _t29_534 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_92, _t32_92, 32), _mm256_permute2f128_pd(_t32_92, _t32_92, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_11, _t32_12), _mm256_unpacklo_pd(_t32_13, _t32_14), 32));
    _t29_535 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_92, _t32_92, 49), _mm256_permute2f128_pd(_t32_92, _t32_92, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_11, _t32_12), _mm256_unpacklo_pd(_t32_13, _t32_14), 32));
    _t29_536 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_92, _t32_92, 49), _mm256_permute2f128_pd(_t32_92, _t32_92, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_11, _t32_12), _mm256_unpacklo_pd(_t32_13, _t32_14), 32));

    // 4-BLAC: 4x4 - 4x4
    _t32_93 = _mm256_sub_pd(_t32_88, _t29_533);
    _t32_94 = _mm256_sub_pd(_t32_89, _t29_534);
    _t32_95 = _mm256_sub_pd(_t32_90, _t29_535);
    _t32_96 = _mm256_sub_pd(_t32_91, _t29_536);

    // AVX Storer:
    _t30_6 = _t32_93;
    _t30_7 = _t32_94;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_97 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_98 = _t29_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_99 = _t32_6;

    // 4-BLAC: 1x4 + 1x4
    _t29_544 = _mm256_add_pd(_t32_98, _t32_99);

    // 4-BLAC: 1x4 / 1x4
    _t32_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_97), _mm256_castpd256_pd128(_t29_544)));

    // AVX Storer:
    _t32_15 = _t32_100;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_101 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_6, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_102 = _t32_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_103 = _t32_5;

    // 4-BLAC: (4x1)^T
    _t29_549 = _t32_103;

    // 4-BLAC: 1x4 Kro 1x4
    _t29_550 = _mm256_mul_pd(_t32_102, _t29_549);

    // 4-BLAC: 1x4 - 1x4
    _t32_104 = _mm256_sub_pd(_t32_101, _t29_550);

    // AVX Storer:
    _t32_16 = _t32_104;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_105 = _t32_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_106 = _t29_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_107 = _t32_4;

    // 4-BLAC: 1x4 + 1x4
    _t29_555 = _mm256_add_pd(_t32_106, _t32_107);

    // 4-BLAC: 1x4 / 1x4
    _t32_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_105), _mm256_castpd256_pd128(_t29_555)));

    // AVX Storer:
    _t32_16 = _t32_108;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 26), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_109 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_6, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_110 = _mm256_blend_pd(_mm256_unpacklo_pd(_t32_15, _t32_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_111 = _t32_3;

    // 4-BLAC: (1x4)^T
    _t29_560 = _t32_111;

    // 4-BLAC: 1x4 * 4x1
    _t29_561 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_110, _t29_560), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_110, _t29_560), _mm256_mul_pd(_t32_110, _t29_560), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_110, _t29_560), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_110, _t29_560), _mm256_mul_pd(_t32_110, _t29_560), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_110, _t29_560), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_110, _t29_560), _mm256_mul_pd(_t32_110, _t29_560), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_112 = _mm256_sub_pd(_t32_109, _t29_561);

    // AVX Storer:
    _t32_17 = _t32_112;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_113 = _t32_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_114 = _t29_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_115 = _t32_2;

    // 4-BLAC: 1x4 + 1x4
    _t29_566 = _mm256_add_pd(_t32_114, _t32_115);

    // 4-BLAC: 1x4 / 1x4
    _t32_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_113), _mm256_castpd256_pd128(_t29_566)));

    // AVX Storer:
    _t32_17 = _t32_116;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 26), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_117 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_6, _t30_6, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_118 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_15, _t32_16), _mm256_unpacklo_pd(_t32_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_119 = _t32_1;

    // 4-BLAC: (1x4)^T
    _t29_571 = _t32_119;

    // 4-BLAC: 1x4 * 4x1
    _t29_572 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_118, _t29_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_118, _t29_571), _mm256_mul_pd(_t32_118, _t29_571), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_118, _t29_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_118, _t29_571), _mm256_mul_pd(_t32_118, _t29_571), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_118, _t29_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_118, _t29_571), _mm256_mul_pd(_t32_118, _t29_571), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_120 = _mm256_sub_pd(_t32_117, _t29_572);

    // AVX Storer:
    _t32_18 = _t32_120;

    // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_121 = _t32_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_122 = _t29_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_123 = _t32_0;

    // 4-BLAC: 1x4 + 1x4
    _t29_577 = _mm256_add_pd(_t32_122, _t32_123);

    // 4-BLAC: 1x4 / 1x4
    _t32_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_121), _mm256_castpd256_pd128(_t29_577)));

    // AVX Storer:
    _t32_18 = _t32_124;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(4, 28, fi1812)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(4, 28, fi1812)) ) ),h(4, 28, fi1812))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_125 = _t29_1;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t29_60 = _mm256_mul_pd(_t32_125, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_15, _t32_16), _mm256_unpacklo_pd(_t32_17, _t32_18), 32));

    // 4-BLAC: 1x4 - 1x4
    _t30_7 = _mm256_sub_pd(_t30_7, _t29_60);

    // AVX Storer:

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812), L[28,28],h(1, 28, fi1812)) ) ),h(1, 28, fi1812))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t30_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_127 = _t29_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_128 = _t32_6;

    // 4-BLAC: 1x4 + 1x4
    _t29_583 = _mm256_add_pd(_t32_127, _t32_128);

    // 4-BLAC: 1x4 / 1x4
    _t32_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_126), _mm256_castpd256_pd128(_t29_583)));

    // AVX Storer:
    _t32_19 = _t32_129;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 1)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812)) Kro T( G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_130 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_7, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_131 = _t32_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_132 = _t32_5;

    // 4-BLAC: (4x1)^T
    _t29_588 = _t32_132;

    // 4-BLAC: 1x4 Kro 1x4
    _t29_589 = _mm256_mul_pd(_t32_131, _t29_588);

    // 4-BLAC: 1x4 - 1x4
    _t32_133 = _mm256_sub_pd(_t32_130, _t29_589);

    // AVX Storer:
    _t32_20 = _t32_133;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 1)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812 + 1), L[28,28],h(1, 28, fi1812 + 1)) ) ),h(1, 28, fi1812 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_134 = _t32_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_135 = _t29_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_136 = _t32_4;

    // 4-BLAC: 1x4 + 1x4
    _t29_594 = _mm256_add_pd(_t32_135, _t32_136);

    // 4-BLAC: 1x4 / 1x4
    _t32_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_134), _mm256_castpd256_pd128(_t29_594)));

    // AVX Storer:
    _t32_20 = _t32_137;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 2)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, fi1812)) * T( G(h(1, 28, fi1812 + 2), L[28,28],h(2, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t30_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t30_7, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t32_19, _t32_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 1x2 -> 1x4
    _t32_140 = _t32_3;

    // 4-BLAC: (1x4)^T
    _t29_599 = _t32_140;

    // 4-BLAC: 1x4 * 4x1
    _t29_600 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_139, _t29_599), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_139, _t29_599), _mm256_mul_pd(_t32_139, _t29_599), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_139, _t29_599), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_139, _t29_599), _mm256_mul_pd(_t32_139, _t29_599), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_139, _t29_599), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_139, _t29_599), _mm256_mul_pd(_t32_139, _t29_599), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_141 = _mm256_sub_pd(_t32_138, _t29_600);

    // AVX Storer:
    _t32_21 = _t32_141;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 2)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812 + 2), L[28,28],h(1, 28, fi1812 + 2)) ) ),h(1, 28, fi1812 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_142 = _t32_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_143 = _t29_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_144 = _t32_2;

    // 4-BLAC: 1x4 + 1x4
    _t29_605 = _mm256_add_pd(_t32_143, _t32_144);

    // 4-BLAC: 1x4 / 1x4
    _t32_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_142), _mm256_castpd256_pd128(_t29_605)));

    // AVX Storer:
    _t32_21 = _t32_145;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 3)) - ( G(h(1, 28, 27), X[28,28],h(3, 28, fi1812)) * T( G(h(1, 28, fi1812 + 3), L[28,28],h(3, 28, fi1812)) ) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_146 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t30_7, _t30_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_147 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_19, _t32_20), _mm256_unpacklo_pd(_t32_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t32_148 = _t32_1;

    // 4-BLAC: (1x4)^T
    _t29_610 = _t32_148;

    // 4-BLAC: 1x4 * 4x1
    _t29_611 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_147, _t29_610), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_147, _t29_610), _mm256_mul_pd(_t32_147, _t29_610), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_147, _t29_610), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_147, _t29_610), _mm256_mul_pd(_t32_147, _t29_610), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_147, _t29_610), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_147, _t29_610), _mm256_mul_pd(_t32_147, _t29_610), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t32_149 = _mm256_sub_pd(_t32_146, _t29_611);

    // AVX Storer:
    _t32_22 = _t32_149;

    // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, fi1812 + 3)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, fi1812 + 3), L[28,28],h(1, 28, fi1812 + 3)) ) ),h(1, 28, fi1812 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_150 = _t32_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_151 = _t29_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t32_152 = _t32_0;

    // 4-BLAC: 1x4 + 1x4
    _t29_616 = _mm256_add_pd(_t32_151, _t32_152);

    // 4-BLAC: 1x4 / 1x4
    _t32_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t32_150), _mm256_castpd256_pd128(_t29_616)));

    // AVX Storer:
    _t32_22 = _t32_153;
    _mm_store_sd(&(C[fi1812 + 672]), _mm256_castpd256_pd128(_t32_7));
    _mm_store_sd(&(C[fi1812 + 673]), _mm256_castpd256_pd128(_t32_8));
    _mm_store_sd(&(C[fi1812 + 674]), _mm256_castpd256_pd128(_t32_9));
    _mm_store_sd(&(C[fi1812 + 675]), _mm256_castpd256_pd128(_t32_10));
    _mm_store_sd(&(C[fi1812 + 700]), _mm256_castpd256_pd128(_t32_11));
    _mm_store_sd(&(C[fi1812 + 701]), _mm256_castpd256_pd128(_t32_12));
    _mm_store_sd(&(C[fi1812 + 702]), _mm256_castpd256_pd128(_t32_13));
    _mm_store_sd(&(C[fi1812 + 703]), _mm256_castpd256_pd128(_t32_14));
    _mm_store_sd(&(C[fi1812 + 728]), _mm256_castpd256_pd128(_t32_15));
    _mm_store_sd(&(C[fi1812 + 729]), _mm256_castpd256_pd128(_t32_16));
    _mm_store_sd(&(C[fi1812 + 730]), _mm256_castpd256_pd128(_t32_17));
    _mm_store_sd(&(C[fi1812 + 731]), _mm256_castpd256_pd128(_t32_18));
    _mm_store_sd(&(C[fi1812 + 756]), _mm256_castpd256_pd128(_t32_19));
    _mm_store_sd(&(C[fi1812 + 757]), _mm256_castpd256_pd128(_t32_20));
    _mm_store_sd(&(C[fi1812 + 758]), _mm256_castpd256_pd128(_t32_21));
    _mm_store_sd(&(C[fi1812 + 759]), _mm256_castpd256_pd128(_t32_22));
  }

  _t33_24 = _mm256_castpd128_pd256(_mm_load_sd(C + 696));
  _t33_25 = _mm256_maskload_pd(C + 724, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t33_26 = _mm256_maskload_pd(C + 752, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t33_27 = _asm256_loadu_pd(C + 780);
  _t33_19 = _mm256_broadcast_sd(L + 672);
  _t33_18 = _mm256_broadcast_sd(L + 673);
  _t33_17 = _mm256_broadcast_sd(L + 674);
  _t33_16 = _mm256_broadcast_sd(L + 675);
  _t33_15 = _mm256_broadcast_sd(L + 700);
  _t33_14 = _mm256_broadcast_sd(L + 701);
  _t33_13 = _mm256_broadcast_sd(L + 702);
  _t33_12 = _mm256_broadcast_sd(L + 703);
  _t33_11 = _mm256_broadcast_sd(L + 728);
  _t33_10 = _mm256_broadcast_sd(L + 729);
  _t33_9 = _mm256_broadcast_sd(L + 730);
  _t33_8 = _mm256_broadcast_sd(L + 731);
  _t33_7 = _mm256_broadcast_sd(L + 756);
  _t33_6 = _mm256_broadcast_sd(L + 757);
  _t33_5 = _mm256_broadcast_sd(L + 758);
  _t33_4 = _mm256_broadcast_sd(L + 759);
  _t33_3 = _asm256_loadu_pd(L + 672);
  _t33_2 = _asm256_loadu_pd(L + 700);
  _t33_1 = _asm256_loadu_pd(L + 728);
  _t33_0 = _asm256_loadu_pd(L + 756);

  // Generating : X[28,28] = ( ( S(h(4, 28, 24), ( G(h(4, 28, 24), C[28,28],h(4, 28, 24)) - ( ( G(h(4, 28, 24), L[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) ) ) + ( G(h(4, 28, 24), X[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), L[28,28],h(4, 28, 0)) ) ) ) ),h(4, 28, 24)) + Sum_{k219} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), L[28,28],h(4, 28, k219)) * T( G(h(4, 28, 24), X[28,28],h(4, 28, k219)) ) ),h(4, 28, 24)) ) ) + Sum_{i100} ( -$(h(4, 28, 24), ( G(h(4, 28, 24), X[28,28],h(4, 28, i100)) * T( G(h(4, 28, 24), L[28,28],h(4, 28, i100)) ) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t33_40 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t33_24, _t33_25, 0), _mm256_shuffle_pd(_t33_26, _t33_27, 0), 32);
  _t33_41 = _mm256_permute2f128_pd(_t33_25, _mm256_shuffle_pd(_t33_26, _t33_27, 3), 32);
  _t33_42 = _mm256_blend_pd(_t33_26, _mm256_shuffle_pd(_t33_26, _t33_27, 3), 12);
  _t33_43 = _t33_27;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t33_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_unpacklo_pd(_t29_20, _t29_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_22, _t29_23), _mm256_unpacklo_pd(_t29_24, _t29_25), 32)), 32);
  _t33_45 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_unpacklo_pd(_t29_20, _t29_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_22, _t29_23), _mm256_unpacklo_pd(_t29_24, _t29_25), 32)), 32);
  _t33_46 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_unpacklo_pd(_t29_20, _t29_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_22, _t29_23), _mm256_unpacklo_pd(_t29_24, _t29_25), 32)), 49);
  _t33_47 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_7, _t29_8), _mm256_unpacklo_pd(_t29_9, _t29_10), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_14, _t29_15), _mm256_unpacklo_pd(_t29_16, _t29_17), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_18, _t29_19), _mm256_unpacklo_pd(_t29_20, _t29_21), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_22, _t29_23), _mm256_unpacklo_pd(_t29_24, _t29_25), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t33_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_19, _t33_44), _mm256_mul_pd(_t33_18, _t33_45)), _mm256_add_pd(_mm256_mul_pd(_t33_17, _t33_46), _mm256_mul_pd(_t33_16, _t33_47)));
  _t33_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_15, _t33_44), _mm256_mul_pd(_t33_14, _t33_45)), _mm256_add_pd(_mm256_mul_pd(_t33_13, _t33_46), _mm256_mul_pd(_t33_12, _t33_47)));
  _t33_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_11, _t33_44), _mm256_mul_pd(_t33_10, _t33_45)), _mm256_add_pd(_mm256_mul_pd(_t33_9, _t33_46), _mm256_mul_pd(_t33_8, _t33_47)));
  _t33_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_7, _t33_44), _mm256_mul_pd(_t33_6, _t33_45)), _mm256_add_pd(_mm256_mul_pd(_t33_5, _t33_46), _mm256_mul_pd(_t33_4, _t33_47)));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t33_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
  _t33_49 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
  _t33_50 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
  _t33_51 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t33_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 32), _mm256_permute2f128_pd(_t29_7, _t29_7, 32), 0), _t33_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_8, _t29_8, 32), _mm256_permute2f128_pd(_t29_8, _t29_8, 32), 0), _t33_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_9, _t29_9, 32), _mm256_permute2f128_pd(_t29_9, _t29_9, 32), 0), _t33_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_10, _t29_10, 32), _mm256_permute2f128_pd(_t29_10, _t29_10, 32), 0), _t33_51)));
  _t33_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_14, _t29_14, 32), _mm256_permute2f128_pd(_t29_14, _t29_14, 32), 0), _t33_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_15, _t29_15, 32), _mm256_permute2f128_pd(_t29_15, _t29_15, 32), 0), _t33_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_16, _t29_16, 32), _mm256_permute2f128_pd(_t29_16, _t29_16, 32), 0), _t33_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_17, _t29_17, 32), _mm256_permute2f128_pd(_t29_17, _t29_17, 32), 0), _t33_51)));
  _t33_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_18, _t29_18, 32), _mm256_permute2f128_pd(_t29_18, _t29_18, 32), 0), _t33_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_19, _t29_19, 32), _mm256_permute2f128_pd(_t29_19, _t29_19, 32), 0), _t33_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_20, _t29_20, 32), _mm256_permute2f128_pd(_t29_20, _t29_20, 32), 0), _t33_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_21, _t29_21, 32), _mm256_permute2f128_pd(_t29_21, _t29_21, 32), 0), _t33_51)));
  _t33_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_22, _t29_22, 32), _mm256_permute2f128_pd(_t29_22, _t29_22, 32), 0), _t33_48), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_23, _t29_23, 32), _mm256_permute2f128_pd(_t29_23, _t29_23, 32), 0), _t33_49)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_24, _t29_24, 32), _mm256_permute2f128_pd(_t29_24, _t29_24, 32), 0), _t33_50), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_25, _t29_25, 32), _mm256_permute2f128_pd(_t29_25, _t29_25, 32), 0), _t33_51)));

  // 4-BLAC: 4x4 + 4x4
  _t33_20 = _mm256_add_pd(_t33_28, _t33_32);
  _t33_21 = _mm256_add_pd(_t33_29, _t33_33);
  _t33_22 = _mm256_add_pd(_t33_30, _t33_34);
  _t33_23 = _mm256_add_pd(_t33_31, _t33_35);

  // 4-BLAC: 4x4 - 4x4
  _t33_36 = _mm256_sub_pd(_t33_40, _t33_20);
  _t33_37 = _mm256_sub_pd(_t33_41, _t33_21);
  _t33_38 = _mm256_sub_pd(_t33_42, _t33_22);
  _t33_39 = _mm256_sub_pd(_t33_43, _t33_23);

  // AVX Storer:

  // 4x4 -> 4x4 - LowSymm
  _t33_24 = _t33_36;
  _t33_25 = _t33_37;
  _t33_26 = _t33_38;
  _t33_27 = _t33_39;


  for( int k219 = 4; k219 <= 23; k219+=4 ) {
    _t34_19 = _mm256_broadcast_sd(L + k219 + 672);
    _t34_18 = _mm256_broadcast_sd(L + k219 + 673);
    _t34_17 = _mm256_broadcast_sd(L + k219 + 674);
    _t34_16 = _mm256_broadcast_sd(L + k219 + 675);
    _t34_15 = _mm256_broadcast_sd(L + k219 + 700);
    _t34_14 = _mm256_broadcast_sd(L + k219 + 701);
    _t34_13 = _mm256_broadcast_sd(L + k219 + 702);
    _t34_12 = _mm256_broadcast_sd(L + k219 + 703);
    _t34_11 = _mm256_broadcast_sd(L + k219 + 728);
    _t34_10 = _mm256_broadcast_sd(L + k219 + 729);
    _t34_9 = _mm256_broadcast_sd(L + k219 + 730);
    _t34_8 = _mm256_broadcast_sd(L + k219 + 731);
    _t34_7 = _mm256_broadcast_sd(L + k219 + 756);
    _t34_6 = _mm256_broadcast_sd(L + k219 + 757);
    _t34_5 = _mm256_broadcast_sd(L + k219 + 758);
    _t34_4 = _mm256_broadcast_sd(L + k219 + 759);
    _t34_3 = _asm256_loadu_pd(C + k219 + 672);
    _t34_2 = _asm256_loadu_pd(C + k219 + 700);
    _t34_1 = _asm256_loadu_pd(C + k219 + 728);
    _t34_0 = _asm256_loadu_pd(C + k219 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t34_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
    _t34_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
    _t34_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
    _t34_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t34_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_19, _t34_28), _mm256_mul_pd(_t34_18, _t34_29)), _mm256_add_pd(_mm256_mul_pd(_t34_17, _t34_30), _mm256_mul_pd(_t34_16, _t34_31)));
    _t34_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_15, _t34_28), _mm256_mul_pd(_t34_14, _t34_29)), _mm256_add_pd(_mm256_mul_pd(_t34_13, _t34_30), _mm256_mul_pd(_t34_12, _t34_31)));
    _t34_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_11, _t34_28), _mm256_mul_pd(_t34_10, _t34_29)), _mm256_add_pd(_mm256_mul_pd(_t34_9, _t34_30), _mm256_mul_pd(_t34_8, _t34_31)));
    _t34_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_7, _t34_28), _mm256_mul_pd(_t34_6, _t34_29)), _mm256_add_pd(_mm256_mul_pd(_t34_5, _t34_30), _mm256_mul_pd(_t34_4, _t34_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t34_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t33_24, _t33_25, 0), _mm256_shuffle_pd(_t33_26, _t33_27, 0), 32);
    _t34_25 = _mm256_permute2f128_pd(_t33_25, _mm256_shuffle_pd(_t33_26, _t33_27, 3), 32);
    _t34_26 = _mm256_blend_pd(_t33_26, _mm256_shuffle_pd(_t33_26, _t33_27, 3), 12);
    _t34_27 = _t33_27;

    // 4-BLAC: 4x4 - 4x4
    _t34_24 = _mm256_sub_pd(_t34_24, _t34_20);
    _t34_25 = _mm256_sub_pd(_t34_25, _t34_21);
    _t34_26 = _mm256_sub_pd(_t34_26, _t34_22);
    _t34_27 = _mm256_sub_pd(_t34_27, _t34_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t33_24 = _t34_24;
    _t33_25 = _t34_25;
    _t33_26 = _t34_26;
    _t33_27 = _t34_27;
  }


  for( int i100 = 4; i100 <= 23; i100+=4 ) {
    _t35_19 = _mm256_broadcast_sd(C + i100 + 672);
    _t35_18 = _mm256_broadcast_sd(C + i100 + 673);
    _t35_17 = _mm256_broadcast_sd(C + i100 + 674);
    _t35_16 = _mm256_broadcast_sd(C + i100 + 675);
    _t35_15 = _mm256_broadcast_sd(C + i100 + 700);
    _t35_14 = _mm256_broadcast_sd(C + i100 + 701);
    _t35_13 = _mm256_broadcast_sd(C + i100 + 702);
    _t35_12 = _mm256_broadcast_sd(C + i100 + 703);
    _t35_11 = _mm256_broadcast_sd(C + i100 + 728);
    _t35_10 = _mm256_broadcast_sd(C + i100 + 729);
    _t35_9 = _mm256_broadcast_sd(C + i100 + 730);
    _t35_8 = _mm256_broadcast_sd(C + i100 + 731);
    _t35_7 = _mm256_broadcast_sd(C + i100 + 756);
    _t35_6 = _mm256_broadcast_sd(C + i100 + 757);
    _t35_5 = _mm256_broadcast_sd(C + i100 + 758);
    _t35_4 = _mm256_broadcast_sd(C + i100 + 759);
    _t35_3 = _asm256_loadu_pd(L + i100 + 672);
    _t35_2 = _asm256_loadu_pd(L + i100 + 700);
    _t35_1 = _asm256_loadu_pd(L + i100 + 728);
    _t35_0 = _asm256_loadu_pd(L + i100 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t35_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
    _t35_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
    _t35_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
    _t35_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t35_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_28), _mm256_mul_pd(_t35_18, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_30), _mm256_mul_pd(_t35_16, _t35_31)));
    _t35_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_28), _mm256_mul_pd(_t35_14, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_30), _mm256_mul_pd(_t35_12, _t35_31)));
    _t35_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_28), _mm256_mul_pd(_t35_10, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_30), _mm256_mul_pd(_t35_8, _t35_31)));
    _t35_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_28), _mm256_mul_pd(_t35_6, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_30), _mm256_mul_pd(_t35_4, _t35_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t35_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t33_24, _t33_25, 0), _mm256_shuffle_pd(_t33_26, _t33_27, 0), 32);
    _t35_25 = _mm256_permute2f128_pd(_t33_25, _mm256_shuffle_pd(_t33_26, _t33_27, 3), 32);
    _t35_26 = _mm256_blend_pd(_t33_26, _mm256_shuffle_pd(_t33_26, _t33_27, 3), 12);
    _t35_27 = _t33_27;

    // 4-BLAC: 4x4 - 4x4
    _t35_24 = _mm256_sub_pd(_t35_24, _t35_20);
    _t35_25 = _mm256_sub_pd(_t35_25, _t35_21);
    _t35_26 = _mm256_sub_pd(_t35_26, _t35_22);
    _t35_27 = _mm256_sub_pd(_t35_27, _t35_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowSymm
    _t33_24 = _t35_24;
    _t33_25 = _t35_25;
    _t33_26 = _t35_26;
    _t33_27 = _t35_27;
  }

  _t36_5 = _mm256_castpd128_pd256(_mm_load_sd(&(L[724])));
  _t36_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 752)), _mm256_castpd128_pd256(_mm_load_sd(L + 780)), 0);
  _t36_3 = _mm256_maskload_pd(L + 752, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t36_2 = _mm256_maskload_pd(L + 780, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t36_1 = _mm256_castpd128_pd256(_mm_load_sd(&(L[782])));
  _t36_0 = _mm256_maskload_pd(L + 780, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : X[28,28] = S(h(1, 28, 24), ( G(h(1, 28, 24), X[28,28],h(1, 28, 24)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_19 = _t33_24;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t36_20 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_21 = _t29_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_22 = _mm256_mul_pd(_t36_20, _t36_21);

  // 4-BLAC: 1x4 / 1x4
  _t36_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_19), _mm256_castpd256_pd128(_t36_22)));

  // AVX Storer:
  _t33_24 = _t36_23;

  // Generating : X[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), X[28,28],h(1, 28, 24)) - ( G(h(3, 28, 25), L[28,28],h(1, 28, 24)) Kro G(h(1, 28, 24), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 3x1 -> 4x1
  _t36_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_25, _t33_26), _mm256_unpacklo_pd(_t33_27, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t36_25 = _t29_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_24, _t33_24, 32), _mm256_permute2f128_pd(_t33_24, _t33_24, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t36_27 = _mm256_mul_pd(_t36_25, _t36_26);

  // 4-BLAC: 4x1 - 4x1
  _t36_28 = _mm256_sub_pd(_t36_24, _t36_27);

  // AVX Storer:
  _t36_6 = _t36_28;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 25), L[28,28],h(1, 28, 25)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t36_6, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_30 = _t29_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_31 = _t29_6;

  // 4-BLAC: 1x4 + 1x4
  _t36_32 = _mm256_add_pd(_t36_30, _t36_31);

  // 4-BLAC: 1x4 / 1x4
  _t36_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_29), _mm256_castpd256_pd128(_t36_32)));

  // AVX Storer:
  _t36_7 = _t36_33;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 24)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) Kro G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 2x1 -> 4x1
  _t36_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t36_6, 2), _mm256_permute2f128_pd(_t36_6, _t36_6, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t36_35 = _t29_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t36_7, _t36_7, 32), _mm256_permute2f128_pd(_t36_7, _t36_7, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t36_37 = _mm256_mul_pd(_t36_35, _t36_36);

  // 4-BLAC: 4x1 - 4x1
  _t36_38 = _mm256_sub_pd(_t36_34, _t36_37);

  // AVX Storer:
  _t36_8 = _t36_38;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 25)) - ( ( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) + ( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_39 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t33_25, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_40 = _t36_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_41 = _t36_7;

  // 4-BLAC: (4x1)^T
  _t36_42 = _t36_41;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_43 = _mm256_mul_pd(_t36_40, _t36_42);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_44 = _t36_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_45 = _t36_5;

  // 4-BLAC: (4x1)^T
  _t36_46 = _t36_45;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_47 = _mm256_mul_pd(_t36_44, _t36_46);

  // 4-BLAC: 1x4 + 1x4
  _t36_48 = _mm256_add_pd(_t36_43, _t36_47);

  // 4-BLAC: 1x4 - 1x4
  _t36_49 = _mm256_sub_pd(_t36_39, _t36_48);

  // AVX Storer:
  _t36_9 = _t36_49;

  // Generating : X[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), X[28,28],h(1, 28, 25)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_50 = _t36_9;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t36_51 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_52 = _t29_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_53 = _mm256_mul_pd(_t36_51, _t36_52);

  // 4-BLAC: 1x4 / 1x4
  _t36_54 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_50), _mm256_castpd256_pd128(_t36_53)));

  // AVX Storer:
  _t36_9 = _t36_54;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), X[28,28],h(1, 28, 24)) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 2x1 -> 4x1
  _t36_55 = _mm256_unpackhi_pd(_mm256_blend_pd(_t33_26, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t33_27, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t36_56 = _t36_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t36_7, _t36_7, 32), _mm256_permute2f128_pd(_t36_7, _t36_7, 32), 0);

  // 4-BLAC: (4x1)^T
  _t36_58 = _t36_57;

  // 4-BLAC: 4x1 Kro 1x4
  _t36_59 = _mm256_mul_pd(_t36_56, _t36_58);

  // 4-BLAC: 4x1 - 4x1
  _t36_60 = _mm256_sub_pd(_t36_55, _t36_59);

  // AVX Storer:
  _t36_10 = _t36_60;

  // Generating : X[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(2, 28, 26), L[28,28],h(1, 28, 25)) Kro G(h(1, 28, 25), X[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 2x1 -> 4x1
  _t36_61 = _t36_10;

  // AVX Loader:

  // 2x1 -> 4x1
  _t36_62 = _t29_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_63 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t36_9, _t36_9, 32), _mm256_permute2f128_pd(_t36_9, _t36_9, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t36_64 = _mm256_mul_pd(_t36_62, _t36_63);

  // 4-BLAC: 4x1 - 4x1
  _t36_65 = _mm256_sub_pd(_t36_61, _t36_64);

  // AVX Storer:
  _t36_10 = _t36_65;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_66 = _mm256_blend_pd(_mm256_setzero_pd(), _t36_8, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_67 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_68 = _t29_6;

  // 4-BLAC: 1x4 + 1x4
  _t36_69 = _mm256_add_pd(_t36_67, _t36_68);

  // 4-BLAC: 1x4 / 1x4
  _t36_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_66), _mm256_castpd256_pd128(_t36_69)));

  // AVX Storer:
  _t36_11 = _t36_70;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 25)) - ( G(h(1, 28, 26), X[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_71 = _mm256_blend_pd(_mm256_setzero_pd(), _t36_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_72 = _t36_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_73 = _t36_5;

  // 4-BLAC: (4x1)^T
  _t36_74 = _t36_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_75 = _mm256_mul_pd(_t36_72, _t36_74);

  // 4-BLAC: 1x4 - 1x4
  _t36_76 = _mm256_sub_pd(_t36_71, _t36_75);

  // AVX Storer:
  _t36_12 = _t36_76;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 25)) Div ( G(h(1, 28, 26), L[28,28],h(1, 28, 26)) + G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_77 = _t36_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_78 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_79 = _t29_4;

  // 4-BLAC: 1x4 + 1x4
  _t36_80 = _mm256_add_pd(_t36_78, _t36_79);

  // 4-BLAC: 1x4 / 1x4
  _t36_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_77), _mm256_castpd256_pd128(_t36_80)));

  // AVX Storer:
  _t36_12 = _t36_81;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(2, 28, 24)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ),h(2, 28, 24))

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_82 = _mm256_unpackhi_pd(_mm256_blend_pd(_t36_8, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t36_10, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_83 = _t29_1;

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_84 = _mm256_blend_pd(_mm256_unpacklo_pd(_t36_11, _t36_12), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 1x4
  _t36_85 = _mm256_mul_pd(_t36_83, _t36_84);

  // 4-BLAC: 1x4 - 1x4
  _t36_86 = _mm256_sub_pd(_t36_82, _t36_85);

  // AVX Storer:
  _t36_13 = _t36_86;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 26)) - ( ( G(h(1, 28, 26), L[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) + ( G(h(1, 28, 26), X[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_87 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t33_26, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t33_26, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_88 = _t36_3;

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_89 = _mm256_blend_pd(_mm256_unpacklo_pd(_t36_11, _t36_12), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t36_90 = _t36_89;

  // 4-BLAC: 1x4 * 4x1
  _t36_91 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_88, _t36_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_88, _t36_90), _mm256_mul_pd(_t36_88, _t36_90), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_88, _t36_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_88, _t36_90), _mm256_mul_pd(_t36_88, _t36_90), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_88, _t36_90), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_88, _t36_90), _mm256_mul_pd(_t36_88, _t36_90), 129)), 1));

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_92 = _mm256_blend_pd(_mm256_unpacklo_pd(_t36_11, _t36_12), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_93 = _t36_3;

  // 4-BLAC: (1x4)^T
  _t36_94 = _t36_93;

  // 4-BLAC: 1x4 * 4x1
  _t36_95 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_92, _t36_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_92, _t36_94), _mm256_mul_pd(_t36_92, _t36_94), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_92, _t36_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_92, _t36_94), _mm256_mul_pd(_t36_92, _t36_94), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_92, _t36_94), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_92, _t36_94), _mm256_mul_pd(_t36_92, _t36_94), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t36_96 = _mm256_add_pd(_t36_91, _t36_95);

  // 4-BLAC: 1x4 - 1x4
  _t36_97 = _mm256_sub_pd(_t36_87, _t36_96);

  // AVX Storer:
  _t36_14 = _t36_97;

  // Generating : X[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), X[28,28],h(1, 28, 26)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_98 = _t36_14;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t36_99 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_100 = _t29_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_101 = _mm256_mul_pd(_t36_99, _t36_100);

  // 4-BLAC: 1x4 / 1x4
  _t36_102 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_98), _mm256_castpd256_pd128(_t36_101)));

  // AVX Storer:
  _t36_14 = _t36_102;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), L[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), X[28,28],h(2, 28, 24)) ) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_103 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t33_27, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t33_27, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_104 = _t36_2;

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_105 = _mm256_blend_pd(_mm256_unpacklo_pd(_t36_11, _t36_12), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t36_106 = _t36_105;

  // 4-BLAC: 1x4 * 4x1
  _t36_107 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_104, _t36_106), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_104, _t36_106), _mm256_mul_pd(_t36_104, _t36_106), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_104, _t36_106), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_104, _t36_106), _mm256_mul_pd(_t36_104, _t36_106), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_104, _t36_106), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_104, _t36_106), _mm256_mul_pd(_t36_104, _t36_106), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t36_108 = _mm256_sub_pd(_t36_103, _t36_107);

  // AVX Storer:
  _t36_15 = _t36_108;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), L[28,28],h(1, 28, 26)) Kro G(h(1, 28, 26), X[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_109 = _t36_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_110 = _t36_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_111 = _t36_14;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_112 = _mm256_mul_pd(_t36_110, _t36_111);

  // 4-BLAC: 1x4 - 1x4
  _t36_113 = _mm256_sub_pd(_t36_109, _t36_112);

  // AVX Storer:
  _t36_15 = _t36_113;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 24)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 24), L[28,28],h(1, 28, 24)) ) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_114 = _mm256_blend_pd(_mm256_setzero_pd(), _t36_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_115 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_116 = _t29_6;

  // 4-BLAC: 1x4 + 1x4
  _t36_117 = _mm256_add_pd(_t36_115, _t36_116);

  // 4-BLAC: 1x4 / 1x4
  _t36_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_114), _mm256_castpd256_pd128(_t36_117)));

  // AVX Storer:
  _t36_16 = _t36_118;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 25)) - ( G(h(1, 28, 27), X[28,28],h(1, 28, 24)) Kro T( G(h(1, 28, 25), L[28,28],h(1, 28, 24)) ) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_119 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t36_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_120 = _t36_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_121 = _t36_5;

  // 4-BLAC: (4x1)^T
  _t36_122 = _t36_121;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_123 = _mm256_mul_pd(_t36_120, _t36_122);

  // 4-BLAC: 1x4 - 1x4
  _t36_124 = _mm256_sub_pd(_t36_119, _t36_123);

  // AVX Storer:
  _t36_17 = _t36_124;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 25)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 25), L[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_125 = _t36_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_126 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_127 = _t29_4;

  // 4-BLAC: 1x4 + 1x4
  _t36_128 = _mm256_add_pd(_t36_126, _t36_127);

  // 4-BLAC: 1x4 / 1x4
  _t36_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_125), _mm256_castpd256_pd128(_t36_128)));

  // AVX Storer:
  _t36_17 = _t36_129;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) - ( G(h(1, 28, 27), X[28,28],h(2, 28, 24)) * T( G(h(1, 28, 26), L[28,28],h(2, 28, 24)) ) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_130 = _t36_15;

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_131 = _mm256_blend_pd(_mm256_unpacklo_pd(_t36_16, _t36_17), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 1x2 -> 1x4
  _t36_132 = _t36_3;

  // 4-BLAC: (1x4)^T
  _t36_133 = _t36_132;

  // 4-BLAC: 1x4 * 4x1
  _t36_134 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_131, _t36_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_131, _t36_133), _mm256_mul_pd(_t36_131, _t36_133), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_131, _t36_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_131, _t36_133), _mm256_mul_pd(_t36_131, _t36_133), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_131, _t36_133), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_131, _t36_133), _mm256_mul_pd(_t36_131, _t36_133), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t36_135 = _mm256_sub_pd(_t36_130, _t36_134);

  // AVX Storer:
  _t36_15 = _t36_135;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 26)) Div ( G(h(1, 28, 27), L[28,28],h(1, 28, 27)) + G(h(1, 28, 26), L[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_136 = _t36_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_137 = _t29_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_138 = _t29_2;

  // 4-BLAC: 1x4 + 1x4
  _t36_139 = _mm256_add_pd(_t36_137, _t36_138);

  // 4-BLAC: 1x4 / 1x4
  _t36_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_136), _mm256_castpd256_pd128(_t36_139)));

  // AVX Storer:
  _t36_15 = _t36_140;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 27)) - ( ( G(h(1, 28, 27), L[28,28],h(3, 28, 24)) * T( G(h(1, 28, 27), X[28,28],h(3, 28, 24)) ) ) + ( G(h(1, 28, 27), X[28,28],h(3, 28, 24)) * T( G(h(1, 28, 27), L[28,28],h(3, 28, 24)) ) ) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_141 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t33_27, _t33_27, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t36_142 = _t36_0;

  // AVX Loader:

  // 1x3 -> 1x4
  _t36_143 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_16, _t36_17), _mm256_unpacklo_pd(_t36_15, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t36_144 = _t36_143;

  // 4-BLAC: 1x4 * 4x1
  _t36_145 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_142, _t36_144), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_142, _t36_144), _mm256_mul_pd(_t36_142, _t36_144), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_142, _t36_144), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_142, _t36_144), _mm256_mul_pd(_t36_142, _t36_144), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_142, _t36_144), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_142, _t36_144), _mm256_mul_pd(_t36_142, _t36_144), 129)), 1));

  // AVX Loader:

  // 1x3 -> 1x4
  _t36_146 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_16, _t36_17), _mm256_unpacklo_pd(_t36_15, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t36_147 = _t36_0;

  // 4-BLAC: (1x4)^T
  _t36_148 = _t36_147;

  // 4-BLAC: 1x4 * 4x1
  _t36_149 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_146, _t36_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_146, _t36_148), _mm256_mul_pd(_t36_146, _t36_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_146, _t36_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_146, _t36_148), _mm256_mul_pd(_t36_146, _t36_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_146, _t36_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_146, _t36_148), _mm256_mul_pd(_t36_146, _t36_148), 129)), 1));

  // 4-BLAC: 1x4 + 1x4
  _t36_150 = _mm256_add_pd(_t36_145, _t36_149);

  // 4-BLAC: 1x4 - 1x4
  _t36_151 = _mm256_sub_pd(_t36_141, _t36_150);

  // AVX Storer:
  _t36_18 = _t36_151;

  // Generating : X[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), X[28,28],h(1, 28, 27)) Div ( G(h(1, 1, 0), 2[1,1],h(1, 1, 0)) Kro G(h(1, 28, 27), L[28,28],h(1, 28, 27)) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_152 = _t36_18;

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t36_153 = _mm256_set_pd(0, 0, 0, 2);

  // AVX Loader:

  // 1x1 -> 1x4
  _t36_154 = _t29_0;

  // 4-BLAC: 1x4 Kro 1x4
  _t36_155 = _mm256_mul_pd(_t36_153, _t36_154);

  // 4-BLAC: 1x4 / 1x4
  _t36_156 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t36_152), _mm256_castpd256_pd128(_t36_155)));

  // AVX Storer:
  _t36_18 = _t36_156;

  _mm_store_sd(&(C[0]), _mm256_castpd256_pd128(_t0_13));
  _mm_store_sd(&(C[28]), _mm256_castpd256_pd128(_t0_15));
  _mm_store_sd(&(C[29]), _mm256_castpd256_pd128(_t0_17));
  _mm_store_sd(&(C[56]), _mm256_castpd256_pd128(_t0_19));
  _mm_store_sd(&(C[57]), _mm256_castpd256_pd128(_t0_20));
  _mm_store_sd(&(C[58]), _mm256_castpd256_pd128(_t0_22));
  _mm_store_sd(&(C[84]), _mm256_castpd256_pd128(_t0_24));
  _mm_store_sd(&(C[85]), _mm256_castpd256_pd128(_t0_25));
  _mm_store_sd(&(C[86]), _mm256_castpd256_pd128(_t0_23));
  _mm_store_sd(&(C[87]), _mm256_castpd256_pd128(_t0_26));
  _mm_store_sd(&(C[112]), _mm256_castpd256_pd128(_t2_7));
  _mm_store_sd(&(C[113]), _mm256_castpd256_pd128(_t2_8));
  _mm_store_sd(&(C[114]), _mm256_castpd256_pd128(_t2_9));
  _mm_store_sd(&(C[115]), _mm256_castpd256_pd128(_t2_10));
  _mm_store_sd(&(C[140]), _mm256_castpd256_pd128(_t2_14));
  _mm_store_sd(&(C[141]), _mm256_castpd256_pd128(_t2_15));
  _mm_store_sd(&(C[142]), _mm256_castpd256_pd128(_t2_16));
  _mm_store_sd(&(C[143]), _mm256_castpd256_pd128(_t2_17));
  _mm_store_sd(&(C[168]), _mm256_castpd256_pd128(_t2_18));
  _mm_store_sd(&(C[169]), _mm256_castpd256_pd128(_t2_19));
  _mm_store_sd(&(C[170]), _mm256_castpd256_pd128(_t2_20));
  _mm_store_sd(&(C[171]), _mm256_castpd256_pd128(_t2_21));
  _mm_store_sd(&(C[196]), _mm256_castpd256_pd128(_t2_22));
  _mm_store_sd(&(C[197]), _mm256_castpd256_pd128(_t2_23));
  _mm_store_sd(&(C[198]), _mm256_castpd256_pd128(_t2_24));
  _mm_store_sd(&(C[199]), _mm256_castpd256_pd128(_t2_25));
  _mm_store_sd(C + 116, _mm256_castpd256_pd128(_t4_30));
  _mm_store_sd(&(C[144]), _mm256_castpd256_pd128(_t4_35));
  _mm_store_sd(&(C[145]), _mm256_castpd256_pd128(_t4_37));
  _mm_store_sd(&(C[172]), _mm256_castpd256_pd128(_t4_39));
  _mm_store_sd(&(C[173]), _mm256_castpd256_pd128(_t4_40));
  _mm_store_sd(&(C[174]), _mm256_castpd256_pd128(_t4_42));
  _mm_store_sd(&(C[200]), _mm256_castpd256_pd128(_t4_44));
  _mm_store_sd(&(C[201]), _mm256_castpd256_pd128(_t4_45));
  _mm_store_sd(&(C[202]), _mm256_castpd256_pd128(_t4_43));
  _mm_store_sd(&(C[203]), _mm256_castpd256_pd128(_t4_46));
  _mm_store_sd(C + 232, _mm256_castpd256_pd128(_t10_50));
  _mm_store_sd(&(C[260]), _mm256_castpd256_pd128(_t10_55));
  _mm_store_sd(&(C[261]), _mm256_castpd256_pd128(_t10_57));
  _mm_store_sd(&(C[288]), _mm256_castpd256_pd128(_t10_59));
  _mm_store_sd(&(C[289]), _mm256_castpd256_pd128(_t10_60));
  _mm_store_sd(&(C[290]), _mm256_castpd256_pd128(_t10_62));
  _mm_store_sd(&(C[316]), _mm256_castpd256_pd128(_t10_64));
  _mm_store_sd(&(C[317]), _mm256_castpd256_pd128(_t10_65));
  _mm_store_sd(&(C[318]), _mm256_castpd256_pd128(_t10_63));
  _mm_store_sd(&(C[319]), _mm256_castpd256_pd128(_t10_66));
  _mm_store_sd(&(C[672]), _mm256_castpd256_pd128(_t29_7));
  _mm_store_sd(&(C[673]), _mm256_castpd256_pd128(_t29_8));
  _mm_store_sd(&(C[674]), _mm256_castpd256_pd128(_t29_9));
  _mm_store_sd(&(C[675]), _mm256_castpd256_pd128(_t29_10));
  _mm_store_sd(&(C[700]), _mm256_castpd256_pd128(_t29_14));
  _mm_store_sd(&(C[701]), _mm256_castpd256_pd128(_t29_15));
  _mm_store_sd(&(C[702]), _mm256_castpd256_pd128(_t29_16));
  _mm_store_sd(&(C[703]), _mm256_castpd256_pd128(_t29_17));
  _mm_store_sd(&(C[728]), _mm256_castpd256_pd128(_t29_18));
  _mm_store_sd(&(C[729]), _mm256_castpd256_pd128(_t29_19));
  _mm_store_sd(&(C[730]), _mm256_castpd256_pd128(_t29_20));
  _mm_store_sd(&(C[731]), _mm256_castpd256_pd128(_t29_21));
  _mm_store_sd(&(C[756]), _mm256_castpd256_pd128(_t29_22));
  _mm_store_sd(&(C[757]), _mm256_castpd256_pd128(_t29_23));
  _mm_store_sd(&(C[758]), _mm256_castpd256_pd128(_t29_24));
  _mm_store_sd(&(C[759]), _mm256_castpd256_pd128(_t29_25));
  _mm_store_sd(C + 696, _mm256_castpd256_pd128(_t33_24));
  _mm_store_sd(&(C[724]), _mm256_castpd256_pd128(_t36_7));
  _mm_store_sd(&(C[725]), _mm256_castpd256_pd128(_t36_9));
  _mm_store_sd(&(C[752]), _mm256_castpd256_pd128(_t36_11));
  _mm_store_sd(&(C[753]), _mm256_castpd256_pd128(_t36_12));
  _mm_store_sd(&(C[754]), _mm256_castpd256_pd128(_t36_14));
  _mm_store_sd(&(C[780]), _mm256_castpd256_pd128(_t36_16));
  _mm_store_sd(&(C[781]), _mm256_castpd256_pd128(_t36_17));
  _mm_store_sd(&(C[782]), _mm256_castpd256_pd128(_t36_15));
  _mm_store_sd(&(C[783]), _mm256_castpd256_pd128(_t36_18));

}
