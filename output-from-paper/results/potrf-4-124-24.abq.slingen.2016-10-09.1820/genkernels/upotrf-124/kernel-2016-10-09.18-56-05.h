/*
 * upotrf_kernel.h
 *
Decl { {u'A': Symmetric[A, (124, 124), USMatAccess], 'T789': Matrix[T789, (1, 124), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'ldiv_ut_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'chol_u_ow_opt': {'m': 'm2.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'chol_u_ow_opt_m2_ldiv_ut_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 124, 0), A[124,124],h(1, 124, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 0), A[124,124],h(1, 124, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, 0), A[124,124],h(1, 124, 0)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(3, 124, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(3, 124, 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(1, 124, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(1, 124, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(1, 124, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(1, 124, 1)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 124, 1), A[124,124],h(1, 124, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 1), A[124,124],h(1, 124, 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(2, 124, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(2, 124, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(1, 124, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(2, 124, 2)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, 1), A[124,124],h(1, 124, 1)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(2, 124, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(2, 124, 2)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(1, 124, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(1, 124, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 0), A[124,124],h(1, 124, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 0), A[124,124],h(1, 124, 2)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 124, 2), A[124,124],h(1, 124, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 2), A[124,124],h(1, 124, 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(1, 124, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(1, 124, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 0), A[124,124],h(1, 124, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 0), A[124,124],h(1, 124, 3)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 124, 2), A[124,124],h(1, 124, 3)) ) = ( Tile( (1, 1), G(h(1, 124, 2), A[124,124],h(1, 124, 3)) ) Div Tile( (1, 1), G(h(1, 124, 2), A[124,124],h(1, 124, 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 3), A[124,124],h(1, 124, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 3), A[124,124],h(1, 124, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 124, 0), A[124,124],h(1, 124, 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 124, 0), A[124,124],h(1, 124, 3)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 124, 3), A[124,124],h(1, 124, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 3), A[124,124],h(1, 124, 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, 2), A[124,124],h(1, 124, 2)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, 3), A[124,124],h(1, 124, 3)) ) )
Eq.ann: {}
Entry 16:
For_{fi127;0;116;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(4, 124, fi127 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 124, 1), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 124, 1), A[124,124],h(4, 124, fi127 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(3, 124, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 0), A[124,124],h(4, 124, fi127 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(4, 124, fi127 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 2), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 2), A[124,124],h(4, 124, fi127 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(2, 124, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 1), A[124,124],h(4, 124, fi127 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(4, 124, fi127 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 3), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 3), A[124,124],h(4, 124, fi127 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(1, 124, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 2), A[124,124],h(4, 124, fi127 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 3), A[124,124],h(4, 124, fi127 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 3), A[124,124],h(4, 124, fi127 + 4)) ) ) )
Eq.ann: {}
 )Entry 17:
For_{fi2;4;119;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 124, fi2), A[124,124],h(4, 124, fi2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 124, fi2), A[124,124],h(4, 124, fi2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi2, 124, 0), A[124,124],h(4, 124, fi2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi2, 124, 0), A[124,124],h(4, 124, fi2)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 124, fi2), A[124,124],h(1, 124, fi2)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, fi2), A[124,124],h(1, 124, fi2)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, fi2), A[124,124],h(1, 124, fi2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(3, 124, fi2 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(3, 124, fi2 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(1, 124, fi2 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(1, 124, fi2 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(1, 124, fi2 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(1, 124, fi2 + 1)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 124, fi2 + 1), A[124,124],h(1, 124, fi2 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, fi2 + 1), A[124,124],h(1, 124, fi2 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(2, 124, fi2 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(2, 124, fi2 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(1, 124, fi2 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(2, 124, fi2 + 2)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, fi2 + 1), A[124,124],h(1, 124, fi2 + 1)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(2, 124, fi2 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(2, 124, fi2 + 2)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, fi2), A[124,124],h(1, 124, fi2 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 124, fi2), A[124,124],h(1, 124, fi2 + 2)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 2)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, fi2), A[124,124],h(1, 124, fi2 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 124, fi2), A[124,124],h(1, 124, fi2 + 3)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 3)) ) = ( Tile( (1, 1), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 3)) ) Div Tile( (1, 1), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 3), A[124,124],h(1, 124, fi2 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 3), A[124,124],h(1, 124, fi2 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 124, fi2), A[124,124],h(1, 124, fi2 + 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 124, fi2), A[124,124],h(1, 124, fi2 + 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 124, fi2 + 3), A[124,124],h(1, 124, fi2 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, fi2 + 3), A[124,124],h(1, 124, fi2 + 3)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 124, fi2), A[124,124],h(-fi2 + 120, 124, fi2 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 124, fi2), A[124,124],h(-fi2 + 120, 124, fi2 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi2, 124, 0), A[124,124],h(4, 124, fi2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi2, 124, 0), A[124,124],h(-fi2 + 120, 124, fi2 + 4)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, fi2 + 3), A[124,124],h(1, 124, fi2 + 3)) ) )
Eq.ann: {}
Entry 18:
For_{fi252;0;-fi2 + 116;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 124, fi2 + 1), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 124, fi2 + 1), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(3, 124, fi2 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 124, fi2 + 2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, fi2 + 2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(2, 124, fi2 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 1), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 3), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 3), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(1, 124, fi2 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 2), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 3), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, fi2 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, fi2 + 3), A[124,124],h(4, 124, fi2 + fi252 + 4)) ) ) )
Eq.ann: {}
 ) )Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 124, 120), A[124,124],h(4, 124, 120)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 124, 120), A[124,124],h(4, 124, 120)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(120, 124, 0), A[124,124],h(4, 124, 120)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(120, 124, 0), A[124,124],h(4, 124, 120)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 124, 120), A[124,124],h(1, 124, 120)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 120), A[124,124],h(1, 124, 120)) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, 120)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, 120), A[124,124],h(1, 124, 120)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 120), A[124,124],h(3, 124, 121)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 120)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 120), A[124,124],h(3, 124, 121)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 121), A[124,124],h(1, 124, 121)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 121), A[124,124],h(1, 124, 121)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 120), A[124,124],h(1, 124, 121)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 120), A[124,124],h(1, 124, 121)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 124, 121), A[124,124],h(1, 124, 121)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 121), A[124,124],h(1, 124, 121)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 121), A[124,124],h(2, 124, 122)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 121), A[124,124],h(2, 124, 122)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 120), A[124,124],h(1, 124, 121)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 120), A[124,124],h(2, 124, 122)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 1, 0), T789[1,124],h(1, 124, 121)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 124, 121), A[124,124],h(1, 124, 121)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 121), A[124,124],h(2, 124, 122)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T789[1,124],h(1, 124, 121)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 121), A[124,124],h(2, 124, 122)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 122), A[124,124],h(1, 124, 122)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 122), A[124,124],h(1, 124, 122)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 120), A[124,124],h(1, 124, 122)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 120), A[124,124],h(1, 124, 122)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 124, 122), A[124,124],h(1, 124, 122)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 122), A[124,124],h(1, 124, 122)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 122), A[124,124],h(1, 124, 123)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 122), A[124,124],h(1, 124, 123)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 120), A[124,124],h(1, 124, 122)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 124, 120), A[124,124],h(1, 124, 123)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 124, 122), A[124,124],h(1, 124, 123)) ) = ( Tile( (1, 1), G(h(1, 124, 122), A[124,124],h(1, 124, 123)) ) Div Tile( (1, 1), G(h(1, 124, 122), A[124,124],h(1, 124, 122)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 123), A[124,124],h(1, 124, 123)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 124, 123), A[124,124],h(1, 124, 123)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 124, 120), A[124,124],h(1, 124, 123)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 124, 120), A[124,124],h(1, 124, 123)) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 124, 123), A[124,124],h(1, 124, 123)) ) = Sqrt( Tile( (1, 1), G(h(1, 124, 123), A[124,124],h(1, 124, 123)) ) )
Eq.ann: {}
 *
 * Created on: 2016-10-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


#define PARAM0 124

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double * A)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84, _t2_85;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39,
	_t6_40, _t6_41, _t6_42, _t6_43, _t6_44, _t6_45, _t6_46, _t6_47,
	_t6_48, _t6_49, _t6_50, _t6_51, _t6_52, _t6_53, _t6_54, _t6_55,
	_t6_56, _t6_57, _t6_58, _t6_59, _t6_60, _t6_61, _t6_62, _t6_63,
	_t6_64, _t6_65, _t6_66, _t6_67, _t6_68, _t6_69, _t6_70, _t6_71,
	_t6_72, _t6_73, _t6_74, _t6_75, _t6_76, _t6_77, _t6_78, _t6_79,
	_t6_80, _t6_81, _t6_82, _t6_83, _t6_84, _t6_85, _t6_86, _t6_87,
	_t6_88, _t6_89, _t6_90, _t6_91, _t6_92, _t6_93, _t6_94, _t6_95,
	_t6_96, _t6_97, _t6_98, _t6_99, _t6_100, _t6_101;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27, _t8_28, _t8_29, _t8_30, _t8_31,
	_t8_32, _t8_33, _t8_34, _t8_35, _t8_36, _t8_37, _t8_38, _t8_39,
	_t8_40, _t8_41, _t8_42, _t8_43, _t8_44, _t8_45, _t8_46;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23,
	_t12_24, _t12_25, _t12_26, _t12_27, _t12_28, _t12_29, _t12_30, _t12_31,
	_t12_32, _t12_33, _t12_34, _t12_35, _t12_36, _t12_37, _t12_38, _t12_39,
	_t12_40, _t12_41, _t12_42, _t12_43, _t12_44, _t12_45, _t12_46, _t12_47,
	_t12_48, _t12_49, _t12_50, _t12_51;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31,
	_t19_32, _t19_33, _t19_34, _t19_35, _t19_36, _t19_37, _t19_38, _t19_39,
	_t19_40, _t19_41, _t19_42, _t19_43, _t19_44, _t19_45, _t19_46, _t19_47,
	_t19_48, _t19_49, _t19_50, _t19_51, _t19_52, _t19_53, _t19_54, _t19_55,
	_t19_56, _t19_57, _t19_58, _t19_59, _t19_60, _t19_61;

  _t0_0 = _mm256_maskload_pd(A, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_2 = _mm256_maskload_pd(A + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_3 = _mm256_maskload_pd(A + 125, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_4 = _mm256_maskload_pd(A + 126, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_6 = _mm256_maskload_pd(A + 250, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_7 = _mm256_maskload_pd(A + 251, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_8 = _mm256_maskload_pd(A + 375, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_14 = _mm256_loadu_pd(A + 4);
  _t0_11 = _mm256_loadu_pd(A + 128);
  _t0_12 = _mm256_loadu_pd(A + 252);
  _t0_13 = _mm256_loadu_pd(A + 376);

  // 1x1 -> 1x4
  _t0_76 = _t0_0;

  // 4-BLAC: sqrt(1x4)
  _t0_77 = _mm256_sqrt_pd(_t0_76);
  _t0_0 = _t0_77;

  // Constant 1x1 -> 1x4
  _t0_78 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_79 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_80 = _mm256_div_pd(_t0_78, _t0_79);
  _t0_1 = _t0_80;

  // 1x1 -> 1x4
  _t0_81 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // 1x3 -> 1x4
  _t0_82 = _t0_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_83 = _mm256_mul_pd(_t0_81, _t0_82);
  _t0_2 = _t0_83;

  // 1x1 -> 1x4
  _t0_84 = _t0_3;

  // 1x1 -> 1x4
  _t0_85 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_86 = _t0_85;

  // 1x1 -> 1x4
  _t0_87 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_88 = _mm256_mul_pd(_t0_86, _t0_87);

  // 4-BLAC: 1x4 - 1x4
  _t0_89 = _mm256_sub_pd(_t0_84, _t0_88);
  _t0_3 = _t0_89;

  // 1x1 -> 1x4
  _t0_90 = _t0_3;

  // 4-BLAC: sqrt(1x4)
  _t0_91 = _mm256_sqrt_pd(_t0_90);
  _t0_3 = _t0_91;

  // 1x2 -> 1x4
  _t0_92 = _t0_4;

  // 1x1 -> 1x4
  _t0_93 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_2, _t0_2, 32), _mm256_permute2f128_pd(_t0_2, _t0_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_94 = _t0_93;

  // 1x2 -> 1x4
  _t0_95 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 6), _mm256_permute2f128_pd(_t0_2, _t0_2, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_96 = _mm256_mul_pd(_t0_94, _t0_95);

  // 4-BLAC: 1x4 - 1x4
  _t0_97 = _mm256_sub_pd(_t0_92, _t0_96);
  _t0_4 = _t0_97;

  // Constant 1x1 -> 1x4
  _t0_98 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_99 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_100 = _mm256_div_pd(_t0_98, _t0_99);
  _t0_5 = _t0_100;

  // 1x1 -> 1x4
  _t0_101 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_5, _t0_5, 32), _mm256_permute2f128_pd(_t0_5, _t0_5, 32), 0);

  // 1x2 -> 1x4
  _t0_102 = _t0_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_103 = _mm256_mul_pd(_t0_101, _t0_102);
  _t0_4 = _t0_103;

  // 1x1 -> 1x4
  _t0_104 = _t0_6;

  // 2x1 -> 4x1
  _t0_105 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t0_106 = _t0_105;

  // 2x1 -> 4x1
  _t0_107 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t0_108 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_mul_pd(_t0_106, _t0_107), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_mul_pd(_t0_106, _t0_107), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_mul_pd(_t0_106, _t0_107), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_16 = _mm256_sub_pd(_t0_104, _t0_108);
  _t0_6 = _t0_16;

  // 1x1 -> 1x4
  _t0_17 = _t0_6;

  // 4-BLAC: sqrt(1x4)
  _t0_18 = _mm256_sqrt_pd(_t0_17);
  _t0_6 = _t0_18;

  // 1x1 -> 1x4
  _t0_19 = _t0_7;

  // 2x1 -> 4x1
  _t0_20 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t0_21 = _t0_20;

  // 2x1 -> 4x1
  _t0_22 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_2, 129), _t0_4, 2);

  // 4-BLAC: 1x4 * 4x1
  _t0_23 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_21, _t0_22), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_21, _t0_22), _mm256_mul_pd(_t0_21, _t0_22), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_21, _t0_22), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_21, _t0_22), _mm256_mul_pd(_t0_21, _t0_22), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_21, _t0_22), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_21, _t0_22), _mm256_mul_pd(_t0_21, _t0_22), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_24 = _mm256_sub_pd(_t0_19, _t0_23);
  _t0_7 = _t0_24;

  // 1x1 -> 1x4
  _t0_25 = _t0_7;

  // 1x1 -> 1x4
  _t0_26 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_27 = _mm256_div_pd(_t0_25, _t0_26);
  _t0_7 = _t0_27;

  // 1x1 -> 1x4
  _t0_28 = _t0_8;

  // 3x1 -> 4x1
  _t0_29 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // 4-BLAC: (4x1)^T
  _t0_30 = _t0_29;

  // 3x1 -> 4x1
  _t0_31 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t0_32 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_30, _t0_31), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_30, _t0_31), _mm256_mul_pd(_t0_30, _t0_31), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_30, _t0_31), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_30, _t0_31), _mm256_mul_pd(_t0_30, _t0_31), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_30, _t0_31), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_30, _t0_31), _mm256_mul_pd(_t0_30, _t0_31), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_33 = _mm256_sub_pd(_t0_28, _t0_32);
  _t0_8 = _t0_33;

  // 1x1 -> 1x4
  _t0_34 = _t0_8;

  // 4-BLAC: sqrt(1x4)
  _t0_35 = _mm256_sqrt_pd(_t0_34);
  _t0_8 = _t0_35;

  // Constant 1x1 -> 1x4
  _t0_36 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_37 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_38 = _mm256_div_pd(_t0_36, _t0_37);
  _t0_9 = _t0_38;

  // Constant 1x1 -> 1x4
  _t0_39 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_40 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_41 = _mm256_div_pd(_t0_39, _t0_40);
  _t0_10 = _t0_41;

  // 1x1 -> 1x4
  _t0_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_14 = _mm256_mul_pd(_t0_42, _t0_14);

  // 3x4 -> 4x4
  _t0_43 = _t0_11;
  _t0_44 = _t0_12;
  _t0_45 = _t0_13;
  _t0_46 = _mm256_setzero_pd();

  // 1x3 -> 1x4
  _t0_47 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t0_48 = _t0_47;

  // 4-BLAC: 4x1 * 1x4
  _t0_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t0_14);
  _t0_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 15), _t0_14);
  _t0_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 49), _mm256_permute2f128_pd(_t0_48, _t0_48, 49), 0), _t0_14);
  _t0_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 49), _mm256_permute2f128_pd(_t0_48, _t0_48, 49), 15), _t0_14);

  // 4-BLAC: 4x4 - 4x4
  _t0_53 = _mm256_sub_pd(_t0_43, _t0_49);
  _t0_54 = _mm256_sub_pd(_t0_44, _t0_50);
  _t0_55 = _mm256_sub_pd(_t0_45, _t0_51);
  _t0_56 = _mm256_sub_pd(_t0_46, _t0_52);
  _t0_11 = _t0_53;
  _t0_12 = _t0_54;
  _t0_13 = _t0_55;

  // 1x1 -> 1x4
  _t0_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_5, _t0_5, 32), _mm256_permute2f128_pd(_t0_5, _t0_5, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_11 = _mm256_mul_pd(_t0_57, _t0_11);

  // 2x4 -> 4x4
  _t0_58 = _t0_12;
  _t0_59 = _t0_13;
  _t0_60 = _mm256_setzero_pd();
  _t0_61 = _mm256_setzero_pd();

  // 1x2 -> 1x4
  _t0_62 = _t0_4;

  // 4-BLAC: (1x4)^T
  _t0_63 = _t0_62;

  // 4-BLAC: 4x1 * 1x4
  _t0_64 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0), _t0_11);
  _t0_65 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 15), _t0_11);
  _t0_66 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 49), _mm256_permute2f128_pd(_t0_63, _t0_63, 49), 0), _t0_11);
  _t0_67 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 49), _mm256_permute2f128_pd(_t0_63, _t0_63, 49), 15), _t0_11);

  // 4-BLAC: 4x4 - 4x4
  _t0_68 = _mm256_sub_pd(_t0_58, _t0_64);
  _t0_69 = _mm256_sub_pd(_t0_59, _t0_65);
  _t0_70 = _mm256_sub_pd(_t0_60, _t0_66);
  _t0_71 = _mm256_sub_pd(_t0_61, _t0_67);
  _t0_12 = _t0_68;
  _t0_13 = _t0_69;

  // 1x1 -> 1x4
  _t0_72 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_12 = _mm256_mul_pd(_t0_72, _t0_12);

  // 1x1 -> 1x4
  _t0_73 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_7, _t0_7, 32), _mm256_permute2f128_pd(_t0_7, _t0_7, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_74 = _t0_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_15 = _mm256_mul_pd(_t0_74, _t0_12);

  // 4-BLAC: 1x4 - 1x4
  _t0_13 = _mm256_sub_pd(_t0_13, _t0_15);

  // 1x1 -> 1x4
  _t0_75 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_10, _t0_10, 32), _mm256_permute2f128_pd(_t0_10, _t0_10, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_13 = _mm256_mul_pd(_t0_75, _t0_13);


  for( int fi127 = 4; fi127 <= 116; fi127+=4 ) {
    _t1_3 = _mm256_loadu_pd(A + fi127 + 4);
    _t1_0 = _mm256_loadu_pd(A + fi127 + 128);
    _t1_1 = _mm256_loadu_pd(A + fi127 + 252);
    _t1_2 = _mm256_loadu_pd(A + fi127 + 376);

    // 1x1 -> 1x4
    _t1_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t1_3 = _mm256_mul_pd(_t1_4, _t1_3);

    // 3x4 -> 4x4
    _t1_5 = _t1_0;
    _t1_6 = _t1_1;
    _t1_7 = _t1_2;
    _t1_8 = _mm256_setzero_pd();

    // 1x3 -> 1x4
    _t1_9 = _t0_2;

    // 4-BLAC: (1x4)^T
    _t0_48 = _t1_9;

    // 4-BLAC: 4x1 * 1x4
    _t0_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t1_3);
    _t0_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 15), _t1_3);
    _t0_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 49), _mm256_permute2f128_pd(_t0_48, _t0_48, 49), 0), _t1_3);
    _t0_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 49), _mm256_permute2f128_pd(_t0_48, _t0_48, 49), 15), _t1_3);

    // 4-BLAC: 4x4 - 4x4
    _t1_10 = _mm256_sub_pd(_t1_5, _t0_49);
    _t1_11 = _mm256_sub_pd(_t1_6, _t0_50);
    _t1_12 = _mm256_sub_pd(_t1_7, _t0_51);
    _t1_13 = _mm256_sub_pd(_t1_8, _t0_52);
    _t1_0 = _t1_10;
    _t1_1 = _t1_11;
    _t1_2 = _t1_12;

    // 1x1 -> 1x4
    _t1_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_5, _t0_5, 32), _mm256_permute2f128_pd(_t0_5, _t0_5, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t1_0 = _mm256_mul_pd(_t1_14, _t1_0);

    // 2x4 -> 4x4
    _t1_15 = _t1_1;
    _t1_16 = _t1_2;
    _t1_17 = _mm256_setzero_pd();
    _t1_18 = _mm256_setzero_pd();

    // 1x2 -> 1x4
    _t1_19 = _t0_4;

    // 4-BLAC: (1x4)^T
    _t0_63 = _t1_19;

    // 4-BLAC: 4x1 * 1x4
    _t0_64 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0), _t1_0);
    _t0_65 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 15), _t1_0);
    _t0_66 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 49), _mm256_permute2f128_pd(_t0_63, _t0_63, 49), 0), _t1_0);
    _t0_67 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 49), _mm256_permute2f128_pd(_t0_63, _t0_63, 49), 15), _t1_0);

    // 4-BLAC: 4x4 - 4x4
    _t1_20 = _mm256_sub_pd(_t1_15, _t0_64);
    _t1_21 = _mm256_sub_pd(_t1_16, _t0_65);
    _t1_22 = _mm256_sub_pd(_t1_17, _t0_66);
    _t1_23 = _mm256_sub_pd(_t1_18, _t0_67);
    _t1_1 = _t1_20;
    _t1_2 = _t1_21;

    // 1x1 -> 1x4
    _t1_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t1_1 = _mm256_mul_pd(_t1_24, _t1_1);

    // 1x1 -> 1x4
    _t1_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_7, _t0_7, 32), _mm256_permute2f128_pd(_t0_7, _t0_7, 32), 0);

    // 4-BLAC: (4x1)^T
    _t0_74 = _t1_25;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_15 = _mm256_mul_pd(_t0_74, _t1_1);

    // 4-BLAC: 1x4 - 1x4
    _t1_2 = _mm256_sub_pd(_t1_2, _t0_15);

    // 1x1 -> 1x4
    _t1_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_10, _t0_10, 32), _mm256_permute2f128_pd(_t0_10, _t0_10, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t1_2 = _mm256_mul_pd(_t1_26, _t1_2);
    _mm256_storeu_pd(A + fi127 + 4, _t1_3);
    _mm256_storeu_pd(A + fi127 + 128, _t1_0);
    _mm256_storeu_pd(A + fi127 + 252, _t1_1);
    _mm256_storeu_pd(A + fi127 + 376, _t1_2);
  }

  _t2_0 = _mm256_loadu_pd(A + 500);
  _t2_1 = _mm256_maskload_pd(A + 624, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t2_2 = _mm256_maskload_pd(A + 748, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t2_3 = _mm256_maskload_pd(A + 872, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // 4x4 -> 4x4 - UpSymm
  _t2_21 = _t2_0;
  _t2_22 = _mm256_blend_pd(_mm256_shuffle_pd(_t2_0, _t2_1, 3), _t2_1, 12);
  _t2_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_0, _t2_1, 0), _t2_2, 49);
  _t2_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_0, _t2_1, 12), _mm256_shuffle_pd(_t2_2, _t2_3, 12), 49);

  // 4-BLAC: (4x4)^T
  _t2_78 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 32);
  _t2_79 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_14, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 32);
  _t2_80 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 49);
  _t2_81 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_14, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 49);

  // 4-BLAC: 4x4 * 4x4
  _t2_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_78, _t2_78, 32), _mm256_permute2f128_pd(_t2_78, _t2_78, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_78, _t2_78, 32), _mm256_permute2f128_pd(_t2_78, _t2_78, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_78, _t2_78, 49), _mm256_permute2f128_pd(_t2_78, _t2_78, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_78, _t2_78, 49), _mm256_permute2f128_pd(_t2_78, _t2_78, 49), 15), _t0_13)));
  _t2_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_79, _t2_79, 32), _mm256_permute2f128_pd(_t2_79, _t2_79, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_79, _t2_79, 32), _mm256_permute2f128_pd(_t2_79, _t2_79, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_79, _t2_79, 49), _mm256_permute2f128_pd(_t2_79, _t2_79, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_79, _t2_79, 49), _mm256_permute2f128_pd(_t2_79, _t2_79, 49), 15), _t0_13)));
  _t2_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_80, _t2_80, 32), _mm256_permute2f128_pd(_t2_80, _t2_80, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_80, _t2_80, 32), _mm256_permute2f128_pd(_t2_80, _t2_80, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_80, _t2_80, 49), _mm256_permute2f128_pd(_t2_80, _t2_80, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_80, _t2_80, 49), _mm256_permute2f128_pd(_t2_80, _t2_80, 49), 15), _t0_13)));
  _t2_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_81, _t2_81, 32), _mm256_permute2f128_pd(_t2_81, _t2_81, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_81, _t2_81, 32), _mm256_permute2f128_pd(_t2_81, _t2_81, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_81, _t2_81, 49), _mm256_permute2f128_pd(_t2_81, _t2_81, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_81, _t2_81, 49), _mm256_permute2f128_pd(_t2_81, _t2_81, 49), 15), _t0_13)));

  // 4-BLAC: 4x4 - 4x4
  _t2_17 = _mm256_sub_pd(_t2_21, _t2_13);
  _t2_18 = _mm256_sub_pd(_t2_22, _t2_14);
  _t2_19 = _mm256_sub_pd(_t2_23, _t2_15);
  _t2_20 = _mm256_sub_pd(_t2_24, _t2_16);

  // 4x4 -> 4x4 - UpSymm
  _t2_0 = _t2_17;
  _t2_1 = _t2_18;
  _t2_2 = _t2_19;
  _t2_3 = _t2_20;

  // 1x1 -> 1x4
  _t2_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_0, 1);

  // 4-BLAC: sqrt(1x4)
  _t2_26 = _mm256_sqrt_pd(_t2_25);
  _t2_4 = _t2_26;

  // Constant 1x1 -> 1x4
  _t2_27 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t2_28 = _t2_4;

  // 4-BLAC: 1x4 / 1x4
  _t2_29 = _mm256_div_pd(_t2_27, _t2_28);
  _t2_5 = _t2_29;

  // 1x1 -> 1x4
  _t2_30 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 32), _mm256_permute2f128_pd(_t2_5, _t2_5, 32), 0);

  // 1x3 -> 1x4
  _t2_31 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_0, 14), _mm256_permute2f128_pd(_t2_0, _t2_0, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t2_32 = _mm256_mul_pd(_t2_30, _t2_31);
  _t2_6 = _t2_32;

  // 1x1 -> 1x4
  _t2_33 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_1, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t2_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_6, 1);

  // 4-BLAC: (4x1)^T
  _t2_35 = _t2_34;

  // 1x1 -> 1x4
  _t2_36 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_6, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t2_37 = _mm256_mul_pd(_t2_35, _t2_36);

  // 4-BLAC: 1x4 - 1x4
  _t2_38 = _mm256_sub_pd(_t2_33, _t2_37);
  _t2_7 = _t2_38;

  // 1x1 -> 1x4
  _t2_39 = _t2_7;

  // 4-BLAC: sqrt(1x4)
  _t2_40 = _mm256_sqrt_pd(_t2_39);
  _t2_7 = _t2_40;

  // 1x2 -> 1x4
  _t2_41 = _mm256_permute2f128_pd(_t2_1, _t2_1, 129);

  // 1x1 -> 1x4
  _t2_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_6, _t2_6, 32), _mm256_permute2f128_pd(_t2_6, _t2_6, 32), 0);

  // 4-BLAC: (4x1)^T
  _t2_43 = _t2_42;

  // 1x2 -> 1x4
  _t2_44 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_6, 6), _mm256_permute2f128_pd(_t2_6, _t2_6, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t2_45 = _mm256_mul_pd(_t2_43, _t2_44);

  // 4-BLAC: 1x4 - 1x4
  _t2_46 = _mm256_sub_pd(_t2_41, _t2_45);
  _t2_8 = _t2_46;

  // Constant 1x1 -> 1x4
  _t2_47 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t2_48 = _t2_7;

  // 4-BLAC: 1x4 / 1x4
  _t2_49 = _mm256_div_pd(_t2_47, _t2_48);
  _t2_9 = _t2_49;

  // 1x1 -> 1x4
  _t2_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_9, _t2_9, 32), _mm256_permute2f128_pd(_t2_9, _t2_9, 32), 0);

  // 1x2 -> 1x4
  _t2_51 = _t2_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_52 = _mm256_mul_pd(_t2_50, _t2_51);
  _t2_8 = _t2_52;

  // 1x1 -> 1x4
  _t2_53 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_2, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_2, 4), 129);

  // 2x1 -> 4x1
  _t2_54 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t2_55 = _t2_54;

  // 2x1 -> 4x1
  _t2_56 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t2_57 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_56), _mm256_mul_pd(_t2_55, _t2_56), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_56), _mm256_mul_pd(_t2_55, _t2_56), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_55, _t2_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_55, _t2_56), _mm256_mul_pd(_t2_55, _t2_56), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_58 = _mm256_sub_pd(_t2_53, _t2_57);
  _t2_10 = _t2_58;

  // 1x1 -> 1x4
  _t2_59 = _t2_10;

  // 4-BLAC: sqrt(1x4)
  _t2_60 = _mm256_sqrt_pd(_t2_59);
  _t2_10 = _t2_60;

  // 1x1 -> 1x4
  _t2_61 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_2, _t2_2, 129), _mm256_setzero_pd());

  // 2x1 -> 4x1
  _t2_62 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t2_63 = _t2_62;

  // 2x1 -> 4x1
  _t2_64 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_6, _t2_6, 129), _t2_8, 2);

  // 4-BLAC: 1x4 * 4x1
  _t2_65 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_63, _t2_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_63, _t2_64), _mm256_mul_pd(_t2_63, _t2_64), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_63, _t2_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_63, _t2_64), _mm256_mul_pd(_t2_63, _t2_64), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_63, _t2_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_63, _t2_64), _mm256_mul_pd(_t2_63, _t2_64), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_66 = _mm256_sub_pd(_t2_61, _t2_65);
  _t2_11 = _t2_66;

  // 1x1 -> 1x4
  _t2_67 = _t2_11;

  // 1x1 -> 1x4
  _t2_68 = _t2_10;

  // 4-BLAC: 1x4 / 1x4
  _t2_69 = _mm256_div_pd(_t2_67, _t2_68);
  _t2_11 = _t2_69;

  // 1x1 -> 1x4
  _t2_70 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_3, _t2_3, 129), _mm256_setzero_pd());

  // 3x1 -> 4x1
  _t2_71 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_6, _t2_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t2_8, 2), 10);

  // 4-BLAC: (4x1)^T
  _t2_72 = _t2_71;

  // 3x1 -> 4x1
  _t2_73 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_6, _t2_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t2_8, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t2_74 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_72, _t2_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_72, _t2_73), _mm256_mul_pd(_t2_72, _t2_73), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_72, _t2_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_72, _t2_73), _mm256_mul_pd(_t2_72, _t2_73), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_72, _t2_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_72, _t2_73), _mm256_mul_pd(_t2_72, _t2_73), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_75 = _mm256_sub_pd(_t2_70, _t2_74);
  _t2_12 = _t2_75;

  // 1x1 -> 1x4
  _t2_76 = _t2_12;

  // 4-BLAC: sqrt(1x4)
  _t2_77 = _mm256_sqrt_pd(_t2_76);
  _t2_12 = _t2_77;

  // 4-BLAC: (4x4)^T
  _t2_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 32);
  _t2_83 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_14, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 32);
  _t2_84 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 49);
  _t2_85 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_14, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 49);


  for( int k81 = 0; k81 <= 115; k81+=4 ) {
    _t3_8 = _mm256_loadu_pd(A + k81 + 504);
    _t3_9 = _mm256_loadu_pd(A + k81 + 628);
    _t3_10 = _mm256_loadu_pd(A + k81 + 752);
    _t3_11 = _mm256_loadu_pd(A + k81 + 876);
    _t3_3 = _mm256_loadu_pd(A + k81 + 8);
    _t3_2 = _mm256_loadu_pd(A + k81 + 132);
    _t3_1 = _mm256_loadu_pd(A + k81 + 256);
    _t3_0 = _mm256_loadu_pd(A + k81 + 380);

    // 4-BLAC: 4x4 * 4x4
    _t3_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_82, _t2_82, 32), _mm256_permute2f128_pd(_t2_82, _t2_82, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_82, _t2_82, 32), _mm256_permute2f128_pd(_t2_82, _t2_82, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_82, _t2_82, 49), _mm256_permute2f128_pd(_t2_82, _t2_82, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_82, _t2_82, 49), _mm256_permute2f128_pd(_t2_82, _t2_82, 49), 15), _t3_0)));
    _t3_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_83, _t2_83, 32), _mm256_permute2f128_pd(_t2_83, _t2_83, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_83, _t2_83, 32), _mm256_permute2f128_pd(_t2_83, _t2_83, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_83, _t2_83, 49), _mm256_permute2f128_pd(_t2_83, _t2_83, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_83, _t2_83, 49), _mm256_permute2f128_pd(_t2_83, _t2_83, 49), 15), _t3_0)));
    _t3_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_84, _t2_84, 32), _mm256_permute2f128_pd(_t2_84, _t2_84, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_84, _t2_84, 32), _mm256_permute2f128_pd(_t2_84, _t2_84, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_84, _t2_84, 49), _mm256_permute2f128_pd(_t2_84, _t2_84, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_84, _t2_84, 49), _mm256_permute2f128_pd(_t2_84, _t2_84, 49), 15), _t3_0)));
    _t3_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_85, _t2_85, 32), _mm256_permute2f128_pd(_t2_85, _t2_85, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_85, _t2_85, 32), _mm256_permute2f128_pd(_t2_85, _t2_85, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_85, _t2_85, 49), _mm256_permute2f128_pd(_t2_85, _t2_85, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_85, _t2_85, 49), _mm256_permute2f128_pd(_t2_85, _t2_85, 49), 15), _t3_0)));

    // 4-BLAC: 4x4 - 4x4
    _t3_8 = _mm256_sub_pd(_t3_8, _t3_4);
    _t3_9 = _mm256_sub_pd(_t3_9, _t3_5);
    _t3_10 = _mm256_sub_pd(_t3_10, _t3_6);
    _t3_11 = _mm256_sub_pd(_t3_11, _t3_7);
    _mm256_storeu_pd(A + k81 + 504, _t3_8);
    _mm256_storeu_pd(A + k81 + 628, _t3_9);
    _mm256_storeu_pd(A + k81 + 752, _t3_10);
    _mm256_storeu_pd(A + k81 + 876, _t3_11);
  }

  _t4_5 = _mm256_loadu_pd(A + 504);
  _t4_2 = _mm256_loadu_pd(A + 628);
  _t4_3 = _mm256_loadu_pd(A + 752);
  _t4_4 = _mm256_loadu_pd(A + 876);

  // Constant 1x1 -> 1x4
  _t4_7 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t4_8 = _t2_10;

  // 4-BLAC: 1x4 / 1x4
  _t4_9 = _mm256_div_pd(_t4_7, _t4_8);
  _t4_0 = _t4_9;

  // Constant 1x1 -> 1x4
  _t4_10 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t4_11 = _t2_12;

  // 4-BLAC: 1x4 / 1x4
  _t4_12 = _mm256_div_pd(_t4_10, _t4_11);
  _t4_1 = _t4_12;

  // 1x1 -> 1x4
  _t4_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 32), _mm256_permute2f128_pd(_t2_5, _t2_5, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t4_5 = _mm256_mul_pd(_t4_13, _t4_5);

  // 3x4 -> 4x4
  _t4_14 = _t4_2;
  _t4_15 = _t4_3;
  _t4_16 = _t4_4;
  _t4_17 = _mm256_setzero_pd();

  // 1x3 -> 1x4
  _t4_18 = _t2_6;

  // 4-BLAC: (1x4)^T
  _t4_19 = _t4_18;

  // 4-BLAC: 4x1 * 1x4
  _t4_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 32), _mm256_permute2f128_pd(_t4_19, _t4_19, 32), 0), _t4_5);
  _t4_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 32), _mm256_permute2f128_pd(_t4_19, _t4_19, 32), 15), _t4_5);
  _t4_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 49), _mm256_permute2f128_pd(_t4_19, _t4_19, 49), 0), _t4_5);
  _t4_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 49), _mm256_permute2f128_pd(_t4_19, _t4_19, 49), 15), _t4_5);

  // 4-BLAC: 4x4 - 4x4
  _t4_24 = _mm256_sub_pd(_t4_14, _t4_20);
  _t4_25 = _mm256_sub_pd(_t4_15, _t4_21);
  _t4_26 = _mm256_sub_pd(_t4_16, _t4_22);
  _t4_27 = _mm256_sub_pd(_t4_17, _t4_23);
  _t4_2 = _t4_24;
  _t4_3 = _t4_25;
  _t4_4 = _t4_26;

  // 1x1 -> 1x4
  _t4_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_9, _t2_9, 32), _mm256_permute2f128_pd(_t2_9, _t2_9, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t4_2 = _mm256_mul_pd(_t4_28, _t4_2);

  // 2x4 -> 4x4
  _t4_29 = _t4_3;
  _t4_30 = _t4_4;
  _t4_31 = _mm256_setzero_pd();
  _t4_32 = _mm256_setzero_pd();

  // 1x2 -> 1x4
  _t4_33 = _t2_8;

  // 4-BLAC: (1x4)^T
  _t4_34 = _t4_33;

  // 4-BLAC: 4x1 * 1x4
  _t4_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 0), _t4_2);
  _t4_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 15), _t4_2);
  _t4_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 49), _mm256_permute2f128_pd(_t4_34, _t4_34, 49), 0), _t4_2);
  _t4_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 49), _mm256_permute2f128_pd(_t4_34, _t4_34, 49), 15), _t4_2);

  // 4-BLAC: 4x4 - 4x4
  _t4_39 = _mm256_sub_pd(_t4_29, _t4_35);
  _t4_40 = _mm256_sub_pd(_t4_30, _t4_36);
  _t4_41 = _mm256_sub_pd(_t4_31, _t4_37);
  _t4_42 = _mm256_sub_pd(_t4_32, _t4_38);
  _t4_3 = _t4_39;
  _t4_4 = _t4_40;

  // 1x1 -> 1x4
  _t4_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_0, _t4_0, 32), _mm256_permute2f128_pd(_t4_0, _t4_0, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t4_3 = _mm256_mul_pd(_t4_43, _t4_3);

  // 1x1 -> 1x4
  _t4_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_11, _t2_11, 32), _mm256_permute2f128_pd(_t2_11, _t2_11, 32), 0);

  // 4-BLAC: (4x1)^T
  _t4_45 = _t4_44;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_6 = _mm256_mul_pd(_t4_45, _t4_3);

  // 4-BLAC: 1x4 - 1x4
  _t4_4 = _mm256_sub_pd(_t4_4, _t4_6);

  // 1x1 -> 1x4
  _t4_46 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_1, _t4_1, 32), _mm256_permute2f128_pd(_t4_1, _t4_1, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t4_4 = _mm256_mul_pd(_t4_46, _t4_4);

  _mm256_storeu_pd(A + 500, _t2_0);
  _mm256_maskstore_pd(A + 624, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t2_1);
  _mm256_maskstore_pd(A + 748, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t2_2);

  for( int fi252 = 4; fi252 <= 112; fi252+=4 ) {
    _t5_3 = _mm256_loadu_pd(A + fi252 + 504);
    _t5_0 = _mm256_loadu_pd(A + fi252 + 628);
    _t5_1 = _mm256_loadu_pd(A + fi252 + 752);
    _t5_2 = _mm256_loadu_pd(A + fi252 + 876);

    // 1x1 -> 1x4
    _t5_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 32), _mm256_permute2f128_pd(_t2_5, _t2_5, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t5_3 = _mm256_mul_pd(_t5_4, _t5_3);

    // 3x4 -> 4x4
    _t5_5 = _t5_0;
    _t5_6 = _t5_1;
    _t5_7 = _t5_2;
    _t5_8 = _mm256_setzero_pd();

    // 1x3 -> 1x4
    _t5_9 = _t2_6;

    // 4-BLAC: (1x4)^T
    _t4_19 = _t5_9;

    // 4-BLAC: 4x1 * 1x4
    _t4_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 32), _mm256_permute2f128_pd(_t4_19, _t4_19, 32), 0), _t5_3);
    _t4_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 32), _mm256_permute2f128_pd(_t4_19, _t4_19, 32), 15), _t5_3);
    _t4_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 49), _mm256_permute2f128_pd(_t4_19, _t4_19, 49), 0), _t5_3);
    _t4_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_19, _t4_19, 49), _mm256_permute2f128_pd(_t4_19, _t4_19, 49), 15), _t5_3);

    // 4-BLAC: 4x4 - 4x4
    _t5_10 = _mm256_sub_pd(_t5_5, _t4_20);
    _t5_11 = _mm256_sub_pd(_t5_6, _t4_21);
    _t5_12 = _mm256_sub_pd(_t5_7, _t4_22);
    _t5_13 = _mm256_sub_pd(_t5_8, _t4_23);
    _t5_0 = _t5_10;
    _t5_1 = _t5_11;
    _t5_2 = _t5_12;

    // 1x1 -> 1x4
    _t5_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_9, _t2_9, 32), _mm256_permute2f128_pd(_t2_9, _t2_9, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t5_0 = _mm256_mul_pd(_t5_14, _t5_0);

    // 2x4 -> 4x4
    _t5_15 = _t5_1;
    _t5_16 = _t5_2;
    _t5_17 = _mm256_setzero_pd();
    _t5_18 = _mm256_setzero_pd();

    // 1x2 -> 1x4
    _t5_19 = _t2_8;

    // 4-BLAC: (1x4)^T
    _t4_34 = _t5_19;

    // 4-BLAC: 4x1 * 1x4
    _t4_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 0), _t5_0);
    _t4_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 15), _t5_0);
    _t4_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 49), _mm256_permute2f128_pd(_t4_34, _t4_34, 49), 0), _t5_0);
    _t4_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 49), _mm256_permute2f128_pd(_t4_34, _t4_34, 49), 15), _t5_0);

    // 4-BLAC: 4x4 - 4x4
    _t5_20 = _mm256_sub_pd(_t5_15, _t4_35);
    _t5_21 = _mm256_sub_pd(_t5_16, _t4_36);
    _t5_22 = _mm256_sub_pd(_t5_17, _t4_37);
    _t5_23 = _mm256_sub_pd(_t5_18, _t4_38);
    _t5_1 = _t5_20;
    _t5_2 = _t5_21;

    // 1x1 -> 1x4
    _t5_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_0, _t4_0, 32), _mm256_permute2f128_pd(_t4_0, _t4_0, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t5_1 = _mm256_mul_pd(_t5_24, _t5_1);

    // 1x1 -> 1x4
    _t5_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_11, _t2_11, 32), _mm256_permute2f128_pd(_t2_11, _t2_11, 32), 0);

    // 4-BLAC: (4x1)^T
    _t4_45 = _t5_25;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_6 = _mm256_mul_pd(_t4_45, _t5_1);

    // 4-BLAC: 1x4 - 1x4
    _t5_2 = _mm256_sub_pd(_t5_2, _t4_6);

    // 1x1 -> 1x4
    _t5_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_1, _t4_1, 32), _mm256_permute2f128_pd(_t4_1, _t4_1, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t5_2 = _mm256_mul_pd(_t5_26, _t5_2);
    _mm256_storeu_pd(A + fi252 + 504, _t5_3);
    _mm256_storeu_pd(A + fi252 + 628, _t5_0);
    _mm256_storeu_pd(A + fi252 + 752, _t5_1);
    _mm256_storeu_pd(A + fi252 + 876, _t5_2);
  }

  _t6_4 = _mm256_loadu_pd(A + 1000);
  _t6_5 = _mm256_maskload_pd(A + 1124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t6_6 = _mm256_maskload_pd(A + 1248, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t6_7 = _mm256_maskload_pd(A + 1372, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t6_3 = _mm256_loadu_pd(A + 8);
  _t6_2 = _mm256_loadu_pd(A + 132);
  _t6_1 = _mm256_loadu_pd(A + 256);
  _t6_0 = _mm256_loadu_pd(A + 380);

  // 4x4 -> 4x4 - UpSymm
  _t6_29 = _t6_4;
  _t6_30 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_4, _t6_5, 3), _t6_5, 12);
  _t6_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_4, _t6_5, 0), _t6_6, 49);
  _t6_32 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_4, _t6_5, 12), _mm256_shuffle_pd(_t6_6, _t6_7, 12), 49);

  // 4-BLAC: (4x4)^T
  _t6_90 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
  _t6_91 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
  _t6_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
  _t6_93 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 32), _mm256_permute2f128_pd(_t6_90, _t6_90, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 32), _mm256_permute2f128_pd(_t6_90, _t6_90, 32), 15), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 49), _mm256_permute2f128_pd(_t6_90, _t6_90, 49), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 49), _mm256_permute2f128_pd(_t6_90, _t6_90, 49), 15), _t6_0)));
  _t6_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 32), _mm256_permute2f128_pd(_t6_91, _t6_91, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 32), _mm256_permute2f128_pd(_t6_91, _t6_91, 32), 15), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 49), _mm256_permute2f128_pd(_t6_91, _t6_91, 49), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 49), _mm256_permute2f128_pd(_t6_91, _t6_91, 49), 15), _t6_0)));
  _t6_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 32), _mm256_permute2f128_pd(_t6_92, _t6_92, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 32), _mm256_permute2f128_pd(_t6_92, _t6_92, 32), 15), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 49), _mm256_permute2f128_pd(_t6_92, _t6_92, 49), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 49), _mm256_permute2f128_pd(_t6_92, _t6_92, 49), 15), _t6_0)));
  _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 32), _mm256_permute2f128_pd(_t6_93, _t6_93, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 32), _mm256_permute2f128_pd(_t6_93, _t6_93, 32), 15), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 49), _mm256_permute2f128_pd(_t6_93, _t6_93, 49), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 49), _mm256_permute2f128_pd(_t6_93, _t6_93, 49), 15), _t6_0)));

  // 4-BLAC: 4x4 - 4x4
  _t6_25 = _mm256_sub_pd(_t6_29, _t6_17);
  _t6_26 = _mm256_sub_pd(_t6_30, _t6_18);
  _t6_27 = _mm256_sub_pd(_t6_31, _t6_19);
  _t6_28 = _mm256_sub_pd(_t6_32, _t6_20);

  // 4x4 -> 4x4 - UpSymm
  _t6_4 = _t6_25;
  _t6_5 = _t6_26;
  _t6_6 = _t6_27;
  _t6_7 = _t6_28;

  // 4-BLAC: (4x4)^T
  _t6_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_5, _t4_2), _mm256_unpacklo_pd(_t4_3, _t4_4), 32);
  _t6_95 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_5, _t4_2), _mm256_unpackhi_pd(_t4_3, _t4_4), 32);
  _t6_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_5, _t4_2), _mm256_unpacklo_pd(_t4_3, _t4_4), 49);
  _t6_97 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_5, _t4_2), _mm256_unpackhi_pd(_t4_3, _t4_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 32), _mm256_permute2f128_pd(_t6_94, _t6_94, 32), 0), _t4_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 32), _mm256_permute2f128_pd(_t6_94, _t6_94, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 49), _mm256_permute2f128_pd(_t6_94, _t6_94, 49), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 49), _mm256_permute2f128_pd(_t6_94, _t6_94, 49), 15), _t4_4)));
  _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 32), _mm256_permute2f128_pd(_t6_95, _t6_95, 32), 0), _t4_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 32), _mm256_permute2f128_pd(_t6_95, _t6_95, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 49), _mm256_permute2f128_pd(_t6_95, _t6_95, 49), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 49), _mm256_permute2f128_pd(_t6_95, _t6_95, 49), 15), _t4_4)));
  _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 32), _mm256_permute2f128_pd(_t6_96, _t6_96, 32), 0), _t4_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 32), _mm256_permute2f128_pd(_t6_96, _t6_96, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 49), _mm256_permute2f128_pd(_t6_96, _t6_96, 49), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 49), _mm256_permute2f128_pd(_t6_96, _t6_96, 49), 15), _t4_4)));
  _t6_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 32), _mm256_permute2f128_pd(_t6_97, _t6_97, 32), 0), _t4_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 32), _mm256_permute2f128_pd(_t6_97, _t6_97, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 49), _mm256_permute2f128_pd(_t6_97, _t6_97, 49), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 49), _mm256_permute2f128_pd(_t6_97, _t6_97, 49), 15), _t4_4)));

  // 4x4 -> 4x4 - UpSymm
  _t6_33 = _t6_4;
  _t6_34 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_4, _t6_5, 3), _t6_5, 12);
  _t6_35 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_4, _t6_5, 0), _t6_6, 49);
  _t6_36 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_4, _t6_5, 12), _mm256_shuffle_pd(_t6_6, _t6_7, 12), 49);

  // 4-BLAC: 4x4 - 4x4
  _t6_33 = _mm256_sub_pd(_t6_33, _t6_21);
  _t6_34 = _mm256_sub_pd(_t6_34, _t6_22);
  _t6_35 = _mm256_sub_pd(_t6_35, _t6_23);
  _t6_36 = _mm256_sub_pd(_t6_36, _t6_24);

  // 4x4 -> 4x4 - UpSymm
  _t6_4 = _t6_33;
  _t6_5 = _t6_34;
  _t6_6 = _t6_35;
  _t6_7 = _t6_36;

  // 1x1 -> 1x4
  _t6_37 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 1);

  // 4-BLAC: sqrt(1x4)
  _t6_38 = _mm256_sqrt_pd(_t6_37);
  _t6_8 = _t6_38;

  // Constant 1x1 -> 1x4
  _t6_39 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t6_40 = _t6_8;

  // 4-BLAC: 1x4 / 1x4
  _t6_41 = _mm256_div_pd(_t6_39, _t6_40);
  _t6_9 = _t6_41;

  // 1x1 -> 1x4
  _t6_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_9, _t6_9, 32), _mm256_permute2f128_pd(_t6_9, _t6_9, 32), 0);

  // 1x3 -> 1x4
  _t6_43 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 14), _mm256_permute2f128_pd(_t6_4, _t6_4, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t6_44 = _mm256_mul_pd(_t6_42, _t6_43);
  _t6_10 = _t6_44;

  // 1x1 -> 1x4
  _t6_45 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t6_46 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 1);

  // 4-BLAC: (4x1)^T
  _t6_47 = _t6_46;

  // 1x1 -> 1x4
  _t6_48 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t6_49 = _mm256_mul_pd(_t6_47, _t6_48);

  // 4-BLAC: 1x4 - 1x4
  _t6_50 = _mm256_sub_pd(_t6_45, _t6_49);
  _t6_11 = _t6_50;

  // 1x1 -> 1x4
  _t6_51 = _t6_11;

  // 4-BLAC: sqrt(1x4)
  _t6_52 = _mm256_sqrt_pd(_t6_51);
  _t6_11 = _t6_52;

  // 1x2 -> 1x4
  _t6_53 = _mm256_permute2f128_pd(_t6_5, _t6_5, 129);

  // 1x1 -> 1x4
  _t6_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_10, _t6_10, 32), _mm256_permute2f128_pd(_t6_10, _t6_10, 32), 0);

  // 4-BLAC: (4x1)^T
  _t6_55 = _t6_54;

  // 1x2 -> 1x4
  _t6_56 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 6), _mm256_permute2f128_pd(_t6_10, _t6_10, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t6_57 = _mm256_mul_pd(_t6_55, _t6_56);

  // 4-BLAC: 1x4 - 1x4
  _t6_58 = _mm256_sub_pd(_t6_53, _t6_57);
  _t6_12 = _t6_58;

  // Constant 1x1 -> 1x4
  _t6_59 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t6_60 = _t6_11;

  // 4-BLAC: 1x4 / 1x4
  _t6_61 = _mm256_div_pd(_t6_59, _t6_60);
  _t6_13 = _t6_61;

  // 1x1 -> 1x4
  _t6_62 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_13, _t6_13, 32), _mm256_permute2f128_pd(_t6_13, _t6_13, 32), 0);

  // 1x2 -> 1x4
  _t6_63 = _t6_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_64 = _mm256_mul_pd(_t6_62, _t6_63);
  _t6_12 = _t6_64;

  // 1x1 -> 1x4
  _t6_65 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 4), 129);

  // 2x1 -> 4x1
  _t6_66 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t6_67 = _t6_66;

  // 2x1 -> 4x1
  _t6_68 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t6_69 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_67, _t6_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_67, _t6_68), _mm256_mul_pd(_t6_67, _t6_68), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_67, _t6_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_67, _t6_68), _mm256_mul_pd(_t6_67, _t6_68), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_67, _t6_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_67, _t6_68), _mm256_mul_pd(_t6_67, _t6_68), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_70 = _mm256_sub_pd(_t6_65, _t6_69);
  _t6_14 = _t6_70;

  // 1x1 -> 1x4
  _t6_71 = _t6_14;

  // 4-BLAC: sqrt(1x4)
  _t6_72 = _mm256_sqrt_pd(_t6_71);
  _t6_14 = _t6_72;

  // 1x1 -> 1x4
  _t6_73 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_6, _t6_6, 129), _mm256_setzero_pd());

  // 2x1 -> 4x1
  _t6_74 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t6_75 = _t6_74;

  // 2x1 -> 4x1
  _t6_76 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_10, _t6_10, 129), _t6_12, 2);

  // 4-BLAC: 1x4 * 4x1
  _t6_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_75, _t6_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_75, _t6_76), _mm256_mul_pd(_t6_75, _t6_76), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_75, _t6_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_75, _t6_76), _mm256_mul_pd(_t6_75, _t6_76), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_75, _t6_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_75, _t6_76), _mm256_mul_pd(_t6_75, _t6_76), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_78 = _mm256_sub_pd(_t6_73, _t6_77);
  _t6_15 = _t6_78;

  // 1x1 -> 1x4
  _t6_79 = _t6_15;

  // 1x1 -> 1x4
  _t6_80 = _t6_14;

  // 4-BLAC: 1x4 / 1x4
  _t6_81 = _mm256_div_pd(_t6_79, _t6_80);
  _t6_15 = _t6_81;

  // 1x1 -> 1x4
  _t6_82 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_7, _t6_7, 129), _mm256_setzero_pd());

  // 3x1 -> 4x1
  _t6_83 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_10, _t6_15, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t6_12, 2), 10);

  // 4-BLAC: (4x1)^T
  _t6_84 = _t6_83;

  // 3x1 -> 4x1
  _t6_85 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_10, _t6_15, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t6_12, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t6_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_84, _t6_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_84, _t6_85), _mm256_mul_pd(_t6_84, _t6_85), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_84, _t6_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_84, _t6_85), _mm256_mul_pd(_t6_84, _t6_85), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_84, _t6_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_84, _t6_85), _mm256_mul_pd(_t6_84, _t6_85), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_87 = _mm256_sub_pd(_t6_82, _t6_86);
  _t6_16 = _t6_87;

  // 1x1 -> 1x4
  _t6_88 = _t6_16;

  // 4-BLAC: sqrt(1x4)
  _t6_89 = _mm256_sqrt_pd(_t6_88);
  _t6_16 = _t6_89;

  // 4-BLAC: (4x4)^T
  _t6_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
  _t6_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
  _t6_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
  _t6_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);


  for( int k81 = 0; k81 <= 111; k81+=4 ) {
    _t7_16 = _mm256_loadu_pd(A + k81 + 1004);
    _t7_17 = _mm256_loadu_pd(A + k81 + 1128);
    _t7_18 = _mm256_loadu_pd(A + k81 + 1252);
    _t7_19 = _mm256_loadu_pd(A + k81 + 1376);
    _t7_7 = _mm256_loadu_pd(A + k81 + 12);
    _t7_6 = _mm256_loadu_pd(A + k81 + 136);
    _t7_5 = _mm256_loadu_pd(A + k81 + 260);
    _t7_4 = _mm256_loadu_pd(A + k81 + 384);
    _t7_3 = _mm256_loadu_pd(A + k81 + 508);
    _t7_2 = _mm256_loadu_pd(A + k81 + 632);
    _t7_1 = _mm256_loadu_pd(A + k81 + 756);
    _t7_0 = _mm256_loadu_pd(A + k81 + 880);

    // 4-BLAC: (4x4)^T
    _t6_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
    _t6_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
    _t6_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
    _t6_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t7_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 32), _mm256_permute2f128_pd(_t6_98, _t6_98, 32), 0), _t7_7), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 32), _mm256_permute2f128_pd(_t6_98, _t6_98, 32), 15), _t7_6)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 49), _mm256_permute2f128_pd(_t6_98, _t6_98, 49), 0), _t7_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 49), _mm256_permute2f128_pd(_t6_98, _t6_98, 49), 15), _t7_4)));
    _t7_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 32), _mm256_permute2f128_pd(_t6_99, _t6_99, 32), 0), _t7_7), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 32), _mm256_permute2f128_pd(_t6_99, _t6_99, 32), 15), _t7_6)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 49), _mm256_permute2f128_pd(_t6_99, _t6_99, 49), 0), _t7_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 49), _mm256_permute2f128_pd(_t6_99, _t6_99, 49), 15), _t7_4)));
    _t7_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 32), _mm256_permute2f128_pd(_t6_100, _t6_100, 32), 0), _t7_7), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 32), _mm256_permute2f128_pd(_t6_100, _t6_100, 32), 15), _t7_6)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 49), _mm256_permute2f128_pd(_t6_100, _t6_100, 49), 0), _t7_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 49), _mm256_permute2f128_pd(_t6_100, _t6_100, 49), 15), _t7_4)));
    _t7_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 32), _mm256_permute2f128_pd(_t6_101, _t6_101, 32), 0), _t7_7), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 32), _mm256_permute2f128_pd(_t6_101, _t6_101, 32), 15), _t7_6)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 49), _mm256_permute2f128_pd(_t6_101, _t6_101, 49), 0), _t7_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 49), _mm256_permute2f128_pd(_t6_101, _t6_101, 49), 15), _t7_4)));

    // 4-BLAC: 4x4 - 4x4
    _t7_16 = _mm256_sub_pd(_t7_16, _t7_8);
    _t7_17 = _mm256_sub_pd(_t7_17, _t7_9);
    _t7_18 = _mm256_sub_pd(_t7_18, _t7_10);
    _t7_19 = _mm256_sub_pd(_t7_19, _t7_11);

    // 4-BLAC: (4x4)^T
    _t7_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_5, _t4_2), _mm256_unpacklo_pd(_t4_3, _t4_4), 32);
    _t7_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_5, _t4_2), _mm256_unpackhi_pd(_t4_3, _t4_4), 32);
    _t7_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_5, _t4_2), _mm256_unpacklo_pd(_t4_3, _t4_4), 49);
    _t7_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_5, _t4_2), _mm256_unpackhi_pd(_t4_3, _t4_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t7_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_20, _t7_20, 32), _mm256_permute2f128_pd(_t7_20, _t7_20, 32), 0), _t7_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_20, _t7_20, 32), _mm256_permute2f128_pd(_t7_20, _t7_20, 32), 15), _t7_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_20, _t7_20, 49), _mm256_permute2f128_pd(_t7_20, _t7_20, 49), 0), _t7_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_20, _t7_20, 49), _mm256_permute2f128_pd(_t7_20, _t7_20, 49), 15), _t7_0)));
    _t7_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_21, _t7_21, 32), _mm256_permute2f128_pd(_t7_21, _t7_21, 32), 0), _t7_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_21, _t7_21, 32), _mm256_permute2f128_pd(_t7_21, _t7_21, 32), 15), _t7_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_21, _t7_21, 49), _mm256_permute2f128_pd(_t7_21, _t7_21, 49), 0), _t7_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_21, _t7_21, 49), _mm256_permute2f128_pd(_t7_21, _t7_21, 49), 15), _t7_0)));
    _t7_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_22, _t7_22, 32), _mm256_permute2f128_pd(_t7_22, _t7_22, 32), 0), _t7_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_22, _t7_22, 32), _mm256_permute2f128_pd(_t7_22, _t7_22, 32), 15), _t7_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_22, _t7_22, 49), _mm256_permute2f128_pd(_t7_22, _t7_22, 49), 0), _t7_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_22, _t7_22, 49), _mm256_permute2f128_pd(_t7_22, _t7_22, 49), 15), _t7_0)));
    _t7_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_23, _t7_23, 32), _mm256_permute2f128_pd(_t7_23, _t7_23, 32), 0), _t7_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_23, _t7_23, 32), _mm256_permute2f128_pd(_t7_23, _t7_23, 32), 15), _t7_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_23, _t7_23, 49), _mm256_permute2f128_pd(_t7_23, _t7_23, 49), 0), _t7_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t7_23, _t7_23, 49), _mm256_permute2f128_pd(_t7_23, _t7_23, 49), 15), _t7_0)));

    // 4-BLAC: 4x4 - 4x4
    _t7_16 = _mm256_sub_pd(_t7_16, _t7_12);
    _t7_17 = _mm256_sub_pd(_t7_17, _t7_13);
    _t7_18 = _mm256_sub_pd(_t7_18, _t7_14);
    _t7_19 = _mm256_sub_pd(_t7_19, _t7_15);
    _mm256_storeu_pd(A + k81 + 1004, _t7_16);
    _mm256_storeu_pd(A + k81 + 1128, _t7_17);
    _mm256_storeu_pd(A + k81 + 1252, _t7_18);
    _mm256_storeu_pd(A + k81 + 1376, _t7_19);
  }

  _t8_5 = _mm256_loadu_pd(A + 1004);
  _t8_2 = _mm256_loadu_pd(A + 1128);
  _t8_3 = _mm256_loadu_pd(A + 1252);
  _t8_4 = _mm256_loadu_pd(A + 1376);

  // Constant 1x1 -> 1x4
  _t8_7 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t8_8 = _t6_14;

  // 4-BLAC: 1x4 / 1x4
  _t8_9 = _mm256_div_pd(_t8_7, _t8_8);
  _t8_0 = _t8_9;

  // Constant 1x1 -> 1x4
  _t8_10 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t8_11 = _t6_16;

  // 4-BLAC: 1x4 / 1x4
  _t8_12 = _mm256_div_pd(_t8_10, _t8_11);
  _t8_1 = _t8_12;

  // 1x1 -> 1x4
  _t8_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_9, _t6_9, 32), _mm256_permute2f128_pd(_t6_9, _t6_9, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t8_5 = _mm256_mul_pd(_t8_13, _t8_5);

  // 3x4 -> 4x4
  _t8_14 = _t8_2;
  _t8_15 = _t8_3;
  _t8_16 = _t8_4;
  _t8_17 = _mm256_setzero_pd();

  // 1x3 -> 1x4
  _t8_18 = _t6_10;

  // 4-BLAC: (1x4)^T
  _t8_19 = _t8_18;

  // 4-BLAC: 4x1 * 1x4
  _t8_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 0), _t8_5);
  _t8_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 15), _t8_5);
  _t8_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 49), _mm256_permute2f128_pd(_t8_19, _t8_19, 49), 0), _t8_5);
  _t8_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 49), _mm256_permute2f128_pd(_t8_19, _t8_19, 49), 15), _t8_5);

  // 4-BLAC: 4x4 - 4x4
  _t8_24 = _mm256_sub_pd(_t8_14, _t8_20);
  _t8_25 = _mm256_sub_pd(_t8_15, _t8_21);
  _t8_26 = _mm256_sub_pd(_t8_16, _t8_22);
  _t8_27 = _mm256_sub_pd(_t8_17, _t8_23);
  _t8_2 = _t8_24;
  _t8_3 = _t8_25;
  _t8_4 = _t8_26;

  // 1x1 -> 1x4
  _t8_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_13, _t6_13, 32), _mm256_permute2f128_pd(_t6_13, _t6_13, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t8_2 = _mm256_mul_pd(_t8_28, _t8_2);

  // 2x4 -> 4x4
  _t8_29 = _t8_3;
  _t8_30 = _t8_4;
  _t8_31 = _mm256_setzero_pd();
  _t8_32 = _mm256_setzero_pd();

  // 1x2 -> 1x4
  _t8_33 = _t6_12;

  // 4-BLAC: (1x4)^T
  _t8_34 = _t8_33;

  // 4-BLAC: 4x1 * 1x4
  _t8_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 0), _t8_2);
  _t8_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 15), _t8_2);
  _t8_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 49), _mm256_permute2f128_pd(_t8_34, _t8_34, 49), 0), _t8_2);
  _t8_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 49), _mm256_permute2f128_pd(_t8_34, _t8_34, 49), 15), _t8_2);

  // 4-BLAC: 4x4 - 4x4
  _t8_39 = _mm256_sub_pd(_t8_29, _t8_35);
  _t8_40 = _mm256_sub_pd(_t8_30, _t8_36);
  _t8_41 = _mm256_sub_pd(_t8_31, _t8_37);
  _t8_42 = _mm256_sub_pd(_t8_32, _t8_38);
  _t8_3 = _t8_39;
  _t8_4 = _t8_40;

  // 1x1 -> 1x4
  _t8_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_0, _t8_0, 32), _mm256_permute2f128_pd(_t8_0, _t8_0, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t8_3 = _mm256_mul_pd(_t8_43, _t8_3);

  // 1x1 -> 1x4
  _t8_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_15, _t6_15, 32), _mm256_permute2f128_pd(_t6_15, _t6_15, 32), 0);

  // 4-BLAC: (4x1)^T
  _t8_45 = _t8_44;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_6 = _mm256_mul_pd(_t8_45, _t8_3);

  // 4-BLAC: 1x4 - 1x4
  _t8_4 = _mm256_sub_pd(_t8_4, _t8_6);

  // 1x1 -> 1x4
  _t8_46 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_1, _t8_1, 32), _mm256_permute2f128_pd(_t8_1, _t8_1, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t8_4 = _mm256_mul_pd(_t8_46, _t8_4);

  _mm256_storeu_pd(A + 1000, _t6_4);
  _mm256_maskstore_pd(A + 1124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t6_5);
  _mm256_maskstore_pd(A + 1248, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t6_6);

  for( int fi252 = 4; fi252 <= 108; fi252+=4 ) {
    _t9_3 = _mm256_loadu_pd(A + fi252 + 1004);
    _t9_0 = _mm256_loadu_pd(A + fi252 + 1128);
    _t9_1 = _mm256_loadu_pd(A + fi252 + 1252);
    _t9_2 = _mm256_loadu_pd(A + fi252 + 1376);

    // 1x1 -> 1x4
    _t9_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_9, _t6_9, 32), _mm256_permute2f128_pd(_t6_9, _t6_9, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t9_3 = _mm256_mul_pd(_t9_4, _t9_3);

    // 3x4 -> 4x4
    _t9_5 = _t9_0;
    _t9_6 = _t9_1;
    _t9_7 = _t9_2;
    _t9_8 = _mm256_setzero_pd();

    // 1x3 -> 1x4
    _t9_9 = _t6_10;

    // 4-BLAC: (1x4)^T
    _t8_19 = _t9_9;

    // 4-BLAC: 4x1 * 1x4
    _t8_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 0), _t9_3);
    _t8_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 15), _t9_3);
    _t8_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 49), _mm256_permute2f128_pd(_t8_19, _t8_19, 49), 0), _t9_3);
    _t8_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 49), _mm256_permute2f128_pd(_t8_19, _t8_19, 49), 15), _t9_3);

    // 4-BLAC: 4x4 - 4x4
    _t9_10 = _mm256_sub_pd(_t9_5, _t8_20);
    _t9_11 = _mm256_sub_pd(_t9_6, _t8_21);
    _t9_12 = _mm256_sub_pd(_t9_7, _t8_22);
    _t9_13 = _mm256_sub_pd(_t9_8, _t8_23);
    _t9_0 = _t9_10;
    _t9_1 = _t9_11;
    _t9_2 = _t9_12;

    // 1x1 -> 1x4
    _t9_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_13, _t6_13, 32), _mm256_permute2f128_pd(_t6_13, _t6_13, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t9_0 = _mm256_mul_pd(_t9_14, _t9_0);

    // 2x4 -> 4x4
    _t9_15 = _t9_1;
    _t9_16 = _t9_2;
    _t9_17 = _mm256_setzero_pd();
    _t9_18 = _mm256_setzero_pd();

    // 1x2 -> 1x4
    _t9_19 = _t6_12;

    // 4-BLAC: (1x4)^T
    _t8_34 = _t9_19;

    // 4-BLAC: 4x1 * 1x4
    _t8_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 0), _t9_0);
    _t8_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 15), _t9_0);
    _t8_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 49), _mm256_permute2f128_pd(_t8_34, _t8_34, 49), 0), _t9_0);
    _t8_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 49), _mm256_permute2f128_pd(_t8_34, _t8_34, 49), 15), _t9_0);

    // 4-BLAC: 4x4 - 4x4
    _t9_20 = _mm256_sub_pd(_t9_15, _t8_35);
    _t9_21 = _mm256_sub_pd(_t9_16, _t8_36);
    _t9_22 = _mm256_sub_pd(_t9_17, _t8_37);
    _t9_23 = _mm256_sub_pd(_t9_18, _t8_38);
    _t9_1 = _t9_20;
    _t9_2 = _t9_21;

    // 1x1 -> 1x4
    _t9_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_0, _t8_0, 32), _mm256_permute2f128_pd(_t8_0, _t8_0, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t9_1 = _mm256_mul_pd(_t9_24, _t9_1);

    // 1x1 -> 1x4
    _t9_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_15, _t6_15, 32), _mm256_permute2f128_pd(_t6_15, _t6_15, 32), 0);

    // 4-BLAC: (4x1)^T
    _t8_45 = _t9_25;

    // 4-BLAC: 1x4 Kro 1x4
    _t8_6 = _mm256_mul_pd(_t8_45, _t9_1);

    // 4-BLAC: 1x4 - 1x4
    _t9_2 = _mm256_sub_pd(_t9_2, _t8_6);

    // 1x1 -> 1x4
    _t9_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_1, _t8_1, 32), _mm256_permute2f128_pd(_t8_1, _t8_1, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t9_2 = _mm256_mul_pd(_t9_26, _t9_2);
    _mm256_storeu_pd(A + fi252 + 1004, _t9_3);
    _mm256_storeu_pd(A + fi252 + 1128, _t9_0);
    _mm256_storeu_pd(A + fi252 + 1252, _t9_1);
    _mm256_storeu_pd(A + fi252 + 1376, _t9_2);
  }

  _mm256_storeu_pd(A + 1004, _t8_5);
  _mm256_storeu_pd(A + 1128, _t8_2);
  _mm256_storeu_pd(A + 1252, _t8_3);
  _mm256_storeu_pd(A + 1376, _t8_4);

  for( int fi127 = 12; fi127 <= 116; fi127+=4 ) {
    _t10_4 = _mm256_loadu_pd(A + 125*fi127);
    _t10_5 = _mm256_maskload_pd(A + 125*fi127 + 124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t10_6 = _mm256_maskload_pd(A + 125*fi127 + 248, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t10_7 = _mm256_maskload_pd(A + 125*fi127 + 372, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t10_3 = _mm256_loadu_pd(A + fi127);
    _t10_2 = _mm256_loadu_pd(A + fi127 + 124);
    _t10_1 = _mm256_loadu_pd(A + fi127 + 248);
    _t10_0 = _mm256_loadu_pd(A + fi127 + 372);

    // 4x4 -> 4x4 - UpSymm
    _t10_8 = _t10_4;
    _t10_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_4, _t10_5, 3), _t10_5, 12);
    _t10_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_4, _t10_5, 0), _t10_6, 49);
    _t10_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_4, _t10_5, 12), _mm256_shuffle_pd(_t10_6, _t10_7, 12), 49);

    // 4-BLAC: (4x4)^T
    _t6_90 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
    _t6_91 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
    _t6_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
    _t6_93 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t6_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 32), _mm256_permute2f128_pd(_t6_90, _t6_90, 32), 0), _t10_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 32), _mm256_permute2f128_pd(_t6_90, _t6_90, 32), 15), _t10_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 49), _mm256_permute2f128_pd(_t6_90, _t6_90, 49), 0), _t10_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_90, _t6_90, 49), _mm256_permute2f128_pd(_t6_90, _t6_90, 49), 15), _t10_0)));
    _t6_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 32), _mm256_permute2f128_pd(_t6_91, _t6_91, 32), 0), _t10_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 32), _mm256_permute2f128_pd(_t6_91, _t6_91, 32), 15), _t10_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 49), _mm256_permute2f128_pd(_t6_91, _t6_91, 49), 0), _t10_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_91, _t6_91, 49), _mm256_permute2f128_pd(_t6_91, _t6_91, 49), 15), _t10_0)));
    _t6_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 32), _mm256_permute2f128_pd(_t6_92, _t6_92, 32), 0), _t10_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 32), _mm256_permute2f128_pd(_t6_92, _t6_92, 32), 15), _t10_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 49), _mm256_permute2f128_pd(_t6_92, _t6_92, 49), 0), _t10_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_92, _t6_92, 49), _mm256_permute2f128_pd(_t6_92, _t6_92, 49), 15), _t10_0)));
    _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 32), _mm256_permute2f128_pd(_t6_93, _t6_93, 32), 0), _t10_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 32), _mm256_permute2f128_pd(_t6_93, _t6_93, 32), 15), _t10_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 49), _mm256_permute2f128_pd(_t6_93, _t6_93, 49), 0), _t10_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_93, _t6_93, 49), _mm256_permute2f128_pd(_t6_93, _t6_93, 49), 15), _t10_0)));

    // 4-BLAC: 4x4 - 4x4
    _t6_25 = _mm256_sub_pd(_t10_8, _t6_17);
    _t6_26 = _mm256_sub_pd(_t10_9, _t6_18);
    _t6_27 = _mm256_sub_pd(_t10_10, _t6_19);
    _t6_28 = _mm256_sub_pd(_t10_11, _t6_20);

    // 4x4 -> 4x4 - UpSymm
    _t10_4 = _t6_25;
    _t10_5 = _t6_26;
    _t10_6 = _t6_27;
    _t10_7 = _t6_28;

    for( int k81 = 4; k81 <= fi127 - 1; k81+=4 ) {
      _t11_3 = _mm256_loadu_pd(A + fi127 + 124*k81);
      _t11_2 = _mm256_loadu_pd(A + fi127 + 124*k81 + 124);
      _t11_1 = _mm256_loadu_pd(A + fi127 + 124*k81 + 248);
      _t11_0 = _mm256_loadu_pd(A + fi127 + 124*k81 + 372);

      // 4-BLAC: (4x4)^T
      _t6_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
      _t6_95 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
      _t6_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
      _t6_97 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 32), _mm256_permute2f128_pd(_t6_94, _t6_94, 32), 0), _t11_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 32), _mm256_permute2f128_pd(_t6_94, _t6_94, 32), 15), _t11_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 49), _mm256_permute2f128_pd(_t6_94, _t6_94, 49), 0), _t11_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_94, _t6_94, 49), _mm256_permute2f128_pd(_t6_94, _t6_94, 49), 15), _t11_0)));
      _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 32), _mm256_permute2f128_pd(_t6_95, _t6_95, 32), 0), _t11_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 32), _mm256_permute2f128_pd(_t6_95, _t6_95, 32), 15), _t11_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 49), _mm256_permute2f128_pd(_t6_95, _t6_95, 49), 0), _t11_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 49), _mm256_permute2f128_pd(_t6_95, _t6_95, 49), 15), _t11_0)));
      _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 32), _mm256_permute2f128_pd(_t6_96, _t6_96, 32), 0), _t11_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 32), _mm256_permute2f128_pd(_t6_96, _t6_96, 32), 15), _t11_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 49), _mm256_permute2f128_pd(_t6_96, _t6_96, 49), 0), _t11_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_96, _t6_96, 49), _mm256_permute2f128_pd(_t6_96, _t6_96, 49), 15), _t11_0)));
      _t6_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 32), _mm256_permute2f128_pd(_t6_97, _t6_97, 32), 0), _t11_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 32), _mm256_permute2f128_pd(_t6_97, _t6_97, 32), 15), _t11_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 49), _mm256_permute2f128_pd(_t6_97, _t6_97, 49), 0), _t11_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_97, _t6_97, 49), _mm256_permute2f128_pd(_t6_97, _t6_97, 49), 15), _t11_0)));

      // 4x4 -> 4x4 - UpSymm
      _t11_4 = _t10_4;
      _t11_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_4, _t10_5, 3), _t10_5, 12);
      _t11_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_4, _t10_5, 0), _t10_6, 49);
      _t11_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_4, _t10_5, 12), _mm256_shuffle_pd(_t10_6, _t10_7, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t11_4 = _mm256_sub_pd(_t11_4, _t6_21);
      _t11_5 = _mm256_sub_pd(_t11_5, _t6_22);
      _t11_6 = _mm256_sub_pd(_t11_6, _t6_23);
      _t11_7 = _mm256_sub_pd(_t11_7, _t6_24);

      // 4x4 -> 4x4 - UpSymm
      _t10_4 = _t11_4;
      _t10_5 = _t11_5;
      _t10_6 = _t11_6;
      _t10_7 = _t11_7;
    }

    // 1x1 -> 1x4
    _t12_9 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_4, 1);

    // 4-BLAC: sqrt(1x4)
    _t12_10 = _mm256_sqrt_pd(_t12_9);
    _t12_0 = _t12_10;

    // Constant 1x1 -> 1x4
    _t12_11 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t12_12 = _t12_0;

    // 4-BLAC: 1x4 / 1x4
    _t12_13 = _mm256_div_pd(_t12_11, _t12_12);
    _t12_1 = _t12_13;

    // 1x1 -> 1x4
    _t12_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_1, _t12_1, 32), _mm256_permute2f128_pd(_t12_1, _t12_1, 32), 0);

    // 1x3 -> 1x4
    _t12_15 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_4, 14), _mm256_permute2f128_pd(_t10_4, _t10_4, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t12_16 = _mm256_mul_pd(_t12_14, _t12_15);
    _t12_2 = _t12_16;

    // 1x1 -> 1x4
    _t12_17 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_5, 2), _mm256_setzero_pd());

    // 1x1 -> 1x4
    _t12_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_2, 1);

    // 4-BLAC: (4x1)^T
    _t6_47 = _t12_18;

    // 1x1 -> 1x4
    _t12_19 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_2, 1);

    // 4-BLAC: 1x4 Kro 1x4
    _t6_49 = _mm256_mul_pd(_t6_47, _t12_19);

    // 4-BLAC: 1x4 - 1x4
    _t12_20 = _mm256_sub_pd(_t12_17, _t6_49);
    _t12_3 = _t12_20;

    // 1x1 -> 1x4
    _t12_21 = _t12_3;

    // 4-BLAC: sqrt(1x4)
    _t12_22 = _mm256_sqrt_pd(_t12_21);
    _t12_3 = _t12_22;

    // 1x2 -> 1x4
    _t12_23 = _mm256_permute2f128_pd(_t10_5, _t10_5, 129);

    // 1x1 -> 1x4
    _t12_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_2, _t12_2, 32), _mm256_permute2f128_pd(_t12_2, _t12_2, 32), 0);

    // 4-BLAC: (4x1)^T
    _t6_55 = _t12_24;

    // 1x2 -> 1x4
    _t12_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_2, 6), _mm256_permute2f128_pd(_t12_2, _t12_2, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t6_57 = _mm256_mul_pd(_t6_55, _t12_25);

    // 4-BLAC: 1x4 - 1x4
    _t12_26 = _mm256_sub_pd(_t12_23, _t6_57);
    _t12_4 = _t12_26;

    // Constant 1x1 -> 1x4
    _t12_27 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t12_28 = _t12_3;

    // 4-BLAC: 1x4 / 1x4
    _t12_29 = _mm256_div_pd(_t12_27, _t12_28);
    _t12_5 = _t12_29;

    // 1x1 -> 1x4
    _t12_30 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_5, _t12_5, 32), _mm256_permute2f128_pd(_t12_5, _t12_5, 32), 0);

    // 1x2 -> 1x4
    _t12_31 = _t12_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t12_32 = _mm256_mul_pd(_t12_30, _t12_31);
    _t12_4 = _t12_32;

    // 1x1 -> 1x4
    _t12_33 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_6, 4), 129);

    // 2x1 -> 4x1
    _t12_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t12_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (4x1)^T
    _t6_67 = _t12_34;

    // 2x1 -> 4x1
    _t12_35 = _mm256_shuffle_pd(_mm256_blend_pd(_t12_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: 1x4 * 4x1
    _t6_69 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_67, _t12_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_67, _t12_35), _mm256_mul_pd(_t6_67, _t12_35), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_67, _t12_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_67, _t12_35), _mm256_mul_pd(_t6_67, _t12_35), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_67, _t12_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_67, _t12_35), _mm256_mul_pd(_t6_67, _t12_35), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t12_36 = _mm256_sub_pd(_t12_33, _t6_69);
    _t12_6 = _t12_36;

    // 1x1 -> 1x4
    _t12_37 = _t12_6;

    // 4-BLAC: sqrt(1x4)
    _t12_38 = _mm256_sqrt_pd(_t12_37);
    _t12_6 = _t12_38;

    // 1x1 -> 1x4
    _t12_39 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_6, _t10_6, 129), _mm256_setzero_pd());

    // 2x1 -> 4x1
    _t12_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t12_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (4x1)^T
    _t6_75 = _t12_40;

    // 2x1 -> 4x1
    _t12_41 = _mm256_blend_pd(_mm256_permute2f128_pd(_t12_2, _t12_2, 129), _t12_4, 2);

    // 4-BLAC: 1x4 * 4x1
    _t6_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_75, _t12_41), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_75, _t12_41), _mm256_mul_pd(_t6_75, _t12_41), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_75, _t12_41), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_75, _t12_41), _mm256_mul_pd(_t6_75, _t12_41), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_75, _t12_41), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_75, _t12_41), _mm256_mul_pd(_t6_75, _t12_41), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t12_42 = _mm256_sub_pd(_t12_39, _t6_77);
    _t12_7 = _t12_42;

    // 1x1 -> 1x4
    _t12_43 = _t12_7;

    // 1x1 -> 1x4
    _t12_44 = _t12_6;

    // 4-BLAC: 1x4 / 1x4
    _t12_45 = _mm256_div_pd(_t12_43, _t12_44);
    _t12_7 = _t12_45;

    // 1x1 -> 1x4
    _t12_46 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_7, _t10_7, 129), _mm256_setzero_pd());

    // 3x1 -> 4x1
    _t12_47 = _mm256_blend_pd(_mm256_permute2f128_pd(_t12_2, _t12_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 2), 10);

    // 4-BLAC: (4x1)^T
    _t6_84 = _t12_47;

    // 3x1 -> 4x1
    _t12_48 = _mm256_blend_pd(_mm256_permute2f128_pd(_t12_2, _t12_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 2), 10);

    // 4-BLAC: 1x4 * 4x1
    _t6_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_84, _t12_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_84, _t12_48), _mm256_mul_pd(_t6_84, _t12_48), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_84, _t12_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_84, _t12_48), _mm256_mul_pd(_t6_84, _t12_48), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_84, _t12_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_84, _t12_48), _mm256_mul_pd(_t6_84, _t12_48), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t12_49 = _mm256_sub_pd(_t12_46, _t6_86);
    _t12_8 = _t12_49;

    // 1x1 -> 1x4
    _t12_50 = _t12_8;

    // 4-BLAC: sqrt(1x4)
    _t12_51 = _mm256_sqrt_pd(_t12_50);
    _t12_8 = _t12_51;

    // 4-BLAC: (4x4)^T
    _t6_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
    _t6_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
    _t6_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
    _t6_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

    for( int k81 = 0; k81 <= -fi127 + 119; k81+=4 ) {
      _t13_8 = _mm256_loadu_pd(A + 125*fi127 + k81 + 4);
      _t13_9 = _mm256_loadu_pd(A + 125*fi127 + k81 + 128);
      _t13_10 = _mm256_loadu_pd(A + 125*fi127 + k81 + 252);
      _t13_11 = _mm256_loadu_pd(A + 125*fi127 + k81 + 376);
      _t13_3 = _mm256_loadu_pd(A + fi127 + k81 + 4);
      _t13_2 = _mm256_loadu_pd(A + fi127 + k81 + 128);
      _t13_1 = _mm256_loadu_pd(A + fi127 + k81 + 252);
      _t13_0 = _mm256_loadu_pd(A + fi127 + k81 + 376);

      // 4-BLAC: (4x4)^T
      _t6_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
      _t6_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
      _t6_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
      _t6_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t13_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 32), _mm256_permute2f128_pd(_t6_98, _t6_98, 32), 0), _t13_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 32), _mm256_permute2f128_pd(_t6_98, _t6_98, 32), 15), _t13_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 49), _mm256_permute2f128_pd(_t6_98, _t6_98, 49), 0), _t13_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_98, _t6_98, 49), _mm256_permute2f128_pd(_t6_98, _t6_98, 49), 15), _t13_0)));
      _t13_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 32), _mm256_permute2f128_pd(_t6_99, _t6_99, 32), 0), _t13_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 32), _mm256_permute2f128_pd(_t6_99, _t6_99, 32), 15), _t13_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 49), _mm256_permute2f128_pd(_t6_99, _t6_99, 49), 0), _t13_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_99, _t6_99, 49), _mm256_permute2f128_pd(_t6_99, _t6_99, 49), 15), _t13_0)));
      _t13_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 32), _mm256_permute2f128_pd(_t6_100, _t6_100, 32), 0), _t13_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 32), _mm256_permute2f128_pd(_t6_100, _t6_100, 32), 15), _t13_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 49), _mm256_permute2f128_pd(_t6_100, _t6_100, 49), 0), _t13_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_100, _t6_100, 49), _mm256_permute2f128_pd(_t6_100, _t6_100, 49), 15), _t13_0)));
      _t13_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 32), _mm256_permute2f128_pd(_t6_101, _t6_101, 32), 0), _t13_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 32), _mm256_permute2f128_pd(_t6_101, _t6_101, 32), 15), _t13_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 49), _mm256_permute2f128_pd(_t6_101, _t6_101, 49), 0), _t13_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_101, _t6_101, 49), _mm256_permute2f128_pd(_t6_101, _t6_101, 49), 15), _t13_0)));

      // 4-BLAC: 4x4 - 4x4
      _t13_8 = _mm256_sub_pd(_t13_8, _t13_4);
      _t13_9 = _mm256_sub_pd(_t13_9, _t13_5);
      _t13_10 = _mm256_sub_pd(_t13_10, _t13_6);
      _t13_11 = _mm256_sub_pd(_t13_11, _t13_7);
      _mm256_storeu_pd(A + 125*fi127 + k81 + 4, _t13_8);
      _mm256_storeu_pd(A + 125*fi127 + k81 + 128, _t13_9);
      _mm256_storeu_pd(A + 125*fi127 + k81 + 252, _t13_10);
      _mm256_storeu_pd(A + 125*fi127 + k81 + 376, _t13_11);

      for( int j128 = 4; j128 <= fi127 - 1; j128+=4 ) {
        _t14_7 = _mm256_loadu_pd(A + fi127 + 124*j128);
        _t14_6 = _mm256_loadu_pd(A + fi127 + 124*j128 + 124);
        _t14_5 = _mm256_loadu_pd(A + fi127 + 124*j128 + 248);
        _t14_4 = _mm256_loadu_pd(A + fi127 + 124*j128 + 372);
        _t14_3 = _mm256_loadu_pd(A + fi127 + 124*j128 + k81 + 4);
        _t14_2 = _mm256_loadu_pd(A + fi127 + 124*j128 + k81 + 128);
        _t14_1 = _mm256_loadu_pd(A + fi127 + 124*j128 + k81 + 252);
        _t14_0 = _mm256_loadu_pd(A + fi127 + 124*j128 + k81 + 376);
        _t14_8 = _mm256_loadu_pd(A + 125*fi127 + k81 + 4);
        _t14_9 = _mm256_loadu_pd(A + 125*fi127 + k81 + 128);
        _t14_10 = _mm256_loadu_pd(A + 125*fi127 + k81 + 252);
        _t14_11 = _mm256_loadu_pd(A + 125*fi127 + k81 + 376);

        // 4-BLAC: (4x4)^T
        _t14_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 32);
        _t14_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 32);
        _t14_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 49);
        _t14_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 49);

        // 4-BLAC: 4x4 * 4x4
        _t14_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_16, _t14_16, 32), _mm256_permute2f128_pd(_t14_16, _t14_16, 32), 0), _t14_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_16, _t14_16, 32), _mm256_permute2f128_pd(_t14_16, _t14_16, 32), 15), _t14_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_16, _t14_16, 49), _mm256_permute2f128_pd(_t14_16, _t14_16, 49), 0), _t14_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_16, _t14_16, 49), _mm256_permute2f128_pd(_t14_16, _t14_16, 49), 15), _t14_0)));
        _t14_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_17, _t14_17, 32), _mm256_permute2f128_pd(_t14_17, _t14_17, 32), 0), _t14_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_17, _t14_17, 32), _mm256_permute2f128_pd(_t14_17, _t14_17, 32), 15), _t14_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_17, _t14_17, 49), _mm256_permute2f128_pd(_t14_17, _t14_17, 49), 0), _t14_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_17, _t14_17, 49), _mm256_permute2f128_pd(_t14_17, _t14_17, 49), 15), _t14_0)));
        _t14_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_18, _t14_18, 32), _mm256_permute2f128_pd(_t14_18, _t14_18, 32), 0), _t14_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_18, _t14_18, 32), _mm256_permute2f128_pd(_t14_18, _t14_18, 32), 15), _t14_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_18, _t14_18, 49), _mm256_permute2f128_pd(_t14_18, _t14_18, 49), 0), _t14_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_18, _t14_18, 49), _mm256_permute2f128_pd(_t14_18, _t14_18, 49), 15), _t14_0)));
        _t14_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_19, _t14_19, 32), _mm256_permute2f128_pd(_t14_19, _t14_19, 32), 0), _t14_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_19, _t14_19, 32), _mm256_permute2f128_pd(_t14_19, _t14_19, 32), 15), _t14_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_19, _t14_19, 49), _mm256_permute2f128_pd(_t14_19, _t14_19, 49), 0), _t14_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_19, _t14_19, 49), _mm256_permute2f128_pd(_t14_19, _t14_19, 49), 15), _t14_0)));

        // 4-BLAC: 4x4 - 4x4
        _t14_8 = _mm256_sub_pd(_t14_8, _t14_12);
        _t14_9 = _mm256_sub_pd(_t14_9, _t14_13);
        _t14_10 = _mm256_sub_pd(_t14_10, _t14_14);
        _t14_11 = _mm256_sub_pd(_t14_11, _t14_15);
        _mm256_storeu_pd(A + 125*fi127 + k81 + 4, _t14_8);
        _mm256_storeu_pd(A + 125*fi127 + k81 + 128, _t14_9);
        _mm256_storeu_pd(A + 125*fi127 + k81 + 252, _t14_10);
        _mm256_storeu_pd(A + 125*fi127 + k81 + 376, _t14_11);
      }
    }

    // Constant 1x1 -> 1x4
    _t15_2 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t15_3 = _t12_6;

    // 4-BLAC: 1x4 / 1x4
    _t15_4 = _mm256_div_pd(_t15_2, _t15_3);
    _t15_0 = _t15_4;

    // Constant 1x1 -> 1x4
    _t15_5 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t15_6 = _t12_8;

    // 4-BLAC: 1x4 / 1x4
    _t15_7 = _mm256_div_pd(_t15_5, _t15_6);
    _t15_1 = _t15_7;
    _mm256_storeu_pd(A + 125*fi127, _t10_4);
    _mm256_maskstore_pd(A + 125*fi127 + 124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_5);
    _mm256_maskstore_pd(A + 125*fi127 + 248, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_6);

    for( int fi252 = 0; fi252 <= -fi127 + 116; fi252+=4 ) {
      _t16_3 = _mm256_loadu_pd(A + 125*fi127 + fi252 + 4);
      _t16_0 = _mm256_loadu_pd(A + 125*fi127 + fi252 + 128);
      _t16_1 = _mm256_loadu_pd(A + 125*fi127 + fi252 + 252);
      _t16_2 = _mm256_loadu_pd(A + 125*fi127 + fi252 + 376);

      // 1x1 -> 1x4
      _t16_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_1, _t12_1, 32), _mm256_permute2f128_pd(_t12_1, _t12_1, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t16_3 = _mm256_mul_pd(_t16_4, _t16_3);

      // 3x4 -> 4x4
      _t16_5 = _t16_0;
      _t16_6 = _t16_1;
      _t16_7 = _t16_2;
      _t16_8 = _mm256_setzero_pd();

      // 1x3 -> 1x4
      _t16_9 = _t12_2;

      // 4-BLAC: (1x4)^T
      _t8_19 = _t16_9;

      // 4-BLAC: 4x1 * 1x4
      _t8_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 0), _t16_3);
      _t8_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 32), _mm256_permute2f128_pd(_t8_19, _t8_19, 32), 15), _t16_3);
      _t8_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 49), _mm256_permute2f128_pd(_t8_19, _t8_19, 49), 0), _t16_3);
      _t8_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_19, _t8_19, 49), _mm256_permute2f128_pd(_t8_19, _t8_19, 49), 15), _t16_3);

      // 4-BLAC: 4x4 - 4x4
      _t16_10 = _mm256_sub_pd(_t16_5, _t8_20);
      _t16_11 = _mm256_sub_pd(_t16_6, _t8_21);
      _t16_12 = _mm256_sub_pd(_t16_7, _t8_22);
      _t16_13 = _mm256_sub_pd(_t16_8, _t8_23);
      _t16_0 = _t16_10;
      _t16_1 = _t16_11;
      _t16_2 = _t16_12;

      // 1x1 -> 1x4
      _t16_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_5, _t12_5, 32), _mm256_permute2f128_pd(_t12_5, _t12_5, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t16_0 = _mm256_mul_pd(_t16_14, _t16_0);

      // 2x4 -> 4x4
      _t16_15 = _t16_1;
      _t16_16 = _t16_2;
      _t16_17 = _mm256_setzero_pd();
      _t16_18 = _mm256_setzero_pd();

      // 1x2 -> 1x4
      _t16_19 = _t12_4;

      // 4-BLAC: (1x4)^T
      _t8_34 = _t16_19;

      // 4-BLAC: 4x1 * 1x4
      _t8_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 0), _t16_0);
      _t8_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 32), _mm256_permute2f128_pd(_t8_34, _t8_34, 32), 15), _t16_0);
      _t8_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 49), _mm256_permute2f128_pd(_t8_34, _t8_34, 49), 0), _t16_0);
      _t8_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_34, _t8_34, 49), _mm256_permute2f128_pd(_t8_34, _t8_34, 49), 15), _t16_0);

      // 4-BLAC: 4x4 - 4x4
      _t16_20 = _mm256_sub_pd(_t16_15, _t8_35);
      _t16_21 = _mm256_sub_pd(_t16_16, _t8_36);
      _t16_22 = _mm256_sub_pd(_t16_17, _t8_37);
      _t16_23 = _mm256_sub_pd(_t16_18, _t8_38);
      _t16_1 = _t16_20;
      _t16_2 = _t16_21;

      // 1x1 -> 1x4
      _t16_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_0, _t15_0, 32), _mm256_permute2f128_pd(_t15_0, _t15_0, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t16_1 = _mm256_mul_pd(_t16_24, _t16_1);

      // 1x1 -> 1x4
      _t16_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_7, _t12_7, 32), _mm256_permute2f128_pd(_t12_7, _t12_7, 32), 0);

      // 4-BLAC: (4x1)^T
      _t8_45 = _t16_25;

      // 4-BLAC: 1x4 Kro 1x4
      _t8_6 = _mm256_mul_pd(_t8_45, _t16_1);

      // 4-BLAC: 1x4 - 1x4
      _t16_2 = _mm256_sub_pd(_t16_2, _t8_6);

      // 1x1 -> 1x4
      _t16_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_1, _t15_1, 32), _mm256_permute2f128_pd(_t15_1, _t15_1, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t16_2 = _mm256_mul_pd(_t16_26, _t16_2);
      _mm256_storeu_pd(A + 125*fi127 + fi252 + 4, _t16_3);
      _mm256_storeu_pd(A + 125*fi127 + fi252 + 128, _t16_0);
      _mm256_storeu_pd(A + 125*fi127 + fi252 + 252, _t16_1);
      _mm256_storeu_pd(A + 125*fi127 + fi252 + 376, _t16_2);
    }
    _mm256_maskstore_pd(A + 125*fi127 + 372, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_7);
    _mm256_maskstore_pd(A + 125*fi127, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_0);
    _mm256_maskstore_pd(A + 125*fi127 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t12_2);
    _mm256_maskstore_pd(A + 125*fi127 + 125, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_3);
    _mm256_maskstore_pd(A + 125*fi127 + 126, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t12_4);
    _mm256_maskstore_pd(A + 125*fi127 + 250, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_6);
    _mm256_maskstore_pd(A + 125*fi127 + 251, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_7);
    _mm256_maskstore_pd(A + 125*fi127 + 375, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_8);
  }

  _t17_4 = _mm256_loadu_pd(A + 15000);
  _t17_5 = _mm256_maskload_pd(A + 15124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t17_6 = _mm256_maskload_pd(A + 15248, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t17_7 = _mm256_maskload_pd(A + 15372, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t17_3 = _mm256_loadu_pd(A + 120);
  _t17_2 = _mm256_loadu_pd(A + 244);
  _t17_1 = _mm256_loadu_pd(A + 368);
  _t17_0 = _mm256_loadu_pd(A + 492);

  // 4x4 -> 4x4 - UpSymm
  _t17_16 = _t17_4;
  _t17_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t17_4, _t17_5, 3), _t17_5, 12);
  _t17_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t17_4, _t17_5, 0), _t17_6, 49);
  _t17_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t17_4, _t17_5, 12), _mm256_shuffle_pd(_t17_6, _t17_7, 12), 49);

  // 4-BLAC: (4x4)^T
  _t17_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
  _t17_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
  _t17_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
  _t17_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t17_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 32), _mm256_permute2f128_pd(_t17_20, _t17_20, 32), 0), _t17_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 32), _mm256_permute2f128_pd(_t17_20, _t17_20, 32), 15), _t17_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 49), _mm256_permute2f128_pd(_t17_20, _t17_20, 49), 0), _t17_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 49), _mm256_permute2f128_pd(_t17_20, _t17_20, 49), 15), _t17_0)));
  _t17_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 32), _mm256_permute2f128_pd(_t17_21, _t17_21, 32), 0), _t17_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 32), _mm256_permute2f128_pd(_t17_21, _t17_21, 32), 15), _t17_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 49), _mm256_permute2f128_pd(_t17_21, _t17_21, 49), 0), _t17_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 49), _mm256_permute2f128_pd(_t17_21, _t17_21, 49), 15), _t17_0)));
  _t17_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 32), _mm256_permute2f128_pd(_t17_22, _t17_22, 32), 0), _t17_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 32), _mm256_permute2f128_pd(_t17_22, _t17_22, 32), 15), _t17_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 49), _mm256_permute2f128_pd(_t17_22, _t17_22, 49), 0), _t17_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 49), _mm256_permute2f128_pd(_t17_22, _t17_22, 49), 15), _t17_0)));
  _t17_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_23, _t17_23, 32), _mm256_permute2f128_pd(_t17_23, _t17_23, 32), 0), _t17_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_23, _t17_23, 32), _mm256_permute2f128_pd(_t17_23, _t17_23, 32), 15), _t17_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_23, _t17_23, 49), _mm256_permute2f128_pd(_t17_23, _t17_23, 49), 0), _t17_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_23, _t17_23, 49), _mm256_permute2f128_pd(_t17_23, _t17_23, 49), 15), _t17_0)));

  // 4-BLAC: 4x4 - 4x4
  _t17_12 = _mm256_sub_pd(_t17_16, _t17_8);
  _t17_13 = _mm256_sub_pd(_t17_17, _t17_9);
  _t17_14 = _mm256_sub_pd(_t17_18, _t17_10);
  _t17_15 = _mm256_sub_pd(_t17_19, _t17_11);

  // 4x4 -> 4x4 - UpSymm
  _t17_4 = _t17_12;
  _t17_5 = _t17_13;
  _t17_6 = _t17_14;
  _t17_7 = _t17_15;


  for( int k81 = 4; k81 <= 119; k81+=4 ) {
    _t18_3 = _mm256_loadu_pd(A + 124*k81 + 120);
    _t18_2 = _mm256_loadu_pd(A + 124*k81 + 244);
    _t18_1 = _mm256_loadu_pd(A + 124*k81 + 368);
    _t18_0 = _mm256_loadu_pd(A + 124*k81 + 492);

    // 4-BLAC: (4x4)^T
    _t18_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
    _t18_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
    _t18_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
    _t18_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t18_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_12, _t18_12, 32), _mm256_permute2f128_pd(_t18_12, _t18_12, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_12, _t18_12, 32), _mm256_permute2f128_pd(_t18_12, _t18_12, 32), 15), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_12, _t18_12, 49), _mm256_permute2f128_pd(_t18_12, _t18_12, 49), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_12, _t18_12, 49), _mm256_permute2f128_pd(_t18_12, _t18_12, 49), 15), _t18_0)));
    _t18_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_13, _t18_13, 32), _mm256_permute2f128_pd(_t18_13, _t18_13, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_13, _t18_13, 32), _mm256_permute2f128_pd(_t18_13, _t18_13, 32), 15), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_13, _t18_13, 49), _mm256_permute2f128_pd(_t18_13, _t18_13, 49), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_13, _t18_13, 49), _mm256_permute2f128_pd(_t18_13, _t18_13, 49), 15), _t18_0)));
    _t18_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_14, _t18_14, 32), _mm256_permute2f128_pd(_t18_14, _t18_14, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_14, _t18_14, 32), _mm256_permute2f128_pd(_t18_14, _t18_14, 32), 15), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_14, _t18_14, 49), _mm256_permute2f128_pd(_t18_14, _t18_14, 49), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_14, _t18_14, 49), _mm256_permute2f128_pd(_t18_14, _t18_14, 49), 15), _t18_0)));
    _t18_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_15, _t18_15, 32), _mm256_permute2f128_pd(_t18_15, _t18_15, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_15, _t18_15, 32), _mm256_permute2f128_pd(_t18_15, _t18_15, 32), 15), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_15, _t18_15, 49), _mm256_permute2f128_pd(_t18_15, _t18_15, 49), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_15, _t18_15, 49), _mm256_permute2f128_pd(_t18_15, _t18_15, 49), 15), _t18_0)));

    // 4x4 -> 4x4 - UpSymm
    _t18_8 = _t17_4;
    _t18_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t17_4, _t17_5, 3), _t17_5, 12);
    _t18_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t17_4, _t17_5, 0), _t17_6, 49);
    _t18_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t17_4, _t17_5, 12), _mm256_shuffle_pd(_t17_6, _t17_7, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t18_8 = _mm256_sub_pd(_t18_8, _t18_4);
    _t18_9 = _mm256_sub_pd(_t18_9, _t18_5);
    _t18_10 = _mm256_sub_pd(_t18_10, _t18_6);
    _t18_11 = _mm256_sub_pd(_t18_11, _t18_7);

    // 4x4 -> 4x4 - UpSymm
    _t17_4 = _t18_8;
    _t17_5 = _t18_9;
    _t17_6 = _t18_10;
    _t17_7 = _t18_11;
  }


  // 1x1 -> 1x4
  _t19_9 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_4, 1);

  // 4-BLAC: sqrt(1x4)
  _t19_10 = _mm256_sqrt_pd(_t19_9);
  _t19_0 = _t19_10;

  // Constant 1x1 -> 1x4
  _t19_11 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t19_12 = _t19_0;

  // 4-BLAC: 1x4 / 1x4
  _t19_13 = _mm256_div_pd(_t19_11, _t19_12);
  _t19_1 = _t19_13;

  // 1x1 -> 1x4
  _t19_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_1, _t19_1, 32), _mm256_permute2f128_pd(_t19_1, _t19_1, 32), 0);

  // 1x3 -> 1x4
  _t19_15 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_4, 14), _mm256_permute2f128_pd(_t17_4, _t17_4, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t19_16 = _mm256_mul_pd(_t19_14, _t19_15);
  _t19_2 = _t19_16;

  // 1x1 -> 1x4
  _t19_17 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_5, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t19_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t19_2, 1);

  // 4-BLAC: (4x1)^T
  _t19_19 = _t19_18;

  // 1x1 -> 1x4
  _t19_20 = _mm256_blend_pd(_mm256_setzero_pd(), _t19_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t19_21 = _mm256_mul_pd(_t19_19, _t19_20);

  // 4-BLAC: 1x4 - 1x4
  _t19_22 = _mm256_sub_pd(_t19_17, _t19_21);
  _t19_3 = _t19_22;

  // 1x1 -> 1x4
  _t19_23 = _t19_3;

  // 4-BLAC: sqrt(1x4)
  _t19_24 = _mm256_sqrt_pd(_t19_23);
  _t19_3 = _t19_24;

  // 1x2 -> 1x4
  _t19_25 = _mm256_permute2f128_pd(_t17_5, _t17_5, 129);

  // 1x1 -> 1x4
  _t19_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_2, _t19_2, 32), _mm256_permute2f128_pd(_t19_2, _t19_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t19_27 = _t19_26;

  // 1x2 -> 1x4
  _t19_28 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t19_2, 6), _mm256_permute2f128_pd(_t19_2, _t19_2, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t19_29 = _mm256_mul_pd(_t19_27, _t19_28);

  // 4-BLAC: 1x4 - 1x4
  _t19_30 = _mm256_sub_pd(_t19_25, _t19_29);
  _t19_4 = _t19_30;

  // Constant 1x1 -> 1x4
  _t19_31 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t19_32 = _t19_3;

  // 4-BLAC: 1x4 / 1x4
  _t19_33 = _mm256_div_pd(_t19_31, _t19_32);
  _t19_5 = _t19_33;

  // 1x1 -> 1x4
  _t19_34 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_5, _t19_5, 32), _mm256_permute2f128_pd(_t19_5, _t19_5, 32), 0);

  // 1x2 -> 1x4
  _t19_35 = _t19_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t19_36 = _mm256_mul_pd(_t19_34, _t19_35);
  _t19_4 = _t19_36;

  // 1x1 -> 1x4
  _t19_37 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_6, 4), 129);

  // 2x1 -> 4x1
  _t19_38 = _mm256_shuffle_pd(_mm256_blend_pd(_t19_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t19_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t19_39 = _t19_38;

  // 2x1 -> 4x1
  _t19_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t19_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t19_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t19_41 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_39, _t19_40), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_39, _t19_40), _mm256_mul_pd(_t19_39, _t19_40), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_39, _t19_40), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_39, _t19_40), _mm256_mul_pd(_t19_39, _t19_40), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_39, _t19_40), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_39, _t19_40), _mm256_mul_pd(_t19_39, _t19_40), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t19_42 = _mm256_sub_pd(_t19_37, _t19_41);
  _t19_6 = _t19_42;

  // 1x1 -> 1x4
  _t19_43 = _t19_6;

  // 4-BLAC: sqrt(1x4)
  _t19_44 = _mm256_sqrt_pd(_t19_43);
  _t19_6 = _t19_44;

  // 1x1 -> 1x4
  _t19_45 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_6, _t17_6, 129), _mm256_setzero_pd());

  // 2x1 -> 4x1
  _t19_46 = _mm256_shuffle_pd(_mm256_blend_pd(_t19_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t19_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t19_47 = _t19_46;

  // 2x1 -> 4x1
  _t19_48 = _mm256_blend_pd(_mm256_permute2f128_pd(_t19_2, _t19_2, 129), _t19_4, 2);

  // 4-BLAC: 1x4 * 4x1
  _t19_49 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_47, _t19_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_47, _t19_48), _mm256_mul_pd(_t19_47, _t19_48), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_47, _t19_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_47, _t19_48), _mm256_mul_pd(_t19_47, _t19_48), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_47, _t19_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_47, _t19_48), _mm256_mul_pd(_t19_47, _t19_48), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t19_50 = _mm256_sub_pd(_t19_45, _t19_49);
  _t19_7 = _t19_50;

  // 1x1 -> 1x4
  _t19_51 = _t19_7;

  // 1x1 -> 1x4
  _t19_52 = _t19_6;

  // 4-BLAC: 1x4 / 1x4
  _t19_53 = _mm256_div_pd(_t19_51, _t19_52);
  _t19_7 = _t19_53;

  // 1x1 -> 1x4
  _t19_54 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_7, _t17_7, 129), _mm256_setzero_pd());

  // 3x1 -> 4x1
  _t19_55 = _mm256_blend_pd(_mm256_permute2f128_pd(_t19_2, _t19_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t19_4, 2), 10);

  // 4-BLAC: (4x1)^T
  _t19_56 = _t19_55;

  // 3x1 -> 4x1
  _t19_57 = _mm256_blend_pd(_mm256_permute2f128_pd(_t19_2, _t19_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t19_4, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t19_58 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_56, _t19_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_56, _t19_57), _mm256_mul_pd(_t19_56, _t19_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_56, _t19_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_56, _t19_57), _mm256_mul_pd(_t19_56, _t19_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_56, _t19_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_56, _t19_57), _mm256_mul_pd(_t19_56, _t19_57), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t19_59 = _mm256_sub_pd(_t19_54, _t19_58);
  _t19_8 = _t19_59;

  // 1x1 -> 1x4
  _t19_60 = _t19_8;

  // 4-BLAC: sqrt(1x4)
  _t19_61 = _mm256_sqrt_pd(_t19_60);
  _t19_8 = _t19_61;

  _mm256_maskstore_pd(A, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_0);
  _mm256_maskstore_pd(A + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t0_2);
  _mm256_maskstore_pd(A + 125, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_3);
  _mm256_maskstore_pd(A + 126, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_4);
  _mm256_maskstore_pd(A + 250, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_6);
  _mm256_maskstore_pd(A + 251, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_7);
  _mm256_maskstore_pd(A + 375, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_8);
  _mm256_storeu_pd(A + 4, _t0_14);
  _mm256_storeu_pd(A + 128, _t0_11);
  _mm256_storeu_pd(A + 252, _t0_12);
  _mm256_storeu_pd(A + 376, _t0_13);
  _mm256_maskstore_pd(A + 872, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t2_3);
  _mm256_maskstore_pd(A + 500, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_4);
  _mm256_maskstore_pd(A + 501, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t2_6);
  _mm256_maskstore_pd(A + 625, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_7);
  _mm256_maskstore_pd(A + 626, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t2_8);
  _mm256_maskstore_pd(A + 750, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_10);
  _mm256_maskstore_pd(A + 751, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_11);
  _mm256_maskstore_pd(A + 875, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_12);
  _mm256_storeu_pd(A + 504, _t4_5);
  _mm256_storeu_pd(A + 628, _t4_2);
  _mm256_storeu_pd(A + 752, _t4_3);
  _mm256_storeu_pd(A + 876, _t4_4);
  _mm256_maskstore_pd(A + 1372, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t6_7);
  _mm256_maskstore_pd(A + 1000, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_8);
  _mm256_maskstore_pd(A + 1001, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t6_10);
  _mm256_maskstore_pd(A + 1125, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_11);
  _mm256_maskstore_pd(A + 1126, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t6_12);
  _mm256_maskstore_pd(A + 1250, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_14);
  _mm256_maskstore_pd(A + 1251, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_15);
  _mm256_maskstore_pd(A + 1375, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_16);
  _mm256_storeu_pd(A + 15000, _t17_4);
  _mm256_maskstore_pd(A + 15124, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t17_5);
  _mm256_maskstore_pd(A + 15248, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t17_6);
  _mm256_maskstore_pd(A + 15372, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t17_7);
  _mm256_maskstore_pd(A + 15000, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t19_0);
  _mm256_maskstore_pd(A + 15001, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t19_2);
  _mm256_maskstore_pd(A + 15125, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t19_3);
  _mm256_maskstore_pd(A + 15126, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t19_4);
  _mm256_maskstore_pd(A + 15250, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t19_6);
  _mm256_maskstore_pd(A + 15251, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t19_7);
  _mm256_maskstore_pd(A + 15375, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t19_8);

}
