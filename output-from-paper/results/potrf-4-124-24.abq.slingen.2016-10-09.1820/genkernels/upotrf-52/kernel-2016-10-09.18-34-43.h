/*
 * upotrf_kernel.h
 *
Decl { {u'A': Symmetric[A, (52, 52), USMatAccess], 'T925': Matrix[T925, (1, 52), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'ldiv_ut_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'chol_u_ow_opt': {'m': 'm1.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'chol_u_ow_opt_m1_ldiv_ut_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 52, 0), A[52,52],h(1, 52, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 0), A[52,52],h(1, 52, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 0), A[52,52],h(1, 52, 0)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(2, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(2, 52, 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), A[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), A[52,52],h(1, 52, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(1, 52, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), A[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), A[52,52],h(1, 52, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(1, 52, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 2)) ) Div Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 1)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), A[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), A[52,52],h(1, 52, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), A[52,52],h(1, 52, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), A[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 52, 2), A[52,52],h(1, 52, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 2), A[52,52],h(1, 52, 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, 0), A[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 0), A[52,52],h(1, 52, 3)) ) Div Tile( (1, 1), G(h(1, 52, 0), A[52,52],h(1, 52, 0)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 1), A[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 1), A[52,52],h(1, 52, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(2, 52, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), A[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 3)) ) Div Tile( (1, 1), G(h(1, 52, 1), A[52,52],h(1, 52, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), A[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), A[52,52],h(1, 52, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), A[52,52],h(1, 52, 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), A[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, 2), A[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 2), A[52,52],h(1, 52, 3)) ) Div Tile( (1, 1), G(h(1, 52, 2), A[52,52],h(1, 52, 2)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), A[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), A[52,52],h(1, 52, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), A[52,52],h(1, 52, 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), A[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 52, 3), A[52,52],h(1, 52, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 3), A[52,52],h(1, 52, 3)) ) )
Eq.ann: {}
Entry 16:
For_{fi524;4;48;4} ( Entry 0:
For_{fi603;0;fi524 - 5;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi603), A[52,52],h(1, 52, fi603)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi603 + 1), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi603 + 1), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603), A[52,52],h(3, 52, fi603 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi603 + 1), A[52,52],h(1, 52, fi603 + 1)) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 1), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 1), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi603 + 2), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi603 + 2), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 1), A[52,52],h(2, 52, fi603 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 1), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi603 + 2), A[52,52],h(1, 52, fi603 + 2)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 2), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 2), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 3), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 3), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 2), A[52,52],h(1, 52, fi603 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 2), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi603 + 3), A[52,52],h(1, 52, fi603 + 3)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 3), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi603 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi603 + 3), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(fi524 - fi603 - 4, 52, fi603 + 4), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(fi524 - fi603 - 4, 52, fi603 + 4), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi603), A[52,52],h(fi524 - fi603 - 4, 52, fi603 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi603), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 4)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi524 - 4), A[52,52],h(1, 52, fi524 - 4)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 4), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 4)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 4), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi524 - 3), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi524 - 3), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 4), A[52,52],h(3, 52, fi524 - 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 4), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi524 - 3), A[52,52],h(1, 52, fi524 - 3)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 3), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 3), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi524 - 2), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi524 - 2), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 3), A[52,52],h(2, 52, fi524 - 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 3), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi524 - 2), A[52,52],h(1, 52, fi524 - 2)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 2), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 2), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 1), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 1), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 2), A[52,52],h(1, 52, fi524 - 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 2), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi524 - 1), A[52,52],h(1, 52, fi524 - 1)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 1), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524 - 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 - 1), A[52,52],h(4, 52, fi524)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi524), A[52,52],h(4, 52, fi524)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi524), A[52,52],h(4, 52, fi524)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi524, 52, 0), A[52,52],h(4, 52, fi524)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi524, 52, 0), A[52,52],h(4, 52, fi524)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(2, 52, fi524 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T925[1,52],h(1, 52, fi524)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(2, 52, fi524 + 1)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 1)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 1)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 2)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 2)) ) Div Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 1)) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi524), A[52,52],h(1, 52, fi524 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi524), A[52,52],h(1, 52, fi524 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 2)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 3)) ) Div Tile( (1, 1), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524)) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(2, 52, fi524 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524), A[52,52],h(1, 52, fi524 + 3)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 3)) ) Div Tile( (1, 1), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 1)) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 1), A[52,52],h(1, 52, fi524 + 3)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 3)) ) Div Tile( (1, 1), G(h(1, 52, fi524 + 2), A[52,52],h(1, 52, fi524 + 2)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 3), A[52,52],h(1, 52, fi524 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi524 + 3), A[52,52],h(1, 52, fi524 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi524), A[52,52],h(1, 52, fi524 + 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi524), A[52,52],h(1, 52, fi524 + 3)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 52, fi524 + 3), A[52,52],h(1, 52, fi524 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi524 + 3), A[52,52],h(1, 52, fi524 + 3)) ) )
Eq.ann: {}
 ) *
 * Created on: 2016-10-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


#define PARAM0 52

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double * A)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205, _t0_206, _t0_207,
	_t0_208, _t0_209, _t0_210, _t0_211, _t0_212, _t0_213, _t0_214, _t0_215,
	_t0_216, _t0_217, _t0_218, _t0_219, _t0_220, _t0_221, _t0_222, _t0_223,
	_t0_224, _t0_225, _t0_226, _t0_227, _t0_228, _t0_229, _t0_230, _t0_231,
	_t0_232, _t0_233, _t0_234, _t0_235, _t0_236, _t0_237, _t0_238, _t0_239,
	_t0_240, _t0_241, _t0_242, _t0_243, _t0_244, _t0_245, _t0_246, _t0_247,
	_t0_248, _t0_249, _t0_250, _t0_251, _t0_252, _t0_253, _t0_254, _t0_255,
	_t0_256, _t0_257, _t0_258, _t0_259, _t0_260, _t0_261, _t0_262, _t0_263,
	_t0_264, _t0_265, _t0_266, _t0_267, _t0_268, _t0_269, _t0_270, _t0_271,
	_t0_272, _t0_273, _t0_274, _t0_275, _t0_276, _t0_277, _t0_278, _t0_279,
	_t0_280, _t0_281, _t0_282, _t0_283, _t0_284, _t0_285, _t0_286, _t0_287,
	_t0_288, _t0_289, _t0_290, _t0_291, _t0_292, _t0_293, _t0_294, _t0_295,
	_t0_296, _t0_297, _t0_298, _t0_299, _t0_300, _t0_301, _t0_302, _t0_303,
	_t0_304, _t0_305, _t0_306, _t0_307, _t0_308, _t0_309, _t0_310, _t0_311,
	_t0_312, _t0_313, _t0_314, _t0_315, _t0_316, _t0_317, _t0_318, _t0_319,
	_t0_320, _t0_321, _t0_322, _t0_323, _t0_324, _t0_325, _t0_326, _t0_327,
	_t0_328, _t0_329, _t0_330, _t0_331, _t0_332, _t0_333, _t0_334, _t0_335,
	_t0_336, _t0_337, _t0_338, _t0_339, _t0_340, _t0_341, _t0_342, _t0_343,
	_t0_344, _t0_345, _t0_346, _t0_347, _t0_348, _t0_349, _t0_350, _t0_351,
	_t0_352, _t0_353, _t0_354, _t0_355, _t0_356, _t0_357, _t0_358, _t0_359,
	_t0_360, _t0_361, _t0_362, _t0_363, _t0_364, _t0_365, _t0_366, _t0_367,
	_t0_368, _t0_369, _t0_370, _t0_371, _t0_372, _t0_373, _t0_374, _t0_375,
	_t0_376, _t0_377, _t0_378, _t0_379, _t0_380, _t0_381, _t0_382, _t0_383,
	_t0_384, _t0_385, _t0_386, _t0_387, _t0_388, _t0_389, _t0_390, _t0_391,
	_t0_392, _t0_393, _t0_394, _t0_395, _t0_396, _t0_397, _t0_398, _t0_399,
	_t0_400, _t0_401, _t0_402, _t0_403, _t0_404, _t0_405, _t0_406, _t0_407,
	_t0_408, _t0_409, _t0_410, _t0_411, _t0_412, _t0_413, _t0_414, _t0_415,
	_t0_416, _t0_417, _t0_418, _t0_419, _t0_420, _t0_421, _t0_422, _t0_423,
	_t0_424, _t0_425, _t0_426, _t0_427, _t0_428, _t0_429, _t0_430, _t0_431,
	_t0_432, _t0_433, _t0_434, _t0_435, _t0_436, _t0_437;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27, _t1_28, _t1_29, _t1_30, _t1_31,
	_t1_32, _t1_33, _t1_34, _t1_35, _t1_36, _t1_37, _t1_38, _t1_39,
	_t1_40, _t1_41, _t1_42, _t1_43, _t1_44, _t1_45, _t1_46, _t1_47,
	_t1_48, _t1_49;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31,
	_t3_32, _t3_33, _t3_34, _t3_35, _t3_36, _t3_37, _t3_38, _t3_39,
	_t3_40, _t3_41, _t3_42, _t3_43, _t3_44, _t3_45, _t3_46, _t3_47,
	_t3_48, _t3_49, _t3_50, _t3_51, _t3_52, _t3_53, _t3_54, _t3_55,
	_t3_56, _t3_57, _t3_58, _t3_59, _t3_60, _t3_61;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60;

  _t0_0 = _mm256_maskload_pd(A, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_2 = _mm256_maskload_pd(A + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_3 = _mm256_maskload_pd(A + 53, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_4 = _mm256_maskload_pd(A + 54, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_5 = _mm256_maskload_pd(A + 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_6 = _mm256_maskload_pd(A + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_7 = _mm256_shuffle_pd(_mm256_maskload_pd(A + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(A + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);
  _t0_10 = _mm256_maskload_pd(A + 159, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t0_53 = _mm256_loadu_pd(A + 4);
  _t0_11 = _mm256_loadu_pd(A + 56);
  _t0_12 = _mm256_loadu_pd(A + 108);
  _t0_13 = _mm256_loadu_pd(A + 160);
  _t0_17 = _mm256_loadu_pd(A + 212);
  _t0_18 = _mm256_maskload_pd(A + 264, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t0_19 = _mm256_maskload_pd(A + 316, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t0_20 = _mm256_maskload_pd(A + 368, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t0_55 = _mm256_loadu_pd(A + 8);
  _t0_32 = _mm256_loadu_pd(A + 60);
  _t0_33 = _mm256_loadu_pd(A + 112);
  _t0_34 = _mm256_loadu_pd(A + 164);
  _t0_78 = _mm256_loadu_pd(A + 216);
  _t0_79 = _mm256_loadu_pd(A + 268);
  _t0_80 = _mm256_loadu_pd(A + 320);
  _t0_81 = _mm256_loadu_pd(A + 372);
  _t0_38 = _mm256_loadu_pd(A + 424);
  _t0_39 = _mm256_maskload_pd(A + 476, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t0_40 = _mm256_maskload_pd(A + 528, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t0_41 = _mm256_maskload_pd(A + 580, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // 1x1 -> 1x4
  _t0_132 = _t0_0;

  // 4-BLAC: sqrt(1x4)
  _t0_151 = _mm256_sqrt_pd(_t0_132);
  _t0_0 = _t0_151;

  // Constant 1x1 -> 1x4
  _t0_168 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_173 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_187 = _mm256_div_pd(_t0_168, _t0_173);
  _t0_1 = _t0_187;

  // 1x1 -> 1x4
  _t0_202 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // 1x2 -> 1x4
  _t0_208 = _t0_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_222 = _mm256_mul_pd(_t0_202, _t0_208);
  _t0_2 = _t0_222;

  // 1x1 -> 1x4
  _t0_236 = _t0_3;

  // 1x1 -> 1x4
  _t0_247 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_266 = _t0_247;

  // 1x1 -> 1x4
  _t0_275 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_296 = _mm256_mul_pd(_t0_266, _t0_275);

  // 4-BLAC: 1x4 - 1x4
  _t0_322 = _mm256_sub_pd(_t0_236, _t0_296);
  _t0_3 = _t0_322;

  // 1x1 -> 1x4
  _t0_342 = _t0_3;

  // 4-BLAC: sqrt(1x4)
  _t0_352 = _mm256_sqrt_pd(_t0_342);
  _t0_3 = _t0_352;

  // 1x1 -> 1x4
  _t0_367 = _t0_4;

  // 1x1 -> 1x4
  _t0_372 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_382 = _t0_372;

  // 1x1 -> 1x4
  _t0_387 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 Kro 1x4
  _t0_402 = _mm256_mul_pd(_t0_382, _t0_387);

  // 4-BLAC: 1x4 - 1x4
  _t0_407 = _mm256_sub_pd(_t0_367, _t0_402);
  _t0_4 = _t0_407;

  // 1x1 -> 1x4
  _t0_408 = _t0_4;

  // 1x1 -> 1x4
  _t0_409 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_410 = _mm256_div_pd(_t0_408, _t0_409);
  _t0_4 = _t0_410;

  // 1x1 -> 1x4
  _t0_411 = _t0_5;

  // 2x1 -> 4x1
  _t0_412 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t0_413 = _t0_412;

  // 2x1 -> 4x1
  _t0_414 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t0_415 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_413, _t0_414), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_413, _t0_414), _mm256_mul_pd(_t0_413, _t0_414), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_413, _t0_414), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_413, _t0_414), _mm256_mul_pd(_t0_413, _t0_414), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_413, _t0_414), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_413, _t0_414), _mm256_mul_pd(_t0_413, _t0_414), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_416 = _mm256_sub_pd(_t0_411, _t0_415);
  _t0_5 = _t0_416;

  // 1x1 -> 1x4
  _t0_417 = _t0_5;

  // 4-BLAC: sqrt(1x4)
  _t0_418 = _mm256_sqrt_pd(_t0_417);
  _t0_5 = _t0_418;

  // 1x1 -> 1x4
  _t0_419 = _t0_6;

  // 1x1 -> 1x4
  _t0_420 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_421 = _mm256_div_pd(_t0_419, _t0_420);
  _t0_6 = _t0_421;

  // 2x1 -> 4x1
  _t0_86 = _t0_7;

  // 1x2 -> 1x4
  _t0_87 = _t0_2;

  // 4-BLAC: (1x4)^T
  _t0_88 = _t0_87;

  // 1x1 -> 1x4
  _t0_89 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_6, _t0_6, 32), _mm256_permute2f128_pd(_t0_6, _t0_6, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_90 = _mm256_mul_pd(_t0_88, _t0_89);

  // 4-BLAC: 4x1 - 4x1
  _t0_91 = _mm256_sub_pd(_t0_86, _t0_90);
  _t0_7 = _t0_91;

  // 1x1 -> 1x4
  _t0_92 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_7, 1);

  // 1x1 -> 1x4
  _t0_93 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_94 = _mm256_div_pd(_t0_92, _t0_93);
  _t0_8 = _t0_94;

  // 1x1 -> 1x4
  _t0_95 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_7, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_96 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_97 = _t0_96;

  // 1x1 -> 1x4
  _t0_98 = _t0_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_99 = _mm256_mul_pd(_t0_97, _t0_98);

  // 4-BLAC: 1x4 - 1x4
  _t0_100 = _mm256_sub_pd(_t0_95, _t0_99);
  _t0_9 = _t0_100;

  // 1x1 -> 1x4
  _t0_101 = _t0_9;

  // 1x1 -> 1x4
  _t0_102 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_103 = _mm256_div_pd(_t0_101, _t0_102);
  _t0_9 = _t0_103;

  // 1x1 -> 1x4
  _t0_104 = _t0_10;

  // 3x1 -> 4x1
  _t0_105 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_6, _t0_8), _mm256_unpacklo_pd(_t0_9, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t0_106 = _t0_105;

  // 3x1 -> 4x1
  _t0_107 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_6, _t0_8), _mm256_unpacklo_pd(_t0_9, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 * 4x1
  _t0_108 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_mul_pd(_t0_106, _t0_107), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_mul_pd(_t0_106, _t0_107), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_106, _t0_107), _mm256_mul_pd(_t0_106, _t0_107), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_109 = _mm256_sub_pd(_t0_104, _t0_108);
  _t0_10 = _t0_109;

  // 1x1 -> 1x4
  _t0_110 = _t0_10;

  // 4-BLAC: sqrt(1x4)
  _t0_111 = _mm256_sqrt_pd(_t0_110);
  _t0_10 = _t0_111;

  // Constant 1x1 -> 1x4
  _t0_112 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_113 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_114 = _mm256_div_pd(_t0_112, _t0_113);
  _t0_1 = _t0_114;

  // 1x1 -> 1x4
  _t0_115 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_53 = _mm256_mul_pd(_t0_115, _t0_53);

  // 3x4 -> 4x4
  _t0_116 = _t0_11;
  _t0_117 = _t0_12;
  _t0_118 = _t0_13;
  _t0_119 = _mm256_setzero_pd();

  // 1x3 -> 1x4
  _t0_120 = _mm256_blend_pd(_t0_2, _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_6, 1), _mm256_blend_pd(_mm256_setzero_pd(), _t0_6, 1), 8), 12);

  // 4-BLAC: (1x4)^T
  _t0_121 = _t0_120;

  // 4-BLAC: 4x1 * 1x4
  _t0_122 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_121, _t0_121, 32), _mm256_permute2f128_pd(_t0_121, _t0_121, 32), 0), _t0_53);
  _t0_123 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_121, _t0_121, 32), _mm256_permute2f128_pd(_t0_121, _t0_121, 32), 15), _t0_53);
  _t0_124 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_121, _t0_121, 49), _mm256_permute2f128_pd(_t0_121, _t0_121, 49), 0), _t0_53);
  _t0_125 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_121, _t0_121, 49), _mm256_permute2f128_pd(_t0_121, _t0_121, 49), 15), _t0_53);

  // 4-BLAC: 4x4 - 4x4
  _t0_126 = _mm256_sub_pd(_t0_116, _t0_122);
  _t0_127 = _mm256_sub_pd(_t0_117, _t0_123);
  _t0_128 = _mm256_sub_pd(_t0_118, _t0_124);
  _t0_129 = _mm256_sub_pd(_t0_119, _t0_125);
  _t0_11 = _t0_126;
  _t0_12 = _t0_127;
  _t0_13 = _t0_128;

  // Constant 1x1 -> 1x4
  _t0_130 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_131 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_133 = _mm256_div_pd(_t0_130, _t0_131);
  _t0_14 = _t0_133;

  // 1x1 -> 1x4
  _t0_134 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_14, _t0_14, 32), _mm256_permute2f128_pd(_t0_14, _t0_14, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_11 = _mm256_mul_pd(_t0_134, _t0_11);

  // 2x4 -> 4x4
  _t0_135 = _t0_12;
  _t0_136 = _t0_13;
  _t0_137 = _mm256_setzero_pd();
  _t0_138 = _mm256_setzero_pd();

  // 1x2 -> 1x4
  _t0_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_140 = _t0_139;

  // 4-BLAC: 4x1 * 1x4
  _t0_141 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_140, _t0_140, 32), _mm256_permute2f128_pd(_t0_140, _t0_140, 32), 0), _t0_11);
  _t0_142 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_140, _t0_140, 32), _mm256_permute2f128_pd(_t0_140, _t0_140, 32), 15), _t0_11);
  _t0_143 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_140, _t0_140, 49), _mm256_permute2f128_pd(_t0_140, _t0_140, 49), 0), _t0_11);
  _t0_144 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_140, _t0_140, 49), _mm256_permute2f128_pd(_t0_140, _t0_140, 49), 15), _t0_11);

  // 4-BLAC: 4x4 - 4x4
  _t0_145 = _mm256_sub_pd(_t0_135, _t0_141);
  _t0_146 = _mm256_sub_pd(_t0_136, _t0_142);
  _t0_147 = _mm256_sub_pd(_t0_137, _t0_143);
  _t0_148 = _mm256_sub_pd(_t0_138, _t0_144);
  _t0_12 = _t0_145;
  _t0_13 = _t0_146;

  // Constant 1x1 -> 1x4
  _t0_149 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_150 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_152 = _mm256_div_pd(_t0_149, _t0_150);
  _t0_15 = _t0_152;

  // 1x1 -> 1x4
  _t0_153 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_12 = _mm256_mul_pd(_t0_153, _t0_12);

  // 1x1 -> 1x4
  _t0_154 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_155 = _t0_154;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_54 = _mm256_mul_pd(_t0_155, _t0_12);

  // 4-BLAC: 1x4 - 1x4
  _t0_13 = _mm256_sub_pd(_t0_13, _t0_54);

  // Constant 1x1 -> 1x4
  _t0_156 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_157 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_158 = _mm256_div_pd(_t0_156, _t0_157);
  _t0_16 = _t0_158;

  // 1x1 -> 1x4
  _t0_159 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_16, _t0_16, 32), _mm256_permute2f128_pd(_t0_16, _t0_16, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_13 = _mm256_mul_pd(_t0_159, _t0_13);

  // 4x4 -> 4x4 - UpSymm
  _t0_160 = _t0_17;
  _t0_161 = _mm256_blend_pd(_mm256_shuffle_pd(_t0_17, _t0_18, 3), _t0_18, 12);
  _t0_162 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_17, _t0_18, 0), _t0_19, 49);
  _t0_163 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_17, _t0_18, 12), _mm256_shuffle_pd(_t0_19, _t0_20, 12), 49);

  // 4-BLAC: (4x4)^T
  _t0_422 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_53, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 32);
  _t0_423 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_53, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 32);
  _t0_424 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_53, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 49);
  _t0_425 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_53, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_58 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_422, _t0_422, 32), _mm256_permute2f128_pd(_t0_422, _t0_422, 32), 0), _t0_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_422, _t0_422, 32), _mm256_permute2f128_pd(_t0_422, _t0_422, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_422, _t0_422, 49), _mm256_permute2f128_pd(_t0_422, _t0_422, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_422, _t0_422, 49), _mm256_permute2f128_pd(_t0_422, _t0_422, 49), 15), _t0_13)));
  _t0_59 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_423, _t0_423, 32), _mm256_permute2f128_pd(_t0_423, _t0_423, 32), 0), _t0_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_423, _t0_423, 32), _mm256_permute2f128_pd(_t0_423, _t0_423, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_423, _t0_423, 49), _mm256_permute2f128_pd(_t0_423, _t0_423, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_423, _t0_423, 49), _mm256_permute2f128_pd(_t0_423, _t0_423, 49), 15), _t0_13)));
  _t0_60 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_424, _t0_424, 32), _mm256_permute2f128_pd(_t0_424, _t0_424, 32), 0), _t0_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_424, _t0_424, 32), _mm256_permute2f128_pd(_t0_424, _t0_424, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_424, _t0_424, 49), _mm256_permute2f128_pd(_t0_424, _t0_424, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_424, _t0_424, 49), _mm256_permute2f128_pd(_t0_424, _t0_424, 49), 15), _t0_13)));
  _t0_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_425, _t0_425, 32), _mm256_permute2f128_pd(_t0_425, _t0_425, 32), 0), _t0_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_425, _t0_425, 32), _mm256_permute2f128_pd(_t0_425, _t0_425, 32), 15), _t0_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_425, _t0_425, 49), _mm256_permute2f128_pd(_t0_425, _t0_425, 49), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_425, _t0_425, 49), _mm256_permute2f128_pd(_t0_425, _t0_425, 49), 15), _t0_13)));

  // 4-BLAC: 4x4 - 4x4
  _t0_74 = _mm256_sub_pd(_t0_160, _t0_58);
  _t0_75 = _mm256_sub_pd(_t0_161, _t0_59);
  _t0_76 = _mm256_sub_pd(_t0_162, _t0_60);
  _t0_77 = _mm256_sub_pd(_t0_163, _t0_61);

  // 4x4 -> 4x4 - UpSymm
  _t0_17 = _t0_74;
  _t0_18 = _t0_75;
  _t0_19 = _t0_76;
  _t0_20 = _t0_77;

  // 1x1 -> 1x4
  _t0_164 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_17, 1);

  // 4-BLAC: sqrt(1x4)
  _t0_165 = _mm256_sqrt_pd(_t0_164);
  _t0_21 = _t0_165;

  // Constant 1x1 -> 1x4
  _t0_166 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_167 = _t0_21;

  // 4-BLAC: 1x4 / 1x4
  _t0_169 = _mm256_div_pd(_t0_166, _t0_167);
  _t0_22 = _t0_169;

  // 1x1 -> 1x4
  _t0_170 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_22, _t0_22, 32), _mm256_permute2f128_pd(_t0_22, _t0_22, 32), 0);

  // 1x2 -> 1x4
  _t0_171 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_17, 6), _mm256_permute2f128_pd(_t0_17, _t0_17, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_172 = _mm256_mul_pd(_t0_170, _t0_171);
  _t0_23 = _t0_172;

  // 1x1 -> 1x4
  _t0_174 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_175 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_23, 1);

  // 4-BLAC: (4x1)^T
  _t0_176 = _t0_175;

  // 1x1 -> 1x4
  _t0_177 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_23, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_178 = _mm256_mul_pd(_t0_176, _t0_177);

  // 4-BLAC: 1x4 - 1x4
  _t0_179 = _mm256_sub_pd(_t0_174, _t0_178);
  _t0_24 = _t0_179;

  // 1x1 -> 1x4
  _t0_180 = _t0_24;

  // 4-BLAC: sqrt(1x4)
  _t0_181 = _mm256_sqrt_pd(_t0_180);
  _t0_24 = _t0_181;

  // 1x1 -> 1x4
  _t0_182 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 4), 129);

  // 1x1 -> 1x4
  _t0_183 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_23, 1);

  // 4-BLAC: (4x1)^T
  _t0_184 = _t0_183;

  // 1x1 -> 1x4
  _t0_185 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_23, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 Kro 1x4
  _t0_186 = _mm256_mul_pd(_t0_184, _t0_185);

  // 4-BLAC: 1x4 - 1x4
  _t0_188 = _mm256_sub_pd(_t0_182, _t0_186);
  _t0_25 = _t0_188;

  // 1x1 -> 1x4
  _t0_189 = _t0_25;

  // 1x1 -> 1x4
  _t0_190 = _t0_24;

  // 4-BLAC: 1x4 / 1x4
  _t0_191 = _mm256_div_pd(_t0_189, _t0_190);
  _t0_25 = _t0_191;

  // 1x1 -> 1x4
  _t0_192 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_19, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_19, 4), 129);

  // 2x1 -> 4x1
  _t0_193 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_25, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t0_194 = _t0_193;

  // 2x1 -> 4x1
  _t0_195 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_25, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t0_196 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_194, _t0_195), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_194, _t0_195), _mm256_mul_pd(_t0_194, _t0_195), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_194, _t0_195), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_194, _t0_195), _mm256_mul_pd(_t0_194, _t0_195), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_194, _t0_195), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_194, _t0_195), _mm256_mul_pd(_t0_194, _t0_195), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_197 = _mm256_sub_pd(_t0_192, _t0_196);
  _t0_26 = _t0_197;

  // 1x1 -> 1x4
  _t0_198 = _t0_26;

  // 4-BLAC: sqrt(1x4)
  _t0_199 = _mm256_sqrt_pd(_t0_198);
  _t0_26 = _t0_199;

  // 1x1 -> 1x4
  _t0_200 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 129), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_201 = _t0_21;

  // 4-BLAC: 1x4 / 1x4
  _t0_203 = _mm256_div_pd(_t0_200, _t0_201);
  _t0_27 = _t0_203;

  // 2x1 -> 4x1
  _t0_204 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_18, _t0_18, 129), _mm256_permute2f128_pd(_t0_19, _t0_19, 129));

  // 1x2 -> 1x4
  _t0_205 = _t0_23;

  // 4-BLAC: (1x4)^T
  _t0_206 = _t0_205;

  // 1x1 -> 1x4
  _t0_207 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_27, _t0_27, 32), _mm256_permute2f128_pd(_t0_27, _t0_27, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_209 = _mm256_mul_pd(_t0_206, _t0_207);

  // 4-BLAC: 4x1 - 4x1
  _t0_210 = _mm256_sub_pd(_t0_204, _t0_209);
  _t0_28 = _t0_210;

  // 1x1 -> 1x4
  _t0_211 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_28, 1);

  // 1x1 -> 1x4
  _t0_212 = _t0_24;

  // 4-BLAC: 1x4 / 1x4
  _t0_213 = _mm256_div_pd(_t0_211, _t0_212);
  _t0_29 = _t0_213;

  // 1x1 -> 1x4
  _t0_214 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_28, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_215 = _t0_25;

  // 4-BLAC: (4x1)^T
  _t0_216 = _t0_215;

  // 1x1 -> 1x4
  _t0_217 = _t0_29;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_218 = _mm256_mul_pd(_t0_216, _t0_217);

  // 4-BLAC: 1x4 - 1x4
  _t0_219 = _mm256_sub_pd(_t0_214, _t0_218);
  _t0_30 = _t0_219;

  // 1x1 -> 1x4
  _t0_220 = _t0_30;

  // 1x1 -> 1x4
  _t0_221 = _t0_26;

  // 4-BLAC: 1x4 / 1x4
  _t0_223 = _mm256_div_pd(_t0_220, _t0_221);
  _t0_30 = _t0_223;

  // 1x1 -> 1x4
  _t0_224 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_20, _t0_20, 129), _mm256_setzero_pd());

  // 3x1 -> 4x1
  _t0_225 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_30, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t0_226 = _t0_225;

  // 3x1 -> 4x1
  _t0_227 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_30, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 * 4x1
  _t0_228 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_226, _t0_227), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_226, _t0_227), _mm256_mul_pd(_t0_226, _t0_227), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_226, _t0_227), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_226, _t0_227), _mm256_mul_pd(_t0_226, _t0_227), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_226, _t0_227), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_226, _t0_227), _mm256_mul_pd(_t0_226, _t0_227), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_229 = _mm256_sub_pd(_t0_224, _t0_228);
  _t0_31 = _t0_229;

  // 1x1 -> 1x4
  _t0_230 = _t0_31;

  // 4-BLAC: sqrt(1x4)
  _t0_231 = _mm256_sqrt_pd(_t0_230);
  _t0_31 = _t0_231;

  // Constant 1x1 -> 1x4
  _t0_232 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_233 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_234 = _mm256_div_pd(_t0_232, _t0_233);
  _t0_1 = _t0_234;

  // 1x1 -> 1x4
  _t0_235 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_55 = _mm256_mul_pd(_t0_235, _t0_55);

  // 3x4 -> 4x4
  _t0_237 = _t0_32;
  _t0_238 = _t0_33;
  _t0_239 = _t0_34;
  _t0_240 = _mm256_setzero_pd();

  // 1x3 -> 1x4
  _t0_241 = _mm256_blend_pd(_t0_2, _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_6, 1), _mm256_blend_pd(_mm256_setzero_pd(), _t0_6, 1), 8), 12);

  // 4-BLAC: (1x4)^T
  _t0_242 = _t0_241;

  // 4-BLAC: 4x1 * 1x4
  _t0_243 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 32), _mm256_permute2f128_pd(_t0_242, _t0_242, 32), 0), _t0_55);
  _t0_244 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 32), _mm256_permute2f128_pd(_t0_242, _t0_242, 32), 15), _t0_55);
  _t0_245 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 49), _mm256_permute2f128_pd(_t0_242, _t0_242, 49), 0), _t0_55);
  _t0_246 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 49), _mm256_permute2f128_pd(_t0_242, _t0_242, 49), 15), _t0_55);

  // 4-BLAC: 4x4 - 4x4
  _t0_248 = _mm256_sub_pd(_t0_237, _t0_243);
  _t0_249 = _mm256_sub_pd(_t0_238, _t0_244);
  _t0_250 = _mm256_sub_pd(_t0_239, _t0_245);
  _t0_251 = _mm256_sub_pd(_t0_240, _t0_246);
  _t0_32 = _t0_248;
  _t0_33 = _t0_249;
  _t0_34 = _t0_250;

  // Constant 1x1 -> 1x4
  _t0_252 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_253 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_254 = _mm256_div_pd(_t0_252, _t0_253);
  _t0_14 = _t0_254;

  // 1x1 -> 1x4
  _t0_255 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_14, _t0_14, 32), _mm256_permute2f128_pd(_t0_14, _t0_14, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_32 = _mm256_mul_pd(_t0_255, _t0_32);

  // 2x4 -> 4x4
  _t0_256 = _t0_33;
  _t0_257 = _t0_34;
  _t0_258 = _mm256_setzero_pd();
  _t0_259 = _mm256_setzero_pd();

  // 1x2 -> 1x4
  _t0_260 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_261 = _t0_260;

  // 4-BLAC: 4x1 * 1x4
  _t0_262 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 32), _mm256_permute2f128_pd(_t0_261, _t0_261, 32), 0), _t0_32);
  _t0_263 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 32), _mm256_permute2f128_pd(_t0_261, _t0_261, 32), 15), _t0_32);
  _t0_264 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 49), _mm256_permute2f128_pd(_t0_261, _t0_261, 49), 0), _t0_32);
  _t0_265 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 49), _mm256_permute2f128_pd(_t0_261, _t0_261, 49), 15), _t0_32);

  // 4-BLAC: 4x4 - 4x4
  _t0_267 = _mm256_sub_pd(_t0_256, _t0_262);
  _t0_268 = _mm256_sub_pd(_t0_257, _t0_263);
  _t0_269 = _mm256_sub_pd(_t0_258, _t0_264);
  _t0_270 = _mm256_sub_pd(_t0_259, _t0_265);
  _t0_33 = _t0_267;
  _t0_34 = _t0_268;

  // Constant 1x1 -> 1x4
  _t0_271 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_272 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_273 = _mm256_div_pd(_t0_271, _t0_272);
  _t0_15 = _t0_273;

  // 1x1 -> 1x4
  _t0_274 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_33 = _mm256_mul_pd(_t0_274, _t0_33);

  // 1x1 -> 1x4
  _t0_276 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_277 = _t0_276;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_56 = _mm256_mul_pd(_t0_277, _t0_33);

  // 4-BLAC: 1x4 - 1x4
  _t0_34 = _mm256_sub_pd(_t0_34, _t0_56);

  // Constant 1x1 -> 1x4
  _t0_278 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_279 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_280 = _mm256_div_pd(_t0_278, _t0_279);
  _t0_16 = _t0_280;

  // 1x1 -> 1x4
  _t0_281 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_16, _t0_16, 32), _mm256_permute2f128_pd(_t0_16, _t0_16, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_34 = _mm256_mul_pd(_t0_281, _t0_34);

  // 4-BLAC: (4x4)^T
  _t0_426 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_53, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 32);
  _t0_427 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_53, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 32);
  _t0_428 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_53, _t0_11), _mm256_unpacklo_pd(_t0_12, _t0_13), 49);
  _t0_429 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_53, _t0_11), _mm256_unpackhi_pd(_t0_12, _t0_13), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 32), _mm256_permute2f128_pd(_t0_426, _t0_426, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 32), _mm256_permute2f128_pd(_t0_426, _t0_426, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 49), _mm256_permute2f128_pd(_t0_426, _t0_426, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 49), _mm256_permute2f128_pd(_t0_426, _t0_426, 49), 15), _t0_34)));
  _t0_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 32), _mm256_permute2f128_pd(_t0_427, _t0_427, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 32), _mm256_permute2f128_pd(_t0_427, _t0_427, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 49), _mm256_permute2f128_pd(_t0_427, _t0_427, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 49), _mm256_permute2f128_pd(_t0_427, _t0_427, 49), 15), _t0_34)));
  _t0_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 32), _mm256_permute2f128_pd(_t0_428, _t0_428, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 32), _mm256_permute2f128_pd(_t0_428, _t0_428, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 49), _mm256_permute2f128_pd(_t0_428, _t0_428, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 49), _mm256_permute2f128_pd(_t0_428, _t0_428, 49), 15), _t0_34)));
  _t0_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 32), _mm256_permute2f128_pd(_t0_429, _t0_429, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 32), _mm256_permute2f128_pd(_t0_429, _t0_429, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 49), _mm256_permute2f128_pd(_t0_429, _t0_429, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 49), _mm256_permute2f128_pd(_t0_429, _t0_429, 49), 15), _t0_34)));

  // 4-BLAC: 4x4 - 4x4
  _t0_78 = _mm256_sub_pd(_t0_78, _t0_62);
  _t0_79 = _mm256_sub_pd(_t0_79, _t0_63);
  _t0_80 = _mm256_sub_pd(_t0_80, _t0_64);
  _t0_81 = _mm256_sub_pd(_t0_81, _t0_65);

  // Constant 1x1 -> 1x4
  _t0_282 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_283 = _t0_21;

  // 4-BLAC: 1x4 / 1x4
  _t0_284 = _mm256_div_pd(_t0_282, _t0_283);
  _t0_22 = _t0_284;

  // 1x1 -> 1x4
  _t0_285 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_22, _t0_22, 32), _mm256_permute2f128_pd(_t0_22, _t0_22, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_78 = _mm256_mul_pd(_t0_285, _t0_78);

  // 3x4 -> 4x4
  _t0_286 = _t0_79;
  _t0_287 = _t0_80;
  _t0_288 = _t0_81;
  _t0_289 = _mm256_setzero_pd();

  // 1x3 -> 1x4
  _t0_290 = _mm256_blend_pd(_t0_23, _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_27, 1), _mm256_blend_pd(_mm256_setzero_pd(), _t0_27, 1), 8), 12);

  // 4-BLAC: (1x4)^T
  _t0_291 = _t0_290;

  // 4-BLAC: 4x1 * 1x4
  _t0_292 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 32), _mm256_permute2f128_pd(_t0_291, _t0_291, 32), 0), _t0_78);
  _t0_293 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 32), _mm256_permute2f128_pd(_t0_291, _t0_291, 32), 15), _t0_78);
  _t0_294 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 49), _mm256_permute2f128_pd(_t0_291, _t0_291, 49), 0), _t0_78);
  _t0_295 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 49), _mm256_permute2f128_pd(_t0_291, _t0_291, 49), 15), _t0_78);

  // 4-BLAC: 4x4 - 4x4
  _t0_297 = _mm256_sub_pd(_t0_286, _t0_292);
  _t0_298 = _mm256_sub_pd(_t0_287, _t0_293);
  _t0_299 = _mm256_sub_pd(_t0_288, _t0_294);
  _t0_300 = _mm256_sub_pd(_t0_289, _t0_295);
  _t0_79 = _t0_297;
  _t0_80 = _t0_298;
  _t0_81 = _t0_299;

  // Constant 1x1 -> 1x4
  _t0_301 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_302 = _t0_24;

  // 4-BLAC: 1x4 / 1x4
  _t0_303 = _mm256_div_pd(_t0_301, _t0_302);
  _t0_35 = _t0_303;

  // 1x1 -> 1x4
  _t0_304 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_35, _t0_35, 32), _mm256_permute2f128_pd(_t0_35, _t0_35, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_79 = _mm256_mul_pd(_t0_304, _t0_79);

  // 2x4 -> 4x4
  _t0_305 = _t0_80;
  _t0_306 = _t0_81;
  _t0_307 = _mm256_setzero_pd();
  _t0_308 = _mm256_setzero_pd();

  // 1x2 -> 1x4
  _t0_309 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_25, _t0_29), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t0_310 = _t0_309;

  // 4-BLAC: 4x1 * 1x4
  _t0_311 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 32), _mm256_permute2f128_pd(_t0_310, _t0_310, 32), 0), _t0_79);
  _t0_312 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 32), _mm256_permute2f128_pd(_t0_310, _t0_310, 32), 15), _t0_79);
  _t0_313 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 49), _mm256_permute2f128_pd(_t0_310, _t0_310, 49), 0), _t0_79);
  _t0_314 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 49), _mm256_permute2f128_pd(_t0_310, _t0_310, 49), 15), _t0_79);

  // 4-BLAC: 4x4 - 4x4
  _t0_315 = _mm256_sub_pd(_t0_305, _t0_311);
  _t0_316 = _mm256_sub_pd(_t0_306, _t0_312);
  _t0_317 = _mm256_sub_pd(_t0_307, _t0_313);
  _t0_318 = _mm256_sub_pd(_t0_308, _t0_314);
  _t0_80 = _t0_315;
  _t0_81 = _t0_316;

  // Constant 1x1 -> 1x4
  _t0_319 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_320 = _t0_26;

  // 4-BLAC: 1x4 / 1x4
  _t0_321 = _mm256_div_pd(_t0_319, _t0_320);
  _t0_36 = _t0_321;

  // 1x1 -> 1x4
  _t0_323 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_36, _t0_36, 32), _mm256_permute2f128_pd(_t0_36, _t0_36, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_80 = _mm256_mul_pd(_t0_323, _t0_80);

  // 1x1 -> 1x4
  _t0_324 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_30, _t0_30, 32), _mm256_permute2f128_pd(_t0_30, _t0_30, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_325 = _t0_324;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_57 = _mm256_mul_pd(_t0_325, _t0_80);

  // 4-BLAC: 1x4 - 1x4
  _t0_81 = _mm256_sub_pd(_t0_81, _t0_57);

  // Constant 1x1 -> 1x4
  _t0_326 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_327 = _t0_31;

  // 4-BLAC: 1x4 / 1x4
  _t0_328 = _mm256_div_pd(_t0_326, _t0_327);
  _t0_37 = _t0_328;

  // 1x1 -> 1x4
  _t0_329 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_37, _t0_37, 32), _mm256_permute2f128_pd(_t0_37, _t0_37, 32), 0);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_81 = _mm256_mul_pd(_t0_329, _t0_81);

  // 4x4 -> 4x4 - UpSymm
  _t0_330 = _t0_38;
  _t0_331 = _mm256_blend_pd(_mm256_shuffle_pd(_t0_38, _t0_39, 3), _t0_39, 12);
  _t0_332 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_38, _t0_39, 0), _t0_40, 49);
  _t0_333 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_38, _t0_39, 12), _mm256_shuffle_pd(_t0_40, _t0_41, 12), 49);

  // 4-BLAC: (4x4)^T
  _t0_430 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_32), _mm256_unpacklo_pd(_t0_33, _t0_34), 32);
  _t0_431 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_55, _t0_32), _mm256_unpackhi_pd(_t0_33, _t0_34), 32);
  _t0_432 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_32), _mm256_unpacklo_pd(_t0_33, _t0_34), 49);
  _t0_433 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_55, _t0_32), _mm256_unpackhi_pd(_t0_33, _t0_34), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 32), _mm256_permute2f128_pd(_t0_430, _t0_430, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 32), _mm256_permute2f128_pd(_t0_430, _t0_430, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 49), _mm256_permute2f128_pd(_t0_430, _t0_430, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 49), _mm256_permute2f128_pd(_t0_430, _t0_430, 49), 15), _t0_34)));
  _t0_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 32), _mm256_permute2f128_pd(_t0_431, _t0_431, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 32), _mm256_permute2f128_pd(_t0_431, _t0_431, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 49), _mm256_permute2f128_pd(_t0_431, _t0_431, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 49), _mm256_permute2f128_pd(_t0_431, _t0_431, 49), 15), _t0_34)));
  _t0_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 32), _mm256_permute2f128_pd(_t0_432, _t0_432, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 32), _mm256_permute2f128_pd(_t0_432, _t0_432, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 49), _mm256_permute2f128_pd(_t0_432, _t0_432, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 49), _mm256_permute2f128_pd(_t0_432, _t0_432, 49), 15), _t0_34)));
  _t0_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 32), _mm256_permute2f128_pd(_t0_433, _t0_433, 32), 0), _t0_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 32), _mm256_permute2f128_pd(_t0_433, _t0_433, 32), 15), _t0_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 49), _mm256_permute2f128_pd(_t0_433, _t0_433, 49), 0), _t0_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 49), _mm256_permute2f128_pd(_t0_433, _t0_433, 49), 15), _t0_34)));

  // 4-BLAC: 4x4 - 4x4
  _t0_82 = _mm256_sub_pd(_t0_330, _t0_66);
  _t0_83 = _mm256_sub_pd(_t0_331, _t0_67);
  _t0_84 = _mm256_sub_pd(_t0_332, _t0_68);
  _t0_85 = _mm256_sub_pd(_t0_333, _t0_69);

  // 4x4 -> 4x4 - UpSymm
  _t0_38 = _t0_82;
  _t0_39 = _t0_83;
  _t0_40 = _t0_84;
  _t0_41 = _t0_85;

  // 4-BLAC: (4x4)^T
  _t0_434 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_78, _t0_79), _mm256_unpacklo_pd(_t0_80, _t0_81), 32);
  _t0_435 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_78, _t0_79), _mm256_unpackhi_pd(_t0_80, _t0_81), 32);
  _t0_436 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_78, _t0_79), _mm256_unpacklo_pd(_t0_80, _t0_81), 49);
  _t0_437 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t0_78, _t0_79), _mm256_unpackhi_pd(_t0_80, _t0_81), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 32), _mm256_permute2f128_pd(_t0_434, _t0_434, 32), 0), _t0_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 32), _mm256_permute2f128_pd(_t0_434, _t0_434, 32), 15), _t0_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 49), _mm256_permute2f128_pd(_t0_434, _t0_434, 49), 0), _t0_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 49), _mm256_permute2f128_pd(_t0_434, _t0_434, 49), 15), _t0_81)));
  _t0_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 32), _mm256_permute2f128_pd(_t0_435, _t0_435, 32), 0), _t0_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 32), _mm256_permute2f128_pd(_t0_435, _t0_435, 32), 15), _t0_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 49), _mm256_permute2f128_pd(_t0_435, _t0_435, 49), 0), _t0_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 49), _mm256_permute2f128_pd(_t0_435, _t0_435, 49), 15), _t0_81)));
  _t0_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 32), _mm256_permute2f128_pd(_t0_436, _t0_436, 32), 0), _t0_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 32), _mm256_permute2f128_pd(_t0_436, _t0_436, 32), 15), _t0_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 49), _mm256_permute2f128_pd(_t0_436, _t0_436, 49), 0), _t0_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 49), _mm256_permute2f128_pd(_t0_436, _t0_436, 49), 15), _t0_81)));
  _t0_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 32), _mm256_permute2f128_pd(_t0_437, _t0_437, 32), 0), _t0_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 32), _mm256_permute2f128_pd(_t0_437, _t0_437, 32), 15), _t0_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 49), _mm256_permute2f128_pd(_t0_437, _t0_437, 49), 0), _t0_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 49), _mm256_permute2f128_pd(_t0_437, _t0_437, 49), 15), _t0_81)));

  // 4x4 -> 4x4 - UpSymm
  _t0_334 = _t0_38;
  _t0_335 = _mm256_blend_pd(_mm256_shuffle_pd(_t0_38, _t0_39, 3), _t0_39, 12);
  _t0_336 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_38, _t0_39, 0), _t0_40, 49);
  _t0_337 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_38, _t0_39, 12), _mm256_shuffle_pd(_t0_40, _t0_41, 12), 49);

  // 4-BLAC: 4x4 - 4x4
  _t0_334 = _mm256_sub_pd(_t0_334, _t0_70);
  _t0_335 = _mm256_sub_pd(_t0_335, _t0_71);
  _t0_336 = _mm256_sub_pd(_t0_336, _t0_72);
  _t0_337 = _mm256_sub_pd(_t0_337, _t0_73);

  // 4x4 -> 4x4 - UpSymm
  _t0_38 = _t0_334;
  _t0_39 = _t0_335;
  _t0_40 = _t0_336;
  _t0_41 = _t0_337;

  // 1x1 -> 1x4
  _t0_338 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_38, 1);

  // 4-BLAC: sqrt(1x4)
  _t0_339 = _mm256_sqrt_pd(_t0_338);
  _t0_42 = _t0_339;

  // Constant 1x1 -> 1x4
  _t0_340 = _mm256_set_pd(0, 0, 0, 1);

  // 1x1 -> 1x4
  _t0_341 = _t0_42;

  // 4-BLAC: 1x4 / 1x4
  _t0_343 = _mm256_div_pd(_t0_340, _t0_341);
  _t0_43 = _t0_343;

  // 1x1 -> 1x4
  _t0_344 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0);

  // 1x2 -> 1x4
  _t0_345 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_38, 6), _mm256_permute2f128_pd(_t0_38, _t0_38, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_346 = _mm256_mul_pd(_t0_344, _t0_345);
  _t0_44 = _t0_346;

  // 1x1 -> 1x4
  _t0_347 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_39, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_348 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 1);

  // 4-BLAC: (4x1)^T
  _t0_349 = _t0_348;

  // 1x1 -> 1x4
  _t0_350 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t0_351 = _mm256_mul_pd(_t0_349, _t0_350);

  // 4-BLAC: 1x4 - 1x4
  _t0_353 = _mm256_sub_pd(_t0_347, _t0_351);
  _t0_45 = _t0_353;

  // 1x1 -> 1x4
  _t0_354 = _t0_45;

  // 4-BLAC: sqrt(1x4)
  _t0_355 = _mm256_sqrt_pd(_t0_354);
  _t0_45 = _t0_355;

  // 1x1 -> 1x4
  _t0_356 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_39, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_39, 4), 129);

  // 1x1 -> 1x4
  _t0_357 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 1);

  // 4-BLAC: (4x1)^T
  _t0_358 = _t0_357;

  // 1x1 -> 1x4
  _t0_359 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 Kro 1x4
  _t0_360 = _mm256_mul_pd(_t0_358, _t0_359);

  // 4-BLAC: 1x4 - 1x4
  _t0_361 = _mm256_sub_pd(_t0_356, _t0_360);
  _t0_46 = _t0_361;

  // 1x1 -> 1x4
  _t0_362 = _t0_46;

  // 1x1 -> 1x4
  _t0_363 = _t0_45;

  // 4-BLAC: 1x4 / 1x4
  _t0_364 = _mm256_div_pd(_t0_362, _t0_363);
  _t0_46 = _t0_364;

  // 1x1 -> 1x4
  _t0_365 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_40, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_40, 4), 129);

  // 2x1 -> 4x1
  _t0_366 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_44, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_46, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t0_368 = _t0_366;

  // 2x1 -> 4x1
  _t0_369 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_44, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_46, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t0_370 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_368, _t0_369), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_368, _t0_369), _mm256_mul_pd(_t0_368, _t0_369), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_368, _t0_369), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_368, _t0_369), _mm256_mul_pd(_t0_368, _t0_369), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_368, _t0_369), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_368, _t0_369), _mm256_mul_pd(_t0_368, _t0_369), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_371 = _mm256_sub_pd(_t0_365, _t0_370);
  _t0_47 = _t0_371;

  // 1x1 -> 1x4
  _t0_373 = _t0_47;

  // 4-BLAC: sqrt(1x4)
  _t0_374 = _mm256_sqrt_pd(_t0_373);
  _t0_47 = _t0_374;

  // 1x1 -> 1x4
  _t0_375 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_38, _t0_38, 129), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_376 = _t0_42;

  // 4-BLAC: 1x4 / 1x4
  _t0_377 = _mm256_div_pd(_t0_375, _t0_376);
  _t0_48 = _t0_377;

  // 2x1 -> 4x1
  _t0_378 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_39, _t0_39, 129), _mm256_permute2f128_pd(_t0_40, _t0_40, 129));

  // 1x2 -> 1x4
  _t0_379 = _t0_44;

  // 4-BLAC: (1x4)^T
  _t0_380 = _t0_379;

  // 1x1 -> 1x4
  _t0_381 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t0_383 = _mm256_mul_pd(_t0_380, _t0_381);

  // 4-BLAC: 4x1 - 4x1
  _t0_384 = _mm256_sub_pd(_t0_378, _t0_383);
  _t0_49 = _t0_384;

  // 1x1 -> 1x4
  _t0_385 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_49, 1);

  // 1x1 -> 1x4
  _t0_386 = _t0_45;

  // 4-BLAC: 1x4 / 1x4
  _t0_388 = _mm256_div_pd(_t0_385, _t0_386);
  _t0_50 = _t0_388;

  // 1x1 -> 1x4
  _t0_389 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_49, 2), _mm256_setzero_pd());

  // 1x1 -> 1x4
  _t0_390 = _t0_46;

  // 4-BLAC: (4x1)^T
  _t0_391 = _t0_390;

  // 1x1 -> 1x4
  _t0_392 = _t0_50;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_393 = _mm256_mul_pd(_t0_391, _t0_392);

  // 4-BLAC: 1x4 - 1x4
  _t0_394 = _mm256_sub_pd(_t0_389, _t0_393);
  _t0_51 = _t0_394;

  // 1x1 -> 1x4
  _t0_395 = _t0_51;

  // 1x1 -> 1x4
  _t0_396 = _t0_47;

  // 4-BLAC: 1x4 / 1x4
  _t0_397 = _mm256_div_pd(_t0_395, _t0_396);
  _t0_51 = _t0_397;

  // 1x1 -> 1x4
  _t0_398 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 129), _mm256_setzero_pd());

  // 3x1 -> 4x1
  _t0_399 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_48, _t0_50), _mm256_unpacklo_pd(_t0_51, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t0_400 = _t0_399;

  // 3x1 -> 4x1
  _t0_401 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_48, _t0_50), _mm256_unpacklo_pd(_t0_51, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 * 4x1
  _t0_403 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_400, _t0_401), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_400, _t0_401), _mm256_mul_pd(_t0_400, _t0_401), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_400, _t0_401), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_400, _t0_401), _mm256_mul_pd(_t0_400, _t0_401), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_400, _t0_401), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_400, _t0_401), _mm256_mul_pd(_t0_400, _t0_401), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_404 = _mm256_sub_pd(_t0_398, _t0_403);
  _t0_52 = _t0_404;

  // 1x1 -> 1x4
  _t0_405 = _t0_52;

  // 4-BLAC: sqrt(1x4)
  _t0_406 = _mm256_sqrt_pd(_t0_405);
  _t0_52 = _t0_406;

  _mm256_maskstore_pd(A, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_0);
  _mm256_maskstore_pd(A + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_2);
  _mm256_maskstore_pd(A + 53, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_3);
  _mm256_maskstore_pd(A + 54, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_4);
  _mm256_maskstore_pd(A + 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_5);
  _mm256_maskstore_pd(A + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_6);
  _mm256_maskstore_pd(A + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_7);
  _mm256_maskstore_pd(A + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_7, _t0_7, 1));
  _mm256_maskstore_pd(A + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_8);
  _mm256_maskstore_pd(A + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_9);
  _mm256_maskstore_pd(A + 159, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_10);
  _mm256_storeu_pd(A + 4, _t0_53);
  _mm256_storeu_pd(A + 56, _t0_11);
  _mm256_storeu_pd(A + 108, _t0_12);
  _mm256_storeu_pd(A + 160, _t0_13);
  _mm256_storeu_pd(A + 212, _t0_17);
  _mm256_maskstore_pd(A + 264, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t0_18);
  _mm256_maskstore_pd(A + 316, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t0_19);
  _mm256_maskstore_pd(A + 368, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t0_20);
  _mm256_maskstore_pd(A + 212, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_21);
  _mm256_maskstore_pd(A + 213, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_23);
  _mm256_maskstore_pd(A + 265, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_24);
  _mm256_maskstore_pd(A + 266, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_25);
  _mm256_maskstore_pd(A + 318, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_26);
  _mm256_maskstore_pd(A + 215, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_27);
  _mm256_maskstore_pd(A + 267, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_28);
  _mm256_maskstore_pd(A + 319, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_28, _t0_28, 1));
  _mm256_maskstore_pd(A + 267, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_29);
  _mm256_maskstore_pd(A + 319, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_30);
  _mm256_maskstore_pd(A + 371, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_31);
  _mm256_storeu_pd(A + 8, _t0_55);
  _mm256_storeu_pd(A + 60, _t0_32);
  _mm256_storeu_pd(A + 112, _t0_33);
  _mm256_storeu_pd(A + 164, _t0_34);
  _mm256_storeu_pd(A + 216, _t0_78);
  _mm256_storeu_pd(A + 268, _t0_79);
  _mm256_storeu_pd(A + 320, _t0_80);
  _mm256_storeu_pd(A + 372, _t0_81);
  _mm256_storeu_pd(A + 424, _t0_38);
  _mm256_maskstore_pd(A + 476, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t0_39);
  _mm256_maskstore_pd(A + 528, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t0_40);
  _mm256_maskstore_pd(A + 580, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t0_41);
  _mm256_maskstore_pd(A + 424, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_42);
  _mm256_maskstore_pd(A + 425, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_44);
  _mm256_maskstore_pd(A + 477, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_45);
  _mm256_maskstore_pd(A + 478, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_46);
  _mm256_maskstore_pd(A + 530, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_47);
  _mm256_maskstore_pd(A + 427, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_48);
  _mm256_maskstore_pd(A + 479, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_49);
  _mm256_maskstore_pd(A + 531, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_49, _t0_49, 1));
  _mm256_maskstore_pd(A + 479, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_50);
  _mm256_maskstore_pd(A + 531, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_51);
  _mm256_maskstore_pd(A + 583, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_52);

  for( int fi524 = 12; fi524 <= 48; fi524+=4 ) {

    for( int fi603 = 0; fi603 <= fi524 - 5; fi603+=4 ) {
      _t1_6 = _mm256_maskload_pd(A + 53*fi603, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
      _t1_14 = _mm256_loadu_pd(A + fi524 + 52*fi603);
      _t1_8 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 52);
      _t1_9 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 104);
      _t1_10 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 156);
      _t1_5 = _mm256_maskload_pd(A + 53*fi603 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_4 = _mm256_maskload_pd(A + 53*fi603 + 53, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
      _t1_3 = _mm256_maskload_pd(A + 53*fi603 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t1_2 = _mm256_maskload_pd(A + 53*fi603 + 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
      _t1_1 = _mm256_broadcast_sd(&(A[53*fi603 + 107]));
      _t1_0 = _mm256_maskload_pd(A + 53*fi603 + 159, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));

      // Constant 1x1 -> 1x4
      _t1_15 = _mm256_set_pd(0, 0, 0, 1);

      // 1x1 -> 1x4
      _t1_16 = _t1_6;

      // 4-BLAC: 1x4 / 1x4
      _t1_17 = _mm256_div_pd(_t1_15, _t1_16);
      _t1_7 = _t1_17;

      // 1x1 -> 1x4
      _t1_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_7, _t1_7, 32), _mm256_permute2f128_pd(_t1_7, _t1_7, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t1_14 = _mm256_mul_pd(_t1_18, _t1_14);

      // 3x4 -> 4x4
      _t1_19 = _t1_8;
      _t1_20 = _t1_9;
      _t1_21 = _t1_10;
      _t1_22 = _mm256_setzero_pd();

      // 1x3 -> 1x4
      _t1_23 = _t1_5;

      // 4-BLAC: (1x4)^T
      _t0_242 = _t1_23;

      // 4-BLAC: 4x1 * 1x4
      _t0_243 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 32), _mm256_permute2f128_pd(_t0_242, _t0_242, 32), 0), _t1_14);
      _t0_244 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 32), _mm256_permute2f128_pd(_t0_242, _t0_242, 32), 15), _t1_14);
      _t0_245 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 49), _mm256_permute2f128_pd(_t0_242, _t0_242, 49), 0), _t1_14);
      _t0_246 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_242, _t0_242, 49), _mm256_permute2f128_pd(_t0_242, _t0_242, 49), 15), _t1_14);

      // 4-BLAC: 4x4 - 4x4
      _t1_24 = _mm256_sub_pd(_t1_19, _t0_243);
      _t1_25 = _mm256_sub_pd(_t1_20, _t0_244);
      _t1_26 = _mm256_sub_pd(_t1_21, _t0_245);
      _t1_27 = _mm256_sub_pd(_t1_22, _t0_246);
      _t1_8 = _t1_24;
      _t1_9 = _t1_25;
      _t1_10 = _t1_26;

      // Constant 1x1 -> 1x4
      _t1_28 = _mm256_set_pd(0, 0, 0, 1);

      // 1x1 -> 1x4
      _t1_29 = _t1_4;

      // 4-BLAC: 1x4 / 1x4
      _t1_30 = _mm256_div_pd(_t1_28, _t1_29);
      _t1_11 = _t1_30;

      // 1x1 -> 1x4
      _t1_31 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_11, _t1_11, 32), _mm256_permute2f128_pd(_t1_11, _t1_11, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t1_8 = _mm256_mul_pd(_t1_31, _t1_8);

      // 2x4 -> 4x4
      _t1_32 = _t1_9;
      _t1_33 = _t1_10;
      _t1_34 = _mm256_setzero_pd();
      _t1_35 = _mm256_setzero_pd();

      // 1x2 -> 1x4
      _t1_36 = _t1_3;

      // 4-BLAC: (1x4)^T
      _t0_261 = _t1_36;

      // 4-BLAC: 4x1 * 1x4
      _t0_262 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 32), _mm256_permute2f128_pd(_t0_261, _t0_261, 32), 0), _t1_8);
      _t0_263 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 32), _mm256_permute2f128_pd(_t0_261, _t0_261, 32), 15), _t1_8);
      _t0_264 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 49), _mm256_permute2f128_pd(_t0_261, _t0_261, 49), 0), _t1_8);
      _t0_265 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_261, _t0_261, 49), _mm256_permute2f128_pd(_t0_261, _t0_261, 49), 15), _t1_8);

      // 4-BLAC: 4x4 - 4x4
      _t1_37 = _mm256_sub_pd(_t1_32, _t0_262);
      _t1_38 = _mm256_sub_pd(_t1_33, _t0_263);
      _t1_39 = _mm256_sub_pd(_t1_34, _t0_264);
      _t1_40 = _mm256_sub_pd(_t1_35, _t0_265);
      _t1_9 = _t1_37;
      _t1_10 = _t1_38;

      // Constant 1x1 -> 1x4
      _t1_41 = _mm256_set_pd(0, 0, 0, 1);

      // 1x1 -> 1x4
      _t1_42 = _t1_2;

      // 4-BLAC: 1x4 / 1x4
      _t1_43 = _mm256_div_pd(_t1_41, _t1_42);
      _t1_12 = _t1_43;

      // 1x1 -> 1x4
      _t1_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_12, _t1_12, 32), _mm256_permute2f128_pd(_t1_12, _t1_12, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t1_9 = _mm256_mul_pd(_t1_44, _t1_9);

      // 1x1 -> 1x4
      _t1_45 = _t1_1;

      // 4-BLAC: (4x1)^T
      _t0_277 = _t1_45;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_56 = _mm256_mul_pd(_t0_277, _t1_9);

      // 4-BLAC: 1x4 - 1x4
      _t1_10 = _mm256_sub_pd(_t1_10, _t0_56);

      // Constant 1x1 -> 1x4
      _t1_46 = _mm256_set_pd(0, 0, 0, 1);

      // 1x1 -> 1x4
      _t1_47 = _t1_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_48 = _mm256_div_pd(_t1_46, _t1_47);
      _t1_13 = _t1_48;

      // 1x1 -> 1x4
      _t1_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_13, _t1_13, 32), _mm256_permute2f128_pd(_t1_13, _t1_13, 32), 0);

      // 4-BLAC: 1x4 Kro 1x4
      _t1_10 = _mm256_mul_pd(_t1_49, _t1_10);
      _mm256_storeu_pd(A + fi524 + 52*fi603, _t1_14);
      _mm256_storeu_pd(A + fi524 + 52*fi603 + 52, _t1_8);
      _mm256_storeu_pd(A + fi524 + 52*fi603 + 104, _t1_9);
      _mm256_storeu_pd(A + fi524 + 52*fi603 + 156, _t1_10);

      for( int i96 = 0; i96 <= fi524 - fi603 - 5; i96+=4 ) {
        _t2_8 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 52*i96 + 208);
        _t2_9 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 52*i96 + 260);
        _t2_10 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 52*i96 + 312);
        _t2_11 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 52*i96 + 364);
        _t2_7 = _mm256_loadu_pd(A + 53*fi603 + i96 + 4);
        _t2_6 = _mm256_loadu_pd(A + 53*fi603 + i96 + 56);
        _t2_5 = _mm256_loadu_pd(A + 53*fi603 + i96 + 108);
        _t2_4 = _mm256_loadu_pd(A + 53*fi603 + i96 + 160);
        _t2_3 = _mm256_loadu_pd(A + fi524 + 52*fi603);
        _t2_2 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 52);
        _t2_1 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 104);
        _t2_0 = _mm256_loadu_pd(A + fi524 + 52*fi603 + 156);

        // 4-BLAC: (4x4)^T
        _t0_426 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_6), _mm256_unpacklo_pd(_t2_5, _t2_4), 32);
        _t0_427 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t2_7, _t2_6), _mm256_unpackhi_pd(_t2_5, _t2_4), 32);
        _t0_428 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_7, _t2_6), _mm256_unpacklo_pd(_t2_5, _t2_4), 49);
        _t0_429 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t2_7, _t2_6), _mm256_unpackhi_pd(_t2_5, _t2_4), 49);

        // 4-BLAC: 4x4 * 4x4
        _t0_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 32), _mm256_permute2f128_pd(_t0_426, _t0_426, 32), 0), _t2_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 32), _mm256_permute2f128_pd(_t0_426, _t0_426, 32), 15), _t2_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 49), _mm256_permute2f128_pd(_t0_426, _t0_426, 49), 0), _t2_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_426, _t0_426, 49), _mm256_permute2f128_pd(_t0_426, _t0_426, 49), 15), _t2_0)));
        _t0_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 32), _mm256_permute2f128_pd(_t0_427, _t0_427, 32), 0), _t2_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 32), _mm256_permute2f128_pd(_t0_427, _t0_427, 32), 15), _t2_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 49), _mm256_permute2f128_pd(_t0_427, _t0_427, 49), 0), _t2_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_427, _t0_427, 49), _mm256_permute2f128_pd(_t0_427, _t0_427, 49), 15), _t2_0)));
        _t0_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 32), _mm256_permute2f128_pd(_t0_428, _t0_428, 32), 0), _t2_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 32), _mm256_permute2f128_pd(_t0_428, _t0_428, 32), 15), _t2_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 49), _mm256_permute2f128_pd(_t0_428, _t0_428, 49), 0), _t2_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_428, _t0_428, 49), _mm256_permute2f128_pd(_t0_428, _t0_428, 49), 15), _t2_0)));
        _t0_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 32), _mm256_permute2f128_pd(_t0_429, _t0_429, 32), 0), _t2_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 32), _mm256_permute2f128_pd(_t0_429, _t0_429, 32), 15), _t2_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 49), _mm256_permute2f128_pd(_t0_429, _t0_429, 49), 0), _t2_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_429, _t0_429, 49), _mm256_permute2f128_pd(_t0_429, _t0_429, 49), 15), _t2_0)));

        // 4-BLAC: 4x4 - 4x4
        _t2_8 = _mm256_sub_pd(_t2_8, _t0_62);
        _t2_9 = _mm256_sub_pd(_t2_9, _t0_63);
        _t2_10 = _mm256_sub_pd(_t2_10, _t0_64);
        _t2_11 = _mm256_sub_pd(_t2_11, _t0_65);
        _mm256_storeu_pd(A + fi524 + 52*fi603 + 52*i96 + 208, _t2_8);
        _mm256_storeu_pd(A + fi524 + 52*fi603 + 52*i96 + 260, _t2_9);
        _mm256_storeu_pd(A + fi524 + 52*fi603 + 52*i96 + 312, _t2_10);
        _mm256_storeu_pd(A + fi524 + 52*fi603 + 52*i96 + 364, _t2_11);
      }
    }
    _t3_10 = _mm256_maskload_pd(A + 53*fi524 - 212, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t3_22 = _mm256_loadu_pd(A + 53*fi524 - 208);
    _t3_12 = _mm256_loadu_pd(A + 53*fi524 - 156);
    _t3_13 = _mm256_loadu_pd(A + 53*fi524 - 104);
    _t3_14 = _mm256_loadu_pd(A + 53*fi524 - 52);
    _t3_9 = _mm256_maskload_pd(A + 53*fi524 - 211, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t3_8 = _mm256_maskload_pd(A + 53*fi524 - 159, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t3_7 = _mm256_maskload_pd(A + 53*fi524 - 158, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t3_6 = _mm256_maskload_pd(A + 53*fi524 - 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t3_5 = _mm256_broadcast_sd(&(A[53*fi524 - 105]));
    _t3_4 = _mm256_maskload_pd(A + 53*fi524 - 53, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
    _t3_18 = _mm256_loadu_pd(A + 53*fi524);
    _t3_19 = _mm256_maskload_pd(A + 53*fi524 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t3_20 = _mm256_maskload_pd(A + 53*fi524 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t3_21 = _mm256_maskload_pd(A + 53*fi524 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t3_3 = _mm256_loadu_pd(A + fi524);
    _t3_2 = _mm256_loadu_pd(A + fi524 + 52);
    _t3_1 = _mm256_loadu_pd(A + fi524 + 104);
    _t3_0 = _mm256_loadu_pd(A + fi524 + 156);

    // Constant 1x1 -> 1x4
    _t3_23 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t3_24 = _t3_10;

    // 4-BLAC: 1x4 / 1x4
    _t3_25 = _mm256_div_pd(_t3_23, _t3_24);
    _t3_11 = _t3_25;

    // 1x1 -> 1x4
    _t3_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_11, _t3_11, 32), _mm256_permute2f128_pd(_t3_11, _t3_11, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t3_22 = _mm256_mul_pd(_t3_26, _t3_22);

    // 3x4 -> 4x4
    _t3_27 = _t3_12;
    _t3_28 = _t3_13;
    _t3_29 = _t3_14;
    _t3_30 = _mm256_setzero_pd();

    // 1x3 -> 1x4
    _t3_31 = _t3_9;

    // 4-BLAC: (1x4)^T
    _t0_291 = _t3_31;

    // 4-BLAC: 4x1 * 1x4
    _t0_292 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 32), _mm256_permute2f128_pd(_t0_291, _t0_291, 32), 0), _t3_22);
    _t0_293 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 32), _mm256_permute2f128_pd(_t0_291, _t0_291, 32), 15), _t3_22);
    _t0_294 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 49), _mm256_permute2f128_pd(_t0_291, _t0_291, 49), 0), _t3_22);
    _t0_295 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_291, _t0_291, 49), _mm256_permute2f128_pd(_t0_291, _t0_291, 49), 15), _t3_22);

    // 4-BLAC: 4x4 - 4x4
    _t3_32 = _mm256_sub_pd(_t3_27, _t0_292);
    _t3_33 = _mm256_sub_pd(_t3_28, _t0_293);
    _t3_34 = _mm256_sub_pd(_t3_29, _t0_294);
    _t3_35 = _mm256_sub_pd(_t3_30, _t0_295);
    _t3_12 = _t3_32;
    _t3_13 = _t3_33;
    _t3_14 = _t3_34;

    // Constant 1x1 -> 1x4
    _t3_36 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t3_37 = _t3_8;

    // 4-BLAC: 1x4 / 1x4
    _t3_38 = _mm256_div_pd(_t3_36, _t3_37);
    _t3_15 = _t3_38;

    // 1x1 -> 1x4
    _t3_39 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_15, _t3_15, 32), _mm256_permute2f128_pd(_t3_15, _t3_15, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t3_12 = _mm256_mul_pd(_t3_39, _t3_12);

    // 2x4 -> 4x4
    _t3_40 = _t3_13;
    _t3_41 = _t3_14;
    _t3_42 = _mm256_setzero_pd();
    _t3_43 = _mm256_setzero_pd();

    // 1x2 -> 1x4
    _t3_44 = _t3_7;

    // 4-BLAC: (1x4)^T
    _t0_310 = _t3_44;

    // 4-BLAC: 4x1 * 1x4
    _t0_311 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 32), _mm256_permute2f128_pd(_t0_310, _t0_310, 32), 0), _t3_12);
    _t0_312 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 32), _mm256_permute2f128_pd(_t0_310, _t0_310, 32), 15), _t3_12);
    _t0_313 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 49), _mm256_permute2f128_pd(_t0_310, _t0_310, 49), 0), _t3_12);
    _t0_314 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_310, _t0_310, 49), _mm256_permute2f128_pd(_t0_310, _t0_310, 49), 15), _t3_12);

    // 4-BLAC: 4x4 - 4x4
    _t3_45 = _mm256_sub_pd(_t3_40, _t0_311);
    _t3_46 = _mm256_sub_pd(_t3_41, _t0_312);
    _t3_47 = _mm256_sub_pd(_t3_42, _t0_313);
    _t3_48 = _mm256_sub_pd(_t3_43, _t0_314);
    _t3_13 = _t3_45;
    _t3_14 = _t3_46;

    // Constant 1x1 -> 1x4
    _t3_49 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t3_50 = _t3_6;

    // 4-BLAC: 1x4 / 1x4
    _t3_51 = _mm256_div_pd(_t3_49, _t3_50);
    _t3_16 = _t3_51;

    // 1x1 -> 1x4
    _t3_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_16, _t3_16, 32), _mm256_permute2f128_pd(_t3_16, _t3_16, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t3_13 = _mm256_mul_pd(_t3_52, _t3_13);

    // 1x1 -> 1x4
    _t3_53 = _t3_5;

    // 4-BLAC: (4x1)^T
    _t0_325 = _t3_53;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_57 = _mm256_mul_pd(_t0_325, _t3_13);

    // 4-BLAC: 1x4 - 1x4
    _t3_14 = _mm256_sub_pd(_t3_14, _t0_57);

    // Constant 1x1 -> 1x4
    _t3_54 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t3_55 = _t3_4;

    // 4-BLAC: 1x4 / 1x4
    _t3_56 = _mm256_div_pd(_t3_54, _t3_55);
    _t3_17 = _t3_56;

    // 1x1 -> 1x4
    _t3_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_17, _t3_17, 32), _mm256_permute2f128_pd(_t3_17, _t3_17, 32), 0);

    // 4-BLAC: 1x4 Kro 1x4
    _t3_14 = _mm256_mul_pd(_t3_57, _t3_14);

    // 4x4 -> 4x4 - UpSymm
    _t3_58 = _t3_18;
    _t3_59 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_18, _t3_19, 3), _t3_19, 12);
    _t3_60 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_18, _t3_19, 0), _t3_20, 49);
    _t3_61 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_18, _t3_19, 12), _mm256_shuffle_pd(_t3_20, _t3_21, 12), 49);

    // 4-BLAC: (4x4)^T
    _t0_430 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32);
    _t0_431 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t3_3, _t3_2), _mm256_unpackhi_pd(_t3_1, _t3_0), 32);
    _t0_432 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 49);
    _t0_433 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t3_3, _t3_2), _mm256_unpackhi_pd(_t3_1, _t3_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t0_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 32), _mm256_permute2f128_pd(_t0_430, _t0_430, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 32), _mm256_permute2f128_pd(_t0_430, _t0_430, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 49), _mm256_permute2f128_pd(_t0_430, _t0_430, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_430, _t0_430, 49), _mm256_permute2f128_pd(_t0_430, _t0_430, 49), 15), _t3_0)));
    _t0_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 32), _mm256_permute2f128_pd(_t0_431, _t0_431, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 32), _mm256_permute2f128_pd(_t0_431, _t0_431, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 49), _mm256_permute2f128_pd(_t0_431, _t0_431, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_431, _t0_431, 49), _mm256_permute2f128_pd(_t0_431, _t0_431, 49), 15), _t3_0)));
    _t0_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 32), _mm256_permute2f128_pd(_t0_432, _t0_432, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 32), _mm256_permute2f128_pd(_t0_432, _t0_432, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 49), _mm256_permute2f128_pd(_t0_432, _t0_432, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_432, _t0_432, 49), _mm256_permute2f128_pd(_t0_432, _t0_432, 49), 15), _t3_0)));
    _t0_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 32), _mm256_permute2f128_pd(_t0_433, _t0_433, 32), 0), _t3_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 32), _mm256_permute2f128_pd(_t0_433, _t0_433, 32), 15), _t3_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 49), _mm256_permute2f128_pd(_t0_433, _t0_433, 49), 0), _t3_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_433, _t0_433, 49), _mm256_permute2f128_pd(_t0_433, _t0_433, 49), 15), _t3_0)));

    // 4-BLAC: 4x4 - 4x4
    _t0_82 = _mm256_sub_pd(_t3_58, _t0_66);
    _t0_83 = _mm256_sub_pd(_t3_59, _t0_67);
    _t0_84 = _mm256_sub_pd(_t3_60, _t0_68);
    _t0_85 = _mm256_sub_pd(_t3_61, _t0_69);

    // 4x4 -> 4x4 - UpSymm
    _t3_18 = _t0_82;
    _t3_19 = _t0_83;
    _t3_20 = _t0_84;
    _t3_21 = _t0_85;
    _mm256_storeu_pd(A + 53*fi524 - 208, _t3_22);
    _mm256_storeu_pd(A + 53*fi524 - 156, _t3_12);
    _mm256_storeu_pd(A + 53*fi524 - 104, _t3_13);
    _mm256_storeu_pd(A + 53*fi524 - 52, _t3_14);

    for( int i96 = 4; i96 <= fi524 - 1; i96+=4 ) {
      _t4_3 = _mm256_loadu_pd(A + fi524 + 52*i96);
      _t4_2 = _mm256_loadu_pd(A + fi524 + 52*i96 + 52);
      _t4_1 = _mm256_loadu_pd(A + fi524 + 52*i96 + 104);
      _t4_0 = _mm256_loadu_pd(A + fi524 + 52*i96 + 156);

      // 4-BLAC: (4x4)^T
      _t0_434 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 32);
      _t0_435 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 32);
      _t0_436 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 49);
      _t0_437 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t0_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 32), _mm256_permute2f128_pd(_t0_434, _t0_434, 32), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 32), _mm256_permute2f128_pd(_t0_434, _t0_434, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 49), _mm256_permute2f128_pd(_t0_434, _t0_434, 49), 0), _t4_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_434, _t0_434, 49), _mm256_permute2f128_pd(_t0_434, _t0_434, 49), 15), _t4_0)));
      _t0_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 32), _mm256_permute2f128_pd(_t0_435, _t0_435, 32), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 32), _mm256_permute2f128_pd(_t0_435, _t0_435, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 49), _mm256_permute2f128_pd(_t0_435, _t0_435, 49), 0), _t4_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_435, _t0_435, 49), _mm256_permute2f128_pd(_t0_435, _t0_435, 49), 15), _t4_0)));
      _t0_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 32), _mm256_permute2f128_pd(_t0_436, _t0_436, 32), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 32), _mm256_permute2f128_pd(_t0_436, _t0_436, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 49), _mm256_permute2f128_pd(_t0_436, _t0_436, 49), 0), _t4_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_436, _t0_436, 49), _mm256_permute2f128_pd(_t0_436, _t0_436, 49), 15), _t4_0)));
      _t0_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 32), _mm256_permute2f128_pd(_t0_437, _t0_437, 32), 0), _t4_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 32), _mm256_permute2f128_pd(_t0_437, _t0_437, 32), 15), _t4_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 49), _mm256_permute2f128_pd(_t0_437, _t0_437, 49), 0), _t4_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_437, _t0_437, 49), _mm256_permute2f128_pd(_t0_437, _t0_437, 49), 15), _t4_0)));

      // 4x4 -> 4x4 - UpSymm
      _t4_4 = _t3_18;
      _t4_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_18, _t3_19, 3), _t3_19, 12);
      _t4_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_18, _t3_19, 0), _t3_20, 49);
      _t4_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_18, _t3_19, 12), _mm256_shuffle_pd(_t3_20, _t3_21, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t4_4 = _mm256_sub_pd(_t4_4, _t0_70);
      _t4_5 = _mm256_sub_pd(_t4_5, _t0_71);
      _t4_6 = _mm256_sub_pd(_t4_6, _t0_72);
      _t4_7 = _mm256_sub_pd(_t4_7, _t0_73);

      // 4x4 -> 4x4 - UpSymm
      _t3_18 = _t4_4;
      _t3_19 = _t4_5;
      _t3_20 = _t4_6;
      _t3_21 = _t4_7;
    }

    // 1x1 -> 1x4
    _t5_11 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_18, 1);

    // 4-BLAC: sqrt(1x4)
    _t5_12 = _mm256_sqrt_pd(_t5_11);
    _t5_0 = _t5_12;

    // Constant 1x1 -> 1x4
    _t5_13 = _mm256_set_pd(0, 0, 0, 1);

    // 1x1 -> 1x4
    _t5_14 = _t5_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_15 = _mm256_div_pd(_t5_13, _t5_14);
    _t5_1 = _t5_15;

    // 1x1 -> 1x4
    _t5_16 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_1, _t5_1, 32), _mm256_permute2f128_pd(_t5_1, _t5_1, 32), 0);

    // 1x2 -> 1x4
    _t5_17 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_18, 6), _mm256_permute2f128_pd(_t3_18, _t3_18, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t5_18 = _mm256_mul_pd(_t5_16, _t5_17);
    _t5_2 = _t5_18;

    // 1x1 -> 1x4
    _t5_19 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_19, 2), _mm256_setzero_pd());

    // 1x1 -> 1x4
    _t5_20 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_2, 1);

    // 4-BLAC: (4x1)^T
    _t0_349 = _t5_20;

    // 1x1 -> 1x4
    _t5_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_2, 1);

    // 4-BLAC: 1x4 Kro 1x4
    _t0_351 = _mm256_mul_pd(_t0_349, _t5_21);

    // 4-BLAC: 1x4 - 1x4
    _t5_22 = _mm256_sub_pd(_t5_19, _t0_351);
    _t5_3 = _t5_22;

    // 1x1 -> 1x4
    _t5_23 = _t5_3;

    // 4-BLAC: sqrt(1x4)
    _t5_24 = _mm256_sqrt_pd(_t5_23);
    _t5_3 = _t5_24;

    // 1x1 -> 1x4
    _t5_25 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_19, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t3_19, 4), 129);

    // 1x1 -> 1x4
    _t5_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_2, 1);

    // 4-BLAC: (4x1)^T
    _t0_358 = _t5_26;

    // 1x1 -> 1x4
    _t5_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_2, 2), _mm256_setzero_pd());

    // 4-BLAC: 1x4 Kro 1x4
    _t0_360 = _mm256_mul_pd(_t0_358, _t5_27);

    // 4-BLAC: 1x4 - 1x4
    _t5_28 = _mm256_sub_pd(_t5_25, _t0_360);
    _t5_4 = _t5_28;

    // 1x1 -> 1x4
    _t5_29 = _t5_4;

    // 1x1 -> 1x4
    _t5_30 = _t5_3;

    // 4-BLAC: 1x4 / 1x4
    _t5_31 = _mm256_div_pd(_t5_29, _t5_30);
    _t5_4 = _t5_31;

    // 1x1 -> 1x4
    _t5_32 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_20, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t3_20, 4), 129);

    // 2x1 -> 4x1
    _t5_33 = _mm256_shuffle_pd(_mm256_blend_pd(_t5_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t5_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (4x1)^T
    _t0_368 = _t5_33;

    // 2x1 -> 4x1
    _t5_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t5_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t5_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: 1x4 * 4x1
    _t0_370 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_368, _t5_34), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_368, _t5_34), _mm256_mul_pd(_t0_368, _t5_34), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_368, _t5_34), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_368, _t5_34), _mm256_mul_pd(_t0_368, _t5_34), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_368, _t5_34), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_368, _t5_34), _mm256_mul_pd(_t0_368, _t5_34), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t5_35 = _mm256_sub_pd(_t5_32, _t0_370);
    _t5_5 = _t5_35;

    // 1x1 -> 1x4
    _t5_36 = _t5_5;

    // 4-BLAC: sqrt(1x4)
    _t5_37 = _mm256_sqrt_pd(_t5_36);
    _t5_5 = _t5_37;

    // 1x1 -> 1x4
    _t5_38 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t3_18, _t3_18, 129), _mm256_setzero_pd());

    // 1x1 -> 1x4
    _t5_39 = _t5_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_40 = _mm256_div_pd(_t5_38, _t5_39);
    _t5_6 = _t5_40;

    // 2x1 -> 4x1
    _t5_41 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t3_19, _t3_19, 129), _mm256_permute2f128_pd(_t3_20, _t3_20, 129));

    // 1x2 -> 1x4
    _t5_42 = _t5_2;

    // 4-BLAC: (1x4)^T
    _t0_380 = _t5_42;

    // 1x1 -> 1x4
    _t5_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_6, _t5_6, 32), _mm256_permute2f128_pd(_t5_6, _t5_6, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t0_383 = _mm256_mul_pd(_t0_380, _t5_43);

    // 4-BLAC: 4x1 - 4x1
    _t5_44 = _mm256_sub_pd(_t5_41, _t0_383);
    _t5_7 = _t5_44;

    // 1x1 -> 1x4
    _t5_45 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_7, 1);

    // 1x1 -> 1x4
    _t5_46 = _t5_3;

    // 4-BLAC: 1x4 / 1x4
    _t5_47 = _mm256_div_pd(_t5_45, _t5_46);
    _t5_8 = _t5_47;

    // 1x1 -> 1x4
    _t5_48 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_7, 2), _mm256_setzero_pd());

    // 1x1 -> 1x4
    _t5_49 = _t5_4;

    // 4-BLAC: (4x1)^T
    _t0_391 = _t5_49;

    // 1x1 -> 1x4
    _t5_50 = _t5_8;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_393 = _mm256_mul_pd(_t0_391, _t5_50);

    // 4-BLAC: 1x4 - 1x4
    _t5_51 = _mm256_sub_pd(_t5_48, _t0_393);
    _t5_9 = _t5_51;

    // 1x1 -> 1x4
    _t5_52 = _t5_9;

    // 1x1 -> 1x4
    _t5_53 = _t5_5;

    // 4-BLAC: 1x4 / 1x4
    _t5_54 = _mm256_div_pd(_t5_52, _t5_53);
    _t5_9 = _t5_54;

    // 1x1 -> 1x4
    _t5_55 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t3_21, _t3_21, 129), _mm256_setzero_pd());

    // 3x1 -> 4x1
    _t5_56 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_6, _t5_8), _mm256_unpacklo_pd(_t5_9, _mm256_setzero_pd()), 32);

    // 4-BLAC: (4x1)^T
    _t0_400 = _t5_56;

    // 3x1 -> 4x1
    _t5_57 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_6, _t5_8), _mm256_unpacklo_pd(_t5_9, _mm256_setzero_pd()), 32);

    // 4-BLAC: 1x4 * 4x1
    _t0_403 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_400, _t5_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_400, _t5_57), _mm256_mul_pd(_t0_400, _t5_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_400, _t5_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_400, _t5_57), _mm256_mul_pd(_t0_400, _t5_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_400, _t5_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_400, _t5_57), _mm256_mul_pd(_t0_400, _t5_57), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t5_58 = _mm256_sub_pd(_t5_55, _t0_403);
    _t5_10 = _t5_58;

    // 1x1 -> 1x4
    _t5_59 = _t5_10;

    // 4-BLAC: sqrt(1x4)
    _t5_60 = _mm256_sqrt_pd(_t5_59);
    _t5_10 = _t5_60;
    _mm256_storeu_pd(A + 53*fi524, _t3_18);
    _mm256_maskstore_pd(A + 53*fi524 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t3_19);
    _mm256_maskstore_pd(A + 53*fi524 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t3_20);
    _mm256_maskstore_pd(A + 53*fi524 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t3_21);
    _mm256_maskstore_pd(A + 53*fi524, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_0);
    _mm256_maskstore_pd(A + 53*fi524 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t5_2);
    _mm256_maskstore_pd(A + 53*fi524 + 53, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_3);
    _mm256_maskstore_pd(A + 53*fi524 + 54, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_4);
    _mm256_maskstore_pd(A + 53*fi524 + 106, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_5);
    _mm256_maskstore_pd(A + 53*fi524 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_6);
    _mm256_maskstore_pd(A + 53*fi524 + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_7);
    _mm256_maskstore_pd(A + 53*fi524 + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t5_7, _t5_7, 1));
    _mm256_maskstore_pd(A + 53*fi524 + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_8);
    _mm256_maskstore_pd(A + 53*fi524 + 107, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_9);
    _mm256_maskstore_pd(A + 53*fi524 + 159, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_10);
  }

}
