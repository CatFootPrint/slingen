/*
 * gpr_kernel.h
 *
Decl { {u'K': Symmetric[K, (36, 36), LSMatAccess], u'L': LowerTriangular[L, (36, 36), GenMatAccess], u'var': Scalar[var, (1, 1), GenMatAccess], u'L0': LowerTriangular[L0, (36, 36), GenMatAccess], u'X': SquaredMatrix[X, (36, 36), GenMatAccess], 'T1473': Matrix[T1473, (1, 36), GenMatAccess], u'a': Matrix[a, (36, 1), GenMatAccess], u'f': Scalar[f, (1, 1), GenMatAccess], u't2': Matrix[t2, (36, 1), GenMatAccess], u't0': Matrix[t0, (36, 1), GenMatAccess], u't1': Matrix[t1, (36, 1), GenMatAccess], u'lp': Scalar[lp, (1, 1), GenMatAccess], u'v': Matrix[v, (36, 1), GenMatAccess], u'y': Matrix[y, (36, 1), GenMatAccess], u'x': Matrix[x, (36, 1), GenMatAccess], u'kx': Matrix[kx, (36, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'rdiv_ltn_ow_opt': {'m': 'm1.ll', 'n': 'n4.ll'}, 'Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt': {'m0': 'm01.ll'}}, 'cl1ck_v': 1, 'variant_tag': 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt_m01_Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt_m04_m21_rdiv_ltn_ow_opt_m1_n4'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1473[1,36],h(1, 36, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 1), L[36,36],h(1, 36, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1473[1,36],h(1, 36, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 1), L[36,36],h(1, 36, 0)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L[36,36],h(1, 36, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L[36,36],h(1, 36, 0)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(1, 36, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(1, 36, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(1, 36, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L[36,36],h(1, 36, 0)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, 2), L[36,36],h(1, 36, 1)) ) = ( Tile( (1, 1), G(h(1, 36, 2), L[36,36],h(1, 36, 1)) ) Div Tile( (1, 1), G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(2, 36, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(2, 36, 0)) ) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 0)) ) Div Tile( (1, 1), G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(2, 36, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(2, 36, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(1, 36, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 1), L[36,36],h(1, 36, 0)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 1)) ) = ( Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 1)) ) Div Tile( (1, 1), G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(1, 36, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L[36,36],h(1, 36, 1)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) = ( Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) Div Tile( (1, 1), G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(3, 36, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L[36,36],h(3, 36, 0)) ) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ) )
Eq.ann: {}
Entry 16:
For_{fi390;4;32;4} ( Entry 0:
For_{fi469;0;fi390 - 5;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) ) Div Tile( (1, 1), G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(3, 36, fi469 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(3, 36, fi469 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(2, 36, fi469 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(2, 36, fi469 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) ) Div Tile( (1, 1), G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, fi469 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, fi469 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, fi469 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, fi469 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) ) Div Tile( (1, 1), G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, fi469 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, fi469 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi469 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi469 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) ) Div Tile( (1, 1), G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi469 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi469 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi469 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi469 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), L[36,36],h(fi390 - fi469 - 4, 36, fi469 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), L[36,36],h(fi390 - fi469 - 4, 36, fi469 + 4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), L[36,36],h(4, 36, fi469)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(fi390 - fi469 - 4, 36, fi469 + 4), L[36,36],h(4, 36, fi469)) ) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) Div Tile( (1, 1), G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), L[36,36],h(4, 36, fi390)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), K[36,36],h(4, 36, fi390)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), L[36,36],h(fi390, 36, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi390), L[36,36],h(fi390, 36, 0)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1473[1,36],h(1, 36, fi390)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1473[1,36],h(1, 36, fi390)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi390)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi390)) ) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) ) Div Tile( (1, 1), G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi390 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi390 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi390)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi390)) ) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) ) )
Eq.ann: {}
 )Entry 17:
For_{fi615;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi615), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi615), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi615), L0[36,36],h(1, 36, fi615)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi615 + 1), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi615 + 1), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi615 + 1), L0[36,36],h(1, 36, fi615)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi615), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, fi615 + 1), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi615 + 1), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi615 + 1), L0[36,36],h(1, 36, fi615 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi615 + 2), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi615 + 2), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi615 + 2), L0[36,36],h(1, 36, fi615 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi615 + 1), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi615 + 2), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi615 + 2), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi615 + 2), L0[36,36],h(1, 36, fi615 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi615 + 3), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi615 + 3), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi615 + 3), L0[36,36],h(1, 36, fi615 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi615 + 2), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, fi615 + 3), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi615 + 3), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi615 + 3), L0[36,36],h(1, 36, fi615 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi615 + 32, 36, fi615 + 4), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi615 + 32, 36, fi615 + 4), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi615 + 32, 36, fi615 + 4), L0[36,36],h(4, 36, fi615)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi615), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 18:
Eq: Tile( (1, 1), G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ) )
Eq.ann: {}
Entry 25:
For_{fi692;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, -fi692 + 35), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi692 + 35), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi692 + 35), L0[36,36],h(1, 36, -fi692 + 35)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 35), L0[36,36],h(3, 36, -fi692 + 32)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 35), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, -fi692 + 34), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi692 + 34), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi692 + 34), L0[36,36],h(1, 36, -fi692 + 34)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 34), L0[36,36],h(2, 36, -fi692 + 32)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 34), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, -fi692 + 33), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi692 + 33), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi692 + 33), L0[36,36],h(1, 36, -fi692 + 33)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 33), L0[36,36],h(1, 36, -fi692 + 32)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi692 + 33), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi692 + 32), L0[36,36],h(1, 36, -fi692 + 32)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi692 + 32, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi692 + 32, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, -fi692 + 32), L0[36,36],h(-fi692 + 32, 36, 0)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 36, -fi692 + 32), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 26:
Eq: Tile( (1, 1), G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 3), L0[36,36],h(1, 36, 3)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L0[36,36],h(3, 36, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 2), L0[36,36],h(1, 36, 2)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L0[36,36],h(2, 36, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 1), L0[36,36],h(1, 36, 1)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L0[36,36],h(1, 36, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 0), L0[36,36],h(1, 36, 0)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), X[36,36] ) ) * Tile( (1, 1), Tile( (4, 4), x[36,1] ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), f[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[36,1] ) ) )
Eq.ann: {}
Entry 35:
For_{fi769;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi769), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi769), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi769), L0[36,36],h(1, 36, fi769)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi769 + 1), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi769 + 1), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi769 + 1), L0[36,36],h(1, 36, fi769)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi769), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, fi769 + 1), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi769 + 1), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi769 + 1), L0[36,36],h(1, 36, fi769 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi769 + 2), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi769 + 2), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi769 + 2), L0[36,36],h(1, 36, fi769 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi769 + 1), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi769 + 2), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi769 + 2), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi769 + 2), L0[36,36],h(1, 36, fi769 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi769 + 3), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi769 + 3), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi769 + 3), L0[36,36],h(1, 36, fi769 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi769 + 2), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, fi769 + 3), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi769 + 3), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi769 + 3), L0[36,36],h(1, 36, fi769 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi769 + 32, 36, fi769 + 4), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi769 + 32, 36, fi769 + 4), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi769 + 32, 36, fi769 + 4), L0[36,36],h(4, 36, fi769)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi769), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 36:
Eq: Tile( (1, 1), G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), Tile( (4, 4), var[1,1] ) ) = ( ( T( Tile( (1, 1), Tile( (4, 4), x[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), x[36,1] ) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), lp[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), y[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[36,1] ) ) )
Eq.ann: {}
 *
 * Created on: 2017-09-02
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 36
#define PARAM1 36

#define ERRTHRESH 1e-5

#define SOFTERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * X, double const * x, double * K, double * y, double * kx, double * f, double * var, double * lp)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205, _t0_206, _t0_207,
	_t0_208, _t0_209, _t0_210, _t0_211, _t0_212, _t0_213, _t0_214, _t0_215,
	_t0_216, _t0_217, _t0_218, _t0_219, _t0_220, _t0_221, _t0_222, _t0_223,
	_t0_224, _t0_225, _t0_226, _t0_227, _t0_228, _t0_229, _t0_230, _t0_231,
	_t0_232, _t0_233, _t0_234, _t0_235, _t0_236, _t0_237, _t0_238, _t0_239,
	_t0_240, _t0_241, _t0_242, _t0_243, _t0_244, _t0_245, _t0_246, _t0_247,
	_t0_248, _t0_249, _t0_250, _t0_251, _t0_252, _t0_253, _t0_254, _t0_255,
	_t0_256, _t0_257, _t0_258, _t0_259, _t0_260, _t0_261, _t0_262, _t0_263,
	_t0_264, _t0_265, _t0_266, _t0_267, _t0_268, _t0_269, _t0_270, _t0_271,
	_t0_272, _t0_273, _t0_274, _t0_275, _t0_276, _t0_277, _t0_278, _t0_279,
	_t0_280, _t0_281, _t0_282, _t0_283, _t0_284, _t0_285, _t0_286, _t0_287,
	_t0_288, _t0_289, _t0_290, _t0_291, _t0_292, _t0_293, _t0_294, _t0_295,
	_t0_296, _t0_297, _t0_298, _t0_299, _t0_300, _t0_301, _t0_302, _t0_303,
	_t0_304, _t0_305, _t0_306, _t0_307, _t0_308, _t0_309, _t0_310, _t0_311,
	_t0_312, _t0_313, _t0_314, _t0_315, _t0_316, _t0_317, _t0_318, _t0_319,
	_t0_320, _t0_321, _t0_322, _t0_323, _t0_324, _t0_325, _t0_326, _t0_327,
	_t0_328, _t0_329, _t0_330, _t0_331, _t0_332, _t0_333, _t0_334, _t0_335,
	_t0_336, _t0_337, _t0_338, _t0_339, _t0_340, _t0_341, _t0_342, _t0_343,
	_t0_344, _t0_345, _t0_346, _t0_347, _t0_348, _t0_349, _t0_350, _t0_351,
	_t0_352, _t0_353, _t0_354, _t0_355, _t0_356, _t0_357, _t0_358, _t0_359,
	_t0_360, _t0_361, _t0_362, _t0_363, _t0_364, _t0_365, _t0_366, _t0_367,
	_t0_368, _t0_369, _t0_370, _t0_371, _t0_372, _t0_373, _t0_374, _t0_375,
	_t0_376, _t0_377, _t0_378, _t0_379, _t0_380, _t0_381, _t0_382, _t0_383,
	_t0_384, _t0_385, _t0_386, _t0_387, _t0_388, _t0_389, _t0_390, _t0_391,
	_t0_392, _t0_393, _t0_394, _t0_395, _t0_396, _t0_397, _t0_398, _t0_399,
	_t0_400, _t0_401, _t0_402, _t0_403, _t0_404, _t0_405, _t0_406, _t0_407,
	_t0_408, _t0_409, _t0_410, _t0_411, _t0_412, _t0_413, _t0_414, _t0_415,
	_t0_416, _t0_417, _t0_418, _t0_419, _t0_420, _t0_421, _t0_422, _t0_423,
	_t0_424, _t0_425, _t0_426, _t0_427, _t0_428, _t0_429, _t0_430, _t0_431,
	_t0_432, _t0_433, _t0_434, _t0_435, _t0_436, _t0_437, _t0_438, _t0_439,
	_t0_440, _t0_441, _t0_442, _t0_443, _t0_444, _t0_445, _t0_446, _t0_447,
	_t0_448, _t0_449, _t0_450, _t0_451, _t0_452, _t0_453, _t0_454, _t0_455,
	_t0_456, _t0_457, _t0_458, _t0_459, _t0_460, _t0_461, _t0_462, _t0_463,
	_t0_464, _t0_465, _t0_466, _t0_467, _t0_468, _t0_469, _t0_470, _t0_471,
	_t0_472, _t0_473, _t0_474, _t0_475, _t0_476, _t0_477, _t0_478, _t0_479,
	_t0_480, _t0_481, _t0_482, _t0_483, _t0_484, _t0_485, _t0_486, _t0_487,
	_t0_488, _t0_489, _t0_490, _t0_491, _t0_492, _t0_493, _t0_494, _t0_495,
	_t0_496, _t0_497, _t0_498, _t0_499, _t0_500, _t0_501, _t0_502, _t0_503,
	_t0_504, _t0_505, _t0_506, _t0_507, _t0_508, _t0_509, _t0_510, _t0_511,
	_t0_512, _t0_513, _t0_514, _t0_515, _t0_516, _t0_517, _t0_518, _t0_519,
	_t0_520, _t0_521, _t0_522, _t0_523, _t0_524, _t0_525, _t0_526, _t0_527,
	_t0_528, _t0_529, _t0_530, _t0_531, _t0_532, _t0_533, _t0_534, _t0_535,
	_t0_536, _t0_537, _t0_538, _t0_539, _t0_540, _t0_541, _t0_542, _t0_543,
	_t0_544, _t0_545, _t0_546, _t0_547, _t0_548, _t0_549, _t0_550, _t0_551,
	_t0_552, _t0_553, _t0_554, _t0_555, _t0_556, _t0_557, _t0_558, _t0_559,
	_t0_560, _t0_561, _t0_562, _t0_563, _t0_564, _t0_565, _t0_566, _t0_567,
	_t0_568, _t0_569, _t0_570, _t0_571, _t0_572, _t0_573, _t0_574, _t0_575,
	_t0_576, _t0_577, _t0_578, _t0_579, _t0_580, _t0_581, _t0_582, _t0_583,
	_t0_584, _t0_585, _t0_586, _t0_587, _t0_588, _t0_589, _t0_590, _t0_591,
	_t0_592, _t0_593, _t0_594, _t0_595, _t0_596, _t0_597, _t0_598, _t0_599,
	_t0_600, _t0_601, _t0_602, _t0_603, _t0_604, _t0_605, _t0_606, _t0_607,
	_t0_608, _t0_609, _t0_610, _t0_611, _t0_612, _t0_613, _t0_614, _t0_615,
	_t0_616, _t0_617, _t0_618, _t0_619, _t0_620, _t0_621, _t0_622, _t0_623,
	_t0_624, _t0_625, _t0_626, _t0_627, _t0_628, _t0_629, _t0_630, _t0_631,
	_t0_632, _t0_633, _t0_634, _t0_635, _t0_636, _t0_637, _t0_638, _t0_639,
	_t0_640, _t0_641, _t0_642, _t0_643, _t0_644, _t0_645, _t0_646, _t0_647,
	_t0_648, _t0_649, _t0_650, _t0_651, _t0_652, _t0_653, _t0_654, _t0_655,
	_t0_656, _t0_657, _t0_658, _t0_659, _t0_660, _t0_661, _t0_662, _t0_663,
	_t0_664, _t0_665, _t0_666, _t0_667, _t0_668, _t0_669, _t0_670, _t0_671,
	_t0_672, _t0_673, _t0_674, _t0_675, _t0_676, _t0_677, _t0_678, _t0_679,
	_t0_680, _t0_681, _t0_682, _t0_683, _t0_684, _t0_685, _t0_686, _t0_687,
	_t0_688, _t0_689, _t0_690, _t0_691, _t0_692, _t0_693, _t0_694, _t0_695,
	_t0_696, _t0_697, _t0_698, _t0_699, _t0_700, _t0_701, _t0_702, _t0_703,
	_t0_704, _t0_705, _t0_706, _t0_707, _t0_708, _t0_709, _t0_710, _t0_711,
	_t0_712;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27, _t1_28, _t1_29, _t1_30, _t1_31,
	_t1_32, _t1_33, _t1_34, _t1_35, _t1_36, _t1_37, _t1_38, _t1_39,
	_t1_40, _t1_41, _t1_42, _t1_43, _t1_44, _t1_45, _t1_46, _t1_47,
	_t1_48, _t1_49, _t1_50, _t1_51, _t1_52, _t1_53, _t1_54, _t1_55,
	_t1_56, _t1_57, _t1_58, _t1_59, _t1_60, _t1_61, _t1_62, _t1_63,
	_t1_64, _t1_65, _t1_66, _t1_67, _t1_68, _t1_69, _t1_70, _t1_71,
	_t1_72, _t1_73, _t1_74, _t1_75, _t1_76, _t1_77, _t1_78, _t1_79,
	_t1_80, _t1_81, _t1_82, _t1_83, _t1_84, _t1_85, _t1_86, _t1_87,
	_t1_88, _t1_89, _t1_90, _t1_91, _t1_92, _t1_93, _t1_94, _t1_95,
	_t1_96, _t1_97, _t1_98, _t1_99, _t1_100, _t1_101, _t1_102, _t1_103,
	_t1_104, _t1_105, _t1_106, _t1_107, _t1_108, _t1_109, _t1_110, _t1_111,
	_t1_112, _t1_113, _t1_114, _t1_115, _t1_116, _t1_117, _t1_118, _t1_119,
	_t1_120, _t1_121, _t1_122, _t1_123, _t1_124, _t1_125, _t1_126;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31,
	_t3_32, _t3_33, _t3_34, _t3_35, _t3_36, _t3_37, _t3_38, _t3_39,
	_t3_40, _t3_41, _t3_42, _t3_43, _t3_44, _t3_45, _t3_46, _t3_47,
	_t3_48, _t3_49, _t3_50, _t3_51, _t3_52, _t3_53, _t3_54, _t3_55,
	_t3_56, _t3_57, _t3_58, _t3_59, _t3_60, _t3_61, _t3_62, _t3_63,
	_t3_64, _t3_65, _t3_66, _t3_67, _t3_68, _t3_69, _t3_70, _t3_71,
	_t3_72, _t3_73, _t3_74, _t3_75, _t3_76, _t3_77, _t3_78, _t3_79,
	_t3_80, _t3_81, _t3_82, _t3_83, _t3_84, _t3_85, _t3_86, _t3_87,
	_t3_88, _t3_89, _t3_90, _t3_91, _t3_92, _t3_93, _t3_94, _t3_95,
	_t3_96, _t3_97, _t3_98, _t3_99, _t3_100, _t3_101, _t3_102, _t3_103,
	_t3_104, _t3_105, _t3_106, _t3_107, _t3_108, _t3_109, _t3_110, _t3_111,
	_t3_112, _t3_113, _t3_114, _t3_115, _t3_116, _t3_117, _t3_118, _t3_119,
	_t3_120, _t3_121, _t3_122, _t3_123, _t3_124, _t3_125, _t3_126, _t3_127,
	_t3_128, _t3_129, _t3_130, _t3_131, _t3_132, _t3_133, _t3_134, _t3_135,
	_t3_136, _t3_137, _t3_138, _t3_139, _t3_140, _t3_141, _t3_142, _t3_143,
	_t3_144, _t3_145, _t3_146, _t3_147, _t3_148, _t3_149, _t3_150, _t3_151,
	_t3_152, _t3_153, _t3_154;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27, _t8_28, _t8_29, _t8_30, _t8_31,
	_t8_32, _t8_33, _t8_34, _t8_35, _t8_36, _t8_37, _t8_38, _t8_39;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27, _t9_28, _t9_29, _t9_30, _t9_31,
	_t9_32, _t9_33, _t9_34, _t9_35, _t9_36, _t9_37, _t9_38, _t9_39,
	_t9_40, _t9_41, _t9_42;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35, _t16_36, _t16_37, _t16_38, _t16_39;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31,
	_t18_32, _t18_33, _t18_34, _t18_35, _t18_36, _t18_37, _t18_38, _t18_39,
	_t18_40;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3;
  __m256d _t21_0, _t21_1, _t21_2;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3;

  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));
  _t0_2 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 72)), 0);
  _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37])));
  _t0_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[73])));
  _t0_5 = _mm256_castpd128_pd256(_mm_load_sd(&(K[74])));
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[108])));
  _t0_7 = _mm256_maskload_pd(K + 109, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_10 = _mm256_castpd128_pd256(_mm_load_sd(&(K[111])));
  _t0_11 = _mm256_castpd128_pd256(_mm_load_sd(&(K[144])));
  _t0_12 = _mm256_maskload_pd(K + 145, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_17 = _mm256_castpd128_pd256(_mm_load_sd(&(K[180])));
  _t0_18 = _mm256_maskload_pd(K + 181, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_23 = _mm256_castpd128_pd256(_mm_load_sd(&(K[216])));
  _t0_24 = _mm256_maskload_pd(K + 217, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_29 = _mm256_castpd128_pd256(_mm_load_sd(&(K[252])));
  _t0_30 = _mm256_maskload_pd(K + 253, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_35 = _mm256_castpd128_pd256(_mm_load_sd(K + 148));
  _t0_36 = _mm256_maskload_pd(K + 184, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_37 = _mm256_maskload_pd(K + 220, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_38 = _mm256_loadu_pd(K + 256);
  _t0_49 = _mm256_castpd128_pd256(_mm_load_sd(&(K[288])));
  _t0_50 = _mm256_maskload_pd(K + 289, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_55 = _mm256_castpd128_pd256(_mm_load_sd(&(K[324])));
  _t0_56 = _mm256_maskload_pd(K + 325, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_61 = _mm256_castpd128_pd256(_mm_load_sd(&(K[360])));
  _t0_62 = _mm256_maskload_pd(K + 361, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_67 = _mm256_castpd128_pd256(_mm_load_sd(&(K[396])));
  _t0_68 = _mm256_maskload_pd(K + 397, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_127 = _mm256_loadu_pd(K + 292);
  _t0_128 = _mm256_loadu_pd(K + 328);
  _t0_129 = _mm256_loadu_pd(K + 364);
  _t0_130 = _mm256_loadu_pd(K + 400);
  _t0_97 = _mm256_castpd128_pd256(_mm_load_sd(K + 296));
  _t0_98 = _mm256_maskload_pd(K + 332, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_99 = _mm256_maskload_pd(K + 368, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_100 = _mm256_loadu_pd(K + 404);

  // Generating : L[36,36] = S(h(1, 36, 0), Sqrt( G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_326 = _t0_0;

  // 4-BLAC: sqrt(1x4)
  _t0_336 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_326)));

  // AVX Storer:
  _t0_0 = _t0_336;

  // Generating : T1473[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_350 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_356 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_370 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_350), _mm256_castpd256_pd128(_t0_356)));

  // AVX Storer:
  _t0_1 = _t0_370;

  // Generating : L[36,36] = S(h(2, 36, 1), ( G(h(1, 1, 0), T1473[1,36],h(1, 36, 0)) Kro G(h(2, 36, 1), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_384 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_390 = _t0_2;

  // 4-BLAC: 1x4 Kro 4x1
  _t0_404 = _mm256_mul_pd(_t0_384, _t0_390);

  // AVX Storer:
  _t0_2 = _t0_404;

  // Generating : L[36,36] = S(h(1, 36, 1), ( G(h(1, 36, 1), L[36,36],h(1, 36, 1)) - ( G(h(1, 36, 1), L[36,36],h(1, 36, 0)) Kro T( G(h(1, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_421 = _t0_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_427 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_433 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_442 = _t0_433;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_456 = _mm256_mul_pd(_t0_427, _t0_442);

  // 4-BLAC: 1x4 - 1x4
  _t0_471 = _mm256_sub_pd(_t0_421, _t0_456);

  // AVX Storer:
  _t0_3 = _t0_471;

  // Generating : L[36,36] = S(h(1, 36, 1), Sqrt( G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_485 = _t0_3;

  // 4-BLAC: sqrt(1x4)
  _t0_495 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_485)));

  // AVX Storer:
  _t0_3 = _t0_495;

  // Generating : L[36,36] = S(h(1, 36, 2), ( G(h(1, 36, 2), L[36,36],h(1, 36, 1)) - ( G(h(1, 36, 2), L[36,36],h(1, 36, 0)) Kro T( G(h(1, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_509 = _t0_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_515 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_520 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_530 = _t0_520;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_544 = _mm256_mul_pd(_t0_515, _t0_530);

  // 4-BLAC: 1x4 - 1x4
  _t0_560 = _mm256_sub_pd(_t0_509, _t0_544);

  // AVX Storer:
  _t0_4 = _t0_560;

  // Generating : L[36,36] = S(h(1, 36, 2), ( G(h(1, 36, 2), L[36,36],h(1, 36, 1)) Div G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_573 = _t0_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_580 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_594 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_573), _mm256_castpd256_pd128(_t0_580)));

  // AVX Storer:
  _t0_4 = _t0_594;

  // Generating : L[36,36] = S(h(1, 36, 2), ( G(h(1, 36, 2), L[36,36],h(1, 36, 2)) - ( G(h(1, 36, 2), L[36,36],h(2, 36, 0)) * T( G(h(1, 36, 2), L[36,36],h(2, 36, 0)) ) ) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_608 = _t0_5;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_614 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_619 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_629 = _t0_619;

  // 4-BLAC: 1x4 * 4x1
  _t0_643 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_614, _t0_629), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_614, _t0_629), _mm256_mul_pd(_t0_614, _t0_629), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_614, _t0_629), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_614, _t0_629), _mm256_mul_pd(_t0_614, _t0_629), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_614, _t0_629), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_614, _t0_629), _mm256_mul_pd(_t0_614, _t0_629), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_658 = _mm256_sub_pd(_t0_608, _t0_643);

  // AVX Storer:
  _t0_5 = _t0_658;

  // Generating : L[36,36] = S(h(1, 36, 2), Sqrt( G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_672 = _t0_5;

  // 4-BLAC: sqrt(1x4)
  _t0_682 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_672)));

  // AVX Storer:
  _t0_5 = _t0_682;

  // Generating : L[36,36] = S(h(1, 36, 3), ( G(h(1, 36, 3), L[36,36],h(1, 36, 0)) Div G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_139 = _t0_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_145 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_159 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_139), _mm256_castpd256_pd128(_t0_145)));

  // AVX Storer:
  _t0_6 = _t0_159;

  // Generating : L[36,36] = S(h(1, 36, 3), ( G(h(1, 36, 3), L[36,36],h(2, 36, 1)) - ( G(h(1, 36, 3), L[36,36],h(1, 36, 0)) Kro T( G(h(2, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(2, 36, 1))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_173 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_179 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_6, _t0_6, 32), _mm256_permute2f128_pd(_t0_6, _t0_6, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_184 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_194 = _t0_184;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_215 = _mm256_mul_pd(_t0_179, _t0_194);

  // 4-BLAC: 1x4 - 1x4
  _t0_228 = _mm256_sub_pd(_t0_173, _t0_215);

  // AVX Storer:
  _t0_7 = _t0_228;

  // Generating : L[36,36] = S(h(1, 36, 3), ( G(h(1, 36, 3), L[36,36],h(1, 36, 1)) Div G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_243 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_248 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_263 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_243), _mm256_castpd256_pd128(_t0_248)));

  // AVX Storer:
  _t0_8 = _t0_263;

  // Generating : L[36,36] = S(h(1, 36, 3), ( G(h(1, 36, 3), L[36,36],h(1, 36, 2)) - ( G(h(1, 36, 3), L[36,36],h(1, 36, 1)) Kro T( G(h(1, 36, 2), L[36,36],h(1, 36, 1)) ) ) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_270 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_7, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_271 = _t0_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_272 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_273 = _t0_272;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_274 = _mm256_mul_pd(_t0_271, _t0_273);

  // 4-BLAC: 1x4 - 1x4
  _t0_275 = _mm256_sub_pd(_t0_270, _t0_274);

  // AVX Storer:
  _t0_9 = _t0_275;

  // Generating : L[36,36] = S(h(1, 36, 3), ( G(h(1, 36, 3), L[36,36],h(1, 36, 2)) Div G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_276 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_277 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_278 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_276), _mm256_castpd256_pd128(_t0_277)));

  // AVX Storer:
  _t0_9 = _t0_278;

  // Generating : L[36,36] = S(h(1, 36, 3), ( G(h(1, 36, 3), L[36,36],h(1, 36, 3)) - ( G(h(1, 36, 3), L[36,36],h(3, 36, 0)) * T( G(h(1, 36, 3), L[36,36],h(3, 36, 0)) ) ) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_279 = _t0_10;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_280 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_6, _t0_8), _mm256_unpacklo_pd(_t0_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_281 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_6, _t0_8), _mm256_unpacklo_pd(_t0_9, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_282 = _t0_281;

  // 4-BLAC: 1x4 * 4x1
  _t0_283 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_280, _t0_282), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_280, _t0_282), _mm256_mul_pd(_t0_280, _t0_282), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_280, _t0_282), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_280, _t0_282), _mm256_mul_pd(_t0_280, _t0_282), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_280, _t0_282), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_280, _t0_282), _mm256_mul_pd(_t0_280, _t0_282), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_284 = _mm256_sub_pd(_t0_279, _t0_283);

  // AVX Storer:
  _t0_10 = _t0_284;

  // Generating : L[36,36] = S(h(1, 36, 3), Sqrt( G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_285 = _t0_10;

  // 4-BLAC: sqrt(1x4)
  _t0_286 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_285)));

  // AVX Storer:
  _t0_10 = _t0_286;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(1, 36, 0)) Div G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_287 = _t0_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_288 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_289 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_287), _mm256_castpd256_pd128(_t0_288)));

  // AVX Storer:
  _t0_11 = _t0_289;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(3, 36, 1)) - ( G(h(1, 36, 4), L[36,36],h(1, 36, 0)) Kro T( G(h(3, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(3, 36, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_290 = _t0_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_291 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_11, _t0_11, 32), _mm256_permute2f128_pd(_t0_11, _t0_11, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_292 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_293 = _t0_292;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_294 = _mm256_mul_pd(_t0_291, _t0_293);

  // 4-BLAC: 1x4 - 1x4
  _t0_295 = _mm256_sub_pd(_t0_290, _t0_294);

  // AVX Storer:
  _t0_12 = _t0_295;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(1, 36, 1)) Div G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_296 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_12, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_297 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_298 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_296), _mm256_castpd256_pd128(_t0_297)));

  // AVX Storer:
  _t0_13 = _t0_298;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(2, 36, 2)) - ( G(h(1, 36, 4), L[36,36],h(1, 36, 1)) Kro T( G(h(2, 36, 2), L[36,36],h(1, 36, 1)) ) ) ),h(2, 36, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_299 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_12, 6), _mm256_permute2f128_pd(_t0_12, _t0_12, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_300 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_301 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_302 = _t0_301;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_303 = _mm256_mul_pd(_t0_300, _t0_302);

  // 4-BLAC: 1x4 - 1x4
  _t0_304 = _mm256_sub_pd(_t0_299, _t0_303);

  // AVX Storer:
  _t0_14 = _t0_304;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(1, 36, 2)) Div G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_305 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_306 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_307 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_305), _mm256_castpd256_pd128(_t0_306)));

  // AVX Storer:
  _t0_15 = _t0_307;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(1, 36, 3)) - ( G(h(1, 36, 4), L[36,36],h(1, 36, 2)) Kro T( G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) ) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_308 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_14, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_309 = _t0_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_310 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_311 = _t0_310;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_312 = _mm256_mul_pd(_t0_309, _t0_311);

  // 4-BLAC: 1x4 - 1x4
  _t0_313 = _mm256_sub_pd(_t0_308, _t0_312);

  // AVX Storer:
  _t0_16 = _t0_313;

  // Generating : L[36,36] = S(h(1, 36, 4), ( G(h(1, 36, 4), L[36,36],h(1, 36, 3)) Div G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_314 = _t0_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_315 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_316 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_314), _mm256_castpd256_pd128(_t0_315)));

  // AVX Storer:
  _t0_16 = _t0_316;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(1, 36, 0)) Div G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_317 = _t0_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_318 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_319 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_317), _mm256_castpd256_pd128(_t0_318)));

  // AVX Storer:
  _t0_17 = _t0_319;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(3, 36, 1)) - ( G(h(1, 36, 5), L[36,36],h(1, 36, 0)) Kro T( G(h(3, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(3, 36, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_320 = _t0_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_321 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_322 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_323 = _t0_322;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_324 = _mm256_mul_pd(_t0_321, _t0_323);

  // 4-BLAC: 1x4 - 1x4
  _t0_325 = _mm256_sub_pd(_t0_320, _t0_324);

  // AVX Storer:
  _t0_18 = _t0_325;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(1, 36, 1)) Div G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_327 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_328 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_329 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_327), _mm256_castpd256_pd128(_t0_328)));

  // AVX Storer:
  _t0_19 = _t0_329;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(2, 36, 2)) - ( G(h(1, 36, 5), L[36,36],h(1, 36, 1)) Kro T( G(h(2, 36, 2), L[36,36],h(1, 36, 1)) ) ) ),h(2, 36, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_330 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 6), _mm256_permute2f128_pd(_t0_18, _t0_18, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_331 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_19, _t0_19, 32), _mm256_permute2f128_pd(_t0_19, _t0_19, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_332 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_333 = _t0_332;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_334 = _mm256_mul_pd(_t0_331, _t0_333);

  // 4-BLAC: 1x4 - 1x4
  _t0_335 = _mm256_sub_pd(_t0_330, _t0_334);

  // AVX Storer:
  _t0_20 = _t0_335;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(1, 36, 2)) Div G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_337 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_20, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_338 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_339 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_337), _mm256_castpd256_pd128(_t0_338)));

  // AVX Storer:
  _t0_21 = _t0_339;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(1, 36, 3)) - ( G(h(1, 36, 5), L[36,36],h(1, 36, 2)) Kro T( G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) ) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_340 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_20, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_341 = _t0_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_342 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_343 = _t0_342;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_344 = _mm256_mul_pd(_t0_341, _t0_343);

  // 4-BLAC: 1x4 - 1x4
  _t0_345 = _mm256_sub_pd(_t0_340, _t0_344);

  // AVX Storer:
  _t0_22 = _t0_345;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(1, 36, 3)) Div G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_346 = _t0_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_347 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_348 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_346), _mm256_castpd256_pd128(_t0_347)));

  // AVX Storer:
  _t0_22 = _t0_348;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 0)) Div G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_349 = _t0_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_351 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_352 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_349), _mm256_castpd256_pd128(_t0_351)));

  // AVX Storer:
  _t0_23 = _t0_352;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(3, 36, 1)) - ( G(h(1, 36, 6), L[36,36],h(1, 36, 0)) Kro T( G(h(3, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(3, 36, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_353 = _t0_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_354 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_23, _t0_23, 32), _mm256_permute2f128_pd(_t0_23, _t0_23, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_355 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_357 = _t0_355;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_358 = _mm256_mul_pd(_t0_354, _t0_357);

  // 4-BLAC: 1x4 - 1x4
  _t0_359 = _mm256_sub_pd(_t0_353, _t0_358);

  // AVX Storer:
  _t0_24 = _t0_359;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 1)) Div G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_360 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_24, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_361 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_362 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_360), _mm256_castpd256_pd128(_t0_361)));

  // AVX Storer:
  _t0_25 = _t0_362;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(2, 36, 2)) - ( G(h(1, 36, 6), L[36,36],h(1, 36, 1)) Kro T( G(h(2, 36, 2), L[36,36],h(1, 36, 1)) ) ) ),h(2, 36, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_363 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_24, 6), _mm256_permute2f128_pd(_t0_24, _t0_24, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_364 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_25, _t0_25, 32), _mm256_permute2f128_pd(_t0_25, _t0_25, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_365 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_366 = _t0_365;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_367 = _mm256_mul_pd(_t0_364, _t0_366);

  // 4-BLAC: 1x4 - 1x4
  _t0_368 = _mm256_sub_pd(_t0_363, _t0_367);

  // AVX Storer:
  _t0_26 = _t0_368;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 2)) Div G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_369 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_26, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_371 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_372 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_369), _mm256_castpd256_pd128(_t0_371)));

  // AVX Storer:
  _t0_27 = _t0_372;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 3)) - ( G(h(1, 36, 6), L[36,36],h(1, 36, 2)) Kro T( G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) ) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_373 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_26, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_374 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_375 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_376 = _t0_375;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_377 = _mm256_mul_pd(_t0_374, _t0_376);

  // 4-BLAC: 1x4 - 1x4
  _t0_378 = _mm256_sub_pd(_t0_373, _t0_377);

  // AVX Storer:
  _t0_28 = _t0_378;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 3)) Div G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_379 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_380 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_381 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_379), _mm256_castpd256_pd128(_t0_380)));

  // AVX Storer:
  _t0_28 = _t0_381;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 0)) Div G(h(1, 36, 0), L[36,36],h(1, 36, 0)) ),h(1, 36, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_382 = _t0_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_383 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_385 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_382), _mm256_castpd256_pd128(_t0_383)));

  // AVX Storer:
  _t0_29 = _t0_385;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(3, 36, 1)) - ( G(h(1, 36, 7), L[36,36],h(1, 36, 0)) Kro T( G(h(3, 36, 1), L[36,36],h(1, 36, 0)) ) ) ),h(3, 36, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_386 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_387 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_29, _t0_29, 32), _mm256_permute2f128_pd(_t0_29, _t0_29, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_388 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_389 = _t0_388;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_391 = _mm256_mul_pd(_t0_387, _t0_389);

  // 4-BLAC: 1x4 - 1x4
  _t0_392 = _mm256_sub_pd(_t0_386, _t0_391);

  // AVX Storer:
  _t0_30 = _t0_392;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 1)) Div G(h(1, 36, 1), L[36,36],h(1, 36, 1)) ),h(1, 36, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_393 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_30, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_394 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_395 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_393), _mm256_castpd256_pd128(_t0_394)));

  // AVX Storer:
  _t0_31 = _t0_395;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(2, 36, 2)) - ( G(h(1, 36, 7), L[36,36],h(1, 36, 1)) Kro T( G(h(2, 36, 2), L[36,36],h(1, 36, 1)) ) ) ),h(2, 36, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_396 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_30, 6), _mm256_permute2f128_pd(_t0_30, _t0_30, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_397 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_31, _t0_31, 32), _mm256_permute2f128_pd(_t0_31, _t0_31, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_398 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_399 = _t0_398;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_400 = _mm256_mul_pd(_t0_397, _t0_399);

  // 4-BLAC: 1x4 - 1x4
  _t0_401 = _mm256_sub_pd(_t0_396, _t0_400);

  // AVX Storer:
  _t0_32 = _t0_401;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 2)) Div G(h(1, 36, 2), L[36,36],h(1, 36, 2)) ),h(1, 36, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_402 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_32, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_403 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_405 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_402), _mm256_castpd256_pd128(_t0_403)));

  // AVX Storer:
  _t0_33 = _t0_405;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 3)) - ( G(h(1, 36, 7), L[36,36],h(1, 36, 2)) Kro T( G(h(1, 36, 3), L[36,36],h(1, 36, 2)) ) ) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_406 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_32, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_407 = _t0_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_408 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_409 = _t0_408;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_410 = _mm256_mul_pd(_t0_407, _t0_409);

  // 4-BLAC: 1x4 - 1x4
  _t0_411 = _mm256_sub_pd(_t0_406, _t0_410);

  // AVX Storer:
  _t0_34 = _t0_411;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 3)) Div G(h(1, 36, 3), L[36,36],h(1, 36, 3)) ),h(1, 36, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_412 = _t0_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_413 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_414 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_412), _mm256_castpd256_pd128(_t0_413)));

  // AVX Storer:
  _t0_34 = _t0_414;

  // Generating : L[36,36] = S(h(4, 36, 4), ( G(h(4, 36, 4), K[36,36],h(4, 36, 4)) - ( G(h(4, 36, 4), L[36,36],h(4, 36, 0)) * T( G(h(4, 36, 4), L[36,36],h(4, 36, 0)) ) ) ),h(4, 36, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_415 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_35, _t0_36, 0), _mm256_shuffle_pd(_t0_37, _t0_38, 0), 32);
  _t0_416 = _mm256_permute2f128_pd(_t0_36, _mm256_shuffle_pd(_t0_37, _t0_38, 3), 32);
  _t0_417 = _mm256_blend_pd(_t0_37, _mm256_shuffle_pd(_t0_37, _t0_38, 3), 12);
  _t0_418 = _t0_38;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_709 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 32);
  _t0_710 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 32);
  _t0_711 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 49);
  _t0_712 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_123 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_11, _t0_11, 32), _mm256_permute2f128_pd(_t0_11, _t0_11, 32), 0), _t0_709), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0), _t0_710)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0), _t0_711), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_16, _t0_16, 32), _mm256_permute2f128_pd(_t0_16, _t0_16, 32), 0), _t0_712)));
  _t0_124 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0), _t0_709), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_19, _t0_19, 32), _mm256_permute2f128_pd(_t0_19, _t0_19, 32), 0), _t0_710)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_21, _t0_21, 32), _mm256_permute2f128_pd(_t0_21, _t0_21, 32), 0), _t0_711), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_22, _t0_22, 32), _mm256_permute2f128_pd(_t0_22, _t0_22, 32), 0), _t0_712)));
  _t0_125 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_23, _t0_23, 32), _mm256_permute2f128_pd(_t0_23, _t0_23, 32), 0), _t0_709), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_25, _t0_25, 32), _mm256_permute2f128_pd(_t0_25, _t0_25, 32), 0), _t0_710)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_27, _t0_27, 32), _mm256_permute2f128_pd(_t0_27, _t0_27, 32), 0), _t0_711), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_28, _t0_28, 32), _mm256_permute2f128_pd(_t0_28, _t0_28, 32), 0), _t0_712)));
  _t0_126 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_29, _t0_29, 32), _mm256_permute2f128_pd(_t0_29, _t0_29, 32), 0), _t0_709), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_31, _t0_31, 32), _mm256_permute2f128_pd(_t0_31, _t0_31, 32), 0), _t0_710)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_33, _t0_33, 32), _mm256_permute2f128_pd(_t0_33, _t0_33, 32), 0), _t0_711), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_34, _t0_34, 32), _mm256_permute2f128_pd(_t0_34, _t0_34, 32), 0), _t0_712)));

  // 4-BLAC: 4x4 - 4x4
  _t0_135 = _mm256_sub_pd(_t0_415, _t0_123);
  _t0_136 = _mm256_sub_pd(_t0_416, _t0_124);
  _t0_137 = _mm256_sub_pd(_t0_417, _t0_125);
  _t0_138 = _mm256_sub_pd(_t0_418, _t0_126);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t0_35 = _t0_135;
  _t0_36 = _t0_136;
  _t0_37 = _t0_137;
  _t0_38 = _t0_138;

  // Generating : L[36,36] = S(h(1, 36, 4), Sqrt( G(h(1, 36, 4), L[36,36],h(1, 36, 4)) ),h(1, 36, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_419 = _t0_35;

  // 4-BLAC: sqrt(1x4)
  _t0_420 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_419)));

  // AVX Storer:
  _t0_35 = _t0_420;

  // Generating : T1473[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, 4), L[36,36],h(1, 36, 4)) ),h(1, 36, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_422 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_423 = _t0_35;

  // 4-BLAC: 1x4 / 1x4
  _t0_424 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_422), _mm256_castpd256_pd128(_t0_423)));

  // AVX Storer:
  _t0_39 = _t0_424;

  // Generating : L[36,36] = S(h(2, 36, 5), ( G(h(1, 1, 0), T1473[1,36],h(1, 36, 4)) Kro G(h(2, 36, 5), L[36,36],h(1, 36, 4)) ),h(1, 36, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_425 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_39, _t0_39, 32), _mm256_permute2f128_pd(_t0_39, _t0_39, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_426 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_36, _t0_37), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 4x1
  _t0_428 = _mm256_mul_pd(_t0_425, _t0_426);

  // AVX Storer:
  _t0_40 = _t0_428;

  // Generating : L[36,36] = S(h(1, 36, 5), ( G(h(1, 36, 5), L[36,36],h(1, 36, 5)) - ( G(h(1, 36, 5), L[36,36],h(1, 36, 4)) Kro T( G(h(1, 36, 5), L[36,36],h(1, 36, 4)) ) ) ),h(1, 36, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_429 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_36, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_430 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_40, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_431 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_40, 1);

  // 4-BLAC: (4x1)^T
  _t0_432 = _t0_431;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_434 = _mm256_mul_pd(_t0_430, _t0_432);

  // 4-BLAC: 1x4 - 1x4
  _t0_435 = _mm256_sub_pd(_t0_429, _t0_434);

  // AVX Storer:
  _t0_41 = _t0_435;

  // Generating : L[36,36] = S(h(1, 36, 5), Sqrt( G(h(1, 36, 5), L[36,36],h(1, 36, 5)) ),h(1, 36, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_436 = _t0_41;

  // 4-BLAC: sqrt(1x4)
  _t0_437 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_436)));

  // AVX Storer:
  _t0_41 = _t0_437;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 5)) - ( G(h(1, 36, 6), L[36,36],h(1, 36, 4)) Kro T( G(h(1, 36, 5), L[36,36],h(1, 36, 4)) ) ) ),h(1, 36, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_438 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_37, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_439 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_40, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_440 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_40, 1);

  // 4-BLAC: (4x1)^T
  _t0_441 = _t0_440;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_443 = _mm256_mul_pd(_t0_439, _t0_441);

  // 4-BLAC: 1x4 - 1x4
  _t0_444 = _mm256_sub_pd(_t0_438, _t0_443);

  // AVX Storer:
  _t0_42 = _t0_444;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 5)) Div G(h(1, 36, 5), L[36,36],h(1, 36, 5)) ),h(1, 36, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_445 = _t0_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_446 = _t0_41;

  // 4-BLAC: 1x4 / 1x4
  _t0_447 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_445), _mm256_castpd256_pd128(_t0_446)));

  // AVX Storer:
  _t0_42 = _t0_447;

  // Generating : L[36,36] = S(h(1, 36, 6), ( G(h(1, 36, 6), L[36,36],h(1, 36, 6)) - ( G(h(1, 36, 6), L[36,36],h(2, 36, 4)) * T( G(h(1, 36, 6), L[36,36],h(2, 36, 4)) ) ) ),h(1, 36, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_448 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_37, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_37, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_449 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_40, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_42, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_450 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_40, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_42, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_451 = _t0_450;

  // 4-BLAC: 1x4 * 4x1
  _t0_452 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_449, _t0_451), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_449, _t0_451), _mm256_mul_pd(_t0_449, _t0_451), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_449, _t0_451), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_449, _t0_451), _mm256_mul_pd(_t0_449, _t0_451), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_449, _t0_451), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_449, _t0_451), _mm256_mul_pd(_t0_449, _t0_451), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_453 = _mm256_sub_pd(_t0_448, _t0_452);

  // AVX Storer:
  _t0_43 = _t0_453;

  // Generating : L[36,36] = S(h(1, 36, 6), Sqrt( G(h(1, 36, 6), L[36,36],h(1, 36, 6)) ),h(1, 36, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_454 = _t0_43;

  // 4-BLAC: sqrt(1x4)
  _t0_455 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_454)));

  // AVX Storer:
  _t0_43 = _t0_455;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 4)) Div G(h(1, 36, 4), L[36,36],h(1, 36, 4)) ),h(1, 36, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_457 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_38, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_458 = _t0_35;

  // 4-BLAC: 1x4 / 1x4
  _t0_459 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_457), _mm256_castpd256_pd128(_t0_458)));

  // AVX Storer:
  _t0_44 = _t0_459;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(2, 36, 5)) - ( G(h(1, 36, 7), L[36,36],h(1, 36, 4)) Kro T( G(h(2, 36, 5), L[36,36],h(1, 36, 4)) ) ) ),h(2, 36, 5))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_460 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_38, 6), _mm256_permute2f128_pd(_t0_38, _t0_38, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_461 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_44, _t0_44, 32), _mm256_permute2f128_pd(_t0_44, _t0_44, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_462 = _t0_40;

  // 4-BLAC: (4x1)^T
  _t0_463 = _t0_462;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_464 = _mm256_mul_pd(_t0_461, _t0_463);

  // 4-BLAC: 1x4 - 1x4
  _t0_465 = _mm256_sub_pd(_t0_460, _t0_464);

  // AVX Storer:
  _t0_45 = _t0_465;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 5)) Div G(h(1, 36, 5), L[36,36],h(1, 36, 5)) ),h(1, 36, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_466 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_45, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_467 = _t0_41;

  // 4-BLAC: 1x4 / 1x4
  _t0_468 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_466), _mm256_castpd256_pd128(_t0_467)));

  // AVX Storer:
  _t0_46 = _t0_468;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 6)) - ( G(h(1, 36, 7), L[36,36],h(1, 36, 5)) Kro T( G(h(1, 36, 6), L[36,36],h(1, 36, 5)) ) ) ),h(1, 36, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_469 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_45, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_470 = _t0_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_472 = _t0_42;

  // 4-BLAC: (4x1)^T
  _t0_473 = _t0_472;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_474 = _mm256_mul_pd(_t0_470, _t0_473);

  // 4-BLAC: 1x4 - 1x4
  _t0_475 = _mm256_sub_pd(_t0_469, _t0_474);

  // AVX Storer:
  _t0_47 = _t0_475;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 6)) Div G(h(1, 36, 6), L[36,36],h(1, 36, 6)) ),h(1, 36, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_476 = _t0_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_477 = _t0_43;

  // 4-BLAC: 1x4 / 1x4
  _t0_478 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_476), _mm256_castpd256_pd128(_t0_477)));

  // AVX Storer:
  _t0_47 = _t0_478;

  // Generating : L[36,36] = S(h(1, 36, 7), ( G(h(1, 36, 7), L[36,36],h(1, 36, 7)) - ( G(h(1, 36, 7), L[36,36],h(3, 36, 4)) * T( G(h(1, 36, 7), L[36,36],h(3, 36, 4)) ) ) ),h(1, 36, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_479 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_38, _t0_38, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_480 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_44, _t0_46), _mm256_unpacklo_pd(_t0_47, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_481 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_44, _t0_46), _mm256_unpacklo_pd(_t0_47, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_482 = _t0_481;

  // 4-BLAC: 1x4 * 4x1
  _t0_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_480, _t0_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_480, _t0_482), _mm256_mul_pd(_t0_480, _t0_482), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_480, _t0_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_480, _t0_482), _mm256_mul_pd(_t0_480, _t0_482), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_480, _t0_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_480, _t0_482), _mm256_mul_pd(_t0_480, _t0_482), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_484 = _mm256_sub_pd(_t0_479, _t0_483);

  // AVX Storer:
  _t0_48 = _t0_484;

  // Generating : L[36,36] = S(h(1, 36, 7), Sqrt( G(h(1, 36, 7), L[36,36],h(1, 36, 7)) ),h(1, 36, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_486 = _t0_48;

  // 4-BLAC: sqrt(1x4)
  _t0_487 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_486)));

  // AVX Storer:
  _t0_48 = _t0_487;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_488 = _t0_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_489 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_490 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_488), _mm256_castpd256_pd128(_t0_489)));

  // AVX Storer:
  _t0_49 = _t0_490;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_491 = _t0_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_492 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_493 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_494 = _t0_493;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_496 = _mm256_mul_pd(_t0_492, _t0_494);

  // 4-BLAC: 1x4 - 1x4
  _t0_497 = _mm256_sub_pd(_t0_491, _t0_496);

  // AVX Storer:
  _t0_50 = _t0_497;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_498 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_50, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_499 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_500 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_498), _mm256_castpd256_pd128(_t0_499)));

  // AVX Storer:
  _t0_51 = _t0_500;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_501 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_50, 6), _mm256_permute2f128_pd(_t0_50, _t0_50, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_502 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_503 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_504 = _t0_503;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_505 = _mm256_mul_pd(_t0_502, _t0_504);

  // 4-BLAC: 1x4 - 1x4
  _t0_506 = _mm256_sub_pd(_t0_501, _t0_505);

  // AVX Storer:
  _t0_52 = _t0_506;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_507 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_52, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_508 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_510 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_507), _mm256_castpd256_pd128(_t0_508)));

  // AVX Storer:
  _t0_53 = _t0_510;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_511 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_52, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_512 = _t0_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_513 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_514 = _t0_513;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_516 = _mm256_mul_pd(_t0_512, _t0_514);

  // 4-BLAC: 1x4 - 1x4
  _t0_517 = _mm256_sub_pd(_t0_511, _t0_516);

  // AVX Storer:
  _t0_54 = _t0_517;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_518 = _t0_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_519 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_521 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_518), _mm256_castpd256_pd128(_t0_519)));

  // AVX Storer:
  _t0_54 = _t0_521;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_522 = _t0_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_523 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_524 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_522), _mm256_castpd256_pd128(_t0_523)));

  // AVX Storer:
  _t0_55 = _t0_524;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_525 = _t0_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_526 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_527 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_528 = _t0_527;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_529 = _mm256_mul_pd(_t0_526, _t0_528);

  // 4-BLAC: 1x4 - 1x4
  _t0_531 = _mm256_sub_pd(_t0_525, _t0_529);

  // AVX Storer:
  _t0_56 = _t0_531;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_532 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_56, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_533 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_534 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_532), _mm256_castpd256_pd128(_t0_533)));

  // AVX Storer:
  _t0_57 = _t0_534;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_535 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_56, 6), _mm256_permute2f128_pd(_t0_56, _t0_56, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_536 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_537 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_538 = _t0_537;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_539 = _mm256_mul_pd(_t0_536, _t0_538);

  // 4-BLAC: 1x4 - 1x4
  _t0_540 = _mm256_sub_pd(_t0_535, _t0_539);

  // AVX Storer:
  _t0_58 = _t0_540;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_541 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_58, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_542 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_543 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_541), _mm256_castpd256_pd128(_t0_542)));

  // AVX Storer:
  _t0_59 = _t0_543;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_545 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_58, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_546 = _t0_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_547 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_548 = _t0_547;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_549 = _mm256_mul_pd(_t0_546, _t0_548);

  // 4-BLAC: 1x4 - 1x4
  _t0_550 = _mm256_sub_pd(_t0_545, _t0_549);

  // AVX Storer:
  _t0_60 = _t0_550;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_551 = _t0_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_552 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_553 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_551), _mm256_castpd256_pd128(_t0_552)));

  // AVX Storer:
  _t0_60 = _t0_553;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_554 = _t0_61;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_555 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_556 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_554), _mm256_castpd256_pd128(_t0_555)));

  // AVX Storer:
  _t0_61 = _t0_556;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_557 = _t0_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_558 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_61, _t0_61, 32), _mm256_permute2f128_pd(_t0_61, _t0_61, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_559 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_561 = _t0_559;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_562 = _mm256_mul_pd(_t0_558, _t0_561);

  // 4-BLAC: 1x4 - 1x4
  _t0_563 = _mm256_sub_pd(_t0_557, _t0_562);

  // AVX Storer:
  _t0_62 = _t0_563;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_564 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_62, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_565 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_566 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_564), _mm256_castpd256_pd128(_t0_565)));

  // AVX Storer:
  _t0_63 = _t0_566;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_567 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_62, 6), _mm256_permute2f128_pd(_t0_62, _t0_62, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_568 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_569 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_570 = _t0_569;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_571 = _mm256_mul_pd(_t0_568, _t0_570);

  // 4-BLAC: 1x4 - 1x4
  _t0_572 = _mm256_sub_pd(_t0_567, _t0_571);

  // AVX Storer:
  _t0_64 = _t0_572;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_574 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_64, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_575 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_576 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_574), _mm256_castpd256_pd128(_t0_575)));

  // AVX Storer:
  _t0_65 = _t0_576;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_577 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_64, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_578 = _t0_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_579 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_581 = _t0_579;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_582 = _mm256_mul_pd(_t0_578, _t0_581);

  // 4-BLAC: 1x4 - 1x4
  _t0_583 = _mm256_sub_pd(_t0_577, _t0_582);

  // AVX Storer:
  _t0_66 = _t0_583;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_584 = _t0_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_585 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_586 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_584), _mm256_castpd256_pd128(_t0_585)));

  // AVX Storer:
  _t0_66 = _t0_586;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_587 = _t0_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_588 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_589 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_587), _mm256_castpd256_pd128(_t0_588)));

  // AVX Storer:
  _t0_67 = _t0_589;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_590 = _t0_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_591 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_67, _t0_67, 32), _mm256_permute2f128_pd(_t0_67, _t0_67, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_592 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_2, _t0_2, 2), _t0_6, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_593 = _t0_592;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_595 = _mm256_mul_pd(_t0_591, _t0_593);

  // 4-BLAC: 1x4 - 1x4
  _t0_596 = _mm256_sub_pd(_t0_590, _t0_595);

  // AVX Storer:
  _t0_68 = _t0_596;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_597 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_68, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_598 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_599 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_597), _mm256_castpd256_pd128(_t0_598)));

  // AVX Storer:
  _t0_69 = _t0_599;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_600 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_68, 6), _mm256_permute2f128_pd(_t0_68, _t0_68, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_601 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_69, _t0_69, 32), _mm256_permute2f128_pd(_t0_69, _t0_69, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_602 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_603 = _t0_602;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_604 = _mm256_mul_pd(_t0_601, _t0_603);

  // 4-BLAC: 1x4 - 1x4
  _t0_605 = _mm256_sub_pd(_t0_600, _t0_604);

  // AVX Storer:
  _t0_70 = _t0_605;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_606 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_70, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_607 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t0_609 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_606), _mm256_castpd256_pd128(_t0_607)));

  // AVX Storer:
  _t0_71 = _t0_609;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_610 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_70, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_611 = _t0_71;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_612 = _t0_9;

  // 4-BLAC: (4x1)^T
  _t0_613 = _t0_612;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_615 = _mm256_mul_pd(_t0_611, _t0_613);

  // 4-BLAC: 1x4 - 1x4
  _t0_616 = _mm256_sub_pd(_t0_610, _t0_615);

  // AVX Storer:
  _t0_72 = _t0_616;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_617 = _t0_72;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_618 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t0_620 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_617), _mm256_castpd256_pd128(_t0_618)));

  // AVX Storer:
  _t0_72 = _t0_620;

  // Generating : L[36,36] = Sum_{j167} ( S(h(4, 36, fi390), ( G(h(4, 36, fi390), L[36,36],h(4, 36, fi469 + j167 + 4)) - ( G(h(4, 36, fi390), L[36,36],h(4, 36, fi469)) * T( G(h(4, 36, fi469 + j167 + 4), L[36,36],h(4, 36, fi469)) ) ) ),h(4, 36, fi469 + j167 + 4)) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_697 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 32);
  _t0_698 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 32);
  _t0_699 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 49);
  _t0_700 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_11, _t0_13), _mm256_unpacklo_pd(_t0_15, _t0_16), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_17, _t0_19), _mm256_unpacklo_pd(_t0_21, _t0_22), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_23, _t0_25), _mm256_unpacklo_pd(_t0_27, _t0_28), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_29, _t0_31), _mm256_unpacklo_pd(_t0_33, _t0_34), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_111 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t0_700)));
  _t0_112 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_59, _t0_59, 32), _mm256_permute2f128_pd(_t0_59, _t0_59, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_60, _t0_60, 32), _mm256_permute2f128_pd(_t0_60, _t0_60, 32), 0), _t0_700)));
  _t0_113 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_61, _t0_61, 32), _mm256_permute2f128_pd(_t0_61, _t0_61, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_65, _t0_65, 32), _mm256_permute2f128_pd(_t0_65, _t0_65, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_66, _t0_66, 32), _mm256_permute2f128_pd(_t0_66, _t0_66, 32), 0), _t0_700)));
  _t0_114 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_67, _t0_67, 32), _mm256_permute2f128_pd(_t0_67, _t0_67, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_69, _t0_69, 32), _mm256_permute2f128_pd(_t0_69, _t0_69, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_71, _t0_71, 32), _mm256_permute2f128_pd(_t0_71, _t0_71, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_72, _t0_72, 32), _mm256_permute2f128_pd(_t0_72, _t0_72, 32), 0), _t0_700)));

  // 4-BLAC: 4x4 - 4x4
  _t0_127 = _mm256_sub_pd(_t0_127, _t0_111);
  _t0_128 = _mm256_sub_pd(_t0_128, _t0_112);
  _t0_129 = _mm256_sub_pd(_t0_129, _t0_113);
  _t0_130 = _mm256_sub_pd(_t0_130, _t0_114);

  // AVX Storer:

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_621 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_127, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_622 = _t0_35;

  // 4-BLAC: 1x4 / 1x4
  _t0_623 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_621), _mm256_castpd256_pd128(_t0_622)));

  // AVX Storer:
  _t0_73 = _t0_623;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_624 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_127, 14), _mm256_permute2f128_pd(_t0_127, _t0_127, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_625 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_73, _t0_73, 32), _mm256_permute2f128_pd(_t0_73, _t0_73, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_626 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_40, _t0_40, 2), _t0_44, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_627 = _t0_626;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_628 = _mm256_mul_pd(_t0_625, _t0_627);

  // 4-BLAC: 1x4 - 1x4
  _t0_630 = _mm256_sub_pd(_t0_624, _t0_628);

  // AVX Storer:
  _t0_74 = _t0_630;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_631 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_74, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_632 = _t0_41;

  // 4-BLAC: 1x4 / 1x4
  _t0_633 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_631), _mm256_castpd256_pd128(_t0_632)));

  // AVX Storer:
  _t0_75 = _t0_633;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_634 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_74, 6), _mm256_permute2f128_pd(_t0_74, _t0_74, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_635 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_75, _t0_75, 32), _mm256_permute2f128_pd(_t0_75, _t0_75, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_636 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_42, _t0_46), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_637 = _t0_636;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_638 = _mm256_mul_pd(_t0_635, _t0_637);

  // 4-BLAC: 1x4 - 1x4
  _t0_639 = _mm256_sub_pd(_t0_634, _t0_638);

  // AVX Storer:
  _t0_76 = _t0_639;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_640 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_76, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_641 = _t0_43;

  // 4-BLAC: 1x4 / 1x4
  _t0_642 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_640), _mm256_castpd256_pd128(_t0_641)));

  // AVX Storer:
  _t0_77 = _t0_642;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_644 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_76, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_645 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_646 = _t0_47;

  // 4-BLAC: (4x1)^T
  _t0_647 = _t0_646;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_648 = _mm256_mul_pd(_t0_645, _t0_647);

  // 4-BLAC: 1x4 - 1x4
  _t0_649 = _mm256_sub_pd(_t0_644, _t0_648);

  // AVX Storer:
  _t0_78 = _t0_649;

  // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_650 = _t0_78;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_651 = _t0_48;

  // 4-BLAC: 1x4 / 1x4
  _t0_652 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_650), _mm256_castpd256_pd128(_t0_651)));

  // AVX Storer:
  _t0_78 = _t0_652;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_653 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_128, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_654 = _t0_35;

  // 4-BLAC: 1x4 / 1x4
  _t0_655 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_653), _mm256_castpd256_pd128(_t0_654)));

  // AVX Storer:
  _t0_79 = _t0_655;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_656 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_128, 14), _mm256_permute2f128_pd(_t0_128, _t0_128, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_657 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_79, _t0_79, 32), _mm256_permute2f128_pd(_t0_79, _t0_79, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_659 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_40, _t0_40, 2), _t0_44, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_660 = _t0_659;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_661 = _mm256_mul_pd(_t0_657, _t0_660);

  // 4-BLAC: 1x4 - 1x4
  _t0_662 = _mm256_sub_pd(_t0_656, _t0_661);

  // AVX Storer:
  _t0_80 = _t0_662;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_663 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_80, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_664 = _t0_41;

  // 4-BLAC: 1x4 / 1x4
  _t0_665 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_663), _mm256_castpd256_pd128(_t0_664)));

  // AVX Storer:
  _t0_81 = _t0_665;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_666 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_80, 6), _mm256_permute2f128_pd(_t0_80, _t0_80, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_667 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_81, _t0_81, 32), _mm256_permute2f128_pd(_t0_81, _t0_81, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_668 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_42, _t0_46), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_669 = _t0_668;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_670 = _mm256_mul_pd(_t0_667, _t0_669);

  // 4-BLAC: 1x4 - 1x4
  _t0_671 = _mm256_sub_pd(_t0_666, _t0_670);

  // AVX Storer:
  _t0_82 = _t0_671;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_673 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_82, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_674 = _t0_43;

  // 4-BLAC: 1x4 / 1x4
  _t0_675 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_673), _mm256_castpd256_pd128(_t0_674)));

  // AVX Storer:
  _t0_83 = _t0_675;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_676 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_82, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_677 = _t0_83;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_678 = _t0_47;

  // 4-BLAC: (4x1)^T
  _t0_679 = _t0_678;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_680 = _mm256_mul_pd(_t0_677, _t0_679);

  // 4-BLAC: 1x4 - 1x4
  _t0_681 = _mm256_sub_pd(_t0_676, _t0_680);

  // AVX Storer:
  _t0_84 = _t0_681;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_683 = _t0_84;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_684 = _t0_48;

  // 4-BLAC: 1x4 / 1x4
  _t0_685 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_683), _mm256_castpd256_pd128(_t0_684)));

  // AVX Storer:
  _t0_84 = _t0_685;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_686 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_129, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_687 = _t0_35;

  // 4-BLAC: 1x4 / 1x4
  _t0_688 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_686), _mm256_castpd256_pd128(_t0_687)));

  // AVX Storer:
  _t0_85 = _t0_688;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_689 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_129, 14), _mm256_permute2f128_pd(_t0_129, _t0_129, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_690 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_85, _t0_85, 32), _mm256_permute2f128_pd(_t0_85, _t0_85, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_691 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_40, _t0_40, 2), _t0_44, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_692 = _t0_691;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_693 = _mm256_mul_pd(_t0_690, _t0_692);

  // 4-BLAC: 1x4 - 1x4
  _t0_694 = _mm256_sub_pd(_t0_689, _t0_693);

  // AVX Storer:
  _t0_86 = _t0_694;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_695 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_86, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_696 = _t0_41;

  // 4-BLAC: 1x4 / 1x4
  _t0_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_695), _mm256_castpd256_pd128(_t0_696)));

  // AVX Storer:
  _t0_87 = _t0_140;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_141 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_86, 6), _mm256_permute2f128_pd(_t0_86, _t0_86, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_142 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_87, _t0_87, 32), _mm256_permute2f128_pd(_t0_87, _t0_87, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_143 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_42, _t0_46), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_144 = _t0_143;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_146 = _mm256_mul_pd(_t0_142, _t0_144);

  // 4-BLAC: 1x4 - 1x4
  _t0_147 = _mm256_sub_pd(_t0_141, _t0_146);

  // AVX Storer:
  _t0_88 = _t0_147;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_148 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_88, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_149 = _t0_43;

  // 4-BLAC: 1x4 / 1x4
  _t0_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_148), _mm256_castpd256_pd128(_t0_149)));

  // AVX Storer:
  _t0_89 = _t0_150;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_151 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_88, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_152 = _t0_89;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_153 = _t0_47;

  // 4-BLAC: (4x1)^T
  _t0_154 = _t0_153;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_155 = _mm256_mul_pd(_t0_152, _t0_154);

  // 4-BLAC: 1x4 - 1x4
  _t0_156 = _mm256_sub_pd(_t0_151, _t0_155);

  // AVX Storer:
  _t0_90 = _t0_156;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_157 = _t0_90;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_158 = _t0_48;

  // 4-BLAC: 1x4 / 1x4
  _t0_160 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_157), _mm256_castpd256_pd128(_t0_158)));

  // AVX Storer:
  _t0_90 = _t0_160;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_161 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_130, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_162 = _t0_35;

  // 4-BLAC: 1x4 / 1x4
  _t0_163 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_161), _mm256_castpd256_pd128(_t0_162)));

  // AVX Storer:
  _t0_91 = _t0_163;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_164 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_130, 14), _mm256_permute2f128_pd(_t0_130, _t0_130, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_165 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_91, _t0_91, 32), _mm256_permute2f128_pd(_t0_91, _t0_91, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_166 = _mm256_blend_pd(_mm256_permute2f128_pd(_mm256_blend_pd(_t0_40, _t0_40, 2), _t0_44, 32), _mm256_setzero_pd(), 8);

  // 4-BLAC: (4x1)^T
  _t0_167 = _t0_166;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_168 = _mm256_mul_pd(_t0_165, _t0_167);

  // 4-BLAC: 1x4 - 1x4
  _t0_169 = _mm256_sub_pd(_t0_164, _t0_168);

  // AVX Storer:
  _t0_92 = _t0_169;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_170 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_92, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_171 = _t0_41;

  // 4-BLAC: 1x4 / 1x4
  _t0_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_170), _mm256_castpd256_pd128(_t0_171)));

  // AVX Storer:
  _t0_93 = _t0_172;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_174 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_92, 6), _mm256_permute2f128_pd(_t0_92, _t0_92, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_175 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_93, _t0_93, 32), _mm256_permute2f128_pd(_t0_93, _t0_93, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_176 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_42, _t0_46), _mm256_setzero_pd(), 12);

  // 4-BLAC: (4x1)^T
  _t0_177 = _t0_176;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_178 = _mm256_mul_pd(_t0_175, _t0_177);

  // 4-BLAC: 1x4 - 1x4
  _t0_180 = _mm256_sub_pd(_t0_174, _t0_178);

  // AVX Storer:
  _t0_94 = _t0_180;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_181 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_94, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_182 = _t0_43;

  // 4-BLAC: 1x4 / 1x4
  _t0_183 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_181), _mm256_castpd256_pd128(_t0_182)));

  // AVX Storer:
  _t0_95 = _t0_183;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_185 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_94, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_186 = _t0_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_187 = _t0_47;

  // 4-BLAC: (4x1)^T
  _t0_188 = _t0_187;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_189 = _mm256_mul_pd(_t0_186, _t0_188);

  // 4-BLAC: 1x4 - 1x4
  _t0_190 = _mm256_sub_pd(_t0_185, _t0_189);

  // AVX Storer:
  _t0_96 = _t0_190;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_191 = _t0_96;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_192 = _t0_48;

  // 4-BLAC: 1x4 / 1x4
  _t0_193 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_191), _mm256_castpd256_pd128(_t0_192)));

  // AVX Storer:
  _t0_96 = _t0_193;

  // Generating : L[36,36] = ( S(h(4, 36, fi390), ( G(h(4, 36, fi390), K[36,36],h(4, 36, fi390)) - ( G(h(4, 36, fi390), L[36,36],h(4, 36, 0)) * T( G(h(4, 36, fi390), L[36,36],h(4, 36, 0)) ) ) ),h(4, 36, fi390)) + Sum_{j167} ( -$(h(4, 36, fi390), ( G(h(4, 36, fi390), L[36,36],h(4, 36, j167)) * T( G(h(4, 36, fi390), L[36,36],h(4, 36, j167)) ) ),h(4, 36, fi390)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t0_195 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t0_97, _t0_98, 0), _mm256_shuffle_pd(_t0_99, _t0_100, 0), 32);
  _t0_196 = _mm256_permute2f128_pd(_t0_98, _mm256_shuffle_pd(_t0_99, _t0_100, 3), 32);
  _t0_197 = _mm256_blend_pd(_t0_99, _mm256_shuffle_pd(_t0_99, _t0_100, 3), 12);
  _t0_198 = _t0_100;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_701 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_49, _t0_51), _mm256_unpacklo_pd(_t0_53, _t0_54), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_57), _mm256_unpacklo_pd(_t0_59, _t0_60), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_61, _t0_63), _mm256_unpacklo_pd(_t0_65, _t0_66), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_67, _t0_69), _mm256_unpacklo_pd(_t0_71, _t0_72), 32)), 32);
  _t0_702 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_49, _t0_51), _mm256_unpacklo_pd(_t0_53, _t0_54), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_57), _mm256_unpacklo_pd(_t0_59, _t0_60), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_61, _t0_63), _mm256_unpacklo_pd(_t0_65, _t0_66), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_67, _t0_69), _mm256_unpacklo_pd(_t0_71, _t0_72), 32)), 32);
  _t0_703 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_49, _t0_51), _mm256_unpacklo_pd(_t0_53, _t0_54), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_57), _mm256_unpacklo_pd(_t0_59, _t0_60), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_61, _t0_63), _mm256_unpacklo_pd(_t0_65, _t0_66), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_67, _t0_69), _mm256_unpacklo_pd(_t0_71, _t0_72), 32)), 49);
  _t0_704 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_49, _t0_51), _mm256_unpacklo_pd(_t0_53, _t0_54), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_57), _mm256_unpacklo_pd(_t0_59, _t0_60), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_61, _t0_63), _mm256_unpacklo_pd(_t0_65, _t0_66), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_67, _t0_69), _mm256_unpacklo_pd(_t0_71, _t0_72), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_115 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t0_701), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t0_702)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t0_703), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t0_704)));
  _t0_116 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_701), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0), _t0_702)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_59, _t0_59, 32), _mm256_permute2f128_pd(_t0_59, _t0_59, 32), 0), _t0_703), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_60, _t0_60, 32), _mm256_permute2f128_pd(_t0_60, _t0_60, 32), 0), _t0_704)));
  _t0_117 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_61, _t0_61, 32), _mm256_permute2f128_pd(_t0_61, _t0_61, 32), 0), _t0_701), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0), _t0_702)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_65, _t0_65, 32), _mm256_permute2f128_pd(_t0_65, _t0_65, 32), 0), _t0_703), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_66, _t0_66, 32), _mm256_permute2f128_pd(_t0_66, _t0_66, 32), 0), _t0_704)));
  _t0_118 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_67, _t0_67, 32), _mm256_permute2f128_pd(_t0_67, _t0_67, 32), 0), _t0_701), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_69, _t0_69, 32), _mm256_permute2f128_pd(_t0_69, _t0_69, 32), 0), _t0_702)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_71, _t0_71, 32), _mm256_permute2f128_pd(_t0_71, _t0_71, 32), 0), _t0_703), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_72, _t0_72, 32), _mm256_permute2f128_pd(_t0_72, _t0_72, 32), 0), _t0_704)));

  // 4-BLAC: 4x4 - 4x4
  _t0_131 = _mm256_sub_pd(_t0_195, _t0_115);
  _t0_132 = _mm256_sub_pd(_t0_196, _t0_116);
  _t0_133 = _mm256_sub_pd(_t0_197, _t0_117);
  _t0_134 = _mm256_sub_pd(_t0_198, _t0_118);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t0_97 = _t0_131;
  _t0_98 = _t0_132;
  _t0_99 = _t0_133;
  _t0_100 = _t0_134;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t0_705 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_73, _t0_75), _mm256_unpacklo_pd(_t0_77, _t0_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_81), _mm256_unpacklo_pd(_t0_83, _t0_84), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_85, _t0_87), _mm256_unpacklo_pd(_t0_89, _t0_90), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_91, _t0_93), _mm256_unpacklo_pd(_t0_95, _t0_96), 32)), 32);
  _t0_706 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_73, _t0_75), _mm256_unpacklo_pd(_t0_77, _t0_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_81), _mm256_unpacklo_pd(_t0_83, _t0_84), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_85, _t0_87), _mm256_unpacklo_pd(_t0_89, _t0_90), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_91, _t0_93), _mm256_unpacklo_pd(_t0_95, _t0_96), 32)), 32);
  _t0_707 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_73, _t0_75), _mm256_unpacklo_pd(_t0_77, _t0_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_81), _mm256_unpacklo_pd(_t0_83, _t0_84), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_85, _t0_87), _mm256_unpacklo_pd(_t0_89, _t0_90), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_91, _t0_93), _mm256_unpacklo_pd(_t0_95, _t0_96), 32)), 49);
  _t0_708 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_73, _t0_75), _mm256_unpacklo_pd(_t0_77, _t0_78), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_81), _mm256_unpacklo_pd(_t0_83, _t0_84), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_85, _t0_87), _mm256_unpacklo_pd(_t0_89, _t0_90), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_91, _t0_93), _mm256_unpacklo_pd(_t0_95, _t0_96), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t0_119 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_73, _t0_73, 32), _mm256_permute2f128_pd(_t0_73, _t0_73, 32), 0), _t0_705), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_75, _t0_75, 32), _mm256_permute2f128_pd(_t0_75, _t0_75, 32), 0), _t0_706)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_77, _t0_77, 32), _mm256_permute2f128_pd(_t0_77, _t0_77, 32), 0), _t0_707), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_78, _t0_78, 32), _mm256_permute2f128_pd(_t0_78, _t0_78, 32), 0), _t0_708)));
  _t0_120 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_79, _t0_79, 32), _mm256_permute2f128_pd(_t0_79, _t0_79, 32), 0), _t0_705), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_81, _t0_81, 32), _mm256_permute2f128_pd(_t0_81, _t0_81, 32), 0), _t0_706)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_83, _t0_83, 32), _mm256_permute2f128_pd(_t0_83, _t0_83, 32), 0), _t0_707), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_84, _t0_84, 32), _mm256_permute2f128_pd(_t0_84, _t0_84, 32), 0), _t0_708)));
  _t0_121 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_85, _t0_85, 32), _mm256_permute2f128_pd(_t0_85, _t0_85, 32), 0), _t0_705), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_87, _t0_87, 32), _mm256_permute2f128_pd(_t0_87, _t0_87, 32), 0), _t0_706)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_89, _t0_89, 32), _mm256_permute2f128_pd(_t0_89, _t0_89, 32), 0), _t0_707), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_90, _t0_90, 32), _mm256_permute2f128_pd(_t0_90, _t0_90, 32), 0), _t0_708)));
  _t0_122 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_91, _t0_91, 32), _mm256_permute2f128_pd(_t0_91, _t0_91, 32), 0), _t0_705), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_93, _t0_93, 32), _mm256_permute2f128_pd(_t0_93, _t0_93, 32), 0), _t0_706)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_95, _t0_95, 32), _mm256_permute2f128_pd(_t0_95, _t0_95, 32), 0), _t0_707), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_96, _t0_96, 32), _mm256_permute2f128_pd(_t0_96, _t0_96, 32), 0), _t0_708)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowTriang
  _t0_199 = _t0_97;
  _t0_200 = _t0_98;
  _t0_201 = _t0_99;
  _t0_202 = _t0_100;

  // 4-BLAC: 4x4 - 4x4
  _t0_199 = _mm256_sub_pd(_t0_199, _t0_119);
  _t0_200 = _mm256_sub_pd(_t0_200, _t0_120);
  _t0_201 = _mm256_sub_pd(_t0_201, _t0_121);
  _t0_202 = _mm256_sub_pd(_t0_202, _t0_122);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t0_97 = _t0_199;
  _t0_98 = _t0_200;
  _t0_99 = _t0_201;
  _t0_100 = _t0_202;

  // Generating : L[36,36] = S(h(1, 36, fi390), Sqrt( G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_203 = _t0_97;

  // 4-BLAC: sqrt(1x4)
  _t0_204 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_203)));

  // AVX Storer:
  _t0_97 = _t0_204;

  // Generating : T1473[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_205 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_206 = _t0_97;

  // 4-BLAC: 1x4 / 1x4
  _t0_207 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_205), _mm256_castpd256_pd128(_t0_206)));

  // AVX Storer:
  _t0_101 = _t0_207;

  // Generating : L[36,36] = S(h(2, 36, fi390 + 1), ( G(h(1, 1, 0), T1473[1,36],h(1, 36, fi390)) Kro G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_208 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_101, _t0_101, 32), _mm256_permute2f128_pd(_t0_101, _t0_101, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_209 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_98, _t0_99), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 4x1
  _t0_210 = _mm256_mul_pd(_t0_208, _t0_209);

  // AVX Storer:
  _t0_102 = _t0_210;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) Kro T( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ),h(1, 36, fi390 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_211 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_98, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_212 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_102, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_213 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_102, 1);

  // 4-BLAC: (4x1)^T
  _t0_214 = _t0_213;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_216 = _mm256_mul_pd(_t0_212, _t0_214);

  // 4-BLAC: 1x4 - 1x4
  _t0_217 = _mm256_sub_pd(_t0_211, _t0_216);

  // AVX Storer:
  _t0_103 = _t0_217;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 1), Sqrt( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ),h(1, 36, fi390 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_218 = _t0_103;

  // 4-BLAC: sqrt(1x4)
  _t0_219 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_218)));

  // AVX Storer:
  _t0_103 = _t0_219;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390)) Kro T( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ),h(1, 36, fi390 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_220 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_99, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_221 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_102, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_222 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_102, 1);

  // 4-BLAC: (4x1)^T
  _t0_223 = _t0_222;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_224 = _mm256_mul_pd(_t0_221, _t0_223);

  // 4-BLAC: 1x4 - 1x4
  _t0_225 = _mm256_sub_pd(_t0_220, _t0_224);

  // AVX Storer:
  _t0_104 = _t0_225;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) Div G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ),h(1, 36, fi390 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_226 = _t0_104;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_227 = _t0_103;

  // 4-BLAC: 1x4 / 1x4
  _t0_229 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_226), _mm256_castpd256_pd128(_t0_227)));

  // AVX Storer:
  _t0_104 = _t0_229;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi390)) * T( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi390)) ) ) ),h(1, 36, fi390 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_230 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_99, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_99, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_231 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_102, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_104, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_232 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_102, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_104, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_233 = _t0_232;

  // 4-BLAC: 1x4 * 4x1
  _t0_234 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_231, _t0_233), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_231, _t0_233), _mm256_mul_pd(_t0_231, _t0_233), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_231, _t0_233), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_231, _t0_233), _mm256_mul_pd(_t0_231, _t0_233), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_231, _t0_233), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_231, _t0_233), _mm256_mul_pd(_t0_231, _t0_233), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_235 = _mm256_sub_pd(_t0_230, _t0_234);

  // AVX Storer:
  _t0_105 = _t0_235;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 2), Sqrt( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ),h(1, 36, fi390 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_236 = _t0_105;

  // 4-BLAC: sqrt(1x4)
  _t0_237 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_236)));

  // AVX Storer:
  _t0_105 = _t0_237;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) Div G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_238 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_100, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_239 = _t0_97;

  // 4-BLAC: 1x4 / 1x4
  _t0_240 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_238), _mm256_castpd256_pd128(_t0_239)));

  // AVX Storer:
  _t0_106 = _t0_240;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi390 + 1)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) Kro T( G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ),h(2, 36, fi390 + 1))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_241 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_100, 6), _mm256_permute2f128_pd(_t0_100, _t0_100, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_242 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_106, _t0_106, 32), _mm256_permute2f128_pd(_t0_106, _t0_106, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_244 = _t0_102;

  // 4-BLAC: (4x1)^T
  _t0_245 = _t0_244;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_246 = _mm256_mul_pd(_t0_242, _t0_245);

  // 4-BLAC: 1x4 - 1x4
  _t0_247 = _mm256_sub_pd(_t0_241, _t0_246);

  // AVX Storer:
  _t0_107 = _t0_247;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) Div G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ),h(1, 36, fi390 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_249 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_107, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_250 = _t0_103;

  // 4-BLAC: 1x4 / 1x4
  _t0_251 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_249), _mm256_castpd256_pd128(_t0_250)));

  // AVX Storer:
  _t0_108 = _t0_251;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) Kro T( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) ) ),h(1, 36, fi390 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_252 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_107, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_253 = _t0_108;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_254 = _t0_104;

  // 4-BLAC: (4x1)^T
  _t0_255 = _t0_254;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_256 = _mm256_mul_pd(_t0_253, _t0_255);

  // 4-BLAC: 1x4 - 1x4
  _t0_257 = _mm256_sub_pd(_t0_252, _t0_256);

  // AVX Storer:
  _t0_109 = _t0_257;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) Div G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ),h(1, 36, fi390 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_258 = _t0_109;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_259 = _t0_105;

  // 4-BLAC: 1x4 / 1x4
  _t0_260 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_258), _mm256_castpd256_pd128(_t0_259)));

  // AVX Storer:
  _t0_109 = _t0_260;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi390)) * T( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi390)) ) ) ),h(1, 36, fi390 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_261 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_100, _t0_100, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_262 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_106, _t0_108), _mm256_unpacklo_pd(_t0_109, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_264 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_106, _t0_108), _mm256_unpacklo_pd(_t0_109, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t0_265 = _t0_264;

  // 4-BLAC: 1x4 * 4x1
  _t0_266 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_262, _t0_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_262, _t0_265), _mm256_mul_pd(_t0_262, _t0_265), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_262, _t0_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_262, _t0_265), _mm256_mul_pd(_t0_262, _t0_265), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_262, _t0_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_262, _t0_265), _mm256_mul_pd(_t0_262, _t0_265), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_267 = _mm256_sub_pd(_t0_261, _t0_266);

  // AVX Storer:
  _t0_110 = _t0_267;

  // Generating : L[36,36] = S(h(1, 36, fi390 + 3), Sqrt( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) ),h(1, 36, fi390 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_268 = _t0_110;

  // 4-BLAC: sqrt(1x4)
  _t0_269 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_268)));

  // AVX Storer:
  _t0_110 = _t0_269;

  _mm_store_sd(&(K[0]), _mm256_castpd256_pd128(_t0_0));
  _mm256_maskstore_pd(K + 36, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_2);
  _mm256_maskstore_pd(K + 72, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_2, _t0_2, 1));
  _mm_store_sd(&(K[37]), _mm256_castpd256_pd128(_t0_3));
  _mm_store_sd(&(K[73]), _mm256_castpd256_pd128(_t0_4));
  _mm_store_sd(&(K[74]), _mm256_castpd256_pd128(_t0_5));
  _mm_store_sd(&(K[108]), _mm256_castpd256_pd128(_t0_6));
  _mm_store_sd(&(K[109]), _mm256_castpd256_pd128(_t0_8));
  _mm_store_sd(&(K[110]), _mm256_castpd256_pd128(_t0_9));
  _mm_store_sd(&(K[111]), _mm256_castpd256_pd128(_t0_10));
  _mm_store_sd(&(K[144]), _mm256_castpd256_pd128(_t0_11));
  _mm_store_sd(&(K[145]), _mm256_castpd256_pd128(_t0_13));
  _mm_store_sd(&(K[146]), _mm256_castpd256_pd128(_t0_15));
  _mm_store_sd(&(K[147]), _mm256_castpd256_pd128(_t0_16));
  _mm_store_sd(&(K[180]), _mm256_castpd256_pd128(_t0_17));
  _mm_store_sd(&(K[181]), _mm256_castpd256_pd128(_t0_19));
  _mm_store_sd(&(K[182]), _mm256_castpd256_pd128(_t0_21));
  _mm_store_sd(&(K[183]), _mm256_castpd256_pd128(_t0_22));
  _mm_store_sd(&(K[216]), _mm256_castpd256_pd128(_t0_23));
  _mm_store_sd(&(K[217]), _mm256_castpd256_pd128(_t0_25));
  _mm_store_sd(&(K[218]), _mm256_castpd256_pd128(_t0_27));
  _mm_store_sd(&(K[219]), _mm256_castpd256_pd128(_t0_28));
  _mm_store_sd(&(K[252]), _mm256_castpd256_pd128(_t0_29));
  _mm_store_sd(&(K[253]), _mm256_castpd256_pd128(_t0_31));
  _mm_store_sd(&(K[254]), _mm256_castpd256_pd128(_t0_33));
  _mm_store_sd(&(K[255]), _mm256_castpd256_pd128(_t0_34));
  _mm_store_sd(K + 148, _mm256_castpd256_pd128(_t0_35));
  _mm256_maskstore_pd(K + 184, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_40);
  _mm256_maskstore_pd(K + 220, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_40, _t0_40, 1));
  _mm_store_sd(&(K[185]), _mm256_castpd256_pd128(_t0_41));
  _mm_store_sd(&(K[221]), _mm256_castpd256_pd128(_t0_42));
  _mm_store_sd(&(K[222]), _mm256_castpd256_pd128(_t0_43));
  _mm_store_sd(&(K[256]), _mm256_castpd256_pd128(_t0_44));
  _mm_store_sd(&(K[257]), _mm256_castpd256_pd128(_t0_46));
  _mm_store_sd(&(K[258]), _mm256_castpd256_pd128(_t0_47));
  _mm_store_sd(&(K[259]), _mm256_castpd256_pd128(_t0_48));
  _mm_store_sd(&(K[288]), _mm256_castpd256_pd128(_t0_49));
  _mm_store_sd(&(K[289]), _mm256_castpd256_pd128(_t0_51));
  _mm_store_sd(&(K[290]), _mm256_castpd256_pd128(_t0_53));
  _mm_store_sd(&(K[291]), _mm256_castpd256_pd128(_t0_54));
  _mm_store_sd(&(K[324]), _mm256_castpd256_pd128(_t0_55));
  _mm_store_sd(&(K[325]), _mm256_castpd256_pd128(_t0_57));
  _mm_store_sd(&(K[326]), _mm256_castpd256_pd128(_t0_59));
  _mm_store_sd(&(K[327]), _mm256_castpd256_pd128(_t0_60));
  _mm_store_sd(&(K[360]), _mm256_castpd256_pd128(_t0_61));
  _mm_store_sd(&(K[361]), _mm256_castpd256_pd128(_t0_63));
  _mm_store_sd(&(K[362]), _mm256_castpd256_pd128(_t0_65));
  _mm_store_sd(&(K[363]), _mm256_castpd256_pd128(_t0_66));
  _mm_store_sd(&(K[396]), _mm256_castpd256_pd128(_t0_67));
  _mm_store_sd(&(K[397]), _mm256_castpd256_pd128(_t0_69));
  _mm_store_sd(&(K[398]), _mm256_castpd256_pd128(_t0_71));
  _mm_store_sd(&(K[399]), _mm256_castpd256_pd128(_t0_72));
  _mm_store_sd(&(K[292]), _mm256_castpd256_pd128(_t0_73));
  _mm_store_sd(&(K[293]), _mm256_castpd256_pd128(_t0_75));
  _mm_store_sd(&(K[294]), _mm256_castpd256_pd128(_t0_77));
  _mm_store_sd(&(K[295]), _mm256_castpd256_pd128(_t0_78));
  _mm_store_sd(&(K[328]), _mm256_castpd256_pd128(_t0_79));
  _mm_store_sd(&(K[329]), _mm256_castpd256_pd128(_t0_81));
  _mm_store_sd(&(K[330]), _mm256_castpd256_pd128(_t0_83));
  _mm_store_sd(&(K[331]), _mm256_castpd256_pd128(_t0_84));
  _mm_store_sd(&(K[364]), _mm256_castpd256_pd128(_t0_85));
  _mm_store_sd(&(K[365]), _mm256_castpd256_pd128(_t0_87));
  _mm_store_sd(&(K[366]), _mm256_castpd256_pd128(_t0_89));
  _mm_store_sd(&(K[367]), _mm256_castpd256_pd128(_t0_90));
  _mm_store_sd(&(K[400]), _mm256_castpd256_pd128(_t0_91));
  _mm_store_sd(&(K[401]), _mm256_castpd256_pd128(_t0_93));
  _mm_store_sd(&(K[402]), _mm256_castpd256_pd128(_t0_95));
  _mm_store_sd(&(K[403]), _mm256_castpd256_pd128(_t0_96));
  _mm_store_sd(K + 296, _mm256_castpd256_pd128(_t0_97));
  _mm256_maskstore_pd(K + 332, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_102);
  _mm256_maskstore_pd(K + 368, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_102, _t0_102, 1));
  _mm_store_sd(&(K[333]), _mm256_castpd256_pd128(_t0_103));
  _mm_store_sd(&(K[369]), _mm256_castpd256_pd128(_t0_104));
  _mm_store_sd(&(K[370]), _mm256_castpd256_pd128(_t0_105));
  _mm_store_sd(&(K[404]), _mm256_castpd256_pd128(_t0_106));
  _mm_store_sd(&(K[405]), _mm256_castpd256_pd128(_t0_108));
  _mm_store_sd(&(K[406]), _mm256_castpd256_pd128(_t0_109));
  _mm_store_sd(&(K[407]), _mm256_castpd256_pd128(_t0_110));

  for( int fi390 = 12; fi390 <= 32; fi390+=4 ) {

    for( int fi469 = 0; fi469 <= fi390 - 5; fi469+=4 ) {
      _t1_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469])));
      _t1_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi469])));
      _t1_8 = _mm256_maskload_pd(K + 36*fi390 + fi469 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi469 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi469 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi469 + 108)), 32);
      _t1_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi469 + 37])));
      _t1_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi469 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi469 + 109)), 0);
      _t1_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi469 + 74])));
      _t1_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi469 + 110])));
      _t1_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi469 + 111])));
      _t1_13 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 36])));
      _t1_14 = _mm256_maskload_pd(K + 36*fi390 + fi469 + 37, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_19 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 72])));
      _t1_20 = _mm256_maskload_pd(K + 36*fi390 + fi469 + 73, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_25 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 108])));
      _t1_26 = _mm256_maskload_pd(K + 36*fi390 + fi469 + 109, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_31 = _t1_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_32 = _t1_6;

      // 4-BLAC: 1x4 / 1x4
      _t1_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_31), _mm256_castpd256_pd128(_t1_32)));

      // AVX Storer:
      _t1_7 = _t1_33;

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_34 = _t1_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_7, _t1_7, 32), _mm256_permute2f128_pd(_t1_7, _t1_7, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_36 = _t1_5;

      // 4-BLAC: (4x1)^T
      _t0_494 = _t1_36;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_496 = _mm256_mul_pd(_t1_35, _t0_494);

      // 4-BLAC: 1x4 - 1x4
      _t1_37 = _mm256_sub_pd(_t1_34, _t0_496);

      // AVX Storer:
      _t1_8 = _t1_37;

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_8, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_39 = _t1_4;

      // 4-BLAC: 1x4 / 1x4
      _t1_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_38), _mm256_castpd256_pd128(_t1_39)));

      // AVX Storer:
      _t1_9 = _t1_40;

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_8, 6), _mm256_permute2f128_pd(_t1_8, _t1_8, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_9, _t1_9, 32), _mm256_permute2f128_pd(_t1_9, _t1_9, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_43 = _t1_3;

      // 4-BLAC: (4x1)^T
      _t0_504 = _t1_43;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_505 = _mm256_mul_pd(_t1_42, _t0_504);

      // 4-BLAC: 1x4 - 1x4
      _t1_44 = _mm256_sub_pd(_t1_41, _t0_505);

      // AVX Storer:
      _t1_10 = _t1_44;

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_45 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_10, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_46 = _t1_2;

      // 4-BLAC: 1x4 / 1x4
      _t1_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_45), _mm256_castpd256_pd128(_t1_46)));

      // AVX Storer:
      _t1_11 = _t1_47;

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_48 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_10, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_49 = _t1_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_50 = _t1_1;

      // 4-BLAC: (4x1)^T
      _t0_514 = _t1_50;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_516 = _mm256_mul_pd(_t1_49, _t0_514);

      // 4-BLAC: 1x4 - 1x4
      _t1_51 = _mm256_sub_pd(_t1_48, _t0_516);

      // AVX Storer:
      _t1_12 = _t1_51;

      // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_52 = _t1_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_53 = _t1_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_54 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_52), _mm256_castpd256_pd128(_t1_53)));

      // AVX Storer:
      _t1_12 = _t1_54;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_55 = _t1_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_56 = _t1_6;

      // 4-BLAC: 1x4 / 1x4
      _t1_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_55), _mm256_castpd256_pd128(_t1_56)));

      // AVX Storer:
      _t1_13 = _t1_57;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_58 = _t1_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_13, _t1_13, 32), _mm256_permute2f128_pd(_t1_13, _t1_13, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_60 = _t1_5;

      // 4-BLAC: (4x1)^T
      _t0_528 = _t1_60;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_529 = _mm256_mul_pd(_t1_59, _t0_528);

      // 4-BLAC: 1x4 - 1x4
      _t1_61 = _mm256_sub_pd(_t1_58, _t0_529);

      // AVX Storer:
      _t1_14 = _t1_61;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_14, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_63 = _t1_4;

      // 4-BLAC: 1x4 / 1x4
      _t1_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_62), _mm256_castpd256_pd128(_t1_63)));

      // AVX Storer:
      _t1_15 = _t1_64;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_65 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_14, 6), _mm256_permute2f128_pd(_t1_14, _t1_14, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_66 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_15, _t1_15, 32), _mm256_permute2f128_pd(_t1_15, _t1_15, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_67 = _t1_3;

      // 4-BLAC: (4x1)^T
      _t0_538 = _t1_67;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_539 = _mm256_mul_pd(_t1_66, _t0_538);

      // 4-BLAC: 1x4 - 1x4
      _t1_68 = _mm256_sub_pd(_t1_65, _t0_539);

      // AVX Storer:
      _t1_16 = _t1_68;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_69 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_16, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_70 = _t1_2;

      // 4-BLAC: 1x4 / 1x4
      _t1_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_69), _mm256_castpd256_pd128(_t1_70)));

      // AVX Storer:
      _t1_17 = _t1_71;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_72 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_16, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_73 = _t1_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_74 = _t1_1;

      // 4-BLAC: (4x1)^T
      _t0_548 = _t1_74;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_549 = _mm256_mul_pd(_t1_73, _t0_548);

      // 4-BLAC: 1x4 - 1x4
      _t1_75 = _mm256_sub_pd(_t1_72, _t0_549);

      // AVX Storer:
      _t1_18 = _t1_75;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_76 = _t1_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_77 = _t1_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_76), _mm256_castpd256_pd128(_t1_77)));

      // AVX Storer:
      _t1_18 = _t1_78;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_79 = _t1_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_80 = _t1_6;

      // 4-BLAC: 1x4 / 1x4
      _t1_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_79), _mm256_castpd256_pd128(_t1_80)));

      // AVX Storer:
      _t1_19 = _t1_81;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_82 = _t1_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_19, _t1_19, 32), _mm256_permute2f128_pd(_t1_19, _t1_19, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_84 = _t1_5;

      // 4-BLAC: (4x1)^T
      _t0_561 = _t1_84;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_562 = _mm256_mul_pd(_t1_83, _t0_561);

      // 4-BLAC: 1x4 - 1x4
      _t1_85 = _mm256_sub_pd(_t1_82, _t0_562);

      // AVX Storer:
      _t1_20 = _t1_85;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_20, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_87 = _t1_4;

      // 4-BLAC: 1x4 / 1x4
      _t1_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_86), _mm256_castpd256_pd128(_t1_87)));

      // AVX Storer:
      _t1_21 = _t1_88;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_89 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_20, 6), _mm256_permute2f128_pd(_t1_20, _t1_20, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_90 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_21, _t1_21, 32), _mm256_permute2f128_pd(_t1_21, _t1_21, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_91 = _t1_3;

      // 4-BLAC: (4x1)^T
      _t0_570 = _t1_91;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_571 = _mm256_mul_pd(_t1_90, _t0_570);

      // 4-BLAC: 1x4 - 1x4
      _t1_92 = _mm256_sub_pd(_t1_89, _t0_571);

      // AVX Storer:
      _t1_22 = _t1_92;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_22, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_94 = _t1_2;

      // 4-BLAC: 1x4 / 1x4
      _t1_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_93), _mm256_castpd256_pd128(_t1_94)));

      // AVX Storer:
      _t1_23 = _t1_95;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_96 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_22, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_97 = _t1_23;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_98 = _t1_1;

      // 4-BLAC: (4x1)^T
      _t0_581 = _t1_98;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_582 = _mm256_mul_pd(_t1_97, _t0_581);

      // 4-BLAC: 1x4 - 1x4
      _t1_99 = _mm256_sub_pd(_t1_96, _t0_582);

      // AVX Storer:
      _t1_24 = _t1_99;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_100 = _t1_24;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_101 = _t1_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_102 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_100), _mm256_castpd256_pd128(_t1_101)));

      // AVX Storer:
      _t1_24 = _t1_102;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) Div G(h(1, 36, fi469), L[36,36],h(1, 36, fi469)) ),h(1, 36, fi469))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_103 = _t1_25;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_104 = _t1_6;

      // 4-BLAC: 1x4 / 1x4
      _t1_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_103), _mm256_castpd256_pd128(_t1_104)));

      // AVX Storer:
      _t1_25 = _t1_105;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi469 + 1)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469)) Kro T( G(h(3, 36, fi469 + 1), L[36,36],h(1, 36, fi469)) ) ) ),h(3, 36, fi469 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_106 = _t1_26;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_25, _t1_25, 32), _mm256_permute2f128_pd(_t1_25, _t1_25, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_108 = _t1_5;

      // 4-BLAC: (4x1)^T
      _t0_593 = _t1_108;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_595 = _mm256_mul_pd(_t1_107, _t0_593);

      // 4-BLAC: 1x4 - 1x4
      _t1_109 = _mm256_sub_pd(_t1_106, _t0_595);

      // AVX Storer:
      _t1_26 = _t1_109;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) Div G(h(1, 36, fi469 + 1), L[36,36],h(1, 36, fi469 + 1)) ),h(1, 36, fi469 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_26, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_111 = _t1_4;

      // 4-BLAC: 1x4 / 1x4
      _t1_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_110), _mm256_castpd256_pd128(_t1_111)));

      // AVX Storer:
      _t1_27 = _t1_112;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi469 + 2)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 1)) Kro T( G(h(2, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 1)) ) ) ),h(2, 36, fi469 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_113 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_26, 6), _mm256_permute2f128_pd(_t1_26, _t1_26, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_114 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_27, _t1_27, 32), _mm256_permute2f128_pd(_t1_27, _t1_27, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_115 = _t1_3;

      // 4-BLAC: (4x1)^T
      _t0_603 = _t1_115;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_604 = _mm256_mul_pd(_t1_114, _t0_603);

      // 4-BLAC: 1x4 - 1x4
      _t1_116 = _mm256_sub_pd(_t1_113, _t0_604);

      // AVX Storer:
      _t1_28 = _t1_116;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) Div G(h(1, 36, fi469 + 2), L[36,36],h(1, 36, fi469 + 2)) ),h(1, 36, fi469 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_117 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_28, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_118 = _t1_2;

      // 4-BLAC: 1x4 / 1x4
      _t1_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_117), _mm256_castpd256_pd128(_t1_118)));

      // AVX Storer:
      _t1_29 = _t1_119;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 2)) Kro T( G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 2)) ) ) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_120 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_28, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_121 = _t1_29;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_122 = _t1_1;

      // 4-BLAC: (4x1)^T
      _t0_613 = _t1_122;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_615 = _mm256_mul_pd(_t1_121, _t0_613);

      // 4-BLAC: 1x4 - 1x4
      _t1_123 = _mm256_sub_pd(_t1_120, _t0_615);

      // AVX Storer:
      _t1_30 = _t1_123;

      // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi469 + 3)) Div G(h(1, 36, fi469 + 3), L[36,36],h(1, 36, fi469 + 3)) ),h(1, 36, fi469 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_124 = _t1_30;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_125 = _t1_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_126 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_124), _mm256_castpd256_pd128(_t1_125)));

      // AVX Storer:
      _t1_30 = _t1_126;

      // Generating : L[36,36] = Sum_{j167} ( S(h(4, 36, fi390), ( G(h(4, 36, fi390), L[36,36],h(4, 36, fi469 + j167 + 4)) - ( G(h(4, 36, fi390), L[36,36],h(4, 36, fi469)) * T( G(h(4, 36, fi469 + j167 + 4), L[36,36],h(4, 36, fi469)) ) ) ),h(4, 36, fi469 + j167 + 4)) )
      _mm_store_sd(&(K[36*fi390 + fi469]), _mm256_castpd256_pd128(_t1_7));
      _mm_store_sd(&(K[36*fi390 + fi469 + 1]), _mm256_castpd256_pd128(_t1_9));
      _mm_store_sd(&(K[36*fi390 + fi469 + 2]), _mm256_castpd256_pd128(_t1_11));
      _mm_store_sd(&(K[36*fi390 + fi469 + 3]), _mm256_castpd256_pd128(_t1_12));
      _mm_store_sd(&(K[36*fi390 + fi469 + 36]), _mm256_castpd256_pd128(_t1_13));
      _mm_store_sd(&(K[36*fi390 + fi469 + 37]), _mm256_castpd256_pd128(_t1_15));
      _mm_store_sd(&(K[36*fi390 + fi469 + 38]), _mm256_castpd256_pd128(_t1_17));
      _mm_store_sd(&(K[36*fi390 + fi469 + 39]), _mm256_castpd256_pd128(_t1_18));
      _mm_store_sd(&(K[36*fi390 + fi469 + 72]), _mm256_castpd256_pd128(_t1_19));
      _mm_store_sd(&(K[36*fi390 + fi469 + 73]), _mm256_castpd256_pd128(_t1_21));
      _mm_store_sd(&(K[36*fi390 + fi469 + 74]), _mm256_castpd256_pd128(_t1_23));
      _mm_store_sd(&(K[36*fi390 + fi469 + 75]), _mm256_castpd256_pd128(_t1_24));
      _mm_store_sd(&(K[36*fi390 + fi469 + 108]), _mm256_castpd256_pd128(_t1_25));
      _mm_store_sd(&(K[36*fi390 + fi469 + 109]), _mm256_castpd256_pd128(_t1_27));
      _mm_store_sd(&(K[36*fi390 + fi469 + 110]), _mm256_castpd256_pd128(_t1_29));
      _mm_store_sd(&(K[36*fi390 + fi469 + 111]), _mm256_castpd256_pd128(_t1_30));

      for( int j167 = 0; j167 <= fi390 - fi469 - 5; j167+=4 ) {
        _t2_20 = _mm256_loadu_pd(K + 36*fi390 + fi469 + j167 + 4);
        _t2_21 = _mm256_loadu_pd(K + 36*fi390 + fi469 + j167 + 40);
        _t2_22 = _mm256_loadu_pd(K + 36*fi390 + fi469 + j167 + 76);
        _t2_23 = _mm256_loadu_pd(K + 36*fi390 + fi469 + j167 + 112);
        _t2_19 = _mm256_loadu_pd(K + 37*fi469 + 36*j167 + 144);
        _t2_18 = _mm256_loadu_pd(K + 37*fi469 + 36*j167 + 180);
        _t2_17 = _mm256_loadu_pd(K + 37*fi469 + 36*j167 + 216);
        _t2_16 = _mm256_loadu_pd(K + 37*fi469 + 36*j167 + 252);
        _t2_15 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469])));
        _t2_14 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 1])));
        _t2_13 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 2])));
        _t2_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 3])));
        _t2_11 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 36])));
        _t2_10 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 37])));
        _t2_9 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 38])));
        _t2_8 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 39])));
        _t2_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 72])));
        _t2_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 73])));
        _t2_5 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 74])));
        _t2_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 75])));
        _t2_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 108])));
        _t2_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 109])));
        _t2_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 110])));
        _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + fi469 + 111])));

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t0_697 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_19, _t2_18), _mm256_unpacklo_pd(_t2_17, _t2_16), 32);
        _t0_698 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t2_19, _t2_18), _mm256_unpackhi_pd(_t2_17, _t2_16), 32);
        _t0_699 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_19, _t2_18), _mm256_unpacklo_pd(_t2_17, _t2_16), 49);
        _t0_700 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t2_19, _t2_18), _mm256_unpackhi_pd(_t2_17, _t2_16), 49);

        // 4-BLAC: 4x4 * 4x4
        _t0_111 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_15, _t2_15, 32), _mm256_permute2f128_pd(_t2_15, _t2_15, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_14, _t2_14, 32), _mm256_permute2f128_pd(_t2_14, _t2_14, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_13, _t2_13, 32), _mm256_permute2f128_pd(_t2_13, _t2_13, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_12, _t2_12, 32), _mm256_permute2f128_pd(_t2_12, _t2_12, 32), 0), _t0_700)));
        _t0_112 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_11, _t2_11, 32), _mm256_permute2f128_pd(_t2_11, _t2_11, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_10, _t2_10, 32), _mm256_permute2f128_pd(_t2_10, _t2_10, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_9, _t2_9, 32), _mm256_permute2f128_pd(_t2_9, _t2_9, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_8, _t2_8, 32), _mm256_permute2f128_pd(_t2_8, _t2_8, 32), 0), _t0_700)));
        _t0_113 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_7, _t2_7, 32), _mm256_permute2f128_pd(_t2_7, _t2_7, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_6, _t2_6, 32), _mm256_permute2f128_pd(_t2_6, _t2_6, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 32), _mm256_permute2f128_pd(_t2_5, _t2_5, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_4, _t2_4, 32), _mm256_permute2f128_pd(_t2_4, _t2_4, 32), 0), _t0_700)));
        _t0_114 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_3, _t2_3, 32), _mm256_permute2f128_pd(_t2_3, _t2_3, 32), 0), _t0_697), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_2, _t2_2, 32), _mm256_permute2f128_pd(_t2_2, _t2_2, 32), 0), _t0_698)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_1, _t2_1, 32), _mm256_permute2f128_pd(_t2_1, _t2_1, 32), 0), _t0_699), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_0, _t2_0, 32), _mm256_permute2f128_pd(_t2_0, _t2_0, 32), 0), _t0_700)));

        // 4-BLAC: 4x4 - 4x4
        _t2_20 = _mm256_sub_pd(_t2_20, _t0_111);
        _t2_21 = _mm256_sub_pd(_t2_21, _t0_112);
        _t2_22 = _mm256_sub_pd(_t2_22, _t0_113);
        _t2_23 = _mm256_sub_pd(_t2_23, _t0_114);

        // AVX Storer:
        _mm256_storeu_pd(K + 36*fi390 + fi469 + j167 + 4, _t2_20);
        _mm256_storeu_pd(K + 36*fi390 + fi469 + j167 + 40, _t2_21);
        _mm256_storeu_pd(K + 36*fi390 + fi469 + j167 + 76, _t2_22);
        _mm256_storeu_pd(K + 36*fi390 + fi469 + j167 + 112, _t2_23);
      }
    }
    _t3_27 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + Max(0, fi390 - 4)])));
    _t3_26 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*Max(0, fi390 - 4)])));
    _t3_28 = _mm256_maskload_pd(K + 36*fi390 + Max(0, fi390 - 4) + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t3_25 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*Max(0, fi390 - 4) + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*Max(0, fi390 - 4) + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*Max(0, fi390 - 4) + 108)), 32);
    _t3_24 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*Max(0, fi390 - 4) + 37])));
    _t3_23 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*Max(0, fi390 - 4) + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*Max(0, fi390 - 4) + 109)), 0);
    _t3_22 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*Max(0, fi390 - 4) + 74])));
    _t3_21 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*Max(0, fi390 - 4) + 110])));
    _t3_20 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*Max(0, fi390 - 4) + 111])));
    _t3_33 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 36])));
    _t3_34 = _mm256_maskload_pd(K + 36*fi390 + Max(0, fi390 - 4) + 37, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t3_39 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 72])));
    _t3_40 = _mm256_maskload_pd(K + 36*fi390 + Max(0, fi390 - 4) + 73, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t3_45 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 108])));
    _t3_46 = _mm256_maskload_pd(K + 36*fi390 + Max(0, fi390 - 4) + 109, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t3_51 = _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390));
    _t3_52 = _mm256_maskload_pd(K + 37*fi390 + 36, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t3_53 = _mm256_maskload_pd(K + 37*fi390 + 72, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t3_54 = _mm256_loadu_pd(K + 37*fi390 + 108);
    _t3_19 = _mm256_broadcast_sd(K + 36*fi390);
    _t3_18 = _mm256_broadcast_sd(K + 36*fi390 + 1);
    _t3_17 = _mm256_broadcast_sd(K + 36*fi390 + 2);
    _t3_16 = _mm256_broadcast_sd(K + 36*fi390 + 3);
    _t3_15 = _mm256_broadcast_sd(K + 36*fi390 + 36);
    _t3_14 = _mm256_broadcast_sd(K + 36*fi390 + 37);
    _t3_13 = _mm256_broadcast_sd(K + 36*fi390 + 38);
    _t3_12 = _mm256_broadcast_sd(K + 36*fi390 + 39);
    _t3_11 = _mm256_broadcast_sd(K + 36*fi390 + 72);
    _t3_10 = _mm256_broadcast_sd(K + 36*fi390 + 73);
    _t3_9 = _mm256_broadcast_sd(K + 36*fi390 + 74);
    _t3_8 = _mm256_broadcast_sd(K + 36*fi390 + 75);
    _t3_7 = _mm256_broadcast_sd(K + 36*fi390 + 108);
    _t3_6 = _mm256_broadcast_sd(K + 36*fi390 + 109);
    _t3_5 = _mm256_broadcast_sd(K + 36*fi390 + 110);
    _t3_4 = _mm256_broadcast_sd(K + 36*fi390 + 111);
    _t3_3 = _mm256_loadu_pd(K + 36*fi390);
    _t3_2 = _mm256_loadu_pd(K + 36*fi390 + 36);
    _t3_1 = _mm256_loadu_pd(K + 36*fi390 + 72);
    _t3_0 = _mm256_loadu_pd(K + 36*fi390 + 108);

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_55 = _t3_27;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_56 = _t3_26;

    // 4-BLAC: 1x4 / 1x4
    _t3_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_55), _mm256_castpd256_pd128(_t3_56)));

    // AVX Storer:
    _t3_27 = _t3_57;

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_58 = _t3_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_27, _t3_27, 32), _mm256_permute2f128_pd(_t3_27, _t3_27, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_60 = _t3_25;

    // 4-BLAC: (4x1)^T
    _t0_627 = _t3_60;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_628 = _mm256_mul_pd(_t3_59, _t0_627);

    // 4-BLAC: 1x4 - 1x4
    _t3_61 = _mm256_sub_pd(_t3_58, _t0_628);

    // AVX Storer:
    _t3_28 = _t3_61;

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_28, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_63 = _t3_24;

    // 4-BLAC: 1x4 / 1x4
    _t3_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_62), _mm256_castpd256_pd128(_t3_63)));

    // AVX Storer:
    _t3_29 = _t3_64;

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_65 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_28, 6), _mm256_permute2f128_pd(_t3_28, _t3_28, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_66 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_29, _t3_29, 32), _mm256_permute2f128_pd(_t3_29, _t3_29, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_67 = _t3_23;

    // 4-BLAC: (4x1)^T
    _t0_637 = _t3_67;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_638 = _mm256_mul_pd(_t3_66, _t0_637);

    // 4-BLAC: 1x4 - 1x4
    _t3_68 = _mm256_sub_pd(_t3_65, _t0_638);

    // AVX Storer:
    _t3_30 = _t3_68;

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_69 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_30, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_70 = _t3_22;

    // 4-BLAC: 1x4 / 1x4
    _t3_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_69), _mm256_castpd256_pd128(_t3_70)));

    // AVX Storer:
    _t3_31 = _t3_71;

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_72 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_30, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_73 = _t3_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_74 = _t3_21;

    // 4-BLAC: (4x1)^T
    _t0_647 = _t3_74;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_648 = _mm256_mul_pd(_t3_73, _t0_647);

    // 4-BLAC: 1x4 - 1x4
    _t3_75 = _mm256_sub_pd(_t3_72, _t0_648);

    // AVX Storer:
    _t3_32 = _t3_75;

    // Generating : L[36,36] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_76 = _t3_32;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_77 = _t3_20;

    // 4-BLAC: 1x4 / 1x4
    _t3_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_76), _mm256_castpd256_pd128(_t3_77)));

    // AVX Storer:
    _t3_32 = _t3_78;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_79 = _t3_33;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_80 = _t3_26;

    // 4-BLAC: 1x4 / 1x4
    _t3_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_79), _mm256_castpd256_pd128(_t3_80)));

    // AVX Storer:
    _t3_33 = _t3_81;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_82 = _t3_34;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_33, _t3_33, 32), _mm256_permute2f128_pd(_t3_33, _t3_33, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_84 = _t3_25;

    // 4-BLAC: (4x1)^T
    _t0_660 = _t3_84;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_661 = _mm256_mul_pd(_t3_83, _t0_660);

    // 4-BLAC: 1x4 - 1x4
    _t3_85 = _mm256_sub_pd(_t3_82, _t0_661);

    // AVX Storer:
    _t3_34 = _t3_85;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_34, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_87 = _t3_24;

    // 4-BLAC: 1x4 / 1x4
    _t3_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_86), _mm256_castpd256_pd128(_t3_87)));

    // AVX Storer:
    _t3_35 = _t3_88;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_89 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_34, 6), _mm256_permute2f128_pd(_t3_34, _t3_34, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_90 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_35, _t3_35, 32), _mm256_permute2f128_pd(_t3_35, _t3_35, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_91 = _t3_23;

    // 4-BLAC: (4x1)^T
    _t0_669 = _t3_91;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_670 = _mm256_mul_pd(_t3_90, _t0_669);

    // 4-BLAC: 1x4 - 1x4
    _t3_92 = _mm256_sub_pd(_t3_89, _t0_670);

    // AVX Storer:
    _t3_36 = _t3_92;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_36, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_94 = _t3_22;

    // 4-BLAC: 1x4 / 1x4
    _t3_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_93), _mm256_castpd256_pd128(_t3_94)));

    // AVX Storer:
    _t3_37 = _t3_95;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_96 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_36, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_97 = _t3_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_98 = _t3_21;

    // 4-BLAC: (4x1)^T
    _t0_679 = _t3_98;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_680 = _mm256_mul_pd(_t3_97, _t0_679);

    // 4-BLAC: 1x4 - 1x4
    _t3_99 = _mm256_sub_pd(_t3_96, _t0_680);

    // AVX Storer:
    _t3_38 = _t3_99;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_100 = _t3_38;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_101 = _t3_20;

    // 4-BLAC: 1x4 / 1x4
    _t3_102 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_100), _mm256_castpd256_pd128(_t3_101)));

    // AVX Storer:
    _t3_38 = _t3_102;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_103 = _t3_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_104 = _t3_26;

    // 4-BLAC: 1x4 / 1x4
    _t3_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_103), _mm256_castpd256_pd128(_t3_104)));

    // AVX Storer:
    _t3_39 = _t3_105;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_106 = _t3_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_39, _t3_39, 32), _mm256_permute2f128_pd(_t3_39, _t3_39, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_108 = _t3_25;

    // 4-BLAC: (4x1)^T
    _t0_692 = _t3_108;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_693 = _mm256_mul_pd(_t3_107, _t0_692);

    // 4-BLAC: 1x4 - 1x4
    _t3_109 = _mm256_sub_pd(_t3_106, _t0_693);

    // AVX Storer:
    _t3_40 = _t3_109;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_40, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_111 = _t3_24;

    // 4-BLAC: 1x4 / 1x4
    _t3_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_110), _mm256_castpd256_pd128(_t3_111)));

    // AVX Storer:
    _t3_41 = _t3_112;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_113 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_40, 6), _mm256_permute2f128_pd(_t3_40, _t3_40, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_114 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_41, _t3_41, 32), _mm256_permute2f128_pd(_t3_41, _t3_41, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_115 = _t3_23;

    // 4-BLAC: (4x1)^T
    _t0_144 = _t3_115;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_146 = _mm256_mul_pd(_t3_114, _t0_144);

    // 4-BLAC: 1x4 - 1x4
    _t3_116 = _mm256_sub_pd(_t3_113, _t0_146);

    // AVX Storer:
    _t3_42 = _t3_116;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_117 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_42, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_118 = _t3_22;

    // 4-BLAC: 1x4 / 1x4
    _t3_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_117), _mm256_castpd256_pd128(_t3_118)));

    // AVX Storer:
    _t3_43 = _t3_119;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_120 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_42, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_121 = _t3_43;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_122 = _t3_21;

    // 4-BLAC: (4x1)^T
    _t0_154 = _t3_122;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_155 = _mm256_mul_pd(_t3_121, _t0_154);

    // 4-BLAC: 1x4 - 1x4
    _t3_123 = _mm256_sub_pd(_t3_120, _t0_155);

    // AVX Storer:
    _t3_44 = _t3_123;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_124 = _t3_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_125 = _t3_20;

    // 4-BLAC: 1x4 / 1x4
    _t3_126 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_124), _mm256_castpd256_pd128(_t3_125)));

    // AVX Storer:
    _t3_44 = _t3_126;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) Div G(h(1, 36, Max(0, fi390 - 4)), L[36,36],h(1, 36, Max(0, fi390 - 4))) ),h(1, 36, Max(0, fi390 - 4)))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_127 = _t3_45;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_128 = _t3_26;

    // 4-BLAC: 1x4 / 1x4
    _t3_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_127), _mm256_castpd256_pd128(_t3_128)));

    // AVX Storer:
    _t3_45 = _t3_129;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, Max(0, fi390 - 4) + 1)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4))) Kro T( G(h(3, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4))) ) ) ),h(3, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_130 = _t3_46;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_131 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_45, _t3_45, 32), _mm256_permute2f128_pd(_t3_45, _t3_45, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_132 = _t3_25;

    // 4-BLAC: (4x1)^T
    _t0_167 = _t3_132;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_168 = _mm256_mul_pd(_t3_131, _t0_167);

    // 4-BLAC: 1x4 - 1x4
    _t3_133 = _mm256_sub_pd(_t3_130, _t0_168);

    // AVX Storer:
    _t3_46 = _t3_133;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Div G(h(1, 36, Max(0, fi390 - 4) + 1), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ),h(1, 36, Max(0, fi390 - 4) + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_134 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_46, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_135 = _t3_24;

    // 4-BLAC: 1x4 / 1x4
    _t3_136 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_134), _mm256_castpd256_pd128(_t3_135)));

    // AVX Storer:
    _t3_47 = _t3_136;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, Max(0, fi390 - 4) + 2)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) Kro T( G(h(2, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 1)) ) ) ),h(2, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_137 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_46, 6), _mm256_permute2f128_pd(_t3_46, _t3_46, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_138 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_47, _t3_47, 32), _mm256_permute2f128_pd(_t3_47, _t3_47, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_139 = _t3_23;

    // 4-BLAC: (4x1)^T
    _t0_177 = _t3_139;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_178 = _mm256_mul_pd(_t3_138, _t0_177);

    // 4-BLAC: 1x4 - 1x4
    _t3_140 = _mm256_sub_pd(_t3_137, _t0_178);

    // AVX Storer:
    _t3_48 = _t3_140;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Div G(h(1, 36, Max(0, fi390 - 4) + 2), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ),h(1, 36, Max(0, fi390 - 4) + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_141 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_48, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_142 = _t3_22;

    // 4-BLAC: 1x4 / 1x4
    _t3_143 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_141), _mm256_castpd256_pd128(_t3_142)));

    // AVX Storer:
    _t3_49 = _t3_143;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) Kro T( G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 2)) ) ) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_144 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_48, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_145 = _t3_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_146 = _t3_21;

    // 4-BLAC: (4x1)^T
    _t0_188 = _t3_146;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_189 = _mm256_mul_pd(_t3_145, _t0_188);

    // 4-BLAC: 1x4 - 1x4
    _t3_147 = _mm256_sub_pd(_t3_144, _t0_189);

    // AVX Storer:
    _t3_50 = _t3_147;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) Div G(h(1, 36, Max(0, fi390 - 4) + 3), L[36,36],h(1, 36, Max(0, fi390 - 4) + 3)) ),h(1, 36, Max(0, fi390 - 4) + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_148 = _t3_50;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_149 = _t3_20;

    // 4-BLAC: 1x4 / 1x4
    _t3_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_148), _mm256_castpd256_pd128(_t3_149)));

    // AVX Storer:
    _t3_50 = _t3_150;

    // Generating : L[36,36] = ( S(h(4, 36, fi390), ( G(h(4, 36, fi390), K[36,36],h(4, 36, fi390)) - ( G(h(4, 36, fi390), L[36,36],h(4, 36, 0)) * T( G(h(4, 36, fi390), L[36,36],h(4, 36, 0)) ) ) ),h(4, 36, fi390)) + Sum_{j167} ( -$(h(4, 36, fi390), ( G(h(4, 36, fi390), L[36,36],h(4, 36, j167)) * T( G(h(4, 36, fi390), L[36,36],h(4, 36, j167)) ) ),h(4, 36, fi390)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t3_151 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_51, _t3_52, 0), _mm256_shuffle_pd(_t3_53, _t3_54, 0), 32);
    _t3_152 = _mm256_permute2f128_pd(_t3_52, _mm256_shuffle_pd(_t3_53, _t3_54, 3), 32);
    _t3_153 = _mm256_blend_pd(_t3_53, _mm256_shuffle_pd(_t3_53, _t3_54, 3), 12);
    _t3_154 = _t3_54;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t0_701 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32);
    _t0_702 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t3_3, _t3_2), _mm256_unpackhi_pd(_t3_1, _t3_0), 32);
    _t0_703 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 49);
    _t0_704 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t3_3, _t3_2), _mm256_unpackhi_pd(_t3_1, _t3_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t0_115 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_19, _t0_701), _mm256_mul_pd(_t3_18, _t0_702)), _mm256_add_pd(_mm256_mul_pd(_t3_17, _t0_703), _mm256_mul_pd(_t3_16, _t0_704)));
    _t0_116 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t0_701), _mm256_mul_pd(_t3_14, _t0_702)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t0_703), _mm256_mul_pd(_t3_12, _t0_704)));
    _t0_117 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t0_701), _mm256_mul_pd(_t3_10, _t0_702)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t0_703), _mm256_mul_pd(_t3_8, _t0_704)));
    _t0_118 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t0_701), _mm256_mul_pd(_t3_6, _t0_702)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t0_703), _mm256_mul_pd(_t3_4, _t0_704)));

    // 4-BLAC: 4x4 - 4x4
    _t0_131 = _mm256_sub_pd(_t3_151, _t0_115);
    _t0_132 = _mm256_sub_pd(_t3_152, _t0_116);
    _t0_133 = _mm256_sub_pd(_t3_153, _t0_117);
    _t0_134 = _mm256_sub_pd(_t3_154, _t0_118);

    // AVX Storer:

    // 4x4 -> 4x4 - LowTriang
    _t3_51 = _t0_131;
    _t3_52 = _t0_132;
    _t3_53 = _t0_133;
    _t3_54 = _t0_134;
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4)]), _mm256_castpd256_pd128(_t3_27));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 1]), _mm256_castpd256_pd128(_t3_29));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 2]), _mm256_castpd256_pd128(_t3_31));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 3]), _mm256_castpd256_pd128(_t3_32));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 36]), _mm256_castpd256_pd128(_t3_33));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 37]), _mm256_castpd256_pd128(_t3_35));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 38]), _mm256_castpd256_pd128(_t3_37));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 39]), _mm256_castpd256_pd128(_t3_38));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 72]), _mm256_castpd256_pd128(_t3_39));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 73]), _mm256_castpd256_pd128(_t3_41));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 74]), _mm256_castpd256_pd128(_t3_43));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 75]), _mm256_castpd256_pd128(_t3_44));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 108]), _mm256_castpd256_pd128(_t3_45));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 109]), _mm256_castpd256_pd128(_t3_47));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 110]), _mm256_castpd256_pd128(_t3_49));
    _mm_store_sd(&(K[36*fi390 + Max(0, fi390 - 4) + 111]), _mm256_castpd256_pd128(_t3_50));

    for( int j167 = 4; j167 <= fi390 - 1; j167+=4 ) {
      _t4_19 = _mm256_broadcast_sd(K + 36*fi390 + j167);
      _t4_18 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 1);
      _t4_17 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 2);
      _t4_16 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 3);
      _t4_15 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 36);
      _t4_14 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 37);
      _t4_13 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 38);
      _t4_12 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 39);
      _t4_11 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 72);
      _t4_10 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 73);
      _t4_9 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 74);
      _t4_8 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 75);
      _t4_7 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 108);
      _t4_6 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 109);
      _t4_5 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 110);
      _t4_4 = _mm256_broadcast_sd(K + 36*fi390 + j167 + 111);
      _t4_3 = _mm256_loadu_pd(K + 36*fi390 + j167);
      _t4_2 = _mm256_loadu_pd(K + 36*fi390 + j167 + 36);
      _t4_1 = _mm256_loadu_pd(K + 36*fi390 + j167 + 72);
      _t4_0 = _mm256_loadu_pd(K + 36*fi390 + j167 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t0_705 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 32);
      _t0_706 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 32);
      _t0_707 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 49);
      _t0_708 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t0_119 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_19, _t0_705), _mm256_mul_pd(_t4_18, _t0_706)), _mm256_add_pd(_mm256_mul_pd(_t4_17, _t0_707), _mm256_mul_pd(_t4_16, _t0_708)));
      _t0_120 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t0_705), _mm256_mul_pd(_t4_14, _t0_706)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t0_707), _mm256_mul_pd(_t4_12, _t0_708)));
      _t0_121 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t0_705), _mm256_mul_pd(_t4_10, _t0_706)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t0_707), _mm256_mul_pd(_t4_8, _t0_708)));
      _t0_122 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t0_705), _mm256_mul_pd(_t4_6, _t0_706)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t0_707), _mm256_mul_pd(_t4_4, _t0_708)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowTriang
      _t4_20 = _t3_51;
      _t4_21 = _t3_52;
      _t4_22 = _t3_53;
      _t4_23 = _t3_54;

      // 4-BLAC: 4x4 - 4x4
      _t4_20 = _mm256_sub_pd(_t4_20, _t0_119);
      _t4_21 = _mm256_sub_pd(_t4_21, _t0_120);
      _t4_22 = _mm256_sub_pd(_t4_22, _t0_121);
      _t4_23 = _mm256_sub_pd(_t4_23, _t0_122);

      // AVX Storer:

      // 4x4 -> 4x4 - LowTriang
      _t3_51 = _t4_20;
      _t3_52 = _t4_21;
      _t3_53 = _t4_22;
      _t3_54 = _t4_23;
    }

    // Generating : L[36,36] = S(h(1, 36, fi390), Sqrt( G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_10 = _t3_51;

    // 4-BLAC: sqrt(1x4)
    _t5_11 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_10)));

    // AVX Storer:
    _t3_51 = _t5_11;

    // Generating : T1473[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t5_12 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_13 = _t3_51;

    // 4-BLAC: 1x4 / 1x4
    _t5_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_12), _mm256_castpd256_pd128(_t5_13)));

    // AVX Storer:
    _t5_0 = _t5_14;

    // Generating : L[36,36] = S(h(2, 36, fi390 + 1), ( G(h(1, 1, 0), T1473[1,36],h(1, 36, fi390)) Kro G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_15 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_0, _t5_0, 32), _mm256_permute2f128_pd(_t5_0, _t5_0, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_16 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_52, _t3_53), _mm256_setzero_pd(), 12);

    // 4-BLAC: 1x4 Kro 4x1
    _t5_17 = _mm256_mul_pd(_t5_15, _t5_16);

    // AVX Storer:
    _t5_1 = _t5_17;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) - ( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) Kro T( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ),h(1, 36, fi390 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_18 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_52, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_19 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_20 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 1);

    // 4-BLAC: (4x1)^T
    _t0_214 = _t5_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_216 = _mm256_mul_pd(_t5_19, _t0_214);

    // 4-BLAC: 1x4 - 1x4
    _t5_21 = _mm256_sub_pd(_t5_18, _t0_216);

    // AVX Storer:
    _t5_2 = _t5_21;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 1), Sqrt( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ),h(1, 36, fi390 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_22 = _t5_2;

    // 4-BLAC: sqrt(1x4)
    _t5_23 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_22)));

    // AVX Storer:
    _t5_2 = _t5_23;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390)) Kro T( G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ),h(1, 36, fi390 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_24 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_53, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_25 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 1);

    // 4-BLAC: (4x1)^T
    _t0_223 = _t5_26;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_224 = _mm256_mul_pd(_t5_25, _t0_223);

    // 4-BLAC: 1x4 - 1x4
    _t5_27 = _mm256_sub_pd(_t5_24, _t0_224);

    // AVX Storer:
    _t5_3 = _t5_27;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) Div G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ),h(1, 36, fi390 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_28 = _t5_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_29 = _t5_2;

    // 4-BLAC: 1x4 / 1x4
    _t5_30 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_28), _mm256_castpd256_pd128(_t5_29)));

    // AVX Storer:
    _t5_3 = _t5_30;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) - ( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi390)) * T( G(h(1, 36, fi390 + 2), L[36,36],h(2, 36, fi390)) ) ) ),h(1, 36, fi390 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_31 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_53, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t3_53, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_32 = _mm256_shuffle_pd(_mm256_blend_pd(_t5_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t5_3, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_33 = _mm256_shuffle_pd(_mm256_blend_pd(_t5_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t5_3, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (1x4)^T
    _t0_233 = _t5_33;

    // 4-BLAC: 1x4 * 4x1
    _t0_234 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_32, _t0_233), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_32, _t0_233), _mm256_mul_pd(_t5_32, _t0_233), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_32, _t0_233), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_32, _t0_233), _mm256_mul_pd(_t5_32, _t0_233), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_32, _t0_233), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_32, _t0_233), _mm256_mul_pd(_t5_32, _t0_233), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t5_34 = _mm256_sub_pd(_t5_31, _t0_234);

    // AVX Storer:
    _t5_4 = _t5_34;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 2), Sqrt( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ),h(1, 36, fi390 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_35 = _t5_4;

    // 4-BLAC: sqrt(1x4)
    _t5_36 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_35)));

    // AVX Storer:
    _t5_4 = _t5_36;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) Div G(h(1, 36, fi390), L[36,36],h(1, 36, fi390)) ),h(1, 36, fi390))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_37 = _mm256_blend_pd(_mm256_setzero_pd(), _t3_54, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_38 = _t3_51;

    // 4-BLAC: 1x4 / 1x4
    _t5_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_37), _mm256_castpd256_pd128(_t5_38)));

    // AVX Storer:
    _t5_5 = _t5_39;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(2, 36, fi390 + 1)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390)) Kro T( G(h(2, 36, fi390 + 1), L[36,36],h(1, 36, fi390)) ) ) ),h(2, 36, fi390 + 1))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_40 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t3_54, 6), _mm256_permute2f128_pd(_t3_54, _t3_54, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_41 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_5, _t5_5, 32), _mm256_permute2f128_pd(_t5_5, _t5_5, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_42 = _t5_1;

    // 4-BLAC: (4x1)^T
    _t0_245 = _t5_42;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_246 = _mm256_mul_pd(_t5_41, _t0_245);

    // 4-BLAC: 1x4 - 1x4
    _t5_43 = _mm256_sub_pd(_t5_40, _t0_246);

    // AVX Storer:
    _t5_6 = _t5_43;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) Div G(h(1, 36, fi390 + 1), L[36,36],h(1, 36, fi390 + 1)) ),h(1, 36, fi390 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_44 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_45 = _t5_2;

    // 4-BLAC: 1x4 / 1x4
    _t5_46 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_44), _mm256_castpd256_pd128(_t5_45)));

    // AVX Storer:
    _t5_7 = _t5_46;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 1)) Kro T( G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 1)) ) ) ),h(1, 36, fi390 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_47 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_6, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_48 = _t5_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_49 = _t5_3;

    // 4-BLAC: (4x1)^T
    _t0_255 = _t5_49;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_256 = _mm256_mul_pd(_t5_48, _t0_255);

    // 4-BLAC: 1x4 - 1x4
    _t5_50 = _mm256_sub_pd(_t5_47, _t0_256);

    // AVX Storer:
    _t5_8 = _t5_50;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 2)) Div G(h(1, 36, fi390 + 2), L[36,36],h(1, 36, fi390 + 2)) ),h(1, 36, fi390 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_51 = _t5_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_52 = _t5_4;

    // 4-BLAC: 1x4 / 1x4
    _t5_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_51), _mm256_castpd256_pd128(_t5_52)));

    // AVX Storer:
    _t5_8 = _t5_53;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) - ( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi390)) * T( G(h(1, 36, fi390 + 3), L[36,36],h(3, 36, fi390)) ) ) ),h(1, 36, fi390 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_54 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t3_54, _t3_54, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_55 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_5, _t5_7), _mm256_unpacklo_pd(_t5_8, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_56 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_5, _t5_7), _mm256_unpacklo_pd(_t5_8, _mm256_setzero_pd()), 32);

    // 4-BLAC: (1x4)^T
    _t0_265 = _t5_56;

    // 4-BLAC: 1x4 * 4x1
    _t0_266 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_55, _t0_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_55, _t0_265), _mm256_mul_pd(_t5_55, _t0_265), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_55, _t0_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_55, _t0_265), _mm256_mul_pd(_t5_55, _t0_265), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_55, _t0_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_55, _t0_265), _mm256_mul_pd(_t5_55, _t0_265), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t5_57 = _mm256_sub_pd(_t5_54, _t0_266);

    // AVX Storer:
    _t5_9 = _t5_57;

    // Generating : L[36,36] = S(h(1, 36, fi390 + 3), Sqrt( G(h(1, 36, fi390 + 3), L[36,36],h(1, 36, fi390 + 3)) ),h(1, 36, fi390 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_58 = _t5_9;

    // 4-BLAC: sqrt(1x4)
    _t5_59 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_58)));

    // AVX Storer:
    _t5_9 = _t5_59;
    _mm_store_sd(K + 37*fi390, _mm256_castpd256_pd128(_t3_51));
    _mm256_maskstore_pd(K + 37*fi390 + 36, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_1);
    _mm256_maskstore_pd(K + 37*fi390 + 72, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t5_1, _t5_1, 1));
    _mm_store_sd(&(K[37*fi390 + 37]), _mm256_castpd256_pd128(_t5_2));
    _mm_store_sd(&(K[37*fi390 + 73]), _mm256_castpd256_pd128(_t5_3));
    _mm_store_sd(&(K[37*fi390 + 74]), _mm256_castpd256_pd128(_t5_4));
    _mm_store_sd(&(K[37*fi390 + 108]), _mm256_castpd256_pd128(_t5_5));
    _mm_store_sd(&(K[37*fi390 + 109]), _mm256_castpd256_pd128(_t5_7));
    _mm_store_sd(&(K[37*fi390 + 110]), _mm256_castpd256_pd128(_t5_8));
    _mm_store_sd(&(K[37*fi390 + 111]), _mm256_castpd256_pd128(_t5_9));
  }


  for( int fi390 = 0; fi390 <= 31; fi390+=4 ) {
    _t6_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi390])));
    _t6_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390])));
    _t6_8 = _mm256_maskload_pd(y + fi390 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t6_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 108)), 32);
    _t6_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 37])));
    _t6_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 109)), 0);
    _t6_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 74])));
    _t6_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 110])));
    _t6_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 111])));

    // Generating : t0[36,1] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390), L0[36,36],h(1, 36, fi390)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_13 = _t6_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_14 = _t6_6;

    // 4-BLAC: 1x4 / 1x4
    _t6_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_13), _mm256_castpd256_pd128(_t6_14)));

    // AVX Storer:
    _t6_7 = _t6_15;

    // Generating : t0[36,1] = S(h(3, 36, fi390 + 1), ( G(h(3, 36, fi390 + 1), t0[36,1],h(1, 1, 0)) - ( G(h(3, 36, fi390 + 1), L0[36,36],h(1, 36, fi390)) Kro G(h(1, 36, fi390), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t6_16 = _t6_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t6_17 = _t6_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_7, _t6_7, 32), _mm256_permute2f128_pd(_t6_7, _t6_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t6_19 = _mm256_mul_pd(_t6_17, _t6_18);

    // 4-BLAC: 4x1 - 4x1
    _t6_20 = _mm256_sub_pd(_t6_16, _t6_19);

    // AVX Storer:
    _t6_8 = _t6_20;

    // Generating : t0[36,1] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390 + 1), L0[36,36],h(1, 36, fi390 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_22 = _t6_4;

    // 4-BLAC: 1x4 / 1x4
    _t6_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_21), _mm256_castpd256_pd128(_t6_22)));

    // AVX Storer:
    _t6_9 = _t6_23;

    // Generating : t0[36,1] = S(h(2, 36, fi390 + 2), ( G(h(2, 36, fi390 + 2), t0[36,1],h(1, 1, 0)) - ( G(h(2, 36, fi390 + 2), L0[36,36],h(1, 36, fi390 + 1)) Kro G(h(1, 36, fi390 + 1), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t6_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_8, 6), _mm256_permute2f128_pd(_t6_8, _t6_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t6_25 = _t6_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_9, _t6_9, 32), _mm256_permute2f128_pd(_t6_9, _t6_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t6_27 = _mm256_mul_pd(_t6_25, _t6_26);

    // 4-BLAC: 4x1 - 4x1
    _t6_28 = _mm256_sub_pd(_t6_24, _t6_27);

    // AVX Storer:
    _t6_10 = _t6_28;

    // Generating : t0[36,1] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390 + 2), L0[36,36],h(1, 36, fi390 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_30 = _t6_2;

    // 4-BLAC: 1x4 / 1x4
    _t6_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_29), _mm256_castpd256_pd128(_t6_30)));

    // AVX Storer:
    _t6_11 = _t6_31;

    // Generating : t0[36,1] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), t0[36,1],h(1, 1, 0)) - ( G(h(1, 36, fi390 + 3), L0[36,36],h(1, 36, fi390 + 2)) Kro G(h(1, 36, fi390 + 2), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_33 = _t6_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_34 = _t6_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t6_35 = _mm256_mul_pd(_t6_33, _t6_34);

    // 4-BLAC: 1x4 - 1x4
    _t6_36 = _mm256_sub_pd(_t6_32, _t6_35);

    // AVX Storer:
    _t6_12 = _t6_36;

    // Generating : t0[36,1] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390 + 3), L0[36,36],h(1, 36, fi390 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_37 = _t6_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_38 = _t6_0;

    // 4-BLAC: 1x4 / 1x4
    _t6_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_37), _mm256_castpd256_pd128(_t6_38)));

    // AVX Storer:
    _t6_12 = _t6_39;

    // Generating : t0[36,1] = Sum_{j167} ( S(h(4, 36, fi390 + j167 + 4), ( G(h(4, 36, fi390 + j167 + 4), t0[36,1],h(1, 1, 0)) - ( G(h(4, 36, fi390 + j167 + 4), L0[36,36],h(4, 36, fi390)) * G(h(4, 36, fi390), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[fi390]), _mm256_castpd256_pd128(_t6_7));
    _mm_store_sd(&(y[fi390 + 1]), _mm256_castpd256_pd128(_t6_9));
    _mm_store_sd(&(y[fi390 + 2]), _mm256_castpd256_pd128(_t6_11));
    _mm_store_sd(&(y[fi390 + 3]), _mm256_castpd256_pd128(_t6_12));

    for( int j167 = 0; j167 <= -fi390 + 31; j167+=4 ) {
      _t7_9 = _mm256_loadu_pd(y + fi390 + j167 + 4);
      _t7_7 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 144);
      _t7_6 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 180);
      _t7_5 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 216);
      _t7_4 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 252);
      _t7_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi390])));
      _t7_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi390 + 1])));
      _t7_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi390 + 2])));
      _t7_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi390 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t7_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t7_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t7_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t7_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t7_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t7_9 = _mm256_sub_pd(_t7_9, _t7_8);

      // AVX Storer:
      _mm256_storeu_pd(y + fi390 + j167 + 4, _t7_9);
    }
  }

  _t8_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[32])));
  _t8_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1184])));
  _t8_8 = _mm256_maskload_pd(y + 33, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t8_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1220)), _mm256_castpd128_pd256(_mm_load_sd(K + 1256))), _mm256_castpd128_pd256(_mm_load_sd(K + 1292)), 32);
  _t8_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1221])));
  _t8_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1257)), _mm256_castpd128_pd256(_mm_load_sd(K + 1293)), 0);
  _t8_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1258])));
  _t8_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1294])));
  _t8_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1295])));

  // Generating : t0[36,1] = S(h(1, 36, 32), ( G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_13 = _t8_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_14 = _t8_6;

  // 4-BLAC: 1x4 / 1x4
  _t8_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_13), _mm256_castpd256_pd128(_t8_14)));

  // AVX Storer:
  _t8_7 = _t8_15;

  // Generating : t0[36,1] = S(h(3, 36, 33), ( G(h(3, 36, 33), t0[36,1],h(1, 1, 0)) - ( G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) Kro G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t8_16 = _t8_8;

  // AVX Loader:

  // 3x1 -> 4x1
  _t8_17 = _t8_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_7, _t8_7, 32), _mm256_permute2f128_pd(_t8_7, _t8_7, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t8_19 = _mm256_mul_pd(_t8_17, _t8_18);

  // 4-BLAC: 4x1 - 4x1
  _t8_20 = _mm256_sub_pd(_t8_16, _t8_19);

  // AVX Storer:
  _t8_8 = _t8_20;

  // Generating : t0[36,1] = S(h(1, 36, 33), ( G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_8, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_22 = _t8_4;

  // 4-BLAC: 1x4 / 1x4
  _t8_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_21), _mm256_castpd256_pd128(_t8_22)));

  // AVX Storer:
  _t8_9 = _t8_23;

  // Generating : t0[36,1] = S(h(2, 36, 34), ( G(h(2, 36, 34), t0[36,1],h(1, 1, 0)) - ( G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) Kro G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t8_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_8, 6), _mm256_permute2f128_pd(_t8_8, _t8_8, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t8_25 = _t8_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_9, _t8_9, 32), _mm256_permute2f128_pd(_t8_9, _t8_9, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t8_27 = _mm256_mul_pd(_t8_25, _t8_26);

  // 4-BLAC: 4x1 - 4x1
  _t8_28 = _mm256_sub_pd(_t8_24, _t8_27);

  // AVX Storer:
  _t8_10 = _t8_28;

  // Generating : t0[36,1] = S(h(1, 36, 34), ( G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_30 = _t8_2;

  // 4-BLAC: 1x4 / 1x4
  _t8_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_29), _mm256_castpd256_pd128(_t8_30)));

  // AVX Storer:
  _t8_11 = _t8_31;

  // Generating : t0[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) - ( G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) Kro G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_33 = _t8_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_34 = _t8_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_35 = _mm256_mul_pd(_t8_33, _t8_34);

  // 4-BLAC: 1x4 - 1x4
  _t8_36 = _mm256_sub_pd(_t8_32, _t8_35);

  // AVX Storer:
  _t8_12 = _t8_36;

  // Generating : t0[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_37 = _t8_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_38 = _t8_0;

  // 4-BLAC: 1x4 / 1x4
  _t8_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_37), _mm256_castpd256_pd128(_t8_38)));

  // AVX Storer:
  _t8_12 = _t8_39;

  _mm_store_sd(&(y[32]), _mm256_castpd256_pd128(_t8_7));
  _mm_store_sd(&(y[33]), _mm256_castpd256_pd128(_t8_9));
  _mm_store_sd(&(y[34]), _mm256_castpd256_pd128(_t8_11));
  _mm_store_sd(&(y[35]), _mm256_castpd256_pd128(_t8_12));

  for( int fi390 = 0; fi390 <= 31; fi390+=4 ) {
    _t9_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi390 + 35])));
    _t9_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi390 + 1295])));
    _t9_8 = _mm256_maskload_pd(y + -fi390 + 32, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_5 = _mm256_maskload_pd(K + -37*fi390 + 1292, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi390 + 1258])));
    _t9_3 = _mm256_maskload_pd(K + -37*fi390 + 1256, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t9_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi390 + 1221])));
    _t9_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi390 + 1220])));
    _t9_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi390 + 1184])));

    // Generating : a[36,1] = S(h(1, 36, -fi390 + 35), ( G(h(1, 36, -fi390 + 35), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi390 + 35), L0[36,36],h(1, 36, -fi390 + 35)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_13 = _t9_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_14 = _t9_6;

    // 4-BLAC: 1x4 / 1x4
    _t9_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_13), _mm256_castpd256_pd128(_t9_14)));

    // AVX Storer:
    _t9_7 = _t9_15;

    // Generating : a[36,1] = S(h(3, 36, -fi390 + 32), ( G(h(3, 36, -fi390 + 32), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, -fi390 + 35), L0[36,36],h(3, 36, -fi390 + 32)) ) Kro G(h(1, 36, -fi390 + 35), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t9_16 = _t9_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_17 = _t9_5;

    // 4-BLAC: (1x4)^T
    _t9_18 = _t9_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_7, _t9_7, 32), _mm256_permute2f128_pd(_t9_7, _t9_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t9_20 = _mm256_mul_pd(_t9_18, _t9_19);

    // 4-BLAC: 4x1 - 4x1
    _t9_21 = _mm256_sub_pd(_t9_16, _t9_20);

    // AVX Storer:
    _t9_8 = _t9_21;

    // Generating : a[36,1] = S(h(1, 36, -fi390 + 34), ( G(h(1, 36, -fi390 + 34), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi390 + 34), L0[36,36],h(1, 36, -fi390 + 34)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_22 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t9_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_23 = _t9_4;

    // 4-BLAC: 1x4 / 1x4
    _t9_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_22), _mm256_castpd256_pd128(_t9_23)));

    // AVX Storer:
    _t9_9 = _t9_24;

    // Generating : a[36,1] = S(h(2, 36, -fi390 + 32), ( G(h(2, 36, -fi390 + 32), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, -fi390 + 34), L0[36,36],h(2, 36, -fi390 + 32)) ) Kro G(h(1, 36, -fi390 + 34), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t9_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_8, 3);

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_26 = _t9_3;

    // 4-BLAC: (1x4)^T
    _t9_27 = _t9_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_9, _t9_9, 32), _mm256_permute2f128_pd(_t9_9, _t9_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t9_29 = _mm256_mul_pd(_t9_27, _t9_28);

    // 4-BLAC: 4x1 - 4x1
    _t9_30 = _mm256_sub_pd(_t9_25, _t9_29);

    // AVX Storer:
    _t9_10 = _t9_30;

    // Generating : a[36,1] = S(h(1, 36, -fi390 + 33), ( G(h(1, 36, -fi390 + 33), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi390 + 33), L0[36,36],h(1, 36, -fi390 + 33)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_31 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_32 = _t9_2;

    // 4-BLAC: 1x4 / 1x4
    _t9_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_31), _mm256_castpd256_pd128(_t9_32)));

    // AVX Storer:
    _t9_11 = _t9_33;

    // Generating : a[36,1] = S(h(1, 36, -fi390 + 32), ( G(h(1, 36, -fi390 + 32), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, -fi390 + 33), L0[36,36],h(1, 36, -fi390 + 32)) ) Kro G(h(1, 36, -fi390 + 33), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_35 = _t9_1;

    // 4-BLAC: (4x1)^T
    _t9_36 = _t9_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_37 = _t9_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_38 = _mm256_mul_pd(_t9_36, _t9_37);

    // 4-BLAC: 1x4 - 1x4
    _t9_39 = _mm256_sub_pd(_t9_34, _t9_38);

    // AVX Storer:
    _t9_12 = _t9_39;

    // Generating : a[36,1] = S(h(1, 36, -fi390 + 32), ( G(h(1, 36, -fi390 + 32), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi390 + 32), L0[36,36],h(1, 36, -fi390 + 32)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_40 = _t9_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_41 = _t9_0;

    // 4-BLAC: 1x4 / 1x4
    _t9_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_40), _mm256_castpd256_pd128(_t9_41)));

    // AVX Storer:
    _t9_12 = _t9_42;

    // Generating : a[36,1] = Sum_{j167} ( S(h(4, 36, j167), ( G(h(4, 36, j167), a[36,1],h(1, 1, 0)) - ( T( G(h(4, 36, -fi390 + 32), L0[36,36],h(4, 36, j167)) ) * G(h(4, 36, -fi390 + 32), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[-fi390 + 35]), _mm256_castpd256_pd128(_t9_7));
    _mm_store_sd(&(y[-fi390 + 34]), _mm256_castpd256_pd128(_t9_9));
    _mm_store_sd(&(y[-fi390 + 33]), _mm256_castpd256_pd128(_t9_11));
    _mm_store_sd(&(y[-fi390 + 32]), _mm256_castpd256_pd128(_t9_12));

    for( int j167 = 0; j167 <= -fi390 + 31; j167+=4 ) {
      _t10_9 = _mm256_loadu_pd(y + j167);
      _t10_7 = _mm256_loadu_pd(K + -36*fi390 + j167 + 1152);
      _t10_6 = _mm256_loadu_pd(K + -36*fi390 + j167 + 1188);
      _t10_5 = _mm256_loadu_pd(K + -36*fi390 + j167 + 1224);
      _t10_4 = _mm256_loadu_pd(K + -36*fi390 + j167 + 1260);
      _t10_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi390 + 35])));
      _t10_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi390 + 34])));
      _t10_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi390 + 33])));
      _t10_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi390 + 32])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t10_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_6), _mm256_unpacklo_pd(_t10_5, _t10_4), 32);
      _t10_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_7, _t10_6), _mm256_unpackhi_pd(_t10_5, _t10_4), 32);
      _t10_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_6), _mm256_unpacklo_pd(_t10_5, _t10_4), 49);
      _t10_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_7, _t10_6), _mm256_unpackhi_pd(_t10_5, _t10_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t10_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t10_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t10_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t10_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t10_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t10_9 = _mm256_sub_pd(_t10_9, _t10_8);

      // AVX Storer:
      _mm256_storeu_pd(y + j167, _t10_9);
    }
  }

  _t0_5 = _mm256_castpd128_pd256(_mm_load_sd(&(K[74])));
  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[108])));
  _t0_10 = _mm256_castpd128_pd256(_mm_load_sd(&(K[111])));
  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(K[109])));
  _t0_9 = _mm256_castpd128_pd256(_mm_load_sd(&(K[110])));
  _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37])));
  _t0_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[73])));
  _t0_2 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 72)), 0);
  _t11_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[3])));
  _t11_1 = _mm256_maskload_pd(y, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : a[36,1] = S(h(1, 36, 3), ( G(h(1, 36, 3), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 3), L0[36,36],h(1, 36, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_6 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_7 = _t0_10;

  // 4-BLAC: 1x4 / 1x4
  _t11_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_6), _mm256_castpd256_pd128(_t11_7)));

  // AVX Storer:
  _t11_0 = _t11_8;

  // Generating : a[36,1] = S(h(3, 36, 0), ( G(h(3, 36, 0), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, 3), L0[36,36],h(3, 36, 0)) ) Kro G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_9 = _t11_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_6, _t0_8), _mm256_unpacklo_pd(_t0_9, _mm256_setzero_pd()), 32);

  // 4-BLAC: (1x4)^T
  _t11_11 = _t11_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_0, _t11_0, 32), _mm256_permute2f128_pd(_t11_0, _t11_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t11_13 = _mm256_mul_pd(_t11_11, _t11_12);

  // 4-BLAC: 4x1 - 4x1
  _t11_14 = _mm256_sub_pd(_t11_9, _t11_13);

  // AVX Storer:
  _t11_1 = _t11_14;

  // Generating : a[36,1] = S(h(1, 36, 2), ( G(h(1, 36, 2), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 2), L0[36,36],h(1, 36, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_15 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_16 = _t0_5;

  // 4-BLAC: 1x4 / 1x4
  _t11_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_15), _mm256_castpd256_pd128(_t11_16)));

  // AVX Storer:
  _t11_2 = _t11_17;

  // Generating : a[36,1] = S(h(2, 36, 0), ( G(h(2, 36, 0), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, 2), L0[36,36],h(2, 36, 0)) ) Kro G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_1, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t11_20 = _t11_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_2, _t11_2, 32), _mm256_permute2f128_pd(_t11_2, _t11_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t11_22 = _mm256_mul_pd(_t11_20, _t11_21);

  // 4-BLAC: 4x1 - 4x1
  _t11_23 = _mm256_sub_pd(_t11_18, _t11_22);

  // AVX Storer:
  _t11_3 = _t11_23;

  // Generating : a[36,1] = S(h(1, 36, 1), ( G(h(1, 36, 1), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 1), L0[36,36],h(1, 36, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_24 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_25 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t11_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_24), _mm256_castpd256_pd128(_t11_25)));

  // AVX Storer:
  _t11_4 = _t11_26;

  // Generating : a[36,1] = S(h(1, 36, 0), ( G(h(1, 36, 0), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, 1), L0[36,36],h(1, 36, 0)) ) Kro G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_27 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_28 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t11_29 = _t11_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_30 = _t11_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_31 = _mm256_mul_pd(_t11_29, _t11_30);

  // 4-BLAC: 1x4 - 1x4
  _t11_32 = _mm256_sub_pd(_t11_27, _t11_31);

  // AVX Storer:
  _t11_5 = _t11_32;

  // Generating : a[36,1] = S(h(1, 36, 0), ( G(h(1, 36, 0), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 0), L0[36,36],h(1, 36, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_33 = _t11_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_34 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t11_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_33), _mm256_castpd256_pd128(_t11_34)));

  // AVX Storer:
  _t11_5 = _t11_35;

  // Generating : kx[36,1] = ( Sum_{k283} ( S(h(4, 36, k283), ( G(h(4, 36, k283), X[36,36],h(4, 36, 0)) * G(h(4, 36, 0), x[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) + Sum_{j167} ( Sum_{k283} ( $(h(4, 36, k283), ( G(h(4, 36, k283), X[36,36],h(4, 36, j167)) * G(h(4, 36, j167), x[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k283 = 0; k283 <= 35; k283+=4 ) {
    _t12_4 = _mm256_loadu_pd(X + 36*k283);
    _t12_3 = _mm256_loadu_pd(X + 36*k283 + 36);
    _t12_2 = _mm256_loadu_pd(X + 36*k283 + 72);
    _t12_1 = _mm256_loadu_pd(X + 36*k283 + 108);
    _t12_0 = _mm256_loadu_pd(x);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t12_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t12_4, _t12_0), _mm256_mul_pd(_t12_3, _t12_0)), _mm256_hadd_pd(_mm256_mul_pd(_t12_2, _t12_0), _mm256_mul_pd(_t12_1, _t12_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t12_4, _t12_0), _mm256_mul_pd(_t12_3, _t12_0)), _mm256_hadd_pd(_mm256_mul_pd(_t12_2, _t12_0), _mm256_mul_pd(_t12_1, _t12_0)), 12));

    // AVX Storer:
    _mm256_storeu_pd(kx + k283, _t12_5);
  }


  for( int j167 = 4; j167 <= 35; j167+=4 ) {

    // AVX Loader:

    for( int k283 = 0; k283 <= 35; k283+=4 ) {
      _t13_4 = _mm256_loadu_pd(X + j167 + 36*k283);
      _t13_3 = _mm256_loadu_pd(X + j167 + 36*k283 + 36);
      _t13_2 = _mm256_loadu_pd(X + j167 + 36*k283 + 72);
      _t13_1 = _mm256_loadu_pd(X + j167 + 36*k283 + 108);
      _t13_0 = _mm256_loadu_pd(x + j167);
      _t13_5 = _mm256_loadu_pd(kx + k283);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t13_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t13_4, _t13_0), _mm256_mul_pd(_t13_3, _t13_0)), _mm256_hadd_pd(_mm256_mul_pd(_t13_2, _t13_0), _mm256_mul_pd(_t13_1, _t13_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t13_4, _t13_0), _mm256_mul_pd(_t13_3, _t13_0)), _mm256_hadd_pd(_mm256_mul_pd(_t13_2, _t13_0), _mm256_mul_pd(_t13_1, _t13_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t13_5 = _mm256_add_pd(_t13_5, _t13_6);

      // AVX Storer:
      _mm256_storeu_pd(kx + k283, _t13_5);
    }
  }

  _t14_0 = _mm256_loadu_pd(kx);

  // Generating : f[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 36, 0), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{j167} ( $(h(1, 1, 0), ( T( G(h(4, 36, j167), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, j167), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t14_3 = _t14_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t14_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), 129)), 1));

  // AVX Storer:
  _t14_1 = _t14_2;


  for( int j167 = 4; j167 <= 35; j167+=4 ) {
    _t15_1 = _mm256_loadu_pd(kx + j167);
    _t15_0 = _mm256_loadu_pd(y + j167);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t15_4 = _t15_1;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t15_3 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_mul_pd(_t15_4, _t15_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_mul_pd(_t15_4, _t15_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_mul_pd(_t15_4, _t15_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t15_2 = _t14_1;

    // 4-BLAC: 1x4 + 1x4
    _t15_2 = _mm256_add_pd(_t15_2, _t15_3);

    // AVX Storer:
    _t14_1 = _t15_2;
  }


  for( int fi390 = 0; fi390 <= 31; fi390+=4 ) {
    _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi390])));
    _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390])));
    _t16_8 = _mm256_maskload_pd(kx + fi390 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t16_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 108)), 32);
    _t16_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 37])));
    _t16_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi390 + 109)), 0);
    _t16_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 74])));
    _t16_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 110])));
    _t16_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi390 + 111])));

    // Generating : v[36,1] = S(h(1, 36, fi390), ( G(h(1, 36, fi390), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390), L0[36,36],h(1, 36, fi390)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_13 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_14 = _t16_6;

    // 4-BLAC: 1x4 / 1x4
    _t16_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_13), _mm256_castpd256_pd128(_t16_14)));

    // AVX Storer:
    _t16_7 = _t16_15;

    // Generating : v[36,1] = S(h(3, 36, fi390 + 1), ( G(h(3, 36, fi390 + 1), v[36,1],h(1, 1, 0)) - ( G(h(3, 36, fi390 + 1), L0[36,36],h(1, 36, fi390)) Kro G(h(1, 36, fi390), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_16 = _t16_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_17 = _t16_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_7, _t16_7, 32), _mm256_permute2f128_pd(_t16_7, _t16_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_19 = _mm256_mul_pd(_t16_17, _t16_18);

    // 4-BLAC: 4x1 - 4x1
    _t16_20 = _mm256_sub_pd(_t16_16, _t16_19);

    // AVX Storer:
    _t16_8 = _t16_20;

    // Generating : v[36,1] = S(h(1, 36, fi390 + 1), ( G(h(1, 36, fi390 + 1), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390 + 1), L0[36,36],h(1, 36, fi390 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_22 = _t16_4;

    // 4-BLAC: 1x4 / 1x4
    _t16_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_21), _mm256_castpd256_pd128(_t16_22)));

    // AVX Storer:
    _t16_9 = _t16_23;

    // Generating : v[36,1] = S(h(2, 36, fi390 + 2), ( G(h(2, 36, fi390 + 2), v[36,1],h(1, 1, 0)) - ( G(h(2, 36, fi390 + 2), L0[36,36],h(1, 36, fi390 + 1)) Kro G(h(1, 36, fi390 + 1), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 6), _mm256_permute2f128_pd(_t16_8, _t16_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_25 = _t16_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_9, _t16_9, 32), _mm256_permute2f128_pd(_t16_9, _t16_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_27 = _mm256_mul_pd(_t16_25, _t16_26);

    // 4-BLAC: 4x1 - 4x1
    _t16_28 = _mm256_sub_pd(_t16_24, _t16_27);

    // AVX Storer:
    _t16_10 = _t16_28;

    // Generating : v[36,1] = S(h(1, 36, fi390 + 2), ( G(h(1, 36, fi390 + 2), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390 + 2), L0[36,36],h(1, 36, fi390 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_30 = _t16_2;

    // 4-BLAC: 1x4 / 1x4
    _t16_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_29), _mm256_castpd256_pd128(_t16_30)));

    // AVX Storer:
    _t16_11 = _t16_31;

    // Generating : v[36,1] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), v[36,1],h(1, 1, 0)) - ( G(h(1, 36, fi390 + 3), L0[36,36],h(1, 36, fi390 + 2)) Kro G(h(1, 36, fi390 + 2), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_33 = _t16_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_34 = _t16_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_35 = _mm256_mul_pd(_t16_33, _t16_34);

    // 4-BLAC: 1x4 - 1x4
    _t16_36 = _mm256_sub_pd(_t16_32, _t16_35);

    // AVX Storer:
    _t16_12 = _t16_36;

    // Generating : v[36,1] = S(h(1, 36, fi390 + 3), ( G(h(1, 36, fi390 + 3), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi390 + 3), L0[36,36],h(1, 36, fi390 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_37 = _t16_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_38 = _t16_0;

    // 4-BLAC: 1x4 / 1x4
    _t16_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_37), _mm256_castpd256_pd128(_t16_38)));

    // AVX Storer:
    _t16_12 = _t16_39;

    // Generating : v[36,1] = Sum_{j167} ( S(h(4, 36, fi390 + j167 + 4), ( G(h(4, 36, fi390 + j167 + 4), v[36,1],h(1, 1, 0)) - ( G(h(4, 36, fi390 + j167 + 4), L0[36,36],h(4, 36, fi390)) * G(h(4, 36, fi390), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(kx[fi390]), _mm256_castpd256_pd128(_t16_7));
    _mm_store_sd(&(kx[fi390 + 1]), _mm256_castpd256_pd128(_t16_9));
    _mm_store_sd(&(kx[fi390 + 2]), _mm256_castpd256_pd128(_t16_11));
    _mm_store_sd(&(kx[fi390 + 3]), _mm256_castpd256_pd128(_t16_12));

    for( int j167 = 0; j167 <= -fi390 + 31; j167+=4 ) {
      _t17_9 = _mm256_loadu_pd(kx + fi390 + j167 + 4);
      _t17_7 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 144);
      _t17_6 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 180);
      _t17_5 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 216);
      _t17_4 = _mm256_loadu_pd(K + 37*fi390 + 36*j167 + 252);
      _t17_3 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi390])));
      _t17_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi390 + 1])));
      _t17_1 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi390 + 2])));
      _t17_0 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi390 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t17_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t17_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t17_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t17_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t17_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t17_9 = _mm256_sub_pd(_t17_9, _t17_8);

      // AVX Storer:
      _mm256_storeu_pd(kx + fi390 + j167 + 4, _t17_9);
    }
  }

  _t18_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[32])));
  _t18_3 = _mm256_maskload_pd(kx + 33, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t18_1 = _mm256_loadu_pd(x);
  _t18_0 = _mm256_loadu_pd(kx);

  // Generating : v[36,1] = S(h(1, 36, 32), ( G(h(1, 36, 32), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_9 = _t18_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_10 = _t8_6;

  // 4-BLAC: 1x4 / 1x4
  _t18_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_9), _mm256_castpd256_pd128(_t18_10)));

  // AVX Storer:
  _t18_2 = _t18_11;

  // Generating : v[36,1] = S(h(3, 36, 33), ( G(h(3, 36, 33), v[36,1],h(1, 1, 0)) - ( G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) Kro G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t18_12 = _t18_3;

  // AVX Loader:

  // 3x1 -> 4x1
  _t18_13 = _t8_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_2, _t18_2, 32), _mm256_permute2f128_pd(_t18_2, _t18_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t18_15 = _mm256_mul_pd(_t18_13, _t18_14);

  // 4-BLAC: 4x1 - 4x1
  _t18_16 = _mm256_sub_pd(_t18_12, _t18_15);

  // AVX Storer:
  _t18_3 = _t18_16;

  // Generating : v[36,1] = S(h(1, 36, 33), ( G(h(1, 36, 33), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_18 = _t8_4;

  // 4-BLAC: 1x4 / 1x4
  _t18_19 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_17), _mm256_castpd256_pd128(_t18_18)));

  // AVX Storer:
  _t18_4 = _t18_19;

  // Generating : v[36,1] = S(h(2, 36, 34), ( G(h(2, 36, 34), v[36,1],h(1, 1, 0)) - ( G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) Kro G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t18_20 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_3, 6), _mm256_permute2f128_pd(_t18_3, _t18_3, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t18_21 = _t8_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_22 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_4, _t18_4, 32), _mm256_permute2f128_pd(_t18_4, _t18_4, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t18_23 = _mm256_mul_pd(_t18_21, _t18_22);

  // 4-BLAC: 4x1 - 4x1
  _t18_24 = _mm256_sub_pd(_t18_20, _t18_23);

  // AVX Storer:
  _t18_5 = _t18_24;

  // Generating : v[36,1] = S(h(1, 36, 34), ( G(h(1, 36, 34), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_26 = _t8_2;

  // 4-BLAC: 1x4 / 1x4
  _t18_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_25), _mm256_castpd256_pd128(_t18_26)));

  // AVX Storer:
  _t18_6 = _t18_27;

  // Generating : v[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), v[36,1],h(1, 1, 0)) - ( G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) Kro G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_28 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_29 = _t8_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_30 = _t18_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t18_31 = _mm256_mul_pd(_t18_29, _t18_30);

  // 4-BLAC: 1x4 - 1x4
  _t18_32 = _mm256_sub_pd(_t18_28, _t18_31);

  // AVX Storer:
  _t18_7 = _t18_32;

  // Generating : v[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_33 = _t18_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_34 = _t8_0;

  // 4-BLAC: 1x4 / 1x4
  _t18_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_33), _mm256_castpd256_pd128(_t18_34)));

  // AVX Storer:
  _t18_7 = _t18_35;

  // Generating : var[1,1] = ( ( S(h(1, 1, 0), ( ( T( G(h(4, 36, 0), x[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), x[36,1],h(1, 1, 0)) ) - ( T( G(h(4, 36, 0), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), kx[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k283} ( $(h(1, 1, 0), ( T( G(h(4, 36, k283), x[36,1],h(1, 1, 0)) ) * G(h(4, 36, k283), x[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + Sum_{j167} ( -$(h(1, 1, 0), ( T( G(h(4, 36, j167), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, j167), kx[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t18_39 = _t18_1;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t18_36 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_mul_pd(_t18_39, _t18_1), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_mul_pd(_t18_39, _t18_1), 129)), _mm256_add_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_mul_pd(_t18_39, _t18_1), 129)), 1));

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t18_40 = _t18_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t18_37 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_mul_pd(_t18_40, _t18_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_mul_pd(_t18_40, _t18_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_mul_pd(_t18_40, _t18_0), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t18_38 = _mm256_sub_pd(_t18_36, _t18_37);

  // AVX Storer:
  _t18_8 = _t18_38;


  for( int k283 = 4; k283 <= 35; k283+=4 ) {
    _t19_0 = _mm256_loadu_pd(x + k283);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t19_3 = _t19_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t19_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_mul_pd(_t19_3, _t19_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_mul_pd(_t19_3, _t19_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_mul_pd(_t19_3, _t19_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t19_1 = _t18_8;

    // 4-BLAC: 1x4 + 1x4
    _t19_1 = _mm256_add_pd(_t19_1, _t19_2);

    // AVX Storer:
    _t18_8 = _t19_1;
  }

  _mm_store_sd(&(kx[32]), _mm256_castpd256_pd128(_t18_2));
  _mm_store_sd(&(kx[33]), _mm256_castpd256_pd128(_t18_4));
  _mm_store_sd(&(kx[34]), _mm256_castpd256_pd128(_t18_6));
  _mm_store_sd(&(kx[35]), _mm256_castpd256_pd128(_t18_7));

  for( int j167 = 4; j167 <= 35; j167+=4 ) {
    _t20_0 = _mm256_loadu_pd(kx + j167);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t20_3 = _t20_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t20_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_mul_pd(_t20_3, _t20_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_mul_pd(_t20_3, _t20_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_mul_pd(_t20_3, _t20_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_1 = _t18_8;

    // 4-BLAC: 1x4 - 1x4
    _t20_1 = _mm256_sub_pd(_t20_1, _t20_2);

    // AVX Storer:
    _t18_8 = _t20_1;
  }


  // Generating : lp[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 36, 0), y[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{j167} ( $(h(1, 1, 0), ( T( G(h(4, 36, j167), y[36,1],h(1, 1, 0)) ) * G(h(4, 36, j167), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t21_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32);

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t21_1 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), _mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_5, _t11_4), _mm256_unpacklo_pd(_t11_2, _t11_0), 32)), 129)), 1));

  // AVX Storer:
  _t21_0 = _t21_1;


  for( int j167 = 4; j167 <= 35; j167+=4 ) {
    _t22_0 = _mm256_loadu_pd(y + j167);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t22_3 = _t22_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t22_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_mul_pd(_t22_3, _t22_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_mul_pd(_t22_3, _t22_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_mul_pd(_t22_3, _t22_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t22_1 = _t21_0;

    // 4-BLAC: 1x4 + 1x4
    _t22_1 = _mm256_add_pd(_t22_1, _t22_2);

    // AVX Storer:
    _t21_0 = _t22_1;
  }

  _mm_store_sd(&(y[3]), _mm256_castpd256_pd128(_t11_0));
  _mm_store_sd(&(y[2]), _mm256_castpd256_pd128(_t11_2));
  _mm_store_sd(&(y[1]), _mm256_castpd256_pd128(_t11_4));
  _mm_store_sd(&(y[0]), _mm256_castpd256_pd128(_t11_5));
  _mm_store_sd(&(f[0]), _mm256_castpd256_pd128(_t14_1));
  _mm_store_sd(&(var[0]), _mm256_castpd256_pd128(_t18_8));
  _mm_store_sd(&(lp[0]), _mm256_castpd256_pd128(_t21_0));

}
