/*
 * gpr_kernel.h
 *
Decl { {u'K': Symmetric[K, (36, 36), LSMatAccess], u'L': LowerTriangular[L, (36, 36), GenMatAccess], u'var': Scalar[var, (1, 1), GenMatAccess], u'L0': LowerTriangular[L0, (36, 36), GenMatAccess], u'X': SquaredMatrix[X, (36, 36), GenMatAccess], 'T1448': Matrix[T1448, (1, 36), GenMatAccess], u'a': Matrix[a, (36, 1), GenMatAccess], u'f': Scalar[f, (1, 1), GenMatAccess], u't2': Matrix[t2, (36, 1), GenMatAccess], u't0': Matrix[t0, (36, 1), GenMatAccess], u't1': Matrix[t1, (36, 1), GenMatAccess], u'lp': Scalar[lp, (1, 1), GenMatAccess], u'v': Matrix[v, (36, 1), GenMatAccess], u'y': Matrix[y, (36, 1), GenMatAccess], u'x': Matrix[x, (36, 1), GenMatAccess], u'kx': Matrix[kx, (36, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'rdiv_ltn_ow_opt': {'m': 'm1.ll', 'n': 'n4.ll'}, 'Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt': {'m0': 'm03.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt_m03_Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt_m04_m21_rdiv_ltn_ow_opt_m1_n4'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
For_{fi18;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1448[1,36],h(1, 36, fi18)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1448[1,36],h(1, 36, fi18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(3, 36, fi18 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(3, 36, fi18 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1448[1,36],h(1, 36, fi18 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1448[1,36],h(1, 36, fi18 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(2, 36, fi18 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(2, 36, fi18 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) )
Eq.ann: {}
Entry 12:
For_{fi79;0;-fi18 + 28;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18)) ) Div Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(3, 36, fi18 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(3, 36, fi18 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(2, 36, fi18 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(2, 36, fi18 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18)) ) Div Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(3, 36, fi18 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(3, 36, fi18 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(2, 36, fi18 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(2, 36, fi18 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18)) ) Div Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(3, 36, fi18 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(3, 36, fi18 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(2, 36, fi18 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(2, 36, fi18 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18)) ) Div Tile( (1, 1), G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(3, 36, fi18 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(3, 36, fi18 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 1)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 1)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(2, 36, fi18 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(2, 36, fi18 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 2)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 2)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 3)) ) = ( Tile( (1, 1), G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 3)) ) Div Tile( (1, 1), G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ) )
Eq.ann: {}
 )Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi18 + 32, 36, fi18 + 4), L[36,36],h(-fi18 + 32, 36, fi18 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi18 + 32, 36, fi18 + 4), L[36,36],h(-fi18 + 32, 36, fi18 + 4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi18 + 32, 36, fi18 + 4), L[36,36],h(4, 36, fi18)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(-fi18 + 32, 36, fi18 + 4), L[36,36],h(4, 36, fi18)) ) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 36, 32), L[36,36],h(1, 36, 32)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 32), L[36,36],h(1, 36, 32)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1448[1,36],h(1, 36, 32)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 36, 32), L[36,36],h(1, 36, 32)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L[36,36],h(1, 36, 32)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1448[1,36],h(1, 36, 32)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L[36,36],h(1, 36, 32)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L[36,36],h(3, 36, 33)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L[36,36],h(3, 36, 33)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L[36,36],h(1, 36, 32)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L[36,36],h(1, 36, 32)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 36, 33), L[36,36],h(1, 36, 33)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 33), L[36,36],h(1, 36, 33)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1448[1,36],h(1, 36, 33)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 36, 33), L[36,36],h(1, 36, 33)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L[36,36],h(1, 36, 33)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1448[1,36],h(1, 36, 33)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L[36,36],h(1, 36, 33)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L[36,36],h(2, 36, 34)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L[36,36],h(2, 36, 34)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L[36,36],h(1, 36, 33)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L[36,36],h(1, 36, 33)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 36, 34), L[36,36],h(1, 36, 34)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 34), L[36,36],h(1, 36, 34)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 36, 35), L[36,36],h(1, 36, 34)) ) = ( Tile( (1, 1), G(h(1, 36, 35), L[36,36],h(1, 36, 34)) ) Div Tile( (1, 1), G(h(1, 36, 34), L[36,36],h(1, 36, 34)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L[36,36],h(1, 36, 35)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L[36,36],h(1, 36, 35)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L[36,36],h(1, 36, 34)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L[36,36],h(1, 36, 34)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 36, 35), L[36,36],h(1, 36, 35)) ) = Sqrt( Tile( (1, 1), G(h(1, 36, 35), L[36,36],h(1, 36, 35)) ) )
Eq.ann: {}
Entry 13:
For_{fi159;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi159), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi159), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi159), L0[36,36],h(1, 36, fi159)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi159 + 1), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi159 + 1), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi159 + 1), L0[36,36],h(1, 36, fi159)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi159), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, fi159 + 1), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi159 + 1), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi159 + 1), L0[36,36],h(1, 36, fi159 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi159 + 2), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi159 + 2), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi159 + 2), L0[36,36],h(1, 36, fi159 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi159 + 1), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi159 + 2), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi159 + 2), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi159 + 2), L0[36,36],h(1, 36, fi159 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi159 + 3), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi159 + 3), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi159 + 3), L0[36,36],h(1, 36, fi159 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi159 + 2), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, fi159 + 3), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi159 + 3), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi159 + 3), L0[36,36],h(1, 36, fi159 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi159 + 32, 36, fi159 + 4), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi159 + 32, 36, fi159 + 4), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi159 + 32, 36, fi159 + 4), L0[36,36],h(4, 36, fi159)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi159), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 14:
Eq: Tile( (1, 1), G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ) )
Eq.ann: {}
Entry 21:
For_{fi236;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, -fi236 + 35), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi236 + 35), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi236 + 35), L0[36,36],h(1, 36, -fi236 + 35)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 35), L0[36,36],h(3, 36, -fi236 + 32)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 35), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, -fi236 + 34), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi236 + 34), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi236 + 34), L0[36,36],h(1, 36, -fi236 + 34)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 34), L0[36,36],h(2, 36, -fi236 + 32)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 34), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, -fi236 + 33), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi236 + 33), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi236 + 33), L0[36,36],h(1, 36, -fi236 + 33)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 33), L0[36,36],h(1, 36, -fi236 + 32)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, -fi236 + 33), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, -fi236 + 32), L0[36,36],h(1, 36, -fi236 + 32)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi236 + 32, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi236 + 32, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 36, -fi236 + 32), L0[36,36],h(-fi236 + 32, 36, 0)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 36, -fi236 + 32), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 22:
Eq: Tile( (1, 1), G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 3), L0[36,36],h(1, 36, 3)) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), L0[36,36],h(3, 36, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 2), L0[36,36],h(1, 36, 2)) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), L0[36,36],h(2, 36, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 1), L0[36,36],h(1, 36, 1)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), L0[36,36],h(1, 36, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 0), a[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 0), L0[36,36],h(1, 36, 0)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), X[36,36] ) ) * Tile( (1, 1), Tile( (4, 4), x[36,1] ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), f[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[36,1] ) ) )
Eq.ann: {}
Entry 31:
For_{fi313;0;31;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 36, fi313), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi313), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi313), L0[36,36],h(1, 36, fi313)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi313 + 1), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi313 + 1), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, fi313 + 1), L0[36,36],h(1, 36, fi313)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi313), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 36, fi313 + 1), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi313 + 1), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi313 + 1), L0[36,36],h(1, 36, fi313 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi313 + 2), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi313 + 2), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, fi313 + 2), L0[36,36],h(1, 36, fi313 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi313 + 1), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 36, fi313 + 2), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi313 + 2), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi313 + 2), L0[36,36],h(1, 36, fi313 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi313 + 3), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi313 + 3), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi313 + 3), L0[36,36],h(1, 36, fi313 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, fi313 + 2), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 36, fi313 + 3), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, fi313 + 3), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, fi313 + 3), L0[36,36],h(1, 36, fi313 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi313 + 32, 36, fi313 + 4), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi313 + 32, 36, fi313 + 4), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi313 + 32, 36, fi313 + 4), L0[36,36],h(4, 36, fi313)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 36, fi313), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 32:
Eq: Tile( (1, 1), G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 36, 35), v[36,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), Tile( (4, 4), var[1,1] ) ) = ( ( T( Tile( (1, 1), Tile( (4, 4), x[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), x[36,1] ) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), kx[36,1] ) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), Tile( (4, 4), lp[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), y[36,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[36,1] ) ) )
Eq.ann: {}
 *
 * Created on: 2017-09-02
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 36
#define PARAM1 36

#define ERRTHRESH 1e-5

#define SOFTERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * X, double const * x, double * K, double * y, double * kx, double * f, double * var, double * lp)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27, _t1_28, _t1_29, _t1_30, _t1_31,
	_t1_32, _t1_33, _t1_34, _t1_35, _t1_36, _t1_37, _t1_38, _t1_39,
	_t1_40, _t1_41, _t1_42, _t1_43, _t1_44, _t1_45, _t1_46, _t1_47,
	_t1_48, _t1_49, _t1_50, _t1_51, _t1_52, _t1_53, _t1_54, _t1_55,
	_t1_56, _t1_57, _t1_58, _t1_59, _t1_60, _t1_61, _t1_62, _t1_63,
	_t1_64, _t1_65, _t1_66, _t1_67, _t1_68, _t1_69, _t1_70, _t1_71,
	_t1_72, _t1_73, _t1_74, _t1_75, _t1_76, _t1_77, _t1_78, _t1_79,
	_t1_80, _t1_81, _t1_82, _t1_83, _t1_84, _t1_85, _t1_86, _t1_87,
	_t1_88, _t1_89, _t1_90, _t1_91, _t1_92, _t1_93, _t1_94, _t1_95,
	_t1_96, _t1_97, _t1_98, _t1_99, _t1_100, _t1_101, _t1_102, _t1_103,
	_t1_104, _t1_105, _t1_106, _t1_107, _t1_108, _t1_109, _t1_110, _t1_111,
	_t1_112, _t1_113, _t1_114, _t1_115, _t1_116, _t1_117, _t1_118, _t1_119,
	_t1_120, _t1_121, _t1_122, _t1_123, _t1_124, _t1_125, _t1_126, _t1_127,
	_t1_128, _t1_129, _t1_130, _t1_131, _t1_132, _t1_133, _t1_134, _t1_135,
	_t1_136, _t1_137, _t1_138, _t1_139, _t1_140, _t1_141, _t1_142, _t1_143;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60, _t5_61, _t5_62, _t5_63,
	_t5_64, _t5_65, _t5_66, _t5_67, _t5_68, _t5_69, _t5_70, _t5_71,
	_t5_72, _t5_73, _t5_74, _t5_75, _t5_76, _t5_77, _t5_78, _t5_79,
	_t5_80, _t5_81, _t5_82, _t5_83, _t5_84, _t5_85, _t5_86, _t5_87,
	_t5_88, _t5_89, _t5_90, _t5_91, _t5_92, _t5_93, _t5_94, _t5_95,
	_t5_96, _t5_97, _t5_98, _t5_99, _t5_100, _t5_101, _t5_102, _t5_103,
	_t5_104, _t5_105, _t5_106, _t5_107, _t5_108, _t5_109, _t5_110, _t5_111,
	_t5_112, _t5_113, _t5_114, _t5_115, _t5_116, _t5_117, _t5_118, _t5_119,
	_t5_120, _t5_121, _t5_122, _t5_123, _t5_124, _t5_125, _t5_126, _t5_127,
	_t5_128, _t5_129, _t5_130, _t5_131, _t5_132, _t5_133, _t5_134, _t5_135,
	_t5_136, _t5_137, _t5_138, _t5_139, _t5_140, _t5_141, _t5_142, _t5_143,
	_t5_144, _t5_145, _t5_146, _t5_147, _t5_148, _t5_149, _t5_150, _t5_151,
	_t5_152, _t5_153, _t5_154, _t5_155, _t5_156, _t5_157, _t5_158, _t5_159,
	_t5_160, _t5_161, _t5_162, _t5_163, _t5_164, _t5_165, _t5_166, _t5_167,
	_t5_168, _t5_169, _t5_170, _t5_171, _t5_172, _t5_173, _t5_174, _t5_175,
	_t5_176, _t5_177, _t5_178, _t5_179, _t5_180, _t5_181, _t5_182, _t5_183,
	_t5_184, _t5_185, _t5_186, _t5_187, _t5_188, _t5_189, _t5_190, _t5_191,
	_t5_192, _t5_193, _t5_194, _t5_195, _t5_196, _t5_197, _t5_198, _t5_199,
	_t5_200, _t5_201, _t5_202, _t5_203, _t5_204, _t5_205, _t5_206, _t5_207,
	_t5_208, _t5_209, _t5_210, _t5_211, _t5_212, _t5_213, _t5_214, _t5_215,
	_t5_216, _t5_217, _t5_218, _t5_219, _t5_220, _t5_221, _t5_222, _t5_223,
	_t5_224, _t5_225, _t5_226, _t5_227, _t5_228, _t5_229, _t5_230, _t5_231,
	_t5_232, _t5_233, _t5_234, _t5_235, _t5_236, _t5_237, _t5_238, _t5_239,
	_t5_240, _t5_241, _t5_242, _t5_243, _t5_244, _t5_245, _t5_246, _t5_247,
	_t5_248, _t5_249, _t5_250, _t5_251, _t5_252, _t5_253, _t5_254, _t5_255,
	_t5_256, _t5_257, _t5_258, _t5_259, _t5_260, _t5_261, _t5_262, _t5_263,
	_t5_264, _t5_265, _t5_266, _t5_267, _t5_268, _t5_269, _t5_270, _t5_271,
	_t5_272, _t5_273, _t5_274, _t5_275, _t5_276, _t5_277, _t5_278, _t5_279,
	_t5_280, _t5_281, _t5_282, _t5_283, _t5_284, _t5_285, _t5_286, _t5_287,
	_t5_288, _t5_289, _t5_290, _t5_291, _t5_292, _t5_293, _t5_294, _t5_295,
	_t5_296, _t5_297, _t5_298, _t5_299, _t5_300, _t5_301, _t5_302, _t5_303,
	_t5_304;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27, _t8_28, _t8_29, _t8_30, _t8_31,
	_t8_32;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27, _t9_28, _t9_29, _t9_30, _t9_31,
	_t9_32, _t9_33, _t9_34, _t9_35, _t9_36, _t9_37, _t9_38, _t9_39,
	_t9_40, _t9_41, _t9_42;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35, _t11_36, _t11_37, _t11_38, _t11_39,
	_t11_40, _t11_41, _t11_42;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35, _t16_36, _t16_37, _t16_38, _t16_39;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31,
	_t18_32, _t18_33, _t18_34, _t18_35, _t18_36, _t18_37, _t18_38, _t18_39,
	_t18_40;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3;
  __m256d _t21_0, _t21_1, _t21_2;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3;


  for( int fi18 = 0; fi18 <= 27; fi18+=4 ) {
    _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18])));
    _t0_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 108)), 32);
    _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 37));
    _t0_4 = _mm256_maskload_pd(K + 37*fi18 + 73, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t0_5 = _mm256_maskload_pd(K + 37*fi18 + 109, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : L[36,36] = S(h(1, 36, fi18), Sqrt( G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_20 = _t0_0;

    // 4-BLAC: sqrt(1x4)
    _t0_21 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_20)));

    // AVX Storer:
    _t0_0 = _t0_21;

    // Generating : T1448[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t0_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_23 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t0_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_22), _mm256_castpd256_pd128(_t0_23)));

    // AVX Storer:
    _t0_1 = _t0_24;

    // Generating : L[36,36] = S(h(3, 36, fi18 + 1), ( G(h(1, 1, 0), T1448[1,36],h(1, 36, fi18)) Kro G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_26 = _t0_2;

    // 4-BLAC: 1x4 Kro 4x1
    _t0_27 = _mm256_mul_pd(_t0_25, _t0_26);

    // AVX Storer:
    _t0_2 = _t0_27;

    // Generating : L[36,36] = S(h(3, 36, fi18 + 1), ( G(h(3, 36, fi18 + 1), L[36,36],h(3, 36, fi18 + 1)) - ( G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) * T( G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ),h(3, 36, fi18 + 1))

    // AVX Loader:

    // 3x3 -> 4x4 - LowTriang
    _t0_28 = _t0_3;
    _t0_29 = _t0_4;
    _t0_30 = _t0_5;
    _t0_31 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_32 = _t0_2;

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_33 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_34 = _t0_33;

    // 4-BLAC: 4x1 * 1x4
    _t0_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 32), _mm256_permute2f128_pd(_t0_32, _t0_32, 32), 0), _t0_34);
    _t0_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 32), _mm256_permute2f128_pd(_t0_32, _t0_32, 32), 15), _t0_34);
    _t0_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 49), _mm256_permute2f128_pd(_t0_32, _t0_32, 49), 0), _t0_34);
    _t0_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 49), _mm256_permute2f128_pd(_t0_32, _t0_32, 49), 15), _t0_34);

    // 4-BLAC: 4x4 - 4x4
    _t0_39 = _mm256_sub_pd(_t0_28, _t0_35);
    _t0_40 = _mm256_sub_pd(_t0_29, _t0_36);
    _t0_41 = _mm256_sub_pd(_t0_30, _t0_37);
    _t0_42 = _mm256_sub_pd(_t0_31, _t0_38);

    // AVX Storer:

    // 4x4 -> 3x3 - LowTriang
    _t0_3 = _t0_39;
    _t0_4 = _t0_40;
    _t0_5 = _t0_41;

    // Generating : L[36,36] = S(h(1, 36, fi18 + 1), Sqrt( G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_43 = _t0_3;

    // 4-BLAC: sqrt(1x4)
    _t0_44 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_43)));

    // AVX Storer:
    _t0_3 = _t0_44;

    // Generating : T1448[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t0_45 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_46 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t0_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_45), _mm256_castpd256_pd128(_t0_46)));

    // AVX Storer:
    _t0_6 = _t0_47;

    // Generating : L[36,36] = S(h(2, 36, fi18 + 2), ( G(h(1, 1, 0), T1448[1,36],h(1, 36, fi18 + 1)) Kro G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_48 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_6, _t0_6, 32), _mm256_permute2f128_pd(_t0_6, _t0_6, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_49 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_4, _t0_5), _mm256_setzero_pd(), 12);

    // 4-BLAC: 1x4 Kro 4x1
    _t0_50 = _mm256_mul_pd(_t0_48, _t0_49);

    // AVX Storer:
    _t0_7 = _t0_50;

    // Generating : L[36,36] = S(h(2, 36, fi18 + 2), ( G(h(2, 36, fi18 + 2), L[36,36],h(2, 36, fi18 + 2)) - ( G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) * T( G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ),h(2, 36, fi18 + 2))

    // AVX Loader:

    // 2x2 -> 4x4 - LowTriang
    _t0_51 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), _mm256_setzero_pd());
    _t0_52 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_5, 6), _mm256_permute2f128_pd(_t0_5, _t0_5, 129), 5);
    _t0_53 = _mm256_setzero_pd();
    _t0_54 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_55 = _t0_7;

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_56 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_57 = _t0_56;

    // 4-BLAC: 4x1 * 1x4
    _t0_58 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_57);
    _t0_59 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 15), _t0_57);
    _t0_60 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 49), _mm256_permute2f128_pd(_t0_55, _t0_55, 49), 0), _t0_57);
    _t0_61 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 49), _mm256_permute2f128_pd(_t0_55, _t0_55, 49), 15), _t0_57);

    // 4-BLAC: 4x4 - 4x4
    _t0_62 = _mm256_sub_pd(_t0_51, _t0_58);
    _t0_63 = _mm256_sub_pd(_t0_52, _t0_59);
    _t0_64 = _mm256_sub_pd(_t0_53, _t0_60);
    _t0_65 = _mm256_sub_pd(_t0_54, _t0_61);

    // AVX Storer:

    // 4x4 -> 2x2 - LowTriang
    _t0_8 = _t0_62;
    _t0_9 = _t0_63;

    // Generating : L[36,36] = S(h(1, 36, fi18 + 2), Sqrt( G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ),h(1, 36, fi18 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_66 = _t0_8;

    // 4-BLAC: sqrt(1x4)
    _t0_67 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_66)));

    // AVX Storer:
    _t0_8 = _t0_67;

    // Generating : L[36,36] = S(h(1, 36, fi18 + 3), ( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) Div G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ),h(1, 36, fi18 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_68 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_69 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t0_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_68), _mm256_castpd256_pd128(_t0_69)));

    // AVX Storer:
    _t0_10 = _t0_70;

    // Generating : L[36,36] = S(h(1, 36, fi18 + 3), ( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) - ( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) Kro T( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ),h(1, 36, fi18 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_12 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_13 = _t0_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_14 = _t0_10;

    // 4-BLAC: (4x1)^T
    _t0_15 = _t0_14;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_16 = _mm256_mul_pd(_t0_13, _t0_15);

    // 4-BLAC: 1x4 - 1x4
    _t0_17 = _mm256_sub_pd(_t0_12, _t0_16);

    // AVX Storer:
    _t0_11 = _t0_17;

    // Generating : L[36,36] = S(h(1, 36, fi18 + 3), Sqrt( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ),h(1, 36, fi18 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_18 = _t0_11;

    // 4-BLAC: sqrt(1x4)
    _t0_19 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_18)));

    // AVX Storer:
    _t0_11 = _t0_19;
    _mm256_maskstore_pd(K + 37*fi18 + 73, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_4);
    _mm256_maskstore_pd(K + 37*fi18 + 109, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t0_5);
    _mm256_maskstore_pd(K + 37*fi18 + 110, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t0_9);

    for( int fi79 = 0; fi79 <= -fi18 + 28; fi79+=4 ) {
      _t1_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 36*fi79 + 144])));
      _t1_1 = _mm256_maskload_pd(K + 37*fi18 + 36*fi79 + 145, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 36*fi79 + 180])));
      _t1_7 = _mm256_maskload_pd(K + 37*fi18 + 36*fi79 + 181, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 36*fi79 + 216])));
      _t1_13 = _mm256_maskload_pd(K + 37*fi18 + 36*fi79 + 217, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t1_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 36*fi79 + 252])));
      _t1_19 = _mm256_maskload_pd(K + 37*fi18 + 36*fi79 + 253, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18)) Div G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_24 = _t1_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_25 = _t0_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_24), _mm256_castpd256_pd128(_t1_25)));

      // AVX Storer:
      _t1_0 = _t1_26;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(3, 36, fi18 + 1)) - ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18)) Kro T( G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ),h(3, 36, fi18 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_27 = _t1_1;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_0, _t1_0, 32), _mm256_permute2f128_pd(_t1_0, _t1_0, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_29 = _t0_2;

      // 4-BLAC: (4x1)^T
      _t1_30 = _t1_29;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_31 = _mm256_mul_pd(_t1_28, _t1_30);

      // 4-BLAC: 1x4 - 1x4
      _t1_32 = _mm256_sub_pd(_t1_27, _t1_31);

      // AVX Storer:
      _t1_1 = _t1_32;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 1)) Div G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_33 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_1, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_34 = _t0_3;

      // 4-BLAC: 1x4 / 1x4
      _t1_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_33), _mm256_castpd256_pd128(_t1_34)));

      // AVX Storer:
      _t1_2 = _t1_35;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(2, 36, fi18 + 2)) - ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 1)) Kro T( G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ),h(2, 36, fi18 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_36 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_1, 6), _mm256_permute2f128_pd(_t1_1, _t1_1, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_37 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_2, _t1_2, 32), _mm256_permute2f128_pd(_t1_2, _t1_2, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_38 = _t0_7;

      // 4-BLAC: (4x1)^T
      _t1_39 = _t1_38;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_40 = _mm256_mul_pd(_t1_37, _t1_39);

      // 4-BLAC: 1x4 - 1x4
      _t1_41 = _mm256_sub_pd(_t1_36, _t1_40);

      // AVX Storer:
      _t1_3 = _t1_41;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 2)) Div G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ),h(1, 36, fi18 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_42 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_3, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_43 = _t0_8;

      // 4-BLAC: 1x4 / 1x4
      _t1_44 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_42), _mm256_castpd256_pd128(_t1_43)));

      // AVX Storer:
      _t1_4 = _t1_44;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 3)) - ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 2)) Kro T( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_45 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_3, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_46 = _t1_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_47 = _t0_10;

      // 4-BLAC: (4x1)^T
      _t1_48 = _t1_47;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_49 = _mm256_mul_pd(_t1_46, _t1_48);

      // 4-BLAC: 1x4 - 1x4
      _t1_50 = _mm256_sub_pd(_t1_45, _t1_49);

      // AVX Storer:
      _t1_5 = _t1_50;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 4), ( G(h(1, 36, fi18 + fi79 + 4), L[36,36],h(1, 36, fi18 + 3)) Div G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_51 = _t1_5;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_52 = _t0_11;

      // 4-BLAC: 1x4 / 1x4
      _t1_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_51), _mm256_castpd256_pd128(_t1_52)));

      // AVX Storer:
      _t1_5 = _t1_53;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18)) Div G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_54 = _t1_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_55 = _t0_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_54), _mm256_castpd256_pd128(_t1_55)));

      // AVX Storer:
      _t1_6 = _t1_56;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(3, 36, fi18 + 1)) - ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18)) Kro T( G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ),h(3, 36, fi18 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_57 = _t1_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_6, _t1_6, 32), _mm256_permute2f128_pd(_t1_6, _t1_6, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_59 = _t0_2;

      // 4-BLAC: (4x1)^T
      _t1_60 = _t1_59;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_61 = _mm256_mul_pd(_t1_58, _t1_60);

      // 4-BLAC: 1x4 - 1x4
      _t1_62 = _mm256_sub_pd(_t1_57, _t1_61);

      // AVX Storer:
      _t1_7 = _t1_62;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 1)) Div G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_63 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_64 = _t0_3;

      // 4-BLAC: 1x4 / 1x4
      _t1_65 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_63), _mm256_castpd256_pd128(_t1_64)));

      // AVX Storer:
      _t1_8 = _t1_65;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(2, 36, fi18 + 2)) - ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 1)) Kro T( G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ),h(2, 36, fi18 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_66 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 6), _mm256_permute2f128_pd(_t1_7, _t1_7, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_67 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_8, _t1_8, 32), _mm256_permute2f128_pd(_t1_8, _t1_8, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_68 = _t0_7;

      // 4-BLAC: (4x1)^T
      _t1_69 = _t1_68;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_70 = _mm256_mul_pd(_t1_67, _t1_69);

      // 4-BLAC: 1x4 - 1x4
      _t1_71 = _mm256_sub_pd(_t1_66, _t1_70);

      // AVX Storer:
      _t1_9 = _t1_71;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 2)) Div G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ),h(1, 36, fi18 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_72 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_9, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_73 = _t0_8;

      // 4-BLAC: 1x4 / 1x4
      _t1_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_72), _mm256_castpd256_pd128(_t1_73)));

      // AVX Storer:
      _t1_10 = _t1_74;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 3)) - ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 2)) Kro T( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_75 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_9, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_76 = _t1_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_77 = _t0_10;

      // 4-BLAC: (4x1)^T
      _t1_78 = _t1_77;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_79 = _mm256_mul_pd(_t1_76, _t1_78);

      // 4-BLAC: 1x4 - 1x4
      _t1_80 = _mm256_sub_pd(_t1_75, _t1_79);

      // AVX Storer:
      _t1_11 = _t1_80;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 5), ( G(h(1, 36, fi18 + fi79 + 5), L[36,36],h(1, 36, fi18 + 3)) Div G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_81 = _t1_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_82 = _t0_11;

      // 4-BLAC: 1x4 / 1x4
      _t1_83 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_81), _mm256_castpd256_pd128(_t1_82)));

      // AVX Storer:
      _t1_11 = _t1_83;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18)) Div G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_84 = _t1_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_85 = _t0_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_86 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_84), _mm256_castpd256_pd128(_t1_85)));

      // AVX Storer:
      _t1_12 = _t1_86;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(3, 36, fi18 + 1)) - ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18)) Kro T( G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ),h(3, 36, fi18 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_87 = _t1_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_88 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_12, _t1_12, 32), _mm256_permute2f128_pd(_t1_12, _t1_12, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_89 = _t0_2;

      // 4-BLAC: (4x1)^T
      _t1_90 = _t1_89;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_91 = _mm256_mul_pd(_t1_88, _t1_90);

      // 4-BLAC: 1x4 - 1x4
      _t1_92 = _mm256_sub_pd(_t1_87, _t1_91);

      // AVX Storer:
      _t1_13 = _t1_92;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 1)) Div G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_13, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_94 = _t0_3;

      // 4-BLAC: 1x4 / 1x4
      _t1_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_93), _mm256_castpd256_pd128(_t1_94)));

      // AVX Storer:
      _t1_14 = _t1_95;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(2, 36, fi18 + 2)) - ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 1)) Kro T( G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ),h(2, 36, fi18 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_96 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_13, 6), _mm256_permute2f128_pd(_t1_13, _t1_13, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_97 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_14, _t1_14, 32), _mm256_permute2f128_pd(_t1_14, _t1_14, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_98 = _t0_7;

      // 4-BLAC: (4x1)^T
      _t1_99 = _t1_98;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_100 = _mm256_mul_pd(_t1_97, _t1_99);

      // 4-BLAC: 1x4 - 1x4
      _t1_101 = _mm256_sub_pd(_t1_96, _t1_100);

      // AVX Storer:
      _t1_15 = _t1_101;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 2)) Div G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ),h(1, 36, fi18 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_102 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_15, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_103 = _t0_8;

      // 4-BLAC: 1x4 / 1x4
      _t1_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_102), _mm256_castpd256_pd128(_t1_103)));

      // AVX Storer:
      _t1_16 = _t1_104;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 3)) - ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 2)) Kro T( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_15, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_106 = _t1_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_107 = _t0_10;

      // 4-BLAC: (4x1)^T
      _t1_108 = _t1_107;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_109 = _mm256_mul_pd(_t1_106, _t1_108);

      // 4-BLAC: 1x4 - 1x4
      _t1_110 = _mm256_sub_pd(_t1_105, _t1_109);

      // AVX Storer:
      _t1_17 = _t1_110;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 6), ( G(h(1, 36, fi18 + fi79 + 6), L[36,36],h(1, 36, fi18 + 3)) Div G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_111 = _t1_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_112 = _t0_11;

      // 4-BLAC: 1x4 / 1x4
      _t1_113 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_111), _mm256_castpd256_pd128(_t1_112)));

      // AVX Storer:
      _t1_17 = _t1_113;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18)) Div G(h(1, 36, fi18), L[36,36],h(1, 36, fi18)) ),h(1, 36, fi18))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_114 = _t1_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_115 = _t0_0;

      // 4-BLAC: 1x4 / 1x4
      _t1_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_114), _mm256_castpd256_pd128(_t1_115)));

      // AVX Storer:
      _t1_18 = _t1_116;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(3, 36, fi18 + 1)) - ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18)) Kro T( G(h(3, 36, fi18 + 1), L[36,36],h(1, 36, fi18)) ) ) ),h(3, 36, fi18 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t1_117 = _t1_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_118 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_18, _t1_18, 32), _mm256_permute2f128_pd(_t1_18, _t1_18, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t1_119 = _t0_2;

      // 4-BLAC: (4x1)^T
      _t1_120 = _t1_119;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_121 = _mm256_mul_pd(_t1_118, _t1_120);

      // 4-BLAC: 1x4 - 1x4
      _t1_122 = _mm256_sub_pd(_t1_117, _t1_121);

      // AVX Storer:
      _t1_19 = _t1_122;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 1)) Div G(h(1, 36, fi18 + 1), L[36,36],h(1, 36, fi18 + 1)) ),h(1, 36, fi18 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_19, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_124 = _t0_3;

      // 4-BLAC: 1x4 / 1x4
      _t1_125 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_123), _mm256_castpd256_pd128(_t1_124)));

      // AVX Storer:
      _t1_20 = _t1_125;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(2, 36, fi18 + 2)) - ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 1)) Kro T( G(h(2, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 1)) ) ) ),h(2, 36, fi18 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t1_126 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_19, 6), _mm256_permute2f128_pd(_t1_19, _t1_19, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_127 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_20, _t1_20, 32), _mm256_permute2f128_pd(_t1_20, _t1_20, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t1_128 = _t0_7;

      // 4-BLAC: (4x1)^T
      _t1_129 = _t1_128;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_130 = _mm256_mul_pd(_t1_127, _t1_129);

      // 4-BLAC: 1x4 - 1x4
      _t1_131 = _mm256_sub_pd(_t1_126, _t1_130);

      // AVX Storer:
      _t1_21 = _t1_131;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 2)) Div G(h(1, 36, fi18 + 2), L[36,36],h(1, 36, fi18 + 2)) ),h(1, 36, fi18 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_132 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_21, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_133 = _t0_8;

      // 4-BLAC: 1x4 / 1x4
      _t1_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_132), _mm256_castpd256_pd128(_t1_133)));

      // AVX Storer:
      _t1_22 = _t1_134;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 3)) - ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 2)) Kro T( G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 2)) ) ) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_135 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_21, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_136 = _t1_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_137 = _t0_10;

      // 4-BLAC: (4x1)^T
      _t1_138 = _t1_137;

      // 4-BLAC: 1x4 Kro 1x4
      _t1_139 = _mm256_mul_pd(_t1_136, _t1_138);

      // 4-BLAC: 1x4 - 1x4
      _t1_140 = _mm256_sub_pd(_t1_135, _t1_139);

      // AVX Storer:
      _t1_23 = _t1_140;

      // Generating : L[36,36] = S(h(1, 36, fi18 + fi79 + 7), ( G(h(1, 36, fi18 + fi79 + 7), L[36,36],h(1, 36, fi18 + 3)) Div G(h(1, 36, fi18 + 3), L[36,36],h(1, 36, fi18 + 3)) ),h(1, 36, fi18 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_141 = _t1_23;

      // AVX Loader:

      // 1x1 -> 1x4
      _t1_142 = _t0_11;

      // 4-BLAC: 1x4 / 1x4
      _t1_143 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_141), _mm256_castpd256_pd128(_t1_142)));

      // AVX Storer:
      _t1_23 = _t1_143;
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 144]), _mm256_castpd256_pd128(_t1_0));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 145]), _mm256_castpd256_pd128(_t1_2));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 146]), _mm256_castpd256_pd128(_t1_4));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 147]), _mm256_castpd256_pd128(_t1_5));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 180]), _mm256_castpd256_pd128(_t1_6));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 181]), _mm256_castpd256_pd128(_t1_8));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 182]), _mm256_castpd256_pd128(_t1_10));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 183]), _mm256_castpd256_pd128(_t1_11));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 216]), _mm256_castpd256_pd128(_t1_12));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 217]), _mm256_castpd256_pd128(_t1_14));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 218]), _mm256_castpd256_pd128(_t1_16));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 219]), _mm256_castpd256_pd128(_t1_17));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 252]), _mm256_castpd256_pd128(_t1_18));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 253]), _mm256_castpd256_pd128(_t1_20));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 254]), _mm256_castpd256_pd128(_t1_22));
      _mm_store_sd(&(K[37*fi18 + 36*fi79 + 255]), _mm256_castpd256_pd128(_t1_23));
    }
    _t2_20 = _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 148));
    _t2_21 = _mm256_maskload_pd(K + 37*fi18 + 184, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t2_22 = _mm256_maskload_pd(K + 37*fi18 + 220, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t2_23 = _mm256_loadu_pd(K + 37*fi18 + 256);
    _t2_19 = _mm256_broadcast_sd(K + 37*fi18 + 144);
    _t2_18 = _mm256_broadcast_sd(K + 37*fi18 + 145);
    _t2_17 = _mm256_broadcast_sd(K + 37*fi18 + 146);
    _t2_16 = _mm256_broadcast_sd(K + 37*fi18 + 147);
    _t2_15 = _mm256_broadcast_sd(K + 37*fi18 + 180);
    _t2_14 = _mm256_broadcast_sd(K + 37*fi18 + 181);
    _t2_13 = _mm256_broadcast_sd(K + 37*fi18 + 182);
    _t2_12 = _mm256_broadcast_sd(K + 37*fi18 + 183);
    _t2_11 = _mm256_broadcast_sd(K + 37*fi18 + 216);
    _t2_10 = _mm256_broadcast_sd(K + 37*fi18 + 217);
    _t2_9 = _mm256_broadcast_sd(K + 37*fi18 + 218);
    _t2_8 = _mm256_broadcast_sd(K + 37*fi18 + 219);
    _t2_7 = _mm256_broadcast_sd(K + 37*fi18 + 252);
    _t2_6 = _mm256_broadcast_sd(K + 37*fi18 + 253);
    _t2_5 = _mm256_broadcast_sd(K + 37*fi18 + 254);
    _t2_4 = _mm256_broadcast_sd(K + 37*fi18 + 255);
    _t2_3 = _mm256_loadu_pd(K + 37*fi18 + 144);
    _t2_2 = _mm256_loadu_pd(K + 37*fi18 + 180);
    _t2_1 = _mm256_loadu_pd(K + 37*fi18 + 216);
    _t2_0 = _mm256_loadu_pd(K + 37*fi18 + 252);

    // Generating : L[36,36] = ( S(h(4, 36, fi18 + 4), ( G(h(4, 36, fi18 + 4), L[36,36],h(4, 36, fi18 + 4)) - ( G(h(4, 36, fi18 + 4), L[36,36],h(4, 36, fi18)) * T( G(h(4, 36, fi18 + 4), L[36,36],h(4, 36, fi18)) ) ) ),h(4, 36, fi18 + 4)) + Sum_{i150} ( ( Sum_{j151} ( S(h(4, 36, fi18 + i150 + 4), ( G(h(4, 36, fi18 + i150 + 4), L[36,36],h(4, 36, fi18 + j151 + 4)) - ( G(h(4, 36, fi18 + i150 + 4), L[36,36],h(4, 36, fi18)) * T( G(h(4, 36, fi18 + j151 + 4), L[36,36],h(4, 36, fi18)) ) ) ),h(4, 36, fi18 + j151 + 4)) ) + S(h(4, 36, fi18 + i150 + 4), ( G(h(4, 36, fi18 + i150 + 4), L[36,36],h(4, 36, fi18 + i150 + 4)) - ( G(h(4, 36, fi18 + i150 + 4), L[36,36],h(4, 36, fi18)) * T( G(h(4, 36, fi18 + i150 + 4), L[36,36],h(4, 36, fi18)) ) ) ),h(4, 36, fi18 + i150 + 4)) ) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowTriang
    _t2_32 = _t2_20;
    _t2_33 = _t2_21;
    _t2_34 = _t2_22;
    _t2_35 = _t2_23;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t2_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_3, _t2_2), _mm256_unpacklo_pd(_t2_1, _t2_0), 32);
    _t2_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t2_3, _t2_2), _mm256_unpackhi_pd(_t2_1, _t2_0), 32);
    _t2_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_3, _t2_2), _mm256_unpacklo_pd(_t2_1, _t2_0), 49);
    _t2_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t2_3, _t2_2), _mm256_unpackhi_pd(_t2_1, _t2_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t2_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_19, _t2_36), _mm256_mul_pd(_t2_18, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_17, _t2_38), _mm256_mul_pd(_t2_16, _t2_39)));
    _t2_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_15, _t2_36), _mm256_mul_pd(_t2_14, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_13, _t2_38), _mm256_mul_pd(_t2_12, _t2_39)));
    _t2_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_11, _t2_36), _mm256_mul_pd(_t2_10, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_9, _t2_38), _mm256_mul_pd(_t2_8, _t2_39)));
    _t2_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_7, _t2_36), _mm256_mul_pd(_t2_6, _t2_37)), _mm256_add_pd(_mm256_mul_pd(_t2_5, _t2_38), _mm256_mul_pd(_t2_4, _t2_39)));

    // 4-BLAC: 4x4 - 4x4
    _t2_28 = _mm256_sub_pd(_t2_32, _t2_24);
    _t2_29 = _mm256_sub_pd(_t2_33, _t2_25);
    _t2_30 = _mm256_sub_pd(_t2_34, _t2_26);
    _t2_31 = _mm256_sub_pd(_t2_35, _t2_27);

    // AVX Storer:

    // 4x4 -> 4x4 - LowTriang
    _t2_20 = _t2_28;
    _t2_21 = _t2_29;
    _t2_22 = _t2_30;
    _t2_23 = _t2_31;
    _mm_store_sd(&(K[37*fi18]), _mm256_castpd256_pd128(_t0_0));
    _mm256_maskstore_pd(K + 37*fi18 + 36, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_2);
    _mm256_maskstore_pd(K + 37*fi18 + 72, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_2, _t0_2, 1));
    _mm256_maskstore_pd(K + 37*fi18 + 108, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t0_2, _t0_2, 129));
    _mm_store_sd(K + 37*fi18 + 37, _mm256_castpd256_pd128(_t0_3));
    _mm256_maskstore_pd(K + 37*fi18 + 73, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_7);
    _mm256_maskstore_pd(K + 37*fi18 + 109, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_7, _t0_7, 1));
    _mm_store_sd(K + 37*fi18 + 74, _mm256_castpd256_pd128(_t0_8));
    _mm_store_sd(&(K[37*fi18 + 110]), _mm256_castpd256_pd128(_t0_10));
    _mm_store_sd(&(K[37*fi18 + 111]), _mm256_castpd256_pd128(_t0_11));
    _mm_store_sd(K + 37*fi18 + 148, _mm256_castpd256_pd128(_t2_20));
    _mm256_maskstore_pd(K + 37*fi18 + 184, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t2_21);
    _mm256_maskstore_pd(K + 37*fi18 + 220, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t2_22);
    _mm256_storeu_pd(K + 37*fi18 + 256, _t2_23);

    for( int i150 = 4; i150 <= -fi18 + 31; i150+=4 ) {

      for( int j151 = 0; j151 <= i150 - 4; j151+=4 ) {
        _t3_24 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + j151 + 148);
        _t3_25 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + j151 + 184);
        _t3_26 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + j151 + 220);
        _t3_27 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + j151 + 256);
        _t3_19 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 144);
        _t3_18 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 145);
        _t3_17 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 146);
        _t3_16 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 147);
        _t3_15 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 180);
        _t3_14 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 181);
        _t3_13 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 182);
        _t3_12 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 183);
        _t3_11 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 216);
        _t3_10 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 217);
        _t3_9 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 218);
        _t3_8 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 219);
        _t3_7 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 252);
        _t3_6 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 253);
        _t3_5 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 254);
        _t3_4 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 255);
        _t3_3 = _mm256_loadu_pd(K + 37*fi18 + 36*j151 + 144);
        _t3_2 = _mm256_loadu_pd(K + 37*fi18 + 36*j151 + 180);
        _t3_1 = _mm256_loadu_pd(K + 37*fi18 + 36*j151 + 216);
        _t3_0 = _mm256_loadu_pd(K + 37*fi18 + 36*j151 + 252);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t3_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 32);
        _t3_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t3_3, _t3_2), _mm256_unpackhi_pd(_t3_1, _t3_0), 32);
        _t3_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_3, _t3_2), _mm256_unpacklo_pd(_t3_1, _t3_0), 49);
        _t3_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t3_3, _t3_2), _mm256_unpackhi_pd(_t3_1, _t3_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t3_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_19, _t3_28), _mm256_mul_pd(_t3_18, _t3_29)), _mm256_add_pd(_mm256_mul_pd(_t3_17, _t3_30), _mm256_mul_pd(_t3_16, _t3_31)));
        _t3_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t3_28), _mm256_mul_pd(_t3_14, _t3_29)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t3_30), _mm256_mul_pd(_t3_12, _t3_31)));
        _t3_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t3_28), _mm256_mul_pd(_t3_10, _t3_29)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t3_30), _mm256_mul_pd(_t3_8, _t3_31)));
        _t3_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t3_28), _mm256_mul_pd(_t3_6, _t3_29)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t3_30), _mm256_mul_pd(_t3_4, _t3_31)));

        // 4-BLAC: 4x4 - 4x4
        _t3_24 = _mm256_sub_pd(_t3_24, _t3_20);
        _t3_25 = _mm256_sub_pd(_t3_25, _t3_21);
        _t3_26 = _mm256_sub_pd(_t3_26, _t3_22);
        _t3_27 = _mm256_sub_pd(_t3_27, _t3_23);

        // AVX Storer:
        _mm256_storeu_pd(K + 37*fi18 + 36*i150 + j151 + 148, _t3_24);
        _mm256_storeu_pd(K + 37*fi18 + 36*i150 + j151 + 184, _t3_25);
        _mm256_storeu_pd(K + 37*fi18 + 36*i150 + j151 + 220, _t3_26);
        _mm256_storeu_pd(K + 37*fi18 + 36*i150 + j151 + 256, _t3_27);
      }
      _t4_20 = _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 37*i150 + 148));
      _t4_21 = _mm256_maskload_pd(K + 37*fi18 + 37*i150 + 184, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t4_22 = _mm256_maskload_pd(K + 37*fi18 + 37*i150 + 220, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t4_23 = _mm256_loadu_pd(K + 37*fi18 + 37*i150 + 256);
      _t4_19 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 144);
      _t4_18 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 145);
      _t4_17 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 146);
      _t4_16 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 147);
      _t4_15 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 180);
      _t4_14 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 181);
      _t4_13 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 182);
      _t4_12 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 183);
      _t4_11 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 216);
      _t4_10 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 217);
      _t4_9 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 218);
      _t4_8 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 219);
      _t4_7 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 252);
      _t4_6 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 253);
      _t4_5 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 254);
      _t4_4 = _mm256_broadcast_sd(K + 37*fi18 + 36*i150 + 255);
      _t4_3 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 144);
      _t4_2 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 180);
      _t4_1 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 216);
      _t4_0 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 252);

      // AVX Loader:

      // 4x4 -> 4x4 - LowTriang
      _t4_32 = _t4_20;
      _t4_33 = _t4_21;
      _t4_34 = _t4_22;
      _t4_35 = _t4_23;

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t4_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 32);
      _t4_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 32);
      _t4_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 49);
      _t4_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t4_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_19, _t4_36), _mm256_mul_pd(_t4_18, _t4_37)), _mm256_add_pd(_mm256_mul_pd(_t4_17, _t4_38), _mm256_mul_pd(_t4_16, _t4_39)));
      _t4_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t4_36), _mm256_mul_pd(_t4_14, _t4_37)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t4_38), _mm256_mul_pd(_t4_12, _t4_39)));
      _t4_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t4_36), _mm256_mul_pd(_t4_10, _t4_37)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t4_38), _mm256_mul_pd(_t4_8, _t4_39)));
      _t4_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t4_36), _mm256_mul_pd(_t4_6, _t4_37)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t4_38), _mm256_mul_pd(_t4_4, _t4_39)));

      // 4-BLAC: 4x4 - 4x4
      _t4_28 = _mm256_sub_pd(_t4_32, _t4_24);
      _t4_29 = _mm256_sub_pd(_t4_33, _t4_25);
      _t4_30 = _mm256_sub_pd(_t4_34, _t4_26);
      _t4_31 = _mm256_sub_pd(_t4_35, _t4_27);

      // AVX Storer:

      // 4x4 -> 4x4 - LowTriang
      _t4_20 = _t4_28;
      _t4_21 = _t4_29;
      _t4_22 = _t4_30;
      _t4_23 = _t4_31;
      _mm_store_sd(K + 37*fi18 + 37*i150 + 148, _mm256_castpd256_pd128(_t4_20));
      _mm256_maskstore_pd(K + 37*fi18 + 37*i150 + 184, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_21);
      _mm256_maskstore_pd(K + 37*fi18 + 37*i150 + 220, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_22);
      _mm256_storeu_pd(K + 37*fi18 + 37*i150 + 256, _t4_23);
    }
  }

  _t5_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1036])));
  _t5_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1072)), _mm256_castpd128_pd256(_mm_load_sd(K + 1108))), _mm256_castpd128_pd256(_mm_load_sd(K + 1144)), 32);
  _t5_3 = _mm256_castpd128_pd256(_mm_load_sd(K + 1073));
  _t5_4 = _mm256_maskload_pd(K + 1109, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t5_5 = _mm256_maskload_pd(K + 1145, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1180])));
  _t5_13 = _mm256_maskload_pd(K + 1181, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1216])));
  _t5_19 = _mm256_maskload_pd(K + 1217, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_24 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1252])));
  _t5_25 = _mm256_maskload_pd(K + 1253, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_30 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1288])));
  _t5_31 = _mm256_maskload_pd(K + 1289, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_36 = _mm256_castpd128_pd256(_mm_load_sd(K + 1184));
  _t5_37 = _mm256_maskload_pd(K + 1220, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t5_38 = _mm256_maskload_pd(K + 1256, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_39 = _mm256_loadu_pd(K + 1292);

  // Generating : L[36,36] = S(h(1, 36, 28), Sqrt( G(h(1, 36, 28), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_59 = _t5_0;

  // 4-BLAC: sqrt(1x4)
  _t5_60 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_59)));

  // AVX Storer:
  _t5_0 = _t5_60;

  // Generating : T1448[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, 28), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t5_61 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_62 = _t5_0;

  // 4-BLAC: 1x4 / 1x4
  _t5_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_61), _mm256_castpd256_pd128(_t5_62)));

  // AVX Storer:
  _t5_1 = _t5_63;

  // Generating : L[36,36] = S(h(3, 36, 29), ( G(h(1, 1, 0), T1448[1,36],h(1, 36, 28)) Kro G(h(3, 36, 29), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_64 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_1, _t5_1, 32), _mm256_permute2f128_pd(_t5_1, _t5_1, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_65 = _t5_2;

  // 4-BLAC: 1x4 Kro 4x1
  _t5_66 = _mm256_mul_pd(_t5_64, _t5_65);

  // AVX Storer:
  _t5_2 = _t5_66;

  // Generating : L[36,36] = S(h(3, 36, 29), ( G(h(3, 36, 29), L[36,36],h(3, 36, 29)) - ( G(h(3, 36, 29), L[36,36],h(1, 36, 28)) * T( G(h(3, 36, 29), L[36,36],h(1, 36, 28)) ) ) ),h(3, 36, 29))

  // AVX Loader:

  // 3x3 -> 4x4 - LowTriang
  _t5_67 = _t5_3;
  _t5_68 = _t5_4;
  _t5_69 = _t5_5;
  _t5_70 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_71 = _t5_2;

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_72 = _t5_2;

  // 4-BLAC: (4x1)^T
  _t5_73 = _t5_72;

  // 4-BLAC: 4x1 * 1x4
  _t5_74 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_71, _t5_71, 32), _mm256_permute2f128_pd(_t5_71, _t5_71, 32), 0), _t5_73);
  _t5_75 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_71, _t5_71, 32), _mm256_permute2f128_pd(_t5_71, _t5_71, 32), 15), _t5_73);
  _t5_76 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_71, _t5_71, 49), _mm256_permute2f128_pd(_t5_71, _t5_71, 49), 0), _t5_73);
  _t5_77 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_71, _t5_71, 49), _mm256_permute2f128_pd(_t5_71, _t5_71, 49), 15), _t5_73);

  // 4-BLAC: 4x4 - 4x4
  _t5_78 = _mm256_sub_pd(_t5_67, _t5_74);
  _t5_79 = _mm256_sub_pd(_t5_68, _t5_75);
  _t5_80 = _mm256_sub_pd(_t5_69, _t5_76);
  _t5_81 = _mm256_sub_pd(_t5_70, _t5_77);

  // AVX Storer:

  // 4x4 -> 3x3 - LowTriang
  _t5_3 = _t5_78;
  _t5_4 = _t5_79;
  _t5_5 = _t5_80;

  // Generating : L[36,36] = S(h(1, 36, 29), Sqrt( G(h(1, 36, 29), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_82 = _t5_3;

  // 4-BLAC: sqrt(1x4)
  _t5_83 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_82)));

  // AVX Storer:
  _t5_3 = _t5_83;

  // Generating : T1448[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, 29), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t5_84 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_85 = _t5_3;

  // 4-BLAC: 1x4 / 1x4
  _t5_86 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_84), _mm256_castpd256_pd128(_t5_85)));

  // AVX Storer:
  _t5_6 = _t5_86;

  // Generating : L[36,36] = S(h(2, 36, 30), ( G(h(1, 1, 0), T1448[1,36],h(1, 36, 29)) Kro G(h(2, 36, 30), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_87 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_6, _t5_6, 32), _mm256_permute2f128_pd(_t5_6, _t5_6, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_88 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_4, _t5_5), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 4x1
  _t5_89 = _mm256_mul_pd(_t5_87, _t5_88);

  // AVX Storer:
  _t5_7 = _t5_89;

  // Generating : L[36,36] = S(h(2, 36, 30), ( G(h(2, 36, 30), L[36,36],h(2, 36, 30)) - ( G(h(2, 36, 30), L[36,36],h(1, 36, 29)) * T( G(h(2, 36, 30), L[36,36],h(1, 36, 29)) ) ) ),h(2, 36, 30))

  // AVX Loader:

  // 2x2 -> 4x4 - LowTriang
  _t5_90 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_4, 2), _mm256_setzero_pd());
  _t5_91 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_5, 6), _mm256_permute2f128_pd(_t5_5, _t5_5, 129), 5);
  _t5_92 = _mm256_setzero_pd();
  _t5_93 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_94 = _t5_7;

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_95 = _t5_7;

  // 4-BLAC: (4x1)^T
  _t5_96 = _t5_95;

  // 4-BLAC: 4x1 * 1x4
  _t5_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_94, _t5_94, 32), _mm256_permute2f128_pd(_t5_94, _t5_94, 32), 0), _t5_96);
  _t5_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_94, _t5_94, 32), _mm256_permute2f128_pd(_t5_94, _t5_94, 32), 15), _t5_96);
  _t5_99 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_94, _t5_94, 49), _mm256_permute2f128_pd(_t5_94, _t5_94, 49), 0), _t5_96);
  _t5_100 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_94, _t5_94, 49), _mm256_permute2f128_pd(_t5_94, _t5_94, 49), 15), _t5_96);

  // 4-BLAC: 4x4 - 4x4
  _t5_101 = _mm256_sub_pd(_t5_90, _t5_97);
  _t5_102 = _mm256_sub_pd(_t5_91, _t5_98);
  _t5_103 = _mm256_sub_pd(_t5_92, _t5_99);
  _t5_104 = _mm256_sub_pd(_t5_93, _t5_100);

  // AVX Storer:

  // 4x4 -> 2x2 - LowTriang
  _t5_8 = _t5_101;
  _t5_9 = _t5_102;

  // Generating : L[36,36] = S(h(1, 36, 30), Sqrt( G(h(1, 36, 30), L[36,36],h(1, 36, 30)) ),h(1, 36, 30))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_105 = _t5_8;

  // 4-BLAC: sqrt(1x4)
  _t5_106 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_105)));

  // AVX Storer:
  _t5_8 = _t5_106;

  // Generating : L[36,36] = S(h(1, 36, 31), ( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) Div G(h(1, 36, 30), L[36,36],h(1, 36, 30)) ),h(1, 36, 30))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_107 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_9, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_108 = _t5_8;

  // 4-BLAC: 1x4 / 1x4
  _t5_109 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_107), _mm256_castpd256_pd128(_t5_108)));

  // AVX Storer:
  _t5_10 = _t5_109;

  // Generating : L[36,36] = S(h(1, 36, 31), ( G(h(1, 36, 31), L[36,36],h(1, 36, 31)) - ( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) Kro T( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) ) ) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_110 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_111 = _t5_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_112 = _t5_10;

  // 4-BLAC: (4x1)^T
  _t5_113 = _t5_112;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_114 = _mm256_mul_pd(_t5_111, _t5_113);

  // 4-BLAC: 1x4 - 1x4
  _t5_115 = _mm256_sub_pd(_t5_110, _t5_114);

  // AVX Storer:
  _t5_11 = _t5_115;

  // Generating : L[36,36] = S(h(1, 36, 31), Sqrt( G(h(1, 36, 31), L[36,36],h(1, 36, 31)) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_116 = _t5_11;

  // 4-BLAC: sqrt(1x4)
  _t5_117 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_116)));

  // AVX Storer:
  _t5_11 = _t5_117;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(1, 36, 28)) Div G(h(1, 36, 28), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_118 = _t5_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_119 = _t5_0;

  // 4-BLAC: 1x4 / 1x4
  _t5_120 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_118), _mm256_castpd256_pd128(_t5_119)));

  // AVX Storer:
  _t5_12 = _t5_120;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(3, 36, 29)) - ( G(h(1, 36, 32), L[36,36],h(1, 36, 28)) Kro T( G(h(3, 36, 29), L[36,36],h(1, 36, 28)) ) ) ),h(3, 36, 29))

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_121 = _t5_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_122 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_123 = _t5_2;

  // 4-BLAC: (4x1)^T
  _t5_124 = _t5_123;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_125 = _mm256_mul_pd(_t5_122, _t5_124);

  // 4-BLAC: 1x4 - 1x4
  _t5_126 = _mm256_sub_pd(_t5_121, _t5_125);

  // AVX Storer:
  _t5_13 = _t5_126;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(1, 36, 29)) Div G(h(1, 36, 29), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_127 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_128 = _t5_3;

  // 4-BLAC: 1x4 / 1x4
  _t5_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_127), _mm256_castpd256_pd128(_t5_128)));

  // AVX Storer:
  _t5_14 = _t5_129;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(2, 36, 30)) - ( G(h(1, 36, 32), L[36,36],h(1, 36, 29)) Kro T( G(h(2, 36, 30), L[36,36],h(1, 36, 29)) ) ) ),h(2, 36, 30))

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_130 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_13, 6), _mm256_permute2f128_pd(_t5_13, _t5_13, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_131 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_132 = _t5_7;

  // 4-BLAC: (4x1)^T
  _t5_133 = _t5_132;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_134 = _mm256_mul_pd(_t5_131, _t5_133);

  // 4-BLAC: 1x4 - 1x4
  _t5_135 = _mm256_sub_pd(_t5_130, _t5_134);

  // AVX Storer:
  _t5_15 = _t5_135;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(1, 36, 30)) Div G(h(1, 36, 30), L[36,36],h(1, 36, 30)) ),h(1, 36, 30))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_136 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_137 = _t5_8;

  // 4-BLAC: 1x4 / 1x4
  _t5_138 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_136), _mm256_castpd256_pd128(_t5_137)));

  // AVX Storer:
  _t5_16 = _t5_138;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(1, 36, 31)) - ( G(h(1, 36, 32), L[36,36],h(1, 36, 30)) Kro T( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) ) ) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_139 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_15, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_140 = _t5_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_141 = _t5_10;

  // 4-BLAC: (4x1)^T
  _t5_142 = _t5_141;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_143 = _mm256_mul_pd(_t5_140, _t5_142);

  // 4-BLAC: 1x4 - 1x4
  _t5_144 = _mm256_sub_pd(_t5_139, _t5_143);

  // AVX Storer:
  _t5_17 = _t5_144;

  // Generating : L[36,36] = S(h(1, 36, 32), ( G(h(1, 36, 32), L[36,36],h(1, 36, 31)) Div G(h(1, 36, 31), L[36,36],h(1, 36, 31)) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_145 = _t5_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_146 = _t5_11;

  // 4-BLAC: 1x4 / 1x4
  _t5_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_145), _mm256_castpd256_pd128(_t5_146)));

  // AVX Storer:
  _t5_17 = _t5_147;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(1, 36, 28)) Div G(h(1, 36, 28), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_148 = _t5_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_149 = _t5_0;

  // 4-BLAC: 1x4 / 1x4
  _t5_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_148), _mm256_castpd256_pd128(_t5_149)));

  // AVX Storer:
  _t5_18 = _t5_150;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(3, 36, 29)) - ( G(h(1, 36, 33), L[36,36],h(1, 36, 28)) Kro T( G(h(3, 36, 29), L[36,36],h(1, 36, 28)) ) ) ),h(3, 36, 29))

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_151 = _t5_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_152 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_153 = _t5_2;

  // 4-BLAC: (4x1)^T
  _t5_154 = _t5_153;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_155 = _mm256_mul_pd(_t5_152, _t5_154);

  // 4-BLAC: 1x4 - 1x4
  _t5_156 = _mm256_sub_pd(_t5_151, _t5_155);

  // AVX Storer:
  _t5_19 = _t5_156;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(1, 36, 29)) Div G(h(1, 36, 29), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_157 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_158 = _t5_3;

  // 4-BLAC: 1x4 / 1x4
  _t5_159 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_157), _mm256_castpd256_pd128(_t5_158)));

  // AVX Storer:
  _t5_20 = _t5_159;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(2, 36, 30)) - ( G(h(1, 36, 33), L[36,36],h(1, 36, 29)) Kro T( G(h(2, 36, 30), L[36,36],h(1, 36, 29)) ) ) ),h(2, 36, 30))

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_160 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_19, 6), _mm256_permute2f128_pd(_t5_19, _t5_19, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_161 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_162 = _t5_7;

  // 4-BLAC: (4x1)^T
  _t5_163 = _t5_162;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_164 = _mm256_mul_pd(_t5_161, _t5_163);

  // 4-BLAC: 1x4 - 1x4
  _t5_165 = _mm256_sub_pd(_t5_160, _t5_164);

  // AVX Storer:
  _t5_21 = _t5_165;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(1, 36, 30)) Div G(h(1, 36, 30), L[36,36],h(1, 36, 30)) ),h(1, 36, 30))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_166 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_167 = _t5_8;

  // 4-BLAC: 1x4 / 1x4
  _t5_168 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_166), _mm256_castpd256_pd128(_t5_167)));

  // AVX Storer:
  _t5_22 = _t5_168;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(1, 36, 31)) - ( G(h(1, 36, 33), L[36,36],h(1, 36, 30)) Kro T( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) ) ) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_169 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_170 = _t5_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_171 = _t5_10;

  // 4-BLAC: (4x1)^T
  _t5_172 = _t5_171;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_173 = _mm256_mul_pd(_t5_170, _t5_172);

  // 4-BLAC: 1x4 - 1x4
  _t5_174 = _mm256_sub_pd(_t5_169, _t5_173);

  // AVX Storer:
  _t5_23 = _t5_174;

  // Generating : L[36,36] = S(h(1, 36, 33), ( G(h(1, 36, 33), L[36,36],h(1, 36, 31)) Div G(h(1, 36, 31), L[36,36],h(1, 36, 31)) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_175 = _t5_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_176 = _t5_11;

  // 4-BLAC: 1x4 / 1x4
  _t5_177 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_175), _mm256_castpd256_pd128(_t5_176)));

  // AVX Storer:
  _t5_23 = _t5_177;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(1, 36, 28)) Div G(h(1, 36, 28), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_178 = _t5_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_179 = _t5_0;

  // 4-BLAC: 1x4 / 1x4
  _t5_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_178), _mm256_castpd256_pd128(_t5_179)));

  // AVX Storer:
  _t5_24 = _t5_180;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(3, 36, 29)) - ( G(h(1, 36, 34), L[36,36],h(1, 36, 28)) Kro T( G(h(3, 36, 29), L[36,36],h(1, 36, 28)) ) ) ),h(3, 36, 29))

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_181 = _t5_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_182 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_24, _t5_24, 32), _mm256_permute2f128_pd(_t5_24, _t5_24, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_183 = _t5_2;

  // 4-BLAC: (4x1)^T
  _t5_184 = _t5_183;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_185 = _mm256_mul_pd(_t5_182, _t5_184);

  // 4-BLAC: 1x4 - 1x4
  _t5_186 = _mm256_sub_pd(_t5_181, _t5_185);

  // AVX Storer:
  _t5_25 = _t5_186;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(1, 36, 29)) Div G(h(1, 36, 29), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_187 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_25, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_188 = _t5_3;

  // 4-BLAC: 1x4 / 1x4
  _t5_189 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_187), _mm256_castpd256_pd128(_t5_188)));

  // AVX Storer:
  _t5_26 = _t5_189;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(2, 36, 30)) - ( G(h(1, 36, 34), L[36,36],h(1, 36, 29)) Kro T( G(h(2, 36, 30), L[36,36],h(1, 36, 29)) ) ) ),h(2, 36, 30))

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_190 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_25, 6), _mm256_permute2f128_pd(_t5_25, _t5_25, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_191 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_26, _t5_26, 32), _mm256_permute2f128_pd(_t5_26, _t5_26, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_192 = _t5_7;

  // 4-BLAC: (4x1)^T
  _t5_193 = _t5_192;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_194 = _mm256_mul_pd(_t5_191, _t5_193);

  // 4-BLAC: 1x4 - 1x4
  _t5_195 = _mm256_sub_pd(_t5_190, _t5_194);

  // AVX Storer:
  _t5_27 = _t5_195;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(1, 36, 30)) Div G(h(1, 36, 30), L[36,36],h(1, 36, 30)) ),h(1, 36, 30))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_196 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_27, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_197 = _t5_8;

  // 4-BLAC: 1x4 / 1x4
  _t5_198 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_196), _mm256_castpd256_pd128(_t5_197)));

  // AVX Storer:
  _t5_28 = _t5_198;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(1, 36, 31)) - ( G(h(1, 36, 34), L[36,36],h(1, 36, 30)) Kro T( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) ) ) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_199 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_27, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_200 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_201 = _t5_10;

  // 4-BLAC: (4x1)^T
  _t5_202 = _t5_201;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_203 = _mm256_mul_pd(_t5_200, _t5_202);

  // 4-BLAC: 1x4 - 1x4
  _t5_204 = _mm256_sub_pd(_t5_199, _t5_203);

  // AVX Storer:
  _t5_29 = _t5_204;

  // Generating : L[36,36] = S(h(1, 36, 34), ( G(h(1, 36, 34), L[36,36],h(1, 36, 31)) Div G(h(1, 36, 31), L[36,36],h(1, 36, 31)) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_205 = _t5_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_206 = _t5_11;

  // 4-BLAC: 1x4 / 1x4
  _t5_207 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_205), _mm256_castpd256_pd128(_t5_206)));

  // AVX Storer:
  _t5_29 = _t5_207;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 28)) Div G(h(1, 36, 28), L[36,36],h(1, 36, 28)) ),h(1, 36, 28))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_208 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_209 = _t5_0;

  // 4-BLAC: 1x4 / 1x4
  _t5_210 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_208), _mm256_castpd256_pd128(_t5_209)));

  // AVX Storer:
  _t5_30 = _t5_210;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(3, 36, 29)) - ( G(h(1, 36, 35), L[36,36],h(1, 36, 28)) Kro T( G(h(3, 36, 29), L[36,36],h(1, 36, 28)) ) ) ),h(3, 36, 29))

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_211 = _t5_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_212 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_30, _t5_30, 32), _mm256_permute2f128_pd(_t5_30, _t5_30, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_213 = _t5_2;

  // 4-BLAC: (4x1)^T
  _t5_214 = _t5_213;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_215 = _mm256_mul_pd(_t5_212, _t5_214);

  // 4-BLAC: 1x4 - 1x4
  _t5_216 = _mm256_sub_pd(_t5_211, _t5_215);

  // AVX Storer:
  _t5_31 = _t5_216;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 29)) Div G(h(1, 36, 29), L[36,36],h(1, 36, 29)) ),h(1, 36, 29))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_217 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_31, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_218 = _t5_3;

  // 4-BLAC: 1x4 / 1x4
  _t5_219 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_217), _mm256_castpd256_pd128(_t5_218)));

  // AVX Storer:
  _t5_32 = _t5_219;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(2, 36, 30)) - ( G(h(1, 36, 35), L[36,36],h(1, 36, 29)) Kro T( G(h(2, 36, 30), L[36,36],h(1, 36, 29)) ) ) ),h(2, 36, 30))

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_220 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_31, 6), _mm256_permute2f128_pd(_t5_31, _t5_31, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_221 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_32, _t5_32, 32), _mm256_permute2f128_pd(_t5_32, _t5_32, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_222 = _t5_7;

  // 4-BLAC: (4x1)^T
  _t5_223 = _t5_222;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_224 = _mm256_mul_pd(_t5_221, _t5_223);

  // 4-BLAC: 1x4 - 1x4
  _t5_225 = _mm256_sub_pd(_t5_220, _t5_224);

  // AVX Storer:
  _t5_33 = _t5_225;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 30)) Div G(h(1, 36, 30), L[36,36],h(1, 36, 30)) ),h(1, 36, 30))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_226 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_33, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_227 = _t5_8;

  // 4-BLAC: 1x4 / 1x4
  _t5_228 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_226), _mm256_castpd256_pd128(_t5_227)));

  // AVX Storer:
  _t5_34 = _t5_228;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 31)) - ( G(h(1, 36, 35), L[36,36],h(1, 36, 30)) Kro T( G(h(1, 36, 31), L[36,36],h(1, 36, 30)) ) ) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_229 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_33, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_230 = _t5_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_231 = _t5_10;

  // 4-BLAC: (4x1)^T
  _t5_232 = _t5_231;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_233 = _mm256_mul_pd(_t5_230, _t5_232);

  // 4-BLAC: 1x4 - 1x4
  _t5_234 = _mm256_sub_pd(_t5_229, _t5_233);

  // AVX Storer:
  _t5_35 = _t5_234;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 31)) Div G(h(1, 36, 31), L[36,36],h(1, 36, 31)) ),h(1, 36, 31))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_235 = _t5_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_236 = _t5_11;

  // 4-BLAC: 1x4 / 1x4
  _t5_237 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_235), _mm256_castpd256_pd128(_t5_236)));

  // AVX Storer:
  _t5_35 = _t5_237;

  // Generating : L[36,36] = S(h(4, 36, 32), ( G(h(4, 36, 32), L[36,36],h(4, 36, 32)) - ( G(h(4, 36, 32), L[36,36],h(4, 36, 28)) * T( G(h(4, 36, 32), L[36,36],h(4, 36, 28)) ) ) ),h(4, 36, 32))

  // AVX Loader:

  // 4x4 -> 4x4 - LowTriang
  _t5_238 = _t5_36;
  _t5_239 = _t5_37;
  _t5_240 = _t5_38;
  _t5_241 = _t5_39;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t5_301 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_12, _t5_14), _mm256_unpacklo_pd(_t5_16, _t5_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_18, _t5_20), _mm256_unpacklo_pd(_t5_22, _t5_23), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_24, _t5_26), _mm256_unpacklo_pd(_t5_28, _t5_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_30, _t5_32), _mm256_unpacklo_pd(_t5_34, _t5_35), 32)), 32);
  _t5_302 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_12, _t5_14), _mm256_unpacklo_pd(_t5_16, _t5_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_18, _t5_20), _mm256_unpacklo_pd(_t5_22, _t5_23), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_24, _t5_26), _mm256_unpacklo_pd(_t5_28, _t5_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_30, _t5_32), _mm256_unpacklo_pd(_t5_34, _t5_35), 32)), 32);
  _t5_303 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_12, _t5_14), _mm256_unpacklo_pd(_t5_16, _t5_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_18, _t5_20), _mm256_unpacklo_pd(_t5_22, _t5_23), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_24, _t5_26), _mm256_unpacklo_pd(_t5_28, _t5_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_30, _t5_32), _mm256_unpacklo_pd(_t5_34, _t5_35), 32)), 49);
  _t5_304 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_12, _t5_14), _mm256_unpacklo_pd(_t5_16, _t5_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_18, _t5_20), _mm256_unpacklo_pd(_t5_22, _t5_23), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_24, _t5_26), _mm256_unpacklo_pd(_t5_28, _t5_29), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_30, _t5_32), _mm256_unpacklo_pd(_t5_34, _t5_35), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t5_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0), _t5_301), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0), _t5_302)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_16, _t5_16, 32), _mm256_permute2f128_pd(_t5_16, _t5_16, 32), 0), _t5_303), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_17, _t5_17, 32), _mm256_permute2f128_pd(_t5_17, _t5_17, 32), 0), _t5_304)));
  _t5_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0), _t5_301), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0), _t5_302)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_22, _t5_22, 32), _mm256_permute2f128_pd(_t5_22, _t5_22, 32), 0), _t5_303), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_23, _t5_23, 32), _mm256_permute2f128_pd(_t5_23, _t5_23, 32), 0), _t5_304)));
  _t5_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_24, _t5_24, 32), _mm256_permute2f128_pd(_t5_24, _t5_24, 32), 0), _t5_301), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_26, _t5_26, 32), _mm256_permute2f128_pd(_t5_26, _t5_26, 32), 0), _t5_302)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_28, _t5_28, 32), _mm256_permute2f128_pd(_t5_28, _t5_28, 32), 0), _t5_303), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_29, _t5_29, 32), _mm256_permute2f128_pd(_t5_29, _t5_29, 32), 0), _t5_304)));
  _t5_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_30, _t5_30, 32), _mm256_permute2f128_pd(_t5_30, _t5_30, 32), 0), _t5_301), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_32, _t5_32, 32), _mm256_permute2f128_pd(_t5_32, _t5_32, 32), 0), _t5_302)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_34, _t5_34, 32), _mm256_permute2f128_pd(_t5_34, _t5_34, 32), 0), _t5_303), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_35, _t5_35, 32), _mm256_permute2f128_pd(_t5_35, _t5_35, 32), 0), _t5_304)));

  // 4-BLAC: 4x4 - 4x4
  _t5_55 = _mm256_sub_pd(_t5_238, _t5_51);
  _t5_56 = _mm256_sub_pd(_t5_239, _t5_52);
  _t5_57 = _mm256_sub_pd(_t5_240, _t5_53);
  _t5_58 = _mm256_sub_pd(_t5_241, _t5_54);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t5_36 = _t5_55;
  _t5_37 = _t5_56;
  _t5_38 = _t5_57;
  _t5_39 = _t5_58;

  // Generating : L[36,36] = S(h(1, 36, 32), Sqrt( G(h(1, 36, 32), L[36,36],h(1, 36, 32)) ),h(1, 36, 32))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_242 = _t5_36;

  // 4-BLAC: sqrt(1x4)
  _t5_243 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_242)));

  // AVX Storer:
  _t5_36 = _t5_243;

  // Generating : T1448[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, 32), L[36,36],h(1, 36, 32)) ),h(1, 36, 32))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t5_244 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_245 = _t5_36;

  // 4-BLAC: 1x4 / 1x4
  _t5_246 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_244), _mm256_castpd256_pd128(_t5_245)));

  // AVX Storer:
  _t5_40 = _t5_246;

  // Generating : L[36,36] = S(h(3, 36, 33), ( G(h(1, 1, 0), T1448[1,36],h(1, 36, 32)) Kro G(h(3, 36, 33), L[36,36],h(1, 36, 32)) ),h(1, 36, 32))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_247 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_40, _t5_40, 32), _mm256_permute2f128_pd(_t5_40, _t5_40, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_248 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_37, _t5_38), _mm256_unpacklo_pd(_t5_39, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t5_249 = _mm256_mul_pd(_t5_247, _t5_248);

  // AVX Storer:
  _t5_41 = _t5_249;

  // Generating : L[36,36] = S(h(3, 36, 33), ( G(h(3, 36, 33), L[36,36],h(3, 36, 33)) - ( G(h(3, 36, 33), L[36,36],h(1, 36, 32)) * T( G(h(3, 36, 33), L[36,36],h(1, 36, 32)) ) ) ),h(3, 36, 33))

  // AVX Loader:

  // 3x3 -> 4x4 - LowTriang
  _t5_250 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_37, 2), _mm256_setzero_pd());
  _t5_251 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_38, 6), _mm256_permute2f128_pd(_t5_38, _t5_38, 129), 5);
  _t5_252 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_39, 14), _mm256_permute2f128_pd(_t5_39, _t5_39, 129), 5);
  _t5_253 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_254 = _t5_41;

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_255 = _t5_41;

  // 4-BLAC: (4x1)^T
  _t5_256 = _t5_255;

  // 4-BLAC: 4x1 * 1x4
  _t5_257 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_254, _t5_254, 32), _mm256_permute2f128_pd(_t5_254, _t5_254, 32), 0), _t5_256);
  _t5_258 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_254, _t5_254, 32), _mm256_permute2f128_pd(_t5_254, _t5_254, 32), 15), _t5_256);
  _t5_259 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_254, _t5_254, 49), _mm256_permute2f128_pd(_t5_254, _t5_254, 49), 0), _t5_256);
  _t5_260 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_254, _t5_254, 49), _mm256_permute2f128_pd(_t5_254, _t5_254, 49), 15), _t5_256);

  // 4-BLAC: 4x4 - 4x4
  _t5_261 = _mm256_sub_pd(_t5_250, _t5_257);
  _t5_262 = _mm256_sub_pd(_t5_251, _t5_258);
  _t5_263 = _mm256_sub_pd(_t5_252, _t5_259);
  _t5_264 = _mm256_sub_pd(_t5_253, _t5_260);

  // AVX Storer:

  // 4x4 -> 3x3 - LowTriang
  _t5_42 = _t5_261;
  _t5_43 = _t5_262;
  _t5_44 = _t5_263;

  // Generating : L[36,36] = S(h(1, 36, 33), Sqrt( G(h(1, 36, 33), L[36,36],h(1, 36, 33)) ),h(1, 36, 33))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_265 = _t5_42;

  // 4-BLAC: sqrt(1x4)
  _t5_266 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_265)));

  // AVX Storer:
  _t5_42 = _t5_266;

  // Generating : T1448[1,36] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 36, 33), L[36,36],h(1, 36, 33)) ),h(1, 36, 33))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t5_267 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_268 = _t5_42;

  // 4-BLAC: 1x4 / 1x4
  _t5_269 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_267), _mm256_castpd256_pd128(_t5_268)));

  // AVX Storer:
  _t5_45 = _t5_269;

  // Generating : L[36,36] = S(h(2, 36, 34), ( G(h(1, 1, 0), T1448[1,36],h(1, 36, 33)) Kro G(h(2, 36, 34), L[36,36],h(1, 36, 33)) ),h(1, 36, 33))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_270 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_45, _t5_45, 32), _mm256_permute2f128_pd(_t5_45, _t5_45, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_271 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_43, _t5_44), _mm256_setzero_pd(), 12);

  // 4-BLAC: 1x4 Kro 4x1
  _t5_272 = _mm256_mul_pd(_t5_270, _t5_271);

  // AVX Storer:
  _t5_46 = _t5_272;

  // Generating : L[36,36] = S(h(2, 36, 34), ( G(h(2, 36, 34), L[36,36],h(2, 36, 34)) - ( G(h(2, 36, 34), L[36,36],h(1, 36, 33)) * T( G(h(2, 36, 34), L[36,36],h(1, 36, 33)) ) ) ),h(2, 36, 34))

  // AVX Loader:

  // 2x2 -> 4x4 - LowTriang
  _t5_273 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_43, 2), _mm256_setzero_pd());
  _t5_274 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_44, 6), _mm256_permute2f128_pd(_t5_44, _t5_44, 129), 5);
  _t5_275 = _mm256_setzero_pd();
  _t5_276 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_277 = _t5_46;

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_278 = _t5_46;

  // 4-BLAC: (4x1)^T
  _t5_279 = _t5_278;

  // 4-BLAC: 4x1 * 1x4
  _t5_280 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_277, _t5_277, 32), _mm256_permute2f128_pd(_t5_277, _t5_277, 32), 0), _t5_279);
  _t5_281 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_277, _t5_277, 32), _mm256_permute2f128_pd(_t5_277, _t5_277, 32), 15), _t5_279);
  _t5_282 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_277, _t5_277, 49), _mm256_permute2f128_pd(_t5_277, _t5_277, 49), 0), _t5_279);
  _t5_283 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_277, _t5_277, 49), _mm256_permute2f128_pd(_t5_277, _t5_277, 49), 15), _t5_279);

  // 4-BLAC: 4x4 - 4x4
  _t5_284 = _mm256_sub_pd(_t5_273, _t5_280);
  _t5_285 = _mm256_sub_pd(_t5_274, _t5_281);
  _t5_286 = _mm256_sub_pd(_t5_275, _t5_282);
  _t5_287 = _mm256_sub_pd(_t5_276, _t5_283);

  // AVX Storer:

  // 4x4 -> 2x2 - LowTriang
  _t5_47 = _t5_284;
  _t5_48 = _t5_285;

  // Generating : L[36,36] = S(h(1, 36, 34), Sqrt( G(h(1, 36, 34), L[36,36],h(1, 36, 34)) ),h(1, 36, 34))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_288 = _t5_47;

  // 4-BLAC: sqrt(1x4)
  _t5_289 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_288)));

  // AVX Storer:
  _t5_47 = _t5_289;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 34)) Div G(h(1, 36, 34), L[36,36],h(1, 36, 34)) ),h(1, 36, 34))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_290 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_48, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_291 = _t5_47;

  // 4-BLAC: 1x4 / 1x4
  _t5_292 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_290), _mm256_castpd256_pd128(_t5_291)));

  // AVX Storer:
  _t5_49 = _t5_292;

  // Generating : L[36,36] = S(h(1, 36, 35), ( G(h(1, 36, 35), L[36,36],h(1, 36, 35)) - ( G(h(1, 36, 35), L[36,36],h(1, 36, 34)) Kro T( G(h(1, 36, 35), L[36,36],h(1, 36, 34)) ) ) ),h(1, 36, 35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_293 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_48, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_294 = _t5_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_295 = _t5_49;

  // 4-BLAC: (4x1)^T
  _t5_296 = _t5_295;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_297 = _mm256_mul_pd(_t5_294, _t5_296);

  // 4-BLAC: 1x4 - 1x4
  _t5_298 = _mm256_sub_pd(_t5_293, _t5_297);

  // AVX Storer:
  _t5_50 = _t5_298;

  // Generating : L[36,36] = S(h(1, 36, 35), Sqrt( G(h(1, 36, 35), L[36,36],h(1, 36, 35)) ),h(1, 36, 35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_299 = _t5_50;

  // 4-BLAC: sqrt(1x4)
  _t5_300 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t5_299)));

  // AVX Storer:
  _t5_50 = _t5_300;

  _mm_store_sd(&(K[1036]), _mm256_castpd256_pd128(_t5_0));
  _mm256_maskstore_pd(K + 1072, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_2);
  _mm256_maskstore_pd(K + 1108, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t5_2, _t5_2, 1));
  _mm256_maskstore_pd(K + 1144, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t5_2, _t5_2, 129));
  _mm_store_sd(K + 1073, _mm256_castpd256_pd128(_t5_3));
  _mm256_maskstore_pd(K + 1109, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_7);
  _mm256_maskstore_pd(K + 1145, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t5_7, _t5_7, 1));
  _mm_store_sd(K + 1110, _mm256_castpd256_pd128(_t5_8));
  _mm_store_sd(&(K[1146]), _mm256_castpd256_pd128(_t5_10));
  _mm_store_sd(&(K[1147]), _mm256_castpd256_pd128(_t5_11));
  _mm_store_sd(&(K[1180]), _mm256_castpd256_pd128(_t5_12));
  _mm_store_sd(&(K[1181]), _mm256_castpd256_pd128(_t5_14));
  _mm_store_sd(&(K[1182]), _mm256_castpd256_pd128(_t5_16));
  _mm_store_sd(&(K[1183]), _mm256_castpd256_pd128(_t5_17));
  _mm_store_sd(&(K[1216]), _mm256_castpd256_pd128(_t5_18));
  _mm_store_sd(&(K[1217]), _mm256_castpd256_pd128(_t5_20));
  _mm_store_sd(&(K[1218]), _mm256_castpd256_pd128(_t5_22));
  _mm_store_sd(&(K[1219]), _mm256_castpd256_pd128(_t5_23));
  _mm_store_sd(&(K[1252]), _mm256_castpd256_pd128(_t5_24));
  _mm_store_sd(&(K[1253]), _mm256_castpd256_pd128(_t5_26));
  _mm_store_sd(&(K[1254]), _mm256_castpd256_pd128(_t5_28));
  _mm_store_sd(&(K[1255]), _mm256_castpd256_pd128(_t5_29));
  _mm_store_sd(&(K[1288]), _mm256_castpd256_pd128(_t5_30));
  _mm_store_sd(&(K[1289]), _mm256_castpd256_pd128(_t5_32));
  _mm_store_sd(&(K[1290]), _mm256_castpd256_pd128(_t5_34));
  _mm_store_sd(&(K[1291]), _mm256_castpd256_pd128(_t5_35));

  for( int fi18 = 0; fi18 <= 31; fi18+=4 ) {
    _t6_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 109)), 0);
    _t6_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 110])));
    _t6_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 37])));
    _t6_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 111])));
    _t6_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18])));
    _t6_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 74])));
    _t6_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 108)), 32);
    _t6_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi18])));
    _t6_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18])));
    _t6_8 = _mm256_maskload_pd(y + fi18 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t6_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 108)), 32);
    _t6_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 37])));
    _t6_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 109)), 0);
    _t6_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 74])));
    _t6_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 110])));
    _t6_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 111])));

    // Generating : t0[36,1] = S(h(1, 36, fi18), ( G(h(1, 36, fi18), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18), L0[36,36],h(1, 36, fi18)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_13 = _t6_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_14 = _t6_6;

    // 4-BLAC: 1x4 / 1x4
    _t6_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_13), _mm256_castpd256_pd128(_t6_14)));

    // AVX Storer:
    _t6_7 = _t6_15;

    // Generating : t0[36,1] = S(h(3, 36, fi18 + 1), ( G(h(3, 36, fi18 + 1), t0[36,1],h(1, 1, 0)) - ( G(h(3, 36, fi18 + 1), L0[36,36],h(1, 36, fi18)) Kro G(h(1, 36, fi18), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t6_16 = _t6_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t6_17 = _t6_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_7, _t6_7, 32), _mm256_permute2f128_pd(_t6_7, _t6_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t6_19 = _mm256_mul_pd(_t6_17, _t6_18);

    // 4-BLAC: 4x1 - 4x1
    _t6_20 = _mm256_sub_pd(_t6_16, _t6_19);

    // AVX Storer:
    _t6_8 = _t6_20;

    // Generating : t0[36,1] = S(h(1, 36, fi18 + 1), ( G(h(1, 36, fi18 + 1), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 1), L0[36,36],h(1, 36, fi18 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_22 = _t6_4;

    // 4-BLAC: 1x4 / 1x4
    _t6_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_21), _mm256_castpd256_pd128(_t6_22)));

    // AVX Storer:
    _t6_9 = _t6_23;

    // Generating : t0[36,1] = S(h(2, 36, fi18 + 2), ( G(h(2, 36, fi18 + 2), t0[36,1],h(1, 1, 0)) - ( G(h(2, 36, fi18 + 2), L0[36,36],h(1, 36, fi18 + 1)) Kro G(h(1, 36, fi18 + 1), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t6_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_8, 6), _mm256_permute2f128_pd(_t6_8, _t6_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t6_25 = _t6_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_9, _t6_9, 32), _mm256_permute2f128_pd(_t6_9, _t6_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t6_27 = _mm256_mul_pd(_t6_25, _t6_26);

    // 4-BLAC: 4x1 - 4x1
    _t6_28 = _mm256_sub_pd(_t6_24, _t6_27);

    // AVX Storer:
    _t6_10 = _t6_28;

    // Generating : t0[36,1] = S(h(1, 36, fi18 + 2), ( G(h(1, 36, fi18 + 2), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 2), L0[36,36],h(1, 36, fi18 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_30 = _t6_2;

    // 4-BLAC: 1x4 / 1x4
    _t6_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_29), _mm256_castpd256_pd128(_t6_30)));

    // AVX Storer:
    _t6_11 = _t6_31;

    // Generating : t0[36,1] = S(h(1, 36, fi18 + 3), ( G(h(1, 36, fi18 + 3), t0[36,1],h(1, 1, 0)) - ( G(h(1, 36, fi18 + 3), L0[36,36],h(1, 36, fi18 + 2)) Kro G(h(1, 36, fi18 + 2), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_33 = _t6_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_34 = _t6_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t6_35 = _mm256_mul_pd(_t6_33, _t6_34);

    // 4-BLAC: 1x4 - 1x4
    _t6_36 = _mm256_sub_pd(_t6_32, _t6_35);

    // AVX Storer:
    _t6_12 = _t6_36;

    // Generating : t0[36,1] = S(h(1, 36, fi18 + 3), ( G(h(1, 36, fi18 + 3), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 3), L0[36,36],h(1, 36, fi18 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_37 = _t6_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t6_38 = _t6_0;

    // 4-BLAC: 1x4 / 1x4
    _t6_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_37), _mm256_castpd256_pd128(_t6_38)));

    // AVX Storer:
    _t6_12 = _t6_39;

    // Generating : t0[36,1] = Sum_{i150} ( S(h(4, 36, fi18 + i150 + 4), ( G(h(4, 36, fi18 + i150 + 4), t0[36,1],h(1, 1, 0)) - ( G(h(4, 36, fi18 + i150 + 4), L0[36,36],h(4, 36, fi18)) * G(h(4, 36, fi18), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[fi18]), _mm256_castpd256_pd128(_t6_7));
    _mm_store_sd(&(y[fi18 + 1]), _mm256_castpd256_pd128(_t6_9));
    _mm_store_sd(&(y[fi18 + 2]), _mm256_castpd256_pd128(_t6_11));
    _mm_store_sd(&(y[fi18 + 3]), _mm256_castpd256_pd128(_t6_12));

    for( int i150 = 0; i150 <= -fi18 + 31; i150+=4 ) {
      _t7_9 = _mm256_loadu_pd(y + fi18 + i150 + 4);
      _t7_7 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 144);
      _t7_6 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 180);
      _t7_5 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 216);
      _t7_4 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 252);
      _t7_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi18])));
      _t7_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi18 + 1])));
      _t7_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi18 + 2])));
      _t7_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi18 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t7_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t7_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t7_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t7_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t7_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32)), _mm256_mul_pd(_t7_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t7_9 = _mm256_sub_pd(_t7_9, _t7_8);

      // AVX Storer:
      _mm256_storeu_pd(y + fi18 + i150 + 4, _t7_9);
    }
  }

  _t8_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[32])));
  _t8_1 = _mm256_maskload_pd(y + 33, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : t0[36,1] = S(h(1, 36, 32), ( G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_6 = _t8_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_7 = _t5_36;

  // 4-BLAC: 1x4 / 1x4
  _t8_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_6), _mm256_castpd256_pd128(_t8_7)));

  // AVX Storer:
  _t8_0 = _t8_8;

  // Generating : t0[36,1] = S(h(3, 36, 33), ( G(h(3, 36, 33), t0[36,1],h(1, 1, 0)) - ( G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) Kro G(h(1, 36, 32), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t8_9 = _t8_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t8_10 = _t5_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_0, _t8_0, 32), _mm256_permute2f128_pd(_t8_0, _t8_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t8_12 = _mm256_mul_pd(_t8_10, _t8_11);

  // 4-BLAC: 4x1 - 4x1
  _t8_13 = _mm256_sub_pd(_t8_9, _t8_12);

  // AVX Storer:
  _t8_1 = _t8_13;

  // Generating : t0[36,1] = S(h(1, 36, 33), ( G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_14 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_15 = _t5_42;

  // 4-BLAC: 1x4 / 1x4
  _t8_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_14), _mm256_castpd256_pd128(_t8_15)));

  // AVX Storer:
  _t8_2 = _t8_16;

  // Generating : t0[36,1] = S(h(2, 36, 34), ( G(h(2, 36, 34), t0[36,1],h(1, 1, 0)) - ( G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) Kro G(h(1, 36, 33), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t8_17 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_1, 6), _mm256_permute2f128_pd(_t8_1, _t8_1, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t8_18 = _t5_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_2, _t8_2, 32), _mm256_permute2f128_pd(_t8_2, _t8_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t8_20 = _mm256_mul_pd(_t8_18, _t8_19);

  // 4-BLAC: 4x1 - 4x1
  _t8_21 = _mm256_sub_pd(_t8_17, _t8_20);

  // AVX Storer:
  _t8_3 = _t8_21;

  // Generating : t0[36,1] = S(h(1, 36, 34), ( G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t8_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_23 = _t5_47;

  // 4-BLAC: 1x4 / 1x4
  _t8_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_22), _mm256_castpd256_pd128(_t8_23)));

  // AVX Storer:
  _t8_4 = _t8_24;

  // Generating : t0[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) - ( G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) Kro G(h(1, 36, 34), t0[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_25 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t8_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_26 = _t5_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_27 = _t8_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t8_28 = _mm256_mul_pd(_t8_26, _t8_27);

  // 4-BLAC: 1x4 - 1x4
  _t8_29 = _mm256_sub_pd(_t8_25, _t8_28);

  // AVX Storer:
  _t8_5 = _t8_29;

  // Generating : t0[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), t0[36,1],h(1, 1, 0)) Div G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_30 = _t8_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t8_31 = _t5_50;

  // 4-BLAC: 1x4 / 1x4
  _t8_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_30), _mm256_castpd256_pd128(_t8_31)));

  // AVX Storer:
  _t8_5 = _t8_32;

  _mm_store_sd(K + 1184, _mm256_castpd256_pd128(_t5_36));
  _mm256_maskstore_pd(K + 1220, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_41);
  _mm256_maskstore_pd(K + 1256, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t5_41, _t5_41, 1));
  _mm256_maskstore_pd(K + 1292, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t5_41, _t5_41, 129));
  _mm_store_sd(K + 1221, _mm256_castpd256_pd128(_t5_42));
  _mm256_maskstore_pd(K + 1257, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t5_46);
  _mm256_maskstore_pd(K + 1293, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t5_46, _t5_46, 1));
  _mm_store_sd(K + 1258, _mm256_castpd256_pd128(_t5_47));
  _mm_store_sd(&(K[1294]), _mm256_castpd256_pd128(_t5_49));
  _mm_store_sd(&(K[1295]), _mm256_castpd256_pd128(_t5_50));
  _mm_store_sd(&(y[32]), _mm256_castpd256_pd128(_t8_0));
  _mm_store_sd(&(y[33]), _mm256_castpd256_pd128(_t8_2));
  _mm_store_sd(&(y[34]), _mm256_castpd256_pd128(_t8_4));
  _mm_store_sd(&(y[35]), _mm256_castpd256_pd128(_t8_5));

  for( int fi18 = 0; fi18 <= 31; fi18+=4 ) {
    _t9_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi18 + 35])));
    _t9_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi18 + 1295])));
    _t9_8 = _mm256_maskload_pd(y + -fi18 + 32, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_5 = _mm256_maskload_pd(K + -37*fi18 + 1292, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi18 + 1258])));
    _t9_3 = _mm256_maskload_pd(K + -37*fi18 + 1256, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t9_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi18 + 1221])));
    _t9_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi18 + 1220])));
    _t9_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-37*fi18 + 1184])));

    // Generating : a[36,1] = S(h(1, 36, -fi18 + 35), ( G(h(1, 36, -fi18 + 35), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi18 + 35), L0[36,36],h(1, 36, -fi18 + 35)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_31 = _t9_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_32 = _t9_6;

    // 4-BLAC: 1x4 / 1x4
    _t9_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_31), _mm256_castpd256_pd128(_t9_32)));

    // AVX Storer:
    _t9_7 = _t9_33;

    // Generating : a[36,1] = S(h(3, 36, -fi18 + 32), ( G(h(3, 36, -fi18 + 32), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, -fi18 + 35), L0[36,36],h(3, 36, -fi18 + 32)) ) Kro G(h(1, 36, -fi18 + 35), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t9_34 = _t9_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_35 = _t9_5;

    // 4-BLAC: (1x4)^T
    _t9_36 = _t9_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_37 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_7, _t9_7, 32), _mm256_permute2f128_pd(_t9_7, _t9_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t9_38 = _mm256_mul_pd(_t9_36, _t9_37);

    // 4-BLAC: 4x1 - 4x1
    _t9_39 = _mm256_sub_pd(_t9_34, _t9_38);

    // AVX Storer:
    _t9_8 = _t9_39;

    // Generating : a[36,1] = S(h(1, 36, -fi18 + 34), ( G(h(1, 36, -fi18 + 34), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi18 + 34), L0[36,36],h(1, 36, -fi18 + 34)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_40 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t9_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_41 = _t9_4;

    // 4-BLAC: 1x4 / 1x4
    _t9_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_40), _mm256_castpd256_pd128(_t9_41)));

    // AVX Storer:
    _t9_9 = _t9_42;

    // Generating : a[36,1] = S(h(2, 36, -fi18 + 32), ( G(h(2, 36, -fi18 + 32), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, -fi18 + 34), L0[36,36],h(2, 36, -fi18 + 32)) ) Kro G(h(1, 36, -fi18 + 34), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t9_13 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_8, 3);

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_14 = _t9_3;

    // 4-BLAC: (1x4)^T
    _t9_15 = _t9_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_16 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_9, _t9_9, 32), _mm256_permute2f128_pd(_t9_9, _t9_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t9_17 = _mm256_mul_pd(_t9_15, _t9_16);

    // 4-BLAC: 4x1 - 4x1
    _t9_18 = _mm256_sub_pd(_t9_13, _t9_17);

    // AVX Storer:
    _t9_10 = _t9_18;

    // Generating : a[36,1] = S(h(1, 36, -fi18 + 33), ( G(h(1, 36, -fi18 + 33), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi18 + 33), L0[36,36],h(1, 36, -fi18 + 33)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_19 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_20 = _t9_2;

    // 4-BLAC: 1x4 / 1x4
    _t9_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_19), _mm256_castpd256_pd128(_t9_20)));

    // AVX Storer:
    _t9_11 = _t9_21;

    // Generating : a[36,1] = S(h(1, 36, -fi18 + 32), ( G(h(1, 36, -fi18 + 32), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, -fi18 + 33), L0[36,36],h(1, 36, -fi18 + 32)) ) Kro G(h(1, 36, -fi18 + 33), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_23 = _t9_1;

    // 4-BLAC: (4x1)^T
    _t9_24 = _t9_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_25 = _t9_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_26 = _mm256_mul_pd(_t9_24, _t9_25);

    // 4-BLAC: 1x4 - 1x4
    _t9_27 = _mm256_sub_pd(_t9_22, _t9_26);

    // AVX Storer:
    _t9_12 = _t9_27;

    // Generating : a[36,1] = S(h(1, 36, -fi18 + 32), ( G(h(1, 36, -fi18 + 32), a[36,1],h(1, 1, 0)) Div G(h(1, 36, -fi18 + 32), L0[36,36],h(1, 36, -fi18 + 32)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_28 = _t9_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_29 = _t9_0;

    // 4-BLAC: 1x4 / 1x4
    _t9_30 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_28), _mm256_castpd256_pd128(_t9_29)));

    // AVX Storer:
    _t9_12 = _t9_30;

    // Generating : a[36,1] = Sum_{i150} ( S(h(4, 36, i150), ( G(h(4, 36, i150), a[36,1],h(1, 1, 0)) - ( T( G(h(4, 36, -fi18 + 32), L0[36,36],h(4, 36, i150)) ) * G(h(4, 36, -fi18 + 32), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[-fi18 + 35]), _mm256_castpd256_pd128(_t9_7));
    _mm_store_sd(&(y[-fi18 + 34]), _mm256_castpd256_pd128(_t9_9));
    _mm_store_sd(&(y[-fi18 + 33]), _mm256_castpd256_pd128(_t9_11));
    _mm_store_sd(&(y[-fi18 + 32]), _mm256_castpd256_pd128(_t9_12));

    for( int i150 = 0; i150 <= -fi18 + 31; i150+=4 ) {
      _t10_9 = _mm256_loadu_pd(y + i150);
      _t10_7 = _mm256_loadu_pd(K + -36*fi18 + i150 + 1152);
      _t10_6 = _mm256_loadu_pd(K + -36*fi18 + i150 + 1188);
      _t10_5 = _mm256_loadu_pd(K + -36*fi18 + i150 + 1224);
      _t10_4 = _mm256_loadu_pd(K + -36*fi18 + i150 + 1260);
      _t10_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi18 + 35])));
      _t10_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi18 + 34])));
      _t10_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi18 + 33])));
      _t10_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi18 + 32])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t10_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_6), _mm256_unpacklo_pd(_t10_5, _t10_4), 32);
      _t10_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_7, _t10_6), _mm256_unpackhi_pd(_t10_5, _t10_4), 32);
      _t10_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_7, _t10_6), _mm256_unpacklo_pd(_t10_5, _t10_4), 49);
      _t10_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_7, _t10_6), _mm256_unpackhi_pd(_t10_5, _t10_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t10_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t10_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t10_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t10_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t10_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32)), _mm256_mul_pd(_t10_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_0, _t10_1), _mm256_unpacklo_pd(_t10_2, _t10_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t10_9 = _mm256_sub_pd(_t10_9, _t10_8);

      // AVX Storer:
      _mm256_storeu_pd(y + i150, _t10_9);
    }
  }

  _t11_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[3])));
  _t11_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[111])));
  _t11_8 = _mm256_maskload_pd(y, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t11_5 = _mm256_maskload_pd(K + 108, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t11_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[74])));
  _t11_3 = _mm256_maskload_pd(K + 72, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t11_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37])));
  _t11_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[36])));
  _t11_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));

  // Generating : a[36,1] = S(h(1, 36, 3), ( G(h(1, 36, 3), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 3), L0[36,36],h(1, 36, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_13 = _t11_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_14 = _t11_6;

  // 4-BLAC: 1x4 / 1x4
  _t11_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_13), _mm256_castpd256_pd128(_t11_14)));

  // AVX Storer:
  _t11_7 = _t11_15;

  // Generating : a[36,1] = S(h(3, 36, 0), ( G(h(3, 36, 0), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, 3), L0[36,36],h(3, 36, 0)) ) Kro G(h(1, 36, 3), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_16 = _t11_8;

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_17 = _t11_5;

  // 4-BLAC: (1x4)^T
  _t11_18 = _t11_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_7, _t11_7, 32), _mm256_permute2f128_pd(_t11_7, _t11_7, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t11_20 = _mm256_mul_pd(_t11_18, _t11_19);

  // 4-BLAC: 4x1 - 4x1
  _t11_21 = _mm256_sub_pd(_t11_16, _t11_20);

  // AVX Storer:
  _t11_8 = _t11_21;

  // Generating : a[36,1] = S(h(1, 36, 2), ( G(h(1, 36, 2), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 2), L0[36,36],h(1, 36, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_22 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_8, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_23 = _t11_4;

  // 4-BLAC: 1x4 / 1x4
  _t11_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_22), _mm256_castpd256_pd128(_t11_23)));

  // AVX Storer:
  _t11_9 = _t11_24;

  // Generating : a[36,1] = S(h(2, 36, 0), ( G(h(2, 36, 0), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, 2), L0[36,36],h(2, 36, 0)) ) Kro G(h(1, 36, 2), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_8, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_26 = _t11_3;

  // 4-BLAC: (1x4)^T
  _t11_27 = _t11_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_9, _t11_9, 32), _mm256_permute2f128_pd(_t11_9, _t11_9, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t11_29 = _mm256_mul_pd(_t11_27, _t11_28);

  // 4-BLAC: 4x1 - 4x1
  _t11_30 = _mm256_sub_pd(_t11_25, _t11_29);

  // AVX Storer:
  _t11_10 = _t11_30;

  // Generating : a[36,1] = S(h(1, 36, 1), ( G(h(1, 36, 1), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 1), L0[36,36],h(1, 36, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_31 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_32 = _t11_2;

  // 4-BLAC: 1x4 / 1x4
  _t11_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_31), _mm256_castpd256_pd128(_t11_32)));

  // AVX Storer:
  _t11_11 = _t11_33;

  // Generating : a[36,1] = S(h(1, 36, 0), ( G(h(1, 36, 0), a[36,1],h(1, 1, 0)) - ( T( G(h(1, 36, 1), L0[36,36],h(1, 36, 0)) ) Kro G(h(1, 36, 1), a[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_35 = _t11_1;

  // 4-BLAC: (4x1)^T
  _t11_36 = _t11_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_37 = _t11_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_38 = _mm256_mul_pd(_t11_36, _t11_37);

  // 4-BLAC: 1x4 - 1x4
  _t11_39 = _mm256_sub_pd(_t11_34, _t11_38);

  // AVX Storer:
  _t11_12 = _t11_39;

  // Generating : a[36,1] = S(h(1, 36, 0), ( G(h(1, 36, 0), a[36,1],h(1, 1, 0)) Div G(h(1, 36, 0), L0[36,36],h(1, 36, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_40 = _t11_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_41 = _t11_0;

  // 4-BLAC: 1x4 / 1x4
  _t11_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_40), _mm256_castpd256_pd128(_t11_41)));

  // AVX Storer:
  _t11_12 = _t11_42;

  // Generating : kx[36,1] = ( Sum_{j151} ( S(h(4, 36, j151), ( G(h(4, 36, j151), X[36,36],h(4, 36, 0)) * G(h(4, 36, 0), x[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) + Sum_{i150} ( Sum_{j151} ( $(h(4, 36, j151), ( G(h(4, 36, j151), X[36,36],h(4, 36, i150)) * G(h(4, 36, i150), x[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int j151 = 0; j151 <= 35; j151+=4 ) {
    _t12_4 = _mm256_loadu_pd(X + 36*j151);
    _t12_3 = _mm256_loadu_pd(X + 36*j151 + 36);
    _t12_2 = _mm256_loadu_pd(X + 36*j151 + 72);
    _t12_1 = _mm256_loadu_pd(X + 36*j151 + 108);
    _t12_0 = _mm256_loadu_pd(x);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t12_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t12_4, _t12_0), _mm256_mul_pd(_t12_3, _t12_0)), _mm256_hadd_pd(_mm256_mul_pd(_t12_2, _t12_0), _mm256_mul_pd(_t12_1, _t12_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t12_4, _t12_0), _mm256_mul_pd(_t12_3, _t12_0)), _mm256_hadd_pd(_mm256_mul_pd(_t12_2, _t12_0), _mm256_mul_pd(_t12_1, _t12_0)), 12));

    // AVX Storer:
    _mm256_storeu_pd(kx + j151, _t12_5);
  }


  for( int i150 = 4; i150 <= 35; i150+=4 ) {

    // AVX Loader:

    for( int j151 = 0; j151 <= 35; j151+=4 ) {
      _t13_4 = _mm256_loadu_pd(X + i150 + 36*j151);
      _t13_3 = _mm256_loadu_pd(X + i150 + 36*j151 + 36);
      _t13_2 = _mm256_loadu_pd(X + i150 + 36*j151 + 72);
      _t13_1 = _mm256_loadu_pd(X + i150 + 36*j151 + 108);
      _t13_0 = _mm256_loadu_pd(x + i150);
      _t13_5 = _mm256_loadu_pd(kx + j151);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t13_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t13_4, _t13_0), _mm256_mul_pd(_t13_3, _t13_0)), _mm256_hadd_pd(_mm256_mul_pd(_t13_2, _t13_0), _mm256_mul_pd(_t13_1, _t13_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t13_4, _t13_0), _mm256_mul_pd(_t13_3, _t13_0)), _mm256_hadd_pd(_mm256_mul_pd(_t13_2, _t13_0), _mm256_mul_pd(_t13_1, _t13_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t13_5 = _mm256_add_pd(_t13_5, _t13_6);

      // AVX Storer:
      _mm256_storeu_pd(kx + j151, _t13_5);
    }
  }

  _t14_0 = _mm256_loadu_pd(kx);

  // Generating : f[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 36, 0), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{i150} ( $(h(1, 1, 0), ( T( G(h(4, 36, i150), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, i150), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t14_3 = _t14_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t14_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_mul_pd(_t14_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), 129)), 1));

  // AVX Storer:
  _t14_1 = _t14_2;


  for( int i150 = 4; i150 <= 35; i150+=4 ) {
    _t15_1 = _mm256_loadu_pd(kx + i150);
    _t15_0 = _mm256_loadu_pd(y + i150);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t15_4 = _t15_1;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t15_3 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_mul_pd(_t15_4, _t15_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_mul_pd(_t15_4, _t15_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t15_4, _t15_0), _mm256_mul_pd(_t15_4, _t15_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t15_2 = _t14_1;

    // 4-BLAC: 1x4 + 1x4
    _t15_2 = _mm256_add_pd(_t15_2, _t15_3);

    // AVX Storer:
    _t14_1 = _t15_2;
  }


  for( int fi18 = 0; fi18 <= 31; fi18+=4 ) {
    _t16_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 37])));
    _t16_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 110])));
    _t16_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 108)), 32);
    _t16_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 111])));
    _t16_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 109)), 0);
    _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18])));
    _t16_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 74])));
    _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi18])));
    _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18])));
    _t16_8 = _mm256_maskload_pd(kx + fi18 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t16_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 36)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 72))), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 108)), 32);
    _t16_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 37])));
    _t16_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 73)), _mm256_castpd128_pd256(_mm_load_sd(K + 37*fi18 + 109)), 0);
    _t16_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 74])));
    _t16_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 110])));
    _t16_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[37*fi18 + 111])));

    // Generating : v[36,1] = S(h(1, 36, fi18), ( G(h(1, 36, fi18), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18), L0[36,36],h(1, 36, fi18)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_13 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_14 = _t16_6;

    // 4-BLAC: 1x4 / 1x4
    _t16_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_13), _mm256_castpd256_pd128(_t16_14)));

    // AVX Storer:
    _t16_7 = _t16_15;

    // Generating : v[36,1] = S(h(3, 36, fi18 + 1), ( G(h(3, 36, fi18 + 1), v[36,1],h(1, 1, 0)) - ( G(h(3, 36, fi18 + 1), L0[36,36],h(1, 36, fi18)) Kro G(h(1, 36, fi18), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_16 = _t16_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t16_17 = _t16_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_7, _t16_7, 32), _mm256_permute2f128_pd(_t16_7, _t16_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_19 = _mm256_mul_pd(_t16_17, _t16_18);

    // 4-BLAC: 4x1 - 4x1
    _t16_20 = _mm256_sub_pd(_t16_16, _t16_19);

    // AVX Storer:
    _t16_8 = _t16_20;

    // Generating : v[36,1] = S(h(1, 36, fi18 + 1), ( G(h(1, 36, fi18 + 1), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 1), L0[36,36],h(1, 36, fi18 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_22 = _t16_4;

    // 4-BLAC: 1x4 / 1x4
    _t16_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_21), _mm256_castpd256_pd128(_t16_22)));

    // AVX Storer:
    _t16_9 = _t16_23;

    // Generating : v[36,1] = S(h(2, 36, fi18 + 2), ( G(h(2, 36, fi18 + 2), v[36,1],h(1, 1, 0)) - ( G(h(2, 36, fi18 + 2), L0[36,36],h(1, 36, fi18 + 1)) Kro G(h(1, 36, fi18 + 1), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 6), _mm256_permute2f128_pd(_t16_8, _t16_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t16_25 = _t16_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_9, _t16_9, 32), _mm256_permute2f128_pd(_t16_9, _t16_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t16_27 = _mm256_mul_pd(_t16_25, _t16_26);

    // 4-BLAC: 4x1 - 4x1
    _t16_28 = _mm256_sub_pd(_t16_24, _t16_27);

    // AVX Storer:
    _t16_10 = _t16_28;

    // Generating : v[36,1] = S(h(1, 36, fi18 + 2), ( G(h(1, 36, fi18 + 2), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 2), L0[36,36],h(1, 36, fi18 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_30 = _t16_2;

    // 4-BLAC: 1x4 / 1x4
    _t16_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_29), _mm256_castpd256_pd128(_t16_30)));

    // AVX Storer:
    _t16_11 = _t16_31;

    // Generating : v[36,1] = S(h(1, 36, fi18 + 3), ( G(h(1, 36, fi18 + 3), v[36,1],h(1, 1, 0)) - ( G(h(1, 36, fi18 + 3), L0[36,36],h(1, 36, fi18 + 2)) Kro G(h(1, 36, fi18 + 2), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_33 = _t16_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_34 = _t16_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t16_35 = _mm256_mul_pd(_t16_33, _t16_34);

    // 4-BLAC: 1x4 - 1x4
    _t16_36 = _mm256_sub_pd(_t16_32, _t16_35);

    // AVX Storer:
    _t16_12 = _t16_36;

    // Generating : v[36,1] = S(h(1, 36, fi18 + 3), ( G(h(1, 36, fi18 + 3), v[36,1],h(1, 1, 0)) Div G(h(1, 36, fi18 + 3), L0[36,36],h(1, 36, fi18 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_37 = _t16_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_38 = _t16_0;

    // 4-BLAC: 1x4 / 1x4
    _t16_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_37), _mm256_castpd256_pd128(_t16_38)));

    // AVX Storer:
    _t16_12 = _t16_39;

    // Generating : v[36,1] = Sum_{i150} ( S(h(4, 36, fi18 + i150 + 4), ( G(h(4, 36, fi18 + i150 + 4), v[36,1],h(1, 1, 0)) - ( G(h(4, 36, fi18 + i150 + 4), L0[36,36],h(4, 36, fi18)) * G(h(4, 36, fi18), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(kx[fi18]), _mm256_castpd256_pd128(_t16_7));
    _mm_store_sd(&(kx[fi18 + 1]), _mm256_castpd256_pd128(_t16_9));
    _mm_store_sd(&(kx[fi18 + 2]), _mm256_castpd256_pd128(_t16_11));
    _mm_store_sd(&(kx[fi18 + 3]), _mm256_castpd256_pd128(_t16_12));

    for( int i150 = 0; i150 <= -fi18 + 31; i150+=4 ) {
      _t17_9 = _mm256_loadu_pd(kx + fi18 + i150 + 4);
      _t17_7 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 144);
      _t17_6 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 180);
      _t17_5 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 216);
      _t17_4 = _mm256_loadu_pd(K + 37*fi18 + 36*i150 + 252);
      _t17_3 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi18])));
      _t17_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi18 + 1])));
      _t17_1 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi18 + 2])));
      _t17_0 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi18 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t17_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t17_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t17_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t17_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t17_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32)), _mm256_mul_pd(_t17_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t17_9 = _mm256_sub_pd(_t17_9, _t17_8);

      // AVX Storer:
      _mm256_storeu_pd(kx + fi18 + i150 + 4, _t17_9);
    }
  }

  _t5_47 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1258])));
  _t5_49 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1294])));
  _t5_46 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1257)), _mm256_castpd128_pd256(_mm_load_sd(K + 1293)), 0);
  _t5_50 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1295])));
  _t5_42 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1221])));
  _t5_36 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1184])));
  _t5_41 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1220)), _mm256_castpd128_pd256(_mm_load_sd(K + 1256))), _mm256_castpd128_pd256(_mm_load_sd(K + 1292)), 32);
  _t18_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[32])));
  _t18_3 = _mm256_maskload_pd(kx + 33, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t18_1 = _mm256_loadu_pd(x);
  _t18_0 = _mm256_loadu_pd(kx);

  // Generating : v[36,1] = S(h(1, 36, 32), ( G(h(1, 36, 32), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 32), L0[36,36],h(1, 36, 32)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_9 = _t18_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_10 = _t5_36;

  // 4-BLAC: 1x4 / 1x4
  _t18_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_9), _mm256_castpd256_pd128(_t18_10)));

  // AVX Storer:
  _t18_2 = _t18_11;

  // Generating : v[36,1] = S(h(3, 36, 33), ( G(h(3, 36, 33), v[36,1],h(1, 1, 0)) - ( G(h(3, 36, 33), L0[36,36],h(1, 36, 32)) Kro G(h(1, 36, 32), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t18_12 = _t18_3;

  // AVX Loader:

  // 3x1 -> 4x1
  _t18_13 = _t5_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_2, _t18_2, 32), _mm256_permute2f128_pd(_t18_2, _t18_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t18_15 = _mm256_mul_pd(_t18_13, _t18_14);

  // 4-BLAC: 4x1 - 4x1
  _t18_16 = _mm256_sub_pd(_t18_12, _t18_15);

  // AVX Storer:
  _t18_3 = _t18_16;

  // Generating : v[36,1] = S(h(1, 36, 33), ( G(h(1, 36, 33), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 33), L0[36,36],h(1, 36, 33)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_18 = _t5_42;

  // 4-BLAC: 1x4 / 1x4
  _t18_19 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_17), _mm256_castpd256_pd128(_t18_18)));

  // AVX Storer:
  _t18_4 = _t18_19;

  // Generating : v[36,1] = S(h(2, 36, 34), ( G(h(2, 36, 34), v[36,1],h(1, 1, 0)) - ( G(h(2, 36, 34), L0[36,36],h(1, 36, 33)) Kro G(h(1, 36, 33), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t18_20 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_3, 6), _mm256_permute2f128_pd(_t18_3, _t18_3, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t18_21 = _t5_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_22 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t18_4, _t18_4, 32), _mm256_permute2f128_pd(_t18_4, _t18_4, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t18_23 = _mm256_mul_pd(_t18_21, _t18_22);

  // 4-BLAC: 4x1 - 4x1
  _t18_24 = _mm256_sub_pd(_t18_20, _t18_23);

  // AVX Storer:
  _t18_5 = _t18_24;

  // Generating : v[36,1] = S(h(1, 36, 34), ( G(h(1, 36, 34), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 34), L0[36,36],h(1, 36, 34)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_26 = _t5_47;

  // 4-BLAC: 1x4 / 1x4
  _t18_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_25), _mm256_castpd256_pd128(_t18_26)));

  // AVX Storer:
  _t18_6 = _t18_27;

  // Generating : v[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), v[36,1],h(1, 1, 0)) - ( G(h(1, 36, 35), L0[36,36],h(1, 36, 34)) Kro G(h(1, 36, 34), v[36,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_28 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_29 = _t5_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_30 = _t18_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t18_31 = _mm256_mul_pd(_t18_29, _t18_30);

  // 4-BLAC: 1x4 - 1x4
  _t18_32 = _mm256_sub_pd(_t18_28, _t18_31);

  // AVX Storer:
  _t18_7 = _t18_32;

  // Generating : v[36,1] = S(h(1, 36, 35), ( G(h(1, 36, 35), v[36,1],h(1, 1, 0)) Div G(h(1, 36, 35), L0[36,36],h(1, 36, 35)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_33 = _t18_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t18_34 = _t5_50;

  // 4-BLAC: 1x4 / 1x4
  _t18_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t18_33), _mm256_castpd256_pd128(_t18_34)));

  // AVX Storer:
  _t18_7 = _t18_35;

  // Generating : var[1,1] = ( ( S(h(1, 1, 0), ( ( T( G(h(4, 36, 0), x[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), x[36,1],h(1, 1, 0)) ) - ( T( G(h(4, 36, 0), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), kx[36,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{j151} ( $(h(1, 1, 0), ( T( G(h(4, 36, j151), x[36,1],h(1, 1, 0)) ) * G(h(4, 36, j151), x[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + Sum_{i150} ( -$(h(1, 1, 0), ( T( G(h(4, 36, i150), kx[36,1],h(1, 1, 0)) ) * G(h(4, 36, i150), kx[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t18_39 = _t18_1;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t18_36 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_mul_pd(_t18_39, _t18_1), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_mul_pd(_t18_39, _t18_1), 129)), _mm256_add_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_39, _t18_1), _mm256_mul_pd(_t18_39, _t18_1), 129)), 1));

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t18_40 = _t18_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t18_37 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_mul_pd(_t18_40, _t18_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_mul_pd(_t18_40, _t18_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t18_40, _t18_0), _mm256_mul_pd(_t18_40, _t18_0), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t18_38 = _mm256_sub_pd(_t18_36, _t18_37);

  // AVX Storer:
  _t18_8 = _t18_38;


  for( int j151 = 4; j151 <= 35; j151+=4 ) {
    _t19_0 = _mm256_loadu_pd(x + j151);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t19_3 = _t19_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t19_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_mul_pd(_t19_3, _t19_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_mul_pd(_t19_3, _t19_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t19_3, _t19_0), _mm256_mul_pd(_t19_3, _t19_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t19_1 = _t18_8;

    // 4-BLAC: 1x4 + 1x4
    _t19_1 = _mm256_add_pd(_t19_1, _t19_2);

    // AVX Storer:
    _t18_8 = _t19_1;
  }

  _mm_store_sd(&(kx[32]), _mm256_castpd256_pd128(_t18_2));
  _mm_store_sd(&(kx[33]), _mm256_castpd256_pd128(_t18_4));
  _mm_store_sd(&(kx[34]), _mm256_castpd256_pd128(_t18_6));
  _mm_store_sd(&(kx[35]), _mm256_castpd256_pd128(_t18_7));

  for( int i150 = 4; i150 <= 35; i150+=4 ) {
    _t20_0 = _mm256_loadu_pd(kx + i150);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t20_3 = _t20_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t20_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_mul_pd(_t20_3, _t20_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_mul_pd(_t20_3, _t20_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_3, _t20_0), _mm256_mul_pd(_t20_3, _t20_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_1 = _t18_8;

    // 4-BLAC: 1x4 - 1x4
    _t20_1 = _mm256_sub_pd(_t20_1, _t20_2);

    // AVX Storer:
    _t18_8 = _t20_1;
  }


  // Generating : lp[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 36, 0), y[36,1],h(1, 1, 0)) ) * G(h(4, 36, 0), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{i150} ( $(h(1, 1, 0), ( T( G(h(4, 36, i150), y[36,1],h(1, 1, 0)) ) * G(h(4, 36, i150), y[36,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t21_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32);

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t21_1 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), _mm256_mul_pd(_t21_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_12, _t11_11), _mm256_unpacklo_pd(_t11_9, _t11_7), 32)), 129)), 1));

  // AVX Storer:
  _t21_0 = _t21_1;


  for( int i150 = 4; i150 <= 35; i150+=4 ) {
    _t22_0 = _mm256_loadu_pd(y + i150);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t22_3 = _t22_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t22_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_mul_pd(_t22_3, _t22_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_mul_pd(_t22_3, _t22_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t22_3, _t22_0), _mm256_mul_pd(_t22_3, _t22_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t22_1 = _t21_0;

    // 4-BLAC: 1x4 + 1x4
    _t22_1 = _mm256_add_pd(_t22_1, _t22_2);

    // AVX Storer:
    _t21_0 = _t22_1;
  }

  _mm_store_sd(&(y[3]), _mm256_castpd256_pd128(_t11_7));
  _mm_store_sd(&(y[2]), _mm256_castpd256_pd128(_t11_9));
  _mm_store_sd(&(y[1]), _mm256_castpd256_pd128(_t11_11));
  _mm_store_sd(&(y[0]), _mm256_castpd256_pd128(_t11_12));
  _mm_store_sd(&(f[0]), _mm256_castpd256_pd128(_t14_1));
  _mm_store_sd(&(var[0]), _mm256_castpd256_pd128(_t18_8));
  _mm_store_sd(&(lp[0]), _mm256_castpd256_pd128(_t21_0));

}
