/*
 * gpr_kernel.h
 *
Decl { {u'K': Symmetric[K, (20, 20), LSMatAccess], u'L': LowerTriangular[L, (20, 20), GenMatAccess], u'var': Scalar[var, (1, 1), GenMatAccess], u'L0': LowerTriangular[L0, (20, 20), GenMatAccess], u'X': SquaredMatrix[X, (20, 20), GenMatAccess], u'a': Matrix[a, (20, 1), GenMatAccess], u'f': Scalar[f, (1, 1), GenMatAccess], u't2': Matrix[t2, (20, 1), GenMatAccess], u't0': Matrix[t0, (20, 1), GenMatAccess], u't1': Matrix[t1, (20, 1), GenMatAccess], 'T1496': Matrix[T1496, (1, 20), GenMatAccess], u'lp': Scalar[lp, (1, 1), GenMatAccess], u'v': Matrix[v, (20, 1), GenMatAccess], u'y': Matrix[y, (20, 1), GenMatAccess], u'x': Matrix[x, (20, 1), GenMatAccess], u'kx': Matrix[kx, (20, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'rdiv_ltn_ow_opt': {'m': 'm1.ll', 'n': 'n4.ll'}, 'Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt': {'m0': 'm02.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt_m02_Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt_m04_m21_rdiv_ltn_ow_opt_m1_n4'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,20],h(1, 20, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,20],h(1, 20, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), L[20,20],h(1, 20, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), L[20,20],h(1, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), L[20,20],h(1, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,20],h(1, 20, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), L[20,20],h(2, 20, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), L[20,20],h(2, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(2, 20, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), L[20,20],h(2, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) = ( Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) Div Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(3, 20, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(3, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 14:
For_{fi971;0;12;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(3, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(3, 20, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) ) Div Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(2, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(2, 20, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) ) Div Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(3, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(3, 20, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) ) Div Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(2, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(2, 20, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) ) Div Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(3, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(3, 20, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) ) Div Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(2, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(2, 20, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) ) Div Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(3, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(3, 20, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) ) Div Tile( (1, 1), G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(2, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(2, 20, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) ) Div Tile( (1, 1), G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
 )Entry 15:
For_{fi846;4;15;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi846), L[20,20],h(4, 20, fi846)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi846), K[20,20],h(4, 20, fi846)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi846), L[20,20],h(fi846, 20, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi846), L[20,20],h(fi846, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,20],h(1, 20, fi846)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,20],h(1, 20, fi846)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,20],h(1, 20, fi846 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,20],h(1, 20, fi846 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 2), L[20,20],h(2, 20, fi846)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 2), L[20,20],h(2, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(2, 20, fi846)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 2), L[20,20],h(2, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(3, 20, fi846)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(3, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi846 + 16, 20, fi846 + 4), L[20,20],h(4, 20, fi846)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi846 + 16, 20, fi846 + 4), K[20,20],h(4, 20, fi846)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi846 + 16, 20, fi846 + 4), L[20,20],h(fi846, 20, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi846), L[20,20],h(fi846, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 16:
For_{fi1090;0;-fi846 + 12;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846)) ) Div Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(3, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(3, 20, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(2, 20, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(2, 20, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 4), L[20,20],h(1, 20, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846)) ) Div Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(3, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(3, 20, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(2, 20, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(2, 20, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 5), L[20,20],h(1, 20, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846)) ) Div Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(3, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(3, 20, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(2, 20, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(2, 20, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 6), L[20,20],h(1, 20, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846)) ) Div Tile( (1, 1), G(h(1, 20, fi846), L[20,20],h(1, 20, fi846)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(3, 20, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(3, 20, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi846 + 1), L[20,20],h(1, 20, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 1), L[20,20],h(1, 20, fi846 + 1)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(2, 20, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(2, 20, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 2), L[20,20],h(1, 20, fi846 + 2)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi1090 + fi846 + 7), L[20,20],h(1, 20, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi846 + 3), L[20,20],h(1, 20, fi846 + 3)) ) )
Eq.ann: {}
 ) )Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 20, 16), L[20,20],h(4, 20, 16)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, 16), K[20,20],h(4, 20, 16)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, 16), L[20,20],h(16, 20, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, 16), L[20,20],h(16, 20, 0)) ) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 20, 16), L[20,20],h(1, 20, 16)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 16), L[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,20],h(1, 20, 16)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 16), L[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), L[20,20],h(1, 20, 16)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,20],h(1, 20, 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), L[20,20],h(1, 20, 16)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), L[20,20],h(1, 20, 16)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), L[20,20],h(1, 20, 16)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L[20,20],h(1, 20, 17)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L[20,20],h(1, 20, 17)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L[20,20],h(1, 20, 16)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), L[20,20],h(1, 20, 16)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,20],h(1, 20, 17)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L[20,20],h(1, 20, 17)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,20],h(1, 20, 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L[20,20],h(1, 20, 17)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), L[20,20],h(2, 20, 16)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), L[20,20],h(2, 20, 16)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(1, 20, 18)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(1, 20, 18)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(2, 20, 16)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), L[20,20],h(2, 20, 16)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 20, 19), L[20,20],h(1, 20, 18)) ) = ( Tile( (1, 1), G(h(1, 20, 19), L[20,20],h(1, 20, 18)) ) Div Tile( (1, 1), G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(1, 20, 19)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(1, 20, 19)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(3, 20, 16)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L[20,20],h(3, 20, 16)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 20, 19), L[20,20],h(1, 20, 19)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 19), L[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 31:
For_{fi1209;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, fi1209), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1209), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1209), L0[20,20],h(1, 20, fi1209)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1209 + 1), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1209 + 1), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1209 + 1), L0[20,20],h(1, 20, fi1209)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1209), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, fi1209 + 1), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1209 + 1), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1209 + 1), L0[20,20],h(1, 20, fi1209 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1209 + 2), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1209 + 2), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1209 + 2), L0[20,20],h(1, 20, fi1209 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1209 + 1), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, fi1209 + 2), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1209 + 2), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1209 + 2), L0[20,20],h(1, 20, fi1209 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1209 + 3), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1209 + 3), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1209 + 3), L0[20,20],h(1, 20, fi1209 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1209 + 2), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, fi1209 + 3), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1209 + 3), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1209 + 3), L0[20,20],h(1, 20, fi1209 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1209 + 16, 20, fi1209 + 4), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1209 + 16, 20, fi1209 + 4), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1209 + 16, 20, fi1209 + 4), L0[20,20],h(4, 20, fi1209)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1209), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 32:
Eq: Tile( (1, 1), G(h(1, 20, 16), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 16), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 16), L0[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), L0[20,20],h(1, 20, 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 20, 17), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 17), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 17), L0[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L0[20,20],h(1, 20, 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 20, 18), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 18), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 18), L0[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), t0[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), t0[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L0[20,20],h(1, 20, 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), t0[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 20, 19), t0[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 19), t0[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 19), L0[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 39:
For_{fi1286;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, -fi1286 + 19), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1286 + 19), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1286 + 19), L0[20,20],h(1, 20, -fi1286 + 19)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 19), L0[20,20],h(3, 20, -fi1286 + 16)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 19), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, -fi1286 + 18), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1286 + 18), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1286 + 18), L0[20,20],h(1, 20, -fi1286 + 18)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 18), L0[20,20],h(2, 20, -fi1286 + 16)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 18), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, -fi1286 + 17), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1286 + 17), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1286 + 17), L0[20,20],h(1, 20, -fi1286 + 17)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 17), L0[20,20],h(1, 20, -fi1286 + 16)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1286 + 17), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1286 + 16), L0[20,20],h(1, 20, -fi1286 + 16)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1286 + 16, 20, 0), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1286 + 16, 20, 0), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, -fi1286 + 16), L0[20,20],h(-fi1286 + 16, 20, 0)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, -fi1286 + 16), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 40:
Eq: Tile( (1, 1), G(h(1, 20, 3), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 3), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 3), L0[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), L0[20,20],h(3, 20, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 20, 2), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 2), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 2), L0[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), L0[20,20],h(2, 20, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 20, 1), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 1), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 1), L0[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), a[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), a[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), L0[20,20],h(1, 20, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), a[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), G(h(1, 20, 0), a[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 0), a[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), L0[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), Tile( (4, 4), kx[20,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), X[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), f[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), kx[20,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) )
Eq.ann: {}
Entry 49:
For_{fi1363;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, fi1363), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1363), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1363), L0[20,20],h(1, 20, fi1363)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1363 + 1), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1363 + 1), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1363 + 1), L0[20,20],h(1, 20, fi1363)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1363), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, fi1363 + 1), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1363 + 1), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1363 + 1), L0[20,20],h(1, 20, fi1363 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1363 + 2), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1363 + 2), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1363 + 2), L0[20,20],h(1, 20, fi1363 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1363 + 1), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, fi1363 + 2), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1363 + 2), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1363 + 2), L0[20,20],h(1, 20, fi1363 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1363 + 3), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1363 + 3), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1363 + 3), L0[20,20],h(1, 20, fi1363 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1363 + 2), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, fi1363 + 3), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1363 + 3), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1363 + 3), L0[20,20],h(1, 20, fi1363 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1363 + 16, 20, fi1363 + 4), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1363 + 16, 20, fi1363 + 4), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1363 + 16, 20, fi1363 + 4), L0[20,20],h(4, 20, fi1363)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1363), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 50:
Eq: Tile( (1, 1), G(h(1, 20, 16), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 16), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 16), L0[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), L0[20,20],h(1, 20, 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), G(h(1, 20, 17), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 17), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 17), L0[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), L0[20,20],h(1, 20, 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), G(h(1, 20, 18), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 18), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 18), L0[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), v[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), v[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), L0[20,20],h(1, 20, 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), v[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 56:
Eq: Tile( (1, 1), G(h(1, 20, 19), v[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 19), v[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 19), L0[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), var[1,1] ) ) = ( ( T( Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), kx[20,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), kx[20,1] ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), Tile( (4, 4), lp[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) )
Eq.ann: {}
 *
 * Created on: 2017-09-02
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 20
#define PARAM1 20

#define ERRTHRESH 1e-5

#define SOFTERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * X, double const * x, double * K, double * y, double * kx, double * f, double * var, double * lp)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27, _t1_28, _t1_29, _t1_30, _t1_31,
	_t1_32, _t1_33, _t1_34, _t1_35, _t1_36, _t1_37, _t1_38, _t1_39,
	_t1_40, _t1_41, _t1_42, _t1_43, _t1_44, _t1_45, _t1_46, _t1_47,
	_t1_48, _t1_49, _t1_50, _t1_51, _t1_52, _t1_53, _t1_54, _t1_55,
	_t1_56, _t1_57, _t1_58, _t1_59, _t1_60, _t1_61, _t1_62, _t1_63,
	_t1_64, _t1_65, _t1_66, _t1_67, _t1_68, _t1_69, _t1_70, _t1_71,
	_t1_72, _t1_73, _t1_74, _t1_75, _t1_76, _t1_77, _t1_78, _t1_79,
	_t1_80, _t1_81, _t1_82, _t1_83, _t1_84, _t1_85, _t1_86, _t1_87,
	_t1_88, _t1_89, _t1_90, _t1_91, _t1_92, _t1_93, _t1_94, _t1_95,
	_t1_96, _t1_97, _t1_98, _t1_99, _t1_100, _t1_101, _t1_102, _t1_103,
	_t1_104, _t1_105, _t1_106, _t1_107, _t1_108, _t1_109, _t1_110, _t1_111,
	_t1_112, _t1_113, _t1_114, _t1_115, _t1_116, _t1_117, _t1_118, _t1_119;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60, _t5_61, _t5_62, _t5_63,
	_t5_64, _t5_65, _t5_66, _t5_67, _t5_68, _t5_69, _t5_70, _t5_71,
	_t5_72, _t5_73, _t5_74, _t5_75, _t5_76, _t5_77, _t5_78, _t5_79,
	_t5_80, _t5_81, _t5_82, _t5_83, _t5_84, _t5_85, _t5_86, _t5_87,
	_t5_88, _t5_89, _t5_90, _t5_91, _t5_92, _t5_93, _t5_94, _t5_95,
	_t5_96, _t5_97, _t5_98, _t5_99, _t5_100, _t5_101, _t5_102, _t5_103,
	_t5_104, _t5_105, _t5_106, _t5_107, _t5_108, _t5_109, _t5_110, _t5_111,
	_t5_112, _t5_113, _t5_114, _t5_115, _t5_116, _t5_117, _t5_118, _t5_119;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39,
	_t6_40, _t6_41, _t6_42, _t6_43, _t6_44, _t6_45, _t6_46, _t6_47,
	_t6_48, _t6_49, _t6_50, _t6_51, _t6_52, _t6_53, _t6_54, _t6_55,
	_t6_56, _t6_57, _t6_58, _t6_59, _t6_60, _t6_61, _t6_62, _t6_63,
	_t6_64, _t6_65, _t6_66, _t6_67, _t6_68, _t6_69, _t6_70, _t6_71,
	_t6_72, _t6_73, _t6_74, _t6_75, _t6_76, _t6_77, _t6_78, _t6_79,
	_t6_80, _t6_81, _t6_82, _t6_83, _t6_84, _t6_85, _t6_86, _t6_87,
	_t6_88, _t6_89, _t6_90, _t6_91, _t6_92, _t6_93, _t6_94, _t6_95,
	_t6_96, _t6_97, _t6_98, _t6_99, _t6_100, _t6_101, _t6_102, _t6_103,
	_t6_104, _t6_105, _t6_106, _t6_107, _t6_108, _t6_109, _t6_110, _t6_111,
	_t6_112, _t6_113, _t6_114, _t6_115, _t6_116;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27, _t10_28, _t10_29, _t10_30, _t10_31,
	_t10_32, _t10_33, _t10_34, _t10_35, _t10_36, _t10_37, _t10_38, _t10_39,
	_t10_40, _t10_41, _t10_42, _t10_43, _t10_44, _t10_45, _t10_46, _t10_47,
	_t10_48, _t10_49, _t10_50, _t10_51, _t10_52, _t10_53, _t10_54, _t10_55,
	_t10_56, _t10_57, _t10_58, _t10_59, _t10_60, _t10_61, _t10_62, _t10_63,
	_t10_64, _t10_65, _t10_66, _t10_67, _t10_68, _t10_69, _t10_70, _t10_71,
	_t10_72, _t10_73, _t10_74, _t10_75, _t10_76, _t10_77, _t10_78, _t10_79,
	_t10_80, _t10_81, _t10_82, _t10_83, _t10_84, _t10_85, _t10_86, _t10_87,
	_t10_88, _t10_89, _t10_90, _t10_91, _t10_92, _t10_93, _t10_94, _t10_95,
	_t10_96, _t10_97, _t10_98, _t10_99, _t10_100, _t10_101, _t10_102, _t10_103,
	_t10_104, _t10_105, _t10_106, _t10_107, _t10_108, _t10_109, _t10_110, _t10_111,
	_t10_112, _t10_113, _t10_114, _t10_115, _t10_116, _t10_117, _t10_118, _t10_119,
	_t10_120, _t10_121, _t10_122, _t10_123, _t10_124, _t10_125, _t10_126, _t10_127,
	_t10_128, _t10_129, _t10_130, _t10_131, _t10_132, _t10_133, _t10_134, _t10_135,
	_t10_136, _t10_137, _t10_138, _t10_139, _t10_140, _t10_141, _t10_142, _t10_143,
	_t10_144, _t10_145, _t10_146, _t10_147, _t10_148, _t10_149, _t10_150, _t10_151,
	_t10_152, _t10_153, _t10_154, _t10_155, _t10_156, _t10_157, _t10_158, _t10_159,
	_t10_160, _t10_161, _t10_162, _t10_163, _t10_164, _t10_165, _t10_166, _t10_167,
	_t10_168, _t10_169, _t10_170, _t10_171, _t10_172, _t10_173, _t10_174, _t10_175,
	_t10_176, _t10_177, _t10_178, _t10_179, _t10_180, _t10_181, _t10_182, _t10_183,
	_t10_184, _t10_185, _t10_186, _t10_187, _t10_188, _t10_189, _t10_190, _t10_191,
	_t10_192, _t10_193, _t10_194, _t10_195, _t10_196, _t10_197, _t10_198, _t10_199,
	_t10_200, _t10_201, _t10_202, _t10_203, _t10_204, _t10_205, _t10_206, _t10_207,
	_t10_208, _t10_209, _t10_210, _t10_211, _t10_212, _t10_213, _t10_214, _t10_215,
	_t10_216, _t10_217, _t10_218, _t10_219, _t10_220, _t10_221, _t10_222, _t10_223,
	_t10_224, _t10_225, _t10_226, _t10_227, _t10_228, _t10_229, _t10_230, _t10_231,
	_t10_232, _t10_233, _t10_234, _t10_235, _t10_236, _t10_237, _t10_238, _t10_239,
	_t10_240, _t10_241, _t10_242, _t10_243, _t10_244, _t10_245, _t10_246, _t10_247,
	_t10_248, _t10_249, _t10_250, _t10_251, _t10_252, _t10_253, _t10_254, _t10_255,
	_t10_256, _t10_257, _t10_258, _t10_259, _t10_260, _t10_261, _t10_262, _t10_263,
	_t10_264, _t10_265, _t10_266, _t10_267, _t10_268, _t10_269, _t10_270, _t10_271,
	_t10_272, _t10_273, _t10_274, _t10_275, _t10_276, _t10_277, _t10_278, _t10_279,
	_t10_280, _t10_281, _t10_282, _t10_283, _t10_284, _t10_285, _t10_286, _t10_287,
	_t10_288, _t10_289, _t10_290, _t10_291;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23,
	_t12_24, _t12_25, _t12_26, _t12_27, _t12_28, _t12_29, _t12_30, _t12_31,
	_t12_32, _t12_33, _t12_34, _t12_35, _t12_36, _t12_37, _t12_38, _t12_39,
	_t12_40, _t12_41, _t12_42, _t12_43, _t12_44, _t12_45, _t12_46, _t12_47,
	_t12_48, _t12_49, _t12_50, _t12_51, _t12_52, _t12_53, _t12_54, _t12_55,
	_t12_56, _t12_57, _t12_58, _t12_59, _t12_60, _t12_61, _t12_62, _t12_63,
	_t12_64, _t12_65, _t12_66, _t12_67, _t12_68, _t12_69, _t12_70, _t12_71,
	_t12_72, _t12_73, _t12_74;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35, _t14_36, _t14_37, _t14_38, _t14_39,
	_t14_40, _t14_41, _t14_42, _t14_43, _t14_44, _t14_45, _t14_46, _t14_47,
	_t14_48, _t14_49, _t14_50, _t14_51, _t14_52, _t14_53, _t14_54, _t14_55,
	_t14_56, _t14_57, _t14_58, _t14_59, _t14_60, _t14_61, _t14_62, _t14_63,
	_t14_64, _t14_65, _t14_66, _t14_67, _t14_68, _t14_69, _t14_70, _t14_71,
	_t14_72, _t14_73, _t14_74, _t14_75, _t14_76, _t14_77, _t14_78, _t14_79,
	_t14_80, _t14_81, _t14_82, _t14_83, _t14_84, _t14_85, _t14_86, _t14_87,
	_t14_88, _t14_89, _t14_90, _t14_91, _t14_92, _t14_93, _t14_94, _t14_95,
	_t14_96, _t14_97, _t14_98, _t14_99, _t14_100, _t14_101, _t14_102, _t14_103,
	_t14_104, _t14_105, _t14_106, _t14_107, _t14_108, _t14_109, _t14_110, _t14_111,
	_t14_112, _t14_113, _t14_114, _t14_115, _t14_116, _t14_117, _t14_118, _t14_119,
	_t14_120, _t14_121, _t14_122, _t14_123, _t14_124, _t14_125, _t14_126, _t14_127,
	_t14_128, _t14_129, _t14_130, _t14_131, _t14_132, _t14_133, _t14_134, _t14_135,
	_t14_136, _t14_137, _t14_138, _t14_139, _t14_140, _t14_141, _t14_142, _t14_143;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35, _t16_36, _t16_37, _t16_38, _t16_39,
	_t16_40, _t16_41, _t16_42, _t16_43, _t16_44, _t16_45, _t16_46, _t16_47,
	_t16_48, _t16_49, _t16_50, _t16_51, _t16_52, _t16_53, _t16_54, _t16_55,
	_t16_56, _t16_57, _t16_58, _t16_59, _t16_60;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23,
	_t17_24, _t17_25, _t17_26, _t17_27, _t17_28, _t17_29, _t17_30, _t17_31,
	_t17_32, _t17_33, _t17_34, _t17_35, _t17_36, _t17_37, _t17_38, _t17_39;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31,
	_t19_32;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31,
	_t20_32, _t20_33, _t20_34, _t20_35, _t20_36, _t20_37, _t20_38, _t20_39,
	_t20_40, _t20_41, _t20_42;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7,
	_t21_8, _t21_9, _t21_10, _t21_11, _t21_12, _t21_13;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19, _t22_20, _t22_21, _t22_22, _t22_23,
	_t22_24, _t22_25, _t22_26, _t22_27, _t22_28, _t22_29, _t22_30, _t22_31,
	_t22_32, _t22_33, _t22_34, _t22_35;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11, _t27_12, _t27_13, _t27_14, _t27_15,
	_t27_16, _t27_17, _t27_18, _t27_19, _t27_20, _t27_21, _t27_22, _t27_23,
	_t27_24, _t27_25, _t27_26, _t27_27, _t27_28, _t27_29, _t27_30, _t27_31,
	_t27_32, _t27_33, _t27_34, _t27_35, _t27_36, _t27_37, _t27_38, _t27_39;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7,
	_t28_8, _t28_9;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31,
	_t29_32, _t29_33, _t29_34, _t29_35, _t29_36, _t29_37, _t29_38, _t29_39,
	_t29_40;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3;
  __m256d _t32_0, _t32_1, _t32_2;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3;

  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));
  _t0_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 20)), _mm256_castpd128_pd256(_mm_load_sd(K + 40))), _mm256_castpd128_pd256(_mm_load_sd(K + 60)), 32);
  _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21])));
  _t0_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 41)), _mm256_castpd128_pd256(_mm_load_sd(K + 61)), 0);
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[42])));
  _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[62])));
  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(K[63])));
  _t0_9 = _mm256_castpd128_pd256(_mm_load_sd(&(K[80])));
  _t0_10 = _mm256_maskload_pd(K + 81, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_15 = _mm256_castpd128_pd256(_mm_load_sd(&(K[100])));
  _t0_16 = _mm256_maskload_pd(K + 101, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_21 = _mm256_castpd128_pd256(_mm_load_sd(&(K[120])));
  _t0_22 = _mm256_maskload_pd(K + 121, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_27 = _mm256_castpd128_pd256(_mm_load_sd(&(K[140])));
  _t0_28 = _mm256_maskload_pd(K + 141, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : L[20,20] = S(h(1, 20, 0), Sqrt( G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_145 = _t0_0;

  // 4-BLAC: sqrt(1x4)
  _t0_155 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_145)));

  // AVX Storer:
  _t0_0 = _t0_155;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_169 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_175 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_181 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_169), _mm256_castpd256_pd128(_t0_175)));

  // AVX Storer:
  _t0_1 = _t0_181;

  // Generating : L[20,20] = S(h(3, 20, 1), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, 0)) Kro G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_182 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_183 = _t0_2;

  // 4-BLAC: 1x4 Kro 4x1
  _t0_184 = _mm256_mul_pd(_t0_182, _t0_183);

  // AVX Storer:
  _t0_2 = _t0_184;

  // Generating : L[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), L[20,20],h(1, 20, 1)) - ( G(h(1, 20, 1), L[20,20],h(1, 20, 0)) Kro T( G(h(1, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_185 = _t0_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_186 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_187 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_188 = _t0_187;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_189 = _mm256_mul_pd(_t0_186, _t0_188);

  // 4-BLAC: 1x4 - 1x4
  _t0_190 = _mm256_sub_pd(_t0_185, _t0_189);

  // AVX Storer:
  _t0_3 = _t0_190;

  // Generating : L[20,20] = S(h(1, 20, 1), Sqrt( G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_191 = _t0_3;

  // 4-BLAC: sqrt(1x4)
  _t0_192 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_191)));

  // AVX Storer:
  _t0_3 = _t0_192;

  // Generating : L[20,20] = S(h(2, 20, 2), ( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) - ( G(h(2, 20, 2), L[20,20],h(1, 20, 0)) Kro T( G(h(1, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(1, 20, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_193 = _t0_4;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_194 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 2), _mm256_permute2f128_pd(_t0_2, _t0_2, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_195 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_2, _t0_2, 32), _mm256_permute2f128_pd(_t0_2, _t0_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_196 = _t0_195;

  // 4-BLAC: 4x1 Kro 1x4
  _t0_197 = _mm256_mul_pd(_t0_194, _t0_196);

  // 4-BLAC: 4x1 - 4x1
  _t0_198 = _mm256_sub_pd(_t0_193, _t0_197);

  // AVX Storer:
  _t0_4 = _t0_198;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_199 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_200 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_201 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_199), _mm256_castpd256_pd128(_t0_200)));

  // AVX Storer:
  _t0_5 = _t0_201;

  // Generating : L[20,20] = S(h(2, 20, 2), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, 1)) Kro G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_202 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_5, _t0_5, 32), _mm256_permute2f128_pd(_t0_5, _t0_5, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_203 = _t0_4;

  // 4-BLAC: 1x4 Kro 4x1
  _t0_204 = _mm256_mul_pd(_t0_202, _t0_203);

  // AVX Storer:
  _t0_4 = _t0_204;

  // Generating : L[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), L[20,20],h(1, 20, 2)) - ( G(h(1, 20, 2), L[20,20],h(2, 20, 0)) * T( G(h(1, 20, 2), L[20,20],h(2, 20, 0)) ) ) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_205 = _t0_6;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_33 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_35 = _t0_34;

  // 4-BLAC: 1x4 * 4x1
  _t0_36 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_mul_pd(_t0_33, _t0_35), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_mul_pd(_t0_33, _t0_35), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_mul_pd(_t0_33, _t0_35), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_37 = _mm256_sub_pd(_t0_205, _t0_36);

  // AVX Storer:
  _t0_6 = _t0_37;

  // Generating : L[20,20] = S(h(1, 20, 2), Sqrt( G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_38 = _t0_6;

  // 4-BLAC: sqrt(1x4)
  _t0_39 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_38)));

  // AVX Storer:
  _t0_6 = _t0_39;

  // Generating : L[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) - ( G(h(1, 20, 3), L[20,20],h(2, 20, 0)) * T( G(h(1, 20, 2), L[20,20],h(2, 20, 0)) ) ) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_40 = _t0_7;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_41 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_2, 129), _t0_4, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_42 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_43 = _t0_42;

  // 4-BLAC: 1x4 * 4x1
  _t0_44 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_mul_pd(_t0_41, _t0_43), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_mul_pd(_t0_41, _t0_43), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_mul_pd(_t0_41, _t0_43), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_45 = _mm256_sub_pd(_t0_40, _t0_44);

  // AVX Storer:
  _t0_7 = _t0_45;

  // Generating : L[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_46 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_47 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_46), _mm256_castpd256_pd128(_t0_47)));

  // AVX Storer:
  _t0_7 = _t0_48;

  // Generating : L[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, 3), L[20,20],h(3, 20, 0)) * T( G(h(1, 20, 3), L[20,20],h(3, 20, 0)) ) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_49 = _t0_8;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_50 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_51 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // 4-BLAC: (1x4)^T
  _t0_52 = _t0_51;

  // 4-BLAC: 1x4 * 4x1
  _t0_53 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_mul_pd(_t0_50, _t0_52), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_mul_pd(_t0_50, _t0_52), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_mul_pd(_t0_50, _t0_52), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_54 = _mm256_sub_pd(_t0_49, _t0_53);

  // AVX Storer:
  _t0_8 = _t0_54;

  // Generating : L[20,20] = S(h(1, 20, 3), Sqrt( G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_55 = _t0_8;

  // 4-BLAC: sqrt(1x4)
  _t0_56 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_55)));

  // AVX Storer:
  _t0_8 = _t0_56;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_57 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_58 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_59 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_57), _mm256_castpd256_pd128(_t0_58)));

  // AVX Storer:
  _t0_9 = _t0_59;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_60 = _t0_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_62 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_63 = _t0_62;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_64 = _mm256_mul_pd(_t0_61, _t0_63);

  // 4-BLAC: 1x4 - 1x4
  _t0_65 = _mm256_sub_pd(_t0_60, _t0_64);

  // AVX Storer:
  _t0_10 = _t0_65;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_66 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_67 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_66), _mm256_castpd256_pd128(_t0_67)));

  // AVX Storer:
  _t0_11 = _t0_68;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_69 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_10, 6), _mm256_permute2f128_pd(_t0_10, _t0_10, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_70 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_11, _t0_11, 32), _mm256_permute2f128_pd(_t0_11, _t0_11, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_71 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_72 = _t0_71;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_73 = _mm256_mul_pd(_t0_70, _t0_72);

  // 4-BLAC: 1x4 - 1x4
  _t0_74 = _mm256_sub_pd(_t0_69, _t0_73);

  // AVX Storer:
  _t0_12 = _t0_74;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_75 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_12, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_76 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_75), _mm256_castpd256_pd128(_t0_76)));

  // AVX Storer:
  _t0_13 = _t0_77;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_78 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_12, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_79 = _t0_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_80 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_81 = _t0_80;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_82 = _mm256_mul_pd(_t0_79, _t0_81);

  // 4-BLAC: 1x4 - 1x4
  _t0_83 = _mm256_sub_pd(_t0_78, _t0_82);

  // AVX Storer:
  _t0_14 = _t0_83;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_84 = _t0_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_85 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_86 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_84), _mm256_castpd256_pd128(_t0_85)));

  // AVX Storer:
  _t0_14 = _t0_86;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_87 = _t0_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_88 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_89 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_87), _mm256_castpd256_pd128(_t0_88)));

  // AVX Storer:
  _t0_15 = _t0_89;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_90 = _t0_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_91 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_92 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_93 = _t0_92;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_94 = _mm256_mul_pd(_t0_91, _t0_93);

  // 4-BLAC: 1x4 - 1x4
  _t0_95 = _mm256_sub_pd(_t0_90, _t0_94);

  // AVX Storer:
  _t0_16 = _t0_95;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_96 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_97 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_96), _mm256_castpd256_pd128(_t0_97)));

  // AVX Storer:
  _t0_17 = _t0_98;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_99 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 6), _mm256_permute2f128_pd(_t0_16, _t0_16, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_101 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_102 = _t0_101;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_103 = _mm256_mul_pd(_t0_100, _t0_102);

  // 4-BLAC: 1x4 - 1x4
  _t0_104 = _mm256_sub_pd(_t0_99, _t0_103);

  // AVX Storer:
  _t0_18 = _t0_104;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_105 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_106 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_105), _mm256_castpd256_pd128(_t0_106)));

  // AVX Storer:
  _t0_19 = _t0_107;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_108 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_109 = _t0_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_110 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_111 = _t0_110;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_112 = _mm256_mul_pd(_t0_109, _t0_111);

  // 4-BLAC: 1x4 - 1x4
  _t0_113 = _mm256_sub_pd(_t0_108, _t0_112);

  // AVX Storer:
  _t0_20 = _t0_113;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_114 = _t0_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_115 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_114), _mm256_castpd256_pd128(_t0_115)));

  // AVX Storer:
  _t0_20 = _t0_116;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_117 = _t0_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_118 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_117), _mm256_castpd256_pd128(_t0_118)));

  // AVX Storer:
  _t0_21 = _t0_119;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_120 = _t0_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_121 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_21, _t0_21, 32), _mm256_permute2f128_pd(_t0_21, _t0_21, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_122 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_123 = _t0_122;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_124 = _mm256_mul_pd(_t0_121, _t0_123);

  // 4-BLAC: 1x4 - 1x4
  _t0_125 = _mm256_sub_pd(_t0_120, _t0_124);

  // AVX Storer:
  _t0_22 = _t0_125;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_22, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_127 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_128 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_126), _mm256_castpd256_pd128(_t0_127)));

  // AVX Storer:
  _t0_23 = _t0_128;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_129 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_22, 6), _mm256_permute2f128_pd(_t0_22, _t0_22, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_130 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_23, _t0_23, 32), _mm256_permute2f128_pd(_t0_23, _t0_23, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_131 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_132 = _t0_131;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_133 = _mm256_mul_pd(_t0_130, _t0_132);

  // 4-BLAC: 1x4 - 1x4
  _t0_134 = _mm256_sub_pd(_t0_129, _t0_133);

  // AVX Storer:
  _t0_24 = _t0_134;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_135 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_24, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_136 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_135), _mm256_castpd256_pd128(_t0_136)));

  // AVX Storer:
  _t0_25 = _t0_137;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_138 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_24, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_139 = _t0_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_140 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_141 = _t0_140;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_142 = _mm256_mul_pd(_t0_139, _t0_141);

  // 4-BLAC: 1x4 - 1x4
  _t0_143 = _mm256_sub_pd(_t0_138, _t0_142);

  // AVX Storer:
  _t0_26 = _t0_143;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_144 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_146 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_144), _mm256_castpd256_pd128(_t0_146)));

  // AVX Storer:
  _t0_26 = _t0_147;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_148 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_149 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_148), _mm256_castpd256_pd128(_t0_149)));

  // AVX Storer:
  _t0_27 = _t0_150;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_151 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_152 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_27, _t0_27, 32), _mm256_permute2f128_pd(_t0_27, _t0_27, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_153 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_154 = _t0_153;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_156 = _mm256_mul_pd(_t0_152, _t0_154);

  // 4-BLAC: 1x4 - 1x4
  _t0_157 = _mm256_sub_pd(_t0_151, _t0_156);

  // AVX Storer:
  _t0_28 = _t0_157;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_158 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_28, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_159 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_160 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_158), _mm256_castpd256_pd128(_t0_159)));

  // AVX Storer:
  _t0_29 = _t0_160;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_161 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_28, 6), _mm256_permute2f128_pd(_t0_28, _t0_28, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_162 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_29, _t0_29, 32), _mm256_permute2f128_pd(_t0_29, _t0_29, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_163 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_164 = _t0_163;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_165 = _mm256_mul_pd(_t0_162, _t0_164);

  // 4-BLAC: 1x4 - 1x4
  _t0_166 = _mm256_sub_pd(_t0_161, _t0_165);

  // AVX Storer:
  _t0_30 = _t0_166;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_167 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_30, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_168 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_170 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_167), _mm256_castpd256_pd128(_t0_168)));

  // AVX Storer:
  _t0_31 = _t0_170;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_171 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_30, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_172 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_173 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_174 = _t0_173;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_176 = _mm256_mul_pd(_t0_172, _t0_174);

  // 4-BLAC: 1x4 - 1x4
  _t0_177 = _mm256_sub_pd(_t0_171, _t0_176);

  // AVX Storer:
  _t0_32 = _t0_177;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_178 = _t0_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_179 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_178), _mm256_castpd256_pd128(_t0_179)));

  // AVX Storer:
  _t0_32 = _t0_180;


  for( int fi971 = 4; fi971 <= 12; fi971+=4 ) {
    _t1_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi971 + 80])));
    _t1_1 = _mm256_maskload_pd(K + 20*fi971 + 81, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t1_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi971 + 100])));
    _t1_7 = _mm256_maskload_pd(K + 20*fi971 + 101, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t1_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi971 + 120])));
    _t1_13 = _mm256_maskload_pd(K + 20*fi971 + 121, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t1_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi971 + 140])));
    _t1_19 = _mm256_maskload_pd(K + 20*fi971 + 141, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_24 = _t1_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_25 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_24), _mm256_castpd256_pd128(_t1_25)));

    // AVX Storer:
    _t1_0 = _t1_26;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_27 = _t1_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_0, _t1_0, 32), _mm256_permute2f128_pd(_t1_0, _t1_0, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_29 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_63 = _t1_29;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_64 = _mm256_mul_pd(_t1_28, _t0_63);

    // 4-BLAC: 1x4 - 1x4
    _t1_30 = _mm256_sub_pd(_t1_27, _t0_64);

    // AVX Storer:
    _t1_1 = _t1_30;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_32 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_31), _mm256_castpd256_pd128(_t1_32)));

    // AVX Storer:
    _t1_2 = _t1_33;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_1, 6), _mm256_permute2f128_pd(_t1_1, _t1_1, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_2, _t1_2, 32), _mm256_permute2f128_pd(_t1_2, _t1_2, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_36 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_72 = _t1_36;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_73 = _mm256_mul_pd(_t1_35, _t0_72);

    // 4-BLAC: 1x4 - 1x4
    _t1_37 = _mm256_sub_pd(_t1_34, _t0_73);

    // AVX Storer:
    _t1_3 = _t1_37;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_3, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_39 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_38), _mm256_castpd256_pd128(_t1_39)));

    // AVX Storer:
    _t1_4 = _t1_40;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_41 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_3, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_42 = _t1_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_43 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_81 = _t1_43;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_82 = _mm256_mul_pd(_t1_42, _t0_81);

    // 4-BLAC: 1x4 - 1x4
    _t1_44 = _mm256_sub_pd(_t1_41, _t0_82);

    // AVX Storer:
    _t1_5 = _t1_44;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 4), ( G(h(1, 20, fi971 + 4), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_45 = _t1_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_46 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_45), _mm256_castpd256_pd128(_t1_46)));

    // AVX Storer:
    _t1_5 = _t1_47;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_48 = _t1_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_49 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_48), _mm256_castpd256_pd128(_t1_49)));

    // AVX Storer:
    _t1_6 = _t1_50;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_51 = _t1_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_6, _t1_6, 32), _mm256_permute2f128_pd(_t1_6, _t1_6, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_53 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_93 = _t1_53;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_94 = _mm256_mul_pd(_t1_52, _t0_93);

    // 4-BLAC: 1x4 - 1x4
    _t1_54 = _mm256_sub_pd(_t1_51, _t0_94);

    // AVX Storer:
    _t1_7 = _t1_54;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_55 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_56 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_55), _mm256_castpd256_pd128(_t1_56)));

    // AVX Storer:
    _t1_8 = _t1_57;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 6), _mm256_permute2f128_pd(_t1_7, _t1_7, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_8, _t1_8, 32), _mm256_permute2f128_pd(_t1_8, _t1_8, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_60 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_102 = _t1_60;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_103 = _mm256_mul_pd(_t1_59, _t0_102);

    // 4-BLAC: 1x4 - 1x4
    _t1_61 = _mm256_sub_pd(_t1_58, _t0_103);

    // AVX Storer:
    _t1_9 = _t1_61;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_63 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_62), _mm256_castpd256_pd128(_t1_63)));

    // AVX Storer:
    _t1_10 = _t1_64;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_65 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_66 = _t1_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_67 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_111 = _t1_67;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_112 = _mm256_mul_pd(_t1_66, _t0_111);

    // 4-BLAC: 1x4 - 1x4
    _t1_68 = _mm256_sub_pd(_t1_65, _t0_112);

    // AVX Storer:
    _t1_11 = _t1_68;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 5), ( G(h(1, 20, fi971 + 5), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_69 = _t1_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_70 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_69), _mm256_castpd256_pd128(_t1_70)));

    // AVX Storer:
    _t1_11 = _t1_71;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_72 = _t1_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_73 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_72), _mm256_castpd256_pd128(_t1_73)));

    // AVX Storer:
    _t1_12 = _t1_74;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_75 = _t1_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_12, _t1_12, 32), _mm256_permute2f128_pd(_t1_12, _t1_12, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_77 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_123 = _t1_77;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_124 = _mm256_mul_pd(_t1_76, _t0_123);

    // 4-BLAC: 1x4 - 1x4
    _t1_78 = _mm256_sub_pd(_t1_75, _t0_124);

    // AVX Storer:
    _t1_13 = _t1_78;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_80 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_79), _mm256_castpd256_pd128(_t1_80)));

    // AVX Storer:
    _t1_14 = _t1_81;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_82 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_13, 6), _mm256_permute2f128_pd(_t1_13, _t1_13, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_14, _t1_14, 32), _mm256_permute2f128_pd(_t1_14, _t1_14, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_84 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_132 = _t1_84;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_133 = _mm256_mul_pd(_t1_83, _t0_132);

    // 4-BLAC: 1x4 - 1x4
    _t1_85 = _mm256_sub_pd(_t1_82, _t0_133);

    // AVX Storer:
    _t1_15 = _t1_85;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_87 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_86), _mm256_castpd256_pd128(_t1_87)));

    // AVX Storer:
    _t1_16 = _t1_88;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_15, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_90 = _t1_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_91 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_141 = _t1_91;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_142 = _mm256_mul_pd(_t1_90, _t0_141);

    // 4-BLAC: 1x4 - 1x4
    _t1_92 = _mm256_sub_pd(_t1_89, _t0_142);

    // AVX Storer:
    _t1_17 = _t1_92;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 6), ( G(h(1, 20, fi971 + 6), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_93 = _t1_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_94 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_93), _mm256_castpd256_pd128(_t1_94)));

    // AVX Storer:
    _t1_17 = _t1_95;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) Div G(h(1, 20, 0), L[20,20],h(1, 20, 0)) ),h(1, 20, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_96 = _t1_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_97 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_96), _mm256_castpd256_pd128(_t1_97)));

    // AVX Storer:
    _t1_18 = _t1_98;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(3, 20, 1)) - ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 0)) Kro T( G(h(3, 20, 1), L[20,20],h(1, 20, 0)) ) ) ),h(3, 20, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_99 = _t1_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_18, _t1_18, 32), _mm256_permute2f128_pd(_t1_18, _t1_18, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_101 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_154 = _t1_101;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_156 = _mm256_mul_pd(_t1_100, _t0_154);

    // 4-BLAC: 1x4 - 1x4
    _t1_102 = _mm256_sub_pd(_t1_99, _t0_156);

    // AVX Storer:
    _t1_19 = _t1_102;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) Div G(h(1, 20, 1), L[20,20],h(1, 20, 1)) ),h(1, 20, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_104 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_103), _mm256_castpd256_pd128(_t1_104)));

    // AVX Storer:
    _t1_20 = _t1_105;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(2, 20, 2)) - ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 1)) Kro T( G(h(2, 20, 2), L[20,20],h(1, 20, 1)) ) ) ),h(2, 20, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_106 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_19, 6), _mm256_permute2f128_pd(_t1_19, _t1_19, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_20, _t1_20, 32), _mm256_permute2f128_pd(_t1_20, _t1_20, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_108 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_164 = _t1_108;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_165 = _mm256_mul_pd(_t1_107, _t0_164);

    // 4-BLAC: 1x4 - 1x4
    _t1_109 = _mm256_sub_pd(_t1_106, _t0_165);

    // AVX Storer:
    _t1_21 = _t1_109;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) Div G(h(1, 20, 2), L[20,20],h(1, 20, 2)) ),h(1, 20, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_111 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_110), _mm256_castpd256_pd128(_t1_111)));

    // AVX Storer:
    _t1_22 = _t1_112;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) - ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 2)) Kro T( G(h(1, 20, 3), L[20,20],h(1, 20, 2)) ) ) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_113 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_21, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_114 = _t1_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_115 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_174 = _t1_115;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_176 = _mm256_mul_pd(_t1_114, _t0_174);

    // 4-BLAC: 1x4 - 1x4
    _t1_116 = _mm256_sub_pd(_t1_113, _t0_176);

    // AVX Storer:
    _t1_23 = _t1_116;

    // Generating : L[20,20] = S(h(1, 20, fi971 + 7), ( G(h(1, 20, fi971 + 7), L[20,20],h(1, 20, 3)) Div G(h(1, 20, 3), L[20,20],h(1, 20, 3)) ),h(1, 20, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_117 = _t1_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_118 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_117), _mm256_castpd256_pd128(_t1_118)));

    // AVX Storer:
    _t1_23 = _t1_119;
    _mm_store_sd(&(K[20*fi971 + 80]), _mm256_castpd256_pd128(_t1_0));
    _mm_store_sd(&(K[20*fi971 + 81]), _mm256_castpd256_pd128(_t1_2));
    _mm_store_sd(&(K[20*fi971 + 82]), _mm256_castpd256_pd128(_t1_4));
    _mm_store_sd(&(K[20*fi971 + 83]), _mm256_castpd256_pd128(_t1_5));
    _mm_store_sd(&(K[20*fi971 + 100]), _mm256_castpd256_pd128(_t1_6));
    _mm_store_sd(&(K[20*fi971 + 101]), _mm256_castpd256_pd128(_t1_8));
    _mm_store_sd(&(K[20*fi971 + 102]), _mm256_castpd256_pd128(_t1_10));
    _mm_store_sd(&(K[20*fi971 + 103]), _mm256_castpd256_pd128(_t1_11));
    _mm_store_sd(&(K[20*fi971 + 120]), _mm256_castpd256_pd128(_t1_12));
    _mm_store_sd(&(K[20*fi971 + 121]), _mm256_castpd256_pd128(_t1_14));
    _mm_store_sd(&(K[20*fi971 + 122]), _mm256_castpd256_pd128(_t1_16));
    _mm_store_sd(&(K[20*fi971 + 123]), _mm256_castpd256_pd128(_t1_17));
    _mm_store_sd(&(K[20*fi971 + 140]), _mm256_castpd256_pd128(_t1_18));
    _mm_store_sd(&(K[20*fi971 + 141]), _mm256_castpd256_pd128(_t1_20));
    _mm_store_sd(&(K[20*fi971 + 142]), _mm256_castpd256_pd128(_t1_22));
    _mm_store_sd(&(K[20*fi971 + 143]), _mm256_castpd256_pd128(_t1_23));
  }

  _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(K + 84));
  _t2_1 = _mm256_maskload_pd(K + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t2_2 = _mm256_maskload_pd(K + 124, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t2_3 = _mm256_loadu_pd(K + 144);

  // Generating : L[20,20] = S(h(4, 20, 4), ( G(h(4, 20, 4), K[20,20],h(4, 20, 4)) - ( G(h(4, 20, 4), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, 4), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t2_20 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_0, _t2_1, 0), _mm256_shuffle_pd(_t2_2, _t2_3, 0), 32);
  _t2_21 = _mm256_permute2f128_pd(_t2_1, _mm256_shuffle_pd(_t2_2, _t2_3, 3), 32);
  _t2_22 = _mm256_blend_pd(_t2_2, _mm256_shuffle_pd(_t2_2, _t2_3, 3), 12);
  _t2_23 = _t2_3;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t2_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_82 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_83 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);
  _t2_84 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t2_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_11, _t0_11, 32), _mm256_permute2f128_pd(_t0_11, _t0_11, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_14, _t0_14, 32), _mm256_permute2f128_pd(_t0_14, _t0_14, 32), 0), _t2_84)));
  _t2_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_19, _t0_19, 32), _mm256_permute2f128_pd(_t0_19, _t0_19, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_20, _t0_20, 32), _mm256_permute2f128_pd(_t0_20, _t0_20, 32), 0), _t2_84)));
  _t2_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_21, _t0_21, 32), _mm256_permute2f128_pd(_t0_21, _t0_21, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_23, _t0_23, 32), _mm256_permute2f128_pd(_t0_23, _t0_23, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_25, _t0_25, 32), _mm256_permute2f128_pd(_t0_25, _t0_25, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_26, _t0_26, 32), _mm256_permute2f128_pd(_t0_26, _t0_26, 32), 0), _t2_84)));
  _t2_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_27, _t0_27, 32), _mm256_permute2f128_pd(_t0_27, _t0_27, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_29, _t0_29, 32), _mm256_permute2f128_pd(_t0_29, _t0_29, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_31, _t0_31, 32), _mm256_permute2f128_pd(_t0_31, _t0_31, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 32), _mm256_permute2f128_pd(_t0_32, _t0_32, 32), 0), _t2_84)));

  // 4-BLAC: 4x4 - 4x4
  _t2_16 = _mm256_sub_pd(_t2_20, _t2_12);
  _t2_17 = _mm256_sub_pd(_t2_21, _t2_13);
  _t2_18 = _mm256_sub_pd(_t2_22, _t2_14);
  _t2_19 = _mm256_sub_pd(_t2_23, _t2_15);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t2_0 = _t2_16;
  _t2_1 = _t2_17;
  _t2_2 = _t2_18;
  _t2_3 = _t2_19;

  // Generating : L[20,20] = S(h(1, 20, 4), Sqrt( G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_24 = _t2_0;

  // 4-BLAC: sqrt(1x4)
  _t2_25 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_24)));

  // AVX Storer:
  _t2_0 = _t2_25;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t2_26 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_27 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t2_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_26), _mm256_castpd256_pd128(_t2_27)));

  // AVX Storer:
  _t2_4 = _t2_28;

  // Generating : L[20,20] = S(h(3, 20, 5), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, 4)) Kro G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_29 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_4, _t2_4, 32), _mm256_permute2f128_pd(_t2_4, _t2_4, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t2_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_1, _t2_2), _mm256_unpacklo_pd(_t2_3, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t2_31 = _mm256_mul_pd(_t2_29, _t2_30);

  // AVX Storer:
  _t2_5 = _t2_31;

  // Generating : L[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), L[20,20],h(1, 20, 5)) - ( G(h(1, 20, 5), L[20,20],h(1, 20, 4)) Kro T( G(h(1, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_1, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_33 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_5, 1);

  // 4-BLAC: (4x1)^T
  _t2_35 = _t2_34;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_36 = _mm256_mul_pd(_t2_33, _t2_35);

  // 4-BLAC: 1x4 - 1x4
  _t2_37 = _mm256_sub_pd(_t2_32, _t2_36);

  // AVX Storer:
  _t2_6 = _t2_37;

  // Generating : L[20,20] = S(h(1, 20, 5), Sqrt( G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_38 = _t2_6;

  // 4-BLAC: sqrt(1x4)
  _t2_39 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_38)));

  // AVX Storer:
  _t2_6 = _t2_39;

  // Generating : L[20,20] = S(h(2, 20, 6), ( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) - ( G(h(2, 20, 6), L[20,20],h(1, 20, 4)) Kro T( G(h(1, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(1, 20, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_40 = _mm256_unpackhi_pd(_mm256_blend_pd(_t2_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_3, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_5, 2), _mm256_permute2f128_pd(_t2_5, _t2_5, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 32), _mm256_permute2f128_pd(_t2_5, _t2_5, 32), 0);

  // 4-BLAC: (4x1)^T
  _t2_43 = _t2_42;

  // 4-BLAC: 4x1 Kro 1x4
  _t2_44 = _mm256_mul_pd(_t2_41, _t2_43);

  // 4-BLAC: 4x1 - 4x1
  _t2_45 = _mm256_sub_pd(_t2_40, _t2_44);

  // AVX Storer:
  _t2_7 = _t2_45;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t2_46 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_47 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t2_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_46), _mm256_castpd256_pd128(_t2_47)));

  // AVX Storer:
  _t2_8 = _t2_48;

  // Generating : L[20,20] = S(h(2, 20, 6), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, 5)) Kro G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_8, _t2_8, 32), _mm256_permute2f128_pd(_t2_8, _t2_8, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_50 = _t2_7;

  // 4-BLAC: 1x4 Kro 4x1
  _t2_51 = _mm256_mul_pd(_t2_49, _t2_50);

  // AVX Storer:
  _t2_7 = _t2_51;

  // Generating : L[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), L[20,20],h(1, 20, 6)) - ( G(h(1, 20, 6), L[20,20],h(2, 20, 4)) * T( G(h(1, 20, 6), L[20,20],h(2, 20, 4)) ) ) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_52 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_2, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_2, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_53 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_5, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_7, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_54 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_5, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_7, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t2_55 = _t2_54;

  // 4-BLAC: 1x4 * 4x1
  _t2_56 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_mul_pd(_t2_53, _t2_55), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_mul_pd(_t2_53, _t2_55), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_mul_pd(_t2_53, _t2_55), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_57 = _mm256_sub_pd(_t2_52, _t2_56);

  // AVX Storer:
  _t2_9 = _t2_57;

  // Generating : L[20,20] = S(h(1, 20, 6), Sqrt( G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_58 = _t2_9;

  // 4-BLAC: sqrt(1x4)
  _t2_59 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_58)));

  // AVX Storer:
  _t2_9 = _t2_59;

  // Generating : L[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) - ( G(h(1, 20, 7), L[20,20],h(2, 20, 4)) * T( G(h(1, 20, 6), L[20,20],h(2, 20, 4)) ) ) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_60 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_3, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_3, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_61 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 129), _t2_7, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_62 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_5, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_7, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t2_63 = _t2_62;

  // 4-BLAC: 1x4 * 4x1
  _t2_64 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_mul_pd(_t2_61, _t2_63), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_mul_pd(_t2_61, _t2_63), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_mul_pd(_t2_61, _t2_63), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_65 = _mm256_sub_pd(_t2_60, _t2_64);

  // AVX Storer:
  _t2_10 = _t2_65;

  // Generating : L[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_66 = _t2_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_67 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t2_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_66), _mm256_castpd256_pd128(_t2_67)));

  // AVX Storer:
  _t2_10 = _t2_68;

  // Generating : L[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, 7), L[20,20],h(3, 20, 4)) * T( G(h(1, 20, 7), L[20,20],h(3, 20, 4)) ) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_69 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_3, _t2_3, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_70 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_5, _t2_10, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t2_7, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_71 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_5, _t2_10, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t2_7, 2), 10);

  // 4-BLAC: (1x4)^T
  _t2_72 = _t2_71;

  // 4-BLAC: 1x4 * 4x1
  _t2_73 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_mul_pd(_t2_70, _t2_72), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_mul_pd(_t2_70, _t2_72), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_mul_pd(_t2_70, _t2_72), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_74 = _mm256_sub_pd(_t2_69, _t2_73);

  // AVX Storer:
  _t2_11 = _t2_74;

  // Generating : L[20,20] = S(h(1, 20, 7), Sqrt( G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_75 = _t2_11;

  // 4-BLAC: sqrt(1x4)
  _t2_76 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_75)));

  // AVX Storer:
  _t2_11 = _t2_76;

  // Generating : L[20,20] = Sum_{k207} ( S(h(4, 20, k207 + 8), ( G(h(4, 20, k207 + 8), K[20,20],h(4, 20, 4)) - ( G(h(4, 20, k207 + 8), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, 4), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, 4)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t2_77 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_78 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_79 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);
  _t2_80 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);


  for( int k207 = 0; k207 <= 11; k207+=4 ) {
    _t3_20 = _mm256_loadu_pd(K + 20*k207 + 164);
    _t3_21 = _mm256_loadu_pd(K + 20*k207 + 184);
    _t3_22 = _mm256_loadu_pd(K + 20*k207 + 204);
    _t3_23 = _mm256_loadu_pd(K + 20*k207 + 224);
    _t3_15 = _mm256_broadcast_sd(K + 20*k207 + 160);
    _t3_14 = _mm256_broadcast_sd(K + 20*k207 + 161);
    _t3_13 = _mm256_broadcast_sd(K + 20*k207 + 162);
    _t3_12 = _mm256_broadcast_sd(K + 20*k207 + 163);
    _t3_11 = _mm256_broadcast_sd(K + 20*k207 + 180);
    _t3_10 = _mm256_broadcast_sd(K + 20*k207 + 181);
    _t3_9 = _mm256_broadcast_sd(K + 20*k207 + 182);
    _t3_8 = _mm256_broadcast_sd(K + 20*k207 + 183);
    _t3_7 = _mm256_broadcast_sd(K + 20*k207 + 200);
    _t3_6 = _mm256_broadcast_sd(K + 20*k207 + 201);
    _t3_5 = _mm256_broadcast_sd(K + 20*k207 + 202);
    _t3_4 = _mm256_broadcast_sd(K + 20*k207 + 203);
    _t3_3 = _mm256_broadcast_sd(K + 20*k207 + 220);
    _t3_2 = _mm256_broadcast_sd(K + 20*k207 + 221);
    _t3_1 = _mm256_broadcast_sd(K + 20*k207 + 222);
    _t3_0 = _mm256_broadcast_sd(K + 20*k207 + 223);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t3_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t2_77), _mm256_mul_pd(_t3_14, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t2_79), _mm256_mul_pd(_t3_12, _t2_80)));
    _t3_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t2_77), _mm256_mul_pd(_t3_10, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t2_79), _mm256_mul_pd(_t3_8, _t2_80)));
    _t3_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t2_77), _mm256_mul_pd(_t3_6, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t2_79), _mm256_mul_pd(_t3_4, _t2_80)));
    _t3_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _t2_77), _mm256_mul_pd(_t3_2, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_1, _t2_79), _mm256_mul_pd(_t3_0, _t2_80)));

    // 4-BLAC: 4x4 - 4x4
    _t3_20 = _mm256_sub_pd(_t3_20, _t3_16);
    _t3_21 = _mm256_sub_pd(_t3_21, _t3_17);
    _t3_22 = _mm256_sub_pd(_t3_22, _t3_18);
    _t3_23 = _mm256_sub_pd(_t3_23, _t3_19);

    // AVX Storer:
    _mm256_storeu_pd(K + 20*k207 + 164, _t3_20);
    _mm256_storeu_pd(K + 20*k207 + 184, _t3_21);
    _mm256_storeu_pd(K + 20*k207 + 204, _t3_22);
    _mm256_storeu_pd(K + 20*k207 + 224, _t3_23);
  }

  _t4_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[164])));
  _t4_1 = _mm256_maskload_pd(K + 165, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[184])));
  _t4_7 = _mm256_maskload_pd(K + 185, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[204])));
  _t4_13 = _mm256_maskload_pd(K + 205, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[224])));
  _t4_19 = _mm256_maskload_pd(K + 225, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_59 = _t4_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_60 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_59), _mm256_castpd256_pd128(_t4_60)));

  // AVX Storer:
  _t4_0 = _t4_61;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_62 = _t4_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_63 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_0, _t4_0, 32), _mm256_permute2f128_pd(_t4_0, _t4_0, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_64 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_65 = _t4_64;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_66 = _mm256_mul_pd(_t4_63, _t4_65);

  // 4-BLAC: 1x4 - 1x4
  _t4_67 = _mm256_sub_pd(_t4_62, _t4_66);

  // AVX Storer:
  _t4_1 = _t4_67;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_68 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_69 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_68), _mm256_castpd256_pd128(_t4_69)));

  // AVX Storer:
  _t4_2 = _t4_70;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_71 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_1, 6), _mm256_permute2f128_pd(_t4_1, _t4_1, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_72 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_2, _t4_2, 32), _mm256_permute2f128_pd(_t4_2, _t4_2, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_73 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_74 = _t4_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_75 = _mm256_mul_pd(_t4_72, _t4_74);

  // 4-BLAC: 1x4 - 1x4
  _t4_76 = _mm256_sub_pd(_t4_71, _t4_75);

  // AVX Storer:
  _t4_3 = _t4_76;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_77 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_78 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_77), _mm256_castpd256_pd128(_t4_78)));

  // AVX Storer:
  _t4_4 = _t4_79;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_80 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_81 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_82 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_83 = _t4_82;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_84 = _mm256_mul_pd(_t4_81, _t4_83);

  // 4-BLAC: 1x4 - 1x4
  _t4_85 = _mm256_sub_pd(_t4_80, _t4_84);

  // AVX Storer:
  _t4_5 = _t4_85;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_86 = _t4_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_87 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_86), _mm256_castpd256_pd128(_t4_87)));

  // AVX Storer:
  _t4_5 = _t4_88;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_89 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_90 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_89), _mm256_castpd256_pd128(_t4_90)));

  // AVX Storer:
  _t4_6 = _t4_91;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_92 = _t4_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_93 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_6, _t4_6, 32), _mm256_permute2f128_pd(_t4_6, _t4_6, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_94 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_95 = _t4_94;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_96 = _mm256_mul_pd(_t4_93, _t4_95);

  // 4-BLAC: 1x4 - 1x4
  _t4_97 = _mm256_sub_pd(_t4_92, _t4_96);

  // AVX Storer:
  _t4_7 = _t4_97;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_98 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_99 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_98), _mm256_castpd256_pd128(_t4_99)));

  // AVX Storer:
  _t4_8 = _t4_100;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_101 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_7, 6), _mm256_permute2f128_pd(_t4_7, _t4_7, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_102 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_8, _t4_8, 32), _mm256_permute2f128_pd(_t4_8, _t4_8, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_103 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_104 = _t4_103;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_105 = _mm256_mul_pd(_t4_102, _t4_104);

  // 4-BLAC: 1x4 - 1x4
  _t4_106 = _mm256_sub_pd(_t4_101, _t4_105);

  // AVX Storer:
  _t4_9 = _t4_106;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_107 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_9, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_108 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_109 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_107), _mm256_castpd256_pd128(_t4_108)));

  // AVX Storer:
  _t4_10 = _t4_109;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_110 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_111 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_112 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_113 = _t4_112;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_114 = _mm256_mul_pd(_t4_111, _t4_113);

  // 4-BLAC: 1x4 - 1x4
  _t4_115 = _mm256_sub_pd(_t4_110, _t4_114);

  // AVX Storer:
  _t4_11 = _t4_115;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_116 = _t4_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_117 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_116), _mm256_castpd256_pd128(_t4_117)));

  // AVX Storer:
  _t4_11 = _t4_118;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_119 = _t4_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_120 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_119), _mm256_castpd256_pd128(_t4_120)));

  // AVX Storer:
  _t4_12 = _t4_121;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_122 = _t4_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_123 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_12, _t4_12, 32), _mm256_permute2f128_pd(_t4_12, _t4_12, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_124 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_125 = _t4_124;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_126 = _mm256_mul_pd(_t4_123, _t4_125);

  // 4-BLAC: 1x4 - 1x4
  _t4_127 = _mm256_sub_pd(_t4_122, _t4_126);

  // AVX Storer:
  _t4_13 = _t4_127;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_128 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_129 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_130 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_128), _mm256_castpd256_pd128(_t4_129)));

  // AVX Storer:
  _t4_14 = _t4_130;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_131 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_13, 6), _mm256_permute2f128_pd(_t4_13, _t4_13, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_132 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_14, _t4_14, 32), _mm256_permute2f128_pd(_t4_14, _t4_14, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_133 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_134 = _t4_133;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_135 = _mm256_mul_pd(_t4_132, _t4_134);

  // 4-BLAC: 1x4 - 1x4
  _t4_136 = _mm256_sub_pd(_t4_131, _t4_135);

  // AVX Storer:
  _t4_15 = _t4_136;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_137 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_138 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_139 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_137), _mm256_castpd256_pd128(_t4_138)));

  // AVX Storer:
  _t4_16 = _t4_139;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_140 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_15, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_141 = _t4_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_142 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_143 = _t4_142;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_24 = _mm256_mul_pd(_t4_141, _t4_143);

  // 4-BLAC: 1x4 - 1x4
  _t4_25 = _mm256_sub_pd(_t4_140, _t4_24);

  // AVX Storer:
  _t4_17 = _t4_25;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_26 = _t4_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_27 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_26), _mm256_castpd256_pd128(_t4_27)));

  // AVX Storer:
  _t4_17 = _t4_28;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_29 = _t4_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_30 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_29), _mm256_castpd256_pd128(_t4_30)));

  // AVX Storer:
  _t4_18 = _t4_31;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_32 = _t4_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_18, _t4_18, 32), _mm256_permute2f128_pd(_t4_18, _t4_18, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_34 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_35 = _t4_34;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_36 = _mm256_mul_pd(_t4_33, _t4_35);

  // 4-BLAC: 1x4 - 1x4
  _t4_37 = _mm256_sub_pd(_t4_32, _t4_36);

  // AVX Storer:
  _t4_19 = _t4_37;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_39 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_38), _mm256_castpd256_pd128(_t4_39)));

  // AVX Storer:
  _t4_20 = _t4_40;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_19, 6), _mm256_permute2f128_pd(_t4_19, _t4_19, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_20, _t4_20, 32), _mm256_permute2f128_pd(_t4_20, _t4_20, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_43 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_44 = _t4_43;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_45 = _mm256_mul_pd(_t4_42, _t4_44);

  // 4-BLAC: 1x4 - 1x4
  _t4_46 = _mm256_sub_pd(_t4_41, _t4_45);

  // AVX Storer:
  _t4_21 = _t4_46;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_47 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_48 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_49 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_47), _mm256_castpd256_pd128(_t4_48)));

  // AVX Storer:
  _t4_22 = _t4_49;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_50 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_51 = _t4_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_52 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_53 = _t4_52;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_54 = _mm256_mul_pd(_t4_51, _t4_53);

  // 4-BLAC: 1x4 - 1x4
  _t4_55 = _mm256_sub_pd(_t4_50, _t4_54);

  // AVX Storer:
  _t4_23 = _t4_55;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_56 = _t4_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_57 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_56), _mm256_castpd256_pd128(_t4_57)));

  // AVX Storer:
  _t4_23 = _t4_58;

  _mm256_maskstore_pd(K + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t2_1);
  _mm256_maskstore_pd(K + 124, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t2_2);
  _mm256_storeu_pd(K + 144, _t2_3);

  for( int fi1090 = 4; fi1090 <= 8; fi1090+=4 ) {
    _t5_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi1090 + 164])));
    _t5_1 = _mm256_maskload_pd(K + 20*fi1090 + 165, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t5_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi1090 + 184])));
    _t5_7 = _mm256_maskload_pd(K + 20*fi1090 + 185, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t5_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi1090 + 204])));
    _t5_13 = _mm256_maskload_pd(K + 20*fi1090 + 205, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t5_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[20*fi1090 + 224])));
    _t5_19 = _mm256_maskload_pd(K + 20*fi1090 + 225, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_24 = _t5_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_25 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_24), _mm256_castpd256_pd128(_t5_25)));

    // AVX Storer:
    _t5_0 = _t5_26;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_27 = _t5_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_0, _t5_0, 32), _mm256_permute2f128_pd(_t5_0, _t5_0, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_29 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_65 = _t5_29;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_66 = _mm256_mul_pd(_t5_28, _t4_65);

    // 4-BLAC: 1x4 - 1x4
    _t5_30 = _mm256_sub_pd(_t5_27, _t4_66);

    // AVX Storer:
    _t5_1 = _t5_30;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_32 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_31), _mm256_castpd256_pd128(_t5_32)));

    // AVX Storer:
    _t5_2 = _t5_33;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 6), _mm256_permute2f128_pd(_t5_1, _t5_1, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_2, _t5_2, 32), _mm256_permute2f128_pd(_t5_2, _t5_2, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_36 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_74 = _t5_36;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_75 = _mm256_mul_pd(_t5_35, _t4_74);

    // 4-BLAC: 1x4 - 1x4
    _t5_37 = _mm256_sub_pd(_t5_34, _t4_75);

    // AVX Storer:
    _t5_3 = _t5_37;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_3, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_39 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_38), _mm256_castpd256_pd128(_t5_39)));

    // AVX Storer:
    _t5_4 = _t5_40;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_41 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_3, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_42 = _t5_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_43 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_83 = _t5_43;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_84 = _mm256_mul_pd(_t5_42, _t4_83);

    // 4-BLAC: 1x4 - 1x4
    _t5_44 = _mm256_sub_pd(_t5_41, _t4_84);

    // AVX Storer:
    _t5_5 = _t5_44;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 8), ( G(h(1, 20, fi1090 + 8), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_45 = _t5_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_46 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_45), _mm256_castpd256_pd128(_t5_46)));

    // AVX Storer:
    _t5_5 = _t5_47;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_48 = _t5_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_49 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_48), _mm256_castpd256_pd128(_t5_49)));

    // AVX Storer:
    _t5_6 = _t5_50;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_51 = _t5_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_6, _t5_6, 32), _mm256_permute2f128_pd(_t5_6, _t5_6, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_53 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_95 = _t5_53;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_96 = _mm256_mul_pd(_t5_52, _t4_95);

    // 4-BLAC: 1x4 - 1x4
    _t5_54 = _mm256_sub_pd(_t5_51, _t4_96);

    // AVX Storer:
    _t5_7 = _t5_54;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_55 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_56 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_55), _mm256_castpd256_pd128(_t5_56)));

    // AVX Storer:
    _t5_8 = _t5_57;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_7, 6), _mm256_permute2f128_pd(_t5_7, _t5_7, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_8, _t5_8, 32), _mm256_permute2f128_pd(_t5_8, _t5_8, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_60 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_104 = _t5_60;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_105 = _mm256_mul_pd(_t5_59, _t4_104);

    // 4-BLAC: 1x4 - 1x4
    _t5_61 = _mm256_sub_pd(_t5_58, _t4_105);

    // AVX Storer:
    _t5_9 = _t5_61;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_63 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_62), _mm256_castpd256_pd128(_t5_63)));

    // AVX Storer:
    _t5_10 = _t5_64;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_65 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_66 = _t5_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_67 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_113 = _t5_67;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_114 = _mm256_mul_pd(_t5_66, _t4_113);

    // 4-BLAC: 1x4 - 1x4
    _t5_68 = _mm256_sub_pd(_t5_65, _t4_114);

    // AVX Storer:
    _t5_11 = _t5_68;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 9), ( G(h(1, 20, fi1090 + 9), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_69 = _t5_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_70 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_69), _mm256_castpd256_pd128(_t5_70)));

    // AVX Storer:
    _t5_11 = _t5_71;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_72 = _t5_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_73 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_72), _mm256_castpd256_pd128(_t5_73)));

    // AVX Storer:
    _t5_12 = _t5_74;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_75 = _t5_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_77 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_125 = _t5_77;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_126 = _mm256_mul_pd(_t5_76, _t4_125);

    // 4-BLAC: 1x4 - 1x4
    _t5_78 = _mm256_sub_pd(_t5_75, _t4_126);

    // AVX Storer:
    _t5_13 = _t5_78;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_80 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_79), _mm256_castpd256_pd128(_t5_80)));

    // AVX Storer:
    _t5_14 = _t5_81;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_82 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_13, 6), _mm256_permute2f128_pd(_t5_13, _t5_13, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_84 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_134 = _t5_84;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_135 = _mm256_mul_pd(_t5_83, _t4_134);

    // 4-BLAC: 1x4 - 1x4
    _t5_85 = _mm256_sub_pd(_t5_82, _t4_135);

    // AVX Storer:
    _t5_15 = _t5_85;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_87 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_86), _mm256_castpd256_pd128(_t5_87)));

    // AVX Storer:
    _t5_16 = _t5_88;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_15, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_90 = _t5_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_91 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_143 = _t5_91;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_24 = _mm256_mul_pd(_t5_90, _t4_143);

    // 4-BLAC: 1x4 - 1x4
    _t5_92 = _mm256_sub_pd(_t5_89, _t4_24);

    // AVX Storer:
    _t5_17 = _t5_92;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 10), ( G(h(1, 20, fi1090 + 10), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_93 = _t5_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_94 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_93), _mm256_castpd256_pd128(_t5_94)));

    // AVX Storer:
    _t5_17 = _t5_95;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 4)) Div G(h(1, 20, 4), L[20,20],h(1, 20, 4)) ),h(1, 20, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_96 = _t5_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_97 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_96), _mm256_castpd256_pd128(_t5_97)));

    // AVX Storer:
    _t5_18 = _t5_98;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(3, 20, 5)) - ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 4)) Kro T( G(h(3, 20, 5), L[20,20],h(1, 20, 4)) ) ) ),h(3, 20, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_99 = _t5_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_101 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_35 = _t5_101;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_36 = _mm256_mul_pd(_t5_100, _t4_35);

    // 4-BLAC: 1x4 - 1x4
    _t5_102 = _mm256_sub_pd(_t5_99, _t4_36);

    // AVX Storer:
    _t5_19 = _t5_102;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 5)) Div G(h(1, 20, 5), L[20,20],h(1, 20, 5)) ),h(1, 20, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_104 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_103), _mm256_castpd256_pd128(_t5_104)));

    // AVX Storer:
    _t5_20 = _t5_105;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(2, 20, 6)) - ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 5)) Kro T( G(h(2, 20, 6), L[20,20],h(1, 20, 5)) ) ) ),h(2, 20, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_106 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_19, 6), _mm256_permute2f128_pd(_t5_19, _t5_19, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_108 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_44 = _t5_108;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_45 = _mm256_mul_pd(_t5_107, _t4_44);

    // 4-BLAC: 1x4 - 1x4
    _t5_109 = _mm256_sub_pd(_t5_106, _t4_45);

    // AVX Storer:
    _t5_21 = _t5_109;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 6)) Div G(h(1, 20, 6), L[20,20],h(1, 20, 6)) ),h(1, 20, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_111 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_110), _mm256_castpd256_pd128(_t5_111)));

    // AVX Storer:
    _t5_22 = _t5_112;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 7)) - ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 6)) Kro T( G(h(1, 20, 7), L[20,20],h(1, 20, 6)) ) ) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_113 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_21, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_114 = _t5_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_115 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_53 = _t5_115;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_54 = _mm256_mul_pd(_t5_114, _t4_53);

    // 4-BLAC: 1x4 - 1x4
    _t5_116 = _mm256_sub_pd(_t5_113, _t4_54);

    // AVX Storer:
    _t5_23 = _t5_116;

    // Generating : L[20,20] = S(h(1, 20, fi1090 + 11), ( G(h(1, 20, fi1090 + 11), L[20,20],h(1, 20, 7)) Div G(h(1, 20, 7), L[20,20],h(1, 20, 7)) ),h(1, 20, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_117 = _t5_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_118 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_117), _mm256_castpd256_pd128(_t5_118)));

    // AVX Storer:
    _t5_23 = _t5_119;
    _mm_store_sd(&(K[20*fi1090 + 164]), _mm256_castpd256_pd128(_t5_0));
    _mm_store_sd(&(K[20*fi1090 + 165]), _mm256_castpd256_pd128(_t5_2));
    _mm_store_sd(&(K[20*fi1090 + 166]), _mm256_castpd256_pd128(_t5_4));
    _mm_store_sd(&(K[20*fi1090 + 167]), _mm256_castpd256_pd128(_t5_5));
    _mm_store_sd(&(K[20*fi1090 + 184]), _mm256_castpd256_pd128(_t5_6));
    _mm_store_sd(&(K[20*fi1090 + 185]), _mm256_castpd256_pd128(_t5_8));
    _mm_store_sd(&(K[20*fi1090 + 186]), _mm256_castpd256_pd128(_t5_10));
    _mm_store_sd(&(K[20*fi1090 + 187]), _mm256_castpd256_pd128(_t5_11));
    _mm_store_sd(&(K[20*fi1090 + 204]), _mm256_castpd256_pd128(_t5_12));
    _mm_store_sd(&(K[20*fi1090 + 205]), _mm256_castpd256_pd128(_t5_14));
    _mm_store_sd(&(K[20*fi1090 + 206]), _mm256_castpd256_pd128(_t5_16));
    _mm_store_sd(&(K[20*fi1090 + 207]), _mm256_castpd256_pd128(_t5_17));
    _mm_store_sd(&(K[20*fi1090 + 224]), _mm256_castpd256_pd128(_t5_18));
    _mm_store_sd(&(K[20*fi1090 + 225]), _mm256_castpd256_pd128(_t5_20));
    _mm_store_sd(&(K[20*fi1090 + 226]), _mm256_castpd256_pd128(_t5_22));
    _mm_store_sd(&(K[20*fi1090 + 227]), _mm256_castpd256_pd128(_t5_23));
  }

  _t6_20 = _mm256_castpd128_pd256(_mm_load_sd(K + 168));
  _t6_21 = _mm256_maskload_pd(K + 188, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t6_22 = _mm256_maskload_pd(K + 208, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t6_23 = _mm256_loadu_pd(K + 228);
  _t6_19 = _mm256_broadcast_sd(K + 160);
  _t6_18 = _mm256_broadcast_sd(K + 161);
  _t6_17 = _mm256_broadcast_sd(K + 162);
  _t6_16 = _mm256_broadcast_sd(K + 163);
  _t6_15 = _mm256_broadcast_sd(K + 180);
  _t6_14 = _mm256_broadcast_sd(K + 181);
  _t6_13 = _mm256_broadcast_sd(K + 182);
  _t6_12 = _mm256_broadcast_sd(K + 183);
  _t6_11 = _mm256_broadcast_sd(K + 200);
  _t6_10 = _mm256_broadcast_sd(K + 201);
  _t6_9 = _mm256_broadcast_sd(K + 202);
  _t6_8 = _mm256_broadcast_sd(K + 203);
  _t6_7 = _mm256_broadcast_sd(K + 220);
  _t6_6 = _mm256_broadcast_sd(K + 221);
  _t6_5 = _mm256_broadcast_sd(K + 222);
  _t6_4 = _mm256_broadcast_sd(K + 223);
  _t6_3 = _mm256_loadu_pd(K + 160);
  _t6_2 = _mm256_loadu_pd(K + 180);
  _t6_1 = _mm256_loadu_pd(K + 200);
  _t6_0 = _mm256_loadu_pd(K + 220);

  // Generating : L[20,20] = ( S(h(4, 20, fi971), ( G(h(4, 20, fi971), K[20,20],h(4, 20, fi971)) - ( G(h(4, 20, fi971), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, fi971)) + Sum_{k159} ( -$(h(4, 20, fi971), ( G(h(4, 20, fi971), L[20,20],h(4, 20, k159)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, k159)) ) ),h(4, 20, fi971)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t6_44 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_20, _t6_21, 0), _mm256_shuffle_pd(_t6_22, _t6_23, 0), 32);
  _t6_45 = _mm256_permute2f128_pd(_t6_21, _mm256_shuffle_pd(_t6_22, _t6_23, 3), 32);
  _t6_46 = _mm256_blend_pd(_t6_22, _mm256_shuffle_pd(_t6_22, _t6_23, 3), 12);
  _t6_47 = _t6_23;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_105 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
  _t6_106 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
  _t6_107 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
  _t6_108 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_19, _t6_105), _mm256_mul_pd(_t6_18, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_17, _t6_107), _mm256_mul_pd(_t6_16, _t6_108)));
  _t6_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t6_105), _mm256_mul_pd(_t6_14, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t6_107), _mm256_mul_pd(_t6_12, _t6_108)));
  _t6_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t6_105), _mm256_mul_pd(_t6_10, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t6_107), _mm256_mul_pd(_t6_8, _t6_108)));
  _t6_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t6_105), _mm256_mul_pd(_t6_6, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t6_107), _mm256_mul_pd(_t6_4, _t6_108)));

  // 4-BLAC: 4x4 - 4x4
  _t6_40 = _mm256_sub_pd(_t6_44, _t6_32);
  _t6_41 = _mm256_sub_pd(_t6_45, _t6_33);
  _t6_42 = _mm256_sub_pd(_t6_46, _t6_34);
  _t6_43 = _mm256_sub_pd(_t6_47, _t6_35);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t6_20 = _t6_40;
  _t6_21 = _t6_41;
  _t6_22 = _t6_42;
  _t6_23 = _t6_43;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t6_110 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t6_111 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);
  _t6_112 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_0, _t4_0, 32), _mm256_permute2f128_pd(_t4_0, _t4_0, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_2, _t4_2, 32), _mm256_permute2f128_pd(_t4_2, _t4_2, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_4, _t4_4, 32), _mm256_permute2f128_pd(_t4_4, _t4_4, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_5, _t4_5, 32), _mm256_permute2f128_pd(_t4_5, _t4_5, 32), 0), _t6_112)));
  _t6_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_6, _t4_6, 32), _mm256_permute2f128_pd(_t4_6, _t4_6, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_8, _t4_8, 32), _mm256_permute2f128_pd(_t4_8, _t4_8, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_10, _t4_10, 32), _mm256_permute2f128_pd(_t4_10, _t4_10, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_11, _t4_11, 32), _mm256_permute2f128_pd(_t4_11, _t4_11, 32), 0), _t6_112)));
  _t6_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_12, _t4_12, 32), _mm256_permute2f128_pd(_t4_12, _t4_12, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_14, _t4_14, 32), _mm256_permute2f128_pd(_t4_14, _t4_14, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_16, _t4_16, 32), _mm256_permute2f128_pd(_t4_16, _t4_16, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_17, _t4_17, 32), _mm256_permute2f128_pd(_t4_17, _t4_17, 32), 0), _t6_112)));
  _t6_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_18, _t4_18, 32), _mm256_permute2f128_pd(_t4_18, _t4_18, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_20, _t4_20, 32), _mm256_permute2f128_pd(_t4_20, _t4_20, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_22, _t4_22, 32), _mm256_permute2f128_pd(_t4_22, _t4_22, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_23, _t4_23, 32), _mm256_permute2f128_pd(_t4_23, _t4_23, 32), 0), _t6_112)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowTriang
  _t6_48 = _t6_20;
  _t6_49 = _t6_21;
  _t6_50 = _t6_22;
  _t6_51 = _t6_23;

  // 4-BLAC: 4x4 - 4x4
  _t6_48 = _mm256_sub_pd(_t6_48, _t6_36);
  _t6_49 = _mm256_sub_pd(_t6_49, _t6_37);
  _t6_50 = _mm256_sub_pd(_t6_50, _t6_38);
  _t6_51 = _mm256_sub_pd(_t6_51, _t6_39);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t6_20 = _t6_48;
  _t6_21 = _t6_49;
  _t6_22 = _t6_50;
  _t6_23 = _t6_51;

  // Generating : L[20,20] = S(h(1, 20, fi971), Sqrt( G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_52 = _t6_20;

  // 4-BLAC: sqrt(1x4)
  _t6_53 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_52)));

  // AVX Storer:
  _t6_20 = _t6_53;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t6_54 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_55 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t6_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_54), _mm256_castpd256_pd128(_t6_55)));

  // AVX Storer:
  _t6_24 = _t6_56;

  // Generating : L[20,20] = S(h(3, 20, fi971 + 1), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, fi971)) Kro G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_24, _t6_24, 32), _mm256_permute2f128_pd(_t6_24, _t6_24, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t6_58 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_21, _t6_22), _mm256_unpacklo_pd(_t6_23, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t6_59 = _mm256_mul_pd(_t6_57, _t6_58);

  // AVX Storer:
  _t6_25 = _t6_59;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 1), ( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) - ( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) Kro T( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_60 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_61 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_25, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_25, 1);

  // 4-BLAC: (4x1)^T
  _t6_63 = _t6_62;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_64 = _mm256_mul_pd(_t6_61, _t6_63);

  // 4-BLAC: 1x4 - 1x4
  _t6_65 = _mm256_sub_pd(_t6_60, _t6_64);

  // AVX Storer:
  _t6_26 = _t6_65;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 1), Sqrt( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_66 = _t6_26;

  // 4-BLAC: sqrt(1x4)
  _t6_67 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_66)));

  // AVX Storer:
  _t6_26 = _t6_67;

  // Generating : L[20,20] = S(h(2, 20, fi971 + 2), ( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) - ( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971)) Kro T( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t6_68 = _mm256_unpackhi_pd(_mm256_blend_pd(_t6_22, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_23, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t6_69 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_25, 2), _mm256_permute2f128_pd(_t6_25, _t6_25, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_70 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_25, _t6_25, 32), _mm256_permute2f128_pd(_t6_25, _t6_25, 32), 0);

  // 4-BLAC: (4x1)^T
  _t6_71 = _t6_70;

  // 4-BLAC: 4x1 Kro 1x4
  _t6_72 = _mm256_mul_pd(_t6_69, _t6_71);

  // 4-BLAC: 4x1 - 4x1
  _t6_73 = _mm256_sub_pd(_t6_68, _t6_72);

  // AVX Storer:
  _t6_27 = _t6_73;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t6_74 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_75 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t6_76 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_74), _mm256_castpd256_pd128(_t6_75)));

  // AVX Storer:
  _t6_28 = _t6_76;

  // Generating : L[20,20] = S(h(2, 20, fi971 + 2), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, fi971 + 1)) Kro G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_77 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_28, _t6_28, 32), _mm256_permute2f128_pd(_t6_28, _t6_28, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t6_78 = _t6_27;

  // 4-BLAC: 1x4 Kro 4x1
  _t6_79 = _mm256_mul_pd(_t6_77, _t6_78);

  // AVX Storer:
  _t6_27 = _t6_79;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 2), ( G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) - ( G(h(1, 20, fi971 + 2), L[20,20],h(2, 20, fi971)) * T( G(h(1, 20, fi971 + 2), L[20,20],h(2, 20, fi971)) ) ) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_80 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_22, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_22, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_81 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_25, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_27, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_82 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_25, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_27, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t6_83 = _t6_82;

  // 4-BLAC: 1x4 * 4x1
  _t6_84 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_mul_pd(_t6_81, _t6_83), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_mul_pd(_t6_81, _t6_83), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_mul_pd(_t6_81, _t6_83), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_85 = _mm256_sub_pd(_t6_80, _t6_84);

  // AVX Storer:
  _t6_29 = _t6_85;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 2), Sqrt( G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_86 = _t6_29;

  // 4-BLAC: sqrt(1x4)
  _t6_87 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_86)));

  // AVX Storer:
  _t6_29 = _t6_87;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) - ( G(h(1, 20, fi971 + 3), L[20,20],h(2, 20, fi971)) * T( G(h(1, 20, fi971 + 2), L[20,20],h(2, 20, fi971)) ) ) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_88 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_23, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_23, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_89 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_25, _t6_25, 129), _t6_27, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_90 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_25, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_27, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t6_91 = _t6_90;

  // 4-BLAC: 1x4 * 4x1
  _t6_92 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_mul_pd(_t6_89, _t6_91), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_mul_pd(_t6_89, _t6_91), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_mul_pd(_t6_89, _t6_91), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_93 = _mm256_sub_pd(_t6_88, _t6_92);

  // AVX Storer:
  _t6_30 = _t6_93;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_94 = _t6_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_95 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t6_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_94), _mm256_castpd256_pd128(_t6_95)));

  // AVX Storer:
  _t6_30 = _t6_96;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi971 + 3), L[20,20],h(3, 20, fi971)) * T( G(h(1, 20, fi971 + 3), L[20,20],h(3, 20, fi971)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_97 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_23, _t6_23, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_98 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_25, _t6_30, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t6_27, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_99 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_25, _t6_30, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t6_27, 2), 10);

  // 4-BLAC: (1x4)^T
  _t6_100 = _t6_99;

  // 4-BLAC: 1x4 * 4x1
  _t6_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_mul_pd(_t6_98, _t6_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_mul_pd(_t6_98, _t6_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_mul_pd(_t6_98, _t6_100), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_102 = _mm256_sub_pd(_t6_97, _t6_101);

  // AVX Storer:
  _t6_31 = _t6_102;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), Sqrt( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_103 = _t6_31;

  // 4-BLAC: sqrt(1x4)
  _t6_104 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_103)));

  // AVX Storer:
  _t6_31 = _t6_104;

  // Generating : L[20,20] = ( Sum_{k207} ( S(h(4, 20, fi971 + k207 + 4), ( G(h(4, 20, fi971 + k207 + 4), K[20,20],h(4, 20, fi971)) - ( G(h(4, 20, fi971 + k207 + 4), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, fi971)) ) + Sum_{k159} ( Sum_{k207} ( -$(h(4, 20, fi971 + k207 + 4), ( G(h(4, 20, fi971 + k207 + 4), L[20,20],h(4, 20, k159)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, k159)) ) ),h(4, 20, fi971)) ) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
  _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
  _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
  _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);


  for( int k207 = 0; k207 <= 7; k207+=4 ) {
    _t7_20 = _mm256_loadu_pd(K + 20*k207 + 248);
    _t7_21 = _mm256_loadu_pd(K + 20*k207 + 268);
    _t7_22 = _mm256_loadu_pd(K + 20*k207 + 288);
    _t7_23 = _mm256_loadu_pd(K + 20*k207 + 308);
    _t7_15 = _mm256_broadcast_sd(K + 20*k207 + 240);
    _t7_14 = _mm256_broadcast_sd(K + 20*k207 + 241);
    _t7_13 = _mm256_broadcast_sd(K + 20*k207 + 242);
    _t7_12 = _mm256_broadcast_sd(K + 20*k207 + 243);
    _t7_11 = _mm256_broadcast_sd(K + 20*k207 + 260);
    _t7_10 = _mm256_broadcast_sd(K + 20*k207 + 261);
    _t7_9 = _mm256_broadcast_sd(K + 20*k207 + 262);
    _t7_8 = _mm256_broadcast_sd(K + 20*k207 + 263);
    _t7_7 = _mm256_broadcast_sd(K + 20*k207 + 280);
    _t7_6 = _mm256_broadcast_sd(K + 20*k207 + 281);
    _t7_5 = _mm256_broadcast_sd(K + 20*k207 + 282);
    _t7_4 = _mm256_broadcast_sd(K + 20*k207 + 283);
    _t7_3 = _mm256_broadcast_sd(K + 20*k207 + 300);
    _t7_2 = _mm256_broadcast_sd(K + 20*k207 + 301);
    _t7_1 = _mm256_broadcast_sd(K + 20*k207 + 302);
    _t7_0 = _mm256_broadcast_sd(K + 20*k207 + 303);

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
    _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
    _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
    _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t7_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t6_113), _mm256_mul_pd(_t7_14, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t6_115), _mm256_mul_pd(_t7_12, _t6_116)));
    _t7_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t6_113), _mm256_mul_pd(_t7_10, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t6_115), _mm256_mul_pd(_t7_8, _t6_116)));
    _t7_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t6_113), _mm256_mul_pd(_t7_6, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t6_115), _mm256_mul_pd(_t7_4, _t6_116)));
    _t7_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_3, _t6_113), _mm256_mul_pd(_t7_2, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_1, _t6_115), _mm256_mul_pd(_t7_0, _t6_116)));

    // 4-BLAC: 4x4 - 4x4
    _t7_20 = _mm256_sub_pd(_t7_20, _t7_16);
    _t7_21 = _mm256_sub_pd(_t7_21, _t7_17);
    _t7_22 = _mm256_sub_pd(_t7_22, _t7_18);
    _t7_23 = _mm256_sub_pd(_t7_23, _t7_19);

    // AVX Storer:
    _mm256_storeu_pd(K + 20*k207 + 248, _t7_20);
    _mm256_storeu_pd(K + 20*k207 + 268, _t7_21);
    _mm256_storeu_pd(K + 20*k207 + 288, _t7_22);
    _mm256_storeu_pd(K + 20*k207 + 308, _t7_23);
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);
  _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);

  _mm256_maskstore_pd(K + 165, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_1);
  _mm256_maskstore_pd(K + 166, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_3);
  _mm256_maskstore_pd(K + 185, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_7);
  _mm256_maskstore_pd(K + 186, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_9);
  _mm256_maskstore_pd(K + 205, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_13);
  _mm256_maskstore_pd(K + 206, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_15);
  _mm256_maskstore_pd(K + 225, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_19);
  _mm256_maskstore_pd(K + 226, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_21);

  for( int k207 = 0; k207 <= 7; k207+=4 ) {
    _t9_15 = _mm256_broadcast_sd(K + 20*k207 + 244);
    _t9_14 = _mm256_broadcast_sd(K + 20*k207 + 245);
    _t9_13 = _mm256_broadcast_sd(K + 20*k207 + 246);
    _t9_12 = _mm256_broadcast_sd(K + 20*k207 + 247);
    _t9_11 = _mm256_broadcast_sd(K + 20*k207 + 264);
    _t9_10 = _mm256_broadcast_sd(K + 20*k207 + 265);
    _t9_9 = _mm256_broadcast_sd(K + 20*k207 + 266);
    _t9_8 = _mm256_broadcast_sd(K + 20*k207 + 267);
    _t9_7 = _mm256_broadcast_sd(K + 20*k207 + 284);
    _t9_6 = _mm256_broadcast_sd(K + 20*k207 + 285);
    _t9_5 = _mm256_broadcast_sd(K + 20*k207 + 286);
    _t9_4 = _mm256_broadcast_sd(K + 20*k207 + 287);
    _t9_3 = _mm256_broadcast_sd(K + 20*k207 + 304);
    _t9_2 = _mm256_broadcast_sd(K + 20*k207 + 305);
    _t9_1 = _mm256_broadcast_sd(K + 20*k207 + 306);
    _t9_0 = _mm256_broadcast_sd(K + 20*k207 + 307);
    _t9_16 = _mm256_loadu_pd(K + 20*k207 + 248);
    _t9_17 = _mm256_loadu_pd(K + 20*k207 + 268);
    _t9_18 = _mm256_loadu_pd(K + 20*k207 + 288);
    _t9_19 = _mm256_loadu_pd(K + 20*k207 + 308);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
    _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
    _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);
    _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);

    // 4-BLAC: 4x4 * 4x4
    _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t8_0), _mm256_mul_pd(_t9_14, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t8_2), _mm256_mul_pd(_t9_12, _t8_3)));
    _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t8_0), _mm256_mul_pd(_t9_10, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t8_2), _mm256_mul_pd(_t9_8, _t8_3)));
    _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t8_0), _mm256_mul_pd(_t9_6, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t8_2), _mm256_mul_pd(_t9_4, _t8_3)));
    _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_3, _t8_0), _mm256_mul_pd(_t9_2, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_1, _t8_2), _mm256_mul_pd(_t9_0, _t8_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t9_16 = _mm256_sub_pd(_t9_16, _t9_20);
    _t9_17 = _mm256_sub_pd(_t9_17, _t9_21);
    _t9_18 = _mm256_sub_pd(_t9_18, _t9_22);
    _t9_19 = _mm256_sub_pd(_t9_19, _t9_23);

    // AVX Storer:
    _mm256_storeu_pd(K + 20*k207 + 248, _t9_16);
    _mm256_storeu_pd(K + 20*k207 + 268, _t9_17);
    _mm256_storeu_pd(K + 20*k207 + 288, _t9_18);
    _mm256_storeu_pd(K + 20*k207 + 308, _t9_19);
  }

  _t10_68 = _mm256_castpd128_pd256(_mm_load_sd(K + 252));
  _t10_69 = _mm256_maskload_pd(K + 272, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t10_70 = _mm256_maskload_pd(K + 292, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_71 = _mm256_loadu_pd(K + 312);
  _t10_19 = _mm256_broadcast_sd(K + 240);
  _t10_18 = _mm256_broadcast_sd(K + 241);
  _t10_17 = _mm256_broadcast_sd(K + 242);
  _t10_16 = _mm256_broadcast_sd(K + 243);
  _t10_15 = _mm256_broadcast_sd(K + 260);
  _t10_14 = _mm256_broadcast_sd(K + 261);
  _t10_13 = _mm256_broadcast_sd(K + 262);
  _t10_12 = _mm256_broadcast_sd(K + 263);
  _t10_11 = _mm256_broadcast_sd(K + 280);
  _t10_10 = _mm256_broadcast_sd(K + 281);
  _t10_9 = _mm256_broadcast_sd(K + 282);
  _t10_8 = _mm256_broadcast_sd(K + 283);
  _t10_7 = _mm256_broadcast_sd(K + 300);
  _t10_6 = _mm256_broadcast_sd(K + 301);
  _t10_5 = _mm256_broadcast_sd(K + 302);
  _t10_4 = _mm256_broadcast_sd(K + 303);
  _t10_3 = _mm256_loadu_pd(K + 240);
  _t10_2 = _mm256_loadu_pd(K + 260);
  _t10_1 = _mm256_loadu_pd(K + 280);
  _t10_0 = _mm256_loadu_pd(K + 300);
  _t10_20 = _mm256_castpd128_pd256(_mm_load_sd(&(K[248])));
  _t10_44 = _mm256_castpd128_pd256(_mm_load_sd(&(K[328])));
  _t10_21 = _mm256_maskload_pd(K + 249, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_45 = _mm256_maskload_pd(K + 329, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_26 = _mm256_castpd128_pd256(_mm_load_sd(&(K[268])));
  _t10_50 = _mm256_castpd128_pd256(_mm_load_sd(&(K[348])));
  _t10_27 = _mm256_maskload_pd(K + 269, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_51 = _mm256_maskload_pd(K + 349, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_32 = _mm256_castpd128_pd256(_mm_load_sd(&(K[288])));
  _t10_56 = _mm256_castpd128_pd256(_mm_load_sd(&(K[368])));
  _t10_33 = _mm256_maskload_pd(K + 289, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_57 = _mm256_maskload_pd(K + 369, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_38 = _mm256_castpd128_pd256(_mm_load_sd(&(K[308])));
  _t10_62 = _mm256_castpd128_pd256(_mm_load_sd(&(K[388])));
  _t10_39 = _mm256_maskload_pd(K + 309, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_63 = _mm256_maskload_pd(K + 389, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_72 = _t10_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_73 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_72), _mm256_castpd256_pd128(_t10_73)));

  // AVX Storer:
  _t10_20 = _t10_74;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_75 = _t10_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_20, _t10_20, 32), _mm256_permute2f128_pd(_t10_20, _t10_20, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_77 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_78 = _t10_77;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_79 = _mm256_mul_pd(_t10_76, _t10_78);

  // 4-BLAC: 1x4 - 1x4
  _t10_80 = _mm256_sub_pd(_t10_75, _t10_79);

  // AVX Storer:
  _t10_21 = _t10_80;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_81 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_82 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_83 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_81), _mm256_castpd256_pd128(_t10_82)));

  // AVX Storer:
  _t10_22 = _t10_83;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_84 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_21, 6), _mm256_permute2f128_pd(_t10_21, _t10_21, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_85 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_22, _t10_22, 32), _mm256_permute2f128_pd(_t10_22, _t10_22, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_86 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_87 = _t10_86;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_88 = _mm256_mul_pd(_t10_85, _t10_87);

  // 4-BLAC: 1x4 - 1x4
  _t10_89 = _mm256_sub_pd(_t10_84, _t10_88);

  // AVX Storer:
  _t10_23 = _t10_89;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_90 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_23, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_91 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_92 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_90), _mm256_castpd256_pd128(_t10_91)));

  // AVX Storer:
  _t10_24 = _t10_92;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_93 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_23, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_94 = _t10_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_95 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_96 = _t10_95;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_97 = _mm256_mul_pd(_t10_94, _t10_96);

  // 4-BLAC: 1x4 - 1x4
  _t10_98 = _mm256_sub_pd(_t10_93, _t10_97);

  // AVX Storer:
  _t10_25 = _t10_98;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_99 = _t10_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_100 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_101 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_99), _mm256_castpd256_pd128(_t10_100)));

  // AVX Storer:
  _t10_25 = _t10_101;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_102 = _t10_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_103 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_102), _mm256_castpd256_pd128(_t10_103)));

  // AVX Storer:
  _t10_26 = _t10_104;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_105 = _t10_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_106 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_26, _t10_26, 32), _mm256_permute2f128_pd(_t10_26, _t10_26, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_107 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_108 = _t10_107;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_109 = _mm256_mul_pd(_t10_106, _t10_108);

  // 4-BLAC: 1x4 - 1x4
  _t10_110 = _mm256_sub_pd(_t10_105, _t10_109);

  // AVX Storer:
  _t10_27 = _t10_110;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_111 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_27, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_112 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_113 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_111), _mm256_castpd256_pd128(_t10_112)));

  // AVX Storer:
  _t10_28 = _t10_113;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_114 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_27, 6), _mm256_permute2f128_pd(_t10_27, _t10_27, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_115 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_28, _t10_28, 32), _mm256_permute2f128_pd(_t10_28, _t10_28, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_116 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_117 = _t10_116;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_118 = _mm256_mul_pd(_t10_115, _t10_117);

  // 4-BLAC: 1x4 - 1x4
  _t10_119 = _mm256_sub_pd(_t10_114, _t10_118);

  // AVX Storer:
  _t10_29 = _t10_119;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_120 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_29, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_121 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_122 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_120), _mm256_castpd256_pd128(_t10_121)));

  // AVX Storer:
  _t10_30 = _t10_122;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_123 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_124 = _t10_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_125 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_126 = _t10_125;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_127 = _mm256_mul_pd(_t10_124, _t10_126);

  // 4-BLAC: 1x4 - 1x4
  _t10_128 = _mm256_sub_pd(_t10_123, _t10_127);

  // AVX Storer:
  _t10_31 = _t10_128;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_129 = _t10_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_130 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_131 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_129), _mm256_castpd256_pd128(_t10_130)));

  // AVX Storer:
  _t10_31 = _t10_131;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_132 = _t10_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_133 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_132), _mm256_castpd256_pd128(_t10_133)));

  // AVX Storer:
  _t10_32 = _t10_134;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_135 = _t10_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_136 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_32, _t10_32, 32), _mm256_permute2f128_pd(_t10_32, _t10_32, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_137 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_138 = _t10_137;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_139 = _mm256_mul_pd(_t10_136, _t10_138);

  // 4-BLAC: 1x4 - 1x4
  _t10_140 = _mm256_sub_pd(_t10_135, _t10_139);

  // AVX Storer:
  _t10_33 = _t10_140;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_141 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_33, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_142 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_143 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_141), _mm256_castpd256_pd128(_t10_142)));

  // AVX Storer:
  _t10_34 = _t10_143;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_144 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_33, 6), _mm256_permute2f128_pd(_t10_33, _t10_33, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_145 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_34, _t10_34, 32), _mm256_permute2f128_pd(_t10_34, _t10_34, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_146 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_147 = _t10_146;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_148 = _mm256_mul_pd(_t10_145, _t10_147);

  // 4-BLAC: 1x4 - 1x4
  _t10_149 = _mm256_sub_pd(_t10_144, _t10_148);

  // AVX Storer:
  _t10_35 = _t10_149;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_150 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_35, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_151 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_152 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_150), _mm256_castpd256_pd128(_t10_151)));

  // AVX Storer:
  _t10_36 = _t10_152;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_153 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_35, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_154 = _t10_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_155 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_156 = _t10_155;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_157 = _mm256_mul_pd(_t10_154, _t10_156);

  // 4-BLAC: 1x4 - 1x4
  _t10_158 = _mm256_sub_pd(_t10_153, _t10_157);

  // AVX Storer:
  _t10_37 = _t10_158;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_159 = _t10_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_160 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_161 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_159), _mm256_castpd256_pd128(_t10_160)));

  // AVX Storer:
  _t10_37 = _t10_161;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_162 = _t10_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_163 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_162), _mm256_castpd256_pd128(_t10_163)));

  // AVX Storer:
  _t10_38 = _t10_164;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_165 = _t10_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_166 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_38, _t10_38, 32), _mm256_permute2f128_pd(_t10_38, _t10_38, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_167 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_168 = _t10_167;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_169 = _mm256_mul_pd(_t10_166, _t10_168);

  // 4-BLAC: 1x4 - 1x4
  _t10_170 = _mm256_sub_pd(_t10_165, _t10_169);

  // AVX Storer:
  _t10_39 = _t10_170;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_171 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_39, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_172 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_173 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_171), _mm256_castpd256_pd128(_t10_172)));

  // AVX Storer:
  _t10_40 = _t10_173;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_174 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_39, 6), _mm256_permute2f128_pd(_t10_39, _t10_39, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_175 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_40, _t10_40, 32), _mm256_permute2f128_pd(_t10_40, _t10_40, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_176 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_177 = _t10_176;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_178 = _mm256_mul_pd(_t10_175, _t10_177);

  // 4-BLAC: 1x4 - 1x4
  _t10_179 = _mm256_sub_pd(_t10_174, _t10_178);

  // AVX Storer:
  _t10_41 = _t10_179;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_180 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_41, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_181 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_182 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_180), _mm256_castpd256_pd128(_t10_181)));

  // AVX Storer:
  _t10_42 = _t10_182;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_183 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_41, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_184 = _t10_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_185 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_186 = _t10_185;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_187 = _mm256_mul_pd(_t10_184, _t10_186);

  // 4-BLAC: 1x4 - 1x4
  _t10_188 = _mm256_sub_pd(_t10_183, _t10_187);

  // AVX Storer:
  _t10_43 = _t10_188;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_189 = _t10_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_190 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_191 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_189), _mm256_castpd256_pd128(_t10_190)));

  // AVX Storer:
  _t10_43 = _t10_191;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_192 = _t10_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_193 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_192), _mm256_castpd256_pd128(_t10_193)));

  // AVX Storer:
  _t10_44 = _t10_194;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_195 = _t10_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_196 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_44, _t10_44, 32), _mm256_permute2f128_pd(_t10_44, _t10_44, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_197 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_78 = _t10_197;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_79 = _mm256_mul_pd(_t10_196, _t10_78);

  // 4-BLAC: 1x4 - 1x4
  _t10_198 = _mm256_sub_pd(_t10_195, _t10_79);

  // AVX Storer:
  _t10_45 = _t10_198;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_199 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_45, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_200 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_201 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_199), _mm256_castpd256_pd128(_t10_200)));

  // AVX Storer:
  _t10_46 = _t10_201;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_202 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_45, 6), _mm256_permute2f128_pd(_t10_45, _t10_45, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_203 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_46, _t10_46, 32), _mm256_permute2f128_pd(_t10_46, _t10_46, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_204 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_87 = _t10_204;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_88 = _mm256_mul_pd(_t10_203, _t10_87);

  // 4-BLAC: 1x4 - 1x4
  _t10_205 = _mm256_sub_pd(_t10_202, _t10_88);

  // AVX Storer:
  _t10_47 = _t10_205;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_206 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_47, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_207 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_208 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_206), _mm256_castpd256_pd128(_t10_207)));

  // AVX Storer:
  _t10_48 = _t10_208;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_209 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_47, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_210 = _t10_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_211 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_96 = _t10_211;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_97 = _mm256_mul_pd(_t10_210, _t10_96);

  // 4-BLAC: 1x4 - 1x4
  _t10_212 = _mm256_sub_pd(_t10_209, _t10_97);

  // AVX Storer:
  _t10_49 = _t10_212;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_213 = _t10_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_214 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_215 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_213), _mm256_castpd256_pd128(_t10_214)));

  // AVX Storer:
  _t10_49 = _t10_215;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_216 = _t10_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_217 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_218 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_216), _mm256_castpd256_pd128(_t10_217)));

  // AVX Storer:
  _t10_50 = _t10_218;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_219 = _t10_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_220 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_50, _t10_50, 32), _mm256_permute2f128_pd(_t10_50, _t10_50, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_221 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_108 = _t10_221;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_109 = _mm256_mul_pd(_t10_220, _t10_108);

  // 4-BLAC: 1x4 - 1x4
  _t10_222 = _mm256_sub_pd(_t10_219, _t10_109);

  // AVX Storer:
  _t10_51 = _t10_222;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_223 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_51, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_224 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_225 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_223), _mm256_castpd256_pd128(_t10_224)));

  // AVX Storer:
  _t10_52 = _t10_225;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_226 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_51, 6), _mm256_permute2f128_pd(_t10_51, _t10_51, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_227 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_52, _t10_52, 32), _mm256_permute2f128_pd(_t10_52, _t10_52, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_228 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_117 = _t10_228;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_118 = _mm256_mul_pd(_t10_227, _t10_117);

  // 4-BLAC: 1x4 - 1x4
  _t10_229 = _mm256_sub_pd(_t10_226, _t10_118);

  // AVX Storer:
  _t10_53 = _t10_229;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_230 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_53, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_231 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_232 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_230), _mm256_castpd256_pd128(_t10_231)));

  // AVX Storer:
  _t10_54 = _t10_232;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_233 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_53, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_234 = _t10_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_235 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_126 = _t10_235;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_127 = _mm256_mul_pd(_t10_234, _t10_126);

  // 4-BLAC: 1x4 - 1x4
  _t10_236 = _mm256_sub_pd(_t10_233, _t10_127);

  // AVX Storer:
  _t10_55 = _t10_236;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_237 = _t10_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_238 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_239 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_237), _mm256_castpd256_pd128(_t10_238)));

  // AVX Storer:
  _t10_55 = _t10_239;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_240 = _t10_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_241 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_242 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_240), _mm256_castpd256_pd128(_t10_241)));

  // AVX Storer:
  _t10_56 = _t10_242;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_243 = _t10_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_244 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_56, _t10_56, 32), _mm256_permute2f128_pd(_t10_56, _t10_56, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_245 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_138 = _t10_245;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_139 = _mm256_mul_pd(_t10_244, _t10_138);

  // 4-BLAC: 1x4 - 1x4
  _t10_246 = _mm256_sub_pd(_t10_243, _t10_139);

  // AVX Storer:
  _t10_57 = _t10_246;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_247 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_57, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_248 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_249 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_247), _mm256_castpd256_pd128(_t10_248)));

  // AVX Storer:
  _t10_58 = _t10_249;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_250 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_57, 6), _mm256_permute2f128_pd(_t10_57, _t10_57, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_251 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_58, _t10_58, 32), _mm256_permute2f128_pd(_t10_58, _t10_58, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_252 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_147 = _t10_252;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_148 = _mm256_mul_pd(_t10_251, _t10_147);

  // 4-BLAC: 1x4 - 1x4
  _t10_253 = _mm256_sub_pd(_t10_250, _t10_148);

  // AVX Storer:
  _t10_59 = _t10_253;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_254 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_59, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_255 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_256 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_254), _mm256_castpd256_pd128(_t10_255)));

  // AVX Storer:
  _t10_60 = _t10_256;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_257 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_59, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_258 = _t10_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_259 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_156 = _t10_259;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_157 = _mm256_mul_pd(_t10_258, _t10_156);

  // 4-BLAC: 1x4 - 1x4
  _t10_260 = _mm256_sub_pd(_t10_257, _t10_157);

  // AVX Storer:
  _t10_61 = _t10_260;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_261 = _t10_61;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_262 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_263 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_261), _mm256_castpd256_pd128(_t10_262)));

  // AVX Storer:
  _t10_61 = _t10_263;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_264 = _t10_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_265 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_266 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_264), _mm256_castpd256_pd128(_t10_265)));

  // AVX Storer:
  _t10_62 = _t10_266;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_267 = _t10_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_268 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_62, _t10_62, 32), _mm256_permute2f128_pd(_t10_62, _t10_62, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_269 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_168 = _t10_269;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_169 = _mm256_mul_pd(_t10_268, _t10_168);

  // 4-BLAC: 1x4 - 1x4
  _t10_270 = _mm256_sub_pd(_t10_267, _t10_169);

  // AVX Storer:
  _t10_63 = _t10_270;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_271 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_63, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_272 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_273 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_271), _mm256_castpd256_pd128(_t10_272)));

  // AVX Storer:
  _t10_64 = _t10_273;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_274 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_63, 6), _mm256_permute2f128_pd(_t10_63, _t10_63, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_275 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_64, _t10_64, 32), _mm256_permute2f128_pd(_t10_64, _t10_64, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_276 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_177 = _t10_276;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_178 = _mm256_mul_pd(_t10_275, _t10_177);

  // 4-BLAC: 1x4 - 1x4
  _t10_277 = _mm256_sub_pd(_t10_274, _t10_178);

  // AVX Storer:
  _t10_65 = _t10_277;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_278 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_65, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_279 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_280 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_278), _mm256_castpd256_pd128(_t10_279)));

  // AVX Storer:
  _t10_66 = _t10_280;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_281 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_65, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_282 = _t10_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_283 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_186 = _t10_283;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_187 = _mm256_mul_pd(_t10_282, _t10_186);

  // 4-BLAC: 1x4 - 1x4
  _t10_284 = _mm256_sub_pd(_t10_281, _t10_187);

  // AVX Storer:
  _t10_67 = _t10_284;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_285 = _t10_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_286 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_287 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_285), _mm256_castpd256_pd128(_t10_286)));

  // AVX Storer:
  _t10_67 = _t10_287;

  // Generating : L[20,20] = ( S(h(4, 20, fi971), ( G(h(4, 20, fi971), K[20,20],h(4, 20, fi971)) - ( G(h(4, 20, fi971), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, fi971)) + Sum_{k159} ( -$(h(4, 20, fi971), ( G(h(4, 20, fi971), L[20,20],h(4, 20, k159)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, k159)) ) ),h(4, 20, fi971)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t10_288 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_68, _t10_69, 0), _mm256_shuffle_pd(_t10_70, _t10_71, 0), 32);
  _t10_289 = _mm256_permute2f128_pd(_t10_69, _mm256_shuffle_pd(_t10_70, _t10_71, 3), 32);
  _t10_290 = _mm256_blend_pd(_t10_70, _mm256_shuffle_pd(_t10_70, _t10_71, 3), 12);
  _t10_291 = _t10_71;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_105 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
  _t6_106 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
  _t6_107 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
  _t6_108 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t6_105), _mm256_mul_pd(_t10_18, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t6_107), _mm256_mul_pd(_t10_16, _t6_108)));
  _t6_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t6_105), _mm256_mul_pd(_t10_14, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t6_107), _mm256_mul_pd(_t10_12, _t6_108)));
  _t6_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t6_105), _mm256_mul_pd(_t10_10, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t6_107), _mm256_mul_pd(_t10_8, _t6_108)));
  _t6_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t6_105), _mm256_mul_pd(_t10_6, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t6_107), _mm256_mul_pd(_t10_4, _t6_108)));

  // 4-BLAC: 4x4 - 4x4
  _t6_40 = _mm256_sub_pd(_t10_288, _t6_32);
  _t6_41 = _mm256_sub_pd(_t10_289, _t6_33);
  _t6_42 = _mm256_sub_pd(_t10_290, _t6_34);
  _t6_43 = _mm256_sub_pd(_t10_291, _t6_35);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t10_68 = _t6_40;
  _t10_69 = _t6_41;
  _t10_70 = _t6_42;
  _t10_71 = _t6_43;

  _mm_store_sd(&(K[248]), _mm256_castpd256_pd128(_t10_20));
  _mm_store_sd(&(K[249]), _mm256_castpd256_pd128(_t10_22));
  _mm_store_sd(&(K[250]), _mm256_castpd256_pd128(_t10_24));
  _mm_store_sd(&(K[251]), _mm256_castpd256_pd128(_t10_25));
  _mm_store_sd(&(K[268]), _mm256_castpd256_pd128(_t10_26));
  _mm_store_sd(&(K[269]), _mm256_castpd256_pd128(_t10_28));
  _mm_store_sd(&(K[270]), _mm256_castpd256_pd128(_t10_30));
  _mm_store_sd(&(K[271]), _mm256_castpd256_pd128(_t10_31));
  _mm_store_sd(&(K[288]), _mm256_castpd256_pd128(_t10_32));
  _mm_store_sd(&(K[289]), _mm256_castpd256_pd128(_t10_34));
  _mm_store_sd(&(K[290]), _mm256_castpd256_pd128(_t10_36));
  _mm_store_sd(&(K[291]), _mm256_castpd256_pd128(_t10_37));
  _mm_store_sd(&(K[308]), _mm256_castpd256_pd128(_t10_38));
  _mm_store_sd(&(K[309]), _mm256_castpd256_pd128(_t10_40));
  _mm_store_sd(&(K[310]), _mm256_castpd256_pd128(_t10_42));
  _mm_store_sd(&(K[311]), _mm256_castpd256_pd128(_t10_43));

  for( int k159 = 4; k159 <= 11; k159+=4 ) {
    _t11_19 = _mm256_broadcast_sd(K + k159 + 240);
    _t11_18 = _mm256_broadcast_sd(K + k159 + 241);
    _t11_17 = _mm256_broadcast_sd(K + k159 + 242);
    _t11_16 = _mm256_broadcast_sd(K + k159 + 243);
    _t11_15 = _mm256_broadcast_sd(K + k159 + 260);
    _t11_14 = _mm256_broadcast_sd(K + k159 + 261);
    _t11_13 = _mm256_broadcast_sd(K + k159 + 262);
    _t11_12 = _mm256_broadcast_sd(K + k159 + 263);
    _t11_11 = _mm256_broadcast_sd(K + k159 + 280);
    _t11_10 = _mm256_broadcast_sd(K + k159 + 281);
    _t11_9 = _mm256_broadcast_sd(K + k159 + 282);
    _t11_8 = _mm256_broadcast_sd(K + k159 + 283);
    _t11_7 = _mm256_broadcast_sd(K + k159 + 300);
    _t11_6 = _mm256_broadcast_sd(K + k159 + 301);
    _t11_5 = _mm256_broadcast_sd(K + k159 + 302);
    _t11_4 = _mm256_broadcast_sd(K + k159 + 303);
    _t11_3 = _mm256_loadu_pd(K + k159 + 240);
    _t11_2 = _mm256_loadu_pd(K + k159 + 260);
    _t11_1 = _mm256_loadu_pd(K + k159 + 280);
    _t11_0 = _mm256_loadu_pd(K + k159 + 300);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t6_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
    _t6_110 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
    _t6_111 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
    _t6_112 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t6_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t6_109), _mm256_mul_pd(_t11_18, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t6_111), _mm256_mul_pd(_t11_16, _t6_112)));
    _t6_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t6_109), _mm256_mul_pd(_t11_14, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t6_111), _mm256_mul_pd(_t11_12, _t6_112)));
    _t6_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t6_109), _mm256_mul_pd(_t11_10, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t6_111), _mm256_mul_pd(_t11_8, _t6_112)));
    _t6_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t6_109), _mm256_mul_pd(_t11_6, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t6_111), _mm256_mul_pd(_t11_4, _t6_112)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowTriang
    _t11_20 = _t10_68;
    _t11_21 = _t10_69;
    _t11_22 = _t10_70;
    _t11_23 = _t10_71;

    // 4-BLAC: 4x4 - 4x4
    _t11_20 = _mm256_sub_pd(_t11_20, _t6_36);
    _t11_21 = _mm256_sub_pd(_t11_21, _t6_37);
    _t11_22 = _mm256_sub_pd(_t11_22, _t6_38);
    _t11_23 = _mm256_sub_pd(_t11_23, _t6_39);

    // AVX Storer:

    // 4x4 -> 4x4 - LowTriang
    _t10_68 = _t11_20;
    _t10_69 = _t11_21;
    _t10_70 = _t11_22;
    _t10_71 = _t11_23;
  }

  _t12_28 = _mm256_loadu_pd(K + 332);
  _t12_29 = _mm256_loadu_pd(K + 352);
  _t12_30 = _mm256_loadu_pd(K + 372);
  _t12_31 = _mm256_loadu_pd(K + 392);
  _t12_15 = _mm256_broadcast_sd(K + 320);
  _t12_14 = _mm256_broadcast_sd(K + 321);
  _t12_13 = _mm256_broadcast_sd(K + 322);
  _t12_12 = _mm256_broadcast_sd(K + 323);
  _t12_11 = _mm256_broadcast_sd(K + 340);
  _t12_10 = _mm256_broadcast_sd(K + 341);
  _t12_9 = _mm256_broadcast_sd(K + 342);
  _t12_8 = _mm256_broadcast_sd(K + 343);
  _t12_7 = _mm256_broadcast_sd(K + 360);
  _t12_6 = _mm256_broadcast_sd(K + 361);
  _t12_5 = _mm256_broadcast_sd(K + 362);
  _t12_4 = _mm256_broadcast_sd(K + 363);
  _t12_3 = _mm256_broadcast_sd(K + 380);
  _t12_2 = _mm256_broadcast_sd(K + 381);
  _t12_1 = _mm256_broadcast_sd(K + 382);
  _t12_0 = _mm256_broadcast_sd(K + 383);

  // Generating : L[20,20] = S(h(1, 20, fi971), Sqrt( G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_32 = _t10_68;

  // 4-BLAC: sqrt(1x4)
  _t12_33 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t12_32)));

  // AVX Storer:
  _t10_68 = _t12_33;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t12_34 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_35 = _t10_68;

  // 4-BLAC: 1x4 / 1x4
  _t12_36 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_34), _mm256_castpd256_pd128(_t12_35)));

  // AVX Storer:
  _t12_16 = _t12_36;

  // Generating : L[20,20] = S(h(3, 20, fi971 + 1), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, fi971)) Kro G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_37 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_16, _t12_16, 32), _mm256_permute2f128_pd(_t12_16, _t12_16, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t12_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_69, _t10_70), _mm256_unpacklo_pd(_t10_71, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t12_39 = _mm256_mul_pd(_t12_37, _t12_38);

  // AVX Storer:
  _t12_17 = _t12_39;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 1), ( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) - ( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) Kro T( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_40 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_69, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_41 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_17, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_42 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_17, 1);

  // 4-BLAC: (4x1)^T
  _t6_63 = _t12_42;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_64 = _mm256_mul_pd(_t12_41, _t6_63);

  // 4-BLAC: 1x4 - 1x4
  _t12_43 = _mm256_sub_pd(_t12_40, _t6_64);

  // AVX Storer:
  _t12_18 = _t12_43;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 1), Sqrt( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_44 = _t12_18;

  // 4-BLAC: sqrt(1x4)
  _t12_45 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t12_44)));

  // AVX Storer:
  _t12_18 = _t12_45;

  // Generating : L[20,20] = S(h(2, 20, fi971 + 2), ( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) - ( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971)) Kro T( G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t12_46 = _mm256_unpackhi_pd(_mm256_blend_pd(_t10_70, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t10_71, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t12_47 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_17, 2), _mm256_permute2f128_pd(_t12_17, _t12_17, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_48 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_17, _t12_17, 32), _mm256_permute2f128_pd(_t12_17, _t12_17, 32), 0);

  // 4-BLAC: (4x1)^T
  _t6_71 = _t12_48;

  // 4-BLAC: 4x1 Kro 1x4
  _t6_72 = _mm256_mul_pd(_t12_47, _t6_71);

  // 4-BLAC: 4x1 - 4x1
  _t12_49 = _mm256_sub_pd(_t12_46, _t6_72);

  // AVX Storer:
  _t12_19 = _t12_49;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t12_50 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_51 = _t12_18;

  // 4-BLAC: 1x4 / 1x4
  _t12_52 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_50), _mm256_castpd256_pd128(_t12_51)));

  // AVX Storer:
  _t12_20 = _t12_52;

  // Generating : L[20,20] = S(h(2, 20, fi971 + 2), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, fi971 + 1)) Kro G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_53 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t12_20, _t12_20, 32), _mm256_permute2f128_pd(_t12_20, _t12_20, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t12_54 = _t12_19;

  // 4-BLAC: 1x4 Kro 4x1
  _t12_55 = _mm256_mul_pd(_t12_53, _t12_54);

  // AVX Storer:
  _t12_19 = _t12_55;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 2), ( G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) - ( G(h(1, 20, fi971 + 2), L[20,20],h(2, 20, fi971)) * T( G(h(1, 20, fi971 + 2), L[20,20],h(2, 20, fi971)) ) ) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_56 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_70, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_70, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_57 = _mm256_shuffle_pd(_mm256_blend_pd(_t12_17, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_19, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_58 = _mm256_shuffle_pd(_mm256_blend_pd(_t12_17, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_19, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t6_83 = _t12_58;

  // 4-BLAC: 1x4 * 4x1
  _t6_84 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_57, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_57, _t6_83), _mm256_mul_pd(_t12_57, _t6_83), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_57, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_57, _t6_83), _mm256_mul_pd(_t12_57, _t6_83), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_57, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_57, _t6_83), _mm256_mul_pd(_t12_57, _t6_83), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_59 = _mm256_sub_pd(_t12_56, _t6_84);

  // AVX Storer:
  _t12_21 = _t12_59;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 2), Sqrt( G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_60 = _t12_21;

  // 4-BLAC: sqrt(1x4)
  _t12_61 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t12_60)));

  // AVX Storer:
  _t12_21 = _t12_61;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) - ( G(h(1, 20, fi971 + 3), L[20,20],h(2, 20, fi971)) * T( G(h(1, 20, fi971 + 2), L[20,20],h(2, 20, fi971)) ) ) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_62 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_71, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t10_71, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_63 = _mm256_blend_pd(_mm256_permute2f128_pd(_t12_17, _t12_17, 129), _t12_19, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t12_64 = _mm256_shuffle_pd(_mm256_blend_pd(_t12_17, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_19, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t6_91 = _t12_64;

  // 4-BLAC: 1x4 * 4x1
  _t6_92 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_63, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_63, _t6_91), _mm256_mul_pd(_t12_63, _t6_91), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_63, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_63, _t6_91), _mm256_mul_pd(_t12_63, _t6_91), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_63, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_63, _t6_91), _mm256_mul_pd(_t12_63, _t6_91), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_65 = _mm256_sub_pd(_t12_62, _t6_92);

  // AVX Storer:
  _t12_22 = _t12_65;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_66 = _t12_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_67 = _t12_21;

  // 4-BLAC: 1x4 / 1x4
  _t12_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t12_66), _mm256_castpd256_pd128(_t12_67)));

  // AVX Storer:
  _t12_22 = _t12_68;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi971 + 3), L[20,20],h(3, 20, fi971)) * T( G(h(1, 20, fi971 + 3), L[20,20],h(3, 20, fi971)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_69 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t10_71, _t10_71, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_70 = _mm256_blend_pd(_mm256_permute2f128_pd(_t12_17, _t12_22, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t12_19, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t12_71 = _mm256_blend_pd(_mm256_permute2f128_pd(_t12_17, _t12_22, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t12_19, 2), 10);

  // 4-BLAC: (1x4)^T
  _t6_100 = _t12_71;

  // 4-BLAC: 1x4 * 4x1
  _t6_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t12_70, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_70, _t6_100), _mm256_mul_pd(_t12_70, _t6_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t12_70, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_70, _t6_100), _mm256_mul_pd(_t12_70, _t6_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t12_70, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t12_70, _t6_100), _mm256_mul_pd(_t12_70, _t6_100), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t12_72 = _mm256_sub_pd(_t12_69, _t6_101);

  // AVX Storer:
  _t12_23 = _t12_72;

  // Generating : L[20,20] = S(h(1, 20, fi971 + 3), Sqrt( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t12_73 = _t12_23;

  // 4-BLAC: sqrt(1x4)
  _t12_74 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t12_73)));

  // AVX Storer:
  _t12_23 = _t12_74;

  // Generating : L[20,20] = ( Sum_{k207} ( S(h(4, 20, fi971 + k207 + 4), ( G(h(4, 20, fi971 + k207 + 4), K[20,20],h(4, 20, fi971)) - ( G(h(4, 20, fi971 + k207 + 4), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, fi971)) ) + Sum_{k159} ( Sum_{k207} ( -$(h(4, 20, fi971 + k207 + 4), ( G(h(4, 20, fi971 + k207 + 4), L[20,20],h(4, 20, k159)) * T( G(h(4, 20, fi971), L[20,20],h(4, 20, k159)) ) ),h(4, 20, fi971)) ) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
  _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
  _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
  _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
  _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
  _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
  _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t12_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t6_113), _mm256_mul_pd(_t12_14, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t6_115), _mm256_mul_pd(_t12_12, _t6_116)));
  _t12_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t6_113), _mm256_mul_pd(_t12_10, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t6_115), _mm256_mul_pd(_t12_8, _t6_116)));
  _t12_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t6_113), _mm256_mul_pd(_t12_6, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t6_115), _mm256_mul_pd(_t12_4, _t6_116)));
  _t12_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_3, _t6_113), _mm256_mul_pd(_t12_2, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t12_1, _t6_115), _mm256_mul_pd(_t12_0, _t6_116)));

  // 4-BLAC: 4x4 - 4x4
  _t12_28 = _mm256_sub_pd(_t12_28, _t12_24);
  _t12_29 = _mm256_sub_pd(_t12_29, _t12_25);
  _t12_30 = _mm256_sub_pd(_t12_30, _t12_26);
  _t12_31 = _mm256_sub_pd(_t12_31, _t12_27);

  // AVX Storer:

  _mm_store_sd(&(K[328]), _mm256_castpd256_pd128(_t10_44));
  _mm_store_sd(&(K[329]), _mm256_castpd256_pd128(_t10_46));
  _mm_store_sd(&(K[330]), _mm256_castpd256_pd128(_t10_48));
  _mm_store_sd(&(K[331]), _mm256_castpd256_pd128(_t10_49));
  _mm_store_sd(&(K[348]), _mm256_castpd256_pd128(_t10_50));
  _mm_store_sd(&(K[349]), _mm256_castpd256_pd128(_t10_52));
  _mm_store_sd(&(K[350]), _mm256_castpd256_pd128(_t10_54));
  _mm_store_sd(&(K[351]), _mm256_castpd256_pd128(_t10_55));
  _mm_store_sd(&(K[368]), _mm256_castpd256_pd128(_t10_56));
  _mm_store_sd(&(K[369]), _mm256_castpd256_pd128(_t10_58));
  _mm_store_sd(&(K[370]), _mm256_castpd256_pd128(_t10_60));
  _mm_store_sd(&(K[371]), _mm256_castpd256_pd128(_t10_61));
  _mm_store_sd(&(K[388]), _mm256_castpd256_pd128(_t10_62));
  _mm_store_sd(&(K[389]), _mm256_castpd256_pd128(_t10_64));
  _mm_store_sd(&(K[390]), _mm256_castpd256_pd128(_t10_66));
  _mm_store_sd(&(K[391]), _mm256_castpd256_pd128(_t10_67));

  for( int k159 = 4; k159 <= 11; k159+=4 ) {
    _t13_19 = _mm256_broadcast_sd(K + k159 + 320);
    _t13_18 = _mm256_broadcast_sd(K + k159 + 321);
    _t13_17 = _mm256_broadcast_sd(K + k159 + 322);
    _t13_16 = _mm256_broadcast_sd(K + k159 + 323);
    _t13_15 = _mm256_broadcast_sd(K + k159 + 340);
    _t13_14 = _mm256_broadcast_sd(K + k159 + 341);
    _t13_13 = _mm256_broadcast_sd(K + k159 + 342);
    _t13_12 = _mm256_broadcast_sd(K + k159 + 343);
    _t13_11 = _mm256_broadcast_sd(K + k159 + 360);
    _t13_10 = _mm256_broadcast_sd(K + k159 + 361);
    _t13_9 = _mm256_broadcast_sd(K + k159 + 362);
    _t13_8 = _mm256_broadcast_sd(K + k159 + 363);
    _t13_7 = _mm256_broadcast_sd(K + k159 + 380);
    _t13_6 = _mm256_broadcast_sd(K + k159 + 381);
    _t13_5 = _mm256_broadcast_sd(K + k159 + 382);
    _t13_4 = _mm256_broadcast_sd(K + k159 + 383);
    _t13_3 = _mm256_loadu_pd(K + k159 + 240);
    _t13_2 = _mm256_loadu_pd(K + k159 + 260);
    _t13_1 = _mm256_loadu_pd(K + k159 + 280);
    _t13_0 = _mm256_loadu_pd(K + k159 + 300);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 32);
    _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 32);
    _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 49);
    _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 32);
    _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 32);
    _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 49);
    _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t8_0), _mm256_mul_pd(_t13_18, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t8_2), _mm256_mul_pd(_t13_16, _t8_3)));
    _t13_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t8_0), _mm256_mul_pd(_t13_14, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t8_2), _mm256_mul_pd(_t13_12, _t8_3)));
    _t13_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t8_0), _mm256_mul_pd(_t13_10, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t8_2), _mm256_mul_pd(_t13_8, _t8_3)));
    _t13_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t8_0), _mm256_mul_pd(_t13_6, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t8_2), _mm256_mul_pd(_t13_4, _t8_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t12_28 = _mm256_sub_pd(_t12_28, _t13_20);
    _t12_29 = _mm256_sub_pd(_t12_29, _t13_21);
    _t12_30 = _mm256_sub_pd(_t12_30, _t13_22);
    _t12_31 = _mm256_sub_pd(_t12_31, _t13_23);

    // AVX Storer:
  }

  _t14_28 = _mm256_castpd128_pd256(_mm_load_sd(K + 336));
  _t14_29 = _mm256_maskload_pd(K + 356, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t14_30 = _mm256_maskload_pd(K + 376, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t14_31 = _mm256_loadu_pd(K + 396);
  _t14_3 = _mm256_loadu_pd(K + 320);
  _t14_2 = _mm256_loadu_pd(K + 340);
  _t14_1 = _mm256_loadu_pd(K + 360);
  _t14_0 = _mm256_loadu_pd(K + 380);

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_40 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_28, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_41 = _t10_68;

  // 4-BLAC: 1x4 / 1x4
  _t14_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_40), _mm256_castpd256_pd128(_t14_41)));

  // AVX Storer:
  _t14_4 = _t14_42;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t14_43 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_28, 14), _mm256_permute2f128_pd(_t12_28, _t12_28, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_4, _t14_4, 32), _mm256_permute2f128_pd(_t14_4, _t14_4, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t14_45 = _t12_17;

  // 4-BLAC: (4x1)^T
  _t10_78 = _t14_45;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_79 = _mm256_mul_pd(_t14_44, _t10_78);

  // 4-BLAC: 1x4 - 1x4
  _t14_46 = _mm256_sub_pd(_t14_43, _t10_79);

  // AVX Storer:
  _t14_5 = _t14_46;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_47 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_48 = _t12_18;

  // 4-BLAC: 1x4 / 1x4
  _t14_49 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_47), _mm256_castpd256_pd128(_t14_48)));

  // AVX Storer:
  _t14_6 = _t14_49;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t14_50 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_5, 6), _mm256_permute2f128_pd(_t14_5, _t14_5, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_6, _t14_6, 32), _mm256_permute2f128_pd(_t14_6, _t14_6, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t14_52 = _t12_19;

  // 4-BLAC: (4x1)^T
  _t10_87 = _t14_52;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_88 = _mm256_mul_pd(_t14_51, _t10_87);

  // 4-BLAC: 1x4 - 1x4
  _t14_53 = _mm256_sub_pd(_t14_50, _t10_88);

  // AVX Storer:
  _t14_7 = _t14_53;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_54 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_55 = _t12_21;

  // 4-BLAC: 1x4 / 1x4
  _t14_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_54), _mm256_castpd256_pd128(_t14_55)));

  // AVX Storer:
  _t14_8 = _t14_56;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_57 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_7, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_58 = _t14_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_59 = _t12_22;

  // 4-BLAC: (4x1)^T
  _t10_96 = _t14_59;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_97 = _mm256_mul_pd(_t14_58, _t10_96);

  // 4-BLAC: 1x4 - 1x4
  _t14_60 = _mm256_sub_pd(_t14_57, _t10_97);

  // AVX Storer:
  _t14_9 = _t14_60;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 4), ( G(h(1, 20, fi1090 + fi971 + 4), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_61 = _t14_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_62 = _t12_23;

  // 4-BLAC: 1x4 / 1x4
  _t14_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_61), _mm256_castpd256_pd128(_t14_62)));

  // AVX Storer:
  _t14_9 = _t14_63;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_64 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_29, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_65 = _t10_68;

  // 4-BLAC: 1x4 / 1x4
  _t14_66 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_64), _mm256_castpd256_pd128(_t14_65)));

  // AVX Storer:
  _t14_10 = _t14_66;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t14_67 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_29, 14), _mm256_permute2f128_pd(_t12_29, _t12_29, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_68 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_10, _t14_10, 32), _mm256_permute2f128_pd(_t14_10, _t14_10, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t14_69 = _t12_17;

  // 4-BLAC: (4x1)^T
  _t10_108 = _t14_69;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_109 = _mm256_mul_pd(_t14_68, _t10_108);

  // 4-BLAC: 1x4 - 1x4
  _t14_70 = _mm256_sub_pd(_t14_67, _t10_109);

  // AVX Storer:
  _t14_11 = _t14_70;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_71 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_11, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_72 = _t12_18;

  // 4-BLAC: 1x4 / 1x4
  _t14_73 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_71), _mm256_castpd256_pd128(_t14_72)));

  // AVX Storer:
  _t14_12 = _t14_73;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t14_74 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_11, 6), _mm256_permute2f128_pd(_t14_11, _t14_11, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_75 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_12, _t14_12, 32), _mm256_permute2f128_pd(_t14_12, _t14_12, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t14_76 = _t12_19;

  // 4-BLAC: (4x1)^T
  _t10_117 = _t14_76;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_118 = _mm256_mul_pd(_t14_75, _t10_117);

  // 4-BLAC: 1x4 - 1x4
  _t14_77 = _mm256_sub_pd(_t14_74, _t10_118);

  // AVX Storer:
  _t14_13 = _t14_77;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_78 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_79 = _t12_21;

  // 4-BLAC: 1x4 / 1x4
  _t14_80 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_78), _mm256_castpd256_pd128(_t14_79)));

  // AVX Storer:
  _t14_14 = _t14_80;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_81 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_82 = _t14_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_83 = _t12_22;

  // 4-BLAC: (4x1)^T
  _t10_126 = _t14_83;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_127 = _mm256_mul_pd(_t14_82, _t10_126);

  // 4-BLAC: 1x4 - 1x4
  _t14_84 = _mm256_sub_pd(_t14_81, _t10_127);

  // AVX Storer:
  _t14_15 = _t14_84;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 5), ( G(h(1, 20, fi1090 + fi971 + 5), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_85 = _t14_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_86 = _t12_23;

  // 4-BLAC: 1x4 / 1x4
  _t14_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_85), _mm256_castpd256_pd128(_t14_86)));

  // AVX Storer:
  _t14_15 = _t14_87;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_88 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_30, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_89 = _t10_68;

  // 4-BLAC: 1x4 / 1x4
  _t14_90 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_88), _mm256_castpd256_pd128(_t14_89)));

  // AVX Storer:
  _t14_16 = _t14_90;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t14_91 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_30, 14), _mm256_permute2f128_pd(_t12_30, _t12_30, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_92 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_16, _t14_16, 32), _mm256_permute2f128_pd(_t14_16, _t14_16, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t14_93 = _t12_17;

  // 4-BLAC: (4x1)^T
  _t10_138 = _t14_93;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_139 = _mm256_mul_pd(_t14_92, _t10_138);

  // 4-BLAC: 1x4 - 1x4
  _t14_94 = _mm256_sub_pd(_t14_91, _t10_139);

  // AVX Storer:
  _t14_17 = _t14_94;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_95 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_17, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_96 = _t12_18;

  // 4-BLAC: 1x4 / 1x4
  _t14_97 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_95), _mm256_castpd256_pd128(_t14_96)));

  // AVX Storer:
  _t14_18 = _t14_97;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t14_98 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_17, 6), _mm256_permute2f128_pd(_t14_17, _t14_17, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_99 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_18, _t14_18, 32), _mm256_permute2f128_pd(_t14_18, _t14_18, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t14_100 = _t12_19;

  // 4-BLAC: (4x1)^T
  _t10_147 = _t14_100;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_148 = _mm256_mul_pd(_t14_99, _t10_147);

  // 4-BLAC: 1x4 - 1x4
  _t14_101 = _mm256_sub_pd(_t14_98, _t10_148);

  // AVX Storer:
  _t14_19 = _t14_101;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_102 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_103 = _t12_21;

  // 4-BLAC: 1x4 / 1x4
  _t14_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_102), _mm256_castpd256_pd128(_t14_103)));

  // AVX Storer:
  _t14_20 = _t14_104;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_19, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_106 = _t14_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_107 = _t12_22;

  // 4-BLAC: (4x1)^T
  _t10_156 = _t14_107;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_157 = _mm256_mul_pd(_t14_106, _t10_156);

  // 4-BLAC: 1x4 - 1x4
  _t14_108 = _mm256_sub_pd(_t14_105, _t10_157);

  // AVX Storer:
  _t14_21 = _t14_108;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 6), ( G(h(1, 20, fi1090 + fi971 + 6), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_109 = _t14_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_110 = _t12_23;

  // 4-BLAC: 1x4 / 1x4
  _t14_111 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_109), _mm256_castpd256_pd128(_t14_110)));

  // AVX Storer:
  _t14_21 = _t14_111;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971)) Div G(h(1, 20, fi971), L[20,20],h(1, 20, fi971)) ),h(1, 20, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_112 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_31, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_113 = _t10_68;

  // 4-BLAC: 1x4 / 1x4
  _t14_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_112), _mm256_castpd256_pd128(_t14_113)));

  // AVX Storer:
  _t14_22 = _t14_114;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(3, 20, fi971 + 1)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971)) Kro T( G(h(3, 20, fi971 + 1), L[20,20],h(1, 20, fi971)) ) ) ),h(3, 20, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t14_115 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_31, 14), _mm256_permute2f128_pd(_t12_31, _t12_31, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_116 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_22, _t14_22, 32), _mm256_permute2f128_pd(_t14_22, _t14_22, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t14_117 = _t12_17;

  // 4-BLAC: (4x1)^T
  _t10_168 = _t14_117;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_169 = _mm256_mul_pd(_t14_116, _t10_168);

  // 4-BLAC: 1x4 - 1x4
  _t14_118 = _mm256_sub_pd(_t14_115, _t10_169);

  // AVX Storer:
  _t14_23 = _t14_118;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 1)) Div G(h(1, 20, fi971 + 1), L[20,20],h(1, 20, fi971 + 1)) ),h(1, 20, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_119 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_23, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_120 = _t12_18;

  // 4-BLAC: 1x4 / 1x4
  _t14_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_119), _mm256_castpd256_pd128(_t14_120)));

  // AVX Storer:
  _t14_24 = _t14_121;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(2, 20, fi971 + 2)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 1)) Kro T( G(h(2, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 1)) ) ) ),h(2, 20, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t14_122 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_23, 6), _mm256_permute2f128_pd(_t14_23, _t14_23, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_123 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_24, _t14_24, 32), _mm256_permute2f128_pd(_t14_24, _t14_24, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t14_124 = _t12_19;

  // 4-BLAC: (4x1)^T
  _t10_177 = _t14_124;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_178 = _mm256_mul_pd(_t14_123, _t10_177);

  // 4-BLAC: 1x4 - 1x4
  _t14_125 = _mm256_sub_pd(_t14_122, _t10_178);

  // AVX Storer:
  _t14_25 = _t14_125;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 2)) Div G(h(1, 20, fi971 + 2), L[20,20],h(1, 20, fi971 + 2)) ),h(1, 20, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_25, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_127 = _t12_21;

  // 4-BLAC: 1x4 / 1x4
  _t14_128 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_126), _mm256_castpd256_pd128(_t14_127)));

  // AVX Storer:
  _t14_26 = _t14_128;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 3)) - ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 2)) Kro T( G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 2)) ) ) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_129 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_25, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_130 = _t14_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_131 = _t12_22;

  // 4-BLAC: (4x1)^T
  _t10_186 = _t14_131;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_187 = _mm256_mul_pd(_t14_130, _t10_186);

  // 4-BLAC: 1x4 - 1x4
  _t14_132 = _mm256_sub_pd(_t14_129, _t10_187);

  // AVX Storer:
  _t14_27 = _t14_132;

  // Generating : L[20,20] = S(h(1, 20, fi1090 + fi971 + 7), ( G(h(1, 20, fi1090 + fi971 + 7), L[20,20],h(1, 20, fi971 + 3)) Div G(h(1, 20, fi971 + 3), L[20,20],h(1, 20, fi971 + 3)) ),h(1, 20, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_133 = _t14_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t14_134 = _t12_23;

  // 4-BLAC: 1x4 / 1x4
  _t14_135 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_133), _mm256_castpd256_pd128(_t14_134)));

  // AVX Storer:
  _t14_27 = _t14_135;

  // Generating : L[20,20] = ( S(h(4, 20, 16), ( G(h(4, 20, 16), K[20,20],h(4, 20, 16)) - ( G(h(4, 20, 16), L[20,20],h(4, 20, 0)) * T( G(h(4, 20, 16), L[20,20],h(4, 20, 0)) ) ) ),h(4, 20, 16)) + Sum_{k159} ( -$(h(4, 20, 16), ( G(h(4, 20, 16), L[20,20],h(4, 20, k159)) * T( G(h(4, 20, 16), L[20,20],h(4, 20, k159)) ) ),h(4, 20, 16)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t14_136 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_28, _t14_29, 0), _mm256_shuffle_pd(_t14_30, _t14_31, 0), 32);
  _t14_137 = _mm256_permute2f128_pd(_t14_29, _mm256_shuffle_pd(_t14_30, _t14_31, 3), 32);
  _t14_138 = _mm256_blend_pd(_t14_30, _mm256_shuffle_pd(_t14_30, _t14_31, 3), 12);
  _t14_139 = _t14_31;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t14_140 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_3, _t14_2), _mm256_unpacklo_pd(_t14_1, _t14_0), 32);
  _t14_141 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_3, _t14_2), _mm256_unpackhi_pd(_t14_1, _t14_0), 32);
  _t14_142 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_3, _t14_2), _mm256_unpacklo_pd(_t14_1, _t14_0), 49);
  _t14_143 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_3, _t14_2), _mm256_unpackhi_pd(_t14_1, _t14_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t14_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t14_140), _mm256_mul_pd(_t12_14, _t14_141)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t14_142), _mm256_mul_pd(_t12_12, _t14_143)));
  _t14_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t14_140), _mm256_mul_pd(_t12_10, _t14_141)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t14_142), _mm256_mul_pd(_t12_8, _t14_143)));
  _t14_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t14_140), _mm256_mul_pd(_t12_6, _t14_141)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t14_142), _mm256_mul_pd(_t12_4, _t14_143)));
  _t14_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_3, _t14_140), _mm256_mul_pd(_t12_2, _t14_141)), _mm256_add_pd(_mm256_mul_pd(_t12_1, _t14_142), _mm256_mul_pd(_t12_0, _t14_143)));

  // 4-BLAC: 4x4 - 4x4
  _t14_36 = _mm256_sub_pd(_t14_136, _t14_32);
  _t14_37 = _mm256_sub_pd(_t14_137, _t14_33);
  _t14_38 = _mm256_sub_pd(_t14_138, _t14_34);
  _t14_39 = _mm256_sub_pd(_t14_139, _t14_35);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t14_28 = _t14_36;
  _t14_29 = _t14_37;
  _t14_30 = _t14_38;
  _t14_31 = _t14_39;

  _mm_store_sd(&(K[332]), _mm256_castpd256_pd128(_t14_4));
  _mm_store_sd(&(K[333]), _mm256_castpd256_pd128(_t14_6));
  _mm_store_sd(&(K[334]), _mm256_castpd256_pd128(_t14_8));
  _mm_store_sd(&(K[335]), _mm256_castpd256_pd128(_t14_9));
  _mm_store_sd(&(K[352]), _mm256_castpd256_pd128(_t14_10));
  _mm_store_sd(&(K[353]), _mm256_castpd256_pd128(_t14_12));
  _mm_store_sd(&(K[354]), _mm256_castpd256_pd128(_t14_14));
  _mm_store_sd(&(K[355]), _mm256_castpd256_pd128(_t14_15));
  _mm_store_sd(&(K[372]), _mm256_castpd256_pd128(_t14_16));
  _mm_store_sd(&(K[373]), _mm256_castpd256_pd128(_t14_18));
  _mm_store_sd(&(K[374]), _mm256_castpd256_pd128(_t14_20));
  _mm_store_sd(&(K[375]), _mm256_castpd256_pd128(_t14_21));
  _mm_store_sd(&(K[392]), _mm256_castpd256_pd128(_t14_22));
  _mm_store_sd(&(K[393]), _mm256_castpd256_pd128(_t14_24));
  _mm_store_sd(&(K[394]), _mm256_castpd256_pd128(_t14_26));
  _mm_store_sd(&(K[395]), _mm256_castpd256_pd128(_t14_27));

  for( int k159 = 4; k159 <= 15; k159+=4 ) {
    _t15_19 = _mm256_broadcast_sd(K + k159 + 320);
    _t15_18 = _mm256_broadcast_sd(K + k159 + 321);
    _t15_17 = _mm256_broadcast_sd(K + k159 + 322);
    _t15_16 = _mm256_broadcast_sd(K + k159 + 323);
    _t15_15 = _mm256_broadcast_sd(K + k159 + 340);
    _t15_14 = _mm256_broadcast_sd(K + k159 + 341);
    _t15_13 = _mm256_broadcast_sd(K + k159 + 342);
    _t15_12 = _mm256_broadcast_sd(K + k159 + 343);
    _t15_11 = _mm256_broadcast_sd(K + k159 + 360);
    _t15_10 = _mm256_broadcast_sd(K + k159 + 361);
    _t15_9 = _mm256_broadcast_sd(K + k159 + 362);
    _t15_8 = _mm256_broadcast_sd(K + k159 + 363);
    _t15_7 = _mm256_broadcast_sd(K + k159 + 380);
    _t15_6 = _mm256_broadcast_sd(K + k159 + 381);
    _t15_5 = _mm256_broadcast_sd(K + k159 + 382);
    _t15_4 = _mm256_broadcast_sd(K + k159 + 383);
    _t15_3 = _mm256_loadu_pd(K + k159 + 320);
    _t15_2 = _mm256_loadu_pd(K + k159 + 340);
    _t15_1 = _mm256_loadu_pd(K + k159 + 360);
    _t15_0 = _mm256_loadu_pd(K + k159 + 380);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t15_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 32);
    _t15_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 32);
    _t15_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 49);
    _t15_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t15_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_28), _mm256_mul_pd(_t15_18, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_30), _mm256_mul_pd(_t15_16, _t15_31)));
    _t15_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_28), _mm256_mul_pd(_t15_14, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_30), _mm256_mul_pd(_t15_12, _t15_31)));
    _t15_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_28), _mm256_mul_pd(_t15_10, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_30), _mm256_mul_pd(_t15_8, _t15_31)));
    _t15_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_7, _t15_28), _mm256_mul_pd(_t15_6, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_5, _t15_30), _mm256_mul_pd(_t15_4, _t15_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowTriang
    _t15_24 = _t14_28;
    _t15_25 = _t14_29;
    _t15_26 = _t14_30;
    _t15_27 = _t14_31;

    // 4-BLAC: 4x4 - 4x4
    _t15_24 = _mm256_sub_pd(_t15_24, _t15_20);
    _t15_25 = _mm256_sub_pd(_t15_25, _t15_21);
    _t15_26 = _mm256_sub_pd(_t15_26, _t15_22);
    _t15_27 = _mm256_sub_pd(_t15_27, _t15_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowTriang
    _t14_28 = _t15_24;
    _t14_29 = _t15_25;
    _t14_30 = _t15_26;
    _t14_31 = _t15_27;
  }


  // Generating : L[20,20] = S(h(1, 20, 16), Sqrt( G(h(1, 20, 16), L[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_8 = _t14_28;

  // 4-BLAC: sqrt(1x4)
  _t16_9 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t16_8)));

  // AVX Storer:
  _t14_28 = _t16_9;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 16), L[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t16_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_11 = _t14_28;

  // 4-BLAC: 1x4 / 1x4
  _t16_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_10), _mm256_castpd256_pd128(_t16_11)));

  // AVX Storer:
  _t16_0 = _t16_12;

  // Generating : L[20,20] = S(h(3, 20, 17), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, 16)) Kro G(h(3, 20, 17), L[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_0, _t16_0, 32), _mm256_permute2f128_pd(_t16_0, _t16_0, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t16_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_29, _t14_30), _mm256_unpacklo_pd(_t14_31, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t16_15 = _mm256_mul_pd(_t16_13, _t16_14);

  // AVX Storer:
  _t16_1 = _t16_15;

  // Generating : L[20,20] = S(h(1, 20, 17), ( G(h(1, 20, 17), L[20,20],h(1, 20, 17)) - ( G(h(1, 20, 17), L[20,20],h(1, 20, 16)) Kro T( G(h(1, 20, 17), L[20,20],h(1, 20, 16)) ) ) ),h(1, 20, 17))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_16 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_1, 1);

  // 4-BLAC: (4x1)^T
  _t16_19 = _t16_18;

  // 4-BLAC: 1x4 Kro 1x4
  _t16_20 = _mm256_mul_pd(_t16_17, _t16_19);

  // 4-BLAC: 1x4 - 1x4
  _t16_21 = _mm256_sub_pd(_t16_16, _t16_20);

  // AVX Storer:
  _t16_2 = _t16_21;

  // Generating : L[20,20] = S(h(1, 20, 17), Sqrt( G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_22 = _t16_2;

  // 4-BLAC: sqrt(1x4)
  _t16_23 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t16_22)));

  // AVX Storer:
  _t16_2 = _t16_23;

  // Generating : L[20,20] = S(h(2, 20, 18), ( G(h(2, 20, 18), L[20,20],h(1, 20, 17)) - ( G(h(2, 20, 18), L[20,20],h(1, 20, 16)) Kro T( G(h(1, 20, 17), L[20,20],h(1, 20, 16)) ) ) ),h(1, 20, 17))

  // AVX Loader:

  // 2x1 -> 4x1
  _t16_24 = _mm256_unpackhi_pd(_mm256_blend_pd(_t14_30, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t14_31, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t16_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_1, 2), _mm256_permute2f128_pd(_t16_1, _t16_1, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_1, _t16_1, 32), _mm256_permute2f128_pd(_t16_1, _t16_1, 32), 0);

  // 4-BLAC: (4x1)^T
  _t16_27 = _t16_26;

  // 4-BLAC: 4x1 Kro 1x4
  _t16_28 = _mm256_mul_pd(_t16_25, _t16_27);

  // 4-BLAC: 4x1 - 4x1
  _t16_29 = _mm256_sub_pd(_t16_24, _t16_28);

  // AVX Storer:
  _t16_3 = _t16_29;

  // Generating : T1496[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 17), L[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t16_30 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_31 = _t16_2;

  // 4-BLAC: 1x4 / 1x4
  _t16_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_30), _mm256_castpd256_pd128(_t16_31)));

  // AVX Storer:
  _t16_4 = _t16_32;

  // Generating : L[20,20] = S(h(2, 20, 18), ( G(h(1, 1, 0), T1496[1,20],h(1, 20, 17)) Kro G(h(2, 20, 18), L[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_4, _t16_4, 32), _mm256_permute2f128_pd(_t16_4, _t16_4, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t16_34 = _t16_3;

  // 4-BLAC: 1x4 Kro 4x1
  _t16_35 = _mm256_mul_pd(_t16_33, _t16_34);

  // AVX Storer:
  _t16_3 = _t16_35;

  // Generating : L[20,20] = S(h(1, 20, 18), ( G(h(1, 20, 18), L[20,20],h(1, 20, 18)) - ( G(h(1, 20, 18), L[20,20],h(2, 20, 16)) * T( G(h(1, 20, 18), L[20,20],h(2, 20, 16)) ) ) ),h(1, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_36 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_30, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t14_30, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t16_37 = _mm256_shuffle_pd(_mm256_blend_pd(_t16_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t16_3, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t16_38 = _mm256_shuffle_pd(_mm256_blend_pd(_t16_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t16_3, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t16_39 = _t16_38;

  // 4-BLAC: 1x4 * 4x1
  _t16_40 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_37, _t16_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_37, _t16_39), _mm256_mul_pd(_t16_37, _t16_39), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_37, _t16_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_37, _t16_39), _mm256_mul_pd(_t16_37, _t16_39), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_37, _t16_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_37, _t16_39), _mm256_mul_pd(_t16_37, _t16_39), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t16_41 = _mm256_sub_pd(_t16_36, _t16_40);

  // AVX Storer:
  _t16_5 = _t16_41;

  // Generating : L[20,20] = S(h(1, 20, 18), Sqrt( G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ),h(1, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_42 = _t16_5;

  // 4-BLAC: sqrt(1x4)
  _t16_43 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t16_42)));

  // AVX Storer:
  _t16_5 = _t16_43;

  // Generating : L[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), L[20,20],h(1, 20, 18)) - ( G(h(1, 20, 19), L[20,20],h(2, 20, 16)) * T( G(h(1, 20, 18), L[20,20],h(2, 20, 16)) ) ) ),h(1, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_44 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_31, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t14_31, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t16_45 = _mm256_blend_pd(_mm256_permute2f128_pd(_t16_1, _t16_1, 129), _t16_3, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t16_46 = _mm256_shuffle_pd(_mm256_blend_pd(_t16_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t16_3, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t16_47 = _t16_46;

  // 4-BLAC: 1x4 * 4x1
  _t16_48 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_45, _t16_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_45, _t16_47), _mm256_mul_pd(_t16_45, _t16_47), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_45, _t16_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_45, _t16_47), _mm256_mul_pd(_t16_45, _t16_47), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_45, _t16_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_45, _t16_47), _mm256_mul_pd(_t16_45, _t16_47), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t16_49 = _mm256_sub_pd(_t16_44, _t16_48);

  // AVX Storer:
  _t16_6 = _t16_49;

  // Generating : L[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), L[20,20],h(1, 20, 18)) Div G(h(1, 20, 18), L[20,20],h(1, 20, 18)) ),h(1, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_50 = _t16_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_51 = _t16_5;

  // 4-BLAC: 1x4 / 1x4
  _t16_52 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_50), _mm256_castpd256_pd128(_t16_51)));

  // AVX Storer:
  _t16_6 = _t16_52;

  // Generating : L[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), L[20,20],h(1, 20, 19)) - ( G(h(1, 20, 19), L[20,20],h(3, 20, 16)) * T( G(h(1, 20, 19), L[20,20],h(3, 20, 16)) ) ) ),h(1, 20, 19))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_53 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t14_31, _t14_31, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t16_54 = _mm256_blend_pd(_mm256_permute2f128_pd(_t16_1, _t16_6, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t16_3, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t16_55 = _mm256_blend_pd(_mm256_permute2f128_pd(_t16_1, _t16_6, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t16_3, 2), 10);

  // 4-BLAC: (1x4)^T
  _t16_56 = _t16_55;

  // 4-BLAC: 1x4 * 4x1
  _t16_57 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t16_54, _t16_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_54, _t16_56), _mm256_mul_pd(_t16_54, _t16_56), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t16_54, _t16_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_54, _t16_56), _mm256_mul_pd(_t16_54, _t16_56), 129)), _mm256_add_pd(_mm256_mul_pd(_t16_54, _t16_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t16_54, _t16_56), _mm256_mul_pd(_t16_54, _t16_56), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t16_58 = _mm256_sub_pd(_t16_53, _t16_57);

  // AVX Storer:
  _t16_7 = _t16_58;

  // Generating : L[20,20] = S(h(1, 20, 19), Sqrt( G(h(1, 20, 19), L[20,20],h(1, 20, 19)) ),h(1, 20, 19))

  // AVX Loader:

  // 1x1 -> 1x4
  _t16_59 = _t16_7;

  // 4-BLAC: sqrt(1x4)
  _t16_60 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t16_59)));

  // AVX Storer:
  _t16_7 = _t16_60;

  _mm_store_sd(&(K[0]), _mm256_castpd256_pd128(_t0_0));
  _mm256_maskstore_pd(K + 20, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_2);
  _mm256_maskstore_pd(K + 40, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_2, _t0_2, 1));
  _mm256_maskstore_pd(K + 60, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t0_2, _t0_2, 129));
  _mm_store_sd(&(K[21]), _mm256_castpd256_pd128(_t0_3));
  _mm256_maskstore_pd(K + 41, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_4);
  _mm256_maskstore_pd(K + 61, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_4, _t0_4, 1));
  _mm_store_sd(&(K[42]), _mm256_castpd256_pd128(_t0_6));
  _mm_store_sd(&(K[62]), _mm256_castpd256_pd128(_t0_7));
  _mm_store_sd(&(K[63]), _mm256_castpd256_pd128(_t0_8));
  _mm_store_sd(&(K[80]), _mm256_castpd256_pd128(_t0_9));
  _mm_store_sd(&(K[81]), _mm256_castpd256_pd128(_t0_11));
  _mm_store_sd(&(K[82]), _mm256_castpd256_pd128(_t0_13));
  _mm_store_sd(&(K[83]), _mm256_castpd256_pd128(_t0_14));
  _mm_store_sd(&(K[100]), _mm256_castpd256_pd128(_t0_15));
  _mm_store_sd(&(K[101]), _mm256_castpd256_pd128(_t0_17));
  _mm_store_sd(&(K[102]), _mm256_castpd256_pd128(_t0_19));
  _mm_store_sd(&(K[103]), _mm256_castpd256_pd128(_t0_20));
  _mm_store_sd(&(K[120]), _mm256_castpd256_pd128(_t0_21));
  _mm_store_sd(&(K[121]), _mm256_castpd256_pd128(_t0_23));
  _mm_store_sd(&(K[122]), _mm256_castpd256_pd128(_t0_25));
  _mm_store_sd(&(K[123]), _mm256_castpd256_pd128(_t0_26));
  _mm_store_sd(&(K[140]), _mm256_castpd256_pd128(_t0_27));
  _mm_store_sd(&(K[141]), _mm256_castpd256_pd128(_t0_29));
  _mm_store_sd(&(K[142]), _mm256_castpd256_pd128(_t0_31));
  _mm_store_sd(&(K[143]), _mm256_castpd256_pd128(_t0_32));
  _mm_store_sd(K + 84, _mm256_castpd256_pd128(_t2_0));
  _mm256_maskstore_pd(K + 104, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_5);
  _mm256_maskstore_pd(K + 124, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t2_5, _t2_5, 1));
  _mm256_maskstore_pd(K + 144, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t2_5, _t2_5, 129));
  _mm_store_sd(&(K[105]), _mm256_castpd256_pd128(_t2_6));
  _mm256_maskstore_pd(K + 125, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_7);
  _mm256_maskstore_pd(K + 145, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t2_7, _t2_7, 1));
  _mm_store_sd(&(K[126]), _mm256_castpd256_pd128(_t2_9));
  _mm_store_sd(&(K[146]), _mm256_castpd256_pd128(_t2_10));
  _mm_store_sd(&(K[147]), _mm256_castpd256_pd128(_t2_11));
  _mm_store_sd(&(K[164]), _mm256_castpd256_pd128(_t4_0));
  _mm_store_sd(&(K[165]), _mm256_castpd256_pd128(_t4_2));
  _mm_store_sd(&(K[166]), _mm256_castpd256_pd128(_t4_4));
  _mm_store_sd(&(K[167]), _mm256_castpd256_pd128(_t4_5));
  _mm_store_sd(&(K[184]), _mm256_castpd256_pd128(_t4_6));
  _mm_store_sd(&(K[185]), _mm256_castpd256_pd128(_t4_8));
  _mm_store_sd(&(K[186]), _mm256_castpd256_pd128(_t4_10));
  _mm_store_sd(&(K[187]), _mm256_castpd256_pd128(_t4_11));
  _mm_store_sd(&(K[204]), _mm256_castpd256_pd128(_t4_12));
  _mm_store_sd(&(K[205]), _mm256_castpd256_pd128(_t4_14));
  _mm_store_sd(&(K[206]), _mm256_castpd256_pd128(_t4_16));
  _mm_store_sd(&(K[207]), _mm256_castpd256_pd128(_t4_17));
  _mm_store_sd(&(K[224]), _mm256_castpd256_pd128(_t4_18));
  _mm_store_sd(&(K[225]), _mm256_castpd256_pd128(_t4_20));
  _mm_store_sd(&(K[226]), _mm256_castpd256_pd128(_t4_22));
  _mm_store_sd(&(K[227]), _mm256_castpd256_pd128(_t4_23));
  _mm_store_sd(K + 168, _mm256_castpd256_pd128(_t6_20));
  _mm256_maskstore_pd(K + 188, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_25);
  _mm256_maskstore_pd(K + 208, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t6_25, _t6_25, 1));
  _mm256_maskstore_pd(K + 228, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t6_25, _t6_25, 129));
  _mm_store_sd(&(K[189]), _mm256_castpd256_pd128(_t6_26));
  _mm256_maskstore_pd(K + 209, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_27);
  _mm256_maskstore_pd(K + 229, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t6_27, _t6_27, 1));
  _mm_store_sd(&(K[210]), _mm256_castpd256_pd128(_t6_29));
  _mm_store_sd(&(K[230]), _mm256_castpd256_pd128(_t6_30));
  _mm_store_sd(&(K[231]), _mm256_castpd256_pd128(_t6_31));
  _mm_store_sd(K + 252, _mm256_castpd256_pd128(_t10_68));
  _mm256_maskstore_pd(K + 272, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_17);
  _mm256_maskstore_pd(K + 292, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t12_17, _t12_17, 1));
  _mm256_maskstore_pd(K + 312, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t12_17, _t12_17, 129));
  _mm_store_sd(&(K[273]), _mm256_castpd256_pd128(_t12_18));
  _mm256_maskstore_pd(K + 293, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t12_19);
  _mm256_maskstore_pd(K + 313, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t12_19, _t12_19, 1));
  _mm_store_sd(&(K[294]), _mm256_castpd256_pd128(_t12_21));
  _mm_store_sd(&(K[314]), _mm256_castpd256_pd128(_t12_22));
  _mm_store_sd(&(K[315]), _mm256_castpd256_pd128(_t12_23));

  for( int fi971 = 0; fi971 <= 15; fi971+=4 ) {
    _t17_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971])));
    _t17_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971])));
    _t17_8 = _mm256_maskload_pd(y + fi971 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t17_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 20)), _mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 40))), _mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 60)), 32);
    _t17_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 21])));
    _t17_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 41)), _mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 61)), 0);
    _t17_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 42])));
    _t17_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 62])));
    _t17_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 63])));

    // Generating : t0[20,1] = S(h(1, 20, fi971), ( G(h(1, 20, fi971), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971), L0[20,20],h(1, 20, fi971)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_13 = _t17_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_14 = _t17_6;

    // 4-BLAC: 1x4 / 1x4
    _t17_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_13), _mm256_castpd256_pd128(_t17_14)));

    // AVX Storer:
    _t17_7 = _t17_15;

    // Generating : t0[20,1] = S(h(3, 20, fi971 + 1), ( G(h(3, 20, fi971 + 1), t0[20,1],h(1, 1, 0)) - ( G(h(3, 20, fi971 + 1), L0[20,20],h(1, 20, fi971)) Kro G(h(1, 20, fi971), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_16 = _t17_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_17 = _t17_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_7, _t17_7, 32), _mm256_permute2f128_pd(_t17_7, _t17_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t17_19 = _mm256_mul_pd(_t17_17, _t17_18);

    // 4-BLAC: 4x1 - 4x1
    _t17_20 = _mm256_sub_pd(_t17_16, _t17_19);

    // AVX Storer:
    _t17_8 = _t17_20;

    // Generating : t0[20,1] = S(h(1, 20, fi971 + 1), ( G(h(1, 20, fi971 + 1), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 1), L0[20,20],h(1, 20, fi971 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_22 = _t17_4;

    // 4-BLAC: 1x4 / 1x4
    _t17_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_21), _mm256_castpd256_pd128(_t17_22)));

    // AVX Storer:
    _t17_9 = _t17_23;

    // Generating : t0[20,1] = S(h(2, 20, fi971 + 2), ( G(h(2, 20, fi971 + 2), t0[20,1],h(1, 1, 0)) - ( G(h(2, 20, fi971 + 2), L0[20,20],h(1, 20, fi971 + 1)) Kro G(h(1, 20, fi971 + 1), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_8, 6), _mm256_permute2f128_pd(_t17_8, _t17_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_25 = _t17_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_9, _t17_9, 32), _mm256_permute2f128_pd(_t17_9, _t17_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t17_27 = _mm256_mul_pd(_t17_25, _t17_26);

    // 4-BLAC: 4x1 - 4x1
    _t17_28 = _mm256_sub_pd(_t17_24, _t17_27);

    // AVX Storer:
    _t17_10 = _t17_28;

    // Generating : t0[20,1] = S(h(1, 20, fi971 + 2), ( G(h(1, 20, fi971 + 2), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 2), L0[20,20],h(1, 20, fi971 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_30 = _t17_2;

    // 4-BLAC: 1x4 / 1x4
    _t17_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_29), _mm256_castpd256_pd128(_t17_30)));

    // AVX Storer:
    _t17_11 = _t17_31;

    // Generating : t0[20,1] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), t0[20,1],h(1, 1, 0)) - ( G(h(1, 20, fi971 + 3), L0[20,20],h(1, 20, fi971 + 2)) Kro G(h(1, 20, fi971 + 2), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_33 = _t17_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_34 = _t17_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t17_35 = _mm256_mul_pd(_t17_33, _t17_34);

    // 4-BLAC: 1x4 - 1x4
    _t17_36 = _mm256_sub_pd(_t17_32, _t17_35);

    // AVX Storer:
    _t17_12 = _t17_36;

    // Generating : t0[20,1] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 3), L0[20,20],h(1, 20, fi971 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_37 = _t17_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_38 = _t17_0;

    // 4-BLAC: 1x4 / 1x4
    _t17_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_37), _mm256_castpd256_pd128(_t17_38)));

    // AVX Storer:
    _t17_12 = _t17_39;

    // Generating : t0[20,1] = Sum_{k159} ( S(h(4, 20, fi971 + k159 + 4), ( G(h(4, 20, fi971 + k159 + 4), t0[20,1],h(1, 1, 0)) - ( G(h(4, 20, fi971 + k159 + 4), L0[20,20],h(4, 20, fi971)) * G(h(4, 20, fi971), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[fi971]), _mm256_castpd256_pd128(_t17_7));
    _mm_store_sd(&(y[fi971 + 1]), _mm256_castpd256_pd128(_t17_9));
    _mm_store_sd(&(y[fi971 + 2]), _mm256_castpd256_pd128(_t17_11));
    _mm_store_sd(&(y[fi971 + 3]), _mm256_castpd256_pd128(_t17_12));

    for( int k159 = 0; k159 <= -fi971 + 15; k159+=4 ) {
      _t18_9 = _mm256_loadu_pd(y + fi971 + k159 + 4);
      _t18_7 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 80);
      _t18_6 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 100);
      _t18_5 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 120);
      _t18_4 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 140);
      _t18_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971])));
      _t18_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971 + 1])));
      _t18_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971 + 2])));
      _t18_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t18_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t18_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32)), _mm256_mul_pd(_t18_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t18_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32)), _mm256_mul_pd(_t18_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t18_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32)), _mm256_mul_pd(_t18_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t18_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32)), _mm256_mul_pd(_t18_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t18_9 = _mm256_sub_pd(_t18_9, _t18_8);

      // AVX Storer:
      _mm256_storeu_pd(y + fi971 + k159 + 4, _t18_9);
    }
  }

  _t19_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[16])));
  _t19_1 = _mm256_maskload_pd(y + 17, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : t0[20,1] = S(h(1, 20, 16), ( G(h(1, 20, 16), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, 16), L0[20,20],h(1, 20, 16)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_6 = _t19_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_7 = _t14_28;

  // 4-BLAC: 1x4 / 1x4
  _t19_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_6), _mm256_castpd256_pd128(_t19_7)));

  // AVX Storer:
  _t19_0 = _t19_8;

  // Generating : t0[20,1] = S(h(3, 20, 17), ( G(h(3, 20, 17), t0[20,1],h(1, 1, 0)) - ( G(h(3, 20, 17), L0[20,20],h(1, 20, 16)) Kro G(h(1, 20, 16), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t19_9 = _t19_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t19_10 = _t16_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_0, _t19_0, 32), _mm256_permute2f128_pd(_t19_0, _t19_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t19_12 = _mm256_mul_pd(_t19_10, _t19_11);

  // 4-BLAC: 4x1 - 4x1
  _t19_13 = _mm256_sub_pd(_t19_9, _t19_12);

  // AVX Storer:
  _t19_1 = _t19_13;

  // Generating : t0[20,1] = S(h(1, 20, 17), ( G(h(1, 20, 17), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, 17), L0[20,20],h(1, 20, 17)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_14 = _mm256_blend_pd(_mm256_setzero_pd(), _t19_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_15 = _t16_2;

  // 4-BLAC: 1x4 / 1x4
  _t19_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_14), _mm256_castpd256_pd128(_t19_15)));

  // AVX Storer:
  _t19_2 = _t19_16;

  // Generating : t0[20,1] = S(h(2, 20, 18), ( G(h(2, 20, 18), t0[20,1],h(1, 1, 0)) - ( G(h(2, 20, 18), L0[20,20],h(1, 20, 17)) Kro G(h(1, 20, 17), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t19_17 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t19_1, 6), _mm256_permute2f128_pd(_t19_1, _t19_1, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t19_18 = _t16_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t19_2, _t19_2, 32), _mm256_permute2f128_pd(_t19_2, _t19_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t19_20 = _mm256_mul_pd(_t19_18, _t19_19);

  // 4-BLAC: 4x1 - 4x1
  _t19_21 = _mm256_sub_pd(_t19_17, _t19_20);

  // AVX Storer:
  _t19_3 = _t19_21;

  // Generating : t0[20,1] = S(h(1, 20, 18), ( G(h(1, 20, 18), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, 18), L0[20,20],h(1, 20, 18)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t19_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_23 = _t16_5;

  // 4-BLAC: 1x4 / 1x4
  _t19_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_22), _mm256_castpd256_pd128(_t19_23)));

  // AVX Storer:
  _t19_4 = _t19_24;

  // Generating : t0[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), t0[20,1],h(1, 1, 0)) - ( G(h(1, 20, 19), L0[20,20],h(1, 20, 18)) Kro G(h(1, 20, 18), t0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_25 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t19_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_26 = _t16_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_27 = _t19_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t19_28 = _mm256_mul_pd(_t19_26, _t19_27);

  // 4-BLAC: 1x4 - 1x4
  _t19_29 = _mm256_sub_pd(_t19_25, _t19_28);

  // AVX Storer:
  _t19_5 = _t19_29;

  // Generating : t0[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), t0[20,1],h(1, 1, 0)) Div G(h(1, 20, 19), L0[20,20],h(1, 20, 19)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_30 = _t19_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t19_31 = _t16_7;

  // 4-BLAC: 1x4 / 1x4
  _t19_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t19_30), _mm256_castpd256_pd128(_t19_31)));

  // AVX Storer:
  _t19_5 = _t19_32;

  _mm_store_sd(K + 336, _mm256_castpd256_pd128(_t14_28));
  _mm256_maskstore_pd(K + 356, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t16_1);
  _mm256_maskstore_pd(K + 376, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t16_1, _t16_1, 1));
  _mm256_maskstore_pd(K + 396, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t16_1, _t16_1, 129));
  _mm_store_sd(&(K[357]), _mm256_castpd256_pd128(_t16_2));
  _mm256_maskstore_pd(K + 377, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t16_3);
  _mm256_maskstore_pd(K + 397, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t16_3, _t16_3, 1));
  _mm_store_sd(&(K[378]), _mm256_castpd256_pd128(_t16_5));
  _mm_store_sd(&(K[398]), _mm256_castpd256_pd128(_t16_6));
  _mm_store_sd(&(K[399]), _mm256_castpd256_pd128(_t16_7));
  _mm_store_sd(&(y[16]), _mm256_castpd256_pd128(_t19_0));
  _mm_store_sd(&(y[17]), _mm256_castpd256_pd128(_t19_2));
  _mm_store_sd(&(y[18]), _mm256_castpd256_pd128(_t19_4));
  _mm_store_sd(&(y[19]), _mm256_castpd256_pd128(_t19_5));

  for( int fi971 = 0; fi971 <= 15; fi971+=4 ) {
    _t20_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 19])));
    _t20_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-21*fi971 + 399])));
    _t20_8 = _mm256_maskload_pd(y + -fi971 + 16, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t20_5 = _mm256_maskload_pd(K + -21*fi971 + 396, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t20_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-21*fi971 + 378])));
    _t20_3 = _mm256_maskload_pd(K + -21*fi971 + 376, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t20_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-21*fi971 + 357])));
    _t20_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-21*fi971 + 356])));
    _t20_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-21*fi971 + 336])));

    // Generating : a[20,1] = S(h(1, 20, -fi971 + 19), ( G(h(1, 20, -fi971 + 19), a[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi971 + 19), L0[20,20],h(1, 20, -fi971 + 19)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_13 = _t20_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_14 = _t20_6;

    // 4-BLAC: 1x4 / 1x4
    _t20_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_13), _mm256_castpd256_pd128(_t20_14)));

    // AVX Storer:
    _t20_7 = _t20_15;

    // Generating : a[20,1] = S(h(3, 20, -fi971 + 16), ( G(h(3, 20, -fi971 + 16), a[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, -fi971 + 19), L0[20,20],h(3, 20, -fi971 + 16)) ) Kro G(h(1, 20, -fi971 + 19), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t20_16 = _t20_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t20_17 = _t20_5;

    // 4-BLAC: (1x4)^T
    _t20_18 = _t20_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_7, _t20_7, 32), _mm256_permute2f128_pd(_t20_7, _t20_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t20_20 = _mm256_mul_pd(_t20_18, _t20_19);

    // 4-BLAC: 4x1 - 4x1
    _t20_21 = _mm256_sub_pd(_t20_16, _t20_20);

    // AVX Storer:
    _t20_8 = _t20_21;

    // Generating : a[20,1] = S(h(1, 20, -fi971 + 18), ( G(h(1, 20, -fi971 + 18), a[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi971 + 18), L0[20,20],h(1, 20, -fi971 + 18)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_22 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t20_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t20_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_23 = _t20_4;

    // 4-BLAC: 1x4 / 1x4
    _t20_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_22), _mm256_castpd256_pd128(_t20_23)));

    // AVX Storer:
    _t20_9 = _t20_24;

    // Generating : a[20,1] = S(h(2, 20, -fi971 + 16), ( G(h(2, 20, -fi971 + 16), a[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, -fi971 + 18), L0[20,20],h(2, 20, -fi971 + 16)) ) Kro G(h(1, 20, -fi971 + 18), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t20_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t20_8, 3);

    // AVX Loader:

    // 1x2 -> 1x4
    _t20_26 = _t20_3;

    // 4-BLAC: (1x4)^T
    _t20_27 = _t20_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_9, _t20_9, 32), _mm256_permute2f128_pd(_t20_9, _t20_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t20_29 = _mm256_mul_pd(_t20_27, _t20_28);

    // 4-BLAC: 4x1 - 4x1
    _t20_30 = _mm256_sub_pd(_t20_25, _t20_29);

    // AVX Storer:
    _t20_10 = _t20_30;

    // Generating : a[20,1] = S(h(1, 20, -fi971 + 17), ( G(h(1, 20, -fi971 + 17), a[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi971 + 17), L0[20,20],h(1, 20, -fi971 + 17)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_31 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t20_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_32 = _t20_2;

    // 4-BLAC: 1x4 / 1x4
    _t20_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_31), _mm256_castpd256_pd128(_t20_32)));

    // AVX Storer:
    _t20_11 = _t20_33;

    // Generating : a[20,1] = S(h(1, 20, -fi971 + 16), ( G(h(1, 20, -fi971 + 16), a[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, -fi971 + 17), L0[20,20],h(1, 20, -fi971 + 16)) ) Kro G(h(1, 20, -fi971 + 17), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t20_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_35 = _t20_1;

    // 4-BLAC: (4x1)^T
    _t20_36 = _t20_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_37 = _t20_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t20_38 = _mm256_mul_pd(_t20_36, _t20_37);

    // 4-BLAC: 1x4 - 1x4
    _t20_39 = _mm256_sub_pd(_t20_34, _t20_38);

    // AVX Storer:
    _t20_12 = _t20_39;

    // Generating : a[20,1] = S(h(1, 20, -fi971 + 16), ( G(h(1, 20, -fi971 + 16), a[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi971 + 16), L0[20,20],h(1, 20, -fi971 + 16)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_40 = _t20_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t20_41 = _t20_0;

    // 4-BLAC: 1x4 / 1x4
    _t20_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_40), _mm256_castpd256_pd128(_t20_41)));

    // AVX Storer:
    _t20_12 = _t20_42;

    // Generating : a[20,1] = Sum_{k159} ( S(h(4, 20, k159), ( G(h(4, 20, k159), a[20,1],h(1, 1, 0)) - ( T( G(h(4, 20, -fi971 + 16), L0[20,20],h(4, 20, k159)) ) * G(h(4, 20, -fi971 + 16), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[-fi971 + 19]), _mm256_castpd256_pd128(_t20_7));
    _mm_store_sd(&(y[-fi971 + 18]), _mm256_castpd256_pd128(_t20_9));
    _mm_store_sd(&(y[-fi971 + 17]), _mm256_castpd256_pd128(_t20_11));
    _mm_store_sd(&(y[-fi971 + 16]), _mm256_castpd256_pd128(_t20_12));

    for( int k159 = 0; k159 <= -fi971 + 15; k159+=4 ) {
      _t21_9 = _mm256_loadu_pd(y + k159);
      _t21_7 = _mm256_loadu_pd(K + -20*fi971 + k159 + 320);
      _t21_6 = _mm256_loadu_pd(K + -20*fi971 + k159 + 340);
      _t21_5 = _mm256_loadu_pd(K + -20*fi971 + k159 + 360);
      _t21_4 = _mm256_loadu_pd(K + -20*fi971 + k159 + 380);
      _t21_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 19])));
      _t21_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 18])));
      _t21_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 17])));
      _t21_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 16])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t21_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_7, _t21_6), _mm256_unpacklo_pd(_t21_5, _t21_4), 32);
      _t21_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t21_7, _t21_6), _mm256_unpackhi_pd(_t21_5, _t21_4), 32);
      _t21_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_7, _t21_6), _mm256_unpacklo_pd(_t21_5, _t21_4), 49);
      _t21_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t21_7, _t21_6), _mm256_unpackhi_pd(_t21_5, _t21_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t21_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32)), _mm256_mul_pd(_t21_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t21_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32)), _mm256_mul_pd(_t21_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32)), _mm256_mul_pd(_t21_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t21_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32)), _mm256_mul_pd(_t21_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t21_0, _t21_1), _mm256_unpacklo_pd(_t21_2, _t21_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t21_9 = _mm256_sub_pd(_t21_9, _t21_8);

      // AVX Storer:
      _mm256_storeu_pd(y + k159, _t21_9);
    }
  }

  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(K[63])));
  _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[62])));
  _t0_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 20)), _mm256_castpd128_pd256(_mm_load_sd(K + 40))), _mm256_castpd128_pd256(_mm_load_sd(K + 60)), 32);
  _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21])));
  _t0_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 41)), _mm256_castpd128_pd256(_mm_load_sd(K + 61)), 0);
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[42])));
  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));
  _t22_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[3])));
  _t22_1 = _mm256_maskload_pd(y, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : a[20,1] = S(h(1, 20, 3), ( G(h(1, 20, 3), a[20,1],h(1, 1, 0)) Div G(h(1, 20, 3), L0[20,20],h(1, 20, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_6 = _t22_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_7 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t22_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t22_6), _mm256_castpd256_pd128(_t22_7)));

  // AVX Storer:
  _t22_0 = _t22_8;

  // Generating : a[20,1] = S(h(3, 20, 0), ( G(h(3, 20, 0), a[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 3), L0[20,20],h(3, 20, 0)) ) Kro G(h(1, 20, 3), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t22_9 = _t22_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t22_10 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // 4-BLAC: (1x4)^T
  _t22_11 = _t22_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_0, _t22_0, 32), _mm256_permute2f128_pd(_t22_0, _t22_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t22_13 = _mm256_mul_pd(_t22_11, _t22_12);

  // 4-BLAC: 4x1 - 4x1
  _t22_14 = _mm256_sub_pd(_t22_9, _t22_13);

  // AVX Storer:
  _t22_1 = _t22_14;

  // Generating : a[20,1] = S(h(1, 20, 2), ( G(h(1, 20, 2), a[20,1],h(1, 1, 0)) Div G(h(1, 20, 2), L0[20,20],h(1, 20, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_15 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t22_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t22_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_16 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t22_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t22_15), _mm256_castpd256_pd128(_t22_16)));

  // AVX Storer:
  _t22_2 = _t22_17;

  // Generating : a[20,1] = S(h(2, 20, 0), ( G(h(2, 20, 0), a[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 2), L0[20,20],h(2, 20, 0)) ) Kro G(h(1, 20, 2), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t22_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t22_1, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t22_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t22_20 = _t22_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_2, _t22_2, 32), _mm256_permute2f128_pd(_t22_2, _t22_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t22_22 = _mm256_mul_pd(_t22_20, _t22_21);

  // 4-BLAC: 4x1 - 4x1
  _t22_23 = _mm256_sub_pd(_t22_18, _t22_22);

  // AVX Storer:
  _t22_3 = _t22_23;

  // Generating : a[20,1] = S(h(1, 20, 1), ( G(h(1, 20, 1), a[20,1],h(1, 1, 0)) Div G(h(1, 20, 1), L0[20,20],h(1, 20, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_24 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t22_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_25 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t22_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t22_24), _mm256_castpd256_pd128(_t22_25)));

  // AVX Storer:
  _t22_4 = _t22_26;

  // Generating : a[20,1] = S(h(1, 20, 0), ( G(h(1, 20, 0), a[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 1), L0[20,20],h(1, 20, 0)) ) Kro G(h(1, 20, 1), a[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_27 = _mm256_blend_pd(_mm256_setzero_pd(), _t22_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_28 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t22_29 = _t22_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_30 = _t22_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t22_31 = _mm256_mul_pd(_t22_29, _t22_30);

  // 4-BLAC: 1x4 - 1x4
  _t22_32 = _mm256_sub_pd(_t22_27, _t22_31);

  // AVX Storer:
  _t22_5 = _t22_32;

  // Generating : a[20,1] = S(h(1, 20, 0), ( G(h(1, 20, 0), a[20,1],h(1, 1, 0)) Div G(h(1, 20, 0), L0[20,20],h(1, 20, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_33 = _t22_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t22_34 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t22_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t22_33), _mm256_castpd256_pd128(_t22_34)));

  // AVX Storer:
  _t22_5 = _t22_35;

  // Generating : kx[20,1] = ( Sum_{k207} ( S(h(4, 20, k207), ( G(h(4, 20, k207), X[20,20],h(4, 20, 0)) * G(h(4, 20, 0), x[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) + Sum_{k159} ( Sum_{k207} ( $(h(4, 20, k207), ( G(h(4, 20, k207), X[20,20],h(4, 20, k159)) * G(h(4, 20, k159), x[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k207 = 0; k207 <= 19; k207+=4 ) {
    _t23_4 = _mm256_loadu_pd(X + 20*k207);
    _t23_3 = _mm256_loadu_pd(X + 20*k207 + 20);
    _t23_2 = _mm256_loadu_pd(X + 20*k207 + 40);
    _t23_1 = _mm256_loadu_pd(X + 20*k207 + 60);
    _t23_0 = _mm256_loadu_pd(x);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t23_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t23_4, _t23_0), _mm256_mul_pd(_t23_3, _t23_0)), _mm256_hadd_pd(_mm256_mul_pd(_t23_2, _t23_0), _mm256_mul_pd(_t23_1, _t23_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t23_4, _t23_0), _mm256_mul_pd(_t23_3, _t23_0)), _mm256_hadd_pd(_mm256_mul_pd(_t23_2, _t23_0), _mm256_mul_pd(_t23_1, _t23_0)), 12));

    // AVX Storer:
    _mm256_storeu_pd(kx + k207, _t23_5);
  }


  for( int k159 = 4; k159 <= 19; k159+=4 ) {

    // AVX Loader:

    for( int k207 = 0; k207 <= 19; k207+=4 ) {
      _t24_4 = _mm256_loadu_pd(X + k159 + 20*k207);
      _t24_3 = _mm256_loadu_pd(X + k159 + 20*k207 + 20);
      _t24_2 = _mm256_loadu_pd(X + k159 + 20*k207 + 40);
      _t24_1 = _mm256_loadu_pd(X + k159 + 20*k207 + 60);
      _t24_0 = _mm256_loadu_pd(x + k159);
      _t24_5 = _mm256_loadu_pd(kx + k207);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t24_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t24_4, _t24_0), _mm256_mul_pd(_t24_3, _t24_0)), _mm256_hadd_pd(_mm256_mul_pd(_t24_2, _t24_0), _mm256_mul_pd(_t24_1, _t24_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t24_4, _t24_0), _mm256_mul_pd(_t24_3, _t24_0)), _mm256_hadd_pd(_mm256_mul_pd(_t24_2, _t24_0), _mm256_mul_pd(_t24_1, _t24_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t24_5 = _mm256_add_pd(_t24_5, _t24_6);

      // AVX Storer:
      _mm256_storeu_pd(kx + k207, _t24_5);
    }
  }

  _t25_0 = _mm256_loadu_pd(kx);

  // Generating : f[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 20, 0), kx[20,1],h(1, 1, 0)) ) * G(h(4, 20, 0), y[20,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{k159} ( $(h(1, 1, 0), ( T( G(h(4, 20, k159), kx[20,1],h(1, 1, 0)) ) * G(h(4, 20, k159), y[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t25_3 = _t25_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t25_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_mul_pd(_t25_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), 129)), 1));

  // AVX Storer:
  _t25_1 = _t25_2;


  for( int k159 = 4; k159 <= 19; k159+=4 ) {
    _t26_1 = _mm256_loadu_pd(kx + k159);
    _t26_0 = _mm256_loadu_pd(y + k159);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t26_4 = _t26_1;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t26_3 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t26_4, _t26_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_4, _t26_0), _mm256_mul_pd(_t26_4, _t26_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t26_4, _t26_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_4, _t26_0), _mm256_mul_pd(_t26_4, _t26_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t26_4, _t26_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t26_4, _t26_0), _mm256_mul_pd(_t26_4, _t26_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t26_2 = _t25_1;

    // 4-BLAC: 1x4 + 1x4
    _t26_2 = _mm256_add_pd(_t26_2, _t26_3);

    // AVX Storer:
    _t25_1 = _t26_2;
  }


  for( int fi971 = 0; fi971 <= 15; fi971+=4 ) {
    _t27_7 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971])));
    _t27_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971])));
    _t27_8 = _mm256_maskload_pd(kx + fi971 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t27_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 20)), _mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 40))), _mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 60)), 32);
    _t27_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 21])));
    _t27_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 41)), _mm256_castpd128_pd256(_mm_load_sd(K + 21*fi971 + 61)), 0);
    _t27_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 42])));
    _t27_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 62])));
    _t27_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[21*fi971 + 63])));

    // Generating : v[20,1] = S(h(1, 20, fi971), ( G(h(1, 20, fi971), v[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971), L0[20,20],h(1, 20, fi971)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_13 = _t27_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_14 = _t27_6;

    // 4-BLAC: 1x4 / 1x4
    _t27_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t27_13), _mm256_castpd256_pd128(_t27_14)));

    // AVX Storer:
    _t27_7 = _t27_15;

    // Generating : v[20,1] = S(h(3, 20, fi971 + 1), ( G(h(3, 20, fi971 + 1), v[20,1],h(1, 1, 0)) - ( G(h(3, 20, fi971 + 1), L0[20,20],h(1, 20, fi971)) Kro G(h(1, 20, fi971), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t27_16 = _t27_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t27_17 = _t27_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_7, _t27_7, 32), _mm256_permute2f128_pd(_t27_7, _t27_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t27_19 = _mm256_mul_pd(_t27_17, _t27_18);

    // 4-BLAC: 4x1 - 4x1
    _t27_20 = _mm256_sub_pd(_t27_16, _t27_19);

    // AVX Storer:
    _t27_8 = _t27_20;

    // Generating : v[20,1] = S(h(1, 20, fi971 + 1), ( G(h(1, 20, fi971 + 1), v[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 1), L0[20,20],h(1, 20, fi971 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t27_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_22 = _t27_4;

    // 4-BLAC: 1x4 / 1x4
    _t27_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t27_21), _mm256_castpd256_pd128(_t27_22)));

    // AVX Storer:
    _t27_9 = _t27_23;

    // Generating : v[20,1] = S(h(2, 20, fi971 + 2), ( G(h(2, 20, fi971 + 2), v[20,1],h(1, 1, 0)) - ( G(h(2, 20, fi971 + 2), L0[20,20],h(1, 20, fi971 + 1)) Kro G(h(1, 20, fi971 + 1), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t27_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t27_8, 6), _mm256_permute2f128_pd(_t27_8, _t27_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t27_25 = _t27_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_9, _t27_9, 32), _mm256_permute2f128_pd(_t27_9, _t27_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t27_27 = _mm256_mul_pd(_t27_25, _t27_26);

    // 4-BLAC: 4x1 - 4x1
    _t27_28 = _mm256_sub_pd(_t27_24, _t27_27);

    // AVX Storer:
    _t27_10 = _t27_28;

    // Generating : v[20,1] = S(h(1, 20, fi971 + 2), ( G(h(1, 20, fi971 + 2), v[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 2), L0[20,20],h(1, 20, fi971 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t27_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_30 = _t27_2;

    // 4-BLAC: 1x4 / 1x4
    _t27_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t27_29), _mm256_castpd256_pd128(_t27_30)));

    // AVX Storer:
    _t27_11 = _t27_31;

    // Generating : v[20,1] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), v[20,1],h(1, 1, 0)) - ( G(h(1, 20, fi971 + 3), L0[20,20],h(1, 20, fi971 + 2)) Kro G(h(1, 20, fi971 + 2), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t27_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_33 = _t27_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_34 = _t27_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t27_35 = _mm256_mul_pd(_t27_33, _t27_34);

    // 4-BLAC: 1x4 - 1x4
    _t27_36 = _mm256_sub_pd(_t27_32, _t27_35);

    // AVX Storer:
    _t27_12 = _t27_36;

    // Generating : v[20,1] = S(h(1, 20, fi971 + 3), ( G(h(1, 20, fi971 + 3), v[20,1],h(1, 1, 0)) Div G(h(1, 20, fi971 + 3), L0[20,20],h(1, 20, fi971 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_37 = _t27_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t27_38 = _t27_0;

    // 4-BLAC: 1x4 / 1x4
    _t27_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t27_37), _mm256_castpd256_pd128(_t27_38)));

    // AVX Storer:
    _t27_12 = _t27_39;

    // Generating : v[20,1] = Sum_{k159} ( S(h(4, 20, fi971 + k159 + 4), ( G(h(4, 20, fi971 + k159 + 4), v[20,1],h(1, 1, 0)) - ( G(h(4, 20, fi971 + k159 + 4), L0[20,20],h(4, 20, fi971)) * G(h(4, 20, fi971), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(kx[fi971]), _mm256_castpd256_pd128(_t27_7));
    _mm_store_sd(&(kx[fi971 + 1]), _mm256_castpd256_pd128(_t27_9));
    _mm_store_sd(&(kx[fi971 + 2]), _mm256_castpd256_pd128(_t27_11));
    _mm_store_sd(&(kx[fi971 + 3]), _mm256_castpd256_pd128(_t27_12));

    for( int k159 = 0; k159 <= -fi971 + 15; k159+=4 ) {
      _t28_9 = _mm256_loadu_pd(kx + fi971 + k159 + 4);
      _t28_7 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 80);
      _t28_6 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 100);
      _t28_5 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 120);
      _t28_4 = _mm256_loadu_pd(K + 21*fi971 + 20*k159 + 140);
      _t28_3 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971])));
      _t28_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971 + 1])));
      _t28_1 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971 + 2])));
      _t28_0 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t28_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t28_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32)), _mm256_mul_pd(_t28_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t28_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32)), _mm256_mul_pd(_t28_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t28_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32)), _mm256_mul_pd(_t28_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t28_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32)), _mm256_mul_pd(_t28_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t28_9 = _mm256_sub_pd(_t28_9, _t28_8);

      // AVX Storer:
      _mm256_storeu_pd(kx + fi971 + k159 + 4, _t28_9);
    }
  }

  _t16_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[357])));
  _t14_28 = _mm256_castpd128_pd256(_mm_load_sd(&(K[336])));
  _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[398])));
  _t16_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 377)), _mm256_castpd128_pd256(_mm_load_sd(K + 397)), 0);
  _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[399])));
  _t16_5 = _mm256_castpd128_pd256(_mm_load_sd(&(K[378])));
  _t16_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 356)), _mm256_castpd128_pd256(_mm_load_sd(K + 376))), _mm256_castpd128_pd256(_mm_load_sd(K + 396)), 32);
  _t29_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[16])));
  _t29_3 = _mm256_maskload_pd(kx + 17, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t29_1 = _mm256_loadu_pd(x);
  _t29_0 = _mm256_loadu_pd(kx);

  // Generating : v[20,1] = S(h(1, 20, 16), ( G(h(1, 20, 16), v[20,1],h(1, 1, 0)) Div G(h(1, 20, 16), L0[20,20],h(1, 20, 16)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_9 = _t29_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_10 = _t14_28;

  // 4-BLAC: 1x4 / 1x4
  _t29_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_9), _mm256_castpd256_pd128(_t29_10)));

  // AVX Storer:
  _t29_2 = _t29_11;

  // Generating : v[20,1] = S(h(3, 20, 17), ( G(h(3, 20, 17), v[20,1],h(1, 1, 0)) - ( G(h(3, 20, 17), L0[20,20],h(1, 20, 16)) Kro G(h(1, 20, 16), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t29_12 = _t29_3;

  // AVX Loader:

  // 3x1 -> 4x1
  _t29_13 = _t16_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_2, _t29_2, 32), _mm256_permute2f128_pd(_t29_2, _t29_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t29_15 = _mm256_mul_pd(_t29_13, _t29_14);

  // 4-BLAC: 4x1 - 4x1
  _t29_16 = _mm256_sub_pd(_t29_12, _t29_15);

  // AVX Storer:
  _t29_3 = _t29_16;

  // Generating : v[20,1] = S(h(1, 20, 17), ( G(h(1, 20, 17), v[20,1],h(1, 1, 0)) Div G(h(1, 20, 17), L0[20,20],h(1, 20, 17)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_18 = _t16_2;

  // 4-BLAC: 1x4 / 1x4
  _t29_19 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_17), _mm256_castpd256_pd128(_t29_18)));

  // AVX Storer:
  _t29_4 = _t29_19;

  // Generating : v[20,1] = S(h(2, 20, 18), ( G(h(2, 20, 18), v[20,1],h(1, 1, 0)) - ( G(h(2, 20, 18), L0[20,20],h(1, 20, 17)) Kro G(h(1, 20, 17), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t29_20 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_3, 6), _mm256_permute2f128_pd(_t29_3, _t29_3, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t29_21 = _t16_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_22 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_4, _t29_4, 32), _mm256_permute2f128_pd(_t29_4, _t29_4, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t29_23 = _mm256_mul_pd(_t29_21, _t29_22);

  // 4-BLAC: 4x1 - 4x1
  _t29_24 = _mm256_sub_pd(_t29_20, _t29_23);

  // AVX Storer:
  _t29_5 = _t29_24;

  // Generating : v[20,1] = S(h(1, 20, 18), ( G(h(1, 20, 18), v[20,1],h(1, 1, 0)) Div G(h(1, 20, 18), L0[20,20],h(1, 20, 18)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t29_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_26 = _t16_5;

  // 4-BLAC: 1x4 / 1x4
  _t29_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_25), _mm256_castpd256_pd128(_t29_26)));

  // AVX Storer:
  _t29_6 = _t29_27;

  // Generating : v[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), v[20,1],h(1, 1, 0)) - ( G(h(1, 20, 19), L0[20,20],h(1, 20, 18)) Kro G(h(1, 20, 18), v[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_28 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t29_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_29 = _t16_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_30 = _t29_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t29_31 = _mm256_mul_pd(_t29_29, _t29_30);

  // 4-BLAC: 1x4 - 1x4
  _t29_32 = _mm256_sub_pd(_t29_28, _t29_31);

  // AVX Storer:
  _t29_7 = _t29_32;

  // Generating : v[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), v[20,1],h(1, 1, 0)) Div G(h(1, 20, 19), L0[20,20],h(1, 20, 19)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_33 = _t29_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t29_34 = _t16_7;

  // 4-BLAC: 1x4 / 1x4
  _t29_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t29_33), _mm256_castpd256_pd128(_t29_34)));

  // AVX Storer:
  _t29_7 = _t29_35;

  // Generating : var[1,1] = ( ( S(h(1, 1, 0), ( ( T( G(h(4, 20, 0), x[20,1],h(1, 1, 0)) ) * G(h(4, 20, 0), x[20,1],h(1, 1, 0)) ) - ( T( G(h(4, 20, 0), kx[20,1],h(1, 1, 0)) ) * G(h(4, 20, 0), kx[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k207} ( $(h(1, 1, 0), ( T( G(h(4, 20, k207), x[20,1],h(1, 1, 0)) ) * G(h(4, 20, k207), x[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + Sum_{k159} ( -$(h(1, 1, 0), ( T( G(h(4, 20, k159), kx[20,1],h(1, 1, 0)) ) * G(h(4, 20, k159), kx[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t29_39 = _t29_1;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t29_36 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_39, _t29_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_39, _t29_1), _mm256_mul_pd(_t29_39, _t29_1), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_39, _t29_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_39, _t29_1), _mm256_mul_pd(_t29_39, _t29_1), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_39, _t29_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_39, _t29_1), _mm256_mul_pd(_t29_39, _t29_1), 129)), 1));

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t29_40 = _t29_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t29_37 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_40, _t29_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_40, _t29_0), _mm256_mul_pd(_t29_40, _t29_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_40, _t29_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_40, _t29_0), _mm256_mul_pd(_t29_40, _t29_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_40, _t29_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_40, _t29_0), _mm256_mul_pd(_t29_40, _t29_0), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t29_38 = _mm256_sub_pd(_t29_36, _t29_37);

  // AVX Storer:
  _t29_8 = _t29_38;


  for( int k207 = 4; k207 <= 19; k207+=4 ) {
    _t30_0 = _mm256_loadu_pd(x + k207);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t30_3 = _t30_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t30_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_3, _t30_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_3, _t30_0), _mm256_mul_pd(_t30_3, _t30_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_3, _t30_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_3, _t30_0), _mm256_mul_pd(_t30_3, _t30_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_3, _t30_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_3, _t30_0), _mm256_mul_pd(_t30_3, _t30_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t30_1 = _t29_8;

    // 4-BLAC: 1x4 + 1x4
    _t30_1 = _mm256_add_pd(_t30_1, _t30_2);

    // AVX Storer:
    _t29_8 = _t30_1;
  }

  _mm_store_sd(&(kx[16]), _mm256_castpd256_pd128(_t29_2));
  _mm_store_sd(&(kx[17]), _mm256_castpd256_pd128(_t29_4));
  _mm_store_sd(&(kx[18]), _mm256_castpd256_pd128(_t29_6));
  _mm_store_sd(&(kx[19]), _mm256_castpd256_pd128(_t29_7));

  for( int k159 = 4; k159 <= 19; k159+=4 ) {
    _t31_0 = _mm256_loadu_pd(kx + k159);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t31_3 = _t31_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t31_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t31_3, _t31_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t31_3, _t31_0), _mm256_mul_pd(_t31_3, _t31_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t31_3, _t31_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t31_3, _t31_0), _mm256_mul_pd(_t31_3, _t31_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t31_3, _t31_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t31_3, _t31_0), _mm256_mul_pd(_t31_3, _t31_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_1 = _t29_8;

    // 4-BLAC: 1x4 - 1x4
    _t31_1 = _mm256_sub_pd(_t31_1, _t31_2);

    // AVX Storer:
    _t29_8 = _t31_1;
  }


  // Generating : lp[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 20, 0), y[20,1],h(1, 1, 0)) ) * G(h(4, 20, 0), y[20,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{k159} ( $(h(1, 1, 0), ( T( G(h(4, 20, k159), y[20,1],h(1, 1, 0)) ) * G(h(4, 20, k159), y[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t32_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32);

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t32_1 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), _mm256_mul_pd(_t32_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_5, _t22_4), _mm256_unpacklo_pd(_t22_2, _t22_0), 32)), 129)), 1));

  // AVX Storer:
  _t32_0 = _t32_1;


  for( int k159 = 4; k159 <= 19; k159+=4 ) {
    _t33_0 = _mm256_loadu_pd(y + k159);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t33_3 = _t33_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t33_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t33_3, _t33_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_3, _t33_0), _mm256_mul_pd(_t33_3, _t33_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t33_3, _t33_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_3, _t33_0), _mm256_mul_pd(_t33_3, _t33_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t33_3, _t33_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_3, _t33_0), _mm256_mul_pd(_t33_3, _t33_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t33_1 = _t32_0;

    // 4-BLAC: 1x4 + 1x4
    _t33_1 = _mm256_add_pd(_t33_1, _t33_2);

    // AVX Storer:
    _t32_0 = _t33_1;
  }

  _mm_store_sd(&(y[3]), _mm256_castpd256_pd128(_t22_0));
  _mm_store_sd(&(y[2]), _mm256_castpd256_pd128(_t22_2));
  _mm_store_sd(&(y[1]), _mm256_castpd256_pd128(_t22_4));
  _mm_store_sd(&(y[0]), _mm256_castpd256_pd128(_t22_5));
  _mm_store_sd(&(f[0]), _mm256_castpd256_pd128(_t25_1));
  _mm_store_sd(&(var[0]), _mm256_castpd256_pd128(_t29_8));
  _mm_store_sd(&(lp[0]), _mm256_castpd256_pd128(_t32_0));

}
