/*
 * gpr_kernel.h
 *
Decl { {u'K': Symmetric[K, (44, 44), LSMatAccess], u'L': LowerTriangular[L, (44, 44), GenMatAccess], u'var': Scalar[var, (1, 1), GenMatAccess], u'L0': LowerTriangular[L0, (44, 44), GenMatAccess], u'X': SquaredMatrix[X, (44, 44), GenMatAccess], u'a': Matrix[a, (44, 1), GenMatAccess], u'f': Scalar[f, (1, 1), GenMatAccess], u't2': Matrix[t2, (44, 1), GenMatAccess], u't0': Matrix[t0, (44, 1), GenMatAccess], u't1': Matrix[t1, (44, 1), GenMatAccess], 'T1496': Matrix[T1496, (1, 44), GenMatAccess], u'lp': Scalar[lp, (1, 1), GenMatAccess], u'v': Matrix[v, (44, 1), GenMatAccess], u'y': Matrix[y, (44, 1), GenMatAccess], u'x': Matrix[x, (44, 1), GenMatAccess], u'kx': Matrix[kx, (44, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'rdiv_ltn_ow_opt': {'m': 'm1.ll', 'n': 'n4.ll'}, 'Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt': {'m0': 'm02.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'Assign_Mul_LowerTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_LowerTriangular_T_LowerTriangular_Symmetric_opt_m02_Assign_Mul_T_LowerTriangular_Matrix_Matrix_opt_m04_m21_rdiv_ltn_ow_opt_m1_n4'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,44],h(1, 44, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,44],h(1, 44, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), L[44,44],h(1, 44, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), L[44,44],h(1, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), L[44,44],h(1, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,44],h(1, 44, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,44],h(1, 44, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), L[44,44],h(2, 44, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), L[44,44],h(2, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(2, 44, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), L[44,44],h(2, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) = ( Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) Div Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(3, 44, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(3, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) )
Eq.ann: {}
Entry 14:
For_{fi971;0;36;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) ) Div Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(3, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(3, 44, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) ) Div Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(2, 44, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(2, 44, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) ) Div Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) ) Div Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) ) Div Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(3, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(3, 44, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) ) Div Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(2, 44, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(2, 44, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) ) Div Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) ) Div Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) ) Div Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(3, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(3, 44, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) ) Div Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(2, 44, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(2, 44, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) ) Div Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) ) Div Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) ) Div Tile( (1, 1), G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(3, 44, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(3, 44, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) ) Div Tile( (1, 1), G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(2, 44, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(2, 44, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) ) Div Tile( (1, 1), G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) ) Div Tile( (1, 1), G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ) )
Eq.ann: {}
 )Entry 15:
For_{fi846;4;39;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi846), L[44,44],h(4, 44, fi846)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi846), K[44,44],h(4, 44, fi846)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi846), L[44,44],h(fi846, 44, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi846), L[44,44],h(fi846, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,44],h(1, 44, fi846)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,44],h(1, 44, fi846)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,44],h(1, 44, fi846 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,44],h(1, 44, fi846 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 2), L[44,44],h(2, 44, fi846)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 2), L[44,44],h(2, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(2, 44, fi846)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 2), L[44,44],h(2, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(3, 44, fi846)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(3, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi846 + 40, 44, fi846 + 4), L[44,44],h(4, 44, fi846)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi846 + 40, 44, fi846 + 4), K[44,44],h(4, 44, fi846)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi846 + 40, 44, fi846 + 4), L[44,44],h(fi846, 44, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi846), L[44,44],h(fi846, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 16:
For_{fi1090;0;-fi846 + 36;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846)) ) Div Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(3, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(3, 44, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(2, 44, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(2, 44, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 4), L[44,44],h(1, 44, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846)) ) Div Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(3, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(3, 44, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(2, 44, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(2, 44, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 5), L[44,44],h(1, 44, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846)) ) Div Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(3, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(3, 44, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(2, 44, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(2, 44, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 6), L[44,44],h(1, 44, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846)) ) Div Tile( (1, 1), G(h(1, 44, fi846), L[44,44],h(1, 44, fi846)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(3, 44, fi846 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(3, 44, fi846 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi846 + 1), L[44,44],h(1, 44, fi846)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 1)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 1)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 1), L[44,44],h(1, 44, fi846 + 1)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(2, 44, fi846 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(2, 44, fi846 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 1)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 1)) ) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 2)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 2)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 2), L[44,44],h(1, 44, fi846 + 2)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 2)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 2)) ) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 3)) ) = ( Tile( (1, 1), G(h(1, 44, fi1090 + fi846 + 7), L[44,44],h(1, 44, fi846 + 3)) ) Div Tile( (1, 1), G(h(1, 44, fi846 + 3), L[44,44],h(1, 44, fi846 + 3)) ) )
Eq.ann: {}
 ) )Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 44, 40), L[44,44],h(4, 44, 40)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, 40), K[44,44],h(4, 44, 40)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, 40), L[44,44],h(40, 44, 0)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, 40), L[44,44],h(40, 44, 0)) ) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 44, 40), L[44,44],h(1, 44, 40)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 40), L[44,44],h(1, 44, 40)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,44],h(1, 44, 40)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 44, 40), L[44,44],h(1, 44, 40)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), L[44,44],h(1, 44, 40)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,44],h(1, 44, 40)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), L[44,44],h(1, 44, 40)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), L[44,44],h(1, 44, 40)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), L[44,44],h(1, 44, 40)) ) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L[44,44],h(1, 44, 41)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L[44,44],h(1, 44, 41)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L[44,44],h(1, 44, 40)) ) ) Kro T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), L[44,44],h(1, 44, 40)) ) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1496[1,44],h(1, 44, 41)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L[44,44],h(1, 44, 41)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1496[1,44],h(1, 44, 41)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L[44,44],h(1, 44, 41)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), L[44,44],h(2, 44, 40)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), L[44,44],h(2, 44, 40)) ) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(1, 44, 42)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(1, 44, 42)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(2, 44, 40)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), L[44,44],h(2, 44, 40)) ) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 44, 43), L[44,44],h(1, 44, 42)) ) = ( Tile( (1, 1), G(h(1, 44, 43), L[44,44],h(1, 44, 42)) ) Div Tile( (1, 1), G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(1, 44, 43)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(1, 44, 43)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(3, 44, 40)) ) ) * T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L[44,44],h(3, 44, 40)) ) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 44, 43), L[44,44],h(1, 44, 43)) ) = Sqrt( Tile( (1, 1), G(h(1, 44, 43), L[44,44],h(1, 44, 43)) ) )
Eq.ann: {}
Entry 31:
For_{fi1209;0;39;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 44, fi1209), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1209), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1209), L0[44,44],h(1, 44, fi1209)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi1209 + 1), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi1209 + 1), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi1209 + 1), L0[44,44],h(1, 44, fi1209)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1209), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 44, fi1209 + 1), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1209 + 1), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1209 + 1), L0[44,44],h(1, 44, fi1209 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi1209 + 2), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi1209 + 2), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi1209 + 2), L0[44,44],h(1, 44, fi1209 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1209 + 1), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 44, fi1209 + 2), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1209 + 2), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1209 + 2), L0[44,44],h(1, 44, fi1209 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1209 + 3), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1209 + 3), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1209 + 3), L0[44,44],h(1, 44, fi1209 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1209 + 2), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 44, fi1209 + 3), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1209 + 3), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1209 + 3), L0[44,44],h(1, 44, fi1209 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1209 + 40, 44, fi1209 + 4), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1209 + 40, 44, fi1209 + 4), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1209 + 40, 44, fi1209 + 4), L0[44,44],h(4, 44, fi1209)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi1209), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 32:
Eq: Tile( (1, 1), G(h(1, 44, 40), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 40), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 40), L0[44,44],h(1, 44, 40)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), L0[44,44],h(1, 44, 40)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 40), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 44, 41), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 41), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 41), L0[44,44],h(1, 44, 41)) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L0[44,44],h(1, 44, 41)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), G(h(1, 44, 42), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 42), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 42), L0[44,44],h(1, 44, 42)) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), t0[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), t0[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L0[44,44],h(1, 44, 42)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), t0[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 44, 43), t0[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 43), t0[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 43), L0[44,44],h(1, 44, 43)) ) )
Eq.ann: {}
Entry 39:
For_{fi1286;0;39;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 44, -fi1286 + 43), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, -fi1286 + 43), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, -fi1286 + 43), L0[44,44],h(1, 44, -fi1286 + 43)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 43), L0[44,44],h(3, 44, -fi1286 + 40)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 43), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 44, -fi1286 + 42), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, -fi1286 + 42), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, -fi1286 + 42), L0[44,44],h(1, 44, -fi1286 + 42)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 42), L0[44,44],h(2, 44, -fi1286 + 40)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 42), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 44, -fi1286 + 41), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, -fi1286 + 41), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, -fi1286 + 41), L0[44,44],h(1, 44, -fi1286 + 41)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 41), L0[44,44],h(1, 44, -fi1286 + 40)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, -fi1286 + 41), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, -fi1286 + 40), L0[44,44],h(1, 44, -fi1286 + 40)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1286 + 40, 44, 0), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1286 + 40, 44, 0), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 44, -fi1286 + 40), L0[44,44],h(-fi1286 + 40, 44, 0)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 44, -fi1286 + 40), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 40:
Eq: Tile( (1, 1), G(h(1, 44, 3), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 3), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 3), L0[44,44],h(1, 44, 3)) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 0), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 0), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), L0[44,44],h(3, 44, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 3), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 44, 2), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 2), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 2), L0[44,44],h(1, 44, 2)) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 0), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 0), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), L0[44,44],h(2, 44, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 2), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 44, 1), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 1), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 1), L0[44,44],h(1, 44, 1)) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 0), a[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 0), a[44,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), L0[44,44],h(1, 44, 0)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 1), a[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), G(h(1, 44, 0), a[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 0), a[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 0), L0[44,44],h(1, 44, 0)) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), Tile( (4, 4), kx[44,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), X[44,44] ) ) * Tile( (1, 1), Tile( (4, 4), x[44,1] ) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), f[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), kx[44,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[44,1] ) ) )
Eq.ann: {}
Entry 49:
For_{fi1363;0;39;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 44, fi1363), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1363), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1363), L0[44,44],h(1, 44, fi1363)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi1363 + 1), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi1363 + 1), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, fi1363 + 1), L0[44,44],h(1, 44, fi1363)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1363), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 44, fi1363 + 1), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1363 + 1), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1363 + 1), L0[44,44],h(1, 44, fi1363 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi1363 + 2), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi1363 + 2), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, fi1363 + 2), L0[44,44],h(1, 44, fi1363 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1363 + 1), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 44, fi1363 + 2), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1363 + 2), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1363 + 2), L0[44,44],h(1, 44, fi1363 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1363 + 3), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1363 + 3), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1363 + 3), L0[44,44],h(1, 44, fi1363 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, fi1363 + 2), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 44, fi1363 + 3), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, fi1363 + 3), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, fi1363 + 3), L0[44,44],h(1, 44, fi1363 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1363 + 40, 44, fi1363 + 4), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1363 + 40, 44, fi1363 + 4), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1363 + 40, 44, fi1363 + 4), L0[44,44],h(4, 44, fi1363)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 44, fi1363), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 50:
Eq: Tile( (1, 1), G(h(1, 44, 40), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 40), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 40), L0[44,44],h(1, 44, 40)) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 44, 41), L0[44,44],h(1, 44, 40)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 40), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), G(h(1, 44, 41), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 41), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 41), L0[44,44],h(1, 44, 41)) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 44, 42), L0[44,44],h(1, 44, 41)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 41), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), G(h(1, 44, 42), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 42), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 42), L0[44,44],h(1, 44, 42)) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), v[44,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), v[44,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 43), L0[44,44],h(1, 44, 42)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 44, 42), v[44,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 56:
Eq: Tile( (1, 1), G(h(1, 44, 43), v[44,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 44, 43), v[44,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 44, 43), L0[44,44],h(1, 44, 43)) ) )
Eq.ann: {}
Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), var[1,1] ) ) = ( ( T( Tile( (1, 1), Tile( (4, 4), x[44,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), x[44,1] ) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), kx[44,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), kx[44,1] ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), Tile( (4, 4), lp[1,1] ) ) = ( T( Tile( (1, 1), Tile( (4, 4), y[44,1] ) ) ) * Tile( (1, 1), Tile( (4, 4), y[44,1] ) ) )
Eq.ann: {}
 *
 * Created on: 2017-09-02
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 44
#define PARAM1 44

#define ERRTHRESH 1e-5

#define SOFTERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * X, double const * x, double * K, double * y, double * kx, double * f, double * var, double * lp)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27, _t1_28, _t1_29, _t1_30, _t1_31,
	_t1_32, _t1_33, _t1_34, _t1_35, _t1_36, _t1_37, _t1_38, _t1_39,
	_t1_40, _t1_41, _t1_42, _t1_43, _t1_44, _t1_45, _t1_46, _t1_47,
	_t1_48, _t1_49, _t1_50, _t1_51, _t1_52, _t1_53, _t1_54, _t1_55,
	_t1_56, _t1_57, _t1_58, _t1_59, _t1_60, _t1_61, _t1_62, _t1_63,
	_t1_64, _t1_65, _t1_66, _t1_67, _t1_68, _t1_69, _t1_70, _t1_71,
	_t1_72, _t1_73, _t1_74, _t1_75, _t1_76, _t1_77, _t1_78, _t1_79,
	_t1_80, _t1_81, _t1_82, _t1_83, _t1_84, _t1_85, _t1_86, _t1_87,
	_t1_88, _t1_89, _t1_90, _t1_91, _t1_92, _t1_93, _t1_94, _t1_95,
	_t1_96, _t1_97, _t1_98, _t1_99, _t1_100, _t1_101, _t1_102, _t1_103,
	_t1_104, _t1_105, _t1_106, _t1_107, _t1_108, _t1_109, _t1_110, _t1_111,
	_t1_112, _t1_113, _t1_114, _t1_115, _t1_116, _t1_117, _t1_118, _t1_119;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60, _t5_61, _t5_62, _t5_63,
	_t5_64, _t5_65, _t5_66, _t5_67, _t5_68, _t5_69, _t5_70, _t5_71,
	_t5_72, _t5_73, _t5_74, _t5_75, _t5_76, _t5_77, _t5_78, _t5_79,
	_t5_80, _t5_81, _t5_82, _t5_83, _t5_84, _t5_85, _t5_86, _t5_87,
	_t5_88, _t5_89, _t5_90, _t5_91, _t5_92, _t5_93, _t5_94, _t5_95,
	_t5_96, _t5_97, _t5_98, _t5_99, _t5_100, _t5_101, _t5_102, _t5_103,
	_t5_104, _t5_105, _t5_106, _t5_107, _t5_108, _t5_109, _t5_110, _t5_111,
	_t5_112, _t5_113, _t5_114, _t5_115, _t5_116, _t5_117, _t5_118, _t5_119;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39,
	_t6_40, _t6_41, _t6_42, _t6_43, _t6_44, _t6_45, _t6_46, _t6_47,
	_t6_48, _t6_49, _t6_50, _t6_51, _t6_52, _t6_53, _t6_54, _t6_55,
	_t6_56, _t6_57, _t6_58, _t6_59, _t6_60, _t6_61, _t6_62, _t6_63,
	_t6_64, _t6_65, _t6_66, _t6_67, _t6_68, _t6_69, _t6_70, _t6_71,
	_t6_72, _t6_73, _t6_74, _t6_75, _t6_76, _t6_77, _t6_78, _t6_79,
	_t6_80, _t6_81, _t6_82, _t6_83, _t6_84, _t6_85, _t6_86, _t6_87,
	_t6_88, _t6_89, _t6_90, _t6_91, _t6_92, _t6_93, _t6_94, _t6_95,
	_t6_96, _t6_97, _t6_98, _t6_99, _t6_100, _t6_101, _t6_102, _t6_103,
	_t6_104, _t6_105, _t6_106, _t6_107, _t6_108, _t6_109, _t6_110, _t6_111,
	_t6_112, _t6_113, _t6_114, _t6_115, _t6_116;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27, _t10_28, _t10_29, _t10_30, _t10_31,
	_t10_32, _t10_33, _t10_34, _t10_35, _t10_36, _t10_37, _t10_38, _t10_39,
	_t10_40, _t10_41, _t10_42, _t10_43, _t10_44, _t10_45, _t10_46, _t10_47,
	_t10_48, _t10_49, _t10_50, _t10_51, _t10_52, _t10_53, _t10_54, _t10_55,
	_t10_56, _t10_57, _t10_58, _t10_59, _t10_60, _t10_61, _t10_62, _t10_63,
	_t10_64, _t10_65, _t10_66, _t10_67, _t10_68, _t10_69, _t10_70, _t10_71,
	_t10_72, _t10_73, _t10_74, _t10_75, _t10_76, _t10_77, _t10_78, _t10_79,
	_t10_80, _t10_81, _t10_82, _t10_83, _t10_84, _t10_85, _t10_86, _t10_87,
	_t10_88, _t10_89, _t10_90, _t10_91, _t10_92, _t10_93, _t10_94, _t10_95,
	_t10_96, _t10_97, _t10_98, _t10_99, _t10_100, _t10_101, _t10_102, _t10_103,
	_t10_104, _t10_105, _t10_106, _t10_107, _t10_108, _t10_109, _t10_110, _t10_111,
	_t10_112, _t10_113, _t10_114, _t10_115, _t10_116, _t10_117, _t10_118, _t10_119,
	_t10_120, _t10_121, _t10_122, _t10_123, _t10_124, _t10_125, _t10_126, _t10_127,
	_t10_128, _t10_129, _t10_130, _t10_131, _t10_132, _t10_133, _t10_134, _t10_135,
	_t10_136, _t10_137, _t10_138, _t10_139, _t10_140, _t10_141, _t10_142, _t10_143;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35, _t11_36, _t11_37, _t11_38, _t11_39,
	_t11_40, _t11_41, _t11_42, _t11_43, _t11_44, _t11_45, _t11_46, _t11_47,
	_t11_48, _t11_49, _t11_50, _t11_51, _t11_52, _t11_53, _t11_54, _t11_55,
	_t11_56, _t11_57, _t11_58, _t11_59, _t11_60, _t11_61, _t11_62, _t11_63,
	_t11_64, _t11_65, _t11_66, _t11_67, _t11_68, _t11_69, _t11_70, _t11_71,
	_t11_72, _t11_73, _t11_74, _t11_75, _t11_76, _t11_77, _t11_78, _t11_79,
	_t11_80, _t11_81, _t11_82, _t11_83, _t11_84, _t11_85, _t11_86, _t11_87,
	_t11_88, _t11_89, _t11_90, _t11_91, _t11_92, _t11_93, _t11_94, _t11_95,
	_t11_96, _t11_97, _t11_98, _t11_99, _t11_100, _t11_101, _t11_102, _t11_103,
	_t11_104, _t11_105, _t11_106, _t11_107, _t11_108, _t11_109, _t11_110, _t11_111,
	_t11_112, _t11_113, _t11_114, _t11_115, _t11_116, _t11_117, _t11_118, _t11_119;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23,
	_t12_24, _t12_25, _t12_26, _t12_27;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35, _t14_36, _t14_37, _t14_38, _t14_39,
	_t14_40, _t14_41, _t14_42, _t14_43, _t14_44, _t14_45, _t14_46, _t14_47,
	_t14_48, _t14_49, _t14_50;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23,
	_t17_24, _t17_25, _t17_26, _t17_27, _t17_28, _t17_29, _t17_30, _t17_31,
	_t17_32, _t17_33, _t17_34, _t17_35, _t17_36, _t17_37, _t17_38, _t17_39,
	_t17_40, _t17_41, _t17_42, _t17_43, _t17_44, _t17_45, _t17_46, _t17_47,
	_t17_48, _t17_49, _t17_50, _t17_51, _t17_52, _t17_53, _t17_54, _t17_55,
	_t17_56, _t17_57, _t17_58, _t17_59, _t17_60, _t17_61, _t17_62, _t17_63,
	_t17_64, _t17_65, _t17_66, _t17_67, _t17_68, _t17_69, _t17_70, _t17_71,
	_t17_72, _t17_73, _t17_74, _t17_75, _t17_76, _t17_77, _t17_78, _t17_79,
	_t17_80, _t17_81, _t17_82, _t17_83, _t17_84, _t17_85, _t17_86, _t17_87,
	_t17_88, _t17_89, _t17_90, _t17_91, _t17_92, _t17_93, _t17_94, _t17_95,
	_t17_96, _t17_97, _t17_98, _t17_99, _t17_100, _t17_101, _t17_102, _t17_103,
	_t17_104, _t17_105, _t17_106, _t17_107, _t17_108, _t17_109, _t17_110, _t17_111,
	_t17_112, _t17_113, _t17_114, _t17_115, _t17_116, _t17_117, _t17_118, _t17_119;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31,
	_t18_32, _t18_33, _t18_34, _t18_35, _t18_36, _t18_37, _t18_38, _t18_39;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31,
	_t20_32, _t20_33, _t20_34, _t20_35, _t20_36, _t20_37, _t20_38, _t20_39,
	_t20_40, _t20_41, _t20_42, _t20_43, _t20_44, _t20_45, _t20_46, _t20_47,
	_t20_48, _t20_49, _t20_50, _t20_51, _t20_52, _t20_53, _t20_54, _t20_55,
	_t20_56, _t20_57, _t20_58, _t20_59, _t20_60;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7,
	_t21_8, _t21_9, _t21_10, _t21_11, _t21_12, _t21_13, _t21_14, _t21_15,
	_t21_16, _t21_17, _t21_18, _t21_19, _t21_20, _t21_21, _t21_22, _t21_23,
	_t21_24, _t21_25, _t21_26, _t21_27, _t21_28, _t21_29, _t21_30, _t21_31,
	_t21_32, _t21_33, _t21_34, _t21_35, _t21_36, _t21_37, _t21_38, _t21_39;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19, _t23_20, _t23_21, _t23_22, _t23_23,
	_t23_24, _t23_25, _t23_26, _t23_27, _t23_28, _t23_29, _t23_30, _t23_31,
	_t23_32;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19, _t24_20, _t24_21, _t24_22, _t24_23,
	_t24_24, _t24_25, _t24_26, _t24_27, _t24_28, _t24_29, _t24_30, _t24_31,
	_t24_32, _t24_33, _t24_34, _t24_35, _t24_36, _t24_37, _t24_38, _t24_39,
	_t24_40, _t24_41, _t24_42;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27, _t26_28, _t26_29, _t26_30, _t26_31,
	_t26_32, _t26_33, _t26_34, _t26_35;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23,
	_t31_24, _t31_25, _t31_26, _t31_27, _t31_28, _t31_29, _t31_30, _t31_31,
	_t31_32, _t31_33, _t31_34, _t31_35, _t31_36, _t31_37, _t31_38, _t31_39;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23,
	_t33_24, _t33_25, _t33_26, _t33_27, _t33_28, _t33_29, _t33_30, _t33_31,
	_t33_32, _t33_33, _t33_34, _t33_35, _t33_36, _t33_37, _t33_38, _t33_39,
	_t33_40;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3;
  __m256d _t36_0, _t36_1, _t36_2;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3;

  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));
  _t0_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 44)), _mm256_castpd128_pd256(_mm_load_sd(K + 88))), _mm256_castpd128_pd256(_mm_load_sd(K + 132)), 32);
  _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45])));
  _t0_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 89)), _mm256_castpd128_pd256(_mm_load_sd(K + 133)), 0);
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[90])));
  _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[134])));
  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(K[135])));
  _t0_9 = _mm256_castpd128_pd256(_mm_load_sd(&(K[176])));
  _t0_10 = _mm256_maskload_pd(K + 177, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_15 = _mm256_castpd128_pd256(_mm_load_sd(&(K[220])));
  _t0_16 = _mm256_maskload_pd(K + 221, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_21 = _mm256_castpd128_pd256(_mm_load_sd(&(K[264])));
  _t0_22 = _mm256_maskload_pd(K + 265, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_27 = _mm256_castpd128_pd256(_mm_load_sd(&(K[308])));
  _t0_28 = _mm256_maskload_pd(K + 309, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : L[44,44] = S(h(1, 44, 0), Sqrt( G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_145 = _t0_0;

  // 4-BLAC: sqrt(1x4)
  _t0_155 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_145)));

  // AVX Storer:
  _t0_0 = _t0_155;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_169 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_175 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_181 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_169), _mm256_castpd256_pd128(_t0_175)));

  // AVX Storer:
  _t0_1 = _t0_181;

  // Generating : L[44,44] = S(h(3, 44, 1), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, 0)) Kro G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_182 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_1, _t0_1, 32), _mm256_permute2f128_pd(_t0_1, _t0_1, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_183 = _t0_2;

  // 4-BLAC: 1x4 Kro 4x1
  _t0_184 = _mm256_mul_pd(_t0_182, _t0_183);

  // AVX Storer:
  _t0_2 = _t0_184;

  // Generating : L[44,44] = S(h(1, 44, 1), ( G(h(1, 44, 1), L[44,44],h(1, 44, 1)) - ( G(h(1, 44, 1), L[44,44],h(1, 44, 0)) Kro T( G(h(1, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_185 = _t0_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_186 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_187 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t0_188 = _t0_187;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_189 = _mm256_mul_pd(_t0_186, _t0_188);

  // 4-BLAC: 1x4 - 1x4
  _t0_190 = _mm256_sub_pd(_t0_185, _t0_189);

  // AVX Storer:
  _t0_3 = _t0_190;

  // Generating : L[44,44] = S(h(1, 44, 1), Sqrt( G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_191 = _t0_3;

  // 4-BLAC: sqrt(1x4)
  _t0_192 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_191)));

  // AVX Storer:
  _t0_3 = _t0_192;

  // Generating : L[44,44] = S(h(2, 44, 2), ( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) - ( G(h(2, 44, 2), L[44,44],h(1, 44, 0)) Kro T( G(h(1, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(1, 44, 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_193 = _t0_4;

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_194 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 2), _mm256_permute2f128_pd(_t0_2, _t0_2, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_195 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_2, _t0_2, 32), _mm256_permute2f128_pd(_t0_2, _t0_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t0_196 = _t0_195;

  // 4-BLAC: 4x1 Kro 1x4
  _t0_197 = _mm256_mul_pd(_t0_194, _t0_196);

  // 4-BLAC: 4x1 - 4x1
  _t0_198 = _mm256_sub_pd(_t0_193, _t0_197);

  // AVX Storer:
  _t0_4 = _t0_198;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t0_199 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_200 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_201 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_199), _mm256_castpd256_pd128(_t0_200)));

  // AVX Storer:
  _t0_5 = _t0_201;

  // Generating : L[44,44] = S(h(2, 44, 2), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, 1)) Kro G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_202 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_5, _t0_5, 32), _mm256_permute2f128_pd(_t0_5, _t0_5, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_203 = _t0_4;

  // 4-BLAC: 1x4 Kro 4x1
  _t0_204 = _mm256_mul_pd(_t0_202, _t0_203);

  // AVX Storer:
  _t0_4 = _t0_204;

  // Generating : L[44,44] = S(h(1, 44, 2), ( G(h(1, 44, 2), L[44,44],h(1, 44, 2)) - ( G(h(1, 44, 2), L[44,44],h(2, 44, 0)) * T( G(h(1, 44, 2), L[44,44],h(2, 44, 0)) ) ) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_205 = _t0_6;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_33 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_35 = _t0_34;

  // 4-BLAC: 1x4 * 4x1
  _t0_36 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_mul_pd(_t0_33, _t0_35), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_mul_pd(_t0_33, _t0_35), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_33, _t0_35), _mm256_mul_pd(_t0_33, _t0_35), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_37 = _mm256_sub_pd(_t0_205, _t0_36);

  // AVX Storer:
  _t0_6 = _t0_37;

  // Generating : L[44,44] = S(h(1, 44, 2), Sqrt( G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_38 = _t0_6;

  // 4-BLAC: sqrt(1x4)
  _t0_39 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_38)));

  // AVX Storer:
  _t0_6 = _t0_39;

  // Generating : L[44,44] = S(h(1, 44, 3), ( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) - ( G(h(1, 44, 3), L[44,44],h(2, 44, 0)) * T( G(h(1, 44, 2), L[44,44],h(2, 44, 0)) ) ) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_40 = _t0_7;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_41 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_2, 129), _t0_4, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_42 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t0_43 = _t0_42;

  // 4-BLAC: 1x4 * 4x1
  _t0_44 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_mul_pd(_t0_41, _t0_43), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_mul_pd(_t0_41, _t0_43), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_41, _t0_43), _mm256_mul_pd(_t0_41, _t0_43), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_45 = _mm256_sub_pd(_t0_40, _t0_44);

  // AVX Storer:
  _t0_7 = _t0_45;

  // Generating : L[44,44] = S(h(1, 44, 3), ( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_46 = _t0_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_47 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_46), _mm256_castpd256_pd128(_t0_47)));

  // AVX Storer:
  _t0_7 = _t0_48;

  // Generating : L[44,44] = S(h(1, 44, 3), ( G(h(1, 44, 3), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, 3), L[44,44],h(3, 44, 0)) * T( G(h(1, 44, 3), L[44,44],h(3, 44, 0)) ) ) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_49 = _t0_8;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_50 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_51 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // 4-BLAC: (1x4)^T
  _t0_52 = _t0_51;

  // 4-BLAC: 1x4 * 4x1
  _t0_53 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_mul_pd(_t0_50, _t0_52), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_mul_pd(_t0_50, _t0_52), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_50, _t0_52), _mm256_mul_pd(_t0_50, _t0_52), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_54 = _mm256_sub_pd(_t0_49, _t0_53);

  // AVX Storer:
  _t0_8 = _t0_54;

  // Generating : L[44,44] = S(h(1, 44, 3), Sqrt( G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_55 = _t0_8;

  // 4-BLAC: sqrt(1x4)
  _t0_56 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t0_55)));

  // AVX Storer:
  _t0_8 = _t0_56;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_57 = _t0_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_58 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_59 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_57), _mm256_castpd256_pd128(_t0_58)));

  // AVX Storer:
  _t0_9 = _t0_59;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_60 = _t0_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_62 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_63 = _t0_62;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_64 = _mm256_mul_pd(_t0_61, _t0_63);

  // 4-BLAC: 1x4 - 1x4
  _t0_65 = _mm256_sub_pd(_t0_60, _t0_64);

  // AVX Storer:
  _t0_10 = _t0_65;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_66 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_67 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_66), _mm256_castpd256_pd128(_t0_67)));

  // AVX Storer:
  _t0_11 = _t0_68;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_69 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_10, 6), _mm256_permute2f128_pd(_t0_10, _t0_10, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_70 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_11, _t0_11, 32), _mm256_permute2f128_pd(_t0_11, _t0_11, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_71 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_72 = _t0_71;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_73 = _mm256_mul_pd(_t0_70, _t0_72);

  // 4-BLAC: 1x4 - 1x4
  _t0_74 = _mm256_sub_pd(_t0_69, _t0_73);

  // AVX Storer:
  _t0_12 = _t0_74;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_75 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_12, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_76 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_75), _mm256_castpd256_pd128(_t0_76)));

  // AVX Storer:
  _t0_13 = _t0_77;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_78 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_12, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_79 = _t0_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_80 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_81 = _t0_80;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_82 = _mm256_mul_pd(_t0_79, _t0_81);

  // 4-BLAC: 1x4 - 1x4
  _t0_83 = _mm256_sub_pd(_t0_78, _t0_82);

  // AVX Storer:
  _t0_14 = _t0_83;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_84 = _t0_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_85 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_86 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_84), _mm256_castpd256_pd128(_t0_85)));

  // AVX Storer:
  _t0_14 = _t0_86;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_87 = _t0_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_88 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_89 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_87), _mm256_castpd256_pd128(_t0_88)));

  // AVX Storer:
  _t0_15 = _t0_89;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_90 = _t0_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_91 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_92 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_93 = _t0_92;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_94 = _mm256_mul_pd(_t0_91, _t0_93);

  // 4-BLAC: 1x4 - 1x4
  _t0_95 = _mm256_sub_pd(_t0_90, _t0_94);

  // AVX Storer:
  _t0_16 = _t0_95;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_96 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_97 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_96), _mm256_castpd256_pd128(_t0_97)));

  // AVX Storer:
  _t0_17 = _t0_98;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_99 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_16, 6), _mm256_permute2f128_pd(_t0_16, _t0_16, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_101 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_102 = _t0_101;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_103 = _mm256_mul_pd(_t0_100, _t0_102);

  // 4-BLAC: 1x4 - 1x4
  _t0_104 = _mm256_sub_pd(_t0_99, _t0_103);

  // AVX Storer:
  _t0_18 = _t0_104;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_105 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_106 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_105), _mm256_castpd256_pd128(_t0_106)));

  // AVX Storer:
  _t0_19 = _t0_107;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_108 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_18, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_109 = _t0_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_110 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_111 = _t0_110;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_112 = _mm256_mul_pd(_t0_109, _t0_111);

  // 4-BLAC: 1x4 - 1x4
  _t0_113 = _mm256_sub_pd(_t0_108, _t0_112);

  // AVX Storer:
  _t0_20 = _t0_113;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_114 = _t0_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_115 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_114), _mm256_castpd256_pd128(_t0_115)));

  // AVX Storer:
  _t0_20 = _t0_116;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_117 = _t0_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_118 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_117), _mm256_castpd256_pd128(_t0_118)));

  // AVX Storer:
  _t0_21 = _t0_119;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_120 = _t0_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_121 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_21, _t0_21, 32), _mm256_permute2f128_pd(_t0_21, _t0_21, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_122 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_123 = _t0_122;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_124 = _mm256_mul_pd(_t0_121, _t0_123);

  // 4-BLAC: 1x4 - 1x4
  _t0_125 = _mm256_sub_pd(_t0_120, _t0_124);

  // AVX Storer:
  _t0_22 = _t0_125;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_22, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_127 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_128 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_126), _mm256_castpd256_pd128(_t0_127)));

  // AVX Storer:
  _t0_23 = _t0_128;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_129 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_22, 6), _mm256_permute2f128_pd(_t0_22, _t0_22, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_130 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_23, _t0_23, 32), _mm256_permute2f128_pd(_t0_23, _t0_23, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_131 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_132 = _t0_131;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_133 = _mm256_mul_pd(_t0_130, _t0_132);

  // 4-BLAC: 1x4 - 1x4
  _t0_134 = _mm256_sub_pd(_t0_129, _t0_133);

  // AVX Storer:
  _t0_24 = _t0_134;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_135 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_24, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_136 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_135), _mm256_castpd256_pd128(_t0_136)));

  // AVX Storer:
  _t0_25 = _t0_137;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_138 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_24, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_139 = _t0_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_140 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_141 = _t0_140;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_142 = _mm256_mul_pd(_t0_139, _t0_141);

  // 4-BLAC: 1x4 - 1x4
  _t0_143 = _mm256_sub_pd(_t0_138, _t0_142);

  // AVX Storer:
  _t0_26 = _t0_143;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_144 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_146 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_144), _mm256_castpd256_pd128(_t0_146)));

  // AVX Storer:
  _t0_26 = _t0_147;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_148 = _t0_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_149 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t0_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_148), _mm256_castpd256_pd128(_t0_149)));

  // AVX Storer:
  _t0_27 = _t0_150;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_151 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_152 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_27, _t0_27, 32), _mm256_permute2f128_pd(_t0_27, _t0_27, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_153 = _t0_2;

  // 4-BLAC: (4x1)^T
  _t0_154 = _t0_153;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_156 = _mm256_mul_pd(_t0_152, _t0_154);

  // 4-BLAC: 1x4 - 1x4
  _t0_157 = _mm256_sub_pd(_t0_151, _t0_156);

  // AVX Storer:
  _t0_28 = _t0_157;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_158 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_28, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_159 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t0_160 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_158), _mm256_castpd256_pd128(_t0_159)));

  // AVX Storer:
  _t0_29 = _t0_160;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_161 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_28, 6), _mm256_permute2f128_pd(_t0_28, _t0_28, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_162 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_29, _t0_29, 32), _mm256_permute2f128_pd(_t0_29, _t0_29, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_163 = _t0_4;

  // 4-BLAC: (4x1)^T
  _t0_164 = _t0_163;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_165 = _mm256_mul_pd(_t0_162, _t0_164);

  // 4-BLAC: 1x4 - 1x4
  _t0_166 = _mm256_sub_pd(_t0_161, _t0_165);

  // AVX Storer:
  _t0_30 = _t0_166;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_167 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_30, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_168 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t0_170 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_167), _mm256_castpd256_pd128(_t0_168)));

  // AVX Storer:
  _t0_31 = _t0_170;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_171 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_30, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_172 = _t0_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_173 = _t0_7;

  // 4-BLAC: (4x1)^T
  _t0_174 = _t0_173;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_176 = _mm256_mul_pd(_t0_172, _t0_174);

  // 4-BLAC: 1x4 - 1x4
  _t0_177 = _mm256_sub_pd(_t0_171, _t0_176);

  // AVX Storer:
  _t0_32 = _t0_177;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_178 = _t0_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_179 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t0_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_178), _mm256_castpd256_pd128(_t0_179)));

  // AVX Storer:
  _t0_32 = _t0_180;


  for( int fi971 = 4; fi971 <= 36; fi971+=4 ) {
    _t1_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi971 + 176])));
    _t1_1 = _mm256_maskload_pd(K + 44*fi971 + 177, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t1_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi971 + 220])));
    _t1_7 = _mm256_maskload_pd(K + 44*fi971 + 221, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t1_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi971 + 264])));
    _t1_13 = _mm256_maskload_pd(K + 44*fi971 + 265, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t1_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi971 + 308])));
    _t1_19 = _mm256_maskload_pd(K + 44*fi971 + 309, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_24 = _t1_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_25 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_24), _mm256_castpd256_pd128(_t1_25)));

    // AVX Storer:
    _t1_0 = _t1_26;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_27 = _t1_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_0, _t1_0, 32), _mm256_permute2f128_pd(_t1_0, _t1_0, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_29 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_63 = _t1_29;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_64 = _mm256_mul_pd(_t1_28, _t0_63);

    // 4-BLAC: 1x4 - 1x4
    _t1_30 = _mm256_sub_pd(_t1_27, _t0_64);

    // AVX Storer:
    _t1_1 = _t1_30;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_32 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_31), _mm256_castpd256_pd128(_t1_32)));

    // AVX Storer:
    _t1_2 = _t1_33;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_1, 6), _mm256_permute2f128_pd(_t1_1, _t1_1, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_2, _t1_2, 32), _mm256_permute2f128_pd(_t1_2, _t1_2, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_36 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_72 = _t1_36;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_73 = _mm256_mul_pd(_t1_35, _t0_72);

    // 4-BLAC: 1x4 - 1x4
    _t1_37 = _mm256_sub_pd(_t1_34, _t0_73);

    // AVX Storer:
    _t1_3 = _t1_37;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_3, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_39 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_38), _mm256_castpd256_pd128(_t1_39)));

    // AVX Storer:
    _t1_4 = _t1_40;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_41 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_3, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_42 = _t1_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_43 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_81 = _t1_43;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_82 = _mm256_mul_pd(_t1_42, _t0_81);

    // 4-BLAC: 1x4 - 1x4
    _t1_44 = _mm256_sub_pd(_t1_41, _t0_82);

    // AVX Storer:
    _t1_5 = _t1_44;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 4), ( G(h(1, 44, fi971 + 4), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_45 = _t1_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_46 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_45), _mm256_castpd256_pd128(_t1_46)));

    // AVX Storer:
    _t1_5 = _t1_47;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_48 = _t1_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_49 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_48), _mm256_castpd256_pd128(_t1_49)));

    // AVX Storer:
    _t1_6 = _t1_50;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_51 = _t1_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_6, _t1_6, 32), _mm256_permute2f128_pd(_t1_6, _t1_6, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_53 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_93 = _t1_53;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_94 = _mm256_mul_pd(_t1_52, _t0_93);

    // 4-BLAC: 1x4 - 1x4
    _t1_54 = _mm256_sub_pd(_t1_51, _t0_94);

    // AVX Storer:
    _t1_7 = _t1_54;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_55 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_56 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_55), _mm256_castpd256_pd128(_t1_56)));

    // AVX Storer:
    _t1_8 = _t1_57;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 6), _mm256_permute2f128_pd(_t1_7, _t1_7, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_8, _t1_8, 32), _mm256_permute2f128_pd(_t1_8, _t1_8, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_60 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_102 = _t1_60;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_103 = _mm256_mul_pd(_t1_59, _t0_102);

    // 4-BLAC: 1x4 - 1x4
    _t1_61 = _mm256_sub_pd(_t1_58, _t0_103);

    // AVX Storer:
    _t1_9 = _t1_61;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_63 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_62), _mm256_castpd256_pd128(_t1_63)));

    // AVX Storer:
    _t1_10 = _t1_64;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_65 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_66 = _t1_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_67 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_111 = _t1_67;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_112 = _mm256_mul_pd(_t1_66, _t0_111);

    // 4-BLAC: 1x4 - 1x4
    _t1_68 = _mm256_sub_pd(_t1_65, _t0_112);

    // AVX Storer:
    _t1_11 = _t1_68;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 5), ( G(h(1, 44, fi971 + 5), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_69 = _t1_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_70 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_69), _mm256_castpd256_pd128(_t1_70)));

    // AVX Storer:
    _t1_11 = _t1_71;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_72 = _t1_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_73 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_72), _mm256_castpd256_pd128(_t1_73)));

    // AVX Storer:
    _t1_12 = _t1_74;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_75 = _t1_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_12, _t1_12, 32), _mm256_permute2f128_pd(_t1_12, _t1_12, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_77 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_123 = _t1_77;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_124 = _mm256_mul_pd(_t1_76, _t0_123);

    // 4-BLAC: 1x4 - 1x4
    _t1_78 = _mm256_sub_pd(_t1_75, _t0_124);

    // AVX Storer:
    _t1_13 = _t1_78;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_80 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_79), _mm256_castpd256_pd128(_t1_80)));

    // AVX Storer:
    _t1_14 = _t1_81;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_82 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_13, 6), _mm256_permute2f128_pd(_t1_13, _t1_13, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_14, _t1_14, 32), _mm256_permute2f128_pd(_t1_14, _t1_14, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_84 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_132 = _t1_84;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_133 = _mm256_mul_pd(_t1_83, _t0_132);

    // 4-BLAC: 1x4 - 1x4
    _t1_85 = _mm256_sub_pd(_t1_82, _t0_133);

    // AVX Storer:
    _t1_15 = _t1_85;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_87 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_86), _mm256_castpd256_pd128(_t1_87)));

    // AVX Storer:
    _t1_16 = _t1_88;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_15, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_90 = _t1_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_91 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_141 = _t1_91;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_142 = _mm256_mul_pd(_t1_90, _t0_141);

    // 4-BLAC: 1x4 - 1x4
    _t1_92 = _mm256_sub_pd(_t1_89, _t0_142);

    // AVX Storer:
    _t1_17 = _t1_92;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 6), ( G(h(1, 44, fi971 + 6), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_93 = _t1_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_94 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_93), _mm256_castpd256_pd128(_t1_94)));

    // AVX Storer:
    _t1_17 = _t1_95;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) Div G(h(1, 44, 0), L[44,44],h(1, 44, 0)) ),h(1, 44, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_96 = _t1_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_97 = _t0_0;

    // 4-BLAC: 1x4 / 1x4
    _t1_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_96), _mm256_castpd256_pd128(_t1_97)));

    // AVX Storer:
    _t1_18 = _t1_98;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(3, 44, 1)) - ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 0)) Kro T( G(h(3, 44, 1), L[44,44],h(1, 44, 0)) ) ) ),h(3, 44, 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t1_99 = _t1_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_18, _t1_18, 32), _mm256_permute2f128_pd(_t1_18, _t1_18, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t1_101 = _t0_2;

    // 4-BLAC: (4x1)^T
    _t0_154 = _t1_101;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_156 = _mm256_mul_pd(_t1_100, _t0_154);

    // 4-BLAC: 1x4 - 1x4
    _t1_102 = _mm256_sub_pd(_t1_99, _t0_156);

    // AVX Storer:
    _t1_19 = _t1_102;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) Div G(h(1, 44, 1), L[44,44],h(1, 44, 1)) ),h(1, 44, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_104 = _t0_3;

    // 4-BLAC: 1x4 / 1x4
    _t1_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_103), _mm256_castpd256_pd128(_t1_104)));

    // AVX Storer:
    _t1_20 = _t1_105;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(2, 44, 2)) - ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 1)) Kro T( G(h(2, 44, 2), L[44,44],h(1, 44, 1)) ) ) ),h(2, 44, 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t1_106 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_19, 6), _mm256_permute2f128_pd(_t1_19, _t1_19, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_20, _t1_20, 32), _mm256_permute2f128_pd(_t1_20, _t1_20, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t1_108 = _t0_4;

    // 4-BLAC: (4x1)^T
    _t0_164 = _t1_108;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_165 = _mm256_mul_pd(_t1_107, _t0_164);

    // 4-BLAC: 1x4 - 1x4
    _t1_109 = _mm256_sub_pd(_t1_106, _t0_165);

    // AVX Storer:
    _t1_21 = _t1_109;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) Div G(h(1, 44, 2), L[44,44],h(1, 44, 2)) ),h(1, 44, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_111 = _t0_6;

    // 4-BLAC: 1x4 / 1x4
    _t1_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_110), _mm256_castpd256_pd128(_t1_111)));

    // AVX Storer:
    _t1_22 = _t1_112;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) - ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 2)) Kro T( G(h(1, 44, 3), L[44,44],h(1, 44, 2)) ) ) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_113 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_21, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_114 = _t1_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_115 = _t0_7;

    // 4-BLAC: (4x1)^T
    _t0_174 = _t1_115;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_176 = _mm256_mul_pd(_t1_114, _t0_174);

    // 4-BLAC: 1x4 - 1x4
    _t1_116 = _mm256_sub_pd(_t1_113, _t0_176);

    // AVX Storer:
    _t1_23 = _t1_116;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 7), ( G(h(1, 44, fi971 + 7), L[44,44],h(1, 44, 3)) Div G(h(1, 44, 3), L[44,44],h(1, 44, 3)) ),h(1, 44, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_117 = _t1_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t1_118 = _t0_8;

    // 4-BLAC: 1x4 / 1x4
    _t1_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t1_117), _mm256_castpd256_pd128(_t1_118)));

    // AVX Storer:
    _t1_23 = _t1_119;
    _mm_store_sd(&(K[44*fi971 + 176]), _mm256_castpd256_pd128(_t1_0));
    _mm_store_sd(&(K[44*fi971 + 177]), _mm256_castpd256_pd128(_t1_2));
    _mm_store_sd(&(K[44*fi971 + 178]), _mm256_castpd256_pd128(_t1_4));
    _mm_store_sd(&(K[44*fi971 + 179]), _mm256_castpd256_pd128(_t1_5));
    _mm_store_sd(&(K[44*fi971 + 220]), _mm256_castpd256_pd128(_t1_6));
    _mm_store_sd(&(K[44*fi971 + 221]), _mm256_castpd256_pd128(_t1_8));
    _mm_store_sd(&(K[44*fi971 + 222]), _mm256_castpd256_pd128(_t1_10));
    _mm_store_sd(&(K[44*fi971 + 223]), _mm256_castpd256_pd128(_t1_11));
    _mm_store_sd(&(K[44*fi971 + 264]), _mm256_castpd256_pd128(_t1_12));
    _mm_store_sd(&(K[44*fi971 + 265]), _mm256_castpd256_pd128(_t1_14));
    _mm_store_sd(&(K[44*fi971 + 266]), _mm256_castpd256_pd128(_t1_16));
    _mm_store_sd(&(K[44*fi971 + 267]), _mm256_castpd256_pd128(_t1_17));
    _mm_store_sd(&(K[44*fi971 + 308]), _mm256_castpd256_pd128(_t1_18));
    _mm_store_sd(&(K[44*fi971 + 309]), _mm256_castpd256_pd128(_t1_20));
    _mm_store_sd(&(K[44*fi971 + 310]), _mm256_castpd256_pd128(_t1_22));
    _mm_store_sd(&(K[44*fi971 + 311]), _mm256_castpd256_pd128(_t1_23));
  }

  _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(K + 180));
  _t2_1 = _mm256_maskload_pd(K + 224, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t2_2 = _mm256_maskload_pd(K + 268, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t2_3 = _mm256_loadu_pd(K + 312);

  // Generating : L[44,44] = S(h(4, 44, 4), ( G(h(4, 44, 4), K[44,44],h(4, 44, 4)) - ( G(h(4, 44, 4), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, 4), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t2_20 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_0, _t2_1, 0), _mm256_shuffle_pd(_t2_2, _t2_3, 0), 32);
  _t2_21 = _mm256_permute2f128_pd(_t2_1, _mm256_shuffle_pd(_t2_2, _t2_3, 3), 32);
  _t2_22 = _mm256_blend_pd(_t2_2, _mm256_shuffle_pd(_t2_2, _t2_3, 3), 12);
  _t2_23 = _t2_3;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t2_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_82 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_83 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);
  _t2_84 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t2_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_9, _t0_9, 32), _mm256_permute2f128_pd(_t0_9, _t0_9, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_11, _t0_11, 32), _mm256_permute2f128_pd(_t0_11, _t0_11, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_13, _t0_13, 32), _mm256_permute2f128_pd(_t0_13, _t0_13, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_14, _t0_14, 32), _mm256_permute2f128_pd(_t0_14, _t0_14, 32), 0), _t2_84)));
  _t2_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_15, _t0_15, 32), _mm256_permute2f128_pd(_t0_15, _t0_15, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_17, _t0_17, 32), _mm256_permute2f128_pd(_t0_17, _t0_17, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_19, _t0_19, 32), _mm256_permute2f128_pd(_t0_19, _t0_19, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_20, _t0_20, 32), _mm256_permute2f128_pd(_t0_20, _t0_20, 32), 0), _t2_84)));
  _t2_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_21, _t0_21, 32), _mm256_permute2f128_pd(_t0_21, _t0_21, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_23, _t0_23, 32), _mm256_permute2f128_pd(_t0_23, _t0_23, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_25, _t0_25, 32), _mm256_permute2f128_pd(_t0_25, _t0_25, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_26, _t0_26, 32), _mm256_permute2f128_pd(_t0_26, _t0_26, 32), 0), _t2_84)));
  _t2_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_27, _t0_27, 32), _mm256_permute2f128_pd(_t0_27, _t0_27, 32), 0), _t2_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_29, _t0_29, 32), _mm256_permute2f128_pd(_t0_29, _t0_29, 32), 0), _t2_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_31, _t0_31, 32), _mm256_permute2f128_pd(_t0_31, _t0_31, 32), 0), _t2_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 32), _mm256_permute2f128_pd(_t0_32, _t0_32, 32), 0), _t2_84)));

  // 4-BLAC: 4x4 - 4x4
  _t2_16 = _mm256_sub_pd(_t2_20, _t2_12);
  _t2_17 = _mm256_sub_pd(_t2_21, _t2_13);
  _t2_18 = _mm256_sub_pd(_t2_22, _t2_14);
  _t2_19 = _mm256_sub_pd(_t2_23, _t2_15);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t2_0 = _t2_16;
  _t2_1 = _t2_17;
  _t2_2 = _t2_18;
  _t2_3 = _t2_19;

  // Generating : L[44,44] = S(h(1, 44, 4), Sqrt( G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_24 = _t2_0;

  // 4-BLAC: sqrt(1x4)
  _t2_25 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_24)));

  // AVX Storer:
  _t2_0 = _t2_25;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t2_26 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_27 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t2_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_26), _mm256_castpd256_pd128(_t2_27)));

  // AVX Storer:
  _t2_4 = _t2_28;

  // Generating : L[44,44] = S(h(3, 44, 5), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, 4)) Kro G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_29 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_4, _t2_4, 32), _mm256_permute2f128_pd(_t2_4, _t2_4, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t2_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_1, _t2_2), _mm256_unpacklo_pd(_t2_3, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t2_31 = _mm256_mul_pd(_t2_29, _t2_30);

  // AVX Storer:
  _t2_5 = _t2_31;

  // Generating : L[44,44] = S(h(1, 44, 5), ( G(h(1, 44, 5), L[44,44],h(1, 44, 5)) - ( G(h(1, 44, 5), L[44,44],h(1, 44, 4)) Kro T( G(h(1, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_1, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_33 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_5, 1);

  // 4-BLAC: (4x1)^T
  _t2_35 = _t2_34;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_36 = _mm256_mul_pd(_t2_33, _t2_35);

  // 4-BLAC: 1x4 - 1x4
  _t2_37 = _mm256_sub_pd(_t2_32, _t2_36);

  // AVX Storer:
  _t2_6 = _t2_37;

  // Generating : L[44,44] = S(h(1, 44, 5), Sqrt( G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_38 = _t2_6;

  // 4-BLAC: sqrt(1x4)
  _t2_39 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_38)));

  // AVX Storer:
  _t2_6 = _t2_39;

  // Generating : L[44,44] = S(h(2, 44, 6), ( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) - ( G(h(2, 44, 6), L[44,44],h(1, 44, 4)) Kro T( G(h(1, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(1, 44, 5))

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_40 = _mm256_unpackhi_pd(_mm256_blend_pd(_t2_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_3, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_5, 2), _mm256_permute2f128_pd(_t2_5, _t2_5, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 32), _mm256_permute2f128_pd(_t2_5, _t2_5, 32), 0);

  // 4-BLAC: (4x1)^T
  _t2_43 = _t2_42;

  // 4-BLAC: 4x1 Kro 1x4
  _t2_44 = _mm256_mul_pd(_t2_41, _t2_43);

  // 4-BLAC: 4x1 - 4x1
  _t2_45 = _mm256_sub_pd(_t2_40, _t2_44);

  // AVX Storer:
  _t2_7 = _t2_45;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t2_46 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_47 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t2_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_46), _mm256_castpd256_pd128(_t2_47)));

  // AVX Storer:
  _t2_8 = _t2_48;

  // Generating : L[44,44] = S(h(2, 44, 6), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, 5)) Kro G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_8, _t2_8, 32), _mm256_permute2f128_pd(_t2_8, _t2_8, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t2_50 = _t2_7;

  // 4-BLAC: 1x4 Kro 4x1
  _t2_51 = _mm256_mul_pd(_t2_49, _t2_50);

  // AVX Storer:
  _t2_7 = _t2_51;

  // Generating : L[44,44] = S(h(1, 44, 6), ( G(h(1, 44, 6), L[44,44],h(1, 44, 6)) - ( G(h(1, 44, 6), L[44,44],h(2, 44, 4)) * T( G(h(1, 44, 6), L[44,44],h(2, 44, 4)) ) ) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_52 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_2, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_2, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_53 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_5, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_7, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_54 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_5, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_7, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t2_55 = _t2_54;

  // 4-BLAC: 1x4 * 4x1
  _t2_56 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_mul_pd(_t2_53, _t2_55), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_mul_pd(_t2_53, _t2_55), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_53, _t2_55), _mm256_mul_pd(_t2_53, _t2_55), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_57 = _mm256_sub_pd(_t2_52, _t2_56);

  // AVX Storer:
  _t2_9 = _t2_57;

  // Generating : L[44,44] = S(h(1, 44, 6), Sqrt( G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_58 = _t2_9;

  // 4-BLAC: sqrt(1x4)
  _t2_59 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_58)));

  // AVX Storer:
  _t2_9 = _t2_59;

  // Generating : L[44,44] = S(h(1, 44, 7), ( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) - ( G(h(1, 44, 7), L[44,44],h(2, 44, 4)) * T( G(h(1, 44, 6), L[44,44],h(2, 44, 4)) ) ) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_60 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_3, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t2_3, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_61 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_5, _t2_5, 129), _t2_7, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_62 = _mm256_shuffle_pd(_mm256_blend_pd(_t2_5, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t2_7, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t2_63 = _t2_62;

  // 4-BLAC: 1x4 * 4x1
  _t2_64 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_mul_pd(_t2_61, _t2_63), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_mul_pd(_t2_61, _t2_63), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_61, _t2_63), _mm256_mul_pd(_t2_61, _t2_63), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_65 = _mm256_sub_pd(_t2_60, _t2_64);

  // AVX Storer:
  _t2_10 = _t2_65;

  // Generating : L[44,44] = S(h(1, 44, 7), ( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_66 = _t2_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_67 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t2_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_66), _mm256_castpd256_pd128(_t2_67)));

  // AVX Storer:
  _t2_10 = _t2_68;

  // Generating : L[44,44] = S(h(1, 44, 7), ( G(h(1, 44, 7), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, 7), L[44,44],h(3, 44, 4)) * T( G(h(1, 44, 7), L[44,44],h(3, 44, 4)) ) ) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_69 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t2_3, _t2_3, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_70 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_5, _t2_10, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t2_7, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_71 = _mm256_blend_pd(_mm256_permute2f128_pd(_t2_5, _t2_10, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t2_7, 2), 10);

  // 4-BLAC: (1x4)^T
  _t2_72 = _t2_71;

  // 4-BLAC: 1x4 * 4x1
  _t2_73 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_mul_pd(_t2_70, _t2_72), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_mul_pd(_t2_70, _t2_72), 129)), _mm256_add_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_permute2f128_pd(_mm256_mul_pd(_t2_70, _t2_72), _mm256_mul_pd(_t2_70, _t2_72), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t2_74 = _mm256_sub_pd(_t2_69, _t2_73);

  // AVX Storer:
  _t2_11 = _t2_74;

  // Generating : L[44,44] = S(h(1, 44, 7), Sqrt( G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_75 = _t2_11;

  // 4-BLAC: sqrt(1x4)
  _t2_76 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t2_75)));

  // AVX Storer:
  _t2_11 = _t2_76;

  // Generating : L[44,44] = Sum_{k207} ( S(h(4, 44, k207 + 8), ( G(h(4, 44, k207 + 8), K[44,44],h(4, 44, 4)) - ( G(h(4, 44, k207 + 8), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, 4), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, 4)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t2_77 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_78 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 32);
  _t2_79 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);
  _t2_80 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_9, _t0_11), _mm256_unpacklo_pd(_t0_13, _t0_14), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_15, _t0_17), _mm256_unpacklo_pd(_t0_19, _t0_20), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_21, _t0_23), _mm256_unpacklo_pd(_t0_25, _t0_26), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_27, _t0_29), _mm256_unpacklo_pd(_t0_31, _t0_32), 32)), 49);


  for( int k207 = 0; k207 <= 35; k207+=4 ) {
    _t3_20 = _mm256_loadu_pd(K + 44*k207 + 356);
    _t3_21 = _mm256_loadu_pd(K + 44*k207 + 400);
    _t3_22 = _mm256_loadu_pd(K + 44*k207 + 444);
    _t3_23 = _mm256_loadu_pd(K + 44*k207 + 488);
    _t3_15 = _mm256_broadcast_sd(K + 44*k207 + 352);
    _t3_14 = _mm256_broadcast_sd(K + 44*k207 + 353);
    _t3_13 = _mm256_broadcast_sd(K + 44*k207 + 354);
    _t3_12 = _mm256_broadcast_sd(K + 44*k207 + 355);
    _t3_11 = _mm256_broadcast_sd(K + 44*k207 + 396);
    _t3_10 = _mm256_broadcast_sd(K + 44*k207 + 397);
    _t3_9 = _mm256_broadcast_sd(K + 44*k207 + 398);
    _t3_8 = _mm256_broadcast_sd(K + 44*k207 + 399);
    _t3_7 = _mm256_broadcast_sd(K + 44*k207 + 440);
    _t3_6 = _mm256_broadcast_sd(K + 44*k207 + 441);
    _t3_5 = _mm256_broadcast_sd(K + 44*k207 + 442);
    _t3_4 = _mm256_broadcast_sd(K + 44*k207 + 443);
    _t3_3 = _mm256_broadcast_sd(K + 44*k207 + 484);
    _t3_2 = _mm256_broadcast_sd(K + 44*k207 + 485);
    _t3_1 = _mm256_broadcast_sd(K + 44*k207 + 486);
    _t3_0 = _mm256_broadcast_sd(K + 44*k207 + 487);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t3_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t2_77), _mm256_mul_pd(_t3_14, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t2_79), _mm256_mul_pd(_t3_12, _t2_80)));
    _t3_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t2_77), _mm256_mul_pd(_t3_10, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t2_79), _mm256_mul_pd(_t3_8, _t2_80)));
    _t3_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t2_77), _mm256_mul_pd(_t3_6, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t2_79), _mm256_mul_pd(_t3_4, _t2_80)));
    _t3_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _t2_77), _mm256_mul_pd(_t3_2, _t2_78)), _mm256_add_pd(_mm256_mul_pd(_t3_1, _t2_79), _mm256_mul_pd(_t3_0, _t2_80)));

    // 4-BLAC: 4x4 - 4x4
    _t3_20 = _mm256_sub_pd(_t3_20, _t3_16);
    _t3_21 = _mm256_sub_pd(_t3_21, _t3_17);
    _t3_22 = _mm256_sub_pd(_t3_22, _t3_18);
    _t3_23 = _mm256_sub_pd(_t3_23, _t3_19);

    // AVX Storer:
    _mm256_storeu_pd(K + 44*k207 + 356, _t3_20);
    _mm256_storeu_pd(K + 44*k207 + 400, _t3_21);
    _mm256_storeu_pd(K + 44*k207 + 444, _t3_22);
    _mm256_storeu_pd(K + 44*k207 + 488, _t3_23);
  }

  _t4_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[356])));
  _t4_1 = _mm256_maskload_pd(K + 357, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[400])));
  _t4_7 = _mm256_maskload_pd(K + 401, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[444])));
  _t4_13 = _mm256_maskload_pd(K + 445, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t4_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[488])));
  _t4_19 = _mm256_maskload_pd(K + 489, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_59 = _t4_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_60 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_59), _mm256_castpd256_pd128(_t4_60)));

  // AVX Storer:
  _t4_0 = _t4_61;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_62 = _t4_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_63 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_0, _t4_0, 32), _mm256_permute2f128_pd(_t4_0, _t4_0, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_64 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_65 = _t4_64;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_66 = _mm256_mul_pd(_t4_63, _t4_65);

  // 4-BLAC: 1x4 - 1x4
  _t4_67 = _mm256_sub_pd(_t4_62, _t4_66);

  // AVX Storer:
  _t4_1 = _t4_67;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_68 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_69 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_68), _mm256_castpd256_pd128(_t4_69)));

  // AVX Storer:
  _t4_2 = _t4_70;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_71 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_1, 6), _mm256_permute2f128_pd(_t4_1, _t4_1, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_72 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_2, _t4_2, 32), _mm256_permute2f128_pd(_t4_2, _t4_2, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_73 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_74 = _t4_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_75 = _mm256_mul_pd(_t4_72, _t4_74);

  // 4-BLAC: 1x4 - 1x4
  _t4_76 = _mm256_sub_pd(_t4_71, _t4_75);

  // AVX Storer:
  _t4_3 = _t4_76;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_77 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_78 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_77), _mm256_castpd256_pd128(_t4_78)));

  // AVX Storer:
  _t4_4 = _t4_79;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_80 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_81 = _t4_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_82 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_83 = _t4_82;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_84 = _mm256_mul_pd(_t4_81, _t4_83);

  // 4-BLAC: 1x4 - 1x4
  _t4_85 = _mm256_sub_pd(_t4_80, _t4_84);

  // AVX Storer:
  _t4_5 = _t4_85;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_86 = _t4_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_87 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_86), _mm256_castpd256_pd128(_t4_87)));

  // AVX Storer:
  _t4_5 = _t4_88;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_89 = _t4_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_90 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_89), _mm256_castpd256_pd128(_t4_90)));

  // AVX Storer:
  _t4_6 = _t4_91;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_92 = _t4_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_93 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_6, _t4_6, 32), _mm256_permute2f128_pd(_t4_6, _t4_6, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_94 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_95 = _t4_94;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_96 = _mm256_mul_pd(_t4_93, _t4_95);

  // 4-BLAC: 1x4 - 1x4
  _t4_97 = _mm256_sub_pd(_t4_92, _t4_96);

  // AVX Storer:
  _t4_7 = _t4_97;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_98 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_99 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_98), _mm256_castpd256_pd128(_t4_99)));

  // AVX Storer:
  _t4_8 = _t4_100;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_101 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_7, 6), _mm256_permute2f128_pd(_t4_7, _t4_7, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_102 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_8, _t4_8, 32), _mm256_permute2f128_pd(_t4_8, _t4_8, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_103 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_104 = _t4_103;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_105 = _mm256_mul_pd(_t4_102, _t4_104);

  // 4-BLAC: 1x4 - 1x4
  _t4_106 = _mm256_sub_pd(_t4_101, _t4_105);

  // AVX Storer:
  _t4_9 = _t4_106;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_107 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_9, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_108 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_109 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_107), _mm256_castpd256_pd128(_t4_108)));

  // AVX Storer:
  _t4_10 = _t4_109;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_110 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_111 = _t4_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_112 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_113 = _t4_112;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_114 = _mm256_mul_pd(_t4_111, _t4_113);

  // 4-BLAC: 1x4 - 1x4
  _t4_115 = _mm256_sub_pd(_t4_110, _t4_114);

  // AVX Storer:
  _t4_11 = _t4_115;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_116 = _t4_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_117 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_116), _mm256_castpd256_pd128(_t4_117)));

  // AVX Storer:
  _t4_11 = _t4_118;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_119 = _t4_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_120 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_119), _mm256_castpd256_pd128(_t4_120)));

  // AVX Storer:
  _t4_12 = _t4_121;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_122 = _t4_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_123 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_12, _t4_12, 32), _mm256_permute2f128_pd(_t4_12, _t4_12, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_124 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_125 = _t4_124;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_126 = _mm256_mul_pd(_t4_123, _t4_125);

  // 4-BLAC: 1x4 - 1x4
  _t4_127 = _mm256_sub_pd(_t4_122, _t4_126);

  // AVX Storer:
  _t4_13 = _t4_127;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_128 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_129 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_130 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_128), _mm256_castpd256_pd128(_t4_129)));

  // AVX Storer:
  _t4_14 = _t4_130;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_131 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_13, 6), _mm256_permute2f128_pd(_t4_13, _t4_13, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_132 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_14, _t4_14, 32), _mm256_permute2f128_pd(_t4_14, _t4_14, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_133 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_134 = _t4_133;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_135 = _mm256_mul_pd(_t4_132, _t4_134);

  // 4-BLAC: 1x4 - 1x4
  _t4_136 = _mm256_sub_pd(_t4_131, _t4_135);

  // AVX Storer:
  _t4_15 = _t4_136;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_137 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_138 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_139 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_137), _mm256_castpd256_pd128(_t4_138)));

  // AVX Storer:
  _t4_16 = _t4_139;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_140 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_15, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_141 = _t4_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_142 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_143 = _t4_142;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_24 = _mm256_mul_pd(_t4_141, _t4_143);

  // 4-BLAC: 1x4 - 1x4
  _t4_25 = _mm256_sub_pd(_t4_140, _t4_24);

  // AVX Storer:
  _t4_17 = _t4_25;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_26 = _t4_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_27 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_26), _mm256_castpd256_pd128(_t4_27)));

  // AVX Storer:
  _t4_17 = _t4_28;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_29 = _t4_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_30 = _t2_0;

  // 4-BLAC: 1x4 / 1x4
  _t4_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_29), _mm256_castpd256_pd128(_t4_30)));

  // AVX Storer:
  _t4_18 = _t4_31;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

  // AVX Loader:

  // 1x3 -> 1x4
  _t4_32 = _t4_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_18, _t4_18, 32), _mm256_permute2f128_pd(_t4_18, _t4_18, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t4_34 = _t2_5;

  // 4-BLAC: (4x1)^T
  _t4_35 = _t4_34;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_36 = _mm256_mul_pd(_t4_33, _t4_35);

  // 4-BLAC: 1x4 - 1x4
  _t4_37 = _mm256_sub_pd(_t4_32, _t4_36);

  // AVX Storer:
  _t4_19 = _t4_37;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_39 = _t2_6;

  // 4-BLAC: 1x4 / 1x4
  _t4_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_38), _mm256_castpd256_pd128(_t4_39)));

  // AVX Storer:
  _t4_20 = _t4_40;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t4_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_19, 6), _mm256_permute2f128_pd(_t4_19, _t4_19, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_20, _t4_20, 32), _mm256_permute2f128_pd(_t4_20, _t4_20, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t4_43 = _t2_7;

  // 4-BLAC: (4x1)^T
  _t4_44 = _t4_43;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_45 = _mm256_mul_pd(_t4_42, _t4_44);

  // 4-BLAC: 1x4 - 1x4
  _t4_46 = _mm256_sub_pd(_t4_41, _t4_45);

  // AVX Storer:
  _t4_21 = _t4_46;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_47 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_48 = _t2_9;

  // 4-BLAC: 1x4 / 1x4
  _t4_49 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_47), _mm256_castpd256_pd128(_t4_48)));

  // AVX Storer:
  _t4_22 = _t4_49;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_50 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_51 = _t4_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_52 = _t2_10;

  // 4-BLAC: (4x1)^T
  _t4_53 = _t4_52;

  // 4-BLAC: 1x4 Kro 1x4
  _t4_54 = _mm256_mul_pd(_t4_51, _t4_53);

  // 4-BLAC: 1x4 - 1x4
  _t4_55 = _mm256_sub_pd(_t4_50, _t4_54);

  // AVX Storer:
  _t4_23 = _t4_55;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_56 = _t4_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t4_57 = _t2_11;

  // 4-BLAC: 1x4 / 1x4
  _t4_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_56), _mm256_castpd256_pd128(_t4_57)));

  // AVX Storer:
  _t4_23 = _t4_58;

  _mm256_maskstore_pd(K + 224, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t2_1);
  _mm256_maskstore_pd(K + 268, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t2_2);
  _mm256_storeu_pd(K + 312, _t2_3);

  for( int fi1090 = 4; fi1090 <= 32; fi1090+=4 ) {
    _t5_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 356])));
    _t5_1 = _mm256_maskload_pd(K + 44*fi1090 + 357, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t5_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 400])));
    _t5_7 = _mm256_maskload_pd(K + 44*fi1090 + 401, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t5_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 444])));
    _t5_13 = _mm256_maskload_pd(K + 44*fi1090 + 445, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t5_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 488])));
    _t5_19 = _mm256_maskload_pd(K + 44*fi1090 + 489, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_24 = _t5_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_25 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_24), _mm256_castpd256_pd128(_t5_25)));

    // AVX Storer:
    _t5_0 = _t5_26;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_27 = _t5_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_0, _t5_0, 32), _mm256_permute2f128_pd(_t5_0, _t5_0, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_29 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_65 = _t5_29;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_66 = _mm256_mul_pd(_t5_28, _t4_65);

    // 4-BLAC: 1x4 - 1x4
    _t5_30 = _mm256_sub_pd(_t5_27, _t4_66);

    // AVX Storer:
    _t5_1 = _t5_30;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_32 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_31), _mm256_castpd256_pd128(_t5_32)));

    // AVX Storer:
    _t5_2 = _t5_33;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_1, 6), _mm256_permute2f128_pd(_t5_1, _t5_1, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_2, _t5_2, 32), _mm256_permute2f128_pd(_t5_2, _t5_2, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_36 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_74 = _t5_36;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_75 = _mm256_mul_pd(_t5_35, _t4_74);

    // 4-BLAC: 1x4 - 1x4
    _t5_37 = _mm256_sub_pd(_t5_34, _t4_75);

    // AVX Storer:
    _t5_3 = _t5_37;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_3, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_39 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_38), _mm256_castpd256_pd128(_t5_39)));

    // AVX Storer:
    _t5_4 = _t5_40;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_41 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_3, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_42 = _t5_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_43 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_83 = _t5_43;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_84 = _mm256_mul_pd(_t5_42, _t4_83);

    // 4-BLAC: 1x4 - 1x4
    _t5_44 = _mm256_sub_pd(_t5_41, _t4_84);

    // AVX Storer:
    _t5_5 = _t5_44;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 8), ( G(h(1, 44, fi1090 + 8), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_45 = _t5_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_46 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_45), _mm256_castpd256_pd128(_t5_46)));

    // AVX Storer:
    _t5_5 = _t5_47;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_48 = _t5_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_49 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_48), _mm256_castpd256_pd128(_t5_49)));

    // AVX Storer:
    _t5_6 = _t5_50;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_51 = _t5_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_6, _t5_6, 32), _mm256_permute2f128_pd(_t5_6, _t5_6, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_53 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_95 = _t5_53;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_96 = _mm256_mul_pd(_t5_52, _t4_95);

    // 4-BLAC: 1x4 - 1x4
    _t5_54 = _mm256_sub_pd(_t5_51, _t4_96);

    // AVX Storer:
    _t5_7 = _t5_54;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_55 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_56 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_55), _mm256_castpd256_pd128(_t5_56)));

    // AVX Storer:
    _t5_8 = _t5_57;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_7, 6), _mm256_permute2f128_pd(_t5_7, _t5_7, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_8, _t5_8, 32), _mm256_permute2f128_pd(_t5_8, _t5_8, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_60 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_104 = _t5_60;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_105 = _mm256_mul_pd(_t5_59, _t4_104);

    // 4-BLAC: 1x4 - 1x4
    _t5_61 = _mm256_sub_pd(_t5_58, _t4_105);

    // AVX Storer:
    _t5_9 = _t5_61;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_63 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_62), _mm256_castpd256_pd128(_t5_63)));

    // AVX Storer:
    _t5_10 = _t5_64;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_65 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_66 = _t5_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_67 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_113 = _t5_67;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_114 = _mm256_mul_pd(_t5_66, _t4_113);

    // 4-BLAC: 1x4 - 1x4
    _t5_68 = _mm256_sub_pd(_t5_65, _t4_114);

    // AVX Storer:
    _t5_11 = _t5_68;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 9), ( G(h(1, 44, fi1090 + 9), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_69 = _t5_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_70 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_69), _mm256_castpd256_pd128(_t5_70)));

    // AVX Storer:
    _t5_11 = _t5_71;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_72 = _t5_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_73 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_72), _mm256_castpd256_pd128(_t5_73)));

    // AVX Storer:
    _t5_12 = _t5_74;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_75 = _t5_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_77 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_125 = _t5_77;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_126 = _mm256_mul_pd(_t5_76, _t4_125);

    // 4-BLAC: 1x4 - 1x4
    _t5_78 = _mm256_sub_pd(_t5_75, _t4_126);

    // AVX Storer:
    _t5_13 = _t5_78;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_80 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_79), _mm256_castpd256_pd128(_t5_80)));

    // AVX Storer:
    _t5_14 = _t5_81;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_82 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_13, 6), _mm256_permute2f128_pd(_t5_13, _t5_13, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_84 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_134 = _t5_84;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_135 = _mm256_mul_pd(_t5_83, _t4_134);

    // 4-BLAC: 1x4 - 1x4
    _t5_85 = _mm256_sub_pd(_t5_82, _t4_135);

    // AVX Storer:
    _t5_15 = _t5_85;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_87 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_86), _mm256_castpd256_pd128(_t5_87)));

    // AVX Storer:
    _t5_16 = _t5_88;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_15, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_90 = _t5_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_91 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_143 = _t5_91;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_24 = _mm256_mul_pd(_t5_90, _t4_143);

    // 4-BLAC: 1x4 - 1x4
    _t5_92 = _mm256_sub_pd(_t5_89, _t4_24);

    // AVX Storer:
    _t5_17 = _t5_92;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 10), ( G(h(1, 44, fi1090 + 10), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_93 = _t5_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_94 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_93), _mm256_castpd256_pd128(_t5_94)));

    // AVX Storer:
    _t5_17 = _t5_95;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 4)) Div G(h(1, 44, 4), L[44,44],h(1, 44, 4)) ),h(1, 44, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_96 = _t5_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_97 = _t2_0;

    // 4-BLAC: 1x4 / 1x4
    _t5_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_96), _mm256_castpd256_pd128(_t5_97)));

    // AVX Storer:
    _t5_18 = _t5_98;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(3, 44, 5)) - ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 4)) Kro T( G(h(3, 44, 5), L[44,44],h(1, 44, 4)) ) ) ),h(3, 44, 5))

    // AVX Loader:

    // 1x3 -> 1x4
    _t5_99 = _t5_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t5_101 = _t2_5;

    // 4-BLAC: (4x1)^T
    _t4_35 = _t5_101;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_36 = _mm256_mul_pd(_t5_100, _t4_35);

    // 4-BLAC: 1x4 - 1x4
    _t5_102 = _mm256_sub_pd(_t5_99, _t4_36);

    // AVX Storer:
    _t5_19 = _t5_102;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 5)) Div G(h(1, 44, 5), L[44,44],h(1, 44, 5)) ),h(1, 44, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_104 = _t2_6;

    // 4-BLAC: 1x4 / 1x4
    _t5_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_103), _mm256_castpd256_pd128(_t5_104)));

    // AVX Storer:
    _t5_20 = _t5_105;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(2, 44, 6)) - ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 5)) Kro T( G(h(2, 44, 6), L[44,44],h(1, 44, 5)) ) ) ),h(2, 44, 6))

    // AVX Loader:

    // 1x2 -> 1x4
    _t5_106 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_19, 6), _mm256_permute2f128_pd(_t5_19, _t5_19, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t5_108 = _t2_7;

    // 4-BLAC: (4x1)^T
    _t4_44 = _t5_108;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_45 = _mm256_mul_pd(_t5_107, _t4_44);

    // 4-BLAC: 1x4 - 1x4
    _t5_109 = _mm256_sub_pd(_t5_106, _t4_45);

    // AVX Storer:
    _t5_21 = _t5_109;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 6)) Div G(h(1, 44, 6), L[44,44],h(1, 44, 6)) ),h(1, 44, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_111 = _t2_9;

    // 4-BLAC: 1x4 / 1x4
    _t5_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_110), _mm256_castpd256_pd128(_t5_111)));

    // AVX Storer:
    _t5_22 = _t5_112;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 7)) - ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 6)) Kro T( G(h(1, 44, 7), L[44,44],h(1, 44, 6)) ) ) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_113 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_21, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_114 = _t5_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_115 = _t2_10;

    // 4-BLAC: (4x1)^T
    _t4_53 = _t5_115;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_54 = _mm256_mul_pd(_t5_114, _t4_53);

    // 4-BLAC: 1x4 - 1x4
    _t5_116 = _mm256_sub_pd(_t5_113, _t4_54);

    // AVX Storer:
    _t5_23 = _t5_116;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + 11), ( G(h(1, 44, fi1090 + 11), L[44,44],h(1, 44, 7)) Div G(h(1, 44, 7), L[44,44],h(1, 44, 7)) ),h(1, 44, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_117 = _t5_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t5_118 = _t2_11;

    // 4-BLAC: 1x4 / 1x4
    _t5_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_117), _mm256_castpd256_pd128(_t5_118)));

    // AVX Storer:
    _t5_23 = _t5_119;
    _mm_store_sd(&(K[44*fi1090 + 356]), _mm256_castpd256_pd128(_t5_0));
    _mm_store_sd(&(K[44*fi1090 + 357]), _mm256_castpd256_pd128(_t5_2));
    _mm_store_sd(&(K[44*fi1090 + 358]), _mm256_castpd256_pd128(_t5_4));
    _mm_store_sd(&(K[44*fi1090 + 359]), _mm256_castpd256_pd128(_t5_5));
    _mm_store_sd(&(K[44*fi1090 + 400]), _mm256_castpd256_pd128(_t5_6));
    _mm_store_sd(&(K[44*fi1090 + 401]), _mm256_castpd256_pd128(_t5_8));
    _mm_store_sd(&(K[44*fi1090 + 402]), _mm256_castpd256_pd128(_t5_10));
    _mm_store_sd(&(K[44*fi1090 + 403]), _mm256_castpd256_pd128(_t5_11));
    _mm_store_sd(&(K[44*fi1090 + 444]), _mm256_castpd256_pd128(_t5_12));
    _mm_store_sd(&(K[44*fi1090 + 445]), _mm256_castpd256_pd128(_t5_14));
    _mm_store_sd(&(K[44*fi1090 + 446]), _mm256_castpd256_pd128(_t5_16));
    _mm_store_sd(&(K[44*fi1090 + 447]), _mm256_castpd256_pd128(_t5_17));
    _mm_store_sd(&(K[44*fi1090 + 488]), _mm256_castpd256_pd128(_t5_18));
    _mm_store_sd(&(K[44*fi1090 + 489]), _mm256_castpd256_pd128(_t5_20));
    _mm_store_sd(&(K[44*fi1090 + 490]), _mm256_castpd256_pd128(_t5_22));
    _mm_store_sd(&(K[44*fi1090 + 491]), _mm256_castpd256_pd128(_t5_23));
  }

  _t6_20 = _mm256_castpd128_pd256(_mm_load_sd(K + 360));
  _t6_21 = _mm256_maskload_pd(K + 404, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t6_22 = _mm256_maskload_pd(K + 448, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t6_23 = _mm256_loadu_pd(K + 492);
  _t6_19 = _mm256_broadcast_sd(K + 352);
  _t6_18 = _mm256_broadcast_sd(K + 353);
  _t6_17 = _mm256_broadcast_sd(K + 354);
  _t6_16 = _mm256_broadcast_sd(K + 355);
  _t6_15 = _mm256_broadcast_sd(K + 396);
  _t6_14 = _mm256_broadcast_sd(K + 397);
  _t6_13 = _mm256_broadcast_sd(K + 398);
  _t6_12 = _mm256_broadcast_sd(K + 399);
  _t6_11 = _mm256_broadcast_sd(K + 440);
  _t6_10 = _mm256_broadcast_sd(K + 441);
  _t6_9 = _mm256_broadcast_sd(K + 442);
  _t6_8 = _mm256_broadcast_sd(K + 443);
  _t6_7 = _mm256_broadcast_sd(K + 484);
  _t6_6 = _mm256_broadcast_sd(K + 485);
  _t6_5 = _mm256_broadcast_sd(K + 486);
  _t6_4 = _mm256_broadcast_sd(K + 487);
  _t6_3 = _mm256_loadu_pd(K + 352);
  _t6_2 = _mm256_loadu_pd(K + 396);
  _t6_1 = _mm256_loadu_pd(K + 440);
  _t6_0 = _mm256_loadu_pd(K + 484);

  // Generating : L[44,44] = ( S(h(4, 44, fi971), ( G(h(4, 44, fi971), K[44,44],h(4, 44, fi971)) - ( G(h(4, 44, fi971), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, fi971)) + Sum_{k159} ( -$(h(4, 44, fi971), ( G(h(4, 44, fi971), L[44,44],h(4, 44, k159)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, k159)) ) ),h(4, 44, fi971)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t6_44 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_20, _t6_21, 0), _mm256_shuffle_pd(_t6_22, _t6_23, 0), 32);
  _t6_45 = _mm256_permute2f128_pd(_t6_21, _mm256_shuffle_pd(_t6_22, _t6_23, 3), 32);
  _t6_46 = _mm256_blend_pd(_t6_22, _mm256_shuffle_pd(_t6_22, _t6_23, 3), 12);
  _t6_47 = _t6_23;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_105 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
  _t6_106 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
  _t6_107 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
  _t6_108 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_19, _t6_105), _mm256_mul_pd(_t6_18, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_17, _t6_107), _mm256_mul_pd(_t6_16, _t6_108)));
  _t6_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t6_105), _mm256_mul_pd(_t6_14, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t6_107), _mm256_mul_pd(_t6_12, _t6_108)));
  _t6_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t6_105), _mm256_mul_pd(_t6_10, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t6_107), _mm256_mul_pd(_t6_8, _t6_108)));
  _t6_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t6_105), _mm256_mul_pd(_t6_6, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t6_107), _mm256_mul_pd(_t6_4, _t6_108)));

  // 4-BLAC: 4x4 - 4x4
  _t6_40 = _mm256_sub_pd(_t6_44, _t6_32);
  _t6_41 = _mm256_sub_pd(_t6_45, _t6_33);
  _t6_42 = _mm256_sub_pd(_t6_46, _t6_34);
  _t6_43 = _mm256_sub_pd(_t6_47, _t6_35);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t6_20 = _t6_40;
  _t6_21 = _t6_41;
  _t6_22 = _t6_42;
  _t6_23 = _t6_43;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t6_110 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t6_111 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);
  _t6_112 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);

  // 4-BLAC: 4x4 * 4x4
  _t6_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_0, _t4_0, 32), _mm256_permute2f128_pd(_t4_0, _t4_0, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_2, _t4_2, 32), _mm256_permute2f128_pd(_t4_2, _t4_2, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_4, _t4_4, 32), _mm256_permute2f128_pd(_t4_4, _t4_4, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_5, _t4_5, 32), _mm256_permute2f128_pd(_t4_5, _t4_5, 32), 0), _t6_112)));
  _t6_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_6, _t4_6, 32), _mm256_permute2f128_pd(_t4_6, _t4_6, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_8, _t4_8, 32), _mm256_permute2f128_pd(_t4_8, _t4_8, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_10, _t4_10, 32), _mm256_permute2f128_pd(_t4_10, _t4_10, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_11, _t4_11, 32), _mm256_permute2f128_pd(_t4_11, _t4_11, 32), 0), _t6_112)));
  _t6_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_12, _t4_12, 32), _mm256_permute2f128_pd(_t4_12, _t4_12, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_14, _t4_14, 32), _mm256_permute2f128_pd(_t4_14, _t4_14, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_16, _t4_16, 32), _mm256_permute2f128_pd(_t4_16, _t4_16, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_17, _t4_17, 32), _mm256_permute2f128_pd(_t4_17, _t4_17, 32), 0), _t6_112)));
  _t6_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_18, _t4_18, 32), _mm256_permute2f128_pd(_t4_18, _t4_18, 32), 0), _t6_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_20, _t4_20, 32), _mm256_permute2f128_pd(_t4_20, _t4_20, 32), 0), _t6_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_22, _t4_22, 32), _mm256_permute2f128_pd(_t4_22, _t4_22, 32), 0), _t6_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_23, _t4_23, 32), _mm256_permute2f128_pd(_t4_23, _t4_23, 32), 0), _t6_112)));

  // AVX Loader:

  // 4x4 -> 4x4 - LowTriang
  _t6_48 = _t6_20;
  _t6_49 = _t6_21;
  _t6_50 = _t6_22;
  _t6_51 = _t6_23;

  // 4-BLAC: 4x4 - 4x4
  _t6_48 = _mm256_sub_pd(_t6_48, _t6_36);
  _t6_49 = _mm256_sub_pd(_t6_49, _t6_37);
  _t6_50 = _mm256_sub_pd(_t6_50, _t6_38);
  _t6_51 = _mm256_sub_pd(_t6_51, _t6_39);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t6_20 = _t6_48;
  _t6_21 = _t6_49;
  _t6_22 = _t6_50;
  _t6_23 = _t6_51;

  // Generating : L[44,44] = S(h(1, 44, fi971), Sqrt( G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_52 = _t6_20;

  // 4-BLAC: sqrt(1x4)
  _t6_53 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_52)));

  // AVX Storer:
  _t6_20 = _t6_53;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t6_54 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_55 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t6_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_54), _mm256_castpd256_pd128(_t6_55)));

  // AVX Storer:
  _t6_24 = _t6_56;

  // Generating : L[44,44] = S(h(3, 44, fi971 + 1), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, fi971)) Kro G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_24, _t6_24, 32), _mm256_permute2f128_pd(_t6_24, _t6_24, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t6_58 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_21, _t6_22), _mm256_unpacklo_pd(_t6_23, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t6_59 = _mm256_mul_pd(_t6_57, _t6_58);

  // AVX Storer:
  _t6_25 = _t6_59;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 1), ( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) - ( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) Kro T( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_60 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_61 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_25, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_25, 1);

  // 4-BLAC: (4x1)^T
  _t6_63 = _t6_62;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_64 = _mm256_mul_pd(_t6_61, _t6_63);

  // 4-BLAC: 1x4 - 1x4
  _t6_65 = _mm256_sub_pd(_t6_60, _t6_64);

  // AVX Storer:
  _t6_26 = _t6_65;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 1), Sqrt( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_66 = _t6_26;

  // 4-BLAC: sqrt(1x4)
  _t6_67 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_66)));

  // AVX Storer:
  _t6_26 = _t6_67;

  // Generating : L[44,44] = S(h(2, 44, fi971 + 2), ( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) - ( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971)) Kro T( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 2x1 -> 4x1
  _t6_68 = _mm256_unpackhi_pd(_mm256_blend_pd(_t6_22, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_23, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t6_69 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_25, 2), _mm256_permute2f128_pd(_t6_25, _t6_25, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_70 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_25, _t6_25, 32), _mm256_permute2f128_pd(_t6_25, _t6_25, 32), 0);

  // 4-BLAC: (4x1)^T
  _t6_71 = _t6_70;

  // 4-BLAC: 4x1 Kro 1x4
  _t6_72 = _mm256_mul_pd(_t6_69, _t6_71);

  // 4-BLAC: 4x1 - 4x1
  _t6_73 = _mm256_sub_pd(_t6_68, _t6_72);

  // AVX Storer:
  _t6_27 = _t6_73;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t6_74 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_75 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t6_76 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_74), _mm256_castpd256_pd128(_t6_75)));

  // AVX Storer:
  _t6_28 = _t6_76;

  // Generating : L[44,44] = S(h(2, 44, fi971 + 2), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, fi971 + 1)) Kro G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_77 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_28, _t6_28, 32), _mm256_permute2f128_pd(_t6_28, _t6_28, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t6_78 = _t6_27;

  // 4-BLAC: 1x4 Kro 4x1
  _t6_79 = _mm256_mul_pd(_t6_77, _t6_78);

  // AVX Storer:
  _t6_27 = _t6_79;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 2), ( G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) - ( G(h(1, 44, fi971 + 2), L[44,44],h(2, 44, fi971)) * T( G(h(1, 44, fi971 + 2), L[44,44],h(2, 44, fi971)) ) ) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_80 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_22, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_22, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_81 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_25, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_27, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_82 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_25, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_27, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t6_83 = _t6_82;

  // 4-BLAC: 1x4 * 4x1
  _t6_84 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_mul_pd(_t6_81, _t6_83), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_mul_pd(_t6_81, _t6_83), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_81, _t6_83), _mm256_mul_pd(_t6_81, _t6_83), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_85 = _mm256_sub_pd(_t6_80, _t6_84);

  // AVX Storer:
  _t6_29 = _t6_85;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 2), Sqrt( G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_86 = _t6_29;

  // 4-BLAC: sqrt(1x4)
  _t6_87 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_86)));

  // AVX Storer:
  _t6_29 = _t6_87;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) - ( G(h(1, 44, fi971 + 3), L[44,44],h(2, 44, fi971)) * T( G(h(1, 44, fi971 + 2), L[44,44],h(2, 44, fi971)) ) ) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_88 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_23, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_23, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_89 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_25, _t6_25, 129), _t6_27, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_90 = _mm256_shuffle_pd(_mm256_blend_pd(_t6_25, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t6_27, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t6_91 = _t6_90;

  // 4-BLAC: 1x4 * 4x1
  _t6_92 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_mul_pd(_t6_89, _t6_91), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_mul_pd(_t6_89, _t6_91), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_89, _t6_91), _mm256_mul_pd(_t6_89, _t6_91), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_93 = _mm256_sub_pd(_t6_88, _t6_92);

  // AVX Storer:
  _t6_30 = _t6_93;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_94 = _t6_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_95 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t6_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_94), _mm256_castpd256_pd128(_t6_95)));

  // AVX Storer:
  _t6_30 = _t6_96;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi971 + 3), L[44,44],h(3, 44, fi971)) * T( G(h(1, 44, fi971 + 3), L[44,44],h(3, 44, fi971)) ) ) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_97 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_23, _t6_23, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_98 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_25, _t6_30, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t6_27, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_99 = _mm256_blend_pd(_mm256_permute2f128_pd(_t6_25, _t6_30, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t6_27, 2), 10);

  // 4-BLAC: (1x4)^T
  _t6_100 = _t6_99;

  // 4-BLAC: 1x4 * 4x1
  _t6_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_mul_pd(_t6_98, _t6_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_mul_pd(_t6_98, _t6_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t6_98, _t6_100), _mm256_mul_pd(_t6_98, _t6_100), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t6_102 = _mm256_sub_pd(_t6_97, _t6_101);

  // AVX Storer:
  _t6_31 = _t6_102;

  // Generating : L[44,44] = S(h(1, 44, fi971 + 3), Sqrt( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_103 = _t6_31;

  // 4-BLAC: sqrt(1x4)
  _t6_104 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t6_103)));

  // AVX Storer:
  _t6_31 = _t6_104;

  // Generating : L[44,44] = ( Sum_{k207} ( S(h(4, 44, fi971 + k207 + 4), ( G(h(4, 44, fi971 + k207 + 4), K[44,44],h(4, 44, fi971)) - ( G(h(4, 44, fi971 + k207 + 4), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, fi971)) ) + Sum_{k159} ( Sum_{k207} ( -$(h(4, 44, fi971 + k207 + 4), ( G(h(4, 44, fi971 + k207 + 4), L[44,44],h(4, 44, k159)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, k159)) ) ),h(4, 44, fi971)) ) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
  _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
  _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
  _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);


  for( int k207 = 0; k207 <= 31; k207+=4 ) {
    _t7_20 = _mm256_loadu_pd(K + 44*k207 + 536);
    _t7_21 = _mm256_loadu_pd(K + 44*k207 + 580);
    _t7_22 = _mm256_loadu_pd(K + 44*k207 + 624);
    _t7_23 = _mm256_loadu_pd(K + 44*k207 + 668);
    _t7_15 = _mm256_broadcast_sd(K + 44*k207 + 528);
    _t7_14 = _mm256_broadcast_sd(K + 44*k207 + 529);
    _t7_13 = _mm256_broadcast_sd(K + 44*k207 + 530);
    _t7_12 = _mm256_broadcast_sd(K + 44*k207 + 531);
    _t7_11 = _mm256_broadcast_sd(K + 44*k207 + 572);
    _t7_10 = _mm256_broadcast_sd(K + 44*k207 + 573);
    _t7_9 = _mm256_broadcast_sd(K + 44*k207 + 574);
    _t7_8 = _mm256_broadcast_sd(K + 44*k207 + 575);
    _t7_7 = _mm256_broadcast_sd(K + 44*k207 + 616);
    _t7_6 = _mm256_broadcast_sd(K + 44*k207 + 617);
    _t7_5 = _mm256_broadcast_sd(K + 44*k207 + 618);
    _t7_4 = _mm256_broadcast_sd(K + 44*k207 + 619);
    _t7_3 = _mm256_broadcast_sd(K + 44*k207 + 660);
    _t7_2 = _mm256_broadcast_sd(K + 44*k207 + 661);
    _t7_1 = _mm256_broadcast_sd(K + 44*k207 + 662);
    _t7_0 = _mm256_broadcast_sd(K + 44*k207 + 663);

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
    _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
    _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
    _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t7_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t6_113), _mm256_mul_pd(_t7_14, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t6_115), _mm256_mul_pd(_t7_12, _t6_116)));
    _t7_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t6_113), _mm256_mul_pd(_t7_10, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t6_115), _mm256_mul_pd(_t7_8, _t6_116)));
    _t7_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t6_113), _mm256_mul_pd(_t7_6, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t6_115), _mm256_mul_pd(_t7_4, _t6_116)));
    _t7_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_3, _t6_113), _mm256_mul_pd(_t7_2, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t7_1, _t6_115), _mm256_mul_pd(_t7_0, _t6_116)));

    // 4-BLAC: 4x4 - 4x4
    _t7_20 = _mm256_sub_pd(_t7_20, _t7_16);
    _t7_21 = _mm256_sub_pd(_t7_21, _t7_17);
    _t7_22 = _mm256_sub_pd(_t7_22, _t7_18);
    _t7_23 = _mm256_sub_pd(_t7_23, _t7_19);

    // AVX Storer:
    _mm256_storeu_pd(K + 44*k207 + 536, _t7_20);
    _mm256_storeu_pd(K + 44*k207 + 580, _t7_21);
    _mm256_storeu_pd(K + 44*k207 + 624, _t7_22);
    _mm256_storeu_pd(K + 44*k207 + 668, _t7_23);
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
  _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);
  _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);

  _mm256_maskstore_pd(K + 357, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_1);
  _mm256_maskstore_pd(K + 358, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_3);
  _mm256_maskstore_pd(K + 401, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_7);
  _mm256_maskstore_pd(K + 402, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_9);
  _mm256_maskstore_pd(K + 445, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_13);
  _mm256_maskstore_pd(K + 446, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_15);
  _mm256_maskstore_pd(K + 489, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t4_19);
  _mm256_maskstore_pd(K + 490, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t4_21);

  for( int k207 = 0; k207 <= 31; k207+=4 ) {
    _t9_15 = _mm256_broadcast_sd(K + 44*k207 + 532);
    _t9_14 = _mm256_broadcast_sd(K + 44*k207 + 533);
    _t9_13 = _mm256_broadcast_sd(K + 44*k207 + 534);
    _t9_12 = _mm256_broadcast_sd(K + 44*k207 + 535);
    _t9_11 = _mm256_broadcast_sd(K + 44*k207 + 576);
    _t9_10 = _mm256_broadcast_sd(K + 44*k207 + 577);
    _t9_9 = _mm256_broadcast_sd(K + 44*k207 + 578);
    _t9_8 = _mm256_broadcast_sd(K + 44*k207 + 579);
    _t9_7 = _mm256_broadcast_sd(K + 44*k207 + 620);
    _t9_6 = _mm256_broadcast_sd(K + 44*k207 + 621);
    _t9_5 = _mm256_broadcast_sd(K + 44*k207 + 622);
    _t9_4 = _mm256_broadcast_sd(K + 44*k207 + 623);
    _t9_3 = _mm256_broadcast_sd(K + 44*k207 + 664);
    _t9_2 = _mm256_broadcast_sd(K + 44*k207 + 665);
    _t9_1 = _mm256_broadcast_sd(K + 44*k207 + 666);
    _t9_0 = _mm256_broadcast_sd(K + 44*k207 + 667);
    _t9_16 = _mm256_loadu_pd(K + 44*k207 + 536);
    _t9_17 = _mm256_loadu_pd(K + 44*k207 + 580);
    _t9_18 = _mm256_loadu_pd(K + 44*k207 + 624);
    _t9_19 = _mm256_loadu_pd(K + 44*k207 + 668);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
    _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 32);
    _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpacklo_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);
    _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_0, _t4_2), _mm256_unpacklo_pd(_t4_4, _t4_5), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_6, _t4_8), _mm256_unpacklo_pd(_t4_10, _t4_11), 32)), _mm256_unpackhi_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_12, _t4_14), _mm256_unpacklo_pd(_t4_16, _t4_17), 32), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_18, _t4_20), _mm256_unpacklo_pd(_t4_22, _t4_23), 32)), 49);

    // 4-BLAC: 4x4 * 4x4
    _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t8_0), _mm256_mul_pd(_t9_14, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t8_2), _mm256_mul_pd(_t9_12, _t8_3)));
    _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t8_0), _mm256_mul_pd(_t9_10, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t8_2), _mm256_mul_pd(_t9_8, _t8_3)));
    _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t8_0), _mm256_mul_pd(_t9_6, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t8_2), _mm256_mul_pd(_t9_4, _t8_3)));
    _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_3, _t8_0), _mm256_mul_pd(_t9_2, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t9_1, _t8_2), _mm256_mul_pd(_t9_0, _t8_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t9_16 = _mm256_sub_pd(_t9_16, _t9_20);
    _t9_17 = _mm256_sub_pd(_t9_17, _t9_21);
    _t9_18 = _mm256_sub_pd(_t9_18, _t9_22);
    _t9_19 = _mm256_sub_pd(_t9_19, _t9_23);

    // AVX Storer:
    _mm256_storeu_pd(K + 44*k207 + 536, _t9_16);
    _mm256_storeu_pd(K + 44*k207 + 580, _t9_17);
    _mm256_storeu_pd(K + 44*k207 + 624, _t9_18);
    _mm256_storeu_pd(K + 44*k207 + 668, _t9_19);
  }

  _t10_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[536])));
  _t10_1 = _mm256_maskload_pd(K + 537, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[580])));
  _t10_7 = _mm256_maskload_pd(K + 581, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[624])));
  _t10_13 = _mm256_maskload_pd(K + 625, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t10_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[668])));
  _t10_19 = _mm256_maskload_pd(K + 669, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_24 = _t10_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_25 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_24), _mm256_castpd256_pd128(_t10_25)));

  // AVX Storer:
  _t10_0 = _t10_26;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_27 = _t10_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_0, _t10_0, 32), _mm256_permute2f128_pd(_t10_0, _t10_0, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_29 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_30 = _t10_29;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_31 = _mm256_mul_pd(_t10_28, _t10_30);

  // 4-BLAC: 1x4 - 1x4
  _t10_32 = _mm256_sub_pd(_t10_27, _t10_31);

  // AVX Storer:
  _t10_1 = _t10_32;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_33 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_34 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_33), _mm256_castpd256_pd128(_t10_34)));

  // AVX Storer:
  _t10_2 = _t10_35;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_36 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_1, 6), _mm256_permute2f128_pd(_t10_1, _t10_1, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_37 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_2, _t10_2, 32), _mm256_permute2f128_pd(_t10_2, _t10_2, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_38 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_39 = _t10_38;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_40 = _mm256_mul_pd(_t10_37, _t10_39);

  // 4-BLAC: 1x4 - 1x4
  _t10_41 = _mm256_sub_pd(_t10_36, _t10_40);

  // AVX Storer:
  _t10_3 = _t10_41;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_42 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_43 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_44 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_42), _mm256_castpd256_pd128(_t10_43)));

  // AVX Storer:
  _t10_4 = _t10_44;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_45 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_46 = _t10_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_47 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_48 = _t10_47;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_49 = _mm256_mul_pd(_t10_46, _t10_48);

  // 4-BLAC: 1x4 - 1x4
  _t10_50 = _mm256_sub_pd(_t10_45, _t10_49);

  // AVX Storer:
  _t10_5 = _t10_50;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_51 = _t10_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_52 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_51), _mm256_castpd256_pd128(_t10_52)));

  // AVX Storer:
  _t10_5 = _t10_53;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_54 = _t10_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_55 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_54), _mm256_castpd256_pd128(_t10_55)));

  // AVX Storer:
  _t10_6 = _t10_56;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_57 = _t10_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_6, _t10_6, 32), _mm256_permute2f128_pd(_t10_6, _t10_6, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_59 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_60 = _t10_59;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_61 = _mm256_mul_pd(_t10_58, _t10_60);

  // 4-BLAC: 1x4 - 1x4
  _t10_62 = _mm256_sub_pd(_t10_57, _t10_61);

  // AVX Storer:
  _t10_7 = _t10_62;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_63 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_64 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_65 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_63), _mm256_castpd256_pd128(_t10_64)));

  // AVX Storer:
  _t10_8 = _t10_65;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_66 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_7, 6), _mm256_permute2f128_pd(_t10_7, _t10_7, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_67 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_8, _t10_8, 32), _mm256_permute2f128_pd(_t10_8, _t10_8, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_68 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_69 = _t10_68;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_70 = _mm256_mul_pd(_t10_67, _t10_69);

  // 4-BLAC: 1x4 - 1x4
  _t10_71 = _mm256_sub_pd(_t10_66, _t10_70);

  // AVX Storer:
  _t10_9 = _t10_71;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_72 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_9, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_73 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_72), _mm256_castpd256_pd128(_t10_73)));

  // AVX Storer:
  _t10_10 = _t10_74;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_75 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_76 = _t10_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_77 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_78 = _t10_77;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_79 = _mm256_mul_pd(_t10_76, _t10_78);

  // 4-BLAC: 1x4 - 1x4
  _t10_80 = _mm256_sub_pd(_t10_75, _t10_79);

  // AVX Storer:
  _t10_11 = _t10_80;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_81 = _t10_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_82 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_83 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_81), _mm256_castpd256_pd128(_t10_82)));

  // AVX Storer:
  _t10_11 = _t10_83;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_84 = _t10_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_85 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_86 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_84), _mm256_castpd256_pd128(_t10_85)));

  // AVX Storer:
  _t10_12 = _t10_86;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_87 = _t10_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_88 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_12, _t10_12, 32), _mm256_permute2f128_pd(_t10_12, _t10_12, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_89 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_90 = _t10_89;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_91 = _mm256_mul_pd(_t10_88, _t10_90);

  // 4-BLAC: 1x4 - 1x4
  _t10_92 = _mm256_sub_pd(_t10_87, _t10_91);

  // AVX Storer:
  _t10_13 = _t10_92;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_13, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_94 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_93), _mm256_castpd256_pd128(_t10_94)));

  // AVX Storer:
  _t10_14 = _t10_95;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_96 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_13, 6), _mm256_permute2f128_pd(_t10_13, _t10_13, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_97 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_14, _t10_14, 32), _mm256_permute2f128_pd(_t10_14, _t10_14, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_98 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_99 = _t10_98;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_100 = _mm256_mul_pd(_t10_97, _t10_99);

  // 4-BLAC: 1x4 - 1x4
  _t10_101 = _mm256_sub_pd(_t10_96, _t10_100);

  // AVX Storer:
  _t10_15 = _t10_101;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_102 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_103 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_102), _mm256_castpd256_pd128(_t10_103)));

  // AVX Storer:
  _t10_16 = _t10_104;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_15, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_106 = _t10_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_107 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_108 = _t10_107;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_109 = _mm256_mul_pd(_t10_106, _t10_108);

  // 4-BLAC: 1x4 - 1x4
  _t10_110 = _mm256_sub_pd(_t10_105, _t10_109);

  // AVX Storer:
  _t10_17 = _t10_110;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_111 = _t10_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_112 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_113 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_111), _mm256_castpd256_pd128(_t10_112)));

  // AVX Storer:
  _t10_17 = _t10_113;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_114 = _t10_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_115 = _t6_20;

  // 4-BLAC: 1x4 / 1x4
  _t10_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_114), _mm256_castpd256_pd128(_t10_115)));

  // AVX Storer:
  _t10_18 = _t10_116;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

  // AVX Loader:

  // 1x3 -> 1x4
  _t10_117 = _t10_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_118 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_18, _t10_18, 32), _mm256_permute2f128_pd(_t10_18, _t10_18, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t10_119 = _t6_25;

  // 4-BLAC: (4x1)^T
  _t10_120 = _t10_119;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_121 = _mm256_mul_pd(_t10_118, _t10_120);

  // 4-BLAC: 1x4 - 1x4
  _t10_122 = _mm256_sub_pd(_t10_117, _t10_121);

  // AVX Storer:
  _t10_19 = _t10_122;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_19, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_124 = _t6_26;

  // 4-BLAC: 1x4 / 1x4
  _t10_125 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_123), _mm256_castpd256_pd128(_t10_124)));

  // AVX Storer:
  _t10_20 = _t10_125;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t10_126 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_19, 6), _mm256_permute2f128_pd(_t10_19, _t10_19, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_127 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_20, _t10_20, 32), _mm256_permute2f128_pd(_t10_20, _t10_20, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t10_128 = _t6_27;

  // 4-BLAC: (4x1)^T
  _t10_129 = _t10_128;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_130 = _mm256_mul_pd(_t10_127, _t10_129);

  // 4-BLAC: 1x4 - 1x4
  _t10_131 = _mm256_sub_pd(_t10_126, _t10_130);

  // AVX Storer:
  _t10_21 = _t10_131;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_132 = _mm256_blend_pd(_mm256_setzero_pd(), _t10_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_133 = _t6_29;

  // 4-BLAC: 1x4 / 1x4
  _t10_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_132), _mm256_castpd256_pd128(_t10_133)));

  // AVX Storer:
  _t10_22 = _t10_134;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_135 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t10_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_136 = _t10_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_137 = _t6_30;

  // 4-BLAC: (4x1)^T
  _t10_138 = _t10_137;

  // 4-BLAC: 1x4 Kro 1x4
  _t10_139 = _mm256_mul_pd(_t10_136, _t10_138);

  // 4-BLAC: 1x4 - 1x4
  _t10_140 = _mm256_sub_pd(_t10_135, _t10_139);

  // AVX Storer:
  _t10_23 = _t10_140;

  // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_141 = _t10_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t10_142 = _t6_31;

  // 4-BLAC: 1x4 / 1x4
  _t10_143 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t10_141), _mm256_castpd256_pd128(_t10_142)));

  // AVX Storer:
  _t10_23 = _t10_143;

  _mm256_maskstore_pd(K + 404, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t6_21);
  _mm256_maskstore_pd(K + 448, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t6_22);
  _mm256_storeu_pd(K + 492, _t6_23);

  for( int fi1090 = 4; fi1090 <= 28; fi1090+=4 ) {
    _t11_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 536])));
    _t11_1 = _mm256_maskload_pd(K + 44*fi1090 + 537, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t11_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 580])));
    _t11_7 = _mm256_maskload_pd(K + 44*fi1090 + 581, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t11_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 624])));
    _t11_13 = _mm256_maskload_pd(K + 44*fi1090 + 625, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t11_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 668])));
    _t11_19 = _mm256_maskload_pd(K + 44*fi1090 + 669, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_24 = _t11_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_25 = _t6_20;

    // 4-BLAC: 1x4 / 1x4
    _t11_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_24), _mm256_castpd256_pd128(_t11_25)));

    // AVX Storer:
    _t11_0 = _t11_26;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t11_27 = _t11_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_0, _t11_0, 32), _mm256_permute2f128_pd(_t11_0, _t11_0, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t11_29 = _t6_25;

    // 4-BLAC: (4x1)^T
    _t10_30 = _t11_29;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_31 = _mm256_mul_pd(_t11_28, _t10_30);

    // 4-BLAC: 1x4 - 1x4
    _t11_30 = _mm256_sub_pd(_t11_27, _t10_31);

    // AVX Storer:
    _t11_1 = _t11_30;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_32 = _t6_26;

    // 4-BLAC: 1x4 / 1x4
    _t11_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_31), _mm256_castpd256_pd128(_t11_32)));

    // AVX Storer:
    _t11_2 = _t11_33;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t11_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_1, 6), _mm256_permute2f128_pd(_t11_1, _t11_1, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_2, _t11_2, 32), _mm256_permute2f128_pd(_t11_2, _t11_2, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t11_36 = _t6_27;

    // 4-BLAC: (4x1)^T
    _t10_39 = _t11_36;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_40 = _mm256_mul_pd(_t11_35, _t10_39);

    // 4-BLAC: 1x4 - 1x4
    _t11_37 = _mm256_sub_pd(_t11_34, _t10_40);

    // AVX Storer:
    _t11_3 = _t11_37;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_3, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_39 = _t6_29;

    // 4-BLAC: 1x4 / 1x4
    _t11_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_38), _mm256_castpd256_pd128(_t11_39)));

    // AVX Storer:
    _t11_4 = _t11_40;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_41 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_3, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_42 = _t11_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_43 = _t6_30;

    // 4-BLAC: (4x1)^T
    _t10_48 = _t11_43;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_49 = _mm256_mul_pd(_t11_42, _t10_48);

    // 4-BLAC: 1x4 - 1x4
    _t11_44 = _mm256_sub_pd(_t11_41, _t10_49);

    // AVX Storer:
    _t11_5 = _t11_44;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_45 = _t11_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_46 = _t6_31;

    // 4-BLAC: 1x4 / 1x4
    _t11_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_45), _mm256_castpd256_pd128(_t11_46)));

    // AVX Storer:
    _t11_5 = _t11_47;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_48 = _t11_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_49 = _t6_20;

    // 4-BLAC: 1x4 / 1x4
    _t11_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_48), _mm256_castpd256_pd128(_t11_49)));

    // AVX Storer:
    _t11_6 = _t11_50;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t11_51 = _t11_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_6, _t11_6, 32), _mm256_permute2f128_pd(_t11_6, _t11_6, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t11_53 = _t6_25;

    // 4-BLAC: (4x1)^T
    _t10_60 = _t11_53;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_61 = _mm256_mul_pd(_t11_52, _t10_60);

    // 4-BLAC: 1x4 - 1x4
    _t11_54 = _mm256_sub_pd(_t11_51, _t10_61);

    // AVX Storer:
    _t11_7 = _t11_54;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_55 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_56 = _t6_26;

    // 4-BLAC: 1x4 / 1x4
    _t11_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_55), _mm256_castpd256_pd128(_t11_56)));

    // AVX Storer:
    _t11_8 = _t11_57;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t11_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_7, 6), _mm256_permute2f128_pd(_t11_7, _t11_7, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_8, _t11_8, 32), _mm256_permute2f128_pd(_t11_8, _t11_8, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t11_60 = _t6_27;

    // 4-BLAC: (4x1)^T
    _t10_69 = _t11_60;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_70 = _mm256_mul_pd(_t11_59, _t10_69);

    // 4-BLAC: 1x4 - 1x4
    _t11_61 = _mm256_sub_pd(_t11_58, _t10_70);

    // AVX Storer:
    _t11_9 = _t11_61;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_9, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_63 = _t6_29;

    // 4-BLAC: 1x4 / 1x4
    _t11_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_62), _mm256_castpd256_pd128(_t11_63)));

    // AVX Storer:
    _t11_10 = _t11_64;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_65 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_66 = _t11_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_67 = _t6_30;

    // 4-BLAC: (4x1)^T
    _t10_78 = _t11_67;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_79 = _mm256_mul_pd(_t11_66, _t10_78);

    // 4-BLAC: 1x4 - 1x4
    _t11_68 = _mm256_sub_pd(_t11_65, _t10_79);

    // AVX Storer:
    _t11_11 = _t11_68;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_69 = _t11_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_70 = _t6_31;

    // 4-BLAC: 1x4 / 1x4
    _t11_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_69), _mm256_castpd256_pd128(_t11_70)));

    // AVX Storer:
    _t11_11 = _t11_71;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_72 = _t11_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_73 = _t6_20;

    // 4-BLAC: 1x4 / 1x4
    _t11_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_72), _mm256_castpd256_pd128(_t11_73)));

    // AVX Storer:
    _t11_12 = _t11_74;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t11_75 = _t11_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_12, _t11_12, 32), _mm256_permute2f128_pd(_t11_12, _t11_12, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t11_77 = _t6_25;

    // 4-BLAC: (4x1)^T
    _t10_90 = _t11_77;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_91 = _mm256_mul_pd(_t11_76, _t10_90);

    // 4-BLAC: 1x4 - 1x4
    _t11_78 = _mm256_sub_pd(_t11_75, _t10_91);

    // AVX Storer:
    _t11_13 = _t11_78;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_13, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_80 = _t6_26;

    // 4-BLAC: 1x4 / 1x4
    _t11_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_79), _mm256_castpd256_pd128(_t11_80)));

    // AVX Storer:
    _t11_14 = _t11_81;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t11_82 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_13, 6), _mm256_permute2f128_pd(_t11_13, _t11_13, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_14, _t11_14, 32), _mm256_permute2f128_pd(_t11_14, _t11_14, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t11_84 = _t6_27;

    // 4-BLAC: (4x1)^T
    _t10_99 = _t11_84;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_100 = _mm256_mul_pd(_t11_83, _t10_99);

    // 4-BLAC: 1x4 - 1x4
    _t11_85 = _mm256_sub_pd(_t11_82, _t10_100);

    // AVX Storer:
    _t11_15 = _t11_85;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_87 = _t6_29;

    // 4-BLAC: 1x4 / 1x4
    _t11_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_86), _mm256_castpd256_pd128(_t11_87)));

    // AVX Storer:
    _t11_16 = _t11_88;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_15, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_90 = _t11_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_91 = _t6_30;

    // 4-BLAC: (4x1)^T
    _t10_108 = _t11_91;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_109 = _mm256_mul_pd(_t11_90, _t10_108);

    // 4-BLAC: 1x4 - 1x4
    _t11_92 = _mm256_sub_pd(_t11_89, _t10_109);

    // AVX Storer:
    _t11_17 = _t11_92;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_93 = _t11_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_94 = _t6_31;

    // 4-BLAC: 1x4 / 1x4
    _t11_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_93), _mm256_castpd256_pd128(_t11_94)));

    // AVX Storer:
    _t11_17 = _t11_95;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_96 = _t11_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_97 = _t6_20;

    // 4-BLAC: 1x4 / 1x4
    _t11_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_96), _mm256_castpd256_pd128(_t11_97)));

    // AVX Storer:
    _t11_18 = _t11_98;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t11_99 = _t11_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_18, _t11_18, 32), _mm256_permute2f128_pd(_t11_18, _t11_18, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t11_101 = _t6_25;

    // 4-BLAC: (4x1)^T
    _t10_120 = _t11_101;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_121 = _mm256_mul_pd(_t11_100, _t10_120);

    // 4-BLAC: 1x4 - 1x4
    _t11_102 = _mm256_sub_pd(_t11_99, _t10_121);

    // AVX Storer:
    _t11_19 = _t11_102;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_19, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_104 = _t6_26;

    // 4-BLAC: 1x4 / 1x4
    _t11_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_103), _mm256_castpd256_pd128(_t11_104)));

    // AVX Storer:
    _t11_20 = _t11_105;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t11_106 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_19, 6), _mm256_permute2f128_pd(_t11_19, _t11_19, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_20, _t11_20, 32), _mm256_permute2f128_pd(_t11_20, _t11_20, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t11_108 = _t6_27;

    // 4-BLAC: (4x1)^T
    _t10_129 = _t11_108;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_130 = _mm256_mul_pd(_t11_107, _t10_129);

    // 4-BLAC: 1x4 - 1x4
    _t11_109 = _mm256_sub_pd(_t11_106, _t10_130);

    // AVX Storer:
    _t11_21 = _t11_109;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_111 = _t6_29;

    // 4-BLAC: 1x4 / 1x4
    _t11_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_110), _mm256_castpd256_pd128(_t11_111)));

    // AVX Storer:
    _t11_22 = _t11_112;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_113 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_21, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_114 = _t11_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_115 = _t6_30;

    // 4-BLAC: (4x1)^T
    _t10_138 = _t11_115;

    // 4-BLAC: 1x4 Kro 1x4
    _t10_139 = _mm256_mul_pd(_t11_114, _t10_138);

    // 4-BLAC: 1x4 - 1x4
    _t11_116 = _mm256_sub_pd(_t11_113, _t10_139);

    // AVX Storer:
    _t11_23 = _t11_116;

    // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_117 = _t11_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t11_118 = _t6_31;

    // 4-BLAC: 1x4 / 1x4
    _t11_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_117), _mm256_castpd256_pd128(_t11_118)));

    // AVX Storer:
    _t11_23 = _t11_119;
    _mm_store_sd(&(K[44*fi1090 + 536]), _mm256_castpd256_pd128(_t11_0));
    _mm_store_sd(&(K[44*fi1090 + 537]), _mm256_castpd256_pd128(_t11_2));
    _mm_store_sd(&(K[44*fi1090 + 538]), _mm256_castpd256_pd128(_t11_4));
    _mm_store_sd(&(K[44*fi1090 + 539]), _mm256_castpd256_pd128(_t11_5));
    _mm_store_sd(&(K[44*fi1090 + 580]), _mm256_castpd256_pd128(_t11_6));
    _mm_store_sd(&(K[44*fi1090 + 581]), _mm256_castpd256_pd128(_t11_8));
    _mm_store_sd(&(K[44*fi1090 + 582]), _mm256_castpd256_pd128(_t11_10));
    _mm_store_sd(&(K[44*fi1090 + 583]), _mm256_castpd256_pd128(_t11_11));
    _mm_store_sd(&(K[44*fi1090 + 624]), _mm256_castpd256_pd128(_t11_12));
    _mm_store_sd(&(K[44*fi1090 + 625]), _mm256_castpd256_pd128(_t11_14));
    _mm_store_sd(&(K[44*fi1090 + 626]), _mm256_castpd256_pd128(_t11_16));
    _mm_store_sd(&(K[44*fi1090 + 627]), _mm256_castpd256_pd128(_t11_17));
    _mm_store_sd(&(K[44*fi1090 + 668]), _mm256_castpd256_pd128(_t11_18));
    _mm_store_sd(&(K[44*fi1090 + 669]), _mm256_castpd256_pd128(_t11_20));
    _mm_store_sd(&(K[44*fi1090 + 670]), _mm256_castpd256_pd128(_t11_22));
    _mm_store_sd(&(K[44*fi1090 + 671]), _mm256_castpd256_pd128(_t11_23));
  }

  _mm_store_sd(&(K[536]), _mm256_castpd256_pd128(_t10_0));
  _mm_store_sd(&(K[537]), _mm256_castpd256_pd128(_t10_2));
  _mm_store_sd(&(K[538]), _mm256_castpd256_pd128(_t10_4));
  _mm_store_sd(&(K[539]), _mm256_castpd256_pd128(_t10_5));
  _mm_store_sd(&(K[580]), _mm256_castpd256_pd128(_t10_6));
  _mm_store_sd(&(K[581]), _mm256_castpd256_pd128(_t10_8));
  _mm_store_sd(&(K[582]), _mm256_castpd256_pd128(_t10_10));
  _mm_store_sd(&(K[583]), _mm256_castpd256_pd128(_t10_11));
  _mm_store_sd(&(K[624]), _mm256_castpd256_pd128(_t10_12));
  _mm_store_sd(&(K[625]), _mm256_castpd256_pd128(_t10_14));
  _mm_store_sd(&(K[626]), _mm256_castpd256_pd128(_t10_16));
  _mm_store_sd(&(K[627]), _mm256_castpd256_pd128(_t10_17));
  _mm_store_sd(&(K[668]), _mm256_castpd256_pd128(_t10_18));
  _mm_store_sd(&(K[669]), _mm256_castpd256_pd128(_t10_20));
  _mm_store_sd(&(K[670]), _mm256_castpd256_pd128(_t10_22));
  _mm_store_sd(&(K[671]), _mm256_castpd256_pd128(_t10_23));

  for( int fi971 = 12; fi971 <= 36; fi971+=4 ) {
    _t12_20 = _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971));
    _t12_21 = _mm256_maskload_pd(K + 45*fi971 + 44, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t12_22 = _mm256_maskload_pd(K + 45*fi971 + 88, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t12_23 = _mm256_loadu_pd(K + 45*fi971 + 132);
    _t12_19 = _mm256_broadcast_sd(K + 44*fi971);
    _t12_18 = _mm256_broadcast_sd(K + 44*fi971 + 1);
    _t12_17 = _mm256_broadcast_sd(K + 44*fi971 + 2);
    _t12_16 = _mm256_broadcast_sd(K + 44*fi971 + 3);
    _t12_15 = _mm256_broadcast_sd(K + 44*fi971 + 44);
    _t12_14 = _mm256_broadcast_sd(K + 44*fi971 + 45);
    _t12_13 = _mm256_broadcast_sd(K + 44*fi971 + 46);
    _t12_12 = _mm256_broadcast_sd(K + 44*fi971 + 47);
    _t12_11 = _mm256_broadcast_sd(K + 44*fi971 + 88);
    _t12_10 = _mm256_broadcast_sd(K + 44*fi971 + 89);
    _t12_9 = _mm256_broadcast_sd(K + 44*fi971 + 90);
    _t12_8 = _mm256_broadcast_sd(K + 44*fi971 + 91);
    _t12_7 = _mm256_broadcast_sd(K + 44*fi971 + 132);
    _t12_6 = _mm256_broadcast_sd(K + 44*fi971 + 133);
    _t12_5 = _mm256_broadcast_sd(K + 44*fi971 + 134);
    _t12_4 = _mm256_broadcast_sd(K + 44*fi971 + 135);
    _t12_3 = _mm256_loadu_pd(K + 44*fi971);
    _t12_2 = _mm256_loadu_pd(K + 44*fi971 + 44);
    _t12_1 = _mm256_loadu_pd(K + 44*fi971 + 88);
    _t12_0 = _mm256_loadu_pd(K + 44*fi971 + 132);

    // Generating : L[44,44] = ( S(h(4, 44, fi971), ( G(h(4, 44, fi971), K[44,44],h(4, 44, fi971)) - ( G(h(4, 44, fi971), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, fi971)) + Sum_{k159} ( -$(h(4, 44, fi971), ( G(h(4, 44, fi971), L[44,44],h(4, 44, k159)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, k159)) ) ),h(4, 44, fi971)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - LowSymm
    _t12_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t12_20, _t12_21, 0), _mm256_shuffle_pd(_t12_22, _t12_23, 0), 32);
    _t12_25 = _mm256_permute2f128_pd(_t12_21, _mm256_shuffle_pd(_t12_22, _t12_23, 3), 32);
    _t12_26 = _mm256_blend_pd(_t12_22, _mm256_shuffle_pd(_t12_22, _t12_23, 3), 12);
    _t12_27 = _t12_23;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t6_105 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 32);
    _t6_106 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 32);
    _t6_107 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 49);
    _t6_108 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t6_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_19, _t6_105), _mm256_mul_pd(_t12_18, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t12_17, _t6_107), _mm256_mul_pd(_t12_16, _t6_108)));
    _t6_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t6_105), _mm256_mul_pd(_t12_14, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t6_107), _mm256_mul_pd(_t12_12, _t6_108)));
    _t6_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t6_105), _mm256_mul_pd(_t12_10, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t6_107), _mm256_mul_pd(_t12_8, _t6_108)));
    _t6_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t6_105), _mm256_mul_pd(_t12_6, _t6_106)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t6_107), _mm256_mul_pd(_t12_4, _t6_108)));

    // 4-BLAC: 4x4 - 4x4
    _t6_40 = _mm256_sub_pd(_t12_24, _t6_32);
    _t6_41 = _mm256_sub_pd(_t12_25, _t6_33);
    _t6_42 = _mm256_sub_pd(_t12_26, _t6_34);
    _t6_43 = _mm256_sub_pd(_t12_27, _t6_35);

    // AVX Storer:

    // 4x4 -> 4x4 - LowTriang
    _t12_20 = _t6_40;
    _t12_21 = _t6_41;
    _t12_22 = _t6_42;
    _t12_23 = _t6_43;

    for( int k159 = 4; k159 <= fi971 - 1; k159+=4 ) {
      _t13_19 = _mm256_broadcast_sd(K + 44*fi971 + k159);
      _t13_18 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 1);
      _t13_17 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 2);
      _t13_16 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 3);
      _t13_15 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44);
      _t13_14 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 45);
      _t13_13 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 46);
      _t13_12 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 47);
      _t13_11 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 88);
      _t13_10 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 89);
      _t13_9 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 90);
      _t13_8 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 91);
      _t13_7 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 132);
      _t13_6 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 133);
      _t13_5 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 134);
      _t13_4 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 135);
      _t13_3 = _mm256_loadu_pd(K + 44*fi971 + k159);
      _t13_2 = _mm256_loadu_pd(K + 44*fi971 + k159 + 44);
      _t13_1 = _mm256_loadu_pd(K + 44*fi971 + k159 + 88);
      _t13_0 = _mm256_loadu_pd(K + 44*fi971 + k159 + 132);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t6_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 32);
      _t6_110 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 32);
      _t6_111 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_3, _t13_2), _mm256_unpacklo_pd(_t13_1, _t13_0), 49);
      _t6_112 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_3, _t13_2), _mm256_unpackhi_pd(_t13_1, _t13_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t6_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t6_109), _mm256_mul_pd(_t13_18, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t6_111), _mm256_mul_pd(_t13_16, _t6_112)));
      _t6_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t6_109), _mm256_mul_pd(_t13_14, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t6_111), _mm256_mul_pd(_t13_12, _t6_112)));
      _t6_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t6_109), _mm256_mul_pd(_t13_10, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t6_111), _mm256_mul_pd(_t13_8, _t6_112)));
      _t6_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t6_109), _mm256_mul_pd(_t13_6, _t6_110)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t6_111), _mm256_mul_pd(_t13_4, _t6_112)));

      // AVX Loader:

      // 4x4 -> 4x4 - LowTriang
      _t13_20 = _t12_20;
      _t13_21 = _t12_21;
      _t13_22 = _t12_22;
      _t13_23 = _t12_23;

      // 4-BLAC: 4x4 - 4x4
      _t13_20 = _mm256_sub_pd(_t13_20, _t6_36);
      _t13_21 = _mm256_sub_pd(_t13_21, _t6_37);
      _t13_22 = _mm256_sub_pd(_t13_22, _t6_38);
      _t13_23 = _mm256_sub_pd(_t13_23, _t6_39);

      // AVX Storer:

      // 4x4 -> 4x4 - LowTriang
      _t12_20 = _t13_20;
      _t12_21 = _t13_21;
      _t12_22 = _t13_22;
      _t12_23 = _t13_23;
    }

    // Generating : L[44,44] = S(h(1, 44, fi971), Sqrt( G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_8 = _t12_20;

    // 4-BLAC: sqrt(1x4)
    _t14_9 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t14_8)));

    // AVX Storer:
    _t12_20 = _t14_9;

    // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t14_10 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_11 = _t12_20;

    // 4-BLAC: 1x4 / 1x4
    _t14_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_10), _mm256_castpd256_pd128(_t14_11)));

    // AVX Storer:
    _t14_0 = _t14_12;

    // Generating : L[44,44] = S(h(3, 44, fi971 + 1), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, fi971)) Kro G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_0, _t14_0, 32), _mm256_permute2f128_pd(_t14_0, _t14_0, 32), 0);

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_21, _t12_22), _mm256_unpacklo_pd(_t12_23, _mm256_setzero_pd()), 32);

    // 4-BLAC: 1x4 Kro 4x1
    _t14_15 = _mm256_mul_pd(_t14_13, _t14_14);

    // AVX Storer:
    _t14_1 = _t14_15;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 1), ( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) - ( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) Kro T( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_16 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_21, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_1, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_1, 1);

    // 4-BLAC: (4x1)^T
    _t6_63 = _t14_18;

    // 4-BLAC: 1x4 Kro 1x4
    _t6_64 = _mm256_mul_pd(_t14_17, _t6_63);

    // 4-BLAC: 1x4 - 1x4
    _t14_19 = _mm256_sub_pd(_t14_16, _t6_64);

    // AVX Storer:
    _t14_2 = _t14_19;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 1), Sqrt( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_20 = _t14_2;

    // 4-BLAC: sqrt(1x4)
    _t14_21 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t14_20)));

    // AVX Storer:
    _t14_2 = _t14_21;

    // Generating : L[44,44] = S(h(2, 44, fi971 + 2), ( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) - ( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971)) Kro T( G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_22 = _mm256_unpackhi_pd(_mm256_blend_pd(_t12_22, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t12_23, _mm256_setzero_pd(), 12));

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_23 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_1, 2), _mm256_permute2f128_pd(_t14_1, _t14_1, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_1, _t14_1, 32), _mm256_permute2f128_pd(_t14_1, _t14_1, 32), 0);

    // 4-BLAC: (4x1)^T
    _t6_71 = _t14_24;

    // 4-BLAC: 4x1 Kro 1x4
    _t6_72 = _mm256_mul_pd(_t14_23, _t6_71);

    // 4-BLAC: 4x1 - 4x1
    _t14_25 = _mm256_sub_pd(_t14_22, _t6_72);

    // AVX Storer:
    _t14_3 = _t14_25;

    // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t14_26 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_27 = _t14_2;

    // 4-BLAC: 1x4 / 1x4
    _t14_28 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_26), _mm256_castpd256_pd128(_t14_27)));

    // AVX Storer:
    _t14_4 = _t14_28;

    // Generating : L[44,44] = S(h(2, 44, fi971 + 2), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, fi971 + 1)) Kro G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_29 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_4, _t14_4, 32), _mm256_permute2f128_pd(_t14_4, _t14_4, 32), 0);

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_30 = _t14_3;

    // 4-BLAC: 1x4 Kro 4x1
    _t14_31 = _mm256_mul_pd(_t14_29, _t14_30);

    // AVX Storer:
    _t14_3 = _t14_31;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 2), ( G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) - ( G(h(1, 44, fi971 + 2), L[44,44],h(2, 44, fi971)) * T( G(h(1, 44, fi971 + 2), L[44,44],h(2, 44, fi971)) ) ) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_32 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_22, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_22, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_33 = _mm256_shuffle_pd(_mm256_blend_pd(_t14_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t14_3, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t14_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t14_3, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (1x4)^T
    _t6_83 = _t14_34;

    // 4-BLAC: 1x4 * 4x1
    _t6_84 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_33, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_33, _t6_83), _mm256_mul_pd(_t14_33, _t6_83), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_33, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_33, _t6_83), _mm256_mul_pd(_t14_33, _t6_83), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_33, _t6_83), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_33, _t6_83), _mm256_mul_pd(_t14_33, _t6_83), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_35 = _mm256_sub_pd(_t14_32, _t6_84);

    // AVX Storer:
    _t14_5 = _t14_35;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 2), Sqrt( G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_36 = _t14_5;

    // 4-BLAC: sqrt(1x4)
    _t14_37 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t14_36)));

    // AVX Storer:
    _t14_5 = _t14_37;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) - ( G(h(1, 44, fi971 + 3), L[44,44],h(2, 44, fi971)) * T( G(h(1, 44, fi971 + 2), L[44,44],h(2, 44, fi971)) ) ) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_38 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_23, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_23, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_39 = _mm256_blend_pd(_mm256_permute2f128_pd(_t14_1, _t14_1, 129), _t14_3, 2);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t14_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t14_3, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (1x4)^T
    _t6_91 = _t14_40;

    // 4-BLAC: 1x4 * 4x1
    _t6_92 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_39, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_39, _t6_91), _mm256_mul_pd(_t14_39, _t6_91), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_39, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_39, _t6_91), _mm256_mul_pd(_t14_39, _t6_91), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_39, _t6_91), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_39, _t6_91), _mm256_mul_pd(_t14_39, _t6_91), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_41 = _mm256_sub_pd(_t14_38, _t6_92);

    // AVX Storer:
    _t14_6 = _t14_41;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_42 = _t14_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_43 = _t14_5;

    // 4-BLAC: 1x4 / 1x4
    _t14_44 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_42), _mm256_castpd256_pd128(_t14_43)));

    // AVX Storer:
    _t14_6 = _t14_44;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi971 + 3), L[44,44],h(3, 44, fi971)) * T( G(h(1, 44, fi971 + 3), L[44,44],h(3, 44, fi971)) ) ) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_45 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_23, _t12_23, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_46 = _mm256_blend_pd(_mm256_permute2f128_pd(_t14_1, _t14_6, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t14_3, 2), 10);

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_47 = _mm256_blend_pd(_mm256_permute2f128_pd(_t14_1, _t14_6, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t14_3, 2), 10);

    // 4-BLAC: (1x4)^T
    _t6_100 = _t14_47;

    // 4-BLAC: 1x4 * 4x1
    _t6_101 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_46, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_46, _t6_100), _mm256_mul_pd(_t14_46, _t6_100), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_46, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_46, _t6_100), _mm256_mul_pd(_t14_46, _t6_100), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_46, _t6_100), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_46, _t6_100), _mm256_mul_pd(_t14_46, _t6_100), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_48 = _mm256_sub_pd(_t14_45, _t6_101);

    // AVX Storer:
    _t14_7 = _t14_48;

    // Generating : L[44,44] = S(h(1, 44, fi971 + 3), Sqrt( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_49 = _t14_7;

    // 4-BLAC: sqrt(1x4)
    _t14_50 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t14_49)));

    // AVX Storer:
    _t14_7 = _t14_50;

    // Generating : L[44,44] = ( Sum_{k207} ( S(h(4, 44, fi971 + k207 + 4), ( G(h(4, 44, fi971 + k207 + 4), K[44,44],h(4, 44, fi971)) - ( G(h(4, 44, fi971 + k207 + 4), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, fi971)) ) + Sum_{k159} ( Sum_{k207} ( -$(h(4, 44, fi971 + k207 + 4), ( G(h(4, 44, fi971 + k207 + 4), L[44,44],h(4, 44, k159)) * T( G(h(4, 44, fi971), L[44,44],h(4, 44, k159)) ) ),h(4, 44, fi971)) ) ) )

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 32);
    _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 32);
    _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 49);
    _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 49);

    for( int k207 = 0; k207 <= -fi971 + 39; k207+=4 ) {
      _t15_20 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 176);
      _t15_21 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 220);
      _t15_22 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 264);
      _t15_23 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 308);
      _t15_15 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 176);
      _t15_14 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 177);
      _t15_13 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 178);
      _t15_12 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 179);
      _t15_11 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 220);
      _t15_10 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 221);
      _t15_9 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 222);
      _t15_8 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 223);
      _t15_7 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 264);
      _t15_6 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 265);
      _t15_5 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 266);
      _t15_4 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 267);
      _t15_3 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 308);
      _t15_2 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 309);
      _t15_1 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 310);
      _t15_0 = _mm256_broadcast_sd(K + 44*fi971 + 44*k207 + 311);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t6_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 32);
      _t6_114 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 32);
      _t6_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_3, _t12_2), _mm256_unpacklo_pd(_t12_1, _t12_0), 49);
      _t6_116 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_3, _t12_2), _mm256_unpackhi_pd(_t12_1, _t12_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t15_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t6_113), _mm256_mul_pd(_t15_14, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t6_115), _mm256_mul_pd(_t15_12, _t6_116)));
      _t15_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t6_113), _mm256_mul_pd(_t15_10, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t6_115), _mm256_mul_pd(_t15_8, _t6_116)));
      _t15_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_7, _t6_113), _mm256_mul_pd(_t15_6, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t15_5, _t6_115), _mm256_mul_pd(_t15_4, _t6_116)));
      _t15_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_3, _t6_113), _mm256_mul_pd(_t15_2, _t6_114)), _mm256_add_pd(_mm256_mul_pd(_t15_1, _t6_115), _mm256_mul_pd(_t15_0, _t6_116)));

      // 4-BLAC: 4x4 - 4x4
      _t15_20 = _mm256_sub_pd(_t15_20, _t15_16);
      _t15_21 = _mm256_sub_pd(_t15_21, _t15_17);
      _t15_22 = _mm256_sub_pd(_t15_22, _t15_18);
      _t15_23 = _mm256_sub_pd(_t15_23, _t15_19);

      // AVX Storer:
      _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 176, _t15_20);
      _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 220, _t15_21);
      _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 264, _t15_22);
      _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 308, _t15_23);
    }

    for( int k159 = 4; k159 <= fi971 - 1; k159+=4 ) {

      for( int k207 = 0; k207 <= -fi971 + 39; k207+=4 ) {
        _t16_19 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 176);
        _t16_18 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 177);
        _t16_17 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 178);
        _t16_16 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 179);
        _t16_15 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 220);
        _t16_14 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 221);
        _t16_13 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 222);
        _t16_12 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 223);
        _t16_11 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 264);
        _t16_10 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 265);
        _t16_9 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 266);
        _t16_8 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 267);
        _t16_7 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 308);
        _t16_6 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 309);
        _t16_5 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 310);
        _t16_4 = _mm256_broadcast_sd(K + 44*fi971 + k159 + 44*k207 + 311);
        _t16_3 = _mm256_loadu_pd(K + 44*fi971 + k159);
        _t16_2 = _mm256_loadu_pd(K + 44*fi971 + k159 + 44);
        _t16_1 = _mm256_loadu_pd(K + 44*fi971 + k159 + 88);
        _t16_0 = _mm256_loadu_pd(K + 44*fi971 + k159 + 132);
        _t16_20 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 176);
        _t16_21 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 220);
        _t16_22 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 264);
        _t16_23 = _mm256_loadu_pd(K + 45*fi971 + 44*k207 + 308);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t8_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
        _t8_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
        _t8_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
        _t8_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t16_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t8_0), _mm256_mul_pd(_t16_18, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t8_2), _mm256_mul_pd(_t16_16, _t8_3)));
        _t16_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t8_0), _mm256_mul_pd(_t16_14, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t8_2), _mm256_mul_pd(_t16_12, _t8_3)));
        _t16_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t8_0), _mm256_mul_pd(_t16_10, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t8_2), _mm256_mul_pd(_t16_8, _t8_3)));
        _t16_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t8_0), _mm256_mul_pd(_t16_6, _t8_1)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t8_2), _mm256_mul_pd(_t16_4, _t8_3)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t16_20 = _mm256_sub_pd(_t16_20, _t16_24);
        _t16_21 = _mm256_sub_pd(_t16_21, _t16_25);
        _t16_22 = _mm256_sub_pd(_t16_22, _t16_26);
        _t16_23 = _mm256_sub_pd(_t16_23, _t16_27);

        // AVX Storer:
        _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 176, _t16_20);
        _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 220, _t16_21);
        _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 264, _t16_22);
        _mm256_storeu_pd(K + 45*fi971 + 44*k207 + 308, _t16_23);
      }
    }
    _mm256_maskstore_pd(K + 45*fi971 + 44, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t12_21);
    _mm256_maskstore_pd(K + 45*fi971 + 88, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t12_22);
    _mm256_storeu_pd(K + 45*fi971 + 132, _t12_23);

    for( int fi1090 = 0; fi1090 <= -fi971 + 36; fi1090+=4 ) {
      _t17_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 45*fi971 + 176])));
      _t17_1 = _mm256_maskload_pd(K + 44*fi1090 + 45*fi971 + 177, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t17_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 45*fi971 + 220])));
      _t17_7 = _mm256_maskload_pd(K + 44*fi1090 + 45*fi971 + 221, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t17_12 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 45*fi971 + 264])));
      _t17_13 = _mm256_maskload_pd(K + 44*fi1090 + 45*fi971 + 265, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t17_18 = _mm256_castpd128_pd256(_mm_load_sd(&(K[44*fi1090 + 45*fi971 + 308])));
      _t17_19 = _mm256_maskload_pd(K + 44*fi1090 + 45*fi971 + 309, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_24 = _t17_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_25 = _t12_20;

      // 4-BLAC: 1x4 / 1x4
      _t17_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_24), _mm256_castpd256_pd128(_t17_25)));

      // AVX Storer:
      _t17_0 = _t17_26;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_27 = _t17_1;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_0, _t17_0, 32), _mm256_permute2f128_pd(_t17_0, _t17_0, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t17_29 = _t14_1;

      // 4-BLAC: (4x1)^T
      _t10_30 = _t17_29;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_31 = _mm256_mul_pd(_t17_28, _t10_30);

      // 4-BLAC: 1x4 - 1x4
      _t17_30 = _mm256_sub_pd(_t17_27, _t10_31);

      // AVX Storer:
      _t17_1 = _t17_30;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_1, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_32 = _t14_2;

      // 4-BLAC: 1x4 / 1x4
      _t17_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_31), _mm256_castpd256_pd128(_t17_32)));

      // AVX Storer:
      _t17_2 = _t17_33;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_34 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_1, 6), _mm256_permute2f128_pd(_t17_1, _t17_1, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_2, _t17_2, 32), _mm256_permute2f128_pd(_t17_2, _t17_2, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t17_36 = _t14_3;

      // 4-BLAC: (4x1)^T
      _t10_39 = _t17_36;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_40 = _mm256_mul_pd(_t17_35, _t10_39);

      // 4-BLAC: 1x4 - 1x4
      _t17_37 = _mm256_sub_pd(_t17_34, _t10_40);

      // AVX Storer:
      _t17_3 = _t17_37;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_38 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_3, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_39 = _t14_5;

      // 4-BLAC: 1x4 / 1x4
      _t17_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_38), _mm256_castpd256_pd128(_t17_39)));

      // AVX Storer:
      _t17_4 = _t17_40;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_41 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_3, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_42 = _t17_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_43 = _t14_6;

      // 4-BLAC: (4x1)^T
      _t10_48 = _t17_43;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_49 = _mm256_mul_pd(_t17_42, _t10_48);

      // 4-BLAC: 1x4 - 1x4
      _t17_44 = _mm256_sub_pd(_t17_41, _t10_49);

      // AVX Storer:
      _t17_5 = _t17_44;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 4), ( G(h(1, 44, fi1090 + fi971 + 4), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_45 = _t17_5;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_46 = _t14_7;

      // 4-BLAC: 1x4 / 1x4
      _t17_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_45), _mm256_castpd256_pd128(_t17_46)));

      // AVX Storer:
      _t17_5 = _t17_47;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_48 = _t17_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_49 = _t12_20;

      // 4-BLAC: 1x4 / 1x4
      _t17_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_48), _mm256_castpd256_pd128(_t17_49)));

      // AVX Storer:
      _t17_6 = _t17_50;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_51 = _t17_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_6, _t17_6, 32), _mm256_permute2f128_pd(_t17_6, _t17_6, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t17_53 = _t14_1;

      // 4-BLAC: (4x1)^T
      _t10_60 = _t17_53;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_61 = _mm256_mul_pd(_t17_52, _t10_60);

      // 4-BLAC: 1x4 - 1x4
      _t17_54 = _mm256_sub_pd(_t17_51, _t10_61);

      // AVX Storer:
      _t17_7 = _t17_54;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_55 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_7, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_56 = _t14_2;

      // 4-BLAC: 1x4 / 1x4
      _t17_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_55), _mm256_castpd256_pd128(_t17_56)));

      // AVX Storer:
      _t17_8 = _t17_57;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_7, 6), _mm256_permute2f128_pd(_t17_7, _t17_7, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_8, _t17_8, 32), _mm256_permute2f128_pd(_t17_8, _t17_8, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t17_60 = _t14_3;

      // 4-BLAC: (4x1)^T
      _t10_69 = _t17_60;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_70 = _mm256_mul_pd(_t17_59, _t10_69);

      // 4-BLAC: 1x4 - 1x4
      _t17_61 = _mm256_sub_pd(_t17_58, _t10_70);

      // AVX Storer:
      _t17_9 = _t17_61;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_9, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_63 = _t14_5;

      // 4-BLAC: 1x4 / 1x4
      _t17_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_62), _mm256_castpd256_pd128(_t17_63)));

      // AVX Storer:
      _t17_10 = _t17_64;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_65 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_9, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_66 = _t17_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_67 = _t14_6;

      // 4-BLAC: (4x1)^T
      _t10_78 = _t17_67;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_79 = _mm256_mul_pd(_t17_66, _t10_78);

      // 4-BLAC: 1x4 - 1x4
      _t17_68 = _mm256_sub_pd(_t17_65, _t10_79);

      // AVX Storer:
      _t17_11 = _t17_68;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 5), ( G(h(1, 44, fi1090 + fi971 + 5), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_69 = _t17_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_70 = _t14_7;

      // 4-BLAC: 1x4 / 1x4
      _t17_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_69), _mm256_castpd256_pd128(_t17_70)));

      // AVX Storer:
      _t17_11 = _t17_71;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_72 = _t17_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_73 = _t12_20;

      // 4-BLAC: 1x4 / 1x4
      _t17_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_72), _mm256_castpd256_pd128(_t17_73)));

      // AVX Storer:
      _t17_12 = _t17_74;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_75 = _t17_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_12, _t17_12, 32), _mm256_permute2f128_pd(_t17_12, _t17_12, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t17_77 = _t14_1;

      // 4-BLAC: (4x1)^T
      _t10_90 = _t17_77;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_91 = _mm256_mul_pd(_t17_76, _t10_90);

      // 4-BLAC: 1x4 - 1x4
      _t17_78 = _mm256_sub_pd(_t17_75, _t10_91);

      // AVX Storer:
      _t17_13 = _t17_78;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_13, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_80 = _t14_2;

      // 4-BLAC: 1x4 / 1x4
      _t17_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_79), _mm256_castpd256_pd128(_t17_80)));

      // AVX Storer:
      _t17_14 = _t17_81;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_82 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_13, 6), _mm256_permute2f128_pd(_t17_13, _t17_13, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_83 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_14, _t17_14, 32), _mm256_permute2f128_pd(_t17_14, _t17_14, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t17_84 = _t14_3;

      // 4-BLAC: (4x1)^T
      _t10_99 = _t17_84;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_100 = _mm256_mul_pd(_t17_83, _t10_99);

      // 4-BLAC: 1x4 - 1x4
      _t17_85 = _mm256_sub_pd(_t17_82, _t10_100);

      // AVX Storer:
      _t17_15 = _t17_85;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_86 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_15, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_87 = _t14_5;

      // 4-BLAC: 1x4 / 1x4
      _t17_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_86), _mm256_castpd256_pd128(_t17_87)));

      // AVX Storer:
      _t17_16 = _t17_88;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_15, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_90 = _t17_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_91 = _t14_6;

      // 4-BLAC: (4x1)^T
      _t10_108 = _t17_91;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_109 = _mm256_mul_pd(_t17_90, _t10_108);

      // 4-BLAC: 1x4 - 1x4
      _t17_92 = _mm256_sub_pd(_t17_89, _t10_109);

      // AVX Storer:
      _t17_17 = _t17_92;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 6), ( G(h(1, 44, fi1090 + fi971 + 6), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_93 = _t17_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_94 = _t14_7;

      // 4-BLAC: 1x4 / 1x4
      _t17_95 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_93), _mm256_castpd256_pd128(_t17_94)));

      // AVX Storer:
      _t17_17 = _t17_95;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971)) Div G(h(1, 44, fi971), L[44,44],h(1, 44, fi971)) ),h(1, 44, fi971))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_96 = _t17_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_97 = _t12_20;

      // 4-BLAC: 1x4 / 1x4
      _t17_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_96), _mm256_castpd256_pd128(_t17_97)));

      // AVX Storer:
      _t17_18 = _t17_98;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(3, 44, fi971 + 1)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971)) Kro T( G(h(3, 44, fi971 + 1), L[44,44],h(1, 44, fi971)) ) ) ),h(3, 44, fi971 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t17_99 = _t17_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_100 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_18, _t17_18, 32), _mm256_permute2f128_pd(_t17_18, _t17_18, 32), 0);

      // AVX Loader:

      // 3x1 -> 4x1
      _t17_101 = _t14_1;

      // 4-BLAC: (4x1)^T
      _t10_120 = _t17_101;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_121 = _mm256_mul_pd(_t17_100, _t10_120);

      // 4-BLAC: 1x4 - 1x4
      _t17_102 = _mm256_sub_pd(_t17_99, _t10_121);

      // AVX Storer:
      _t17_19 = _t17_102;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 1)) Div G(h(1, 44, fi971 + 1), L[44,44],h(1, 44, fi971 + 1)) ),h(1, 44, fi971 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_19, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_104 = _t14_2;

      // 4-BLAC: 1x4 / 1x4
      _t17_105 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_103), _mm256_castpd256_pd128(_t17_104)));

      // AVX Storer:
      _t17_20 = _t17_105;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(2, 44, fi971 + 2)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 1)) Kro T( G(h(2, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 1)) ) ) ),h(2, 44, fi971 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t17_106 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_19, 6), _mm256_permute2f128_pd(_t17_19, _t17_19, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_107 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 32), _mm256_permute2f128_pd(_t17_20, _t17_20, 32), 0);

      // AVX Loader:

      // 2x1 -> 4x1
      _t17_108 = _t14_3;

      // 4-BLAC: (4x1)^T
      _t10_129 = _t17_108;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_130 = _mm256_mul_pd(_t17_107, _t10_129);

      // 4-BLAC: 1x4 - 1x4
      _t17_109 = _mm256_sub_pd(_t17_106, _t10_130);

      // AVX Storer:
      _t17_21 = _t17_109;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 2)) Div G(h(1, 44, fi971 + 2), L[44,44],h(1, 44, fi971 + 2)) ),h(1, 44, fi971 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_21, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_111 = _t14_5;

      // 4-BLAC: 1x4 / 1x4
      _t17_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_110), _mm256_castpd256_pd128(_t17_111)));

      // AVX Storer:
      _t17_22 = _t17_112;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 3)) - ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 2)) Kro T( G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 2)) ) ) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_113 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_21, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_114 = _t17_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_115 = _t14_6;

      // 4-BLAC: (4x1)^T
      _t10_138 = _t17_115;

      // 4-BLAC: 1x4 Kro 1x4
      _t10_139 = _mm256_mul_pd(_t17_114, _t10_138);

      // 4-BLAC: 1x4 - 1x4
      _t17_116 = _mm256_sub_pd(_t17_113, _t10_139);

      // AVX Storer:
      _t17_23 = _t17_116;

      // Generating : L[44,44] = S(h(1, 44, fi1090 + fi971 + 7), ( G(h(1, 44, fi1090 + fi971 + 7), L[44,44],h(1, 44, fi971 + 3)) Div G(h(1, 44, fi971 + 3), L[44,44],h(1, 44, fi971 + 3)) ),h(1, 44, fi971 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_117 = _t17_23;

      // AVX Loader:

      // 1x1 -> 1x4
      _t17_118 = _t14_7;

      // 4-BLAC: 1x4 / 1x4
      _t17_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_117), _mm256_castpd256_pd128(_t17_118)));

      // AVX Storer:
      _t17_23 = _t17_119;
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 176]), _mm256_castpd256_pd128(_t17_0));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 177]), _mm256_castpd256_pd128(_t17_2));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 178]), _mm256_castpd256_pd128(_t17_4));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 179]), _mm256_castpd256_pd128(_t17_5));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 220]), _mm256_castpd256_pd128(_t17_6));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 221]), _mm256_castpd256_pd128(_t17_8));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 222]), _mm256_castpd256_pd128(_t17_10));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 223]), _mm256_castpd256_pd128(_t17_11));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 264]), _mm256_castpd256_pd128(_t17_12));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 265]), _mm256_castpd256_pd128(_t17_14));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 266]), _mm256_castpd256_pd128(_t17_16));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 267]), _mm256_castpd256_pd128(_t17_17));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 308]), _mm256_castpd256_pd128(_t17_18));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 309]), _mm256_castpd256_pd128(_t17_20));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 310]), _mm256_castpd256_pd128(_t17_22));
      _mm_store_sd(&(K[44*fi1090 + 45*fi971 + 311]), _mm256_castpd256_pd128(_t17_23));
    }
    _mm_store_sd(K + 45*fi971, _mm256_castpd256_pd128(_t12_20));
    _mm256_maskstore_pd(K + 45*fi971 + 44, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t14_1);
    _mm256_maskstore_pd(K + 45*fi971 + 88, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t14_1, _t14_1, 1));
    _mm256_maskstore_pd(K + 45*fi971 + 132, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t14_1, _t14_1, 129));
    _mm_store_sd(&(K[45*fi971 + 45]), _mm256_castpd256_pd128(_t14_2));
    _mm256_maskstore_pd(K + 45*fi971 + 89, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t14_3);
    _mm256_maskstore_pd(K + 45*fi971 + 133, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t14_3, _t14_3, 1));
    _mm_store_sd(&(K[45*fi971 + 90]), _mm256_castpd256_pd128(_t14_5));
    _mm_store_sd(&(K[45*fi971 + 134]), _mm256_castpd256_pd128(_t14_6));
    _mm_store_sd(&(K[45*fi971 + 135]), _mm256_castpd256_pd128(_t14_7));
  }

  _t18_20 = _mm256_castpd128_pd256(_mm_load_sd(K + 1800));
  _t18_21 = _mm256_maskload_pd(K + 1844, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t18_22 = _mm256_maskload_pd(K + 1888, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t18_23 = _mm256_loadu_pd(K + 1932);
  _t18_19 = _mm256_broadcast_sd(K + 1760);
  _t18_18 = _mm256_broadcast_sd(K + 1761);
  _t18_17 = _mm256_broadcast_sd(K + 1762);
  _t18_16 = _mm256_broadcast_sd(K + 1763);
  _t18_15 = _mm256_broadcast_sd(K + 1804);
  _t18_14 = _mm256_broadcast_sd(K + 1805);
  _t18_13 = _mm256_broadcast_sd(K + 1806);
  _t18_12 = _mm256_broadcast_sd(K + 1807);
  _t18_11 = _mm256_broadcast_sd(K + 1848);
  _t18_10 = _mm256_broadcast_sd(K + 1849);
  _t18_9 = _mm256_broadcast_sd(K + 1850);
  _t18_8 = _mm256_broadcast_sd(K + 1851);
  _t18_7 = _mm256_broadcast_sd(K + 1892);
  _t18_6 = _mm256_broadcast_sd(K + 1893);
  _t18_5 = _mm256_broadcast_sd(K + 1894);
  _t18_4 = _mm256_broadcast_sd(K + 1895);
  _t18_3 = _mm256_loadu_pd(K + 1760);
  _t18_2 = _mm256_loadu_pd(K + 1804);
  _t18_1 = _mm256_loadu_pd(K + 1848);
  _t18_0 = _mm256_loadu_pd(K + 1892);

  // Generating : L[44,44] = ( S(h(4, 44, 40), ( G(h(4, 44, 40), K[44,44],h(4, 44, 40)) - ( G(h(4, 44, 40), L[44,44],h(4, 44, 0)) * T( G(h(4, 44, 40), L[44,44],h(4, 44, 0)) ) ) ),h(4, 44, 40)) + Sum_{k159} ( -$(h(4, 44, 40), ( G(h(4, 44, 40), L[44,44],h(4, 44, k159)) * T( G(h(4, 44, 40), L[44,44],h(4, 44, k159)) ) ),h(4, 44, 40)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - LowSymm
  _t18_32 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_20, _t18_21, 0), _mm256_shuffle_pd(_t18_22, _t18_23, 0), 32);
  _t18_33 = _mm256_permute2f128_pd(_t18_21, _mm256_shuffle_pd(_t18_22, _t18_23, 3), 32);
  _t18_34 = _mm256_blend_pd(_t18_22, _mm256_shuffle_pd(_t18_22, _t18_23, 3), 12);
  _t18_35 = _t18_23;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t18_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
  _t18_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
  _t18_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
  _t18_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t18_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_36), _mm256_mul_pd(_t18_18, _t18_37)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_38), _mm256_mul_pd(_t18_16, _t18_39)));
  _t18_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_36), _mm256_mul_pd(_t18_14, _t18_37)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_38), _mm256_mul_pd(_t18_12, _t18_39)));
  _t18_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_36), _mm256_mul_pd(_t18_10, _t18_37)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_38), _mm256_mul_pd(_t18_8, _t18_39)));
  _t18_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_36), _mm256_mul_pd(_t18_6, _t18_37)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_38), _mm256_mul_pd(_t18_4, _t18_39)));

  // 4-BLAC: 4x4 - 4x4
  _t18_28 = _mm256_sub_pd(_t18_32, _t18_24);
  _t18_29 = _mm256_sub_pd(_t18_33, _t18_25);
  _t18_30 = _mm256_sub_pd(_t18_34, _t18_26);
  _t18_31 = _mm256_sub_pd(_t18_35, _t18_27);

  // AVX Storer:

  // 4x4 -> 4x4 - LowTriang
  _t18_20 = _t18_28;
  _t18_21 = _t18_29;
  _t18_22 = _t18_30;
  _t18_23 = _t18_31;


  for( int k159 = 4; k159 <= 39; k159+=4 ) {
    _t19_19 = _mm256_broadcast_sd(K + k159 + 1760);
    _t19_18 = _mm256_broadcast_sd(K + k159 + 1761);
    _t19_17 = _mm256_broadcast_sd(K + k159 + 1762);
    _t19_16 = _mm256_broadcast_sd(K + k159 + 1763);
    _t19_15 = _mm256_broadcast_sd(K + k159 + 1804);
    _t19_14 = _mm256_broadcast_sd(K + k159 + 1805);
    _t19_13 = _mm256_broadcast_sd(K + k159 + 1806);
    _t19_12 = _mm256_broadcast_sd(K + k159 + 1807);
    _t19_11 = _mm256_broadcast_sd(K + k159 + 1848);
    _t19_10 = _mm256_broadcast_sd(K + k159 + 1849);
    _t19_9 = _mm256_broadcast_sd(K + k159 + 1850);
    _t19_8 = _mm256_broadcast_sd(K + k159 + 1851);
    _t19_7 = _mm256_broadcast_sd(K + k159 + 1892);
    _t19_6 = _mm256_broadcast_sd(K + k159 + 1893);
    _t19_5 = _mm256_broadcast_sd(K + k159 + 1894);
    _t19_4 = _mm256_broadcast_sd(K + k159 + 1895);
    _t19_3 = _mm256_loadu_pd(K + k159 + 1760);
    _t19_2 = _mm256_loadu_pd(K + k159 + 1804);
    _t19_1 = _mm256_loadu_pd(K + k159 + 1848);
    _t19_0 = _mm256_loadu_pd(K + k159 + 1892);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t19_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_3, _t19_2), _mm256_unpacklo_pd(_t19_1, _t19_0), 32);
    _t19_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_3, _t19_2), _mm256_unpackhi_pd(_t19_1, _t19_0), 32);
    _t19_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_3, _t19_2), _mm256_unpacklo_pd(_t19_1, _t19_0), 49);
    _t19_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_3, _t19_2), _mm256_unpackhi_pd(_t19_1, _t19_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t19_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_28), _mm256_mul_pd(_t19_18, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_30), _mm256_mul_pd(_t19_16, _t19_31)));
    _t19_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_28), _mm256_mul_pd(_t19_14, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_30), _mm256_mul_pd(_t19_12, _t19_31)));
    _t19_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_28), _mm256_mul_pd(_t19_10, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_30), _mm256_mul_pd(_t19_8, _t19_31)));
    _t19_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_7, _t19_28), _mm256_mul_pd(_t19_6, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_5, _t19_30), _mm256_mul_pd(_t19_4, _t19_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - LowTriang
    _t19_24 = _t18_20;
    _t19_25 = _t18_21;
    _t19_26 = _t18_22;
    _t19_27 = _t18_23;

    // 4-BLAC: 4x4 - 4x4
    _t19_24 = _mm256_sub_pd(_t19_24, _t19_20);
    _t19_25 = _mm256_sub_pd(_t19_25, _t19_21);
    _t19_26 = _mm256_sub_pd(_t19_26, _t19_22);
    _t19_27 = _mm256_sub_pd(_t19_27, _t19_23);

    // AVX Storer:

    // 4x4 -> 4x4 - LowTriang
    _t18_20 = _t19_24;
    _t18_21 = _t19_25;
    _t18_22 = _t19_26;
    _t18_23 = _t19_27;
  }


  // Generating : L[44,44] = S(h(1, 44, 40), Sqrt( G(h(1, 44, 40), L[44,44],h(1, 44, 40)) ),h(1, 44, 40))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_8 = _t18_20;

  // 4-BLAC: sqrt(1x4)
  _t20_9 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t20_8)));

  // AVX Storer:
  _t18_20 = _t20_9;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, 40), L[44,44],h(1, 44, 40)) ),h(1, 44, 40))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t20_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_11 = _t18_20;

  // 4-BLAC: 1x4 / 1x4
  _t20_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_10), _mm256_castpd256_pd128(_t20_11)));

  // AVX Storer:
  _t20_0 = _t20_12;

  // Generating : L[44,44] = S(h(3, 44, 41), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, 40)) Kro G(h(3, 44, 41), L[44,44],h(1, 44, 40)) ),h(1, 44, 40))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_0, _t20_0, 32), _mm256_permute2f128_pd(_t20_0, _t20_0, 32), 0);

  // AVX Loader:

  // 3x1 -> 4x1
  _t20_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_21, _t18_22), _mm256_unpacklo_pd(_t18_23, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 Kro 4x1
  _t20_15 = _mm256_mul_pd(_t20_13, _t20_14);

  // AVX Storer:
  _t20_1 = _t20_15;

  // Generating : L[44,44] = S(h(1, 44, 41), ( G(h(1, 44, 41), L[44,44],h(1, 44, 41)) - ( G(h(1, 44, 41), L[44,44],h(1, 44, 40)) Kro T( G(h(1, 44, 41), L[44,44],h(1, 44, 40)) ) ) ),h(1, 44, 41))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_16 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t20_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t20_1, 1);

  // 4-BLAC: (4x1)^T
  _t20_19 = _t20_18;

  // 4-BLAC: 1x4 Kro 1x4
  _t20_20 = _mm256_mul_pd(_t20_17, _t20_19);

  // 4-BLAC: 1x4 - 1x4
  _t20_21 = _mm256_sub_pd(_t20_16, _t20_20);

  // AVX Storer:
  _t20_2 = _t20_21;

  // Generating : L[44,44] = S(h(1, 44, 41), Sqrt( G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ),h(1, 44, 41))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_22 = _t20_2;

  // 4-BLAC: sqrt(1x4)
  _t20_23 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t20_22)));

  // AVX Storer:
  _t20_2 = _t20_23;

  // Generating : L[44,44] = S(h(2, 44, 42), ( G(h(2, 44, 42), L[44,44],h(1, 44, 41)) - ( G(h(2, 44, 42), L[44,44],h(1, 44, 40)) Kro T( G(h(1, 44, 41), L[44,44],h(1, 44, 40)) ) ) ),h(1, 44, 41))

  // AVX Loader:

  // 2x1 -> 4x1
  _t20_24 = _mm256_unpackhi_pd(_mm256_blend_pd(_t18_22, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t18_23, _mm256_setzero_pd(), 12));

  // AVX Loader:

  // 2x1 -> 4x1
  _t20_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t20_1, 2), _mm256_permute2f128_pd(_t20_1, _t20_1, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_1, _t20_1, 32), _mm256_permute2f128_pd(_t20_1, _t20_1, 32), 0);

  // 4-BLAC: (4x1)^T
  _t20_27 = _t20_26;

  // 4-BLAC: 4x1 Kro 1x4
  _t20_28 = _mm256_mul_pd(_t20_25, _t20_27);

  // 4-BLAC: 4x1 - 4x1
  _t20_29 = _mm256_sub_pd(_t20_24, _t20_28);

  // AVX Storer:
  _t20_3 = _t20_29;

  // Generating : T1496[1,44] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 44, 41), L[44,44],h(1, 44, 41)) ),h(1, 44, 41))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t20_30 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_31 = _t20_2;

  // 4-BLAC: 1x4 / 1x4
  _t20_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_30), _mm256_castpd256_pd128(_t20_31)));

  // AVX Storer:
  _t20_4 = _t20_32;

  // Generating : L[44,44] = S(h(2, 44, 42), ( G(h(1, 1, 0), T1496[1,44],h(1, 44, 41)) Kro G(h(2, 44, 42), L[44,44],h(1, 44, 41)) ),h(1, 44, 41))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_4, _t20_4, 32), _mm256_permute2f128_pd(_t20_4, _t20_4, 32), 0);

  // AVX Loader:

  // 2x1 -> 4x1
  _t20_34 = _t20_3;

  // 4-BLAC: 1x4 Kro 4x1
  _t20_35 = _mm256_mul_pd(_t20_33, _t20_34);

  // AVX Storer:
  _t20_3 = _t20_35;

  // Generating : L[44,44] = S(h(1, 44, 42), ( G(h(1, 44, 42), L[44,44],h(1, 44, 42)) - ( G(h(1, 44, 42), L[44,44],h(2, 44, 40)) * T( G(h(1, 44, 42), L[44,44],h(2, 44, 40)) ) ) ),h(1, 44, 42))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_36 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_22, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_22, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t20_37 = _mm256_shuffle_pd(_mm256_blend_pd(_t20_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t20_3, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x2 -> 1x4
  _t20_38 = _mm256_shuffle_pd(_mm256_blend_pd(_t20_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t20_3, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t20_39 = _t20_38;

  // 4-BLAC: 1x4 * 4x1
  _t20_40 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_37, _t20_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_37, _t20_39), _mm256_mul_pd(_t20_37, _t20_39), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_37, _t20_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_37, _t20_39), _mm256_mul_pd(_t20_37, _t20_39), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_37, _t20_39), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_37, _t20_39), _mm256_mul_pd(_t20_37, _t20_39), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t20_41 = _mm256_sub_pd(_t20_36, _t20_40);

  // AVX Storer:
  _t20_5 = _t20_41;

  // Generating : L[44,44] = S(h(1, 44, 42), Sqrt( G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ),h(1, 44, 42))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_42 = _t20_5;

  // 4-BLAC: sqrt(1x4)
  _t20_43 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t20_42)));

  // AVX Storer:
  _t20_5 = _t20_43;

  // Generating : L[44,44] = S(h(1, 44, 43), ( G(h(1, 44, 43), L[44,44],h(1, 44, 42)) - ( G(h(1, 44, 43), L[44,44],h(2, 44, 40)) * T( G(h(1, 44, 42), L[44,44],h(2, 44, 40)) ) ) ),h(1, 44, 42))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_44 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_23, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_23, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t20_45 = _mm256_blend_pd(_mm256_permute2f128_pd(_t20_1, _t20_1, 129), _t20_3, 2);

  // AVX Loader:

  // 1x2 -> 1x4
  _t20_46 = _mm256_shuffle_pd(_mm256_blend_pd(_t20_1, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t20_3, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t20_47 = _t20_46;

  // 4-BLAC: 1x4 * 4x1
  _t20_48 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_45, _t20_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_45, _t20_47), _mm256_mul_pd(_t20_45, _t20_47), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_45, _t20_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_45, _t20_47), _mm256_mul_pd(_t20_45, _t20_47), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_45, _t20_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_45, _t20_47), _mm256_mul_pd(_t20_45, _t20_47), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t20_49 = _mm256_sub_pd(_t20_44, _t20_48);

  // AVX Storer:
  _t20_6 = _t20_49;

  // Generating : L[44,44] = S(h(1, 44, 43), ( G(h(1, 44, 43), L[44,44],h(1, 44, 42)) Div G(h(1, 44, 42), L[44,44],h(1, 44, 42)) ),h(1, 44, 42))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_50 = _t20_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_51 = _t20_5;

  // 4-BLAC: 1x4 / 1x4
  _t20_52 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_50), _mm256_castpd256_pd128(_t20_51)));

  // AVX Storer:
  _t20_6 = _t20_52;

  // Generating : L[44,44] = S(h(1, 44, 43), ( G(h(1, 44, 43), L[44,44],h(1, 44, 43)) - ( G(h(1, 44, 43), L[44,44],h(3, 44, 40)) * T( G(h(1, 44, 43), L[44,44],h(3, 44, 40)) ) ) ),h(1, 44, 43))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_53 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t18_23, _t18_23, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t20_54 = _mm256_blend_pd(_mm256_permute2f128_pd(_t20_1, _t20_6, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t20_3, 2), 10);

  // AVX Loader:

  // 1x3 -> 1x4
  _t20_55 = _mm256_blend_pd(_mm256_permute2f128_pd(_t20_1, _t20_6, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t20_3, 2), 10);

  // 4-BLAC: (1x4)^T
  _t20_56 = _t20_55;

  // 4-BLAC: 1x4 * 4x1
  _t20_57 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_54, _t20_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_54, _t20_56), _mm256_mul_pd(_t20_54, _t20_56), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_54, _t20_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_54, _t20_56), _mm256_mul_pd(_t20_54, _t20_56), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_54, _t20_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_54, _t20_56), _mm256_mul_pd(_t20_54, _t20_56), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t20_58 = _mm256_sub_pd(_t20_53, _t20_57);

  // AVX Storer:
  _t20_7 = _t20_58;

  // Generating : L[44,44] = S(h(1, 44, 43), Sqrt( G(h(1, 44, 43), L[44,44],h(1, 44, 43)) ),h(1, 44, 43))

  // AVX Loader:

  // 1x1 -> 1x4
  _t20_59 = _t20_7;

  // 4-BLAC: sqrt(1x4)
  _t20_60 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t20_59)));

  // AVX Storer:
  _t20_7 = _t20_60;

  _mm_store_sd(&(K[0]), _mm256_castpd256_pd128(_t0_0));
  _mm256_maskstore_pd(K + 44, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_2);
  _mm256_maskstore_pd(K + 88, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_2, _t0_2, 1));
  _mm256_maskstore_pd(K + 132, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t0_2, _t0_2, 129));
  _mm_store_sd(&(K[45]), _mm256_castpd256_pd128(_t0_3));
  _mm256_maskstore_pd(K + 89, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t0_4);
  _mm256_maskstore_pd(K + 133, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t0_4, _t0_4, 1));
  _mm_store_sd(&(K[90]), _mm256_castpd256_pd128(_t0_6));
  _mm_store_sd(&(K[134]), _mm256_castpd256_pd128(_t0_7));
  _mm_store_sd(&(K[135]), _mm256_castpd256_pd128(_t0_8));
  _mm_store_sd(&(K[176]), _mm256_castpd256_pd128(_t0_9));
  _mm_store_sd(&(K[177]), _mm256_castpd256_pd128(_t0_11));
  _mm_store_sd(&(K[178]), _mm256_castpd256_pd128(_t0_13));
  _mm_store_sd(&(K[179]), _mm256_castpd256_pd128(_t0_14));
  _mm_store_sd(&(K[220]), _mm256_castpd256_pd128(_t0_15));
  _mm_store_sd(&(K[221]), _mm256_castpd256_pd128(_t0_17));
  _mm_store_sd(&(K[222]), _mm256_castpd256_pd128(_t0_19));
  _mm_store_sd(&(K[223]), _mm256_castpd256_pd128(_t0_20));
  _mm_store_sd(&(K[264]), _mm256_castpd256_pd128(_t0_21));
  _mm_store_sd(&(K[265]), _mm256_castpd256_pd128(_t0_23));
  _mm_store_sd(&(K[266]), _mm256_castpd256_pd128(_t0_25));
  _mm_store_sd(&(K[267]), _mm256_castpd256_pd128(_t0_26));
  _mm_store_sd(&(K[308]), _mm256_castpd256_pd128(_t0_27));
  _mm_store_sd(&(K[309]), _mm256_castpd256_pd128(_t0_29));
  _mm_store_sd(&(K[310]), _mm256_castpd256_pd128(_t0_31));
  _mm_store_sd(&(K[311]), _mm256_castpd256_pd128(_t0_32));
  _mm_store_sd(K + 180, _mm256_castpd256_pd128(_t2_0));
  _mm256_maskstore_pd(K + 224, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_5);
  _mm256_maskstore_pd(K + 268, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t2_5, _t2_5, 1));
  _mm256_maskstore_pd(K + 312, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t2_5, _t2_5, 129));
  _mm_store_sd(&(K[225]), _mm256_castpd256_pd128(_t2_6));
  _mm256_maskstore_pd(K + 269, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t2_7);
  _mm256_maskstore_pd(K + 313, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t2_7, _t2_7, 1));
  _mm_store_sd(&(K[270]), _mm256_castpd256_pd128(_t2_9));
  _mm_store_sd(&(K[314]), _mm256_castpd256_pd128(_t2_10));
  _mm_store_sd(&(K[315]), _mm256_castpd256_pd128(_t2_11));
  _mm_store_sd(&(K[356]), _mm256_castpd256_pd128(_t4_0));
  _mm_store_sd(&(K[357]), _mm256_castpd256_pd128(_t4_2));
  _mm_store_sd(&(K[358]), _mm256_castpd256_pd128(_t4_4));
  _mm_store_sd(&(K[359]), _mm256_castpd256_pd128(_t4_5));
  _mm_store_sd(&(K[400]), _mm256_castpd256_pd128(_t4_6));
  _mm_store_sd(&(K[401]), _mm256_castpd256_pd128(_t4_8));
  _mm_store_sd(&(K[402]), _mm256_castpd256_pd128(_t4_10));
  _mm_store_sd(&(K[403]), _mm256_castpd256_pd128(_t4_11));
  _mm_store_sd(&(K[444]), _mm256_castpd256_pd128(_t4_12));
  _mm_store_sd(&(K[445]), _mm256_castpd256_pd128(_t4_14));
  _mm_store_sd(&(K[446]), _mm256_castpd256_pd128(_t4_16));
  _mm_store_sd(&(K[447]), _mm256_castpd256_pd128(_t4_17));
  _mm_store_sd(&(K[488]), _mm256_castpd256_pd128(_t4_18));
  _mm_store_sd(&(K[489]), _mm256_castpd256_pd128(_t4_20));
  _mm_store_sd(&(K[490]), _mm256_castpd256_pd128(_t4_22));
  _mm_store_sd(&(K[491]), _mm256_castpd256_pd128(_t4_23));
  _mm_store_sd(K + 360, _mm256_castpd256_pd128(_t6_20));
  _mm256_maskstore_pd(K + 404, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_25);
  _mm256_maskstore_pd(K + 448, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t6_25, _t6_25, 1));
  _mm256_maskstore_pd(K + 492, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t6_25, _t6_25, 129));
  _mm_store_sd(&(K[405]), _mm256_castpd256_pd128(_t6_26));
  _mm256_maskstore_pd(K + 449, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t6_27);
  _mm256_maskstore_pd(K + 493, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t6_27, _t6_27, 1));
  _mm_store_sd(&(K[450]), _mm256_castpd256_pd128(_t6_29));
  _mm_store_sd(&(K[494]), _mm256_castpd256_pd128(_t6_30));
  _mm_store_sd(&(K[495]), _mm256_castpd256_pd128(_t6_31));

  for( int fi971 = 0; fi971 <= 39; fi971+=4 ) {
    _t21_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971])));
    _t21_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971])));
    _t21_8 = _mm256_maskload_pd(y + fi971 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t21_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 44)), _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 88))), _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 132)), 32);
    _t21_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 45])));
    _t21_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 89)), _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 133)), 0);
    _t21_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 90])));
    _t21_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 134])));
    _t21_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 135])));

    // Generating : t0[44,1] = S(h(1, 44, fi971), ( G(h(1, 44, fi971), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971), L0[44,44],h(1, 44, fi971)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_13 = _t21_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_14 = _t21_6;

    // 4-BLAC: 1x4 / 1x4
    _t21_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_13), _mm256_castpd256_pd128(_t21_14)));

    // AVX Storer:
    _t21_7 = _t21_15;

    // Generating : t0[44,1] = S(h(3, 44, fi971 + 1), ( G(h(3, 44, fi971 + 1), t0[44,1],h(1, 1, 0)) - ( G(h(3, 44, fi971 + 1), L0[44,44],h(1, 44, fi971)) Kro G(h(1, 44, fi971), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t21_16 = _t21_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t21_17 = _t21_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_7, _t21_7, 32), _mm256_permute2f128_pd(_t21_7, _t21_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t21_19 = _mm256_mul_pd(_t21_17, _t21_18);

    // 4-BLAC: 4x1 - 4x1
    _t21_20 = _mm256_sub_pd(_t21_16, _t21_19);

    // AVX Storer:
    _t21_8 = _t21_20;

    // Generating : t0[44,1] = S(h(1, 44, fi971 + 1), ( G(h(1, 44, fi971 + 1), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 1), L0[44,44],h(1, 44, fi971 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t21_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_22 = _t21_4;

    // 4-BLAC: 1x4 / 1x4
    _t21_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_21), _mm256_castpd256_pd128(_t21_22)));

    // AVX Storer:
    _t21_9 = _t21_23;

    // Generating : t0[44,1] = S(h(2, 44, fi971 + 2), ( G(h(2, 44, fi971 + 2), t0[44,1],h(1, 1, 0)) - ( G(h(2, 44, fi971 + 2), L0[44,44],h(1, 44, fi971 + 1)) Kro G(h(1, 44, fi971 + 1), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t21_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_8, 6), _mm256_permute2f128_pd(_t21_8, _t21_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t21_25 = _t21_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t21_9, _t21_9, 32), _mm256_permute2f128_pd(_t21_9, _t21_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t21_27 = _mm256_mul_pd(_t21_25, _t21_26);

    // 4-BLAC: 4x1 - 4x1
    _t21_28 = _mm256_sub_pd(_t21_24, _t21_27);

    // AVX Storer:
    _t21_10 = _t21_28;

    // Generating : t0[44,1] = S(h(1, 44, fi971 + 2), ( G(h(1, 44, fi971 + 2), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 2), L0[44,44],h(1, 44, fi971 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t21_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_30 = _t21_2;

    // 4-BLAC: 1x4 / 1x4
    _t21_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_29), _mm256_castpd256_pd128(_t21_30)));

    // AVX Storer:
    _t21_11 = _t21_31;

    // Generating : t0[44,1] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), t0[44,1],h(1, 1, 0)) - ( G(h(1, 44, fi971 + 3), L0[44,44],h(1, 44, fi971 + 2)) Kro G(h(1, 44, fi971 + 2), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t21_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_33 = _t21_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_34 = _t21_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t21_35 = _mm256_mul_pd(_t21_33, _t21_34);

    // 4-BLAC: 1x4 - 1x4
    _t21_36 = _mm256_sub_pd(_t21_32, _t21_35);

    // AVX Storer:
    _t21_12 = _t21_36;

    // Generating : t0[44,1] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 3), L0[44,44],h(1, 44, fi971 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_37 = _t21_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t21_38 = _t21_0;

    // 4-BLAC: 1x4 / 1x4
    _t21_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t21_37), _mm256_castpd256_pd128(_t21_38)));

    // AVX Storer:
    _t21_12 = _t21_39;

    // Generating : t0[44,1] = Sum_{k159} ( S(h(4, 44, fi971 + k159 + 4), ( G(h(4, 44, fi971 + k159 + 4), t0[44,1],h(1, 1, 0)) - ( G(h(4, 44, fi971 + k159 + 4), L0[44,44],h(4, 44, fi971)) * G(h(4, 44, fi971), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[fi971]), _mm256_castpd256_pd128(_t21_7));
    _mm_store_sd(&(y[fi971 + 1]), _mm256_castpd256_pd128(_t21_9));
    _mm_store_sd(&(y[fi971 + 2]), _mm256_castpd256_pd128(_t21_11));
    _mm_store_sd(&(y[fi971 + 3]), _mm256_castpd256_pd128(_t21_12));

    for( int k159 = 0; k159 <= -fi971 + 39; k159+=4 ) {
      _t22_9 = _mm256_loadu_pd(y + fi971 + k159 + 4);
      _t22_7 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 176);
      _t22_6 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 220);
      _t22_5 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 264);
      _t22_4 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 308);
      _t22_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971])));
      _t22_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971 + 1])));
      _t22_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971 + 2])));
      _t22_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[fi971 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t22_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32)), _mm256_mul_pd(_t22_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t22_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32)), _mm256_mul_pd(_t22_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32)), _mm256_mul_pd(_t22_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t22_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32)), _mm256_mul_pd(_t22_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_3, _t22_2), _mm256_unpacklo_pd(_t22_1, _t22_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t22_9 = _mm256_sub_pd(_t22_9, _t22_8);

      // AVX Storer:
      _mm256_storeu_pd(y + fi971 + k159 + 4, _t22_9);
    }
  }

  _t23_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[40])));
  _t23_1 = _mm256_maskload_pd(y + 41, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : t0[44,1] = S(h(1, 44, 40), ( G(h(1, 44, 40), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, 40), L0[44,44],h(1, 44, 40)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_6 = _t23_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_7 = _t18_20;

  // 4-BLAC: 1x4 / 1x4
  _t23_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t23_6), _mm256_castpd256_pd128(_t23_7)));

  // AVX Storer:
  _t23_0 = _t23_8;

  // Generating : t0[44,1] = S(h(3, 44, 41), ( G(h(3, 44, 41), t0[44,1],h(1, 1, 0)) - ( G(h(3, 44, 41), L0[44,44],h(1, 44, 40)) Kro G(h(1, 44, 40), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t23_9 = _t23_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t23_10 = _t20_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t23_0, _t23_0, 32), _mm256_permute2f128_pd(_t23_0, _t23_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t23_12 = _mm256_mul_pd(_t23_10, _t23_11);

  // 4-BLAC: 4x1 - 4x1
  _t23_13 = _mm256_sub_pd(_t23_9, _t23_12);

  // AVX Storer:
  _t23_1 = _t23_13;

  // Generating : t0[44,1] = S(h(1, 44, 41), ( G(h(1, 44, 41), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, 41), L0[44,44],h(1, 44, 41)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_14 = _mm256_blend_pd(_mm256_setzero_pd(), _t23_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_15 = _t20_2;

  // 4-BLAC: 1x4 / 1x4
  _t23_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t23_14), _mm256_castpd256_pd128(_t23_15)));

  // AVX Storer:
  _t23_2 = _t23_16;

  // Generating : t0[44,1] = S(h(2, 44, 42), ( G(h(2, 44, 42), t0[44,1],h(1, 1, 0)) - ( G(h(2, 44, 42), L0[44,44],h(1, 44, 41)) Kro G(h(1, 44, 41), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t23_17 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t23_1, 6), _mm256_permute2f128_pd(_t23_1, _t23_1, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t23_18 = _t20_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t23_2, _t23_2, 32), _mm256_permute2f128_pd(_t23_2, _t23_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t23_20 = _mm256_mul_pd(_t23_18, _t23_19);

  // 4-BLAC: 4x1 - 4x1
  _t23_21 = _mm256_sub_pd(_t23_17, _t23_20);

  // AVX Storer:
  _t23_3 = _t23_21;

  // Generating : t0[44,1] = S(h(1, 44, 42), ( G(h(1, 44, 42), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, 42), L0[44,44],h(1, 44, 42)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t23_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_23 = _t20_5;

  // 4-BLAC: 1x4 / 1x4
  _t23_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t23_22), _mm256_castpd256_pd128(_t23_23)));

  // AVX Storer:
  _t23_4 = _t23_24;

  // Generating : t0[44,1] = S(h(1, 44, 43), ( G(h(1, 44, 43), t0[44,1],h(1, 1, 0)) - ( G(h(1, 44, 43), L0[44,44],h(1, 44, 42)) Kro G(h(1, 44, 42), t0[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_25 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t23_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_26 = _t20_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_27 = _t23_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t23_28 = _mm256_mul_pd(_t23_26, _t23_27);

  // 4-BLAC: 1x4 - 1x4
  _t23_29 = _mm256_sub_pd(_t23_25, _t23_28);

  // AVX Storer:
  _t23_5 = _t23_29;

  // Generating : t0[44,1] = S(h(1, 44, 43), ( G(h(1, 44, 43), t0[44,1],h(1, 1, 0)) Div G(h(1, 44, 43), L0[44,44],h(1, 44, 43)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_30 = _t23_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t23_31 = _t20_7;

  // 4-BLAC: 1x4 / 1x4
  _t23_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t23_30), _mm256_castpd256_pd128(_t23_31)));

  // AVX Storer:
  _t23_5 = _t23_32;

  _mm_store_sd(K + 1800, _mm256_castpd256_pd128(_t18_20));
  _mm256_maskstore_pd(K + 1844, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t20_1);
  _mm256_maskstore_pd(K + 1888, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t20_1, _t20_1, 1));
  _mm256_maskstore_pd(K + 1932, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_permute2f128_pd(_t20_1, _t20_1, 129));
  _mm_store_sd(&(K[1845]), _mm256_castpd256_pd128(_t20_2));
  _mm256_maskstore_pd(K + 1889, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t20_3);
  _mm256_maskstore_pd(K + 1933, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _mm256_shuffle_pd(_t20_3, _t20_3, 1));
  _mm_store_sd(&(K[1890]), _mm256_castpd256_pd128(_t20_5));
  _mm_store_sd(&(K[1934]), _mm256_castpd256_pd128(_t20_6));
  _mm_store_sd(&(K[1935]), _mm256_castpd256_pd128(_t20_7));
  _mm_store_sd(&(y[40]), _mm256_castpd256_pd128(_t23_0));
  _mm_store_sd(&(y[41]), _mm256_castpd256_pd128(_t23_2));
  _mm_store_sd(&(y[42]), _mm256_castpd256_pd128(_t23_4));
  _mm_store_sd(&(y[43]), _mm256_castpd256_pd128(_t23_5));

  for( int fi971 = 0; fi971 <= 39; fi971+=4 ) {
    _t24_7 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 43])));
    _t24_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-45*fi971 + 1935])));
    _t24_8 = _mm256_maskload_pd(y + -fi971 + 40, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t24_5 = _mm256_maskload_pd(K + -45*fi971 + 1932, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t24_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-45*fi971 + 1890])));
    _t24_3 = _mm256_maskload_pd(K + -45*fi971 + 1888, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t24_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-45*fi971 + 1845])));
    _t24_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-45*fi971 + 1844])));
    _t24_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[-45*fi971 + 1800])));

    // Generating : a[44,1] = S(h(1, 44, -fi971 + 43), ( G(h(1, 44, -fi971 + 43), a[44,1],h(1, 1, 0)) Div G(h(1, 44, -fi971 + 43), L0[44,44],h(1, 44, -fi971 + 43)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_13 = _t24_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_14 = _t24_6;

    // 4-BLAC: 1x4 / 1x4
    _t24_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_13), _mm256_castpd256_pd128(_t24_14)));

    // AVX Storer:
    _t24_7 = _t24_15;

    // Generating : a[44,1] = S(h(3, 44, -fi971 + 40), ( G(h(3, 44, -fi971 + 40), a[44,1],h(1, 1, 0)) - ( T( G(h(1, 44, -fi971 + 43), L0[44,44],h(3, 44, -fi971 + 40)) ) Kro G(h(1, 44, -fi971 + 43), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t24_16 = _t24_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t24_17 = _t24_5;

    // 4-BLAC: (1x4)^T
    _t24_18 = _t24_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_7, _t24_7, 32), _mm256_permute2f128_pd(_t24_7, _t24_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t24_20 = _mm256_mul_pd(_t24_18, _t24_19);

    // 4-BLAC: 4x1 - 4x1
    _t24_21 = _mm256_sub_pd(_t24_16, _t24_20);

    // AVX Storer:
    _t24_8 = _t24_21;

    // Generating : a[44,1] = S(h(1, 44, -fi971 + 42), ( G(h(1, 44, -fi971 + 42), a[44,1],h(1, 1, 0)) Div G(h(1, 44, -fi971 + 42), L0[44,44],h(1, 44, -fi971 + 42)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_22 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t24_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_23 = _t24_4;

    // 4-BLAC: 1x4 / 1x4
    _t24_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_22), _mm256_castpd256_pd128(_t24_23)));

    // AVX Storer:
    _t24_9 = _t24_24;

    // Generating : a[44,1] = S(h(2, 44, -fi971 + 40), ( G(h(2, 44, -fi971 + 40), a[44,1],h(1, 1, 0)) - ( T( G(h(1, 44, -fi971 + 42), L0[44,44],h(2, 44, -fi971 + 40)) ) Kro G(h(1, 44, -fi971 + 42), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t24_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_8, 3);

    // AVX Loader:

    // 1x2 -> 1x4
    _t24_26 = _t24_3;

    // 4-BLAC: (1x4)^T
    _t24_27 = _t24_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t24_9, _t24_9, 32), _mm256_permute2f128_pd(_t24_9, _t24_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t24_29 = _mm256_mul_pd(_t24_27, _t24_28);

    // 4-BLAC: 4x1 - 4x1
    _t24_30 = _mm256_sub_pd(_t24_25, _t24_29);

    // AVX Storer:
    _t24_10 = _t24_30;

    // Generating : a[44,1] = S(h(1, 44, -fi971 + 41), ( G(h(1, 44, -fi971 + 41), a[44,1],h(1, 1, 0)) Div G(h(1, 44, -fi971 + 41), L0[44,44],h(1, 44, -fi971 + 41)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_31 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t24_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_32 = _t24_2;

    // 4-BLAC: 1x4 / 1x4
    _t24_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_31), _mm256_castpd256_pd128(_t24_32)));

    // AVX Storer:
    _t24_11 = _t24_33;

    // Generating : a[44,1] = S(h(1, 44, -fi971 + 40), ( G(h(1, 44, -fi971 + 40), a[44,1],h(1, 1, 0)) - ( T( G(h(1, 44, -fi971 + 41), L0[44,44],h(1, 44, -fi971 + 40)) ) Kro G(h(1, 44, -fi971 + 41), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t24_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_35 = _t24_1;

    // 4-BLAC: (4x1)^T
    _t24_36 = _t24_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_37 = _t24_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t24_38 = _mm256_mul_pd(_t24_36, _t24_37);

    // 4-BLAC: 1x4 - 1x4
    _t24_39 = _mm256_sub_pd(_t24_34, _t24_38);

    // AVX Storer:
    _t24_12 = _t24_39;

    // Generating : a[44,1] = S(h(1, 44, -fi971 + 40), ( G(h(1, 44, -fi971 + 40), a[44,1],h(1, 1, 0)) Div G(h(1, 44, -fi971 + 40), L0[44,44],h(1, 44, -fi971 + 40)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_40 = _t24_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t24_41 = _t24_0;

    // 4-BLAC: 1x4 / 1x4
    _t24_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t24_40), _mm256_castpd256_pd128(_t24_41)));

    // AVX Storer:
    _t24_12 = _t24_42;

    // Generating : a[44,1] = Sum_{k159} ( S(h(4, 44, k159), ( G(h(4, 44, k159), a[44,1],h(1, 1, 0)) - ( T( G(h(4, 44, -fi971 + 40), L0[44,44],h(4, 44, k159)) ) * G(h(4, 44, -fi971 + 40), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(y[-fi971 + 43]), _mm256_castpd256_pd128(_t24_7));
    _mm_store_sd(&(y[-fi971 + 42]), _mm256_castpd256_pd128(_t24_9));
    _mm_store_sd(&(y[-fi971 + 41]), _mm256_castpd256_pd128(_t24_11));
    _mm_store_sd(&(y[-fi971 + 40]), _mm256_castpd256_pd128(_t24_12));

    for( int k159 = 0; k159 <= -fi971 + 39; k159+=4 ) {
      _t25_9 = _mm256_loadu_pd(y + k159);
      _t25_7 = _mm256_loadu_pd(K + -44*fi971 + k159 + 1760);
      _t25_6 = _mm256_loadu_pd(K + -44*fi971 + k159 + 1804);
      _t25_5 = _mm256_loadu_pd(K + -44*fi971 + k159 + 1848);
      _t25_4 = _mm256_loadu_pd(K + -44*fi971 + k159 + 1892);
      _t25_3 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 43])));
      _t25_2 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 42])));
      _t25_1 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 41])));
      _t25_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[-fi971 + 40])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t25_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_7, _t25_6), _mm256_unpacklo_pd(_t25_5, _t25_4), 32);
      _t25_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_7, _t25_6), _mm256_unpackhi_pd(_t25_5, _t25_4), 32);
      _t25_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_7, _t25_6), _mm256_unpacklo_pd(_t25_5, _t25_4), 49);
      _t25_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_7, _t25_6), _mm256_unpackhi_pd(_t25_5, _t25_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t25_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t25_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32)), _mm256_mul_pd(_t25_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t25_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32)), _mm256_mul_pd(_t25_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t25_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32)), _mm256_mul_pd(_t25_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t25_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32)), _mm256_mul_pd(_t25_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_0, _t25_1), _mm256_unpacklo_pd(_t25_2, _t25_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t25_9 = _mm256_sub_pd(_t25_9, _t25_8);

      // AVX Storer:
      _mm256_storeu_pd(y + k159, _t25_9);
    }
  }

  _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(K[135])));
  _t0_3 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45])));
  _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[134])));
  _t0_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 44)), _mm256_castpd128_pd256(_mm_load_sd(K + 88))), _mm256_castpd128_pd256(_mm_load_sd(K + 132)), 32);
  _t0_4 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 89)), _mm256_castpd128_pd256(_mm_load_sd(K + 133)), 0);
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[90])));
  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[0])));
  _t26_0 = _mm256_castpd128_pd256(_mm_load_sd(&(y[3])));
  _t26_1 = _mm256_maskload_pd(y, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : a[44,1] = S(h(1, 44, 3), ( G(h(1, 44, 3), a[44,1],h(1, 1, 0)) Div G(h(1, 44, 3), L0[44,44],h(1, 44, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_6 = _t26_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_7 = _t0_8;

  // 4-BLAC: 1x4 / 1x4
  _t26_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_6), _mm256_castpd256_pd128(_t26_7)));

  // AVX Storer:
  _t26_0 = _t26_8;

  // Generating : a[44,1] = S(h(3, 44, 0), ( G(h(3, 44, 0), a[44,1],h(1, 1, 0)) - ( T( G(h(1, 44, 3), L0[44,44],h(3, 44, 0)) ) Kro G(h(1, 44, 3), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t26_9 = _t26_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t26_10 = _mm256_blend_pd(_mm256_permute2f128_pd(_t0_2, _t0_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t0_4, 2), 10);

  // 4-BLAC: (1x4)^T
  _t26_11 = _t26_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_0, _t26_0, 32), _mm256_permute2f128_pd(_t26_0, _t26_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t26_13 = _mm256_mul_pd(_t26_11, _t26_12);

  // 4-BLAC: 4x1 - 4x1
  _t26_14 = _mm256_sub_pd(_t26_9, _t26_13);

  // AVX Storer:
  _t26_1 = _t26_14;

  // Generating : a[44,1] = S(h(1, 44, 2), ( G(h(1, 44, 2), a[44,1],h(1, 1, 0)) Div G(h(1, 44, 2), L0[44,44],h(1, 44, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_15 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t26_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_16 = _t0_6;

  // 4-BLAC: 1x4 / 1x4
  _t26_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_15), _mm256_castpd256_pd128(_t26_16)));

  // AVX Storer:
  _t26_2 = _t26_17;

  // Generating : a[44,1] = S(h(2, 44, 0), ( G(h(2, 44, 0), a[44,1],h(1, 1, 0)) - ( T( G(h(1, 44, 2), L0[44,44],h(2, 44, 0)) ) Kro G(h(1, 44, 2), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t26_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t26_1, 3);

  // AVX Loader:

  // 1x2 -> 1x4
  _t26_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t0_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t0_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (1x4)^T
  _t26_20 = _t26_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t26_2, _t26_2, 32), _mm256_permute2f128_pd(_t26_2, _t26_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t26_22 = _mm256_mul_pd(_t26_20, _t26_21);

  // 4-BLAC: 4x1 - 4x1
  _t26_23 = _mm256_sub_pd(_t26_18, _t26_22);

  // AVX Storer:
  _t26_3 = _t26_23;

  // Generating : a[44,1] = S(h(1, 44, 1), ( G(h(1, 44, 1), a[44,1],h(1, 1, 0)) Div G(h(1, 44, 1), L0[44,44],h(1, 44, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_24 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t26_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_25 = _t0_3;

  // 4-BLAC: 1x4 / 1x4
  _t26_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_24), _mm256_castpd256_pd128(_t26_25)));

  // AVX Storer:
  _t26_4 = _t26_26;

  // Generating : a[44,1] = S(h(1, 44, 0), ( G(h(1, 44, 0), a[44,1],h(1, 1, 0)) - ( T( G(h(1, 44, 1), L0[44,44],h(1, 44, 0)) ) Kro G(h(1, 44, 1), a[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_27 = _mm256_blend_pd(_mm256_setzero_pd(), _t26_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_28 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_2, 1);

  // 4-BLAC: (4x1)^T
  _t26_29 = _t26_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_30 = _t26_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t26_31 = _mm256_mul_pd(_t26_29, _t26_30);

  // 4-BLAC: 1x4 - 1x4
  _t26_32 = _mm256_sub_pd(_t26_27, _t26_31);

  // AVX Storer:
  _t26_5 = _t26_32;

  // Generating : a[44,1] = S(h(1, 44, 0), ( G(h(1, 44, 0), a[44,1],h(1, 1, 0)) Div G(h(1, 44, 0), L0[44,44],h(1, 44, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_33 = _t26_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t26_34 = _t0_0;

  // 4-BLAC: 1x4 / 1x4
  _t26_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t26_33), _mm256_castpd256_pd128(_t26_34)));

  // AVX Storer:
  _t26_5 = _t26_35;

  // Generating : kx[44,1] = ( Sum_{k207} ( S(h(4, 44, k207), ( G(h(4, 44, k207), X[44,44],h(4, 44, 0)) * G(h(4, 44, 0), x[44,1],h(1, 1, 0)) ),h(1, 1, 0)) ) + Sum_{k159} ( Sum_{k207} ( $(h(4, 44, k207), ( G(h(4, 44, k207), X[44,44],h(4, 44, k159)) * G(h(4, 44, k159), x[44,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k207 = 0; k207 <= 43; k207+=4 ) {
    _t27_4 = _mm256_loadu_pd(X + 44*k207);
    _t27_3 = _mm256_loadu_pd(X + 44*k207 + 44);
    _t27_2 = _mm256_loadu_pd(X + 44*k207 + 88);
    _t27_1 = _mm256_loadu_pd(X + 44*k207 + 132);
    _t27_0 = _mm256_loadu_pd(x);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t27_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t27_4, _t27_0), _mm256_mul_pd(_t27_3, _t27_0)), _mm256_hadd_pd(_mm256_mul_pd(_t27_2, _t27_0), _mm256_mul_pd(_t27_1, _t27_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t27_4, _t27_0), _mm256_mul_pd(_t27_3, _t27_0)), _mm256_hadd_pd(_mm256_mul_pd(_t27_2, _t27_0), _mm256_mul_pd(_t27_1, _t27_0)), 12));

    // AVX Storer:
    _mm256_storeu_pd(kx + k207, _t27_5);
  }


  for( int k159 = 4; k159 <= 43; k159+=4 ) {

    // AVX Loader:

    for( int k207 = 0; k207 <= 43; k207+=4 ) {
      _t28_4 = _mm256_loadu_pd(X + k159 + 44*k207);
      _t28_3 = _mm256_loadu_pd(X + k159 + 44*k207 + 44);
      _t28_2 = _mm256_loadu_pd(X + k159 + 44*k207 + 88);
      _t28_1 = _mm256_loadu_pd(X + k159 + 44*k207 + 132);
      _t28_0 = _mm256_loadu_pd(x + k159);
      _t28_5 = _mm256_loadu_pd(kx + k207);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t28_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t28_4, _t28_0), _mm256_mul_pd(_t28_3, _t28_0)), _mm256_hadd_pd(_mm256_mul_pd(_t28_2, _t28_0), _mm256_mul_pd(_t28_1, _t28_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t28_4, _t28_0), _mm256_mul_pd(_t28_3, _t28_0)), _mm256_hadd_pd(_mm256_mul_pd(_t28_2, _t28_0), _mm256_mul_pd(_t28_1, _t28_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t28_5 = _mm256_add_pd(_t28_5, _t28_6);

      // AVX Storer:
      _mm256_storeu_pd(kx + k207, _t28_5);
    }
  }

  _t29_0 = _mm256_loadu_pd(kx);

  // Generating : f[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 44, 0), kx[44,1],h(1, 1, 0)) ) * G(h(4, 44, 0), y[44,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{k159} ( $(h(1, 1, 0), ( T( G(h(4, 44, k159), kx[44,1],h(1, 1, 0)) ) * G(h(4, 44, k159), y[44,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t29_3 = _t29_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t29_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_mul_pd(_t29_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), 129)), 1));

  // AVX Storer:
  _t29_1 = _t29_2;


  for( int k159 = 4; k159 <= 43; k159+=4 ) {
    _t30_1 = _mm256_loadu_pd(kx + k159);
    _t30_0 = _mm256_loadu_pd(y + k159);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t30_4 = _t30_1;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t30_3 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t30_4, _t30_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_4, _t30_0), _mm256_mul_pd(_t30_4, _t30_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t30_4, _t30_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_4, _t30_0), _mm256_mul_pd(_t30_4, _t30_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t30_4, _t30_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t30_4, _t30_0), _mm256_mul_pd(_t30_4, _t30_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t30_2 = _t29_1;

    // 4-BLAC: 1x4 + 1x4
    _t30_2 = _mm256_add_pd(_t30_2, _t30_3);

    // AVX Storer:
    _t29_1 = _t30_2;
  }


  for( int fi971 = 0; fi971 <= 39; fi971+=4 ) {
    _t31_7 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971])));
    _t31_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971])));
    _t31_8 = _mm256_maskload_pd(kx + fi971 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t31_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 44)), _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 88))), _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 132)), 32);
    _t31_4 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 45])));
    _t31_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 89)), _mm256_castpd128_pd256(_mm_load_sd(K + 45*fi971 + 133)), 0);
    _t31_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 90])));
    _t31_1 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 134])));
    _t31_0 = _mm256_castpd128_pd256(_mm_load_sd(&(K[45*fi971 + 135])));

    // Generating : v[44,1] = S(h(1, 44, fi971), ( G(h(1, 44, fi971), v[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971), L0[44,44],h(1, 44, fi971)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_13 = _t31_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_14 = _t31_6;

    // 4-BLAC: 1x4 / 1x4
    _t31_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t31_13), _mm256_castpd256_pd128(_t31_14)));

    // AVX Storer:
    _t31_7 = _t31_15;

    // Generating : v[44,1] = S(h(3, 44, fi971 + 1), ( G(h(3, 44, fi971 + 1), v[44,1],h(1, 1, 0)) - ( G(h(3, 44, fi971 + 1), L0[44,44],h(1, 44, fi971)) Kro G(h(1, 44, fi971), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t31_16 = _t31_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t31_17 = _t31_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_7, _t31_7, 32), _mm256_permute2f128_pd(_t31_7, _t31_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t31_19 = _mm256_mul_pd(_t31_17, _t31_18);

    // 4-BLAC: 4x1 - 4x1
    _t31_20 = _mm256_sub_pd(_t31_16, _t31_19);

    // AVX Storer:
    _t31_8 = _t31_20;

    // Generating : v[44,1] = S(h(1, 44, fi971 + 1), ( G(h(1, 44, fi971 + 1), v[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 1), L0[44,44],h(1, 44, fi971 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t31_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_22 = _t31_4;

    // 4-BLAC: 1x4 / 1x4
    _t31_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t31_21), _mm256_castpd256_pd128(_t31_22)));

    // AVX Storer:
    _t31_9 = _t31_23;

    // Generating : v[44,1] = S(h(2, 44, fi971 + 2), ( G(h(2, 44, fi971 + 2), v[44,1],h(1, 1, 0)) - ( G(h(2, 44, fi971 + 2), L0[44,44],h(1, 44, fi971 + 1)) Kro G(h(1, 44, fi971 + 1), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t31_24 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t31_8, 6), _mm256_permute2f128_pd(_t31_8, _t31_8, 129), 5);

    // AVX Loader:

    // 2x1 -> 4x1
    _t31_25 = _t31_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_9, _t31_9, 32), _mm256_permute2f128_pd(_t31_9, _t31_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t31_27 = _mm256_mul_pd(_t31_25, _t31_26);

    // 4-BLAC: 4x1 - 4x1
    _t31_28 = _mm256_sub_pd(_t31_24, _t31_27);

    // AVX Storer:
    _t31_10 = _t31_28;

    // Generating : v[44,1] = S(h(1, 44, fi971 + 2), ( G(h(1, 44, fi971 + 2), v[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 2), L0[44,44],h(1, 44, fi971 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_29 = _mm256_blend_pd(_mm256_setzero_pd(), _t31_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_30 = _t31_2;

    // 4-BLAC: 1x4 / 1x4
    _t31_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t31_29), _mm256_castpd256_pd128(_t31_30)));

    // AVX Storer:
    _t31_11 = _t31_31;

    // Generating : v[44,1] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), v[44,1],h(1, 1, 0)) - ( G(h(1, 44, fi971 + 3), L0[44,44],h(1, 44, fi971 + 2)) Kro G(h(1, 44, fi971 + 2), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_32 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t31_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_33 = _t31_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_34 = _t31_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t31_35 = _mm256_mul_pd(_t31_33, _t31_34);

    // 4-BLAC: 1x4 - 1x4
    _t31_36 = _mm256_sub_pd(_t31_32, _t31_35);

    // AVX Storer:
    _t31_12 = _t31_36;

    // Generating : v[44,1] = S(h(1, 44, fi971 + 3), ( G(h(1, 44, fi971 + 3), v[44,1],h(1, 1, 0)) Div G(h(1, 44, fi971 + 3), L0[44,44],h(1, 44, fi971 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_37 = _t31_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t31_38 = _t31_0;

    // 4-BLAC: 1x4 / 1x4
    _t31_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t31_37), _mm256_castpd256_pd128(_t31_38)));

    // AVX Storer:
    _t31_12 = _t31_39;

    // Generating : v[44,1] = Sum_{k159} ( S(h(4, 44, fi971 + k159 + 4), ( G(h(4, 44, fi971 + k159 + 4), v[44,1],h(1, 1, 0)) - ( G(h(4, 44, fi971 + k159 + 4), L0[44,44],h(4, 44, fi971)) * G(h(4, 44, fi971), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(kx[fi971]), _mm256_castpd256_pd128(_t31_7));
    _mm_store_sd(&(kx[fi971 + 1]), _mm256_castpd256_pd128(_t31_9));
    _mm_store_sd(&(kx[fi971 + 2]), _mm256_castpd256_pd128(_t31_11));
    _mm_store_sd(&(kx[fi971 + 3]), _mm256_castpd256_pd128(_t31_12));

    for( int k159 = 0; k159 <= -fi971 + 39; k159+=4 ) {
      _t32_9 = _mm256_loadu_pd(kx + fi971 + k159 + 4);
      _t32_7 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 176);
      _t32_6 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 220);
      _t32_5 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 264);
      _t32_4 = _mm256_loadu_pd(K + 45*fi971 + 44*k159 + 308);
      _t32_3 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971])));
      _t32_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971 + 1])));
      _t32_1 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971 + 2])));
      _t32_0 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[fi971 + 3])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t32_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t32_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32)), _mm256_mul_pd(_t32_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t32_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32)), _mm256_mul_pd(_t32_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t32_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32)), _mm256_mul_pd(_t32_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t32_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32)), _mm256_mul_pd(_t32_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t32_9 = _mm256_sub_pd(_t32_9, _t32_8);

      // AVX Storer:
      _mm256_storeu_pd(kx + fi971 + k159 + 4, _t32_9);
    }
  }

  _t20_2 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1845])));
  _t20_7 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1935])));
  _t20_5 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1890])));
  _t18_20 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1800])));
  _t20_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1889)), _mm256_castpd128_pd256(_mm_load_sd(K + 1933)), 0);
  _t20_6 = _mm256_castpd128_pd256(_mm_load_sd(&(K[1934])));
  _t20_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(K + 1844)), _mm256_castpd128_pd256(_mm_load_sd(K + 1888))), _mm256_castpd128_pd256(_mm_load_sd(K + 1932)), 32);
  _t33_2 = _mm256_castpd128_pd256(_mm_load_sd(&(kx[40])));
  _t33_3 = _mm256_maskload_pd(kx + 41, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t33_1 = _mm256_loadu_pd(x);
  _t33_0 = _mm256_loadu_pd(kx);

  // Generating : v[44,1] = S(h(1, 44, 40), ( G(h(1, 44, 40), v[44,1],h(1, 1, 0)) Div G(h(1, 44, 40), L0[44,44],h(1, 44, 40)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_9 = _t33_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_10 = _t18_20;

  // 4-BLAC: 1x4 / 1x4
  _t33_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t33_9), _mm256_castpd256_pd128(_t33_10)));

  // AVX Storer:
  _t33_2 = _t33_11;

  // Generating : v[44,1] = S(h(3, 44, 41), ( G(h(3, 44, 41), v[44,1],h(1, 1, 0)) - ( G(h(3, 44, 41), L0[44,44],h(1, 44, 40)) Kro G(h(1, 44, 40), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t33_12 = _t33_3;

  // AVX Loader:

  // 3x1 -> 4x1
  _t33_13 = _t20_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_2, _t33_2, 32), _mm256_permute2f128_pd(_t33_2, _t33_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t33_15 = _mm256_mul_pd(_t33_13, _t33_14);

  // 4-BLAC: 4x1 - 4x1
  _t33_16 = _mm256_sub_pd(_t33_12, _t33_15);

  // AVX Storer:
  _t33_3 = _t33_16;

  // Generating : v[44,1] = S(h(1, 44, 41), ( G(h(1, 44, 41), v[44,1],h(1, 1, 0)) Div G(h(1, 44, 41), L0[44,44],h(1, 44, 41)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t33_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_18 = _t20_2;

  // 4-BLAC: 1x4 / 1x4
  _t33_19 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t33_17), _mm256_castpd256_pd128(_t33_18)));

  // AVX Storer:
  _t33_4 = _t33_19;

  // Generating : v[44,1] = S(h(2, 44, 42), ( G(h(2, 44, 42), v[44,1],h(1, 1, 0)) - ( G(h(2, 44, 42), L0[44,44],h(1, 44, 41)) Kro G(h(1, 44, 41), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t33_20 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t33_3, 6), _mm256_permute2f128_pd(_t33_3, _t33_3, 129), 5);

  // AVX Loader:

  // 2x1 -> 4x1
  _t33_21 = _t20_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_22 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t33_23 = _mm256_mul_pd(_t33_21, _t33_22);

  // 4-BLAC: 4x1 - 4x1
  _t33_24 = _mm256_sub_pd(_t33_20, _t33_23);

  // AVX Storer:
  _t33_5 = _t33_24;

  // Generating : v[44,1] = S(h(1, 44, 42), ( G(h(1, 44, 42), v[44,1],h(1, 1, 0)) Div G(h(1, 44, 42), L0[44,44],h(1, 44, 42)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t33_5, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_26 = _t20_5;

  // 4-BLAC: 1x4 / 1x4
  _t33_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t33_25), _mm256_castpd256_pd128(_t33_26)));

  // AVX Storer:
  _t33_6 = _t33_27;

  // Generating : v[44,1] = S(h(1, 44, 43), ( G(h(1, 44, 43), v[44,1],h(1, 1, 0)) - ( G(h(1, 44, 43), L0[44,44],h(1, 44, 42)) Kro G(h(1, 44, 42), v[44,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_28 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t33_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_29 = _t20_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_30 = _t33_6;

  // 4-BLAC: 1x4 Kro 1x4
  _t33_31 = _mm256_mul_pd(_t33_29, _t33_30);

  // 4-BLAC: 1x4 - 1x4
  _t33_32 = _mm256_sub_pd(_t33_28, _t33_31);

  // AVX Storer:
  _t33_7 = _t33_32;

  // Generating : v[44,1] = S(h(1, 44, 43), ( G(h(1, 44, 43), v[44,1],h(1, 1, 0)) Div G(h(1, 44, 43), L0[44,44],h(1, 44, 43)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_33 = _t33_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t33_34 = _t20_7;

  // 4-BLAC: 1x4 / 1x4
  _t33_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t33_33), _mm256_castpd256_pd128(_t33_34)));

  // AVX Storer:
  _t33_7 = _t33_35;

  // Generating : var[1,1] = ( ( S(h(1, 1, 0), ( ( T( G(h(4, 44, 0), x[44,1],h(1, 1, 0)) ) * G(h(4, 44, 0), x[44,1],h(1, 1, 0)) ) - ( T( G(h(4, 44, 0), kx[44,1],h(1, 1, 0)) ) * G(h(4, 44, 0), kx[44,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k207} ( $(h(1, 1, 0), ( T( G(h(4, 44, k207), x[44,1],h(1, 1, 0)) ) * G(h(4, 44, k207), x[44,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + Sum_{k159} ( -$(h(1, 1, 0), ( T( G(h(4, 44, k159), kx[44,1],h(1, 1, 0)) ) * G(h(4, 44, k159), kx[44,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t33_39 = _t33_1;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t33_36 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t33_39, _t33_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_39, _t33_1), _mm256_mul_pd(_t33_39, _t33_1), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t33_39, _t33_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_39, _t33_1), _mm256_mul_pd(_t33_39, _t33_1), 129)), _mm256_add_pd(_mm256_mul_pd(_t33_39, _t33_1), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_39, _t33_1), _mm256_mul_pd(_t33_39, _t33_1), 129)), 1));

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t33_40 = _t33_0;

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t33_37 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t33_40, _t33_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_40, _t33_0), _mm256_mul_pd(_t33_40, _t33_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t33_40, _t33_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_40, _t33_0), _mm256_mul_pd(_t33_40, _t33_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t33_40, _t33_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t33_40, _t33_0), _mm256_mul_pd(_t33_40, _t33_0), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t33_38 = _mm256_sub_pd(_t33_36, _t33_37);

  // AVX Storer:
  _t33_8 = _t33_38;


  for( int k207 = 4; k207 <= 43; k207+=4 ) {
    _t34_0 = _mm256_loadu_pd(x + k207);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t34_3 = _t34_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t34_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t34_3, _t34_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_3, _t34_0), _mm256_mul_pd(_t34_3, _t34_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t34_3, _t34_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_3, _t34_0), _mm256_mul_pd(_t34_3, _t34_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t34_3, _t34_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t34_3, _t34_0), _mm256_mul_pd(_t34_3, _t34_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t34_1 = _t33_8;

    // 4-BLAC: 1x4 + 1x4
    _t34_1 = _mm256_add_pd(_t34_1, _t34_2);

    // AVX Storer:
    _t33_8 = _t34_1;
  }

  _mm_store_sd(&(kx[40]), _mm256_castpd256_pd128(_t33_2));
  _mm_store_sd(&(kx[41]), _mm256_castpd256_pd128(_t33_4));
  _mm_store_sd(&(kx[42]), _mm256_castpd256_pd128(_t33_6));
  _mm_store_sd(&(kx[43]), _mm256_castpd256_pd128(_t33_7));

  for( int k159 = 4; k159 <= 43; k159+=4 ) {
    _t35_0 = _mm256_loadu_pd(kx + k159);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t35_3 = _t35_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t35_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t35_3, _t35_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t35_3, _t35_0), _mm256_mul_pd(_t35_3, _t35_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t35_3, _t35_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t35_3, _t35_0), _mm256_mul_pd(_t35_3, _t35_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t35_3, _t35_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t35_3, _t35_0), _mm256_mul_pd(_t35_3, _t35_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t35_1 = _t33_8;

    // 4-BLAC: 1x4 - 1x4
    _t35_1 = _mm256_sub_pd(_t35_1, _t35_2);

    // AVX Storer:
    _t33_8 = _t35_1;
  }


  // Generating : lp[1,1] = ( S(h(1, 1, 0), ( T( G(h(4, 44, 0), y[44,1],h(1, 1, 0)) ) * G(h(4, 44, 0), y[44,1],h(1, 1, 0)) ),h(1, 1, 0)) + Sum_{k159} ( $(h(1, 1, 0), ( T( G(h(4, 44, k159), y[44,1],h(1, 1, 0)) ) * G(h(4, 44, k159), y[44,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // 4-BLAC: (4x1)^T
  _t36_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32);

  // AVX Loader:

  // 4-BLAC: 1x4 * 4x1
  _t36_1 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), 129)), _mm256_add_pd(_mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_permute2f128_pd(_mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), _mm256_mul_pd(_t36_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t26_5, _t26_4), _mm256_unpacklo_pd(_t26_2, _t26_0), 32)), 129)), 1));

  // AVX Storer:
  _t36_0 = _t36_1;


  for( int k159 = 4; k159 <= 43; k159+=4 ) {
    _t37_0 = _mm256_loadu_pd(y + k159);

    // AVX Loader:

    // 4-BLAC: (4x1)^T
    _t37_3 = _t37_0;

    // AVX Loader:

    // 4-BLAC: 1x4 * 4x1
    _t37_2 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t37_3, _t37_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_3, _t37_0), _mm256_mul_pd(_t37_3, _t37_0), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t37_3, _t37_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_3, _t37_0), _mm256_mul_pd(_t37_3, _t37_0), 129)), _mm256_add_pd(_mm256_mul_pd(_t37_3, _t37_0), _mm256_permute2f128_pd(_mm256_mul_pd(_t37_3, _t37_0), _mm256_mul_pd(_t37_3, _t37_0), 129)), 1));

    // AVX Loader:

    // 1x1 -> 1x4
    _t37_1 = _t36_0;

    // 4-BLAC: 1x4 + 1x4
    _t37_1 = _mm256_add_pd(_t37_1, _t37_2);

    // AVX Storer:
    _t36_0 = _t37_1;
  }

  _mm_store_sd(&(y[3]), _mm256_castpd256_pd128(_t26_0));
  _mm_store_sd(&(y[2]), _mm256_castpd256_pd128(_t26_2));
  _mm_store_sd(&(y[1]), _mm256_castpd256_pd128(_t26_4));
  _mm_store_sd(&(y[0]), _mm256_castpd256_pd128(_t26_5));
  _mm_store_sd(&(f[0]), _mm256_castpd256_pd128(_t29_1));
  _mm_store_sd(&(var[0]), _mm256_castpd256_pd128(_t33_8));
  _mm_store_sd(&(lp[0]), _mm256_castpd256_pd128(_t36_0));

}
