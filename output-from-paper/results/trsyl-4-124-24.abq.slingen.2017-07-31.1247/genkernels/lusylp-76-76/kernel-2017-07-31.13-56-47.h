/*
 * lusylp_kernel.h
 *
Decl { {u'X': SquaredMatrix[X, (76, 76), GenMatAccess], u'C': SquaredMatrix[C, (76, 76), GenMatAccess], u'U': UpperTriangular[U, (76, 76), GenMatAccess], u'L': LowerTriangular[L, (76, 76), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_SquaredMatrix_Mul_SquaredMatrix_UpperTriangular_SquaredMatrix_opt': {'m0': 'm03.ll', 'm2': 'm22.ll'}}, 'cl1ck_v': 3, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_SquaredMatrix_Mul_SquaredMatrix_UpperTriangular_SquaredMatrix_opt_m03_m22'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
For_{fi1306;0;71;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(3, 76, fi1306 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(3, 76, fi1306 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(2, 76, fi1306 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(2, 76, fi1306 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(4, 76, fi1306)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(4, 76, fi1306)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), L[76,76],h(1, 76, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(4, 76, fi1306)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(3, 76, fi1306 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(3, 76, fi1306 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(2, 76, fi1306 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(2, 76, fi1306 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(4, 76, fi1306)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(4, 76, fi1306)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), L[76,76],h(2, 76, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 76, 0), X[76,76],h(4, 76, fi1306)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(3, 76, fi1306 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(3, 76, fi1306 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(2, 76, fi1306 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(2, 76, fi1306 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(4, 76, fi1306)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(4, 76, fi1306)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), L[76,76],h(3, 76, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 76, 0), X[76,76],h(4, 76, fi1306)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(3, 76, fi1306 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(3, 76, fi1306 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(2, 76, fi1306 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(2, 76, fi1306 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 76, 0), X[76,76],h(-fi1306 + 72, 76, fi1306 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 76, 0), X[76,76],h(-fi1306 + 72, 76, fi1306 + 4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 76, 0), X[76,76],h(4, 76, fi1306)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1306), U[76,76],h(-fi1306 + 72, 76, fi1306 + 4)) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, 0), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, 0), L[76,76],h(1, 76, 0)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(4, 76, 72)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(4, 76, 72)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), L[76,76],h(1, 76, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 0), X[76,76],h(4, 76, 72)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 1), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, 1), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, 1), L[76,76],h(1, 76, 1)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(4, 76, 72)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(4, 76, 72)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), L[76,76],h(2, 76, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 76, 0), X[76,76],h(4, 76, 72)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 2), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, 2), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, 2), L[76,76],h(1, 76, 2)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(4, 76, 72)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(4, 76, 72)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), L[76,76],h(3, 76, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 76, 0), X[76,76],h(4, 76, 72)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 3), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, 3), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, 3), L[76,76],h(1, 76, 3)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 32:
For_{fi1285;4;72;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1285), X[76,76],h(76, 76, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1285), C[76,76],h(76, 76, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1285), L[76,76],h(fi1285, 76, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1285, 76, 0), X[76,76],h(76, 76, 0)) ) ) ) )
Eq.ann: {}
Entry 1:
For_{fi1509;0;71;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(3, 76, fi1509 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(3, 76, fi1509 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(2, 76, fi1509 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(2, 76, fi1509 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, fi1509 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(4, 76, fi1509)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(4, 76, fi1509)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(4, 76, fi1509)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(3, 76, fi1509 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(3, 76, fi1509 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(2, 76, fi1509 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(2, 76, fi1509 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, fi1509 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(4, 76, fi1509)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(4, 76, fi1509)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), L[76,76],h(2, 76, fi1285)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 76, fi1285), X[76,76],h(4, 76, fi1509)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(3, 76, fi1509 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(3, 76, fi1509 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(2, 76, fi1509 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(2, 76, fi1509 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, fi1509 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(4, 76, fi1509)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(4, 76, fi1509)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), L[76,76],h(3, 76, fi1285)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 76, fi1285), X[76,76],h(4, 76, fi1509)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(3, 76, fi1509 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(3, 76, fi1509 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 1)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 1)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(2, 76, fi1509 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(2, 76, fi1509 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 2)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 2)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 3)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, fi1509 + 3)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1285), X[76,76],h(-fi1509 + 72, 76, fi1509 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1285), X[76,76],h(-fi1509 + 72, 76, fi1509 + 4)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1285), X[76,76],h(4, 76, fi1509)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 76, fi1509), U[76,76],h(-fi1509 + 72, 76, fi1509 + 4)) ) ) ) )
Eq.ann: {}
 )Entry 2:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285), L[76,76],h(1, 76, fi1285)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(4, 76, 72)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(4, 76, 72)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285), X[76,76],h(4, 76, 72)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 1), L[76,76],h(1, 76, fi1285 + 1)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(4, 76, 72)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(4, 76, 72)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), L[76,76],h(2, 76, fi1285)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 76, fi1285), X[76,76],h(4, 76, 72)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 2), L[76,76],h(1, 76, fi1285 + 2)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(4, 76, 72)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(4, 76, 72)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), L[76,76],h(3, 76, fi1285)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 76, fi1285), X[76,76],h(4, 76, 72)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 72)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 72)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(3, 76, 73)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(3, 76, 73)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 72)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 73)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 73)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(2, 76, 74)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(2, 76, 74)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 73)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 74)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 74)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 75)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 75)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 74)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 75)) ) = ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), X[76,76],h(1, 76, 75)) ) Div ( Tile( (1, 1), G(h(1, 76, fi1285 + 3), L[76,76],h(1, 76, fi1285 + 3)) ) + Tile( (1, 1), G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ) )
Eq.ann: {}
 ) *
 * Created on: 2017-07-31
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 76
#define PARAM1 76

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double const * U, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7,
	_t1_8, _t1_9, _t1_10, _t1_11, _t1_12, _t1_13, _t1_14, _t1_15,
	_t1_16, _t1_17, _t1_18, _t1_19, _t1_20, _t1_21, _t1_22, _t1_23,
	_t1_24, _t1_25, _t1_26, _t1_27;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23,
	_t2_24, _t2_25, _t2_26, _t2_27, _t2_28, _t2_29, _t2_30, _t2_31,
	_t2_32, _t2_33, _t2_34, _t2_35, _t2_36, _t2_37, _t2_38, _t2_39,
	_t2_40, _t2_41, _t2_42, _t2_43, _t2_44, _t2_45, _t2_46, _t2_47,
	_t2_48, _t2_49, _t2_50, _t2_51, _t2_52, _t2_53, _t2_54, _t2_55,
	_t2_56, _t2_57, _t2_58, _t2_59, _t2_60, _t2_61, _t2_62, _t2_63,
	_t2_64, _t2_65, _t2_66, _t2_67, _t2_68, _t2_69, _t2_70, _t2_71,
	_t2_72, _t2_73, _t2_74, _t2_75, _t2_76, _t2_77, _t2_78, _t2_79,
	_t2_80, _t2_81, _t2_82, _t2_83, _t2_84, _t2_85, _t2_86, _t2_87,
	_t2_88, _t2_89, _t2_90, _t2_91, _t2_92, _t2_93, _t2_94, _t2_95,
	_t2_96, _t2_97, _t2_98, _t2_99, _t2_100, _t2_101, _t2_102, _t2_103,
	_t2_104, _t2_105, _t2_106, _t2_107, _t2_108, _t2_109, _t2_110, _t2_111,
	_t2_112, _t2_113, _t2_114, _t2_115, _t2_116, _t2_117, _t2_118, _t2_119,
	_t2_120, _t2_121, _t2_122, _t2_123, _t2_124, _t2_125, _t2_126, _t2_127,
	_t2_128, _t2_129, _t2_130, _t2_131, _t2_132, _t2_133, _t2_134, _t2_135,
	_t2_136, _t2_137, _t2_138, _t2_139, _t2_140, _t2_141, _t2_142, _t2_143,
	_t2_144, _t2_145, _t2_146, _t2_147, _t2_148, _t2_149, _t2_150, _t2_151,
	_t2_152, _t2_153, _t2_154, _t2_155, _t2_156, _t2_157, _t2_158, _t2_159,
	_t2_160, _t2_161, _t2_162, _t2_163, _t2_164, _t2_165, _t2_166, _t2_167,
	_t2_168, _t2_169, _t2_170, _t2_171, _t2_172, _t2_173, _t2_174, _t2_175,
	_t2_176, _t2_177, _t2_178, _t2_179, _t2_180, _t2_181, _t2_182, _t2_183,
	_t2_184, _t2_185, _t2_186, _t2_187, _t2_188, _t2_189, _t2_190, _t2_191,
	_t2_192, _t2_193, _t2_194;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27, _t4_28, _t4_29, _t4_30, _t4_31,
	_t4_32, _t4_33, _t4_34, _t4_35, _t4_36, _t4_37, _t4_38, _t4_39,
	_t4_40, _t4_41, _t4_42, _t4_43, _t4_44, _t4_45, _t4_46, _t4_47,
	_t4_48, _t4_49, _t4_50, _t4_51, _t4_52, _t4_53, _t4_54, _t4_55,
	_t4_56, _t4_57, _t4_58, _t4_59, _t4_60, _t4_61, _t4_62, _t4_63,
	_t4_64, _t4_65, _t4_66, _t4_67, _t4_68, _t4_69, _t4_70, _t4_71,
	_t4_72, _t4_73, _t4_74, _t4_75, _t4_76, _t4_77, _t4_78, _t4_79,
	_t4_80, _t4_81, _t4_82, _t4_83, _t4_84, _t4_85, _t4_86, _t4_87,
	_t4_88, _t4_89, _t4_90, _t4_91, _t4_92, _t4_93, _t4_94, _t4_95,
	_t4_96, _t4_97, _t4_98, _t4_99, _t4_100, _t4_101, _t4_102, _t4_103,
	_t4_104, _t4_105, _t4_106, _t4_107, _t4_108, _t4_109, _t4_110, _t4_111,
	_t4_112, _t4_113, _t4_114, _t4_115, _t4_116, _t4_117, _t4_118, _t4_119,
	_t4_120, _t4_121, _t4_122, _t4_123, _t4_124, _t4_125, _t4_126, _t4_127,
	_t4_128, _t4_129, _t4_130, _t4_131, _t4_132, _t4_133, _t4_134, _t4_135,
	_t4_136, _t4_137, _t4_138, _t4_139, _t4_140, _t4_141, _t4_142, _t4_143,
	_t4_144, _t4_145, _t4_146, _t4_147, _t4_148, _t4_149, _t4_150, _t4_151,
	_t4_152, _t4_153, _t4_154, _t4_155, _t4_156, _t4_157, _t4_158, _t4_159,
	_t4_160, _t4_161, _t4_162, _t4_163, _t4_164, _t4_165, _t4_166, _t4_167,
	_t4_168, _t4_169, _t4_170, _t4_171, _t4_172, _t4_173, _t4_174, _t4_175,
	_t4_176, _t4_177, _t4_178, _t4_179, _t4_180, _t4_181, _t4_182, _t4_183,
	_t4_184, _t4_185, _t4_186, _t4_187, _t4_188, _t4_189, _t4_190, _t4_191,
	_t4_192, _t4_193, _t4_194;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27, _t6_28, _t6_29, _t6_30, _t6_31,
	_t6_32, _t6_33, _t6_34, _t6_35, _t6_36, _t6_37, _t6_38, _t6_39,
	_t6_40, _t6_41, _t6_42, _t6_43, _t6_44, _t6_45, _t6_46, _t6_47,
	_t6_48, _t6_49, _t6_50, _t6_51, _t6_52, _t6_53, _t6_54, _t6_55,
	_t6_56, _t6_57, _t6_58, _t6_59, _t6_60, _t6_61, _t6_62, _t6_63,
	_t6_64, _t6_65, _t6_66, _t6_67, _t6_68, _t6_69, _t6_70, _t6_71,
	_t6_72, _t6_73, _t6_74, _t6_75, _t6_76, _t6_77, _t6_78, _t6_79,
	_t6_80, _t6_81, _t6_82, _t6_83, _t6_84, _t6_85, _t6_86, _t6_87,
	_t6_88, _t6_89, _t6_90, _t6_91, _t6_92, _t6_93, _t6_94, _t6_95,
	_t6_96, _t6_97, _t6_98, _t6_99, _t6_100, _t6_101, _t6_102, _t6_103,
	_t6_104, _t6_105, _t6_106, _t6_107, _t6_108, _t6_109, _t6_110, _t6_111,
	_t6_112, _t6_113, _t6_114, _t6_115, _t6_116, _t6_117, _t6_118, _t6_119,
	_t6_120, _t6_121, _t6_122, _t6_123, _t6_124, _t6_125, _t6_126, _t6_127,
	_t6_128, _t6_129, _t6_130, _t6_131, _t6_132, _t6_133, _t6_134, _t6_135,
	_t6_136, _t6_137, _t6_138, _t6_139, _t6_140, _t6_141, _t6_142, _t6_143,
	_t6_144, _t6_145, _t6_146, _t6_147, _t6_148, _t6_149, _t6_150, _t6_151,
	_t6_152, _t6_153, _t6_154, _t6_155, _t6_156, _t6_157, _t6_158, _t6_159,
	_t6_160, _t6_161, _t6_162, _t6_163, _t6_164, _t6_165, _t6_166, _t6_167,
	_t6_168, _t6_169, _t6_170, _t6_171, _t6_172, _t6_173, _t6_174, _t6_175,
	_t6_176, _t6_177, _t6_178, _t6_179, _t6_180, _t6_181, _t6_182, _t6_183,
	_t6_184, _t6_185, _t6_186, _t6_187;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27, _t9_28, _t9_29, _t9_30, _t9_31,
	_t9_32, _t9_33, _t9_34, _t9_35, _t9_36, _t9_37, _t9_38, _t9_39,
	_t9_40, _t9_41, _t9_42, _t9_43, _t9_44, _t9_45, _t9_46, _t9_47,
	_t9_48, _t9_49, _t9_50, _t9_51, _t9_52, _t9_53, _t9_54, _t9_55,
	_t9_56, _t9_57, _t9_58, _t9_59, _t9_60, _t9_61, _t9_62, _t9_63,
	_t9_64, _t9_65, _t9_66, _t9_67, _t9_68, _t9_69, _t9_70, _t9_71,
	_t9_72, _t9_73, _t9_74, _t9_75, _t9_76, _t9_77, _t9_78, _t9_79,
	_t9_80, _t9_81, _t9_82, _t9_83, _t9_84, _t9_85, _t9_86, _t9_87,
	_t9_88, _t9_89, _t9_90, _t9_91, _t9_92, _t9_93, _t9_94, _t9_95,
	_t9_96, _t9_97, _t9_98, _t9_99, _t9_100, _t9_101, _t9_102, _t9_103,
	_t9_104, _t9_105, _t9_106, _t9_107, _t9_108, _t9_109, _t9_110, _t9_111,
	_t9_112, _t9_113, _t9_114, _t9_115, _t9_116, _t9_117, _t9_118, _t9_119,
	_t9_120, _t9_121, _t9_122, _t9_123, _t9_124, _t9_125, _t9_126, _t9_127,
	_t9_128, _t9_129, _t9_130, _t9_131, _t9_132, _t9_133, _t9_134, _t9_135,
	_t9_136, _t9_137, _t9_138, _t9_139, _t9_140, _t9_141, _t9_142, _t9_143,
	_t9_144, _t9_145, _t9_146, _t9_147, _t9_148, _t9_149, _t9_150, _t9_151,
	_t9_152, _t9_153, _t9_154, _t9_155, _t9_156, _t9_157, _t9_158, _t9_159,
	_t9_160, _t9_161, _t9_162, _t9_163, _t9_164, _t9_165, _t9_166, _t9_167,
	_t9_168, _t9_169, _t9_170, _t9_171, _t9_172, _t9_173, _t9_174, _t9_175,
	_t9_176, _t9_177, _t9_178, _t9_179, _t9_180, _t9_181, _t9_182, _t9_183,
	_t9_184, _t9_185, _t9_186, _t9_187, _t9_188, _t9_189, _t9_190, _t9_191,
	_t9_192, _t9_193, _t9_194;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35, _t11_36, _t11_37, _t11_38, _t11_39,
	_t11_40, _t11_41, _t11_42, _t11_43, _t11_44, _t11_45, _t11_46, _t11_47,
	_t11_48, _t11_49, _t11_50, _t11_51, _t11_52, _t11_53, _t11_54, _t11_55,
	_t11_56, _t11_57, _t11_58, _t11_59, _t11_60, _t11_61, _t11_62, _t11_63,
	_t11_64, _t11_65, _t11_66, _t11_67, _t11_68, _t11_69, _t11_70, _t11_71,
	_t11_72, _t11_73, _t11_74, _t11_75, _t11_76, _t11_77, _t11_78, _t11_79,
	_t11_80, _t11_81, _t11_82, _t11_83, _t11_84, _t11_85, _t11_86, _t11_87,
	_t11_88, _t11_89, _t11_90, _t11_91, _t11_92, _t11_93, _t11_94, _t11_95,
	_t11_96, _t11_97, _t11_98, _t11_99, _t11_100, _t11_101, _t11_102, _t11_103,
	_t11_104, _t11_105, _t11_106, _t11_107, _t11_108, _t11_109, _t11_110, _t11_111,
	_t11_112, _t11_113, _t11_114, _t11_115, _t11_116, _t11_117, _t11_118, _t11_119,
	_t11_120, _t11_121, _t11_122, _t11_123, _t11_124, _t11_125, _t11_126, _t11_127,
	_t11_128, _t11_129, _t11_130, _t11_131, _t11_132, _t11_133, _t11_134, _t11_135,
	_t11_136, _t11_137, _t11_138, _t11_139, _t11_140, _t11_141, _t11_142, _t11_143,
	_t11_144, _t11_145, _t11_146, _t11_147, _t11_148, _t11_149, _t11_150, _t11_151,
	_t11_152, _t11_153, _t11_154, _t11_155, _t11_156, _t11_157, _t11_158, _t11_159,
	_t11_160, _t11_161, _t11_162, _t11_163, _t11_164, _t11_165, _t11_166, _t11_167,
	_t11_168, _t11_169, _t11_170, _t11_171, _t11_172, _t11_173, _t11_174, _t11_175,
	_t11_176, _t11_177, _t11_178, _t11_179, _t11_180, _t11_181, _t11_182, _t11_183,
	_t11_184, _t11_185, _t11_186, _t11_187;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23,
	_t12_24, _t12_25, _t12_26, _t12_27;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35, _t14_36, _t14_37, _t14_38, _t14_39,
	_t14_40, _t14_41, _t14_42, _t14_43, _t14_44, _t14_45, _t14_46, _t14_47,
	_t14_48, _t14_49, _t14_50, _t14_51, _t14_52, _t14_53, _t14_54, _t14_55,
	_t14_56, _t14_57, _t14_58, _t14_59, _t14_60, _t14_61, _t14_62, _t14_63,
	_t14_64, _t14_65, _t14_66, _t14_67, _t14_68, _t14_69, _t14_70, _t14_71,
	_t14_72, _t14_73, _t14_74, _t14_75, _t14_76, _t14_77, _t14_78, _t14_79,
	_t14_80, _t14_81, _t14_82, _t14_83, _t14_84, _t14_85, _t14_86, _t14_87,
	_t14_88, _t14_89, _t14_90, _t14_91, _t14_92, _t14_93, _t14_94, _t14_95,
	_t14_96, _t14_97, _t14_98, _t14_99, _t14_100, _t14_101, _t14_102, _t14_103,
	_t14_104, _t14_105, _t14_106, _t14_107, _t14_108, _t14_109, _t14_110, _t14_111,
	_t14_112, _t14_113, _t14_114, _t14_115, _t14_116, _t14_117, _t14_118, _t14_119,
	_t14_120, _t14_121, _t14_122, _t14_123, _t14_124, _t14_125, _t14_126, _t14_127,
	_t14_128, _t14_129, _t14_130, _t14_131, _t14_132, _t14_133, _t14_134, _t14_135,
	_t14_136, _t14_137, _t14_138, _t14_139, _t14_140, _t14_141, _t14_142, _t14_143,
	_t14_144, _t14_145, _t14_146, _t14_147, _t14_148, _t14_149, _t14_150, _t14_151,
	_t14_152, _t14_153, _t14_154, _t14_155, _t14_156, _t14_157, _t14_158, _t14_159,
	_t14_160, _t14_161, _t14_162, _t14_163, _t14_164, _t14_165, _t14_166, _t14_167,
	_t14_168, _t14_169, _t14_170, _t14_171, _t14_172, _t14_173, _t14_174, _t14_175,
	_t14_176, _t14_177, _t14_178, _t14_179, _t14_180, _t14_181, _t14_182, _t14_183,
	_t14_184, _t14_185, _t14_186, _t14_187, _t14_188, _t14_189, _t14_190, _t14_191,
	_t14_192, _t14_193, _t14_194;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35, _t16_36, _t16_37, _t16_38, _t16_39,
	_t16_40, _t16_41, _t16_42, _t16_43, _t16_44, _t16_45, _t16_46, _t16_47,
	_t16_48, _t16_49, _t16_50, _t16_51, _t16_52, _t16_53, _t16_54, _t16_55,
	_t16_56, _t16_57, _t16_58, _t16_59, _t16_60, _t16_61, _t16_62, _t16_63,
	_t16_64, _t16_65, _t16_66, _t16_67, _t16_68, _t16_69, _t16_70, _t16_71,
	_t16_72, _t16_73, _t16_74, _t16_75, _t16_76, _t16_77, _t16_78, _t16_79,
	_t16_80, _t16_81, _t16_82, _t16_83, _t16_84, _t16_85, _t16_86, _t16_87,
	_t16_88, _t16_89, _t16_90, _t16_91, _t16_92, _t16_93, _t16_94, _t16_95,
	_t16_96, _t16_97, _t16_98, _t16_99, _t16_100, _t16_101, _t16_102, _t16_103,
	_t16_104, _t16_105, _t16_106, _t16_107, _t16_108, _t16_109, _t16_110, _t16_111,
	_t16_112, _t16_113, _t16_114, _t16_115, _t16_116, _t16_117, _t16_118, _t16_119,
	_t16_120, _t16_121, _t16_122, _t16_123, _t16_124, _t16_125, _t16_126, _t16_127,
	_t16_128, _t16_129, _t16_130, _t16_131, _t16_132, _t16_133, _t16_134, _t16_135,
	_t16_136, _t16_137, _t16_138, _t16_139, _t16_140, _t16_141, _t16_142, _t16_143,
	_t16_144, _t16_145, _t16_146, _t16_147, _t16_148, _t16_149, _t16_150, _t16_151,
	_t16_152, _t16_153, _t16_154, _t16_155, _t16_156;


  for( int fi1306 = 0; fi1306 <= 71; fi1306+=4 ) {
    _t0_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306])));
    _t0_13 = _mm256_castpd128_pd256(_mm_load_sd(&(L[0])));
    _t0_12 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1306])));
    _t0_15 = _mm256_maskload_pd(C + fi1306 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t0_11 = _mm256_maskload_pd(U + 77*fi1306 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t0_10 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1306 + 77])));
    _t0_9 = _mm256_maskload_pd(U + 77*fi1306 + 78, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t0_8 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1306 + 154])));
    _t0_7 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1306 + 155])));
    _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1306 + 231])));
    _t0_41 = _asm256_loadu_pd(C + fi1306 + 76);
    _t0_5 = _mm256_broadcast_sd(&(L[76]));
    _t0_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77])));
    _t0_42 = _asm256_loadu_pd(C + fi1306 + 152);
    _t0_3 = _mm256_maskload_pd(L + 152, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t0_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[154])));
    _t0_43 = _asm256_loadu_pd(C + fi1306 + 228);
    _t0_1 = _mm256_maskload_pd(L + 228, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[231])));

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ),h(1, 76, fi1306))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_93 = _t0_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_98 = _t0_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_104 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t0_119 = _mm256_add_pd(_t0_98, _t0_104);

    // 4-BLAC: 1x4 / 1x4
    _t0_136 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_93), _mm256_castpd256_pd128(_t0_119)));

    // AVX Storer:
    _t0_14 = _t0_136;

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(3, 76, fi1306 + 1)) - ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ),h(3, 76, fi1306 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_151 = _t0_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_157 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_14, _t0_14, 32), _mm256_permute2f128_pd(_t0_14, _t0_14, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_163 = _t0_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_166 = _mm256_mul_pd(_t0_157, _t0_163);

    // 4-BLAC: 1x4 - 1x4
    _t0_167 = _mm256_sub_pd(_t0_151, _t0_166);

    // AVX Storer:
    _t0_15 = _t0_167;

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 1)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ),h(1, 76, fi1306 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_168 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_169 = _t0_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_170 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t0_171 = _mm256_add_pd(_t0_169, _t0_170);

    // 4-BLAC: 1x4 / 1x4
    _t0_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_168), _mm256_castpd256_pd128(_t0_171)));

    // AVX Storer:
    _t0_16 = _t0_172;

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(2, 76, fi1306 + 2)) - ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 1)) Kro G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ),h(2, 76, fi1306 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_173 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_15, 6), _mm256_permute2f128_pd(_t0_15, _t0_15, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_174 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_16, _t0_16, 32), _mm256_permute2f128_pd(_t0_16, _t0_16, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_175 = _t0_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_176 = _mm256_mul_pd(_t0_174, _t0_175);

    // 4-BLAC: 1x4 - 1x4
    _t0_177 = _mm256_sub_pd(_t0_173, _t0_176);

    // AVX Storer:
    _t0_17 = _t0_177;

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 2)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ),h(1, 76, fi1306 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_178 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_17, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_179 = _t0_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_180 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t0_181 = _mm256_add_pd(_t0_179, _t0_180);

    // 4-BLAC: 1x4 / 1x4
    _t0_182 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_178), _mm256_castpd256_pd128(_t0_181)));

    // AVX Storer:
    _t0_18 = _t0_182;

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 3)) - ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 2)) Kro G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_183 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_17, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_184 = _t0_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_185 = _t0_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_186 = _mm256_mul_pd(_t0_184, _t0_185);

    // 4-BLAC: 1x4 - 1x4
    _t0_187 = _mm256_sub_pd(_t0_183, _t0_186);

    // AVX Storer:
    _t0_19 = _t0_187;

    // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, fi1306 + 3)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_188 = _t0_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_189 = _t0_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_190 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_191 = _mm256_add_pd(_t0_189, _t0_190);

    // 4-BLAC: 1x4 / 1x4
    _t0_192 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_188), _mm256_castpd256_pd128(_t0_191)));

    // AVX Storer:
    _t0_19 = _t0_192;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(4, 76, fi1306)) - ( G(h(1, 76, 1), L[76,76],h(1, 76, 0)) Kro G(h(1, 76, 0), X[76,76],h(4, 76, fi1306)) ) ),h(4, 76, fi1306))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_193 = _t0_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t0_38 = _mm256_mul_pd(_t0_193, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_16), _mm256_unpacklo_pd(_t0_18, _t0_19), 32));

    // 4-BLAC: 1x4 - 1x4
    _t0_41 = _mm256_sub_pd(_t0_41, _t0_38);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ),h(1, 76, fi1306))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_194 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_41, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_44 = _t0_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_45 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t0_46 = _mm256_add_pd(_t0_44, _t0_45);

    // 4-BLAC: 1x4 / 1x4
    _t0_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_194), _mm256_castpd256_pd128(_t0_46)));

    // AVX Storer:
    _t0_20 = _t0_47;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(3, 76, fi1306 + 1)) - ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ),h(3, 76, fi1306 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_48 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_41, 14), _mm256_permute2f128_pd(_t0_41, _t0_41, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_20, _t0_20, 32), _mm256_permute2f128_pd(_t0_20, _t0_20, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_50 = _t0_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_51 = _mm256_mul_pd(_t0_49, _t0_50);

    // 4-BLAC: 1x4 - 1x4
    _t0_52 = _mm256_sub_pd(_t0_48, _t0_51);

    // AVX Storer:
    _t0_21 = _t0_52;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 1)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ),h(1, 76, fi1306 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_53 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_54 = _t0_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_55 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t0_56 = _mm256_add_pd(_t0_54, _t0_55);

    // 4-BLAC: 1x4 / 1x4
    _t0_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_53), _mm256_castpd256_pd128(_t0_56)));

    // AVX Storer:
    _t0_22 = _t0_57;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(2, 76, fi1306 + 2)) - ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 1)) Kro G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ),h(2, 76, fi1306 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_58 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_21, 6), _mm256_permute2f128_pd(_t0_21, _t0_21, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_22, _t0_22, 32), _mm256_permute2f128_pd(_t0_22, _t0_22, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_60 = _t0_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_61 = _mm256_mul_pd(_t0_59, _t0_60);

    // 4-BLAC: 1x4 - 1x4
    _t0_62 = _mm256_sub_pd(_t0_58, _t0_61);

    // AVX Storer:
    _t0_23 = _t0_62;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 2)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ),h(1, 76, fi1306 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_63 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_23, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_64 = _t0_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_65 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t0_66 = _mm256_add_pd(_t0_64, _t0_65);

    // 4-BLAC: 1x4 / 1x4
    _t0_67 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_63), _mm256_castpd256_pd128(_t0_66)));

    // AVX Storer:
    _t0_24 = _t0_67;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 3)) - ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 2)) Kro G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_68 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_23, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_69 = _t0_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_70 = _t0_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_71 = _mm256_mul_pd(_t0_69, _t0_70);

    // 4-BLAC: 1x4 - 1x4
    _t0_72 = _mm256_sub_pd(_t0_68, _t0_71);

    // AVX Storer:
    _t0_25 = _t0_72;

    // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, fi1306 + 3)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_73 = _t0_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_74 = _t0_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_75 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_76 = _mm256_add_pd(_t0_74, _t0_75);

    // 4-BLAC: 1x4 / 1x4
    _t0_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_73), _mm256_castpd256_pd128(_t0_76)));

    // AVX Storer:
    _t0_25 = _t0_77;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(4, 76, fi1306)) - ( G(h(1, 76, 2), L[76,76],h(2, 76, 0)) * G(h(2, 76, 0), X[76,76],h(4, 76, fi1306)) ) ),h(4, 76, fi1306))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_78 = _t0_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t0_79 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_16), _mm256_unpacklo_pd(_t0_18, _t0_19), 32);
    _t0_80 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_20, _t0_22), _mm256_unpacklo_pd(_t0_24, _t0_25), 32);
    _t0_81 = _mm256_setzero_pd();
    _t0_82 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t0_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_78, _t0_78, 32), _mm256_permute2f128_pd(_t0_78, _t0_78, 32), 0), _t0_79), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_78, _t0_78, 32), _mm256_permute2f128_pd(_t0_78, _t0_78, 32), 15), _t0_80)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_78, _t0_78, 49), _mm256_permute2f128_pd(_t0_78, _t0_78, 49), 0), _t0_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_78, _t0_78, 49), _mm256_permute2f128_pd(_t0_78, _t0_78, 49), 15), _t0_82)));

    // 4-BLAC: 1x4 - 1x4
    _t0_42 = _mm256_sub_pd(_t0_42, _t0_39);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ),h(1, 76, fi1306))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_83 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_42, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_84 = _t0_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_85 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t0_86 = _mm256_add_pd(_t0_84, _t0_85);

    // 4-BLAC: 1x4 / 1x4
    _t0_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_83), _mm256_castpd256_pd128(_t0_86)));

    // AVX Storer:
    _t0_26 = _t0_87;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(3, 76, fi1306 + 1)) - ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ),h(3, 76, fi1306 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_88 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_42, 14), _mm256_permute2f128_pd(_t0_42, _t0_42, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_89 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_26, _t0_26, 32), _mm256_permute2f128_pd(_t0_26, _t0_26, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_90 = _t0_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_91 = _mm256_mul_pd(_t0_89, _t0_90);

    // 4-BLAC: 1x4 - 1x4
    _t0_92 = _mm256_sub_pd(_t0_88, _t0_91);

    // AVX Storer:
    _t0_27 = _t0_92;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 1)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ),h(1, 76, fi1306 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_94 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_27, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_95 = _t0_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_96 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t0_97 = _mm256_add_pd(_t0_95, _t0_96);

    // 4-BLAC: 1x4 / 1x4
    _t0_99 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_94), _mm256_castpd256_pd128(_t0_97)));

    // AVX Storer:
    _t0_28 = _t0_99;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(2, 76, fi1306 + 2)) - ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 1)) Kro G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ),h(2, 76, fi1306 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_100 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_27, 6), _mm256_permute2f128_pd(_t0_27, _t0_27, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_101 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_28, _t0_28, 32), _mm256_permute2f128_pd(_t0_28, _t0_28, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_102 = _t0_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_103 = _mm256_mul_pd(_t0_101, _t0_102);

    // 4-BLAC: 1x4 - 1x4
    _t0_105 = _mm256_sub_pd(_t0_100, _t0_103);

    // AVX Storer:
    _t0_29 = _t0_105;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 2)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ),h(1, 76, fi1306 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_106 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_29, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_107 = _t0_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_108 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t0_109 = _mm256_add_pd(_t0_107, _t0_108);

    // 4-BLAC: 1x4 / 1x4
    _t0_110 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_106), _mm256_castpd256_pd128(_t0_109)));

    // AVX Storer:
    _t0_30 = _t0_110;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 3)) - ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 2)) Kro G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_111 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_29, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_112 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_113 = _t0_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_114 = _mm256_mul_pd(_t0_112, _t0_113);

    // 4-BLAC: 1x4 - 1x4
    _t0_115 = _mm256_sub_pd(_t0_111, _t0_114);

    // AVX Storer:
    _t0_31 = _t0_115;

    // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, fi1306 + 3)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_116 = _t0_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_117 = _t0_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_118 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_120 = _mm256_add_pd(_t0_117, _t0_118);

    // 4-BLAC: 1x4 / 1x4
    _t0_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_116), _mm256_castpd256_pd128(_t0_120)));

    // AVX Storer:
    _t0_31 = _t0_121;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(4, 76, fi1306)) - ( G(h(1, 76, 3), L[76,76],h(3, 76, 0)) * G(h(3, 76, 0), X[76,76],h(4, 76, fi1306)) ) ),h(4, 76, fi1306))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_122 = _t0_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t0_123 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_14, _t0_16), _mm256_unpacklo_pd(_t0_18, _t0_19), 32);
    _t0_124 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_20, _t0_22), _mm256_unpacklo_pd(_t0_24, _t0_25), 32);
    _t0_125 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_26, _t0_28), _mm256_unpacklo_pd(_t0_30, _t0_31), 32);
    _t0_126 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t0_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_122, _t0_122, 32), _mm256_permute2f128_pd(_t0_122, _t0_122, 32), 0), _t0_123), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_122, _t0_122, 32), _mm256_permute2f128_pd(_t0_122, _t0_122, 32), 15), _t0_124)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_122, _t0_122, 49), _mm256_permute2f128_pd(_t0_122, _t0_122, 49), 0), _t0_125), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_122, _t0_122, 49), _mm256_permute2f128_pd(_t0_122, _t0_122, 49), 15), _t0_126)));

    // 4-BLAC: 1x4 - 1x4
    _t0_43 = _mm256_sub_pd(_t0_43, _t0_40);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, fi1306), U[76,76],h(1, 76, fi1306)) ) ),h(1, 76, fi1306))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_127 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_43, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_128 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_129 = _t0_12;

    // 4-BLAC: 1x4 + 1x4
    _t0_130 = _mm256_add_pd(_t0_128, _t0_129);

    // 4-BLAC: 1x4 / 1x4
    _t0_131 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_127), _mm256_castpd256_pd128(_t0_130)));

    // AVX Storer:
    _t0_32 = _t0_131;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(3, 76, fi1306 + 1)) - ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), U[76,76],h(3, 76, fi1306 + 1)) ) ),h(3, 76, fi1306 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_132 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_43, 14), _mm256_permute2f128_pd(_t0_43, _t0_43, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_133 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_32, _t0_32, 32), _mm256_permute2f128_pd(_t0_32, _t0_32, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_134 = _t0_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_135 = _mm256_mul_pd(_t0_133, _t0_134);

    // 4-BLAC: 1x4 - 1x4
    _t0_137 = _mm256_sub_pd(_t0_132, _t0_135);

    // AVX Storer:
    _t0_33 = _t0_137;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 1)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, fi1306 + 1), U[76,76],h(1, 76, fi1306 + 1)) ) ),h(1, 76, fi1306 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_138 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_33, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_139 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_140 = _t0_10;

    // 4-BLAC: 1x4 + 1x4
    _t0_141 = _mm256_add_pd(_t0_139, _t0_140);

    // 4-BLAC: 1x4 / 1x4
    _t0_142 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_138), _mm256_castpd256_pd128(_t0_141)));

    // AVX Storer:
    _t0_34 = _t0_142;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(2, 76, fi1306 + 2)) - ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 1)) Kro G(h(1, 76, fi1306 + 1), U[76,76],h(2, 76, fi1306 + 2)) ) ),h(2, 76, fi1306 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_143 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_33, 6), _mm256_permute2f128_pd(_t0_33, _t0_33, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_144 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_34, _t0_34, 32), _mm256_permute2f128_pd(_t0_34, _t0_34, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_145 = _t0_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_146 = _mm256_mul_pd(_t0_144, _t0_145);

    // 4-BLAC: 1x4 - 1x4
    _t0_147 = _mm256_sub_pd(_t0_143, _t0_146);

    // AVX Storer:
    _t0_35 = _t0_147;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 2)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 2)) ) ),h(1, 76, fi1306 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_148 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_35, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_149 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_150 = _t0_8;

    // 4-BLAC: 1x4 + 1x4
    _t0_152 = _mm256_add_pd(_t0_149, _t0_150);

    // 4-BLAC: 1x4 / 1x4
    _t0_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_148), _mm256_castpd256_pd128(_t0_152)));

    // AVX Storer:
    _t0_36 = _t0_153;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 3)) - ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 2)) Kro G(h(1, 76, fi1306 + 2), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_154 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_35, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_155 = _t0_36;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_156 = _t0_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_158 = _mm256_mul_pd(_t0_155, _t0_156);

    // 4-BLAC: 1x4 - 1x4
    _t0_159 = _mm256_sub_pd(_t0_154, _t0_158);

    // AVX Storer:
    _t0_37 = _t0_159;

    // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, fi1306 + 3)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, fi1306 + 3), U[76,76],h(1, 76, fi1306 + 3)) ) ),h(1, 76, fi1306 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_160 = _t0_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_161 = _t0_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_162 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_164 = _mm256_add_pd(_t0_161, _t0_162);

    // 4-BLAC: 1x4 / 1x4
    _t0_165 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_160), _mm256_castpd256_pd128(_t0_164)));

    // AVX Storer:
    _t0_37 = _t0_165;

    // Generating : X[76,76] = Sum_{j123} ( S(h(4, 76, 0), ( G(h(4, 76, 0), X[76,76],h(4, 76, fi1306 + j123 + 4)) - ( G(h(4, 76, 0), X[76,76],h(4, 76, fi1306)) * G(h(4, 76, fi1306), U[76,76],h(4, 76, fi1306 + j123 + 4)) ) ),h(4, 76, fi1306 + j123 + 4)) )

    // AVX Loader:
    _mm_store_sd(&(C[fi1306]), _mm256_castpd256_pd128(_t0_14));
    _mm_store_sd(&(C[fi1306 + 1]), _mm256_castpd256_pd128(_t0_16));
    _mm_store_sd(&(C[fi1306 + 2]), _mm256_castpd256_pd128(_t0_18));
    _mm_store_sd(&(C[fi1306 + 3]), _mm256_castpd256_pd128(_t0_19));
    _mm_store_sd(&(C[fi1306 + 76]), _mm256_castpd256_pd128(_t0_20));
    _mm_store_sd(&(C[fi1306 + 77]), _mm256_castpd256_pd128(_t0_22));
    _mm_store_sd(&(C[fi1306 + 78]), _mm256_castpd256_pd128(_t0_24));
    _mm_store_sd(&(C[fi1306 + 79]), _mm256_castpd256_pd128(_t0_25));
    _mm_store_sd(&(C[fi1306 + 152]), _mm256_castpd256_pd128(_t0_26));
    _mm_store_sd(&(C[fi1306 + 153]), _mm256_castpd256_pd128(_t0_28));
    _mm_store_sd(&(C[fi1306 + 154]), _mm256_castpd256_pd128(_t0_30));
    _mm_store_sd(&(C[fi1306 + 155]), _mm256_castpd256_pd128(_t0_31));
    _mm_store_sd(&(C[fi1306 + 228]), _mm256_castpd256_pd128(_t0_32));
    _mm_store_sd(&(C[fi1306 + 229]), _mm256_castpd256_pd128(_t0_34));
    _mm_store_sd(&(C[fi1306 + 230]), _mm256_castpd256_pd128(_t0_36));
    _mm_store_sd(&(C[fi1306 + 231]), _mm256_castpd256_pd128(_t0_37));

    for( int j123 = 0; j123 <= -fi1306 + 71; j123+=4 ) {
      _t1_24 = _asm256_loadu_pd(C + fi1306 + j123 + 4);
      _t1_25 = _asm256_loadu_pd(C + fi1306 + j123 + 80);
      _t1_26 = _asm256_loadu_pd(C + fi1306 + j123 + 156);
      _t1_27 = _asm256_loadu_pd(C + fi1306 + j123 + 232);
      _t1_19 = _asm256_loadu_pd(U + 77*fi1306 + j123 + 4);
      _t1_18 = _asm256_loadu_pd(U + 77*fi1306 + j123 + 80);
      _t1_17 = _asm256_loadu_pd(U + 77*fi1306 + j123 + 156);
      _t1_16 = _asm256_loadu_pd(U + 77*fi1306 + j123 + 232);
      _t1_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306])));
      _t1_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 1])));
      _t1_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 2])));
      _t1_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 3])));
      _t1_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 76])));
      _t1_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 77])));
      _t1_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 78])));
      _t1_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 79])));
      _t1_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 152])));
      _t1_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 153])));
      _t1_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 154])));
      _t1_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 155])));
      _t1_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 228])));
      _t1_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 229])));
      _t1_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 230])));
      _t1_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1306 + 231])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t1_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_15, _t1_15, 32), _mm256_permute2f128_pd(_t1_15, _t1_15, 32), 0), _t1_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_14, _t1_14, 32), _mm256_permute2f128_pd(_t1_14, _t1_14, 32), 0), _t1_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_13, _t1_13, 32), _mm256_permute2f128_pd(_t1_13, _t1_13, 32), 0), _t1_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_12, _t1_12, 32), _mm256_permute2f128_pd(_t1_12, _t1_12, 32), 0), _t1_16)));
      _t1_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_11, _t1_11, 32), _mm256_permute2f128_pd(_t1_11, _t1_11, 32), 0), _t1_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_10, _t1_10, 32), _mm256_permute2f128_pd(_t1_10, _t1_10, 32), 0), _t1_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_9, _t1_9, 32), _mm256_permute2f128_pd(_t1_9, _t1_9, 32), 0), _t1_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_8, _t1_8, 32), _mm256_permute2f128_pd(_t1_8, _t1_8, 32), 0), _t1_16)));
      _t1_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_7, _t1_7, 32), _mm256_permute2f128_pd(_t1_7, _t1_7, 32), 0), _t1_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_6, _t1_6, 32), _mm256_permute2f128_pd(_t1_6, _t1_6, 32), 0), _t1_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_5, _t1_5, 32), _mm256_permute2f128_pd(_t1_5, _t1_5, 32), 0), _t1_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_4, _t1_4, 32), _mm256_permute2f128_pd(_t1_4, _t1_4, 32), 0), _t1_16)));
      _t1_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_3, _t1_3, 32), _mm256_permute2f128_pd(_t1_3, _t1_3, 32), 0), _t1_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_2, _t1_2, 32), _mm256_permute2f128_pd(_t1_2, _t1_2, 32), 0), _t1_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_1, _t1_1, 32), _mm256_permute2f128_pd(_t1_1, _t1_1, 32), 0), _t1_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t1_0, _t1_0, 32), _mm256_permute2f128_pd(_t1_0, _t1_0, 32), 0), _t1_16)));

      // 4-BLAC: 4x4 - 4x4
      _t1_24 = _mm256_sub_pd(_t1_24, _t1_20);
      _t1_25 = _mm256_sub_pd(_t1_25, _t1_21);
      _t1_26 = _mm256_sub_pd(_t1_26, _t1_22);
      _t1_27 = _mm256_sub_pd(_t1_27, _t1_23);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1306 + j123 + 4, _t1_24);
      _asm256_storeu_pd(C + fi1306 + j123 + 80, _t1_25);
      _asm256_storeu_pd(C + fi1306 + j123 + 156, _t1_26);
      _asm256_storeu_pd(C + fi1306 + j123 + 232, _t1_27);
    }
  }

  _t2_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[72])));
  _t2_13 = _mm256_castpd128_pd256(_mm_load_sd(&(L[0])));
  _t2_12 = _mm256_castpd128_pd256(_mm_load_sd(&(U[5544])));
  _t2_15 = _mm256_maskload_pd(C + 73, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t2_11 = _mm256_maskload_pd(U + 5545, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t2_10 = _mm256_castpd128_pd256(_mm_load_sd(&(U[5621])));
  _t2_9 = _mm256_maskload_pd(U + 5622, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t2_8 = _mm256_castpd128_pd256(_mm_load_sd(&(U[5698])));
  _t2_7 = _mm256_castpd128_pd256(_mm_load_sd(&(U[5699])));
  _t2_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[5775])));
  _t2_43 = _asm256_loadu_pd(C + 148);
  _t2_5 = _mm256_broadcast_sd(&(L[76]));
  _t2_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77])));
  _t2_41 = _asm256_loadu_pd(C + 224);
  _t2_3 = _mm256_maskload_pd(L + 152, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t2_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[154])));
  _t2_42 = _asm256_loadu_pd(C + 300);
  _t2_1 = _mm256_maskload_pd(L + 228, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t2_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[231])));

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_44 = _t2_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_45 = _t2_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_46 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_47 = _mm256_add_pd(_t2_45, _t2_46);

  // 4-BLAC: 1x4 / 1x4
  _t2_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_44), _mm256_castpd256_pd128(_t2_47)));

  // AVX Storer:
  _t2_14 = _t2_48;

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 0), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_49 = _t2_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_14, _t2_14, 32), _mm256_permute2f128_pd(_t2_14, _t2_14, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_51 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_52 = _mm256_mul_pd(_t2_50, _t2_51);

  // 4-BLAC: 1x4 - 1x4
  _t2_53 = _mm256_sub_pd(_t2_49, _t2_52);

  // AVX Storer:
  _t2_15 = _t2_53;

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_54 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_55 = _t2_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_56 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_57 = _mm256_add_pd(_t2_55, _t2_56);

  // 4-BLAC: 1x4 / 1x4
  _t2_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_54), _mm256_castpd256_pd128(_t2_57)));

  // AVX Storer:
  _t2_16 = _t2_58;

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 0), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_59 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_15, 6), _mm256_permute2f128_pd(_t2_15, _t2_15, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_60 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_16, _t2_16, 32), _mm256_permute2f128_pd(_t2_16, _t2_16, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_61 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_62 = _mm256_mul_pd(_t2_60, _t2_61);

  // 4-BLAC: 1x4 - 1x4
  _t2_63 = _mm256_sub_pd(_t2_59, _t2_62);

  // AVX Storer:
  _t2_17 = _t2_63;

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_64 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_17, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_65 = _t2_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_66 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t2_67 = _mm256_add_pd(_t2_65, _t2_66);

  // 4-BLAC: 1x4 / 1x4
  _t2_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_64), _mm256_castpd256_pd128(_t2_67)));

  // AVX Storer:
  _t2_18 = _t2_68;

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 0), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_69 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_17, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_70 = _t2_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_71 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_72 = _mm256_mul_pd(_t2_70, _t2_71);

  // 4-BLAC: 1x4 - 1x4
  _t2_73 = _mm256_sub_pd(_t2_69, _t2_72);

  // AVX Storer:
  _t2_19 = _t2_73;

  // Generating : X[76,76] = S(h(1, 76, 0), ( G(h(1, 76, 0), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 0), L[76,76],h(1, 76, 0)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_74 = _t2_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_75 = _t2_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_76 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_77 = _mm256_add_pd(_t2_75, _t2_76);

  // 4-BLAC: 1x4 / 1x4
  _t2_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_74), _mm256_castpd256_pd128(_t2_77)));

  // AVX Storer:
  _t2_19 = _t2_78;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, 1), L[76,76],h(1, 76, 0)) Kro G(h(1, 76, 0), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_79 = _t2_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t2_38 = _mm256_mul_pd(_t2_79, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_16), _mm256_unpacklo_pd(_t2_18, _t2_19), 32));

  // 4-BLAC: 1x4 - 1x4
  _t2_43 = _mm256_sub_pd(_t2_43, _t2_38);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_80 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_43, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_81 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_82 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_83 = _mm256_add_pd(_t2_81, _t2_82);

  // 4-BLAC: 1x4 / 1x4
  _t2_84 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_80), _mm256_castpd256_pd128(_t2_83)));

  // AVX Storer:
  _t2_20 = _t2_84;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 1), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_85 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_43, 14), _mm256_permute2f128_pd(_t2_43, _t2_43, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_86 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_20, _t2_20, 32), _mm256_permute2f128_pd(_t2_20, _t2_20, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_87 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_88 = _mm256_mul_pd(_t2_86, _t2_87);

  // 4-BLAC: 1x4 - 1x4
  _t2_89 = _mm256_sub_pd(_t2_85, _t2_88);

  // AVX Storer:
  _t2_21 = _t2_89;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_90 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_91 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_92 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_93 = _mm256_add_pd(_t2_91, _t2_92);

  // 4-BLAC: 1x4 / 1x4
  _t2_94 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_90), _mm256_castpd256_pd128(_t2_93)));

  // AVX Storer:
  _t2_22 = _t2_94;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 1), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_95 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_21, 6), _mm256_permute2f128_pd(_t2_21, _t2_21, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_96 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_22, _t2_22, 32), _mm256_permute2f128_pd(_t2_22, _t2_22, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_97 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_98 = _mm256_mul_pd(_t2_96, _t2_97);

  // 4-BLAC: 1x4 - 1x4
  _t2_99 = _mm256_sub_pd(_t2_95, _t2_98);

  // AVX Storer:
  _t2_23 = _t2_99;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_100 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_23, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_101 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_102 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t2_103 = _mm256_add_pd(_t2_101, _t2_102);

  // 4-BLAC: 1x4 / 1x4
  _t2_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_100), _mm256_castpd256_pd128(_t2_103)));

  // AVX Storer:
  _t2_24 = _t2_104;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 1), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_23, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_106 = _t2_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_107 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_108 = _mm256_mul_pd(_t2_106, _t2_107);

  // 4-BLAC: 1x4 - 1x4
  _t2_109 = _mm256_sub_pd(_t2_105, _t2_108);

  // AVX Storer:
  _t2_25 = _t2_109;

  // Generating : X[76,76] = S(h(1, 76, 1), ( G(h(1, 76, 1), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 1), L[76,76],h(1, 76, 1)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_110 = _t2_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_111 = _t2_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_112 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_113 = _mm256_add_pd(_t2_111, _t2_112);

  // 4-BLAC: 1x4 / 1x4
  _t2_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_110), _mm256_castpd256_pd128(_t2_113)));

  // AVX Storer:
  _t2_25 = _t2_114;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, 2), L[76,76],h(2, 76, 0)) * G(h(2, 76, 0), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_115 = _t2_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t2_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_16), _mm256_unpacklo_pd(_t2_18, _t2_19), 32);
  _t2_117 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_20, _t2_22), _mm256_unpacklo_pd(_t2_24, _t2_25), 32);
  _t2_118 = _mm256_setzero_pd();
  _t2_119 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t2_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_115, _t2_115, 32), _mm256_permute2f128_pd(_t2_115, _t2_115, 32), 0), _t2_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_115, _t2_115, 32), _mm256_permute2f128_pd(_t2_115, _t2_115, 32), 15), _t2_117)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_115, _t2_115, 49), _mm256_permute2f128_pd(_t2_115, _t2_115, 49), 0), _t2_118), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_115, _t2_115, 49), _mm256_permute2f128_pd(_t2_115, _t2_115, 49), 15), _t2_119)));

  // 4-BLAC: 1x4 - 1x4
  _t2_41 = _mm256_sub_pd(_t2_41, _t2_39);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_120 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_41, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_121 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_122 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_123 = _mm256_add_pd(_t2_121, _t2_122);

  // 4-BLAC: 1x4 / 1x4
  _t2_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_120), _mm256_castpd256_pd128(_t2_123)));

  // AVX Storer:
  _t2_26 = _t2_124;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 2), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_125 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_41, 14), _mm256_permute2f128_pd(_t2_41, _t2_41, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_126 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_26, _t2_26, 32), _mm256_permute2f128_pd(_t2_26, _t2_26, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_127 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_128 = _mm256_mul_pd(_t2_126, _t2_127);

  // 4-BLAC: 1x4 - 1x4
  _t2_129 = _mm256_sub_pd(_t2_125, _t2_128);

  // AVX Storer:
  _t2_27 = _t2_129;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_130 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_27, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_131 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_132 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_133 = _mm256_add_pd(_t2_131, _t2_132);

  // 4-BLAC: 1x4 / 1x4
  _t2_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_130), _mm256_castpd256_pd128(_t2_133)));

  // AVX Storer:
  _t2_28 = _t2_134;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 2), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_135 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_27, 6), _mm256_permute2f128_pd(_t2_27, _t2_27, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_136 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_28, _t2_28, 32), _mm256_permute2f128_pd(_t2_28, _t2_28, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_137 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_138 = _mm256_mul_pd(_t2_136, _t2_137);

  // 4-BLAC: 1x4 - 1x4
  _t2_139 = _mm256_sub_pd(_t2_135, _t2_138);

  // AVX Storer:
  _t2_29 = _t2_139;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_140 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_29, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_141 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_142 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t2_143 = _mm256_add_pd(_t2_141, _t2_142);

  // 4-BLAC: 1x4 / 1x4
  _t2_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_140), _mm256_castpd256_pd128(_t2_143)));

  // AVX Storer:
  _t2_30 = _t2_144;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 2), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_145 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_146 = _t2_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_147 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_148 = _mm256_mul_pd(_t2_146, _t2_147);

  // 4-BLAC: 1x4 - 1x4
  _t2_149 = _mm256_sub_pd(_t2_145, _t2_148);

  // AVX Storer:
  _t2_31 = _t2_149;

  // Generating : X[76,76] = S(h(1, 76, 2), ( G(h(1, 76, 2), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 2), L[76,76],h(1, 76, 2)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_150 = _t2_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_151 = _t2_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_152 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_153 = _mm256_add_pd(_t2_151, _t2_152);

  // 4-BLAC: 1x4 / 1x4
  _t2_154 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_150), _mm256_castpd256_pd128(_t2_153)));

  // AVX Storer:
  _t2_31 = _t2_154;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, 3), L[76,76],h(3, 76, 0)) * G(h(3, 76, 0), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_155 = _t2_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t2_156 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_14, _t2_16), _mm256_unpacklo_pd(_t2_18, _t2_19), 32);
  _t2_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_20, _t2_22), _mm256_unpacklo_pd(_t2_24, _t2_25), 32);
  _t2_158 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t2_26, _t2_28), _mm256_unpacklo_pd(_t2_30, _t2_31), 32);
  _t2_159 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t2_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_155, _t2_155, 32), _mm256_permute2f128_pd(_t2_155, _t2_155, 32), 0), _t2_156), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_155, _t2_155, 32), _mm256_permute2f128_pd(_t2_155, _t2_155, 32), 15), _t2_157)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_155, _t2_155, 49), _mm256_permute2f128_pd(_t2_155, _t2_155, 49), 0), _t2_158), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_155, _t2_155, 49), _mm256_permute2f128_pd(_t2_155, _t2_155, 49), 15), _t2_159)));

  // 4-BLAC: 1x4 - 1x4
  _t2_42 = _mm256_sub_pd(_t2_42, _t2_40);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_160 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_42, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_161 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_162 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t2_163 = _mm256_add_pd(_t2_161, _t2_162);

  // 4-BLAC: 1x4 / 1x4
  _t2_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_160), _mm256_castpd256_pd128(_t2_163)));

  // AVX Storer:
  _t2_32 = _t2_164;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 3), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_165 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_42, 14), _mm256_permute2f128_pd(_t2_42, _t2_42, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_166 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_32, _t2_32, 32), _mm256_permute2f128_pd(_t2_32, _t2_32, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t2_167 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_168 = _mm256_mul_pd(_t2_166, _t2_167);

  // 4-BLAC: 1x4 - 1x4
  _t2_169 = _mm256_sub_pd(_t2_165, _t2_168);

  // AVX Storer:
  _t2_33 = _t2_169;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_170 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_33, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_171 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_172 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t2_173 = _mm256_add_pd(_t2_171, _t2_172);

  // 4-BLAC: 1x4 / 1x4
  _t2_174 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_170), _mm256_castpd256_pd128(_t2_173)));

  // AVX Storer:
  _t2_34 = _t2_174;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 3), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_175 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_33, 6), _mm256_permute2f128_pd(_t2_33, _t2_33, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_176 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t2_34, _t2_34, 32), _mm256_permute2f128_pd(_t2_34, _t2_34, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t2_177 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_178 = _mm256_mul_pd(_t2_176, _t2_177);

  // 4-BLAC: 1x4 - 1x4
  _t2_179 = _mm256_sub_pd(_t2_175, _t2_178);

  // AVX Storer:
  _t2_35 = _t2_179;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_180 = _mm256_blend_pd(_mm256_setzero_pd(), _t2_35, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_181 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_182 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t2_183 = _mm256_add_pd(_t2_181, _t2_182);

  // 4-BLAC: 1x4 / 1x4
  _t2_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_180), _mm256_castpd256_pd128(_t2_183)));

  // AVX Storer:
  _t2_36 = _t2_184;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 3), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_185 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t2_35, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_186 = _t2_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_187 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t2_188 = _mm256_mul_pd(_t2_186, _t2_187);

  // 4-BLAC: 1x4 - 1x4
  _t2_189 = _mm256_sub_pd(_t2_185, _t2_188);

  // AVX Storer:
  _t2_37 = _t2_189;

  // Generating : X[76,76] = S(h(1, 76, 3), ( G(h(1, 76, 3), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 3), L[76,76],h(1, 76, 3)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_190 = _t2_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_191 = _t2_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t2_192 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t2_193 = _mm256_add_pd(_t2_191, _t2_192);

  // 4-BLAC: 1x4 / 1x4
  _t2_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t2_190), _mm256_castpd256_pd128(_t2_193)));

  // AVX Storer:
  _t2_37 = _t2_194;

  // Generating : X[76,76] = Sum_{k249} ( S(h(4, 76, 4), ( G(h(4, 76, 4), C[76,76],h(4, 76, k249)) - ( G(h(4, 76, 4), L[76,76],h(4, 76, 0)) * G(h(4, 76, 0), X[76,76],h(4, 76, k249)) ) ),h(4, 76, k249)) )

  // AVX Loader:

  _mm_store_sd(&(C[72]), _mm256_castpd256_pd128(_t2_14));
  _mm_store_sd(&(C[73]), _mm256_castpd256_pd128(_t2_16));
  _mm_store_sd(&(C[74]), _mm256_castpd256_pd128(_t2_18));
  _mm_store_sd(&(C[75]), _mm256_castpd256_pd128(_t2_19));
  _mm_store_sd(&(C[148]), _mm256_castpd256_pd128(_t2_20));
  _mm_store_sd(&(C[149]), _mm256_castpd256_pd128(_t2_22));
  _mm_store_sd(&(C[150]), _mm256_castpd256_pd128(_t2_24));
  _mm_store_sd(&(C[151]), _mm256_castpd256_pd128(_t2_25));
  _mm_store_sd(&(C[224]), _mm256_castpd256_pd128(_t2_26));
  _mm_store_sd(&(C[225]), _mm256_castpd256_pd128(_t2_28));
  _mm_store_sd(&(C[226]), _mm256_castpd256_pd128(_t2_30));
  _mm_store_sd(&(C[227]), _mm256_castpd256_pd128(_t2_31));
  _mm_store_sd(&(C[300]), _mm256_castpd256_pd128(_t2_32));
  _mm_store_sd(&(C[301]), _mm256_castpd256_pd128(_t2_34));
  _mm_store_sd(&(C[302]), _mm256_castpd256_pd128(_t2_36));
  _mm_store_sd(&(C[303]), _mm256_castpd256_pd128(_t2_37));

  for( int k249 = 0; k249 <= 75; k249+=4 ) {
    _t3_24 = _asm256_loadu_pd(C + k249 + 304);
    _t3_25 = _asm256_loadu_pd(C + k249 + 380);
    _t3_26 = _asm256_loadu_pd(C + k249 + 456);
    _t3_27 = _asm256_loadu_pd(C + k249 + 532);
    _t3_19 = _mm256_broadcast_sd(L + 304);
    _t3_18 = _mm256_broadcast_sd(L + 305);
    _t3_17 = _mm256_broadcast_sd(L + 306);
    _t3_16 = _mm256_broadcast_sd(L + 307);
    _t3_15 = _mm256_broadcast_sd(L + 380);
    _t3_14 = _mm256_broadcast_sd(L + 381);
    _t3_13 = _mm256_broadcast_sd(L + 382);
    _t3_12 = _mm256_broadcast_sd(L + 383);
    _t3_11 = _mm256_broadcast_sd(L + 456);
    _t3_10 = _mm256_broadcast_sd(L + 457);
    _t3_9 = _mm256_broadcast_sd(L + 458);
    _t3_8 = _mm256_broadcast_sd(L + 459);
    _t3_7 = _mm256_broadcast_sd(L + 532);
    _t3_6 = _mm256_broadcast_sd(L + 533);
    _t3_5 = _mm256_broadcast_sd(L + 534);
    _t3_4 = _mm256_broadcast_sd(L + 535);
    _t3_3 = _asm256_loadu_pd(C + k249);
    _t3_2 = _asm256_loadu_pd(C + k249 + 76);
    _t3_1 = _asm256_loadu_pd(C + k249 + 152);
    _t3_0 = _asm256_loadu_pd(C + k249 + 228);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t3_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_19, _t3_3), _mm256_mul_pd(_t3_18, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_17, _t3_1), _mm256_mul_pd(_t3_16, _t3_0)));
    _t3_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t3_3), _mm256_mul_pd(_t3_14, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t3_1), _mm256_mul_pd(_t3_12, _t3_0)));
    _t3_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t3_3), _mm256_mul_pd(_t3_10, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t3_1), _mm256_mul_pd(_t3_8, _t3_0)));
    _t3_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t3_3), _mm256_mul_pd(_t3_6, _t3_2)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t3_1), _mm256_mul_pd(_t3_4, _t3_0)));

    // 4-BLAC: 4x4 - 4x4
    _t3_24 = _mm256_sub_pd(_t3_24, _t3_20);
    _t3_25 = _mm256_sub_pd(_t3_25, _t3_21);
    _t3_26 = _mm256_sub_pd(_t3_26, _t3_22);
    _t3_27 = _mm256_sub_pd(_t3_27, _t3_23);

    // AVX Storer:
    _asm256_storeu_pd(C + k249 + 304, _t3_24);
    _asm256_storeu_pd(C + k249 + 380, _t3_25);
    _asm256_storeu_pd(C + k249 + 456, _t3_26);
    _asm256_storeu_pd(C + k249 + 532, _t3_27);
  }


  for( int fi1509 = 0; fi1509 <= 71; fi1509+=4 ) {
    _t4_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 304])));
    _t4_13 = _mm256_castpd128_pd256(_mm_load_sd(&(L[308])));
    _t4_12 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509])));
    _t4_15 = _mm256_maskload_pd(C + fi1509 + 305, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t4_11 = _mm256_maskload_pd(U + 77*fi1509 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t4_10 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 77])));
    _t4_9 = _mm256_maskload_pd(U + 77*fi1509 + 78, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t4_8 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 154])));
    _t4_7 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 155])));
    _t4_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 231])));
    _t4_41 = _asm256_loadu_pd(C + fi1509 + 380);
    _t4_5 = _mm256_broadcast_sd(&(L[384]));
    _t4_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[385])));
    _t4_42 = _asm256_loadu_pd(C + fi1509 + 456);
    _t4_3 = _mm256_maskload_pd(L + 460, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t4_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[462])));
    _t4_43 = _asm256_loadu_pd(C + fi1509 + 532);
    _t4_1 = _mm256_maskload_pd(L + 536, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t4_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[539])));

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_44 = _t4_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_45 = _t4_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_46 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t4_47 = _mm256_add_pd(_t4_45, _t4_46);

    // 4-BLAC: 1x4 / 1x4
    _t4_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_44), _mm256_castpd256_pd128(_t4_47)));

    // AVX Storer:
    _t4_14 = _t4_48;

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_49 = _t4_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_14, _t4_14, 32), _mm256_permute2f128_pd(_t4_14, _t4_14, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_51 = _t4_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_52 = _mm256_mul_pd(_t4_50, _t4_51);

    // 4-BLAC: 1x4 - 1x4
    _t4_53 = _mm256_sub_pd(_t4_49, _t4_52);

    // AVX Storer:
    _t4_15 = _t4_53;

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_54 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_55 = _t4_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_56 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t4_57 = _mm256_add_pd(_t4_55, _t4_56);

    // 4-BLAC: 1x4 / 1x4
    _t4_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_54), _mm256_castpd256_pd128(_t4_57)));

    // AVX Storer:
    _t4_16 = _t4_58;

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_59 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_15, 6), _mm256_permute2f128_pd(_t4_15, _t4_15, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_60 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_16, _t4_16, 32), _mm256_permute2f128_pd(_t4_16, _t4_16, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_61 = _t4_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_62 = _mm256_mul_pd(_t4_60, _t4_61);

    // 4-BLAC: 1x4 - 1x4
    _t4_63 = _mm256_sub_pd(_t4_59, _t4_62);

    // AVX Storer:
    _t4_17 = _t4_63;

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_64 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_17, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_65 = _t4_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_66 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t4_67 = _mm256_add_pd(_t4_65, _t4_66);

    // 4-BLAC: 1x4 / 1x4
    _t4_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_64), _mm256_castpd256_pd128(_t4_67)));

    // AVX Storer:
    _t4_18 = _t4_68;

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_69 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_17, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_70 = _t4_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_71 = _t4_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_72 = _mm256_mul_pd(_t4_70, _t4_71);

    // 4-BLAC: 1x4 - 1x4
    _t4_73 = _mm256_sub_pd(_t4_69, _t4_72);

    // AVX Storer:
    _t4_19 = _t4_73;

    // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_74 = _t4_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_75 = _t4_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_76 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_77 = _mm256_add_pd(_t4_75, _t4_76);

    // 4-BLAC: 1x4 / 1x4
    _t4_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_74), _mm256_castpd256_pd128(_t4_77)));

    // AVX Storer:
    _t4_19 = _t4_78;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, 5), L[76,76],h(1, 76, 4)) Kro G(h(1, 76, 4), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_79 = _t4_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t4_38 = _mm256_mul_pd(_t4_79, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_14, _t4_16), _mm256_unpacklo_pd(_t4_18, _t4_19), 32));

    // 4-BLAC: 1x4 - 1x4
    _t4_41 = _mm256_sub_pd(_t4_41, _t4_38);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_80 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_41, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_81 = _t4_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_82 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t4_83 = _mm256_add_pd(_t4_81, _t4_82);

    // 4-BLAC: 1x4 / 1x4
    _t4_84 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_80), _mm256_castpd256_pd128(_t4_83)));

    // AVX Storer:
    _t4_20 = _t4_84;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_85 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_41, 14), _mm256_permute2f128_pd(_t4_41, _t4_41, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_86 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_20, _t4_20, 32), _mm256_permute2f128_pd(_t4_20, _t4_20, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_87 = _t4_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_88 = _mm256_mul_pd(_t4_86, _t4_87);

    // 4-BLAC: 1x4 - 1x4
    _t4_89 = _mm256_sub_pd(_t4_85, _t4_88);

    // AVX Storer:
    _t4_21 = _t4_89;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_90 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_91 = _t4_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_92 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t4_93 = _mm256_add_pd(_t4_91, _t4_92);

    // 4-BLAC: 1x4 / 1x4
    _t4_94 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_90), _mm256_castpd256_pd128(_t4_93)));

    // AVX Storer:
    _t4_22 = _t4_94;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_95 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_21, 6), _mm256_permute2f128_pd(_t4_21, _t4_21, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_96 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_22, _t4_22, 32), _mm256_permute2f128_pd(_t4_22, _t4_22, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_97 = _t4_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_98 = _mm256_mul_pd(_t4_96, _t4_97);

    // 4-BLAC: 1x4 - 1x4
    _t4_99 = _mm256_sub_pd(_t4_95, _t4_98);

    // AVX Storer:
    _t4_23 = _t4_99;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_100 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_23, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_101 = _t4_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_102 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t4_103 = _mm256_add_pd(_t4_101, _t4_102);

    // 4-BLAC: 1x4 / 1x4
    _t4_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_100), _mm256_castpd256_pd128(_t4_103)));

    // AVX Storer:
    _t4_24 = _t4_104;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_23, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_106 = _t4_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_107 = _t4_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_108 = _mm256_mul_pd(_t4_106, _t4_107);

    // 4-BLAC: 1x4 - 1x4
    _t4_109 = _mm256_sub_pd(_t4_105, _t4_108);

    // AVX Storer:
    _t4_25 = _t4_109;

    // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_110 = _t4_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_111 = _t4_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_112 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_113 = _mm256_add_pd(_t4_111, _t4_112);

    // 4-BLAC: 1x4 / 1x4
    _t4_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_110), _mm256_castpd256_pd128(_t4_113)));

    // AVX Storer:
    _t4_25 = _t4_114;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, 6), L[76,76],h(2, 76, 4)) * G(h(2, 76, 4), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_115 = _t4_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t4_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_14, _t4_16), _mm256_unpacklo_pd(_t4_18, _t4_19), 32);
    _t4_117 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_20, _t4_22), _mm256_unpacklo_pd(_t4_24, _t4_25), 32);
    _t4_118 = _mm256_setzero_pd();
    _t4_119 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t4_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_115, _t4_115, 32), _mm256_permute2f128_pd(_t4_115, _t4_115, 32), 0), _t4_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_115, _t4_115, 32), _mm256_permute2f128_pd(_t4_115, _t4_115, 32), 15), _t4_117)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_115, _t4_115, 49), _mm256_permute2f128_pd(_t4_115, _t4_115, 49), 0), _t4_118), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_115, _t4_115, 49), _mm256_permute2f128_pd(_t4_115, _t4_115, 49), 15), _t4_119)));

    // 4-BLAC: 1x4 - 1x4
    _t4_42 = _mm256_sub_pd(_t4_42, _t4_39);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_120 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_42, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_121 = _t4_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_122 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t4_123 = _mm256_add_pd(_t4_121, _t4_122);

    // 4-BLAC: 1x4 / 1x4
    _t4_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_120), _mm256_castpd256_pd128(_t4_123)));

    // AVX Storer:
    _t4_26 = _t4_124;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_125 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_42, 14), _mm256_permute2f128_pd(_t4_42, _t4_42, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_126 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_26, _t4_26, 32), _mm256_permute2f128_pd(_t4_26, _t4_26, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_127 = _t4_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_128 = _mm256_mul_pd(_t4_126, _t4_127);

    // 4-BLAC: 1x4 - 1x4
    _t4_129 = _mm256_sub_pd(_t4_125, _t4_128);

    // AVX Storer:
    _t4_27 = _t4_129;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_130 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_27, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_131 = _t4_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_132 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t4_133 = _mm256_add_pd(_t4_131, _t4_132);

    // 4-BLAC: 1x4 / 1x4
    _t4_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_130), _mm256_castpd256_pd128(_t4_133)));

    // AVX Storer:
    _t4_28 = _t4_134;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_135 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_27, 6), _mm256_permute2f128_pd(_t4_27, _t4_27, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_136 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_28, _t4_28, 32), _mm256_permute2f128_pd(_t4_28, _t4_28, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_137 = _t4_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_138 = _mm256_mul_pd(_t4_136, _t4_137);

    // 4-BLAC: 1x4 - 1x4
    _t4_139 = _mm256_sub_pd(_t4_135, _t4_138);

    // AVX Storer:
    _t4_29 = _t4_139;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_140 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_29, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_141 = _t4_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_142 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t4_143 = _mm256_add_pd(_t4_141, _t4_142);

    // 4-BLAC: 1x4 / 1x4
    _t4_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_140), _mm256_castpd256_pd128(_t4_143)));

    // AVX Storer:
    _t4_30 = _t4_144;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_145 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_29, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_146 = _t4_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_147 = _t4_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_148 = _mm256_mul_pd(_t4_146, _t4_147);

    // 4-BLAC: 1x4 - 1x4
    _t4_149 = _mm256_sub_pd(_t4_145, _t4_148);

    // AVX Storer:
    _t4_31 = _t4_149;

    // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_150 = _t4_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_151 = _t4_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_152 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_153 = _mm256_add_pd(_t4_151, _t4_152);

    // 4-BLAC: 1x4 / 1x4
    _t4_154 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_150), _mm256_castpd256_pd128(_t4_153)));

    // AVX Storer:
    _t4_31 = _t4_154;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, 7), L[76,76],h(3, 76, 4)) * G(h(3, 76, 4), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_155 = _t4_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t4_156 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_14, _t4_16), _mm256_unpacklo_pd(_t4_18, _t4_19), 32);
    _t4_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_20, _t4_22), _mm256_unpacklo_pd(_t4_24, _t4_25), 32);
    _t4_158 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_26, _t4_28), _mm256_unpacklo_pd(_t4_30, _t4_31), 32);
    _t4_159 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t4_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_155, _t4_155, 32), _mm256_permute2f128_pd(_t4_155, _t4_155, 32), 0), _t4_156), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_155, _t4_155, 32), _mm256_permute2f128_pd(_t4_155, _t4_155, 32), 15), _t4_157)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_155, _t4_155, 49), _mm256_permute2f128_pd(_t4_155, _t4_155, 49), 0), _t4_158), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_155, _t4_155, 49), _mm256_permute2f128_pd(_t4_155, _t4_155, 49), 15), _t4_159)));

    // 4-BLAC: 1x4 - 1x4
    _t4_43 = _mm256_sub_pd(_t4_43, _t4_40);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_160 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_43, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_161 = _t4_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_162 = _t4_12;

    // 4-BLAC: 1x4 + 1x4
    _t4_163 = _mm256_add_pd(_t4_161, _t4_162);

    // 4-BLAC: 1x4 / 1x4
    _t4_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_160), _mm256_castpd256_pd128(_t4_163)));

    // AVX Storer:
    _t4_32 = _t4_164;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_165 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_43, 14), _mm256_permute2f128_pd(_t4_43, _t4_43, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_166 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_32, _t4_32, 32), _mm256_permute2f128_pd(_t4_32, _t4_32, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t4_167 = _t4_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_168 = _mm256_mul_pd(_t4_166, _t4_167);

    // 4-BLAC: 1x4 - 1x4
    _t4_169 = _mm256_sub_pd(_t4_165, _t4_168);

    // AVX Storer:
    _t4_33 = _t4_169;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_170 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_33, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_171 = _t4_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_172 = _t4_10;

    // 4-BLAC: 1x4 + 1x4
    _t4_173 = _mm256_add_pd(_t4_171, _t4_172);

    // 4-BLAC: 1x4 / 1x4
    _t4_174 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_170), _mm256_castpd256_pd128(_t4_173)));

    // AVX Storer:
    _t4_34 = _t4_174;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_175 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_33, 6), _mm256_permute2f128_pd(_t4_33, _t4_33, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_176 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t4_34, _t4_34, 32), _mm256_permute2f128_pd(_t4_34, _t4_34, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t4_177 = _t4_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_178 = _mm256_mul_pd(_t4_176, _t4_177);

    // 4-BLAC: 1x4 - 1x4
    _t4_179 = _mm256_sub_pd(_t4_175, _t4_178);

    // AVX Storer:
    _t4_35 = _t4_179;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_180 = _mm256_blend_pd(_mm256_setzero_pd(), _t4_35, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_181 = _t4_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_182 = _t4_8;

    // 4-BLAC: 1x4 + 1x4
    _t4_183 = _mm256_add_pd(_t4_181, _t4_182);

    // 4-BLAC: 1x4 / 1x4
    _t4_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_180), _mm256_castpd256_pd128(_t4_183)));

    // AVX Storer:
    _t4_36 = _t4_184;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_185 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t4_35, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_186 = _t4_36;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_187 = _t4_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t4_188 = _mm256_mul_pd(_t4_186, _t4_187);

    // 4-BLAC: 1x4 - 1x4
    _t4_189 = _mm256_sub_pd(_t4_185, _t4_188);

    // AVX Storer:
    _t4_37 = _t4_189;

    // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_190 = _t4_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_191 = _t4_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t4_192 = _t4_6;

    // 4-BLAC: 1x4 + 1x4
    _t4_193 = _mm256_add_pd(_t4_191, _t4_192);

    // 4-BLAC: 1x4 / 1x4
    _t4_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t4_190), _mm256_castpd256_pd128(_t4_193)));

    // AVX Storer:
    _t4_37 = _t4_194;

    // Generating : X[76,76] = Sum_{j123} ( S(h(4, 76, 4), ( G(h(4, 76, 4), X[76,76],h(4, 76, fi1509 + j123 + 4)) - ( G(h(4, 76, 4), X[76,76],h(4, 76, fi1509)) * G(h(4, 76, fi1509), U[76,76],h(4, 76, fi1509 + j123 + 4)) ) ),h(4, 76, fi1509 + j123 + 4)) )

    // AVX Loader:
    _mm_store_sd(&(C[fi1509 + 304]), _mm256_castpd256_pd128(_t4_14));
    _mm_store_sd(&(C[fi1509 + 305]), _mm256_castpd256_pd128(_t4_16));
    _mm_store_sd(&(C[fi1509 + 306]), _mm256_castpd256_pd128(_t4_18));
    _mm_store_sd(&(C[fi1509 + 307]), _mm256_castpd256_pd128(_t4_19));
    _mm_store_sd(&(C[fi1509 + 380]), _mm256_castpd256_pd128(_t4_20));
    _mm_store_sd(&(C[fi1509 + 381]), _mm256_castpd256_pd128(_t4_22));
    _mm_store_sd(&(C[fi1509 + 382]), _mm256_castpd256_pd128(_t4_24));
    _mm_store_sd(&(C[fi1509 + 383]), _mm256_castpd256_pd128(_t4_25));
    _mm_store_sd(&(C[fi1509 + 456]), _mm256_castpd256_pd128(_t4_26));
    _mm_store_sd(&(C[fi1509 + 457]), _mm256_castpd256_pd128(_t4_28));
    _mm_store_sd(&(C[fi1509 + 458]), _mm256_castpd256_pd128(_t4_30));
    _mm_store_sd(&(C[fi1509 + 459]), _mm256_castpd256_pd128(_t4_31));
    _mm_store_sd(&(C[fi1509 + 532]), _mm256_castpd256_pd128(_t4_32));
    _mm_store_sd(&(C[fi1509 + 533]), _mm256_castpd256_pd128(_t4_34));
    _mm_store_sd(&(C[fi1509 + 534]), _mm256_castpd256_pd128(_t4_36));
    _mm_store_sd(&(C[fi1509 + 535]), _mm256_castpd256_pd128(_t4_37));

    for( int j123 = 0; j123 <= -fi1509 + 71; j123+=4 ) {
      _t5_24 = _asm256_loadu_pd(C + fi1509 + j123 + 308);
      _t5_25 = _asm256_loadu_pd(C + fi1509 + j123 + 384);
      _t5_26 = _asm256_loadu_pd(C + fi1509 + j123 + 460);
      _t5_27 = _asm256_loadu_pd(C + fi1509 + j123 + 536);
      _t5_19 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 4);
      _t5_18 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 80);
      _t5_17 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 156);
      _t5_16 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 232);
      _t5_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 304])));
      _t5_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 305])));
      _t5_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 306])));
      _t5_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 307])));
      _t5_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 380])));
      _t5_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 381])));
      _t5_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 382])));
      _t5_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 383])));
      _t5_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 456])));
      _t5_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 457])));
      _t5_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 458])));
      _t5_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 459])));
      _t5_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 532])));
      _t5_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 533])));
      _t5_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 534])));
      _t5_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 535])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_15, _t5_15, 32), _mm256_permute2f128_pd(_t5_15, _t5_15, 32), 0), _t5_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0), _t5_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_13, _t5_13, 32), _mm256_permute2f128_pd(_t5_13, _t5_13, 32), 0), _t5_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0), _t5_16)));
      _t5_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_11, _t5_11, 32), _mm256_permute2f128_pd(_t5_11, _t5_11, 32), 0), _t5_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_10, _t5_10, 32), _mm256_permute2f128_pd(_t5_10, _t5_10, 32), 0), _t5_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_9, _t5_9, 32), _mm256_permute2f128_pd(_t5_9, _t5_9, 32), 0), _t5_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_8, _t5_8, 32), _mm256_permute2f128_pd(_t5_8, _t5_8, 32), 0), _t5_16)));
      _t5_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_7, _t5_7, 32), _mm256_permute2f128_pd(_t5_7, _t5_7, 32), 0), _t5_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_6, _t5_6, 32), _mm256_permute2f128_pd(_t5_6, _t5_6, 32), 0), _t5_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_5, _t5_5, 32), _mm256_permute2f128_pd(_t5_5, _t5_5, 32), 0), _t5_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_4, _t5_4, 32), _mm256_permute2f128_pd(_t5_4, _t5_4, 32), 0), _t5_16)));
      _t5_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_3, _t5_3, 32), _mm256_permute2f128_pd(_t5_3, _t5_3, 32), 0), _t5_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_2, _t5_2, 32), _mm256_permute2f128_pd(_t5_2, _t5_2, 32), 0), _t5_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_1, _t5_1, 32), _mm256_permute2f128_pd(_t5_1, _t5_1, 32), 0), _t5_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_0, _t5_0, 32), _mm256_permute2f128_pd(_t5_0, _t5_0, 32), 0), _t5_16)));

      // 4-BLAC: 4x4 - 4x4
      _t5_24 = _mm256_sub_pd(_t5_24, _t5_20);
      _t5_25 = _mm256_sub_pd(_t5_25, _t5_21);
      _t5_26 = _mm256_sub_pd(_t5_26, _t5_22);
      _t5_27 = _mm256_sub_pd(_t5_27, _t5_23);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1509 + j123 + 308, _t5_24);
      _asm256_storeu_pd(C + fi1509 + j123 + 384, _t5_25);
      _asm256_storeu_pd(C + fi1509 + j123 + 460, _t5_26);
      _asm256_storeu_pd(C + fi1509 + j123 + 536, _t5_27);
    }
  }

  _t6_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[376])));
  _t6_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[308])));
  _t6_8 = _mm256_maskload_pd(C + 377, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t6_34 = _asm256_loadu_pd(C + 452);
  _t6_5 = _mm256_broadcast_sd(&(L[384]));
  _t6_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[385])));
  _t6_35 = _asm256_loadu_pd(C + 528);
  _t6_3 = _mm256_maskload_pd(L + 460, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t6_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[462])));
  _t6_36 = _asm256_loadu_pd(C + 604);
  _t6_1 = _mm256_maskload_pd(L + 536, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t6_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[539])));

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_175 = _t6_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_176 = _t6_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_177 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t6_178 = _mm256_add_pd(_t6_176, _t6_177);

  // 4-BLAC: 1x4 / 1x4
  _t6_179 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_175), _mm256_castpd256_pd128(_t6_178)));

  // AVX Storer:
  _t6_7 = _t6_179;

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 4), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_180 = _t6_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_181 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_7, _t6_7, 32), _mm256_permute2f128_pd(_t6_7, _t6_7, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_182 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_183 = _mm256_mul_pd(_t6_181, _t6_182);

  // 4-BLAC: 1x4 - 1x4
  _t6_184 = _mm256_sub_pd(_t6_180, _t6_183);

  // AVX Storer:
  _t6_8 = _t6_184;

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_185 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_8, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_186 = _t6_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_187 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t6_37 = _mm256_add_pd(_t6_186, _t6_187);

  // 4-BLAC: 1x4 / 1x4
  _t6_38 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_185), _mm256_castpd256_pd128(_t6_37)));

  // AVX Storer:
  _t6_9 = _t6_38;

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 4), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_39 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_8, 6), _mm256_permute2f128_pd(_t6_8, _t6_8, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_40 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_9, _t6_9, 32), _mm256_permute2f128_pd(_t6_9, _t6_9, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_41 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_42 = _mm256_mul_pd(_t6_40, _t6_41);

  // 4-BLAC: 1x4 - 1x4
  _t6_43 = _mm256_sub_pd(_t6_39, _t6_42);

  // AVX Storer:
  _t6_10 = _t6_43;

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_44 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_45 = _t6_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_46 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t6_47 = _mm256_add_pd(_t6_45, _t6_46);

  // 4-BLAC: 1x4 / 1x4
  _t6_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_44), _mm256_castpd256_pd128(_t6_47)));

  // AVX Storer:
  _t6_11 = _t6_48;

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 4), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_49 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_50 = _t6_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_51 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_52 = _mm256_mul_pd(_t6_50, _t6_51);

  // 4-BLAC: 1x4 - 1x4
  _t6_53 = _mm256_sub_pd(_t6_49, _t6_52);

  // AVX Storer:
  _t6_12 = _t6_53;

  // Generating : X[76,76] = S(h(1, 76, 4), ( G(h(1, 76, 4), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 4), L[76,76],h(1, 76, 4)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_54 = _t6_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_55 = _t6_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_56 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t6_57 = _mm256_add_pd(_t6_55, _t6_56);

  // 4-BLAC: 1x4 / 1x4
  _t6_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_54), _mm256_castpd256_pd128(_t6_57)));

  // AVX Storer:
  _t6_12 = _t6_58;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, 5), L[76,76],h(1, 76, 4)) Kro G(h(1, 76, 4), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_59 = _t6_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t6_31 = _mm256_mul_pd(_t6_59, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_7, _t6_9), _mm256_unpacklo_pd(_t6_11, _t6_12), 32));

  // 4-BLAC: 1x4 - 1x4
  _t6_34 = _mm256_sub_pd(_t6_34, _t6_31);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_60 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_34, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_61 = _t6_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_62 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t6_63 = _mm256_add_pd(_t6_61, _t6_62);

  // 4-BLAC: 1x4 / 1x4
  _t6_64 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_60), _mm256_castpd256_pd128(_t6_63)));

  // AVX Storer:
  _t6_13 = _t6_64;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 5), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_65 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_34, 14), _mm256_permute2f128_pd(_t6_34, _t6_34, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_66 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_13, _t6_13, 32), _mm256_permute2f128_pd(_t6_13, _t6_13, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_67 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_68 = _mm256_mul_pd(_t6_66, _t6_67);

  // 4-BLAC: 1x4 - 1x4
  _t6_69 = _mm256_sub_pd(_t6_65, _t6_68);

  // AVX Storer:
  _t6_14 = _t6_69;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_70 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_14, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_71 = _t6_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_72 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t6_73 = _mm256_add_pd(_t6_71, _t6_72);

  // 4-BLAC: 1x4 / 1x4
  _t6_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_70), _mm256_castpd256_pd128(_t6_73)));

  // AVX Storer:
  _t6_15 = _t6_74;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 5), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_75 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_14, 6), _mm256_permute2f128_pd(_t6_14, _t6_14, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_15, _t6_15, 32), _mm256_permute2f128_pd(_t6_15, _t6_15, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_77 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_78 = _mm256_mul_pd(_t6_76, _t6_77);

  // 4-BLAC: 1x4 - 1x4
  _t6_79 = _mm256_sub_pd(_t6_75, _t6_78);

  // AVX Storer:
  _t6_16 = _t6_79;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_80 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_81 = _t6_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_82 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t6_83 = _mm256_add_pd(_t6_81, _t6_82);

  // 4-BLAC: 1x4 / 1x4
  _t6_84 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_80), _mm256_castpd256_pd128(_t6_83)));

  // AVX Storer:
  _t6_17 = _t6_84;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 5), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_85 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_16, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_86 = _t6_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_87 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_88 = _mm256_mul_pd(_t6_86, _t6_87);

  // 4-BLAC: 1x4 - 1x4
  _t6_89 = _mm256_sub_pd(_t6_85, _t6_88);

  // AVX Storer:
  _t6_18 = _t6_89;

  // Generating : X[76,76] = S(h(1, 76, 5), ( G(h(1, 76, 5), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 5), L[76,76],h(1, 76, 5)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_90 = _t6_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_91 = _t6_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_92 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t6_93 = _mm256_add_pd(_t6_91, _t6_92);

  // 4-BLAC: 1x4 / 1x4
  _t6_94 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_90), _mm256_castpd256_pd128(_t6_93)));

  // AVX Storer:
  _t6_18 = _t6_94;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, 6), L[76,76],h(2, 76, 4)) * G(h(2, 76, 4), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_95 = _t6_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t6_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_7, _t6_9), _mm256_unpacklo_pd(_t6_11, _t6_12), 32);
  _t6_97 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_13, _t6_15), _mm256_unpacklo_pd(_t6_17, _t6_18), 32);
  _t6_98 = _mm256_setzero_pd();
  _t6_99 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t6_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 32), _mm256_permute2f128_pd(_t6_95, _t6_95, 32), 0), _t6_96), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 32), _mm256_permute2f128_pd(_t6_95, _t6_95, 32), 15), _t6_97)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 49), _mm256_permute2f128_pd(_t6_95, _t6_95, 49), 0), _t6_98), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_95, _t6_95, 49), _mm256_permute2f128_pd(_t6_95, _t6_95, 49), 15), _t6_99)));

  // 4-BLAC: 1x4 - 1x4
  _t6_35 = _mm256_sub_pd(_t6_35, _t6_32);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_100 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_35, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_101 = _t6_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_102 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t6_103 = _mm256_add_pd(_t6_101, _t6_102);

  // 4-BLAC: 1x4 / 1x4
  _t6_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_100), _mm256_castpd256_pd128(_t6_103)));

  // AVX Storer:
  _t6_19 = _t6_104;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 6), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_105 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_35, 14), _mm256_permute2f128_pd(_t6_35, _t6_35, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_106 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_19, _t6_19, 32), _mm256_permute2f128_pd(_t6_19, _t6_19, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_107 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_108 = _mm256_mul_pd(_t6_106, _t6_107);

  // 4-BLAC: 1x4 - 1x4
  _t6_109 = _mm256_sub_pd(_t6_105, _t6_108);

  // AVX Storer:
  _t6_20 = _t6_109;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_110 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_20, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_111 = _t6_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_112 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t6_113 = _mm256_add_pd(_t6_111, _t6_112);

  // 4-BLAC: 1x4 / 1x4
  _t6_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_110), _mm256_castpd256_pd128(_t6_113)));

  // AVX Storer:
  _t6_21 = _t6_114;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 6), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_115 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_20, 6), _mm256_permute2f128_pd(_t6_20, _t6_20, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_116 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_21, _t6_21, 32), _mm256_permute2f128_pd(_t6_21, _t6_21, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_117 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_118 = _mm256_mul_pd(_t6_116, _t6_117);

  // 4-BLAC: 1x4 - 1x4
  _t6_119 = _mm256_sub_pd(_t6_115, _t6_118);

  // AVX Storer:
  _t6_22 = _t6_119;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_120 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_22, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_121 = _t6_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_122 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t6_123 = _mm256_add_pd(_t6_121, _t6_122);

  // 4-BLAC: 1x4 / 1x4
  _t6_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_120), _mm256_castpd256_pd128(_t6_123)));

  // AVX Storer:
  _t6_23 = _t6_124;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 6), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_125 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_22, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_126 = _t6_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_127 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_128 = _mm256_mul_pd(_t6_126, _t6_127);

  // 4-BLAC: 1x4 - 1x4
  _t6_129 = _mm256_sub_pd(_t6_125, _t6_128);

  // AVX Storer:
  _t6_24 = _t6_129;

  // Generating : X[76,76] = S(h(1, 76, 6), ( G(h(1, 76, 6), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 6), L[76,76],h(1, 76, 6)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_130 = _t6_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_131 = _t6_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_132 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t6_133 = _mm256_add_pd(_t6_131, _t6_132);

  // 4-BLAC: 1x4 / 1x4
  _t6_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_130), _mm256_castpd256_pd128(_t6_133)));

  // AVX Storer:
  _t6_24 = _t6_134;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, 7), L[76,76],h(3, 76, 4)) * G(h(3, 76, 4), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_135 = _t6_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t6_136 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_7, _t6_9), _mm256_unpacklo_pd(_t6_11, _t6_12), 32);
  _t6_137 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_13, _t6_15), _mm256_unpacklo_pd(_t6_17, _t6_18), 32);
  _t6_138 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_19, _t6_21), _mm256_unpacklo_pd(_t6_23, _t6_24), 32);
  _t6_139 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t6_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_135, _t6_135, 32), _mm256_permute2f128_pd(_t6_135, _t6_135, 32), 0), _t6_136), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_135, _t6_135, 32), _mm256_permute2f128_pd(_t6_135, _t6_135, 32), 15), _t6_137)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_135, _t6_135, 49), _mm256_permute2f128_pd(_t6_135, _t6_135, 49), 0), _t6_138), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_135, _t6_135, 49), _mm256_permute2f128_pd(_t6_135, _t6_135, 49), 15), _t6_139)));

  // 4-BLAC: 1x4 - 1x4
  _t6_36 = _mm256_sub_pd(_t6_36, _t6_33);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_140 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_36, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_141 = _t6_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_142 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t6_143 = _mm256_add_pd(_t6_141, _t6_142);

  // 4-BLAC: 1x4 / 1x4
  _t6_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_140), _mm256_castpd256_pd128(_t6_143)));

  // AVX Storer:
  _t6_25 = _t6_144;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, 7), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_145 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_36, 14), _mm256_permute2f128_pd(_t6_36, _t6_36, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_146 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_25, _t6_25, 32), _mm256_permute2f128_pd(_t6_25, _t6_25, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t6_147 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_148 = _mm256_mul_pd(_t6_146, _t6_147);

  // 4-BLAC: 1x4 - 1x4
  _t6_149 = _mm256_sub_pd(_t6_145, _t6_148);

  // AVX Storer:
  _t6_26 = _t6_149;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_150 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_26, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_151 = _t6_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_152 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t6_153 = _mm256_add_pd(_t6_151, _t6_152);

  // 4-BLAC: 1x4 / 1x4
  _t6_154 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_150), _mm256_castpd256_pd128(_t6_153)));

  // AVX Storer:
  _t6_27 = _t6_154;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, 7), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_155 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_26, 6), _mm256_permute2f128_pd(_t6_26, _t6_26, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_156 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t6_27, _t6_27, 32), _mm256_permute2f128_pd(_t6_27, _t6_27, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t6_157 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_158 = _mm256_mul_pd(_t6_156, _t6_157);

  // 4-BLAC: 1x4 - 1x4
  _t6_159 = _mm256_sub_pd(_t6_155, _t6_158);

  // AVX Storer:
  _t6_28 = _t6_159;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_160 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_28, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_161 = _t6_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_162 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t6_163 = _mm256_add_pd(_t6_161, _t6_162);

  // 4-BLAC: 1x4 / 1x4
  _t6_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_160), _mm256_castpd256_pd128(_t6_163)));

  // AVX Storer:
  _t6_29 = _t6_164;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, 7), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_165 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_28, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_166 = _t6_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_167 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t6_168 = _mm256_mul_pd(_t6_166, _t6_167);

  // 4-BLAC: 1x4 - 1x4
  _t6_169 = _mm256_sub_pd(_t6_165, _t6_168);

  // AVX Storer:
  _t6_30 = _t6_169;

  // Generating : X[76,76] = S(h(1, 76, 7), ( G(h(1, 76, 7), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, 7), L[76,76],h(1, 76, 7)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_170 = _t6_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_171 = _t6_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t6_172 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t6_173 = _mm256_add_pd(_t6_171, _t6_172);

  // 4-BLAC: 1x4 / 1x4
  _t6_174 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t6_170), _mm256_castpd256_pd128(_t6_173)));

  // AVX Storer:
  _t6_30 = _t6_174;

  // Generating : X[76,76] = ( Sum_{k249} ( S(h(4, 76, fi1306), ( G(h(4, 76, fi1306), C[76,76],h(4, 76, k249)) - ( G(h(4, 76, fi1306), L[76,76],h(4, 76, 0)) * G(h(4, 76, 0), X[76,76],h(4, 76, k249)) ) ),h(4, 76, k249)) ) + Sum_{j123} ( Sum_{k249} ( -$(h(4, 76, fi1306), ( G(h(4, 76, fi1306), L[76,76],h(4, 76, j123)) * G(h(4, 76, j123), X[76,76],h(4, 76, k249)) ),h(4, 76, k249)) ) ) )

  // AVX Loader:


  for( int k249 = 0; k249 <= 75; k249+=4 ) {
    _t7_24 = _asm256_loadu_pd(C + k249 + 608);
    _t7_25 = _asm256_loadu_pd(C + k249 + 684);
    _t7_26 = _asm256_loadu_pd(C + k249 + 760);
    _t7_27 = _asm256_loadu_pd(C + k249 + 836);
    _t7_19 = _mm256_broadcast_sd(L + 608);
    _t7_18 = _mm256_broadcast_sd(L + 609);
    _t7_17 = _mm256_broadcast_sd(L + 610);
    _t7_16 = _mm256_broadcast_sd(L + 611);
    _t7_15 = _mm256_broadcast_sd(L + 684);
    _t7_14 = _mm256_broadcast_sd(L + 685);
    _t7_13 = _mm256_broadcast_sd(L + 686);
    _t7_12 = _mm256_broadcast_sd(L + 687);
    _t7_11 = _mm256_broadcast_sd(L + 760);
    _t7_10 = _mm256_broadcast_sd(L + 761);
    _t7_9 = _mm256_broadcast_sd(L + 762);
    _t7_8 = _mm256_broadcast_sd(L + 763);
    _t7_7 = _mm256_broadcast_sd(L + 836);
    _t7_6 = _mm256_broadcast_sd(L + 837);
    _t7_5 = _mm256_broadcast_sd(L + 838);
    _t7_4 = _mm256_broadcast_sd(L + 839);
    _t7_3 = _asm256_loadu_pd(C + k249);
    _t7_2 = _asm256_loadu_pd(C + k249 + 76);
    _t7_1 = _asm256_loadu_pd(C + k249 + 152);
    _t7_0 = _asm256_loadu_pd(C + k249 + 228);

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t7_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_3), _mm256_mul_pd(_t7_18, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_1), _mm256_mul_pd(_t7_16, _t7_0)));
    _t7_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_3), _mm256_mul_pd(_t7_14, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_1), _mm256_mul_pd(_t7_12, _t7_0)));
    _t7_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_3), _mm256_mul_pd(_t7_10, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_1), _mm256_mul_pd(_t7_8, _t7_0)));
    _t7_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_3), _mm256_mul_pd(_t7_6, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_1), _mm256_mul_pd(_t7_4, _t7_0)));

    // 4-BLAC: 4x4 - 4x4
    _t7_24 = _mm256_sub_pd(_t7_24, _t7_20);
    _t7_25 = _mm256_sub_pd(_t7_25, _t7_21);
    _t7_26 = _mm256_sub_pd(_t7_26, _t7_22);
    _t7_27 = _mm256_sub_pd(_t7_27, _t7_23);

    // AVX Storer:
    _asm256_storeu_pd(C + k249 + 608, _t7_24);
    _asm256_storeu_pd(C + k249 + 684, _t7_25);
    _asm256_storeu_pd(C + k249 + 760, _t7_26);
    _asm256_storeu_pd(C + k249 + 836, _t7_27);
  }


  // AVX Loader:

  _mm_store_sd(&(C[376]), _mm256_castpd256_pd128(_t6_7));
  _mm_store_sd(&(C[377]), _mm256_castpd256_pd128(_t6_9));
  _mm_store_sd(&(C[378]), _mm256_castpd256_pd128(_t6_11));
  _mm_store_sd(&(C[379]), _mm256_castpd256_pd128(_t6_12));
  _mm_store_sd(&(C[452]), _mm256_castpd256_pd128(_t6_13));
  _mm_store_sd(&(C[453]), _mm256_castpd256_pd128(_t6_15));
  _mm_store_sd(&(C[454]), _mm256_castpd256_pd128(_t6_17));
  _mm_store_sd(&(C[455]), _mm256_castpd256_pd128(_t6_18));
  _mm_store_sd(&(C[528]), _mm256_castpd256_pd128(_t6_19));
  _mm_store_sd(&(C[529]), _mm256_castpd256_pd128(_t6_21));
  _mm_store_sd(&(C[530]), _mm256_castpd256_pd128(_t6_23));
  _mm_store_sd(&(C[531]), _mm256_castpd256_pd128(_t6_24));
  _mm_store_sd(&(C[604]), _mm256_castpd256_pd128(_t6_25));
  _mm_store_sd(&(C[605]), _mm256_castpd256_pd128(_t6_27));
  _mm_store_sd(&(C[606]), _mm256_castpd256_pd128(_t6_29));
  _mm_store_sd(&(C[607]), _mm256_castpd256_pd128(_t6_30));

  for( int k249 = 0; k249 <= 75; k249+=4 ) {
    _t8_19 = _mm256_broadcast_sd(L + 612);
    _t8_18 = _mm256_broadcast_sd(L + 613);
    _t8_17 = _mm256_broadcast_sd(L + 614);
    _t8_16 = _mm256_broadcast_sd(L + 615);
    _t8_15 = _mm256_broadcast_sd(L + 688);
    _t8_14 = _mm256_broadcast_sd(L + 689);
    _t8_13 = _mm256_broadcast_sd(L + 690);
    _t8_12 = _mm256_broadcast_sd(L + 691);
    _t8_11 = _mm256_broadcast_sd(L + 764);
    _t8_10 = _mm256_broadcast_sd(L + 765);
    _t8_9 = _mm256_broadcast_sd(L + 766);
    _t8_8 = _mm256_broadcast_sd(L + 767);
    _t8_7 = _mm256_broadcast_sd(L + 840);
    _t8_6 = _mm256_broadcast_sd(L + 841);
    _t8_5 = _mm256_broadcast_sd(L + 842);
    _t8_4 = _mm256_broadcast_sd(L + 843);
    _t8_3 = _asm256_loadu_pd(C + k249 + 304);
    _t8_2 = _asm256_loadu_pd(C + k249 + 380);
    _t8_1 = _asm256_loadu_pd(C + k249 + 456);
    _t8_0 = _asm256_loadu_pd(C + k249 + 532);
    _t8_20 = _asm256_loadu_pd(C + k249 + 608);
    _t8_21 = _asm256_loadu_pd(C + k249 + 684);
    _t8_22 = _asm256_loadu_pd(C + k249 + 760);
    _t8_23 = _asm256_loadu_pd(C + k249 + 836);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t8_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_19, _t8_3), _mm256_mul_pd(_t8_18, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_17, _t8_1), _mm256_mul_pd(_t8_16, _t8_0)));
    _t8_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_3), _mm256_mul_pd(_t8_14, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_1), _mm256_mul_pd(_t8_12, _t8_0)));
    _t8_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_3), _mm256_mul_pd(_t8_10, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_1), _mm256_mul_pd(_t8_8, _t8_0)));
    _t8_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_3), _mm256_mul_pd(_t8_6, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_1), _mm256_mul_pd(_t8_4, _t8_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t8_20 = _mm256_sub_pd(_t8_20, _t8_24);
    _t8_21 = _mm256_sub_pd(_t8_21, _t8_25);
    _t8_22 = _mm256_sub_pd(_t8_22, _t8_26);
    _t8_23 = _mm256_sub_pd(_t8_23, _t8_27);

    // AVX Storer:
    _asm256_storeu_pd(C + k249 + 608, _t8_20);
    _asm256_storeu_pd(C + k249 + 684, _t8_21);
    _asm256_storeu_pd(C + k249 + 760, _t8_22);
    _asm256_storeu_pd(C + k249 + 836, _t8_23);
  }


  for( int fi1509 = 0; fi1509 <= 71; fi1509+=4 ) {
    _t9_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 608])));
    _t9_13 = _mm256_castpd128_pd256(_mm_load_sd(&(L[616])));
    _t9_12 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509])));
    _t9_15 = _mm256_maskload_pd(C + fi1509 + 609, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_11 = _mm256_maskload_pd(U + 77*fi1509 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_10 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 77])));
    _t9_9 = _mm256_maskload_pd(U + 77*fi1509 + 78, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t9_8 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 154])));
    _t9_7 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 155])));
    _t9_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 231])));
    _t9_41 = _asm256_loadu_pd(C + fi1509 + 684);
    _t9_5 = _mm256_broadcast_sd(&(L[692]));
    _t9_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[693])));
    _t9_42 = _asm256_loadu_pd(C + fi1509 + 760);
    _t9_3 = _mm256_maskload_pd(L + 768, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t9_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[770])));
    _t9_43 = _asm256_loadu_pd(C + fi1509 + 836);
    _t9_1 = _mm256_maskload_pd(L + 844, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t9_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[847])));

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_44 = _t9_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_45 = _t9_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_46 = _t9_12;

    // 4-BLAC: 1x4 + 1x4
    _t9_47 = _mm256_add_pd(_t9_45, _t9_46);

    // 4-BLAC: 1x4 / 1x4
    _t9_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_44), _mm256_castpd256_pd128(_t9_47)));

    // AVX Storer:
    _t9_14 = _t9_48;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_49 = _t9_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_14, _t9_14, 32), _mm256_permute2f128_pd(_t9_14, _t9_14, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_51 = _t9_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_52 = _mm256_mul_pd(_t9_50, _t9_51);

    // 4-BLAC: 1x4 - 1x4
    _t9_53 = _mm256_sub_pd(_t9_49, _t9_52);

    // AVX Storer:
    _t9_15 = _t9_53;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_54 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_15, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_55 = _t9_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_56 = _t9_10;

    // 4-BLAC: 1x4 + 1x4
    _t9_57 = _mm256_add_pd(_t9_55, _t9_56);

    // 4-BLAC: 1x4 / 1x4
    _t9_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_54), _mm256_castpd256_pd128(_t9_57)));

    // AVX Storer:
    _t9_16 = _t9_58;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_59 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_15, 6), _mm256_permute2f128_pd(_t9_15, _t9_15, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_60 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_16, _t9_16, 32), _mm256_permute2f128_pd(_t9_16, _t9_16, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_61 = _t9_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_62 = _mm256_mul_pd(_t9_60, _t9_61);

    // 4-BLAC: 1x4 - 1x4
    _t9_63 = _mm256_sub_pd(_t9_59, _t9_62);

    // AVX Storer:
    _t9_17 = _t9_63;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_64 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_17, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_65 = _t9_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_66 = _t9_8;

    // 4-BLAC: 1x4 + 1x4
    _t9_67 = _mm256_add_pd(_t9_65, _t9_66);

    // 4-BLAC: 1x4 / 1x4
    _t9_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_64), _mm256_castpd256_pd128(_t9_67)));

    // AVX Storer:
    _t9_18 = _t9_68;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_69 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_17, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_70 = _t9_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_71 = _t9_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_72 = _mm256_mul_pd(_t9_70, _t9_71);

    // 4-BLAC: 1x4 - 1x4
    _t9_73 = _mm256_sub_pd(_t9_69, _t9_72);

    // AVX Storer:
    _t9_19 = _t9_73;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_74 = _t9_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_75 = _t9_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_76 = _t9_6;

    // 4-BLAC: 1x4 + 1x4
    _t9_77 = _mm256_add_pd(_t9_75, _t9_76);

    // 4-BLAC: 1x4 / 1x4
    _t9_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_74), _mm256_castpd256_pd128(_t9_77)));

    // AVX Storer:
    _t9_19 = _t9_78;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_79 = _t9_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t9_38 = _mm256_mul_pd(_t9_79, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_14, _t9_16), _mm256_unpacklo_pd(_t9_18, _t9_19), 32));

    // 4-BLAC: 1x4 - 1x4
    _t9_41 = _mm256_sub_pd(_t9_41, _t9_38);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_80 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_41, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_81 = _t9_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_82 = _t9_12;

    // 4-BLAC: 1x4 + 1x4
    _t9_83 = _mm256_add_pd(_t9_81, _t9_82);

    // 4-BLAC: 1x4 / 1x4
    _t9_84 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_80), _mm256_castpd256_pd128(_t9_83)));

    // AVX Storer:
    _t9_20 = _t9_84;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_85 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_41, 14), _mm256_permute2f128_pd(_t9_41, _t9_41, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_86 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_20, _t9_20, 32), _mm256_permute2f128_pd(_t9_20, _t9_20, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_87 = _t9_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_88 = _mm256_mul_pd(_t9_86, _t9_87);

    // 4-BLAC: 1x4 - 1x4
    _t9_89 = _mm256_sub_pd(_t9_85, _t9_88);

    // AVX Storer:
    _t9_21 = _t9_89;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_90 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_21, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_91 = _t9_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_92 = _t9_10;

    // 4-BLAC: 1x4 + 1x4
    _t9_93 = _mm256_add_pd(_t9_91, _t9_92);

    // 4-BLAC: 1x4 / 1x4
    _t9_94 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_90), _mm256_castpd256_pd128(_t9_93)));

    // AVX Storer:
    _t9_22 = _t9_94;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_95 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_21, 6), _mm256_permute2f128_pd(_t9_21, _t9_21, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_96 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_22, _t9_22, 32), _mm256_permute2f128_pd(_t9_22, _t9_22, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_97 = _t9_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_98 = _mm256_mul_pd(_t9_96, _t9_97);

    // 4-BLAC: 1x4 - 1x4
    _t9_99 = _mm256_sub_pd(_t9_95, _t9_98);

    // AVX Storer:
    _t9_23 = _t9_99;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_100 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_23, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_101 = _t9_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_102 = _t9_8;

    // 4-BLAC: 1x4 + 1x4
    _t9_103 = _mm256_add_pd(_t9_101, _t9_102);

    // 4-BLAC: 1x4 / 1x4
    _t9_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_100), _mm256_castpd256_pd128(_t9_103)));

    // AVX Storer:
    _t9_24 = _t9_104;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_23, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_106 = _t9_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_107 = _t9_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_108 = _mm256_mul_pd(_t9_106, _t9_107);

    // 4-BLAC: 1x4 - 1x4
    _t9_109 = _mm256_sub_pd(_t9_105, _t9_108);

    // AVX Storer:
    _t9_25 = _t9_109;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_110 = _t9_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_111 = _t9_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_112 = _t9_6;

    // 4-BLAC: 1x4 + 1x4
    _t9_113 = _mm256_add_pd(_t9_111, _t9_112);

    // 4-BLAC: 1x4 / 1x4
    _t9_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_110), _mm256_castpd256_pd128(_t9_113)));

    // AVX Storer:
    _t9_25 = _t9_114;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, fi1306 + 2), L[76,76],h(2, 76, fi1306)) * G(h(2, 76, fi1306), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_115 = _t9_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t9_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_14, _t9_16), _mm256_unpacklo_pd(_t9_18, _t9_19), 32);
    _t9_117 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_20, _t9_22), _mm256_unpacklo_pd(_t9_24, _t9_25), 32);
    _t9_118 = _mm256_setzero_pd();
    _t9_119 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t9_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_115, _t9_115, 32), _mm256_permute2f128_pd(_t9_115, _t9_115, 32), 0), _t9_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_115, _t9_115, 32), _mm256_permute2f128_pd(_t9_115, _t9_115, 32), 15), _t9_117)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_115, _t9_115, 49), _mm256_permute2f128_pd(_t9_115, _t9_115, 49), 0), _t9_118), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_115, _t9_115, 49), _mm256_permute2f128_pd(_t9_115, _t9_115, 49), 15), _t9_119)));

    // 4-BLAC: 1x4 - 1x4
    _t9_42 = _mm256_sub_pd(_t9_42, _t9_39);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_120 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_42, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_121 = _t9_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_122 = _t9_12;

    // 4-BLAC: 1x4 + 1x4
    _t9_123 = _mm256_add_pd(_t9_121, _t9_122);

    // 4-BLAC: 1x4 / 1x4
    _t9_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_120), _mm256_castpd256_pd128(_t9_123)));

    // AVX Storer:
    _t9_26 = _t9_124;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_125 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_42, 14), _mm256_permute2f128_pd(_t9_42, _t9_42, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_126 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_26, _t9_26, 32), _mm256_permute2f128_pd(_t9_26, _t9_26, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_127 = _t9_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_128 = _mm256_mul_pd(_t9_126, _t9_127);

    // 4-BLAC: 1x4 - 1x4
    _t9_129 = _mm256_sub_pd(_t9_125, _t9_128);

    // AVX Storer:
    _t9_27 = _t9_129;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_130 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_27, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_131 = _t9_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_132 = _t9_10;

    // 4-BLAC: 1x4 + 1x4
    _t9_133 = _mm256_add_pd(_t9_131, _t9_132);

    // 4-BLAC: 1x4 / 1x4
    _t9_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_130), _mm256_castpd256_pd128(_t9_133)));

    // AVX Storer:
    _t9_28 = _t9_134;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_135 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_27, 6), _mm256_permute2f128_pd(_t9_27, _t9_27, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_136 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_28, _t9_28, 32), _mm256_permute2f128_pd(_t9_28, _t9_28, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_137 = _t9_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_138 = _mm256_mul_pd(_t9_136, _t9_137);

    // 4-BLAC: 1x4 - 1x4
    _t9_139 = _mm256_sub_pd(_t9_135, _t9_138);

    // AVX Storer:
    _t9_29 = _t9_139;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_140 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_29, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_141 = _t9_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_142 = _t9_8;

    // 4-BLAC: 1x4 + 1x4
    _t9_143 = _mm256_add_pd(_t9_141, _t9_142);

    // 4-BLAC: 1x4 / 1x4
    _t9_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_140), _mm256_castpd256_pd128(_t9_143)));

    // AVX Storer:
    _t9_30 = _t9_144;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_145 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_29, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_146 = _t9_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_147 = _t9_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_148 = _mm256_mul_pd(_t9_146, _t9_147);

    // 4-BLAC: 1x4 - 1x4
    _t9_149 = _mm256_sub_pd(_t9_145, _t9_148);

    // AVX Storer:
    _t9_31 = _t9_149;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_150 = _t9_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_151 = _t9_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_152 = _t9_6;

    // 4-BLAC: 1x4 + 1x4
    _t9_153 = _mm256_add_pd(_t9_151, _t9_152);

    // 4-BLAC: 1x4 / 1x4
    _t9_154 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_150), _mm256_castpd256_pd128(_t9_153)));

    // AVX Storer:
    _t9_31 = _t9_154;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, fi1306 + 3), L[76,76],h(3, 76, fi1306)) * G(h(3, 76, fi1306), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_155 = _t9_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t9_156 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_14, _t9_16), _mm256_unpacklo_pd(_t9_18, _t9_19), 32);
    _t9_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_20, _t9_22), _mm256_unpacklo_pd(_t9_24, _t9_25), 32);
    _t9_158 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_26, _t9_28), _mm256_unpacklo_pd(_t9_30, _t9_31), 32);
    _t9_159 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t9_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_155, _t9_155, 32), _mm256_permute2f128_pd(_t9_155, _t9_155, 32), 0), _t9_156), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_155, _t9_155, 32), _mm256_permute2f128_pd(_t9_155, _t9_155, 32), 15), _t9_157)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_155, _t9_155, 49), _mm256_permute2f128_pd(_t9_155, _t9_155, 49), 0), _t9_158), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_155, _t9_155, 49), _mm256_permute2f128_pd(_t9_155, _t9_155, 49), 15), _t9_159)));

    // 4-BLAC: 1x4 - 1x4
    _t9_43 = _mm256_sub_pd(_t9_43, _t9_40);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_160 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_43, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_161 = _t9_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_162 = _t9_12;

    // 4-BLAC: 1x4 + 1x4
    _t9_163 = _mm256_add_pd(_t9_161, _t9_162);

    // 4-BLAC: 1x4 / 1x4
    _t9_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_160), _mm256_castpd256_pd128(_t9_163)));

    // AVX Storer:
    _t9_32 = _t9_164;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_165 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_43, 14), _mm256_permute2f128_pd(_t9_43, _t9_43, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_166 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_32, _t9_32, 32), _mm256_permute2f128_pd(_t9_32, _t9_32, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t9_167 = _t9_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_168 = _mm256_mul_pd(_t9_166, _t9_167);

    // 4-BLAC: 1x4 - 1x4
    _t9_169 = _mm256_sub_pd(_t9_165, _t9_168);

    // AVX Storer:
    _t9_33 = _t9_169;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_170 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_33, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_171 = _t9_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_172 = _t9_10;

    // 4-BLAC: 1x4 + 1x4
    _t9_173 = _mm256_add_pd(_t9_171, _t9_172);

    // 4-BLAC: 1x4 / 1x4
    _t9_174 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_170), _mm256_castpd256_pd128(_t9_173)));

    // AVX Storer:
    _t9_34 = _t9_174;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_175 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_33, 6), _mm256_permute2f128_pd(_t9_33, _t9_33, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_176 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t9_34, _t9_34, 32), _mm256_permute2f128_pd(_t9_34, _t9_34, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t9_177 = _t9_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_178 = _mm256_mul_pd(_t9_176, _t9_177);

    // 4-BLAC: 1x4 - 1x4
    _t9_179 = _mm256_sub_pd(_t9_175, _t9_178);

    // AVX Storer:
    _t9_35 = _t9_179;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_180 = _mm256_blend_pd(_mm256_setzero_pd(), _t9_35, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_181 = _t9_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_182 = _t9_8;

    // 4-BLAC: 1x4 + 1x4
    _t9_183 = _mm256_add_pd(_t9_181, _t9_182);

    // 4-BLAC: 1x4 / 1x4
    _t9_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_180), _mm256_castpd256_pd128(_t9_183)));

    // AVX Storer:
    _t9_36 = _t9_184;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_185 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t9_35, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_186 = _t9_36;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_187 = _t9_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t9_188 = _mm256_mul_pd(_t9_186, _t9_187);

    // 4-BLAC: 1x4 - 1x4
    _t9_189 = _mm256_sub_pd(_t9_185, _t9_188);

    // AVX Storer:
    _t9_37 = _t9_189;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_190 = _t9_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_191 = _t9_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t9_192 = _t9_6;

    // 4-BLAC: 1x4 + 1x4
    _t9_193 = _mm256_add_pd(_t9_191, _t9_192);

    // 4-BLAC: 1x4 / 1x4
    _t9_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t9_190), _mm256_castpd256_pd128(_t9_193)));

    // AVX Storer:
    _t9_37 = _t9_194;

    // Generating : X[76,76] = Sum_{j123} ( S(h(4, 76, fi1306), ( G(h(4, 76, fi1306), X[76,76],h(4, 76, fi1509 + j123 + 4)) - ( G(h(4, 76, fi1306), X[76,76],h(4, 76, fi1509)) * G(h(4, 76, fi1509), U[76,76],h(4, 76, fi1509 + j123 + 4)) ) ),h(4, 76, fi1509 + j123 + 4)) )

    // AVX Loader:
    _mm_store_sd(&(C[fi1509 + 608]), _mm256_castpd256_pd128(_t9_14));
    _mm_store_sd(&(C[fi1509 + 609]), _mm256_castpd256_pd128(_t9_16));
    _mm_store_sd(&(C[fi1509 + 610]), _mm256_castpd256_pd128(_t9_18));
    _mm_store_sd(&(C[fi1509 + 611]), _mm256_castpd256_pd128(_t9_19));
    _mm_store_sd(&(C[fi1509 + 684]), _mm256_castpd256_pd128(_t9_20));
    _mm_store_sd(&(C[fi1509 + 685]), _mm256_castpd256_pd128(_t9_22));
    _mm_store_sd(&(C[fi1509 + 686]), _mm256_castpd256_pd128(_t9_24));
    _mm_store_sd(&(C[fi1509 + 687]), _mm256_castpd256_pd128(_t9_25));
    _mm_store_sd(&(C[fi1509 + 760]), _mm256_castpd256_pd128(_t9_26));
    _mm_store_sd(&(C[fi1509 + 761]), _mm256_castpd256_pd128(_t9_28));
    _mm_store_sd(&(C[fi1509 + 762]), _mm256_castpd256_pd128(_t9_30));
    _mm_store_sd(&(C[fi1509 + 763]), _mm256_castpd256_pd128(_t9_31));
    _mm_store_sd(&(C[fi1509 + 836]), _mm256_castpd256_pd128(_t9_32));
    _mm_store_sd(&(C[fi1509 + 837]), _mm256_castpd256_pd128(_t9_34));
    _mm_store_sd(&(C[fi1509 + 838]), _mm256_castpd256_pd128(_t9_36));
    _mm_store_sd(&(C[fi1509 + 839]), _mm256_castpd256_pd128(_t9_37));

    for( int j123 = 0; j123 <= -fi1509 + 71; j123+=4 ) {
      _t10_24 = _asm256_loadu_pd(C + fi1509 + j123 + 612);
      _t10_25 = _asm256_loadu_pd(C + fi1509 + j123 + 688);
      _t10_26 = _asm256_loadu_pd(C + fi1509 + j123 + 764);
      _t10_27 = _asm256_loadu_pd(C + fi1509 + j123 + 840);
      _t10_19 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 4);
      _t10_18 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 80);
      _t10_17 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 156);
      _t10_16 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 232);
      _t10_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 608])));
      _t10_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 609])));
      _t10_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 610])));
      _t10_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 611])));
      _t10_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 684])));
      _t10_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 685])));
      _t10_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 686])));
      _t10_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 687])));
      _t10_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 760])));
      _t10_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 761])));
      _t10_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 762])));
      _t10_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 763])));
      _t10_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 836])));
      _t10_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 837])));
      _t10_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 838])));
      _t10_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[fi1509 + 839])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t10_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_15, _t10_15, 32), _mm256_permute2f128_pd(_t10_15, _t10_15, 32), 0), _t10_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_14, _t10_14, 32), _mm256_permute2f128_pd(_t10_14, _t10_14, 32), 0), _t10_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_13, _t10_13, 32), _mm256_permute2f128_pd(_t10_13, _t10_13, 32), 0), _t10_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_12, _t10_12, 32), _mm256_permute2f128_pd(_t10_12, _t10_12, 32), 0), _t10_16)));
      _t10_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_11, _t10_11, 32), _mm256_permute2f128_pd(_t10_11, _t10_11, 32), 0), _t10_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_10, _t10_10, 32), _mm256_permute2f128_pd(_t10_10, _t10_10, 32), 0), _t10_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_9, _t10_9, 32), _mm256_permute2f128_pd(_t10_9, _t10_9, 32), 0), _t10_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_8, _t10_8, 32), _mm256_permute2f128_pd(_t10_8, _t10_8, 32), 0), _t10_16)));
      _t10_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_7, _t10_7, 32), _mm256_permute2f128_pd(_t10_7, _t10_7, 32), 0), _t10_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_6, _t10_6, 32), _mm256_permute2f128_pd(_t10_6, _t10_6, 32), 0), _t10_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_5, _t10_5, 32), _mm256_permute2f128_pd(_t10_5, _t10_5, 32), 0), _t10_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_4, _t10_4, 32), _mm256_permute2f128_pd(_t10_4, _t10_4, 32), 0), _t10_16)));
      _t10_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_3, _t10_3, 32), _mm256_permute2f128_pd(_t10_3, _t10_3, 32), 0), _t10_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_2, _t10_2, 32), _mm256_permute2f128_pd(_t10_2, _t10_2, 32), 0), _t10_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_1, _t10_1, 32), _mm256_permute2f128_pd(_t10_1, _t10_1, 32), 0), _t10_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t10_0, _t10_0, 32), _mm256_permute2f128_pd(_t10_0, _t10_0, 32), 0), _t10_16)));

      // 4-BLAC: 4x4 - 4x4
      _t10_24 = _mm256_sub_pd(_t10_24, _t10_20);
      _t10_25 = _mm256_sub_pd(_t10_25, _t10_21);
      _t10_26 = _mm256_sub_pd(_t10_26, _t10_22);
      _t10_27 = _mm256_sub_pd(_t10_27, _t10_23);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1509 + j123 + 612, _t10_24);
      _asm256_storeu_pd(C + fi1509 + j123 + 688, _t10_25);
      _asm256_storeu_pd(C + fi1509 + j123 + 764, _t10_26);
      _asm256_storeu_pd(C + fi1509 + j123 + 840, _t10_27);
    }
  }

  _t11_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[680])));
  _t11_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[616])));
  _t11_8 = _mm256_maskload_pd(C + 681, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t11_34 = _asm256_loadu_pd(C + 756);
  _t11_5 = _mm256_broadcast_sd(&(L[692]));
  _t11_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[693])));
  _t11_35 = _asm256_loadu_pd(C + 832);
  _t11_3 = _mm256_maskload_pd(L + 768, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t11_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[770])));
  _t11_36 = _asm256_loadu_pd(C + 908);
  _t11_1 = _mm256_maskload_pd(L + 844, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t11_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[847])));

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_37 = _t11_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_38 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_39 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t11_40 = _mm256_add_pd(_t11_38, _t11_39);

  // 4-BLAC: 1x4 / 1x4
  _t11_41 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_37), _mm256_castpd256_pd128(_t11_40)));

  // AVX Storer:
  _t11_7 = _t11_41;

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_42 = _t11_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_7, _t11_7, 32), _mm256_permute2f128_pd(_t11_7, _t11_7, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_44 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_45 = _mm256_mul_pd(_t11_43, _t11_44);

  // 4-BLAC: 1x4 - 1x4
  _t11_46 = _mm256_sub_pd(_t11_42, _t11_45);

  // AVX Storer:
  _t11_8 = _t11_46;

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_47 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_8, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_48 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_49 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t11_50 = _mm256_add_pd(_t11_48, _t11_49);

  // 4-BLAC: 1x4 / 1x4
  _t11_51 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_47), _mm256_castpd256_pd128(_t11_50)));

  // AVX Storer:
  _t11_9 = _t11_51;

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_52 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_8, 6), _mm256_permute2f128_pd(_t11_8, _t11_8, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_53 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_9, _t11_9, 32), _mm256_permute2f128_pd(_t11_9, _t11_9, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_54 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_55 = _mm256_mul_pd(_t11_53, _t11_54);

  // 4-BLAC: 1x4 - 1x4
  _t11_56 = _mm256_sub_pd(_t11_52, _t11_55);

  // AVX Storer:
  _t11_10 = _t11_56;

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_57 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_58 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_59 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t11_60 = _mm256_add_pd(_t11_58, _t11_59);

  // 4-BLAC: 1x4 / 1x4
  _t11_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_57), _mm256_castpd256_pd128(_t11_60)));

  // AVX Storer:
  _t11_11 = _t11_61;

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_62 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_63 = _t11_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_64 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_65 = _mm256_mul_pd(_t11_63, _t11_64);

  // 4-BLAC: 1x4 - 1x4
  _t11_66 = _mm256_sub_pd(_t11_62, _t11_65);

  // AVX Storer:
  _t11_12 = _t11_66;

  // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_67 = _t11_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_68 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_69 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_70 = _mm256_add_pd(_t11_68, _t11_69);

  // 4-BLAC: 1x4 / 1x4
  _t11_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_67), _mm256_castpd256_pd128(_t11_70)));

  // AVX Storer:
  _t11_12 = _t11_71;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_72 = _t11_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t11_31 = _mm256_mul_pd(_t11_72, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_9), _mm256_unpacklo_pd(_t11_11, _t11_12), 32));

  // 4-BLAC: 1x4 - 1x4
  _t11_34 = _mm256_sub_pd(_t11_34, _t11_31);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_73 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_34, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_74 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_75 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t11_76 = _mm256_add_pd(_t11_74, _t11_75);

  // 4-BLAC: 1x4 / 1x4
  _t11_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_73), _mm256_castpd256_pd128(_t11_76)));

  // AVX Storer:
  _t11_13 = _t11_77;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_78 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_34, 14), _mm256_permute2f128_pd(_t11_34, _t11_34, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_79 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_13, _t11_13, 32), _mm256_permute2f128_pd(_t11_13, _t11_13, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_80 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_81 = _mm256_mul_pd(_t11_79, _t11_80);

  // 4-BLAC: 1x4 - 1x4
  _t11_82 = _mm256_sub_pd(_t11_78, _t11_81);

  // AVX Storer:
  _t11_14 = _t11_82;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_83 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_14, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_84 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_85 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t11_86 = _mm256_add_pd(_t11_84, _t11_85);

  // 4-BLAC: 1x4 / 1x4
  _t11_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_83), _mm256_castpd256_pd128(_t11_86)));

  // AVX Storer:
  _t11_15 = _t11_87;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_88 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_14, 6), _mm256_permute2f128_pd(_t11_14, _t11_14, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_89 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_15, _t11_15, 32), _mm256_permute2f128_pd(_t11_15, _t11_15, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_90 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_91 = _mm256_mul_pd(_t11_89, _t11_90);

  // 4-BLAC: 1x4 - 1x4
  _t11_92 = _mm256_sub_pd(_t11_88, _t11_91);

  // AVX Storer:
  _t11_16 = _t11_92;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_93 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_16, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_94 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_95 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t11_96 = _mm256_add_pd(_t11_94, _t11_95);

  // 4-BLAC: 1x4 / 1x4
  _t11_97 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_93), _mm256_castpd256_pd128(_t11_96)));

  // AVX Storer:
  _t11_17 = _t11_97;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_98 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_16, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_99 = _t11_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_100 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_101 = _mm256_mul_pd(_t11_99, _t11_100);

  // 4-BLAC: 1x4 - 1x4
  _t11_102 = _mm256_sub_pd(_t11_98, _t11_101);

  // AVX Storer:
  _t11_18 = _t11_102;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_103 = _t11_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_104 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_105 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_106 = _mm256_add_pd(_t11_104, _t11_105);

  // 4-BLAC: 1x4 / 1x4
  _t11_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_103), _mm256_castpd256_pd128(_t11_106)));

  // AVX Storer:
  _t11_18 = _t11_107;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, fi1306 + 2), L[76,76],h(2, 76, fi1306)) * G(h(2, 76, fi1306), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_108 = _t11_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t11_109 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_9), _mm256_unpacklo_pd(_t11_11, _t11_12), 32);
  _t11_110 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_13, _t11_15), _mm256_unpacklo_pd(_t11_17, _t11_18), 32);
  _t11_111 = _mm256_setzero_pd();
  _t11_112 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_108, _t11_108, 32), _mm256_permute2f128_pd(_t11_108, _t11_108, 32), 0), _t11_109), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_108, _t11_108, 32), _mm256_permute2f128_pd(_t11_108, _t11_108, 32), 15), _t11_110)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_108, _t11_108, 49), _mm256_permute2f128_pd(_t11_108, _t11_108, 49), 0), _t11_111), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_108, _t11_108, 49), _mm256_permute2f128_pd(_t11_108, _t11_108, 49), 15), _t11_112)));

  // 4-BLAC: 1x4 - 1x4
  _t11_35 = _mm256_sub_pd(_t11_35, _t11_32);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_113 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_35, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_114 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_115 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t11_116 = _mm256_add_pd(_t11_114, _t11_115);

  // 4-BLAC: 1x4 / 1x4
  _t11_117 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_113), _mm256_castpd256_pd128(_t11_116)));

  // AVX Storer:
  _t11_19 = _t11_117;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_118 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_35, 14), _mm256_permute2f128_pd(_t11_35, _t11_35, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_119 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_19, _t11_19, 32), _mm256_permute2f128_pd(_t11_19, _t11_19, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_120 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_121 = _mm256_mul_pd(_t11_119, _t11_120);

  // 4-BLAC: 1x4 - 1x4
  _t11_122 = _mm256_sub_pd(_t11_118, _t11_121);

  // AVX Storer:
  _t11_20 = _t11_122;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_20, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_124 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_125 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t11_126 = _mm256_add_pd(_t11_124, _t11_125);

  // 4-BLAC: 1x4 / 1x4
  _t11_127 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_123), _mm256_castpd256_pd128(_t11_126)));

  // AVX Storer:
  _t11_21 = _t11_127;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_128 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_20, 6), _mm256_permute2f128_pd(_t11_20, _t11_20, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_129 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_21, _t11_21, 32), _mm256_permute2f128_pd(_t11_21, _t11_21, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_130 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_131 = _mm256_mul_pd(_t11_129, _t11_130);

  // 4-BLAC: 1x4 - 1x4
  _t11_132 = _mm256_sub_pd(_t11_128, _t11_131);

  // AVX Storer:
  _t11_22 = _t11_132;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_133 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_22, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_134 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_135 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t11_136 = _mm256_add_pd(_t11_134, _t11_135);

  // 4-BLAC: 1x4 / 1x4
  _t11_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_133), _mm256_castpd256_pd128(_t11_136)));

  // AVX Storer:
  _t11_23 = _t11_137;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_138 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_22, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_139 = _t11_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_140 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_141 = _mm256_mul_pd(_t11_139, _t11_140);

  // 4-BLAC: 1x4 - 1x4
  _t11_142 = _mm256_sub_pd(_t11_138, _t11_141);

  // AVX Storer:
  _t11_24 = _t11_142;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_143 = _t11_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_144 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_145 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_146 = _mm256_add_pd(_t11_144, _t11_145);

  // 4-BLAC: 1x4 / 1x4
  _t11_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_143), _mm256_castpd256_pd128(_t11_146)));

  // AVX Storer:
  _t11_24 = _t11_147;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, fi1306 + 3), L[76,76],h(3, 76, fi1306)) * G(h(3, 76, fi1306), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_148 = _t11_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t11_149 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_9), _mm256_unpacklo_pd(_t11_11, _t11_12), 32);
  _t11_150 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_13, _t11_15), _mm256_unpacklo_pd(_t11_17, _t11_18), 32);
  _t11_151 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_19, _t11_21), _mm256_unpacklo_pd(_t11_23, _t11_24), 32);
  _t11_152 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_148, _t11_148, 32), _mm256_permute2f128_pd(_t11_148, _t11_148, 32), 0), _t11_149), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_148, _t11_148, 32), _mm256_permute2f128_pd(_t11_148, _t11_148, 32), 15), _t11_150)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_148, _t11_148, 49), _mm256_permute2f128_pd(_t11_148, _t11_148, 49), 0), _t11_151), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_148, _t11_148, 49), _mm256_permute2f128_pd(_t11_148, _t11_148, 49), 15), _t11_152)));

  // 4-BLAC: 1x4 - 1x4
  _t11_36 = _mm256_sub_pd(_t11_36, _t11_33);

  // AVX Storer:

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_153 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_36, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_154 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_155 = _t2_12;

  // 4-BLAC: 1x4 + 1x4
  _t11_156 = _mm256_add_pd(_t11_154, _t11_155);

  // 4-BLAC: 1x4 / 1x4
  _t11_157 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_153), _mm256_castpd256_pd128(_t11_156)));

  // AVX Storer:
  _t11_25 = _t11_157;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_158 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_36, 14), _mm256_permute2f128_pd(_t11_36, _t11_36, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_159 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_25, _t11_25, 32), _mm256_permute2f128_pd(_t11_25, _t11_25, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_160 = _t2_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_161 = _mm256_mul_pd(_t11_159, _t11_160);

  // 4-BLAC: 1x4 - 1x4
  _t11_162 = _mm256_sub_pd(_t11_158, _t11_161);

  // AVX Storer:
  _t11_26 = _t11_162;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_163 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_26, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_164 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_165 = _t2_10;

  // 4-BLAC: 1x4 + 1x4
  _t11_166 = _mm256_add_pd(_t11_164, _t11_165);

  // 4-BLAC: 1x4 / 1x4
  _t11_167 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_163), _mm256_castpd256_pd128(_t11_166)));

  // AVX Storer:
  _t11_27 = _t11_167;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_168 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_26, 6), _mm256_permute2f128_pd(_t11_26, _t11_26, 129), 5);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_169 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_27, _t11_27, 32), _mm256_permute2f128_pd(_t11_27, _t11_27, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_170 = _t2_9;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_171 = _mm256_mul_pd(_t11_169, _t11_170);

  // 4-BLAC: 1x4 - 1x4
  _t11_172 = _mm256_sub_pd(_t11_168, _t11_171);

  // AVX Storer:
  _t11_28 = _t11_172;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_173 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_28, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_174 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_175 = _t2_8;

  // 4-BLAC: 1x4 + 1x4
  _t11_176 = _mm256_add_pd(_t11_174, _t11_175);

  // 4-BLAC: 1x4 / 1x4
  _t11_177 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_173), _mm256_castpd256_pd128(_t11_176)));

  // AVX Storer:
  _t11_29 = _t11_177;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_178 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_28, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_179 = _t11_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_180 = _t2_7;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_181 = _mm256_mul_pd(_t11_179, _t11_180);

  // 4-BLAC: 1x4 - 1x4
  _t11_182 = _mm256_sub_pd(_t11_178, _t11_181);

  // AVX Storer:
  _t11_30 = _t11_182;

  // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_183 = _t11_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_184 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_185 = _t2_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_186 = _mm256_add_pd(_t11_184, _t11_185);

  // 4-BLAC: 1x4 / 1x4
  _t11_187 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_183), _mm256_castpd256_pd128(_t11_186)));

  // AVX Storer:
  _t11_30 = _t11_187;

  _mm_store_sd(&(C[680]), _mm256_castpd256_pd128(_t11_7));
  _mm_store_sd(&(C[681]), _mm256_castpd256_pd128(_t11_9));
  _mm_store_sd(&(C[682]), _mm256_castpd256_pd128(_t11_11));
  _mm_store_sd(&(C[683]), _mm256_castpd256_pd128(_t11_12));
  _mm_store_sd(&(C[756]), _mm256_castpd256_pd128(_t11_13));
  _mm_store_sd(&(C[757]), _mm256_castpd256_pd128(_t11_15));
  _mm_store_sd(&(C[758]), _mm256_castpd256_pd128(_t11_17));
  _mm_store_sd(&(C[759]), _mm256_castpd256_pd128(_t11_18));
  _mm_store_sd(&(C[832]), _mm256_castpd256_pd128(_t11_19));
  _mm_store_sd(&(C[833]), _mm256_castpd256_pd128(_t11_21));
  _mm_store_sd(&(C[834]), _mm256_castpd256_pd128(_t11_23));
  _mm_store_sd(&(C[835]), _mm256_castpd256_pd128(_t11_24));
  _mm_store_sd(&(C[908]), _mm256_castpd256_pd128(_t11_25));
  _mm_store_sd(&(C[909]), _mm256_castpd256_pd128(_t11_27));
  _mm_store_sd(&(C[910]), _mm256_castpd256_pd128(_t11_29));
  _mm_store_sd(&(C[911]), _mm256_castpd256_pd128(_t11_30));

  for( int fi1306 = 12; fi1306 <= 72; fi1306+=4 ) {

    // Generating : X[76,76] = ( Sum_{k249} ( S(h(4, 76, fi1306), ( G(h(4, 76, fi1306), C[76,76],h(4, 76, k249)) - ( G(h(4, 76, fi1306), L[76,76],h(4, 76, 0)) * G(h(4, 76, 0), X[76,76],h(4, 76, k249)) ) ),h(4, 76, k249)) ) + Sum_{j123} ( Sum_{k249} ( -$(h(4, 76, fi1306), ( G(h(4, 76, fi1306), L[76,76],h(4, 76, j123)) * G(h(4, 76, j123), X[76,76],h(4, 76, k249)) ),h(4, 76, k249)) ) ) )

    // AVX Loader:

    for( int k249 = 0; k249 <= 75; k249+=4 ) {
      _t12_24 = _asm256_loadu_pd(C + 76*fi1306 + k249);
      _t12_25 = _asm256_loadu_pd(C + 76*fi1306 + k249 + 76);
      _t12_26 = _asm256_loadu_pd(C + 76*fi1306 + k249 + 152);
      _t12_27 = _asm256_loadu_pd(C + 76*fi1306 + k249 + 228);
      _t12_19 = _mm256_broadcast_sd(L + 76*fi1306);
      _t12_18 = _mm256_broadcast_sd(L + 76*fi1306 + 1);
      _t12_17 = _mm256_broadcast_sd(L + 76*fi1306 + 2);
      _t12_16 = _mm256_broadcast_sd(L + 76*fi1306 + 3);
      _t12_15 = _mm256_broadcast_sd(L + 76*fi1306 + 76);
      _t12_14 = _mm256_broadcast_sd(L + 76*fi1306 + 77);
      _t12_13 = _mm256_broadcast_sd(L + 76*fi1306 + 78);
      _t12_12 = _mm256_broadcast_sd(L + 76*fi1306 + 79);
      _t12_11 = _mm256_broadcast_sd(L + 76*fi1306 + 152);
      _t12_10 = _mm256_broadcast_sd(L + 76*fi1306 + 153);
      _t12_9 = _mm256_broadcast_sd(L + 76*fi1306 + 154);
      _t12_8 = _mm256_broadcast_sd(L + 76*fi1306 + 155);
      _t12_7 = _mm256_broadcast_sd(L + 76*fi1306 + 228);
      _t12_6 = _mm256_broadcast_sd(L + 76*fi1306 + 229);
      _t12_5 = _mm256_broadcast_sd(L + 76*fi1306 + 230);
      _t12_4 = _mm256_broadcast_sd(L + 76*fi1306 + 231);
      _t12_3 = _asm256_loadu_pd(C + k249);
      _t12_2 = _asm256_loadu_pd(C + k249 + 76);
      _t12_1 = _asm256_loadu_pd(C + k249 + 152);
      _t12_0 = _asm256_loadu_pd(C + k249 + 228);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t12_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_19, _t12_3), _mm256_mul_pd(_t12_18, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_17, _t12_1), _mm256_mul_pd(_t12_16, _t12_0)));
      _t12_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t12_3), _mm256_mul_pd(_t12_14, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t12_1), _mm256_mul_pd(_t12_12, _t12_0)));
      _t12_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t12_3), _mm256_mul_pd(_t12_10, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t12_1), _mm256_mul_pd(_t12_8, _t12_0)));
      _t12_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t12_3), _mm256_mul_pd(_t12_6, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t12_1), _mm256_mul_pd(_t12_4, _t12_0)));

      // 4-BLAC: 4x4 - 4x4
      _t12_24 = _mm256_sub_pd(_t12_24, _t12_20);
      _t12_25 = _mm256_sub_pd(_t12_25, _t12_21);
      _t12_26 = _mm256_sub_pd(_t12_26, _t12_22);
      _t12_27 = _mm256_sub_pd(_t12_27, _t12_23);

      // AVX Storer:
      _asm256_storeu_pd(C + 76*fi1306 + k249, _t12_24);
      _asm256_storeu_pd(C + 76*fi1306 + k249 + 76, _t12_25);
      _asm256_storeu_pd(C + 76*fi1306 + k249 + 152, _t12_26);
      _asm256_storeu_pd(C + 76*fi1306 + k249 + 228, _t12_27);
    }

    for( int j123 = 4; j123 <= fi1306 - 1; j123+=4 ) {

      for( int k249 = 0; k249 <= 75; k249+=4 ) {
        _t13_19 = _mm256_broadcast_sd(L + 76*fi1306 + j123);
        _t13_18 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 1);
        _t13_17 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 2);
        _t13_16 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 3);
        _t13_15 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 76);
        _t13_14 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 77);
        _t13_13 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 78);
        _t13_12 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 79);
        _t13_11 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 152);
        _t13_10 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 153);
        _t13_9 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 154);
        _t13_8 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 155);
        _t13_7 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 228);
        _t13_6 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 229);
        _t13_5 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 230);
        _t13_4 = _mm256_broadcast_sd(L + 76*fi1306 + j123 + 231);
        _t13_3 = _asm256_loadu_pd(C + 76*j123 + k249);
        _t13_2 = _asm256_loadu_pd(C + 76*j123 + k249 + 76);
        _t13_1 = _asm256_loadu_pd(C + 76*j123 + k249 + 152);
        _t13_0 = _asm256_loadu_pd(C + 76*j123 + k249 + 228);
        _t13_20 = _asm256_loadu_pd(C + 76*fi1306 + k249);
        _t13_21 = _asm256_loadu_pd(C + 76*fi1306 + k249 + 76);
        _t13_22 = _asm256_loadu_pd(C + 76*fi1306 + k249 + 152);
        _t13_23 = _asm256_loadu_pd(C + 76*fi1306 + k249 + 228);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t13_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_3), _mm256_mul_pd(_t13_18, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_1), _mm256_mul_pd(_t13_16, _t13_0)));
        _t13_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_3), _mm256_mul_pd(_t13_14, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_1), _mm256_mul_pd(_t13_12, _t13_0)));
        _t13_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_3), _mm256_mul_pd(_t13_10, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_1), _mm256_mul_pd(_t13_8, _t13_0)));
        _t13_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_3), _mm256_mul_pd(_t13_6, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_1), _mm256_mul_pd(_t13_4, _t13_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t13_20 = _mm256_sub_pd(_t13_20, _t13_24);
        _t13_21 = _mm256_sub_pd(_t13_21, _t13_25);
        _t13_22 = _mm256_sub_pd(_t13_22, _t13_26);
        _t13_23 = _mm256_sub_pd(_t13_23, _t13_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 76*fi1306 + k249, _t13_20);
        _asm256_storeu_pd(C + 76*fi1306 + k249 + 76, _t13_21);
        _asm256_storeu_pd(C + 76*fi1306 + k249 + 152, _t13_22);
        _asm256_storeu_pd(C + 76*fi1306 + k249 + 228, _t13_23);
      }
    }

    for( int fi1509 = 0; fi1509 <= 71; fi1509+=4 ) {
      _t14_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509])));
      _t14_13 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306])));
      _t14_12 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509])));
      _t14_15 = _mm256_maskload_pd(C + 76*fi1306 + fi1509 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t14_11 = _mm256_maskload_pd(U + 77*fi1509 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t14_10 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 77])));
      _t14_9 = _mm256_maskload_pd(U + 77*fi1509 + 78, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t14_8 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 154])));
      _t14_7 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 155])));
      _t14_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[77*fi1509 + 231])));
      _t14_41 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + 76);
      _t14_5 = _mm256_broadcast_sd(&(L[77*fi1306 + 76]));
      _t14_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306 + 77])));
      _t14_42 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + 152);
      _t14_3 = _mm256_maskload_pd(L + 77*fi1306 + 152, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t14_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306 + 154])));
      _t14_43 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + 228);
      _t14_1 = _mm256_maskload_pd(L + 77*fi1306 + 228, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t14_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306 + 231])));

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_44 = _t14_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_45 = _t14_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_46 = _t14_12;

      // 4-BLAC: 1x4 + 1x4
      _t14_47 = _mm256_add_pd(_t14_45, _t14_46);

      // 4-BLAC: 1x4 / 1x4
      _t14_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_44), _mm256_castpd256_pd128(_t14_47)));

      // AVX Storer:
      _t14_14 = _t14_48;

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_49 = _t14_15;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_14, _t14_14, 32), _mm256_permute2f128_pd(_t14_14, _t14_14, 32), 0);

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_51 = _t14_11;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_52 = _mm256_mul_pd(_t14_50, _t14_51);

      // 4-BLAC: 1x4 - 1x4
      _t14_53 = _mm256_sub_pd(_t14_49, _t14_52);

      // AVX Storer:
      _t14_15 = _t14_53;

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_54 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_15, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_55 = _t14_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_56 = _t14_10;

      // 4-BLAC: 1x4 + 1x4
      _t14_57 = _mm256_add_pd(_t14_55, _t14_56);

      // 4-BLAC: 1x4 / 1x4
      _t14_58 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_54), _mm256_castpd256_pd128(_t14_57)));

      // AVX Storer:
      _t14_16 = _t14_58;

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_59 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_15, 6), _mm256_permute2f128_pd(_t14_15, _t14_15, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_60 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_16, _t14_16, 32), _mm256_permute2f128_pd(_t14_16, _t14_16, 32), 0);

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_61 = _t14_9;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_62 = _mm256_mul_pd(_t14_60, _t14_61);

      // 4-BLAC: 1x4 - 1x4
      _t14_63 = _mm256_sub_pd(_t14_59, _t14_62);

      // AVX Storer:
      _t14_17 = _t14_63;

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_64 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_17, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_65 = _t14_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_66 = _t14_8;

      // 4-BLAC: 1x4 + 1x4
      _t14_67 = _mm256_add_pd(_t14_65, _t14_66);

      // 4-BLAC: 1x4 / 1x4
      _t14_68 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_64), _mm256_castpd256_pd128(_t14_67)));

      // AVX Storer:
      _t14_18 = _t14_68;

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_69 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_17, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_70 = _t14_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_71 = _t14_7;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_72 = _mm256_mul_pd(_t14_70, _t14_71);

      // 4-BLAC: 1x4 - 1x4
      _t14_73 = _mm256_sub_pd(_t14_69, _t14_72);

      // AVX Storer:
      _t14_19 = _t14_73;

      // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_74 = _t14_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_75 = _t14_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_76 = _t14_6;

      // 4-BLAC: 1x4 + 1x4
      _t14_77 = _mm256_add_pd(_t14_75, _t14_76);

      // 4-BLAC: 1x4 / 1x4
      _t14_78 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_74), _mm256_castpd256_pd128(_t14_77)));

      // AVX Storer:
      _t14_19 = _t14_78;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_79 = _t14_5;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t14_38 = _mm256_mul_pd(_t14_79, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_14, _t14_16), _mm256_unpacklo_pd(_t14_18, _t14_19), 32));

      // 4-BLAC: 1x4 - 1x4
      _t14_41 = _mm256_sub_pd(_t14_41, _t14_38);

      // AVX Storer:

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_80 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_41, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_81 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_82 = _t14_12;

      // 4-BLAC: 1x4 + 1x4
      _t14_83 = _mm256_add_pd(_t14_81, _t14_82);

      // 4-BLAC: 1x4 / 1x4
      _t14_84 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_80), _mm256_castpd256_pd128(_t14_83)));

      // AVX Storer:
      _t14_20 = _t14_84;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_85 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_41, 14), _mm256_permute2f128_pd(_t14_41, _t14_41, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_86 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_20, _t14_20, 32), _mm256_permute2f128_pd(_t14_20, _t14_20, 32), 0);

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_87 = _t14_11;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_88 = _mm256_mul_pd(_t14_86, _t14_87);

      // 4-BLAC: 1x4 - 1x4
      _t14_89 = _mm256_sub_pd(_t14_85, _t14_88);

      // AVX Storer:
      _t14_21 = _t14_89;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_90 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_21, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_91 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_92 = _t14_10;

      // 4-BLAC: 1x4 + 1x4
      _t14_93 = _mm256_add_pd(_t14_91, _t14_92);

      // 4-BLAC: 1x4 / 1x4
      _t14_94 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_90), _mm256_castpd256_pd128(_t14_93)));

      // AVX Storer:
      _t14_22 = _t14_94;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_95 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_21, 6), _mm256_permute2f128_pd(_t14_21, _t14_21, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_96 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_22, _t14_22, 32), _mm256_permute2f128_pd(_t14_22, _t14_22, 32), 0);

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_97 = _t14_9;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_98 = _mm256_mul_pd(_t14_96, _t14_97);

      // 4-BLAC: 1x4 - 1x4
      _t14_99 = _mm256_sub_pd(_t14_95, _t14_98);

      // AVX Storer:
      _t14_23 = _t14_99;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_100 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_23, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_101 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_102 = _t14_8;

      // 4-BLAC: 1x4 + 1x4
      _t14_103 = _mm256_add_pd(_t14_101, _t14_102);

      // 4-BLAC: 1x4 / 1x4
      _t14_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_100), _mm256_castpd256_pd128(_t14_103)));

      // AVX Storer:
      _t14_24 = _t14_104;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_23, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_106 = _t14_24;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_107 = _t14_7;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_108 = _mm256_mul_pd(_t14_106, _t14_107);

      // 4-BLAC: 1x4 - 1x4
      _t14_109 = _mm256_sub_pd(_t14_105, _t14_108);

      // AVX Storer:
      _t14_25 = _t14_109;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_110 = _t14_25;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_111 = _t14_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_112 = _t14_6;

      // 4-BLAC: 1x4 + 1x4
      _t14_113 = _mm256_add_pd(_t14_111, _t14_112);

      // 4-BLAC: 1x4 / 1x4
      _t14_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_110), _mm256_castpd256_pd128(_t14_113)));

      // AVX Storer:
      _t14_25 = _t14_114;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, fi1306 + 2), L[76,76],h(2, 76, fi1306)) * G(h(2, 76, fi1306), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

      // AVX Loader:

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_115 = _t14_3;

      // AVX Loader:

      // 2x4 -> 4x4
      _t14_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_14, _t14_16), _mm256_unpacklo_pd(_t14_18, _t14_19), 32);
      _t14_117 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_20, _t14_22), _mm256_unpacklo_pd(_t14_24, _t14_25), 32);
      _t14_118 = _mm256_setzero_pd();
      _t14_119 = _mm256_setzero_pd();

      // 4-BLAC: 1x4 * 4x4
      _t14_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_115, _t14_115, 32), _mm256_permute2f128_pd(_t14_115, _t14_115, 32), 0), _t14_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_115, _t14_115, 32), _mm256_permute2f128_pd(_t14_115, _t14_115, 32), 15), _t14_117)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_115, _t14_115, 49), _mm256_permute2f128_pd(_t14_115, _t14_115, 49), 0), _t14_118), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_115, _t14_115, 49), _mm256_permute2f128_pd(_t14_115, _t14_115, 49), 15), _t14_119)));

      // 4-BLAC: 1x4 - 1x4
      _t14_42 = _mm256_sub_pd(_t14_42, _t14_39);

      // AVX Storer:

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_120 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_42, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_121 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_122 = _t14_12;

      // 4-BLAC: 1x4 + 1x4
      _t14_123 = _mm256_add_pd(_t14_121, _t14_122);

      // 4-BLAC: 1x4 / 1x4
      _t14_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_120), _mm256_castpd256_pd128(_t14_123)));

      // AVX Storer:
      _t14_26 = _t14_124;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_125 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_42, 14), _mm256_permute2f128_pd(_t14_42, _t14_42, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_126 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_26, _t14_26, 32), _mm256_permute2f128_pd(_t14_26, _t14_26, 32), 0);

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_127 = _t14_11;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_128 = _mm256_mul_pd(_t14_126, _t14_127);

      // 4-BLAC: 1x4 - 1x4
      _t14_129 = _mm256_sub_pd(_t14_125, _t14_128);

      // AVX Storer:
      _t14_27 = _t14_129;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_130 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_27, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_131 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_132 = _t14_10;

      // 4-BLAC: 1x4 + 1x4
      _t14_133 = _mm256_add_pd(_t14_131, _t14_132);

      // 4-BLAC: 1x4 / 1x4
      _t14_134 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_130), _mm256_castpd256_pd128(_t14_133)));

      // AVX Storer:
      _t14_28 = _t14_134;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_135 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_27, 6), _mm256_permute2f128_pd(_t14_27, _t14_27, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_136 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_28, _t14_28, 32), _mm256_permute2f128_pd(_t14_28, _t14_28, 32), 0);

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_137 = _t14_9;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_138 = _mm256_mul_pd(_t14_136, _t14_137);

      // 4-BLAC: 1x4 - 1x4
      _t14_139 = _mm256_sub_pd(_t14_135, _t14_138);

      // AVX Storer:
      _t14_29 = _t14_139;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_140 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_29, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_141 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_142 = _t14_8;

      // 4-BLAC: 1x4 + 1x4
      _t14_143 = _mm256_add_pd(_t14_141, _t14_142);

      // 4-BLAC: 1x4 / 1x4
      _t14_144 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_140), _mm256_castpd256_pd128(_t14_143)));

      // AVX Storer:
      _t14_30 = _t14_144;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_145 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_29, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_146 = _t14_30;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_147 = _t14_7;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_148 = _mm256_mul_pd(_t14_146, _t14_147);

      // 4-BLAC: 1x4 - 1x4
      _t14_149 = _mm256_sub_pd(_t14_145, _t14_148);

      // AVX Storer:
      _t14_31 = _t14_149;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_150 = _t14_31;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_151 = _t14_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_152 = _t14_6;

      // 4-BLAC: 1x4 + 1x4
      _t14_153 = _mm256_add_pd(_t14_151, _t14_152);

      // 4-BLAC: 1x4 / 1x4
      _t14_154 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_150), _mm256_castpd256_pd128(_t14_153)));

      // AVX Storer:
      _t14_31 = _t14_154;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(4, 76, fi1509)) - ( G(h(1, 76, fi1306 + 3), L[76,76],h(3, 76, fi1306)) * G(h(3, 76, fi1306), X[76,76],h(4, 76, fi1509)) ) ),h(4, 76, fi1509))

      // AVX Loader:

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_155 = _t14_1;

      // AVX Loader:

      // 3x4 -> 4x4
      _t14_156 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_14, _t14_16), _mm256_unpacklo_pd(_t14_18, _t14_19), 32);
      _t14_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_20, _t14_22), _mm256_unpacklo_pd(_t14_24, _t14_25), 32);
      _t14_158 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_26, _t14_28), _mm256_unpacklo_pd(_t14_30, _t14_31), 32);
      _t14_159 = _mm256_setzero_pd();

      // 4-BLAC: 1x4 * 4x4
      _t14_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_155, _t14_155, 32), _mm256_permute2f128_pd(_t14_155, _t14_155, 32), 0), _t14_156), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_155, _t14_155, 32), _mm256_permute2f128_pd(_t14_155, _t14_155, 32), 15), _t14_157)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_155, _t14_155, 49), _mm256_permute2f128_pd(_t14_155, _t14_155, 49), 0), _t14_158), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_155, _t14_155, 49), _mm256_permute2f128_pd(_t14_155, _t14_155, 49), 15), _t14_159)));

      // 4-BLAC: 1x4 - 1x4
      _t14_43 = _mm256_sub_pd(_t14_43, _t14_40);

      // AVX Storer:

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509), U[76,76],h(1, 76, fi1509)) ) ),h(1, 76, fi1509))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_160 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_43, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_161 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_162 = _t14_12;

      // 4-BLAC: 1x4 + 1x4
      _t14_163 = _mm256_add_pd(_t14_161, _t14_162);

      // 4-BLAC: 1x4 / 1x4
      _t14_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_160), _mm256_castpd256_pd128(_t14_163)));

      // AVX Storer:
      _t14_32 = _t14_164;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(3, 76, fi1509 + 1)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509)) Kro G(h(1, 76, fi1509), U[76,76],h(3, 76, fi1509 + 1)) ) ),h(3, 76, fi1509 + 1))

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_165 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_43, 14), _mm256_permute2f128_pd(_t14_43, _t14_43, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_166 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_32, _t14_32, 32), _mm256_permute2f128_pd(_t14_32, _t14_32, 32), 0);

      // AVX Loader:

      // 1x3 -> 1x4
      _t14_167 = _t14_11;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_168 = _mm256_mul_pd(_t14_166, _t14_167);

      // 4-BLAC: 1x4 - 1x4
      _t14_169 = _mm256_sub_pd(_t14_165, _t14_168);

      // AVX Storer:
      _t14_33 = _t14_169;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 1)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509 + 1), U[76,76],h(1, 76, fi1509 + 1)) ) ),h(1, 76, fi1509 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_170 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_33, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_171 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_172 = _t14_10;

      // 4-BLAC: 1x4 + 1x4
      _t14_173 = _mm256_add_pd(_t14_171, _t14_172);

      // 4-BLAC: 1x4 / 1x4
      _t14_174 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_170), _mm256_castpd256_pd128(_t14_173)));

      // AVX Storer:
      _t14_34 = _t14_174;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(2, 76, fi1509 + 2)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 1)) Kro G(h(1, 76, fi1509 + 1), U[76,76],h(2, 76, fi1509 + 2)) ) ),h(2, 76, fi1509 + 2))

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_175 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_33, 6), _mm256_permute2f128_pd(_t14_33, _t14_33, 129), 5);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_176 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_34, _t14_34, 32), _mm256_permute2f128_pd(_t14_34, _t14_34, 32), 0);

      // AVX Loader:

      // 1x2 -> 1x4
      _t14_177 = _t14_9;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_178 = _mm256_mul_pd(_t14_176, _t14_177);

      // 4-BLAC: 1x4 - 1x4
      _t14_179 = _mm256_sub_pd(_t14_175, _t14_178);

      // AVX Storer:
      _t14_35 = _t14_179;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 2)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 2)) ) ),h(1, 76, fi1509 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_180 = _mm256_blend_pd(_mm256_setzero_pd(), _t14_35, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_181 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_182 = _t14_8;

      // 4-BLAC: 1x4 + 1x4
      _t14_183 = _mm256_add_pd(_t14_181, _t14_182);

      // 4-BLAC: 1x4 / 1x4
      _t14_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_180), _mm256_castpd256_pd128(_t14_183)));

      // AVX Storer:
      _t14_36 = _t14_184;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 3)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 2)) Kro G(h(1, 76, fi1509 + 2), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_185 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t14_35, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_186 = _t14_36;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_187 = _t14_7;

      // 4-BLAC: 1x4 Kro 1x4
      _t14_188 = _mm256_mul_pd(_t14_186, _t14_187);

      // 4-BLAC: 1x4 - 1x4
      _t14_189 = _mm256_sub_pd(_t14_185, _t14_188);

      // AVX Storer:
      _t14_37 = _t14_189;

      // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, fi1509 + 3)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, fi1509 + 3), U[76,76],h(1, 76, fi1509 + 3)) ) ),h(1, 76, fi1509 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_190 = _t14_37;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_191 = _t14_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t14_192 = _t14_6;

      // 4-BLAC: 1x4 + 1x4
      _t14_193 = _mm256_add_pd(_t14_191, _t14_192);

      // 4-BLAC: 1x4 / 1x4
      _t14_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_190), _mm256_castpd256_pd128(_t14_193)));

      // AVX Storer:
      _t14_37 = _t14_194;

      // Generating : X[76,76] = Sum_{j123} ( S(h(4, 76, fi1306), ( G(h(4, 76, fi1306), X[76,76],h(4, 76, fi1509 + j123 + 4)) - ( G(h(4, 76, fi1306), X[76,76],h(4, 76, fi1509)) * G(h(4, 76, fi1509), U[76,76],h(4, 76, fi1509 + j123 + 4)) ) ),h(4, 76, fi1509 + j123 + 4)) )

      // AVX Loader:
      _mm_store_sd(&(C[76*fi1306 + fi1509]), _mm256_castpd256_pd128(_t14_14));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 1]), _mm256_castpd256_pd128(_t14_16));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 2]), _mm256_castpd256_pd128(_t14_18));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 3]), _mm256_castpd256_pd128(_t14_19));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 76]), _mm256_castpd256_pd128(_t14_20));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 77]), _mm256_castpd256_pd128(_t14_22));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 78]), _mm256_castpd256_pd128(_t14_24));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 79]), _mm256_castpd256_pd128(_t14_25));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 152]), _mm256_castpd256_pd128(_t14_26));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 153]), _mm256_castpd256_pd128(_t14_28));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 154]), _mm256_castpd256_pd128(_t14_30));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 155]), _mm256_castpd256_pd128(_t14_31));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 228]), _mm256_castpd256_pd128(_t14_32));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 229]), _mm256_castpd256_pd128(_t14_34));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 230]), _mm256_castpd256_pd128(_t14_36));
      _mm_store_sd(&(C[76*fi1306 + fi1509 + 231]), _mm256_castpd256_pd128(_t14_37));

      for( int j123 = 0; j123 <= -fi1509 + 71; j123+=4 ) {
        _t15_24 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + j123 + 4);
        _t15_25 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + j123 + 80);
        _t15_26 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + j123 + 156);
        _t15_27 = _asm256_loadu_pd(C + 76*fi1306 + fi1509 + j123 + 232);
        _t15_19 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 4);
        _t15_18 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 80);
        _t15_17 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 156);
        _t15_16 = _asm256_loadu_pd(U + 77*fi1509 + j123 + 232);
        _t15_15 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509])));
        _t15_14 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 1])));
        _t15_13 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 2])));
        _t15_12 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 3])));
        _t15_11 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 76])));
        _t15_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 77])));
        _t15_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 78])));
        _t15_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 79])));
        _t15_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 152])));
        _t15_6 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 153])));
        _t15_5 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 154])));
        _t15_4 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 155])));
        _t15_3 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 228])));
        _t15_2 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 229])));
        _t15_1 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 230])));
        _t15_0 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + fi1509 + 231])));

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t15_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_15, _t15_15, 32), _mm256_permute2f128_pd(_t15_15, _t15_15, 32), 0), _t15_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_14, _t15_14, 32), _mm256_permute2f128_pd(_t15_14, _t15_14, 32), 0), _t15_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_13, _t15_13, 32), _mm256_permute2f128_pd(_t15_13, _t15_13, 32), 0), _t15_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_12, _t15_12, 32), _mm256_permute2f128_pd(_t15_12, _t15_12, 32), 0), _t15_16)));
        _t15_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_11, _t15_11, 32), _mm256_permute2f128_pd(_t15_11, _t15_11, 32), 0), _t15_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_10, _t15_10, 32), _mm256_permute2f128_pd(_t15_10, _t15_10, 32), 0), _t15_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_9, _t15_9, 32), _mm256_permute2f128_pd(_t15_9, _t15_9, 32), 0), _t15_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_8, _t15_8, 32), _mm256_permute2f128_pd(_t15_8, _t15_8, 32), 0), _t15_16)));
        _t15_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_7, _t15_7, 32), _mm256_permute2f128_pd(_t15_7, _t15_7, 32), 0), _t15_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_6, _t15_6, 32), _mm256_permute2f128_pd(_t15_6, _t15_6, 32), 0), _t15_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_5, _t15_5, 32), _mm256_permute2f128_pd(_t15_5, _t15_5, 32), 0), _t15_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_4, _t15_4, 32), _mm256_permute2f128_pd(_t15_4, _t15_4, 32), 0), _t15_16)));
        _t15_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_3, _t15_3, 32), _mm256_permute2f128_pd(_t15_3, _t15_3, 32), 0), _t15_19), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_2, _t15_2, 32), _mm256_permute2f128_pd(_t15_2, _t15_2, 32), 0), _t15_18)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_1, _t15_1, 32), _mm256_permute2f128_pd(_t15_1, _t15_1, 32), 0), _t15_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t15_0, _t15_0, 32), _mm256_permute2f128_pd(_t15_0, _t15_0, 32), 0), _t15_16)));

        // 4-BLAC: 4x4 - 4x4
        _t15_24 = _mm256_sub_pd(_t15_24, _t15_20);
        _t15_25 = _mm256_sub_pd(_t15_25, _t15_21);
        _t15_26 = _mm256_sub_pd(_t15_26, _t15_22);
        _t15_27 = _mm256_sub_pd(_t15_27, _t15_23);

        // AVX Storer:
        _asm256_storeu_pd(C + 76*fi1306 + fi1509 + j123 + 4, _t15_24);
        _asm256_storeu_pd(C + 76*fi1306 + fi1509 + j123 + 80, _t15_25);
        _asm256_storeu_pd(C + 76*fi1306 + fi1509 + j123 + 156, _t15_26);
        _asm256_storeu_pd(C + 76*fi1306 + fi1509 + j123 + 232, _t15_27);
      }
    }
    _t16_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[76*fi1306 + 72])));
    _t16_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306])));
    _t16_8 = _mm256_maskload_pd(C + 76*fi1306 + 73, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t16_31 = _asm256_loadu_pd(C + 76*fi1306 + 148);
    _t16_5 = _mm256_broadcast_sd(&(L[77*fi1306 + 76]));
    _t16_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306 + 77])));
    _t16_32 = _asm256_loadu_pd(C + 76*fi1306 + 224);
    _t16_3 = _mm256_maskload_pd(L + 77*fi1306 + 152, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t16_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306 + 154])));
    _t16_33 = _asm256_loadu_pd(C + 76*fi1306 + 300);
    _t16_1 = _mm256_maskload_pd(L + 77*fi1306 + 228, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t16_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[77*fi1306 + 231])));

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_34 = _t16_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_35 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_36 = _t2_12;

    // 4-BLAC: 1x4 + 1x4
    _t11_40 = _mm256_add_pd(_t16_35, _t16_36);

    // 4-BLAC: 1x4 / 1x4
    _t16_37 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_34), _mm256_castpd256_pd128(_t11_40)));

    // AVX Storer:
    _t16_7 = _t16_37;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_38 = _t16_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_39 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_7, _t16_7, 32), _mm256_permute2f128_pd(_t16_7, _t16_7, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_40 = _t2_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_45 = _mm256_mul_pd(_t16_39, _t16_40);

    // 4-BLAC: 1x4 - 1x4
    _t16_41 = _mm256_sub_pd(_t16_38, _t11_45);

    // AVX Storer:
    _t16_8 = _t16_41;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_42 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_43 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_44 = _t2_10;

    // 4-BLAC: 1x4 + 1x4
    _t11_50 = _mm256_add_pd(_t16_43, _t16_44);

    // 4-BLAC: 1x4 / 1x4
    _t16_45 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_42), _mm256_castpd256_pd128(_t11_50)));

    // AVX Storer:
    _t16_9 = _t16_45;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_46 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_8, 6), _mm256_permute2f128_pd(_t16_8, _t16_8, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_47 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_9, _t16_9, 32), _mm256_permute2f128_pd(_t16_9, _t16_9, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_48 = _t2_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_55 = _mm256_mul_pd(_t16_47, _t16_48);

    // 4-BLAC: 1x4 - 1x4
    _t16_49 = _mm256_sub_pd(_t16_46, _t11_55);

    // AVX Storer:
    _t16_10 = _t16_49;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_50 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_51 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_52 = _t2_8;

    // 4-BLAC: 1x4 + 1x4
    _t11_60 = _mm256_add_pd(_t16_51, _t16_52);

    // 4-BLAC: 1x4 / 1x4
    _t16_53 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_50), _mm256_castpd256_pd128(_t11_60)));

    // AVX Storer:
    _t16_11 = _t16_53;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_54 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_55 = _t16_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_56 = _t2_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_65 = _mm256_mul_pd(_t16_55, _t16_56);

    // 4-BLAC: 1x4 - 1x4
    _t16_57 = _mm256_sub_pd(_t16_54, _t11_65);

    // AVX Storer:
    _t16_12 = _t16_57;

    // Generating : X[76,76] = S(h(1, 76, fi1306), ( G(h(1, 76, fi1306), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306), L[76,76],h(1, 76, fi1306)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_58 = _t16_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_59 = _t16_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_60 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_70 = _mm256_add_pd(_t16_59, _t16_60);

    // 4-BLAC: 1x4 / 1x4
    _t16_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_58), _mm256_castpd256_pd128(_t11_70)));

    // AVX Storer:
    _t16_12 = _t16_61;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306)) Kro G(h(1, 76, fi1306), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_62 = _t16_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t11_31 = _mm256_mul_pd(_t16_62, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_7, _t16_9), _mm256_unpacklo_pd(_t16_11, _t16_12), 32));

    // 4-BLAC: 1x4 - 1x4
    _t16_31 = _mm256_sub_pd(_t16_31, _t11_31);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_63 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_31, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_64 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_65 = _t2_12;

    // 4-BLAC: 1x4 + 1x4
    _t11_76 = _mm256_add_pd(_t16_64, _t16_65);

    // 4-BLAC: 1x4 / 1x4
    _t16_66 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_63), _mm256_castpd256_pd128(_t11_76)));

    // AVX Storer:
    _t16_13 = _t16_66;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_67 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_31, 14), _mm256_permute2f128_pd(_t16_31, _t16_31, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_68 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_13, _t16_13, 32), _mm256_permute2f128_pd(_t16_13, _t16_13, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_69 = _t2_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_81 = _mm256_mul_pd(_t16_68, _t16_69);

    // 4-BLAC: 1x4 - 1x4
    _t16_70 = _mm256_sub_pd(_t16_67, _t11_81);

    // AVX Storer:
    _t16_14 = _t16_70;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_71 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_14, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_72 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_73 = _t2_10;

    // 4-BLAC: 1x4 + 1x4
    _t11_86 = _mm256_add_pd(_t16_72, _t16_73);

    // 4-BLAC: 1x4 / 1x4
    _t16_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_71), _mm256_castpd256_pd128(_t11_86)));

    // AVX Storer:
    _t16_15 = _t16_74;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_75 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_14, 6), _mm256_permute2f128_pd(_t16_14, _t16_14, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_76 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_15, _t16_15, 32), _mm256_permute2f128_pd(_t16_15, _t16_15, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_77 = _t2_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_91 = _mm256_mul_pd(_t16_76, _t16_77);

    // 4-BLAC: 1x4 - 1x4
    _t16_78 = _mm256_sub_pd(_t16_75, _t11_91);

    // AVX Storer:
    _t16_16 = _t16_78;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_79 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_16, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_80 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_81 = _t2_8;

    // 4-BLAC: 1x4 + 1x4
    _t11_96 = _mm256_add_pd(_t16_80, _t16_81);

    // 4-BLAC: 1x4 / 1x4
    _t16_82 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_79), _mm256_castpd256_pd128(_t11_96)));

    // AVX Storer:
    _t16_17 = _t16_82;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_83 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_16, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_84 = _t16_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_85 = _t2_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_101 = _mm256_mul_pd(_t16_84, _t16_85);

    // 4-BLAC: 1x4 - 1x4
    _t16_86 = _mm256_sub_pd(_t16_83, _t11_101);

    // AVX Storer:
    _t16_18 = _t16_86;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 1), ( G(h(1, 76, fi1306 + 1), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306 + 1), L[76,76],h(1, 76, fi1306 + 1)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_87 = _t16_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_88 = _t16_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_89 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_106 = _mm256_add_pd(_t16_88, _t16_89);

    // 4-BLAC: 1x4 / 1x4
    _t16_90 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_87), _mm256_castpd256_pd128(_t11_106)));

    // AVX Storer:
    _t16_18 = _t16_90;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, fi1306 + 2), L[76,76],h(2, 76, fi1306)) * G(h(2, 76, fi1306), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_91 = _t16_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t16_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_7, _t16_9), _mm256_unpacklo_pd(_t16_11, _t16_12), 32);
    _t16_93 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_13, _t16_15), _mm256_unpacklo_pd(_t16_17, _t16_18), 32);
    _t16_94 = _mm256_setzero_pd();
    _t16_95 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_91, _t16_91, 32), _mm256_permute2f128_pd(_t16_91, _t16_91, 32), 0), _t16_92), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_91, _t16_91, 32), _mm256_permute2f128_pd(_t16_91, _t16_91, 32), 15), _t16_93)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_91, _t16_91, 49), _mm256_permute2f128_pd(_t16_91, _t16_91, 49), 0), _t16_94), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_91, _t16_91, 49), _mm256_permute2f128_pd(_t16_91, _t16_91, 49), 15), _t16_95)));

    // 4-BLAC: 1x4 - 1x4
    _t16_32 = _mm256_sub_pd(_t16_32, _t11_32);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_96 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_32, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_97 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_98 = _t2_12;

    // 4-BLAC: 1x4 + 1x4
    _t11_116 = _mm256_add_pd(_t16_97, _t16_98);

    // 4-BLAC: 1x4 / 1x4
    _t16_99 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_96), _mm256_castpd256_pd128(_t11_116)));

    // AVX Storer:
    _t16_19 = _t16_99;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_100 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_32, 14), _mm256_permute2f128_pd(_t16_32, _t16_32, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_101 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_19, _t16_19, 32), _mm256_permute2f128_pd(_t16_19, _t16_19, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_102 = _t2_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_121 = _mm256_mul_pd(_t16_101, _t16_102);

    // 4-BLAC: 1x4 - 1x4
    _t16_103 = _mm256_sub_pd(_t16_100, _t11_121);

    // AVX Storer:
    _t16_20 = _t16_103;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_104 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_20, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_105 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_106 = _t2_10;

    // 4-BLAC: 1x4 + 1x4
    _t11_126 = _mm256_add_pd(_t16_105, _t16_106);

    // 4-BLAC: 1x4 / 1x4
    _t16_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_104), _mm256_castpd256_pd128(_t11_126)));

    // AVX Storer:
    _t16_21 = _t16_107;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_108 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_20, 6), _mm256_permute2f128_pd(_t16_20, _t16_20, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_109 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_21, _t16_21, 32), _mm256_permute2f128_pd(_t16_21, _t16_21, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_110 = _t2_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_131 = _mm256_mul_pd(_t16_109, _t16_110);

    // 4-BLAC: 1x4 - 1x4
    _t16_111 = _mm256_sub_pd(_t16_108, _t11_131);

    // AVX Storer:
    _t16_22 = _t16_111;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_112 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_22, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_113 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_114 = _t2_8;

    // 4-BLAC: 1x4 + 1x4
    _t11_136 = _mm256_add_pd(_t16_113, _t16_114);

    // 4-BLAC: 1x4 / 1x4
    _t16_115 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_112), _mm256_castpd256_pd128(_t11_136)));

    // AVX Storer:
    _t16_23 = _t16_115;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_116 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_22, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_117 = _t16_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_118 = _t2_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_141 = _mm256_mul_pd(_t16_117, _t16_118);

    // 4-BLAC: 1x4 - 1x4
    _t16_119 = _mm256_sub_pd(_t16_116, _t11_141);

    // AVX Storer:
    _t16_24 = _t16_119;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 2), ( G(h(1, 76, fi1306 + 2), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306 + 2), L[76,76],h(1, 76, fi1306 + 2)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_120 = _t16_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_121 = _t16_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_122 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_146 = _mm256_add_pd(_t16_121, _t16_122);

    // 4-BLAC: 1x4 / 1x4
    _t16_123 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_120), _mm256_castpd256_pd128(_t11_146)));

    // AVX Storer:
    _t16_24 = _t16_123;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(4, 76, 72)) - ( G(h(1, 76, fi1306 + 3), L[76,76],h(3, 76, fi1306)) * G(h(3, 76, fi1306), X[76,76],h(4, 76, 72)) ) ),h(4, 76, 72))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_124 = _t16_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t16_125 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_7, _t16_9), _mm256_unpacklo_pd(_t16_11, _t16_12), 32);
    _t16_126 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_13, _t16_15), _mm256_unpacklo_pd(_t16_17, _t16_18), 32);
    _t16_127 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_19, _t16_21), _mm256_unpacklo_pd(_t16_23, _t16_24), 32);
    _t16_128 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_124, _t16_124, 32), _mm256_permute2f128_pd(_t16_124, _t16_124, 32), 0), _t16_125), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_124, _t16_124, 32), _mm256_permute2f128_pd(_t16_124, _t16_124, 32), 15), _t16_126)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_124, _t16_124, 49), _mm256_permute2f128_pd(_t16_124, _t16_124, 49), 0), _t16_127), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_124, _t16_124, 49), _mm256_permute2f128_pd(_t16_124, _t16_124, 49), 15), _t16_128)));

    // 4-BLAC: 1x4 - 1x4
    _t16_33 = _mm256_sub_pd(_t16_33, _t11_33);

    // AVX Storer:

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 72)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 72), U[76,76],h(1, 76, 72)) ) ),h(1, 76, 72))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_129 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_33, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_130 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_131 = _t2_12;

    // 4-BLAC: 1x4 + 1x4
    _t11_156 = _mm256_add_pd(_t16_130, _t16_131);

    // 4-BLAC: 1x4 / 1x4
    _t16_132 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_129), _mm256_castpd256_pd128(_t11_156)));

    // AVX Storer:
    _t16_25 = _t16_132;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(3, 76, 73)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 72)) Kro G(h(1, 76, 72), U[76,76],h(3, 76, 73)) ) ),h(3, 76, 73))

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_133 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_33, 14), _mm256_permute2f128_pd(_t16_33, _t16_33, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_134 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_25, _t16_25, 32), _mm256_permute2f128_pd(_t16_25, _t16_25, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t16_135 = _t2_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_161 = _mm256_mul_pd(_t16_134, _t16_135);

    // 4-BLAC: 1x4 - 1x4
    _t16_136 = _mm256_sub_pd(_t16_133, _t11_161);

    // AVX Storer:
    _t16_26 = _t16_136;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 73)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 73), U[76,76],h(1, 76, 73)) ) ),h(1, 76, 73))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_137 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_26, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_138 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_139 = _t2_10;

    // 4-BLAC: 1x4 + 1x4
    _t11_166 = _mm256_add_pd(_t16_138, _t16_139);

    // 4-BLAC: 1x4 / 1x4
    _t16_140 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_137), _mm256_castpd256_pd128(_t11_166)));

    // AVX Storer:
    _t16_27 = _t16_140;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(2, 76, 74)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 73)) Kro G(h(1, 76, 73), U[76,76],h(2, 76, 74)) ) ),h(2, 76, 74))

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_141 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_26, 6), _mm256_permute2f128_pd(_t16_26, _t16_26, 129), 5);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_142 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t16_27, _t16_27, 32), _mm256_permute2f128_pd(_t16_27, _t16_27, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t16_143 = _t2_9;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_171 = _mm256_mul_pd(_t16_142, _t16_143);

    // 4-BLAC: 1x4 - 1x4
    _t16_144 = _mm256_sub_pd(_t16_141, _t11_171);

    // AVX Storer:
    _t16_28 = _t16_144;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 74)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 74), U[76,76],h(1, 76, 74)) ) ),h(1, 76, 74))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_145 = _mm256_blend_pd(_mm256_setzero_pd(), _t16_28, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_146 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_147 = _t2_8;

    // 4-BLAC: 1x4 + 1x4
    _t11_176 = _mm256_add_pd(_t16_146, _t16_147);

    // 4-BLAC: 1x4 / 1x4
    _t16_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_145), _mm256_castpd256_pd128(_t11_176)));

    // AVX Storer:
    _t16_29 = _t16_148;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 75)) - ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 74)) Kro G(h(1, 76, 74), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_149 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t16_28, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_150 = _t16_29;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_151 = _t2_7;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_181 = _mm256_mul_pd(_t16_150, _t16_151);

    // 4-BLAC: 1x4 - 1x4
    _t16_152 = _mm256_sub_pd(_t16_149, _t11_181);

    // AVX Storer:
    _t16_30 = _t16_152;

    // Generating : X[76,76] = S(h(1, 76, fi1306 + 3), ( G(h(1, 76, fi1306 + 3), X[76,76],h(1, 76, 75)) Div ( G(h(1, 76, fi1306 + 3), L[76,76],h(1, 76, fi1306 + 3)) + G(h(1, 76, 75), U[76,76],h(1, 76, 75)) ) ),h(1, 76, 75))

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_153 = _t16_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_154 = _t16_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t16_155 = _t2_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_186 = _mm256_add_pd(_t16_154, _t16_155);

    // 4-BLAC: 1x4 / 1x4
    _t16_156 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t16_153), _mm256_castpd256_pd128(_t11_186)));

    // AVX Storer:
    _t16_30 = _t16_156;
    _mm_store_sd(&(C[76*fi1306 + 72]), _mm256_castpd256_pd128(_t16_7));
    _mm_store_sd(&(C[76*fi1306 + 73]), _mm256_castpd256_pd128(_t16_9));
    _mm_store_sd(&(C[76*fi1306 + 74]), _mm256_castpd256_pd128(_t16_11));
    _mm_store_sd(&(C[76*fi1306 + 75]), _mm256_castpd256_pd128(_t16_12));
    _mm_store_sd(&(C[76*fi1306 + 148]), _mm256_castpd256_pd128(_t16_13));
    _mm_store_sd(&(C[76*fi1306 + 149]), _mm256_castpd256_pd128(_t16_15));
    _mm_store_sd(&(C[76*fi1306 + 150]), _mm256_castpd256_pd128(_t16_17));
    _mm_store_sd(&(C[76*fi1306 + 151]), _mm256_castpd256_pd128(_t16_18));
    _mm_store_sd(&(C[76*fi1306 + 224]), _mm256_castpd256_pd128(_t16_19));
    _mm_store_sd(&(C[76*fi1306 + 225]), _mm256_castpd256_pd128(_t16_21));
    _mm_store_sd(&(C[76*fi1306 + 226]), _mm256_castpd256_pd128(_t16_23));
    _mm_store_sd(&(C[76*fi1306 + 227]), _mm256_castpd256_pd128(_t16_24));
    _mm_store_sd(&(C[76*fi1306 + 300]), _mm256_castpd256_pd128(_t16_25));
    _mm_store_sd(&(C[76*fi1306 + 301]), _mm256_castpd256_pd128(_t16_27));
    _mm_store_sd(&(C[76*fi1306 + 302]), _mm256_castpd256_pd128(_t16_29));
    _mm_store_sd(&(C[76*fi1306 + 303]), _mm256_castpd256_pd128(_t16_30));
  }

}
