/*
 * lusylp_kernel.h
 *
Decl { {u'X': SquaredMatrix[X, (52, 52), GenMatAccess], u'C': SquaredMatrix[C, (52, 52), GenMatAccess], u'U': UpperTriangular[U, (52, 52), GenMatAccess], u'L': LowerTriangular[L, (52, 52), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_SquaredMatrix_Mul_SquaredMatrix_UpperTriangular_SquaredMatrix_opt': {'m0': 'm03.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_SquaredMatrix_Mul_SquaredMatrix_UpperTriangular_SquaredMatrix_opt_m03_m21'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 31:
For_{fi879;4;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, 0), X[52,52],h(4, 52, fi879)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, 0), X[52,52],h(4, 52, fi879)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, 0), X[52,52],h(fi879, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi879, 52, 0), U[52,52],h(4, 52, fi879)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(2, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(3, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 0), L[52,52],h(1, 52, 0)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(4, 52, fi879)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(4, 52, fi879)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), L[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), X[52,52],h(4, 52, fi879)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(2, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), X[52,52],h(3, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 1), L[52,52],h(1, 52, 1)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(4, 52, fi879)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(4, 52, fi879)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), L[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), X[52,52],h(4, 52, fi879)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(2, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), X[52,52],h(3, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 2), L[52,52],h(1, 52, 2)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(4, 52, fi879)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(4, 52, fi879)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), L[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), X[52,52],h(4, 52, fi879)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(2, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), X[52,52],h(3, 52, fi879)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, 3), L[52,52],h(1, 52, 3)) ) + Tile( (1, 1), G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ) )
Eq.ann: {}
 )Entry 32:
For_{fi858;4;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi858), X[52,52],h(52, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi858), C[52,52],h(52, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi858), L[52,52],h(fi858, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi858, 52, 0), X[52,52],h(52, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), L[52,52],h(2, 52, fi858)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi858), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(4, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(4, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), L[52,52],h(3, 52, fi858)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi858), X[52,52],h(4, 52, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 0)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(2, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(3, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ) )
Eq.ann: {}
Entry 32:
For_{fi1082;4;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi858), X[52,52],h(4, 52, fi1082)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi858), X[52,52],h(4, 52, fi1082)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi858), X[52,52],h(fi1082, 52, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1082, 52, 0), U[52,52],h(4, 52, fi1082)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(2, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(3, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858), X[52,52],h(1, 52, fi1082 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858), L[52,52],h(1, 52, fi858)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(4, 52, fi1082)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(4, 52, fi1082)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858), X[52,52],h(4, 52, fi1082)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(2, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 1), X[52,52],h(3, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 1), X[52,52],h(1, 52, fi1082 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 1), L[52,52],h(1, 52, fi858 + 1)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(4, 52, fi1082)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(4, 52, fi1082)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), L[52,52],h(2, 52, fi858)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi858), X[52,52],h(4, 52, fi1082)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(2, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 2), X[52,52],h(3, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 2), X[52,52],h(1, 52, fi1082 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 2), L[52,52],h(1, 52, fi858 + 2)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(4, 52, fi1082)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(4, 52, fi1082)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), L[52,52],h(3, 52, fi858)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi858), X[52,52],h(4, 52, fi1082)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 1)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 1)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(2, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 2)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 2)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi858 + 3), X[52,52],h(3, 52, fi1082)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi858 + 3), X[52,52],h(1, 52, fi1082 + 3)) ) Div ( Tile( (1, 1), G(h(1, 52, fi858 + 3), L[52,52],h(1, 52, fi858 + 3)) ) + Tile( (1, 1), G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ) )
Eq.ann: {}
 ) ) *
 * Created on: 2017-07-31
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 52
#define PARAM1 52

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double const * U, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205, _t0_206, _t0_207,
	_t0_208, _t0_209, _t0_210, _t0_211, _t0_212, _t0_213, _t0_214, _t0_215,
	_t0_216, _t0_217, _t0_218, _t0_219, _t0_220, _t0_221, _t0_222, _t0_223,
	_t0_224, _t0_225, _t0_226, _t0_227, _t0_228, _t0_229, _t0_230, _t0_231,
	_t0_232, _t0_233, _t0_234, _t0_235, _t0_236, _t0_237, _t0_238, _t0_239,
	_t0_240, _t0_241, _t0_242, _t0_243, _t0_244, _t0_245, _t0_246, _t0_247,
	_t0_248, _t0_249, _t0_250, _t0_251, _t0_252, _t0_253, _t0_254, _t0_255,
	_t0_256, _t0_257, _t0_258, _t0_259, _t0_260, _t0_261, _t0_262, _t0_263,
	_t0_264, _t0_265, _t0_266, _t0_267, _t0_268, _t0_269, _t0_270, _t0_271,
	_t0_272, _t0_273, _t0_274, _t0_275, _t0_276, _t0_277, _t0_278, _t0_279,
	_t0_280, _t0_281, _t0_282, _t0_283, _t0_284, _t0_285, _t0_286, _t0_287,
	_t0_288, _t0_289, _t0_290, _t0_291, _t0_292, _t0_293, _t0_294, _t0_295,
	_t0_296, _t0_297, _t0_298, _t0_299, _t0_300, _t0_301, _t0_302, _t0_303,
	_t0_304, _t0_305, _t0_306, _t0_307, _t0_308, _t0_309, _t0_310, _t0_311,
	_t0_312, _t0_313, _t0_314, _t0_315, _t0_316, _t0_317, _t0_318, _t0_319,
	_t0_320, _t0_321, _t0_322, _t0_323, _t0_324, _t0_325, _t0_326, _t0_327,
	_t0_328, _t0_329, _t0_330, _t0_331, _t0_332, _t0_333, _t0_334, _t0_335,
	_t0_336, _t0_337, _t0_338, _t0_339, _t0_340, _t0_341, _t0_342, _t0_343,
	_t0_344, _t0_345, _t0_346, _t0_347, _t0_348, _t0_349, _t0_350, _t0_351,
	_t0_352, _t0_353, _t0_354, _t0_355, _t0_356, _t0_357, _t0_358, _t0_359,
	_t0_360, _t0_361, _t0_362, _t0_363, _t0_364, _t0_365, _t0_366, _t0_367,
	_t0_368, _t0_369, _t0_370, _t0_371, _t0_372, _t0_373, _t0_374, _t0_375,
	_t0_376, _t0_377, _t0_378, _t0_379, _t0_380, _t0_381, _t0_382, _t0_383,
	_t0_384, _t0_385, _t0_386, _t0_387, _t0_388, _t0_389, _t0_390, _t0_391,
	_t0_392, _t0_393, _t0_394, _t0_395, _t0_396, _t0_397, _t0_398, _t0_399,
	_t0_400, _t0_401, _t0_402, _t0_403, _t0_404, _t0_405, _t0_406, _t0_407,
	_t0_408, _t0_409, _t0_410, _t0_411, _t0_412, _t0_413, _t0_414, _t0_415,
	_t0_416, _t0_417, _t0_418, _t0_419, _t0_420, _t0_421, _t0_422, _t0_423,
	_t0_424, _t0_425, _t0_426, _t0_427, _t0_428, _t0_429, _t0_430, _t0_431,
	_t0_432, _t0_433, _t0_434, _t0_435, _t0_436, _t0_437, _t0_438, _t0_439,
	_t0_440, _t0_441, _t0_442, _t0_443, _t0_444, _t0_445, _t0_446, _t0_447,
	_t0_448, _t0_449, _t0_450, _t0_451, _t0_452, _t0_453, _t0_454, _t0_455,
	_t0_456, _t0_457, _t0_458, _t0_459, _t0_460, _t0_461, _t0_462, _t0_463,
	_t0_464, _t0_465, _t0_466, _t0_467, _t0_468, _t0_469, _t0_470, _t0_471,
	_t0_472, _t0_473, _t0_474, _t0_475, _t0_476, _t0_477, _t0_478, _t0_479,
	_t0_480, _t0_481, _t0_482, _t0_483, _t0_484, _t0_485, _t0_486, _t0_487,
	_t0_488, _t0_489, _t0_490, _t0_491, _t0_492, _t0_493, _t0_494, _t0_495,
	_t0_496, _t0_497, _t0_498, _t0_499, _t0_500, _t0_501, _t0_502, _t0_503,
	_t0_504, _t0_505, _t0_506, _t0_507, _t0_508, _t0_509, _t0_510, _t0_511,
	_t0_512, _t0_513, _t0_514, _t0_515, _t0_516, _t0_517, _t0_518, _t0_519,
	_t0_520, _t0_521, _t0_522, _t0_523, _t0_524, _t0_525, _t0_526, _t0_527,
	_t0_528, _t0_529, _t0_530, _t0_531, _t0_532, _t0_533, _t0_534, _t0_535,
	_t0_536, _t0_537, _t0_538, _t0_539, _t0_540, _t0_541, _t0_542, _t0_543,
	_t0_544, _t0_545, _t0_546, _t0_547, _t0_548, _t0_549, _t0_550, _t0_551,
	_t0_552, _t0_553, _t0_554, _t0_555, _t0_556, _t0_557, _t0_558, _t0_559,
	_t0_560, _t0_561, _t0_562, _t0_563, _t0_564, _t0_565, _t0_566, _t0_567,
	_t0_568, _t0_569, _t0_570, _t0_571, _t0_572;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31,
	_t3_32, _t3_33, _t3_34, _t3_35, _t3_36, _t3_37, _t3_38, _t3_39,
	_t3_40, _t3_41, _t3_42, _t3_43, _t3_44, _t3_45, _t3_46, _t3_47,
	_t3_48, _t3_49, _t3_50, _t3_51, _t3_52, _t3_53, _t3_54, _t3_55,
	_t3_56, _t3_57, _t3_58, _t3_59, _t3_60, _t3_61, _t3_62, _t3_63,
	_t3_64, _t3_65, _t3_66, _t3_67, _t3_68, _t3_69, _t3_70, _t3_71,
	_t3_72, _t3_73, _t3_74, _t3_75, _t3_76, _t3_77, _t3_78, _t3_79,
	_t3_80, _t3_81, _t3_82, _t3_83, _t3_84, _t3_85, _t3_86, _t3_87,
	_t3_88, _t3_89, _t3_90, _t3_91, _t3_92, _t3_93, _t3_94, _t3_95,
	_t3_96, _t3_97, _t3_98, _t3_99, _t3_100, _t3_101, _t3_102, _t3_103,
	_t3_104, _t3_105, _t3_106, _t3_107, _t3_108, _t3_109, _t3_110, _t3_111,
	_t3_112, _t3_113, _t3_114, _t3_115, _t3_116, _t3_117, _t3_118, _t3_119,
	_t3_120, _t3_121, _t3_122, _t3_123, _t3_124, _t3_125, _t3_126, _t3_127,
	_t3_128, _t3_129, _t3_130, _t3_131, _t3_132, _t3_133, _t3_134, _t3_135,
	_t3_136, _t3_137, _t3_138, _t3_139, _t3_140, _t3_141, _t3_142, _t3_143,
	_t3_144, _t3_145;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60, _t5_61, _t5_62, _t5_63,
	_t5_64, _t5_65, _t5_66, _t5_67, _t5_68, _t5_69, _t5_70, _t5_71,
	_t5_72, _t5_73, _t5_74, _t5_75, _t5_76, _t5_77, _t5_78, _t5_79,
	_t5_80, _t5_81, _t5_82, _t5_83, _t5_84, _t5_85, _t5_86, _t5_87,
	_t5_88, _t5_89, _t5_90, _t5_91, _t5_92, _t5_93, _t5_94, _t5_95,
	_t5_96, _t5_97, _t5_98, _t5_99, _t5_100, _t5_101, _t5_102, _t5_103,
	_t5_104, _t5_105, _t5_106, _t5_107, _t5_108, _t5_109, _t5_110, _t5_111,
	_t5_112, _t5_113, _t5_114, _t5_115, _t5_116, _t5_117, _t5_118, _t5_119,
	_t5_120, _t5_121, _t5_122, _t5_123, _t5_124, _t5_125, _t5_126, _t5_127,
	_t5_128, _t5_129, _t5_130, _t5_131, _t5_132, _t5_133, _t5_134, _t5_135,
	_t5_136, _t5_137, _t5_138, _t5_139, _t5_140, _t5_141, _t5_142, _t5_143,
	_t5_144, _t5_145, _t5_146, _t5_147, _t5_148, _t5_149, _t5_150, _t5_151,
	_t5_152, _t5_153, _t5_154, _t5_155, _t5_156, _t5_157, _t5_158, _t5_159,
	_t5_160, _t5_161, _t5_162, _t5_163, _t5_164, _t5_165, _t5_166, _t5_167,
	_t5_168, _t5_169, _t5_170, _t5_171, _t5_172, _t5_173, _t5_174, _t5_175,
	_t5_176, _t5_177, _t5_178, _t5_179, _t5_180, _t5_181, _t5_182, _t5_183,
	_t5_184, _t5_185, _t5_186, _t5_187, _t5_188, _t5_189, _t5_190, _t5_191,
	_t5_192, _t5_193, _t5_194, _t5_195, _t5_196, _t5_197, _t5_198, _t5_199,
	_t5_200, _t5_201, _t5_202, _t5_203, _t5_204, _t5_205, _t5_206, _t5_207,
	_t5_208, _t5_209, _t5_210, _t5_211, _t5_212, _t5_213, _t5_214, _t5_215,
	_t5_216, _t5_217, _t5_218, _t5_219, _t5_220, _t5_221, _t5_222, _t5_223,
	_t5_224, _t5_225, _t5_226, _t5_227, _t5_228, _t5_229, _t5_230, _t5_231,
	_t5_232, _t5_233, _t5_234, _t5_235, _t5_236, _t5_237, _t5_238, _t5_239,
	_t5_240, _t5_241, _t5_242, _t5_243, _t5_244, _t5_245, _t5_246, _t5_247,
	_t5_248, _t5_249, _t5_250, _t5_251, _t5_252, _t5_253, _t5_254, _t5_255,
	_t5_256, _t5_257, _t5_258, _t5_259, _t5_260, _t5_261, _t5_262, _t5_263,
	_t5_264, _t5_265, _t5_266, _t5_267, _t5_268, _t5_269, _t5_270, _t5_271,
	_t5_272, _t5_273, _t5_274, _t5_275, _t5_276, _t5_277, _t5_278, _t5_279,
	_t5_280, _t5_281, _t5_282, _t5_283, _t5_284, _t5_285, _t5_286, _t5_287,
	_t5_288, _t5_289, _t5_290, _t5_291, _t5_292, _t5_293, _t5_294, _t5_295,
	_t5_296, _t5_297, _t5_298, _t5_299, _t5_300, _t5_301, _t5_302, _t5_303,
	_t5_304, _t5_305, _t5_306, _t5_307, _t5_308, _t5_309, _t5_310, _t5_311,
	_t5_312, _t5_313, _t5_314, _t5_315, _t5_316, _t5_317, _t5_318, _t5_319,
	_t5_320, _t5_321, _t5_322, _t5_323, _t5_324, _t5_325, _t5_326, _t5_327,
	_t5_328, _t5_329, _t5_330, _t5_331, _t5_332, _t5_333, _t5_334, _t5_335,
	_t5_336, _t5_337, _t5_338, _t5_339, _t5_340, _t5_341, _t5_342, _t5_343,
	_t5_344, _t5_345, _t5_346, _t5_347, _t5_348, _t5_349, _t5_350, _t5_351,
	_t5_352, _t5_353, _t5_354, _t5_355, _t5_356, _t5_357, _t5_358, _t5_359,
	_t5_360, _t5_361, _t5_362, _t5_363, _t5_364, _t5_365, _t5_366, _t5_367,
	_t5_368, _t5_369, _t5_370, _t5_371, _t5_372, _t5_373, _t5_374, _t5_375,
	_t5_376, _t5_377, _t5_378, _t5_379, _t5_380, _t5_381, _t5_382, _t5_383,
	_t5_384, _t5_385, _t5_386, _t5_387, _t5_388, _t5_389, _t5_390, _t5_391,
	_t5_392, _t5_393, _t5_394, _t5_395, _t5_396, _t5_397, _t5_398, _t5_399,
	_t5_400, _t5_401, _t5_402, _t5_403, _t5_404, _t5_405, _t5_406, _t5_407,
	_t5_408, _t5_409, _t5_410, _t5_411, _t5_412, _t5_413, _t5_414, _t5_415,
	_t5_416, _t5_417, _t5_418, _t5_419, _t5_420, _t5_421, _t5_422, _t5_423,
	_t5_424, _t5_425, _t5_426, _t5_427, _t5_428, _t5_429, _t5_430, _t5_431,
	_t5_432, _t5_433, _t5_434, _t5_435, _t5_436, _t5_437, _t5_438, _t5_439,
	_t5_440, _t5_441, _t5_442, _t5_443, _t5_444, _t5_445, _t5_446, _t5_447,
	_t5_448, _t5_449, _t5_450, _t5_451, _t5_452, _t5_453, _t5_454, _t5_455,
	_t5_456, _t5_457, _t5_458, _t5_459, _t5_460, _t5_461, _t5_462, _t5_463,
	_t5_464, _t5_465, _t5_466, _t5_467, _t5_468, _t5_469, _t5_470, _t5_471,
	_t5_472, _t5_473, _t5_474, _t5_475, _t5_476, _t5_477, _t5_478, _t5_479,
	_t5_480, _t5_481, _t5_482, _t5_483, _t5_484, _t5_485, _t5_486, _t5_487,
	_t5_488, _t5_489, _t5_490, _t5_491, _t5_492, _t5_493, _t5_494, _t5_495,
	_t5_496, _t5_497, _t5_498, _t5_499, _t5_500, _t5_501, _t5_502, _t5_503,
	_t5_504, _t5_505, _t5_506, _t5_507, _t5_508, _t5_509, _t5_510, _t5_511,
	_t5_512, _t5_513, _t5_514, _t5_515, _t5_516, _t5_517, _t5_518, _t5_519,
	_t5_520, _t5_521, _t5_522, _t5_523, _t5_524, _t5_525, _t5_526, _t5_527,
	_t5_528, _t5_529, _t5_530, _t5_531, _t5_532, _t5_533, _t5_534, _t5_535,
	_t5_536, _t5_537, _t5_538, _t5_539;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27, _t8_28, _t8_29, _t8_30, _t8_31,
	_t8_32, _t8_33, _t8_34, _t8_35, _t8_36, _t8_37, _t8_38, _t8_39,
	_t8_40, _t8_41, _t8_42, _t8_43, _t8_44, _t8_45, _t8_46, _t8_47,
	_t8_48, _t8_49, _t8_50, _t8_51, _t8_52, _t8_53, _t8_54, _t8_55,
	_t8_56, _t8_57, _t8_58, _t8_59, _t8_60, _t8_61, _t8_62, _t8_63,
	_t8_64, _t8_65, _t8_66, _t8_67, _t8_68, _t8_69, _t8_70, _t8_71,
	_t8_72, _t8_73, _t8_74, _t8_75, _t8_76, _t8_77, _t8_78, _t8_79,
	_t8_80, _t8_81, _t8_82, _t8_83, _t8_84, _t8_85, _t8_86, _t8_87,
	_t8_88, _t8_89, _t8_90, _t8_91, _t8_92, _t8_93, _t8_94, _t8_95,
	_t8_96, _t8_97, _t8_98, _t8_99, _t8_100, _t8_101, _t8_102, _t8_103,
	_t8_104, _t8_105, _t8_106, _t8_107, _t8_108, _t8_109, _t8_110, _t8_111,
	_t8_112, _t8_113, _t8_114, _t8_115, _t8_116, _t8_117, _t8_118, _t8_119,
	_t8_120, _t8_121, _t8_122, _t8_123, _t8_124, _t8_125, _t8_126, _t8_127,
	_t8_128, _t8_129, _t8_130, _t8_131, _t8_132, _t8_133, _t8_134, _t8_135,
	_t8_136, _t8_137, _t8_138, _t8_139, _t8_140, _t8_141, _t8_142, _t8_143,
	_t8_144, _t8_145;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35, _t11_36, _t11_37, _t11_38, _t11_39,
	_t11_40, _t11_41, _t11_42, _t11_43, _t11_44, _t11_45, _t11_46, _t11_47,
	_t11_48, _t11_49, _t11_50, _t11_51, _t11_52, _t11_53, _t11_54, _t11_55,
	_t11_56, _t11_57, _t11_58, _t11_59, _t11_60, _t11_61, _t11_62, _t11_63,
	_t11_64, _t11_65, _t11_66, _t11_67, _t11_68, _t11_69, _t11_70, _t11_71,
	_t11_72, _t11_73, _t11_74, _t11_75, _t11_76, _t11_77, _t11_78, _t11_79,
	_t11_80, _t11_81, _t11_82, _t11_83, _t11_84, _t11_85, _t11_86, _t11_87,
	_t11_88, _t11_89, _t11_90, _t11_91, _t11_92, _t11_93, _t11_94, _t11_95,
	_t11_96, _t11_97, _t11_98, _t11_99, _t11_100, _t11_101, _t11_102, _t11_103,
	_t11_104, _t11_105, _t11_106, _t11_107, _t11_108, _t11_109, _t11_110, _t11_111,
	_t11_112, _t11_113, _t11_114, _t11_115, _t11_116, _t11_117, _t11_118, _t11_119,
	_t11_120, _t11_121, _t11_122, _t11_123, _t11_124, _t11_125, _t11_126, _t11_127,
	_t11_128, _t11_129, _t11_130, _t11_131, _t11_132, _t11_133, _t11_134, _t11_135,
	_t11_136, _t11_137, _t11_138, _t11_139, _t11_140, _t11_141, _t11_142, _t11_143,
	_t11_144, _t11_145, _t11_146, _t11_147, _t11_148, _t11_149, _t11_150, _t11_151,
	_t11_152, _t11_153, _t11_154, _t11_155, _t11_156, _t11_157, _t11_158, _t11_159,
	_t11_160, _t11_161, _t11_162, _t11_163, _t11_164, _t11_165, _t11_166, _t11_167,
	_t11_168, _t11_169, _t11_170, _t11_171, _t11_172, _t11_173, _t11_174, _t11_175,
	_t11_176, _t11_177, _t11_178, _t11_179, _t11_180, _t11_181, _t11_182, _t11_183,
	_t11_184, _t11_185, _t11_186, _t11_187, _t11_188, _t11_189, _t11_190, _t11_191,
	_t11_192, _t11_193, _t11_194, _t11_195, _t11_196, _t11_197, _t11_198, _t11_199,
	_t11_200, _t11_201, _t11_202, _t11_203, _t11_204, _t11_205, _t11_206, _t11_207,
	_t11_208, _t11_209, _t11_210, _t11_211, _t11_212, _t11_213, _t11_214, _t11_215,
	_t11_216, _t11_217, _t11_218, _t11_219, _t11_220, _t11_221, _t11_222, _t11_223,
	_t11_224, _t11_225, _t11_226, _t11_227, _t11_228, _t11_229, _t11_230, _t11_231,
	_t11_232, _t11_233, _t11_234, _t11_235, _t11_236, _t11_237, _t11_238, _t11_239,
	_t11_240, _t11_241, _t11_242, _t11_243, _t11_244, _t11_245, _t11_246, _t11_247,
	_t11_248, _t11_249, _t11_250, _t11_251, _t11_252, _t11_253, _t11_254, _t11_255,
	_t11_256, _t11_257, _t11_258, _t11_259, _t11_260, _t11_261, _t11_262, _t11_263,
	_t11_264, _t11_265, _t11_266, _t11_267, _t11_268, _t11_269, _t11_270, _t11_271,
	_t11_272, _t11_273, _t11_274, _t11_275, _t11_276, _t11_277, _t11_278, _t11_279,
	_t11_280, _t11_281, _t11_282, _t11_283, _t11_284, _t11_285, _t11_286, _t11_287,
	_t11_288, _t11_289, _t11_290, _t11_291, _t11_292, _t11_293, _t11_294, _t11_295,
	_t11_296, _t11_297, _t11_298, _t11_299, _t11_300, _t11_301, _t11_302, _t11_303,
	_t11_304, _t11_305, _t11_306, _t11_307, _t11_308, _t11_309, _t11_310, _t11_311,
	_t11_312, _t11_313, _t11_314, _t11_315, _t11_316, _t11_317, _t11_318, _t11_319,
	_t11_320, _t11_321, _t11_322, _t11_323, _t11_324, _t11_325, _t11_326, _t11_327,
	_t11_328, _t11_329, _t11_330, _t11_331, _t11_332, _t11_333, _t11_334, _t11_335,
	_t11_336, _t11_337, _t11_338, _t11_339, _t11_340, _t11_341, _t11_342, _t11_343,
	_t11_344, _t11_345, _t11_346, _t11_347, _t11_348, _t11_349, _t11_350, _t11_351,
	_t11_352, _t11_353, _t11_354, _t11_355, _t11_356, _t11_357, _t11_358, _t11_359,
	_t11_360, _t11_361, _t11_362, _t11_363, _t11_364, _t11_365, _t11_366, _t11_367,
	_t11_368, _t11_369, _t11_370, _t11_371, _t11_372, _t11_373, _t11_374, _t11_375,
	_t11_376, _t11_377, _t11_378, _t11_379, _t11_380, _t11_381, _t11_382, _t11_383,
	_t11_384, _t11_385, _t11_386, _t11_387, _t11_388, _t11_389, _t11_390, _t11_391,
	_t11_392, _t11_393, _t11_394, _t11_395, _t11_396, _t11_397, _t11_398, _t11_399,
	_t11_400, _t11_401, _t11_402, _t11_403, _t11_404, _t11_405, _t11_406, _t11_407,
	_t11_408, _t11_409, _t11_410, _t11_411, _t11_412, _t11_413, _t11_414, _t11_415,
	_t11_416, _t11_417, _t11_418, _t11_419, _t11_420, _t11_421, _t11_422, _t11_423,
	_t11_424, _t11_425, _t11_426, _t11_427, _t11_428, _t11_429, _t11_430, _t11_431,
	_t11_432, _t11_433, _t11_434, _t11_435, _t11_436, _t11_437, _t11_438, _t11_439,
	_t11_440, _t11_441, _t11_442, _t11_443, _t11_444, _t11_445, _t11_446, _t11_447,
	_t11_448, _t11_449, _t11_450, _t11_451, _t11_452, _t11_453, _t11_454, _t11_455,
	_t11_456, _t11_457, _t11_458, _t11_459, _t11_460, _t11_461, _t11_462, _t11_463,
	_t11_464, _t11_465, _t11_466, _t11_467, _t11_468, _t11_469, _t11_470, _t11_471,
	_t11_472, _t11_473, _t11_474, _t11_475, _t11_476, _t11_477, _t11_478, _t11_479,
	_t11_480, _t11_481, _t11_482, _t11_483, _t11_484, _t11_485, _t11_486, _t11_487,
	_t11_488, _t11_489, _t11_490, _t11_491, _t11_492, _t11_493, _t11_494, _t11_495,
	_t11_496, _t11_497, _t11_498, _t11_499, _t11_500, _t11_501, _t11_502, _t11_503,
	_t11_504, _t11_505, _t11_506, _t11_507, _t11_508, _t11_509, _t11_510, _t11_511,
	_t11_512, _t11_513, _t11_514, _t11_515, _t11_516, _t11_517, _t11_518, _t11_519,
	_t11_520, _t11_521, _t11_522, _t11_523, _t11_524, _t11_525, _t11_526, _t11_527,
	_t11_528, _t11_529, _t11_530, _t11_531, _t11_532, _t11_533, _t11_534, _t11_535,
	_t11_536, _t11_537, _t11_538, _t11_539;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35, _t14_36, _t14_37, _t14_38, _t14_39,
	_t14_40, _t14_41, _t14_42, _t14_43, _t14_44, _t14_45, _t14_46, _t14_47,
	_t14_48, _t14_49, _t14_50, _t14_51, _t14_52, _t14_53, _t14_54, _t14_55,
	_t14_56, _t14_57, _t14_58, _t14_59, _t14_60, _t14_61, _t14_62, _t14_63,
	_t14_64, _t14_65, _t14_66, _t14_67, _t14_68, _t14_69, _t14_70, _t14_71,
	_t14_72, _t14_73, _t14_74, _t14_75, _t14_76, _t14_77, _t14_78, _t14_79,
	_t14_80, _t14_81, _t14_82, _t14_83, _t14_84, _t14_85, _t14_86, _t14_87,
	_t14_88, _t14_89, _t14_90, _t14_91, _t14_92, _t14_93, _t14_94, _t14_95,
	_t14_96, _t14_97, _t14_98, _t14_99, _t14_100, _t14_101, _t14_102, _t14_103,
	_t14_104, _t14_105, _t14_106, _t14_107, _t14_108, _t14_109, _t14_110, _t14_111,
	_t14_112, _t14_113, _t14_114, _t14_115, _t14_116, _t14_117, _t14_118, _t14_119,
	_t14_120, _t14_121, _t14_122, _t14_123, _t14_124, _t14_125, _t14_126, _t14_127,
	_t14_128, _t14_129, _t14_130, _t14_131, _t14_132, _t14_133, _t14_134, _t14_135,
	_t14_136, _t14_137, _t14_138, _t14_139, _t14_140, _t14_141, _t14_142, _t14_143,
	_t14_144, _t14_145;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23,
	_t17_24, _t17_25, _t17_26, _t17_27, _t17_28, _t17_29, _t17_30, _t17_31,
	_t17_32, _t17_33, _t17_34, _t17_35, _t17_36, _t17_37, _t17_38, _t17_39,
	_t17_40, _t17_41, _t17_42, _t17_43, _t17_44, _t17_45, _t17_46, _t17_47,
	_t17_48, _t17_49, _t17_50, _t17_51, _t17_52, _t17_53, _t17_54, _t17_55,
	_t17_56, _t17_57, _t17_58, _t17_59, _t17_60, _t17_61, _t17_62, _t17_63,
	_t17_64, _t17_65, _t17_66, _t17_67, _t17_68, _t17_69, _t17_70, _t17_71,
	_t17_72, _t17_73, _t17_74, _t17_75, _t17_76, _t17_77, _t17_78, _t17_79,
	_t17_80, _t17_81, _t17_82, _t17_83, _t17_84, _t17_85, _t17_86, _t17_87,
	_t17_88, _t17_89, _t17_90, _t17_91, _t17_92, _t17_93, _t17_94, _t17_95,
	_t17_96, _t17_97, _t17_98, _t17_99, _t17_100, _t17_101, _t17_102, _t17_103,
	_t17_104, _t17_105, _t17_106, _t17_107, _t17_108, _t17_109, _t17_110, _t17_111,
	_t17_112, _t17_113, _t17_114, _t17_115, _t17_116, _t17_117, _t17_118, _t17_119,
	_t17_120, _t17_121, _t17_122, _t17_123, _t17_124, _t17_125, _t17_126, _t17_127,
	_t17_128, _t17_129, _t17_130, _t17_131, _t17_132, _t17_133, _t17_134, _t17_135,
	_t17_136, _t17_137, _t17_138, _t17_139, _t17_140, _t17_141, _t17_142, _t17_143,
	_t17_144, _t17_145, _t17_146, _t17_147, _t17_148, _t17_149, _t17_150, _t17_151,
	_t17_152, _t17_153, _t17_154, _t17_155, _t17_156, _t17_157, _t17_158, _t17_159,
	_t17_160, _t17_161, _t17_162, _t17_163, _t17_164, _t17_165, _t17_166, _t17_167,
	_t17_168, _t17_169, _t17_170, _t17_171, _t17_172, _t17_173, _t17_174, _t17_175,
	_t17_176, _t17_177, _t17_178, _t17_179, _t17_180, _t17_181, _t17_182, _t17_183,
	_t17_184, _t17_185, _t17_186, _t17_187, _t17_188, _t17_189, _t17_190, _t17_191,
	_t17_192, _t17_193, _t17_194, _t17_195, _t17_196, _t17_197, _t17_198, _t17_199,
	_t17_200, _t17_201, _t17_202, _t17_203, _t17_204, _t17_205, _t17_206, _t17_207,
	_t17_208, _t17_209, _t17_210, _t17_211, _t17_212, _t17_213, _t17_214, _t17_215,
	_t17_216, _t17_217, _t17_218, _t17_219, _t17_220, _t17_221, _t17_222, _t17_223,
	_t17_224, _t17_225, _t17_226, _t17_227, _t17_228, _t17_229, _t17_230, _t17_231,
	_t17_232, _t17_233, _t17_234, _t17_235, _t17_236, _t17_237, _t17_238, _t17_239,
	_t17_240, _t17_241, _t17_242, _t17_243, _t17_244, _t17_245, _t17_246, _t17_247,
	_t17_248, _t17_249, _t17_250, _t17_251, _t17_252, _t17_253, _t17_254, _t17_255,
	_t17_256, _t17_257, _t17_258, _t17_259, _t17_260, _t17_261, _t17_262, _t17_263,
	_t17_264, _t17_265, _t17_266, _t17_267, _t17_268, _t17_269, _t17_270, _t17_271,
	_t17_272, _t17_273, _t17_274, _t17_275, _t17_276, _t17_277, _t17_278, _t17_279,
	_t17_280, _t17_281, _t17_282, _t17_283, _t17_284, _t17_285, _t17_286, _t17_287,
	_t17_288, _t17_289, _t17_290, _t17_291, _t17_292, _t17_293, _t17_294, _t17_295,
	_t17_296, _t17_297, _t17_298, _t17_299, _t17_300, _t17_301, _t17_302, _t17_303,
	_t17_304, _t17_305, _t17_306, _t17_307, _t17_308, _t17_309, _t17_310, _t17_311,
	_t17_312, _t17_313, _t17_314, _t17_315, _t17_316, _t17_317, _t17_318, _t17_319,
	_t17_320, _t17_321, _t17_322, _t17_323, _t17_324, _t17_325, _t17_326, _t17_327,
	_t17_328, _t17_329, _t17_330, _t17_331, _t17_332, _t17_333, _t17_334, _t17_335,
	_t17_336, _t17_337, _t17_338, _t17_339, _t17_340, _t17_341, _t17_342, _t17_343,
	_t17_344, _t17_345, _t17_346, _t17_347, _t17_348, _t17_349, _t17_350, _t17_351,
	_t17_352, _t17_353, _t17_354, _t17_355, _t17_356, _t17_357, _t17_358, _t17_359,
	_t17_360, _t17_361, _t17_362, _t17_363, _t17_364, _t17_365, _t17_366, _t17_367,
	_t17_368, _t17_369, _t17_370, _t17_371, _t17_372, _t17_373, _t17_374, _t17_375,
	_t17_376, _t17_377, _t17_378, _t17_379, _t17_380, _t17_381, _t17_382, _t17_383,
	_t17_384, _t17_385, _t17_386, _t17_387, _t17_388, _t17_389, _t17_390, _t17_391,
	_t17_392, _t17_393, _t17_394, _t17_395, _t17_396, _t17_397, _t17_398, _t17_399,
	_t17_400, _t17_401, _t17_402, _t17_403, _t17_404, _t17_405, _t17_406, _t17_407,
	_t17_408, _t17_409, _t17_410, _t17_411, _t17_412, _t17_413, _t17_414, _t17_415,
	_t17_416, _t17_417, _t17_418, _t17_419, _t17_420, _t17_421, _t17_422, _t17_423,
	_t17_424, _t17_425, _t17_426, _t17_427, _t17_428, _t17_429, _t17_430, _t17_431,
	_t17_432, _t17_433, _t17_434;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31,
	_t20_32, _t20_33, _t20_34, _t20_35, _t20_36, _t20_37, _t20_38, _t20_39,
	_t20_40, _t20_41, _t20_42, _t20_43, _t20_44, _t20_45, _t20_46, _t20_47,
	_t20_48, _t20_49, _t20_50, _t20_51, _t20_52, _t20_53, _t20_54, _t20_55,
	_t20_56, _t20_57, _t20_58, _t20_59, _t20_60, _t20_61, _t20_62, _t20_63,
	_t20_64, _t20_65, _t20_66, _t20_67, _t20_68, _t20_69, _t20_70, _t20_71,
	_t20_72, _t20_73, _t20_74, _t20_75, _t20_76, _t20_77, _t20_78, _t20_79,
	_t20_80, _t20_81, _t20_82, _t20_83, _t20_84, _t20_85, _t20_86, _t20_87,
	_t20_88, _t20_89, _t20_90, _t20_91, _t20_92, _t20_93, _t20_94, _t20_95,
	_t20_96, _t20_97, _t20_98, _t20_99, _t20_100, _t20_101, _t20_102, _t20_103,
	_t20_104, _t20_105, _t20_106, _t20_107, _t20_108, _t20_109, _t20_110, _t20_111,
	_t20_112, _t20_113, _t20_114, _t20_115, _t20_116, _t20_117, _t20_118, _t20_119,
	_t20_120, _t20_121, _t20_122, _t20_123, _t20_124, _t20_125, _t20_126, _t20_127,
	_t20_128, _t20_129, _t20_130, _t20_131, _t20_132, _t20_133, _t20_134, _t20_135,
	_t20_136, _t20_137, _t20_138, _t20_139, _t20_140, _t20_141, _t20_142, _t20_143,
	_t20_144, _t20_145;

  _t0_40 = _mm256_castpd128_pd256(_mm_load_sd(&(C[0])));
  _t0_39 = _mm256_castpd128_pd256(_mm_load_sd(&(L[0])));
  _t0_38 = _mm256_castpd128_pd256(_mm_load_sd(&(U[0])));
  _t0_41 = _mm256_castpd128_pd256(_mm_load_sd(&(C[1])));
  _t0_37 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1])));
  _t0_36 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53])));
  _t0_42 = _mm256_castpd128_pd256(_mm_load_sd(&(C[2])));
  _t0_35 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 54)), 0);
  _t0_34 = _mm256_castpd128_pd256(_mm_load_sd(&(U[106])));
  _t0_43 = _mm256_castpd128_pd256(_mm_load_sd(&(C[3])));
  _t0_33 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 55))), _mm256_castpd128_pd256(_mm_load_sd(U + 107)), 32);
  _t0_32 = _mm256_castpd128_pd256(_mm_load_sd(&(U[159])));
  _t0_113 = _asm256_loadu_pd(C + 52);
  _t0_31 = _mm256_broadcast_sd(&(L[52]));
  _t0_30 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53])));
  _t0_114 = _asm256_loadu_pd(C + 104);
  _t0_29 = _mm256_maskload_pd(L + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t0_28 = _mm256_castpd128_pd256(_mm_load_sd(&(L[106])));
  _t0_115 = _asm256_loadu_pd(C + 156);
  _t0_27 = _mm256_maskload_pd(L + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t0_26 = _mm256_castpd128_pd256(_mm_load_sd(&(L[159])));
  _t0_116 = _asm256_loadu_pd(C + 4);
  _t0_117 = _asm256_loadu_pd(C + 56);
  _t0_118 = _asm256_loadu_pd(C + 108);
  _t0_119 = _asm256_loadu_pd(C + 160);
  _t0_25 = _asm256_loadu_pd(U + 4);
  _t0_24 = _asm256_loadu_pd(U + 56);
  _t0_23 = _asm256_loadu_pd(U + 108);
  _t0_22 = _asm256_loadu_pd(U + 160);
  _t0_21 = _mm256_castpd128_pd256(_mm_load_sd(&(U[212])));
  _t0_20 = _mm256_castpd128_pd256(_mm_load_sd(&(U[213])));
  _t0_19 = _mm256_castpd128_pd256(_mm_load_sd(&(U[265])));
  _t0_18 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 214)), _mm256_castpd128_pd256(_mm_load_sd(U + 266)), 0);
  _t0_17 = _mm256_castpd128_pd256(_mm_load_sd(&(U[318])));
  _t0_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 215)), _mm256_castpd128_pd256(_mm_load_sd(U + 267))), _mm256_castpd128_pd256(_mm_load_sd(U + 319)), 32);
  _t0_15 = _mm256_castpd128_pd256(_mm_load_sd(&(U[371])));
  _t0_109 = _asm256_loadu_pd(C + 8);
  _t0_110 = _asm256_loadu_pd(C + 60);
  _t0_111 = _asm256_loadu_pd(C + 112);
  _t0_112 = _asm256_loadu_pd(C + 164);
  _t0_14 = _asm256_loadu_pd(U + 8);
  _t0_13 = _asm256_loadu_pd(U + 60);
  _t0_12 = _asm256_loadu_pd(U + 112);
  _t0_11 = _asm256_loadu_pd(U + 164);
  _t0_10 = _asm256_loadu_pd(U + 216);
  _t0_9 = _asm256_loadu_pd(U + 268);
  _t0_8 = _asm256_loadu_pd(U + 320);
  _t0_7 = _asm256_loadu_pd(U + 372);
  _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[424])));
  _t0_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[425])));
  _t0_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[477])));
  _t0_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 426)), _mm256_castpd128_pd256(_mm_load_sd(U + 478)), 0);
  _t0_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[530])));
  _t0_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 427)), _mm256_castpd128_pd256(_mm_load_sd(U + 479))), _mm256_castpd128_pd256(_mm_load_sd(U + 531)), 32);
  _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[583])));

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_139 = _t0_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_145 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_148 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t0_149 = _mm256_add_pd(_t0_145, _t0_148);

  // 4-BLAC: 1x4 / 1x4
  _t0_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_139), _mm256_castpd256_pd128(_t0_149)));

  // AVX Storer:
  _t0_40 = _t0_150;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 0), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_151 = _t0_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_152 = _t0_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_153 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_154 = _mm256_mul_pd(_t0_152, _t0_153);

  // 4-BLAC: 1x4 - 1x4
  _t0_155 = _mm256_sub_pd(_t0_151, _t0_154);

  // AVX Storer:
  _t0_41 = _t0_155;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_156 = _t0_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_157 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_158 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t0_159 = _mm256_add_pd(_t0_157, _t0_158);

  // 4-BLAC: 1x4 / 1x4
  _t0_160 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_156), _mm256_castpd256_pd128(_t0_159)));

  // AVX Storer:
  _t0_41 = _t0_160;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 0), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_161 = _t0_42;

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_162 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_163 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t0_164 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_162, _t0_163), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_162, _t0_163), _mm256_mul_pd(_t0_162, _t0_163), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_162, _t0_163), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_162, _t0_163), _mm256_mul_pd(_t0_162, _t0_163), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_162, _t0_163), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_162, _t0_163), _mm256_mul_pd(_t0_162, _t0_163), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_165 = _mm256_sub_pd(_t0_161, _t0_164);

  // AVX Storer:
  _t0_42 = _t0_165;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_166 = _t0_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_167 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_168 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t0_169 = _mm256_add_pd(_t0_167, _t0_168);

  // 4-BLAC: 1x4 / 1x4
  _t0_170 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_166), _mm256_castpd256_pd128(_t0_169)));

  // AVX Storer:
  _t0_42 = _t0_170;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 0), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_171 = _t0_43;

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_172 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_173 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t0_174 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_172, _t0_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_172, _t0_173), _mm256_mul_pd(_t0_172, _t0_173), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_172, _t0_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_172, _t0_173), _mm256_mul_pd(_t0_172, _t0_173), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_172, _t0_173), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_172, _t0_173), _mm256_mul_pd(_t0_172, _t0_173), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_175 = _mm256_sub_pd(_t0_171, _t0_174);

  // AVX Storer:
  _t0_43 = _t0_175;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_176 = _t0_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_177 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_178 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t0_179 = _mm256_add_pd(_t0_177, _t0_178);

  // 4-BLAC: 1x4 / 1x4
  _t0_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_176), _mm256_castpd256_pd128(_t0_179)));

  // AVX Storer:
  _t0_43 = _t0_180;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_181 = _t0_31;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t0_89 = _mm256_mul_pd(_t0_181, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_113 = _mm256_sub_pd(_t0_113, _t0_89);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_182 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_183 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_184 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t0_185 = _mm256_add_pd(_t0_183, _t0_184);

  // 4-BLAC: 1x4 / 1x4
  _t0_186 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_182), _mm256_castpd256_pd128(_t0_185)));

  // AVX Storer:
  _t0_44 = _t0_186;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 1), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_187 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_188 = _t0_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_189 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_190 = _mm256_mul_pd(_t0_188, _t0_189);

  // 4-BLAC: 1x4 - 1x4
  _t0_191 = _mm256_sub_pd(_t0_187, _t0_190);

  // AVX Storer:
  _t0_45 = _t0_191;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_192 = _t0_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_193 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_194 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t0_195 = _mm256_add_pd(_t0_193, _t0_194);

  // 4-BLAC: 1x4 / 1x4
  _t0_196 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_192), _mm256_castpd256_pd128(_t0_195)));

  // AVX Storer:
  _t0_45 = _t0_196;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 1), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_197 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_198 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_44, _t0_45), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_199 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t0_200 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_198, _t0_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_198, _t0_199), _mm256_mul_pd(_t0_198, _t0_199), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_198, _t0_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_198, _t0_199), _mm256_mul_pd(_t0_198, _t0_199), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_198, _t0_199), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_198, _t0_199), _mm256_mul_pd(_t0_198, _t0_199), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_201 = _mm256_sub_pd(_t0_197, _t0_200);

  // AVX Storer:
  _t0_46 = _t0_201;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_202 = _t0_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_203 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_204 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t0_205 = _mm256_add_pd(_t0_203, _t0_204);

  // 4-BLAC: 1x4 / 1x4
  _t0_206 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_202), _mm256_castpd256_pd128(_t0_205)));

  // AVX Storer:
  _t0_46 = _t0_206;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 1), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_207 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_113, _t0_113, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_208 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_44, _t0_45), _mm256_unpacklo_pd(_t0_46, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_209 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t0_210 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_208, _t0_209), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_208, _t0_209), _mm256_mul_pd(_t0_208, _t0_209), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_208, _t0_209), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_208, _t0_209), _mm256_mul_pd(_t0_208, _t0_209), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_208, _t0_209), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_208, _t0_209), _mm256_mul_pd(_t0_208, _t0_209), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_211 = _mm256_sub_pd(_t0_207, _t0_210);

  // AVX Storer:
  _t0_47 = _t0_211;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_212 = _t0_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_213 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_214 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t0_215 = _mm256_add_pd(_t0_213, _t0_214);

  // 4-BLAC: 1x4 / 1x4
  _t0_216 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_212), _mm256_castpd256_pd128(_t0_215)));

  // AVX Storer:
  _t0_47 = _t0_216;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) * G(h(2, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_217 = _t0_29;

  // AVX Loader:

  // 2x4 -> 4x4
  _t0_218 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32);
  _t0_219 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_44, _t0_45), _mm256_unpacklo_pd(_t0_46, _t0_47), 32);
  _t0_220 = _mm256_setzero_pd();
  _t0_221 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_217, _t0_217, 32), _mm256_permute2f128_pd(_t0_217, _t0_217, 32), 0), _t0_218), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_217, _t0_217, 32), _mm256_permute2f128_pd(_t0_217, _t0_217, 32), 15), _t0_219)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_217, _t0_217, 49), _mm256_permute2f128_pd(_t0_217, _t0_217, 49), 0), _t0_220), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_217, _t0_217, 49), _mm256_permute2f128_pd(_t0_217, _t0_217, 49), 15), _t0_221)));

  // 4-BLAC: 1x4 - 1x4
  _t0_114 = _mm256_sub_pd(_t0_114, _t0_103);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_222 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_114, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_223 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_224 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t0_225 = _mm256_add_pd(_t0_223, _t0_224);

  // 4-BLAC: 1x4 / 1x4
  _t0_226 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_222), _mm256_castpd256_pd128(_t0_225)));

  // AVX Storer:
  _t0_48 = _t0_226;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 2), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_227 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_114, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_228 = _t0_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_229 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_230 = _mm256_mul_pd(_t0_228, _t0_229);

  // 4-BLAC: 1x4 - 1x4
  _t0_231 = _mm256_sub_pd(_t0_227, _t0_230);

  // AVX Storer:
  _t0_49 = _t0_231;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_232 = _t0_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_233 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_234 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t0_235 = _mm256_add_pd(_t0_233, _t0_234);

  // 4-BLAC: 1x4 / 1x4
  _t0_236 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_232), _mm256_castpd256_pd128(_t0_235)));

  // AVX Storer:
  _t0_49 = _t0_236;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 2), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_237 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_114, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_114, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_238 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_48, _t0_49), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_239 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t0_240 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_238, _t0_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_238, _t0_239), _mm256_mul_pd(_t0_238, _t0_239), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_238, _t0_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_238, _t0_239), _mm256_mul_pd(_t0_238, _t0_239), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_238, _t0_239), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_238, _t0_239), _mm256_mul_pd(_t0_238, _t0_239), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_241 = _mm256_sub_pd(_t0_237, _t0_240);

  // AVX Storer:
  _t0_50 = _t0_241;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_242 = _t0_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_243 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_244 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t0_245 = _mm256_add_pd(_t0_243, _t0_244);

  // 4-BLAC: 1x4 / 1x4
  _t0_246 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_242), _mm256_castpd256_pd128(_t0_245)));

  // AVX Storer:
  _t0_50 = _t0_246;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 2), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_247 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_114, _t0_114, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_248 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_48, _t0_49), _mm256_unpacklo_pd(_t0_50, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_249 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t0_250 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_248, _t0_249), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_248, _t0_249), _mm256_mul_pd(_t0_248, _t0_249), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_248, _t0_249), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_248, _t0_249), _mm256_mul_pd(_t0_248, _t0_249), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_248, _t0_249), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_248, _t0_249), _mm256_mul_pd(_t0_248, _t0_249), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_251 = _mm256_sub_pd(_t0_247, _t0_250);

  // AVX Storer:
  _t0_51 = _t0_251;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_252 = _t0_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_253 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_254 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t0_255 = _mm256_add_pd(_t0_253, _t0_254);

  // 4-BLAC: 1x4 / 1x4
  _t0_256 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_252), _mm256_castpd256_pd128(_t0_255)));

  // AVX Storer:
  _t0_51 = _t0_256;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) * G(h(3, 52, 0), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_257 = _t0_27;

  // AVX Loader:

  // 3x4 -> 4x4
  _t0_258 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32);
  _t0_259 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_44, _t0_45), _mm256_unpacklo_pd(_t0_46, _t0_47), 32);
  _t0_260 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_48, _t0_49), _mm256_unpacklo_pd(_t0_50, _t0_51), 32);
  _t0_261 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_257, _t0_257, 32), _mm256_permute2f128_pd(_t0_257, _t0_257, 32), 0), _t0_258), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_257, _t0_257, 32), _mm256_permute2f128_pd(_t0_257, _t0_257, 32), 15), _t0_259)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_257, _t0_257, 49), _mm256_permute2f128_pd(_t0_257, _t0_257, 49), 0), _t0_260), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_257, _t0_257, 49), _mm256_permute2f128_pd(_t0_257, _t0_257, 49), 15), _t0_261)));

  // 4-BLAC: 1x4 - 1x4
  _t0_115 = _mm256_sub_pd(_t0_115, _t0_104);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_262 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_115, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_263 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_264 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t0_265 = _mm256_add_pd(_t0_263, _t0_264);

  // 4-BLAC: 1x4 / 1x4
  _t0_266 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_262), _mm256_castpd256_pd128(_t0_265)));

  // AVX Storer:
  _t0_52 = _t0_266;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 3), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_267 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_115, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_268 = _t0_52;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_269 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_270 = _mm256_mul_pd(_t0_268, _t0_269);

  // 4-BLAC: 1x4 - 1x4
  _t0_271 = _mm256_sub_pd(_t0_267, _t0_270);

  // AVX Storer:
  _t0_53 = _t0_271;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_272 = _t0_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_273 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_274 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t0_275 = _mm256_add_pd(_t0_273, _t0_274);

  // 4-BLAC: 1x4 / 1x4
  _t0_276 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_272), _mm256_castpd256_pd128(_t0_275)));

  // AVX Storer:
  _t0_53 = _t0_276;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 3), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_277 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_115, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_115, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_278 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_52, _t0_53), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_279 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t0_280 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_278, _t0_279), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_278, _t0_279), _mm256_mul_pd(_t0_278, _t0_279), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_278, _t0_279), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_278, _t0_279), _mm256_mul_pd(_t0_278, _t0_279), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_278, _t0_279), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_278, _t0_279), _mm256_mul_pd(_t0_278, _t0_279), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_281 = _mm256_sub_pd(_t0_277, _t0_280);

  // AVX Storer:
  _t0_54 = _t0_281;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_282 = _t0_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_283 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_284 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t0_285 = _mm256_add_pd(_t0_283, _t0_284);

  // 4-BLAC: 1x4 / 1x4
  _t0_286 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_282), _mm256_castpd256_pd128(_t0_285)));

  // AVX Storer:
  _t0_54 = _t0_286;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 3), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_287 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_115, _t0_115, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_288 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_52, _t0_53), _mm256_unpacklo_pd(_t0_54, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_289 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t0_290 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_288, _t0_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_288, _t0_289), _mm256_mul_pd(_t0_288, _t0_289), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_288, _t0_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_288, _t0_289), _mm256_mul_pd(_t0_288, _t0_289), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_288, _t0_289), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_288, _t0_289), _mm256_mul_pd(_t0_288, _t0_289), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_291 = _mm256_sub_pd(_t0_287, _t0_290);

  // AVX Storer:
  _t0_55 = _t0_291;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_292 = _t0_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_293 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_294 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t0_295 = _mm256_add_pd(_t0_293, _t0_294);

  // 4-BLAC: 1x4 / 1x4
  _t0_296 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_292), _mm256_castpd256_pd128(_t0_295)));

  // AVX Storer:
  _t0_55 = _t0_296;

  // Generating : X[52,52] = S(h(4, 52, 0), ( G(h(4, 52, 0), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, 0), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t0_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_40, _t0_40, 32), _mm256_permute2f128_pd(_t0_40, _t0_40, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_42, _t0_42, 32), _mm256_permute2f128_pd(_t0_42, _t0_42, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0), _t0_22)));
  _t0_106 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_44, _t0_44, 32), _mm256_permute2f128_pd(_t0_44, _t0_44, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_45, _t0_45, 32), _mm256_permute2f128_pd(_t0_45, _t0_45, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_46, _t0_46, 32), _mm256_permute2f128_pd(_t0_46, _t0_46, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_47, _t0_47, 32), _mm256_permute2f128_pd(_t0_47, _t0_47, 32), 0), _t0_22)));
  _t0_107 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_50, _t0_50, 32), _mm256_permute2f128_pd(_t0_50, _t0_50, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t0_22)));
  _t0_108 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_52, _t0_52, 32), _mm256_permute2f128_pd(_t0_52, _t0_52, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_22)));

  // 4-BLAC: 4x4 - 4x4
  _t0_116 = _mm256_sub_pd(_t0_116, _t0_105);
  _t0_117 = _mm256_sub_pd(_t0_117, _t0_106);
  _t0_118 = _mm256_sub_pd(_t0_118, _t0_107);
  _t0_119 = _mm256_sub_pd(_t0_119, _t0_108);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_297 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_116, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_298 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_299 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t0_300 = _mm256_add_pd(_t0_298, _t0_299);

  // 4-BLAC: 1x4 / 1x4
  _t0_301 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_297), _mm256_castpd256_pd128(_t0_300)));

  // AVX Storer:
  _t0_56 = _t0_301;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 0), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_302 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_116, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_303 = _t0_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_304 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_305 = _mm256_mul_pd(_t0_303, _t0_304);

  // 4-BLAC: 1x4 - 1x4
  _t0_306 = _mm256_sub_pd(_t0_302, _t0_305);

  // AVX Storer:
  _t0_57 = _t0_306;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_307 = _t0_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_308 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_309 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t0_310 = _mm256_add_pd(_t0_308, _t0_309);

  // 4-BLAC: 1x4 / 1x4
  _t0_311 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_307), _mm256_castpd256_pd128(_t0_310)));

  // AVX Storer:
  _t0_57 = _t0_311;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 0), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_312 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_116, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_116, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_313 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_56, _t0_57), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_314 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t0_315 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_313, _t0_314), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_313, _t0_314), _mm256_mul_pd(_t0_313, _t0_314), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_313, _t0_314), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_313, _t0_314), _mm256_mul_pd(_t0_313, _t0_314), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_313, _t0_314), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_313, _t0_314), _mm256_mul_pd(_t0_313, _t0_314), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_316 = _mm256_sub_pd(_t0_312, _t0_315);

  // AVX Storer:
  _t0_58 = _t0_316;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_317 = _t0_58;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_318 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_319 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t0_320 = _mm256_add_pd(_t0_318, _t0_319);

  // 4-BLAC: 1x4 / 1x4
  _t0_321 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_317), _mm256_castpd256_pd128(_t0_320)));

  // AVX Storer:
  _t0_58 = _t0_321;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 0), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_322 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_116, _t0_116, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_323 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_56, _t0_57), _mm256_unpacklo_pd(_t0_58, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_324 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t0_325 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_323, _t0_324), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_323, _t0_324), _mm256_mul_pd(_t0_323, _t0_324), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_323, _t0_324), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_323, _t0_324), _mm256_mul_pd(_t0_323, _t0_324), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_323, _t0_324), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_323, _t0_324), _mm256_mul_pd(_t0_323, _t0_324), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_326 = _mm256_sub_pd(_t0_322, _t0_325);

  // AVX Storer:
  _t0_59 = _t0_326;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_327 = _t0_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_328 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_329 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t0_330 = _mm256_add_pd(_t0_328, _t0_329);

  // 4-BLAC: 1x4 / 1x4
  _t0_331 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_327), _mm256_castpd256_pd128(_t0_330)));

  // AVX Storer:
  _t0_59 = _t0_331;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_332 = _t0_31;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t0_90 = _mm256_mul_pd(_t0_332, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_56, _t0_57), _mm256_unpacklo_pd(_t0_58, _t0_59), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_117 = _mm256_sub_pd(_t0_117, _t0_90);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_333 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_117, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_334 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_335 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t0_336 = _mm256_add_pd(_t0_334, _t0_335);

  // 4-BLAC: 1x4 / 1x4
  _t0_337 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_333), _mm256_castpd256_pd128(_t0_336)));

  // AVX Storer:
  _t0_60 = _t0_337;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 1), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_338 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_117, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_339 = _t0_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_340 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_341 = _mm256_mul_pd(_t0_339, _t0_340);

  // 4-BLAC: 1x4 - 1x4
  _t0_342 = _mm256_sub_pd(_t0_338, _t0_341);

  // AVX Storer:
  _t0_61 = _t0_342;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_343 = _t0_61;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_344 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_345 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t0_346 = _mm256_add_pd(_t0_344, _t0_345);

  // 4-BLAC: 1x4 / 1x4
  _t0_347 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_343), _mm256_castpd256_pd128(_t0_346)));

  // AVX Storer:
  _t0_61 = _t0_347;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 1), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_348 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_117, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_117, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_349 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_60, _t0_61), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_350 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t0_351 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_349, _t0_350), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_349, _t0_350), _mm256_mul_pd(_t0_349, _t0_350), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_349, _t0_350), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_349, _t0_350), _mm256_mul_pd(_t0_349, _t0_350), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_349, _t0_350), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_349, _t0_350), _mm256_mul_pd(_t0_349, _t0_350), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_352 = _mm256_sub_pd(_t0_348, _t0_351);

  // AVX Storer:
  _t0_62 = _t0_352;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_353 = _t0_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_354 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_355 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t0_356 = _mm256_add_pd(_t0_354, _t0_355);

  // 4-BLAC: 1x4 / 1x4
  _t0_357 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_353), _mm256_castpd256_pd128(_t0_356)));

  // AVX Storer:
  _t0_62 = _t0_357;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 1), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_358 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_117, _t0_117, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_359 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_60, _t0_61), _mm256_unpacklo_pd(_t0_62, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_360 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t0_361 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_359, _t0_360), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_359, _t0_360), _mm256_mul_pd(_t0_359, _t0_360), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_359, _t0_360), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_359, _t0_360), _mm256_mul_pd(_t0_359, _t0_360), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_359, _t0_360), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_359, _t0_360), _mm256_mul_pd(_t0_359, _t0_360), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_362 = _mm256_sub_pd(_t0_358, _t0_361);

  // AVX Storer:
  _t0_63 = _t0_362;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_363 = _t0_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_364 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_365 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t0_366 = _mm256_add_pd(_t0_364, _t0_365);

  // 4-BLAC: 1x4 / 1x4
  _t0_367 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_363), _mm256_castpd256_pd128(_t0_366)));

  // AVX Storer:
  _t0_63 = _t0_367;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) * G(h(2, 52, 0), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_368 = _t0_29;

  // AVX Loader:

  // 2x4 -> 4x4
  _t0_369 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_56, _t0_57), _mm256_unpacklo_pd(_t0_58, _t0_59), 32);
  _t0_370 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_60, _t0_61), _mm256_unpacklo_pd(_t0_62, _t0_63), 32);
  _t0_371 = _mm256_setzero_pd();
  _t0_372 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_91 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_368, _t0_368, 32), _mm256_permute2f128_pd(_t0_368, _t0_368, 32), 0), _t0_369), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_368, _t0_368, 32), _mm256_permute2f128_pd(_t0_368, _t0_368, 32), 15), _t0_370)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_368, _t0_368, 49), _mm256_permute2f128_pd(_t0_368, _t0_368, 49), 0), _t0_371), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_368, _t0_368, 49), _mm256_permute2f128_pd(_t0_368, _t0_368, 49), 15), _t0_372)));

  // 4-BLAC: 1x4 - 1x4
  _t0_118 = _mm256_sub_pd(_t0_118, _t0_91);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_373 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_374 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_375 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t0_376 = _mm256_add_pd(_t0_374, _t0_375);

  // 4-BLAC: 1x4 / 1x4
  _t0_377 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_373), _mm256_castpd256_pd128(_t0_376)));

  // AVX Storer:
  _t0_64 = _t0_377;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 2), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_378 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_379 = _t0_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_380 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_381 = _mm256_mul_pd(_t0_379, _t0_380);

  // 4-BLAC: 1x4 - 1x4
  _t0_382 = _mm256_sub_pd(_t0_378, _t0_381);

  // AVX Storer:
  _t0_65 = _t0_382;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_383 = _t0_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_384 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_385 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t0_386 = _mm256_add_pd(_t0_384, _t0_385);

  // 4-BLAC: 1x4 / 1x4
  _t0_387 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_383), _mm256_castpd256_pd128(_t0_386)));

  // AVX Storer:
  _t0_65 = _t0_387;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 2), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_388 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_118, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_389 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_64, _t0_65), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_390 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t0_391 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_389, _t0_390), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_389, _t0_390), _mm256_mul_pd(_t0_389, _t0_390), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_389, _t0_390), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_389, _t0_390), _mm256_mul_pd(_t0_389, _t0_390), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_389, _t0_390), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_389, _t0_390), _mm256_mul_pd(_t0_389, _t0_390), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_392 = _mm256_sub_pd(_t0_388, _t0_391);

  // AVX Storer:
  _t0_66 = _t0_392;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_393 = _t0_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_394 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_395 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t0_396 = _mm256_add_pd(_t0_394, _t0_395);

  // 4-BLAC: 1x4 / 1x4
  _t0_397 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_393), _mm256_castpd256_pd128(_t0_396)));

  // AVX Storer:
  _t0_66 = _t0_397;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 2), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_398 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_118, _t0_118, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_399 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_64, _t0_65), _mm256_unpacklo_pd(_t0_66, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_400 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t0_401 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_399, _t0_400), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_399, _t0_400), _mm256_mul_pd(_t0_399, _t0_400), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_399, _t0_400), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_399, _t0_400), _mm256_mul_pd(_t0_399, _t0_400), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_399, _t0_400), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_399, _t0_400), _mm256_mul_pd(_t0_399, _t0_400), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_402 = _mm256_sub_pd(_t0_398, _t0_401);

  // AVX Storer:
  _t0_67 = _t0_402;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_403 = _t0_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_404 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_405 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t0_406 = _mm256_add_pd(_t0_404, _t0_405);

  // 4-BLAC: 1x4 / 1x4
  _t0_407 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_403), _mm256_castpd256_pd128(_t0_406)));

  // AVX Storer:
  _t0_67 = _t0_407;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) * G(h(3, 52, 0), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_408 = _t0_27;

  // AVX Loader:

  // 3x4 -> 4x4
  _t0_409 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_56, _t0_57), _mm256_unpacklo_pd(_t0_58, _t0_59), 32);
  _t0_410 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_60, _t0_61), _mm256_unpacklo_pd(_t0_62, _t0_63), 32);
  _t0_411 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_64, _t0_65), _mm256_unpacklo_pd(_t0_66, _t0_67), 32);
  _t0_412 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_92 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_408, _t0_408, 32), _mm256_permute2f128_pd(_t0_408, _t0_408, 32), 0), _t0_409), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_408, _t0_408, 32), _mm256_permute2f128_pd(_t0_408, _t0_408, 32), 15), _t0_410)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_408, _t0_408, 49), _mm256_permute2f128_pd(_t0_408, _t0_408, 49), 0), _t0_411), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_408, _t0_408, 49), _mm256_permute2f128_pd(_t0_408, _t0_408, 49), 15), _t0_412)));

  // 4-BLAC: 1x4 - 1x4
  _t0_119 = _mm256_sub_pd(_t0_119, _t0_92);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_413 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_119, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_414 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_415 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t0_416 = _mm256_add_pd(_t0_414, _t0_415);

  // 4-BLAC: 1x4 / 1x4
  _t0_417 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_413), _mm256_castpd256_pd128(_t0_416)));

  // AVX Storer:
  _t0_68 = _t0_417;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 3), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_418 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_119, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_419 = _t0_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_420 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_421 = _mm256_mul_pd(_t0_419, _t0_420);

  // 4-BLAC: 1x4 - 1x4
  _t0_422 = _mm256_sub_pd(_t0_418, _t0_421);

  // AVX Storer:
  _t0_69 = _t0_422;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_423 = _t0_69;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_424 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_425 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t0_426 = _mm256_add_pd(_t0_424, _t0_425);

  // 4-BLAC: 1x4 / 1x4
  _t0_427 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_423), _mm256_castpd256_pd128(_t0_426)));

  // AVX Storer:
  _t0_69 = _t0_427;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 3), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_428 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_119, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_119, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_429 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_68, _t0_69), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_430 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t0_431 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_429, _t0_430), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_429, _t0_430), _mm256_mul_pd(_t0_429, _t0_430), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_429, _t0_430), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_429, _t0_430), _mm256_mul_pd(_t0_429, _t0_430), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_429, _t0_430), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_429, _t0_430), _mm256_mul_pd(_t0_429, _t0_430), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_432 = _mm256_sub_pd(_t0_428, _t0_431);

  // AVX Storer:
  _t0_70 = _t0_432;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_433 = _t0_70;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_434 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_435 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t0_436 = _mm256_add_pd(_t0_434, _t0_435);

  // 4-BLAC: 1x4 / 1x4
  _t0_437 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_433), _mm256_castpd256_pd128(_t0_436)));

  // AVX Storer:
  _t0_70 = _t0_437;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 3), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_438 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_119, _t0_119, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_439 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_68, _t0_69), _mm256_unpacklo_pd(_t0_70, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_440 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t0_441 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_439, _t0_440), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_439, _t0_440), _mm256_mul_pd(_t0_439, _t0_440), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_439, _t0_440), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_439, _t0_440), _mm256_mul_pd(_t0_439, _t0_440), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_439, _t0_440), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_439, _t0_440), _mm256_mul_pd(_t0_439, _t0_440), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_442 = _mm256_sub_pd(_t0_438, _t0_441);

  // AVX Storer:
  _t0_71 = _t0_442;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_443 = _t0_71;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_444 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_445 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t0_446 = _mm256_add_pd(_t0_444, _t0_445);

  // 4-BLAC: 1x4 / 1x4
  _t0_447 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_443), _mm256_castpd256_pd128(_t0_446)));

  // AVX Storer:
  _t0_71 = _t0_447;

  // Generating : X[52,52] = ( S(h(4, 52, 0), ( G(h(4, 52, 0), X[52,52],h(4, 52, fi879)) - ( G(h(4, 52, 0), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879)) + Sum_{k116} ( -$(h(4, 52, 0), ( G(h(4, 52, 0), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi879)) ),h(4, 52, fi879)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t0_93 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_40, _t0_40, 32), _mm256_permute2f128_pd(_t0_40, _t0_40, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_42, _t0_42, 32), _mm256_permute2f128_pd(_t0_42, _t0_42, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0), _t0_11)));
  _t0_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_44, _t0_44, 32), _mm256_permute2f128_pd(_t0_44, _t0_44, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_45, _t0_45, 32), _mm256_permute2f128_pd(_t0_45, _t0_45, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_46, _t0_46, 32), _mm256_permute2f128_pd(_t0_46, _t0_46, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_47, _t0_47, 32), _mm256_permute2f128_pd(_t0_47, _t0_47, 32), 0), _t0_11)));
  _t0_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_50, _t0_50, 32), _mm256_permute2f128_pd(_t0_50, _t0_50, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t0_11)));
  _t0_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_52, _t0_52, 32), _mm256_permute2f128_pd(_t0_52, _t0_52, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_11)));

  // 4-BLAC: 4x4 - 4x4
  _t0_109 = _mm256_sub_pd(_t0_109, _t0_93);
  _t0_110 = _mm256_sub_pd(_t0_110, _t0_94);
  _t0_111 = _mm256_sub_pd(_t0_111, _t0_95);
  _t0_112 = _mm256_sub_pd(_t0_112, _t0_96);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t0_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_56, _t0_56, 32), _mm256_permute2f128_pd(_t0_56, _t0_56, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_58, _t0_58, 32), _mm256_permute2f128_pd(_t0_58, _t0_58, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_59, _t0_59, 32), _mm256_permute2f128_pd(_t0_59, _t0_59, 32), 0), _t0_7)));
  _t0_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_60, _t0_60, 32), _mm256_permute2f128_pd(_t0_60, _t0_60, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_61, _t0_61, 32), _mm256_permute2f128_pd(_t0_61, _t0_61, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_62, _t0_62, 32), _mm256_permute2f128_pd(_t0_62, _t0_62, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0), _t0_7)));
  _t0_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_64, _t0_64, 32), _mm256_permute2f128_pd(_t0_64, _t0_64, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_65, _t0_65, 32), _mm256_permute2f128_pd(_t0_65, _t0_65, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_66, _t0_66, 32), _mm256_permute2f128_pd(_t0_66, _t0_66, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_67, _t0_67, 32), _mm256_permute2f128_pd(_t0_67, _t0_67, 32), 0), _t0_7)));
  _t0_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_68, _t0_68, 32), _mm256_permute2f128_pd(_t0_68, _t0_68, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_69, _t0_69, 32), _mm256_permute2f128_pd(_t0_69, _t0_69, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_70, _t0_70, 32), _mm256_permute2f128_pd(_t0_70, _t0_70, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_71, _t0_71, 32), _mm256_permute2f128_pd(_t0_71, _t0_71, 32), 0), _t0_7)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t0_109 = _mm256_sub_pd(_t0_109, _t0_97);
  _t0_110 = _mm256_sub_pd(_t0_110, _t0_98);
  _t0_111 = _mm256_sub_pd(_t0_111, _t0_99);
  _t0_112 = _mm256_sub_pd(_t0_112, _t0_100);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_448 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_449 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_450 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t0_451 = _mm256_add_pd(_t0_449, _t0_450);

  // 4-BLAC: 1x4 / 1x4
  _t0_452 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_448), _mm256_castpd256_pd128(_t0_451)));

  // AVX Storer:
  _t0_72 = _t0_452;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_453 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_454 = _t0_72;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_455 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_456 = _mm256_mul_pd(_t0_454, _t0_455);

  // 4-BLAC: 1x4 - 1x4
  _t0_457 = _mm256_sub_pd(_t0_453, _t0_456);

  // AVX Storer:
  _t0_73 = _t0_457;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_458 = _t0_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_459 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_460 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t0_461 = _mm256_add_pd(_t0_459, _t0_460);

  // 4-BLAC: 1x4 / 1x4
  _t0_462 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_458), _mm256_castpd256_pd128(_t0_461)));

  // AVX Storer:
  _t0_73 = _t0_462;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 0), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_463 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_464 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_72, _t0_73), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_465 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t0_466 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_464, _t0_465), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_464, _t0_465), _mm256_mul_pd(_t0_464, _t0_465), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_464, _t0_465), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_464, _t0_465), _mm256_mul_pd(_t0_464, _t0_465), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_464, _t0_465), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_464, _t0_465), _mm256_mul_pd(_t0_464, _t0_465), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_467 = _mm256_sub_pd(_t0_463, _t0_466);

  // AVX Storer:
  _t0_74 = _t0_467;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_468 = _t0_74;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_469 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_470 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t0_471 = _mm256_add_pd(_t0_469, _t0_470);

  // 4-BLAC: 1x4 / 1x4
  _t0_472 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_468), _mm256_castpd256_pd128(_t0_471)));

  // AVX Storer:
  _t0_74 = _t0_472;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 0), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_473 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_109, _t0_109, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_474 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_73), _mm256_unpacklo_pd(_t0_74, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_475 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t0_476 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_474, _t0_475), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_474, _t0_475), _mm256_mul_pd(_t0_474, _t0_475), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_474, _t0_475), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_474, _t0_475), _mm256_mul_pd(_t0_474, _t0_475), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_474, _t0_475), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_474, _t0_475), _mm256_mul_pd(_t0_474, _t0_475), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_477 = _mm256_sub_pd(_t0_473, _t0_476);

  // AVX Storer:
  _t0_75 = _t0_477;

  // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_478 = _t0_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_479 = _t0_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_480 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t0_481 = _mm256_add_pd(_t0_479, _t0_480);

  // 4-BLAC: 1x4 / 1x4
  _t0_482 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_478), _mm256_castpd256_pd128(_t0_481)));

  // AVX Storer:
  _t0_75 = _t0_482;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(4, 52, fi879)) - ( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), X[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_483 = _t0_31;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t0_88 = _mm256_mul_pd(_t0_483, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_73), _mm256_unpacklo_pd(_t0_74, _t0_75), 32));

  // 4-BLAC: 1x4 - 1x4
  _t0_110 = _mm256_sub_pd(_t0_110, _t0_88);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_484 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_485 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_486 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t0_487 = _mm256_add_pd(_t0_485, _t0_486);

  // 4-BLAC: 1x4 / 1x4
  _t0_488 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_484), _mm256_castpd256_pd128(_t0_487)));

  // AVX Storer:
  _t0_76 = _t0_488;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_489 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_490 = _t0_76;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_491 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_492 = _mm256_mul_pd(_t0_490, _t0_491);

  // 4-BLAC: 1x4 - 1x4
  _t0_493 = _mm256_sub_pd(_t0_489, _t0_492);

  // AVX Storer:
  _t0_77 = _t0_493;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_494 = _t0_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_495 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_496 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t0_497 = _mm256_add_pd(_t0_495, _t0_496);

  // 4-BLAC: 1x4 / 1x4
  _t0_498 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_494), _mm256_castpd256_pd128(_t0_497)));

  // AVX Storer:
  _t0_77 = _t0_498;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 1), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_499 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_500 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_76, _t0_77), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_501 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t0_502 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_500, _t0_501), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_500, _t0_501), _mm256_mul_pd(_t0_500, _t0_501), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_500, _t0_501), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_500, _t0_501), _mm256_mul_pd(_t0_500, _t0_501), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_500, _t0_501), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_500, _t0_501), _mm256_mul_pd(_t0_500, _t0_501), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_503 = _mm256_sub_pd(_t0_499, _t0_502);

  // AVX Storer:
  _t0_78 = _t0_503;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_504 = _t0_78;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_505 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_506 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t0_507 = _mm256_add_pd(_t0_505, _t0_506);

  // 4-BLAC: 1x4 / 1x4
  _t0_508 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_504), _mm256_castpd256_pd128(_t0_507)));

  // AVX Storer:
  _t0_78 = _t0_508;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 1), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_509 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_110, _t0_110, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_510 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_76, _t0_77), _mm256_unpacklo_pd(_t0_78, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_511 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t0_512 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_510, _t0_511), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_510, _t0_511), _mm256_mul_pd(_t0_510, _t0_511), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_510, _t0_511), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_510, _t0_511), _mm256_mul_pd(_t0_510, _t0_511), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_510, _t0_511), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_510, _t0_511), _mm256_mul_pd(_t0_510, _t0_511), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_513 = _mm256_sub_pd(_t0_509, _t0_512);

  // AVX Storer:
  _t0_79 = _t0_513;

  // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_514 = _t0_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_515 = _t0_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_516 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t0_517 = _mm256_add_pd(_t0_515, _t0_516);

  // 4-BLAC: 1x4 / 1x4
  _t0_518 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_514), _mm256_castpd256_pd128(_t0_517)));

  // AVX Storer:
  _t0_79 = _t0_518;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(4, 52, fi879)) - ( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) * G(h(2, 52, 0), X[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_519 = _t0_29;

  // AVX Loader:

  // 2x4 -> 4x4
  _t0_520 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_73), _mm256_unpacklo_pd(_t0_74, _t0_75), 32);
  _t0_521 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_76, _t0_77), _mm256_unpacklo_pd(_t0_78, _t0_79), 32);
  _t0_522 = _mm256_setzero_pd();
  _t0_523 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_519, _t0_519, 32), _mm256_permute2f128_pd(_t0_519, _t0_519, 32), 0), _t0_520), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_519, _t0_519, 32), _mm256_permute2f128_pd(_t0_519, _t0_519, 32), 15), _t0_521)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_519, _t0_519, 49), _mm256_permute2f128_pd(_t0_519, _t0_519, 49), 0), _t0_522), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_519, _t0_519, 49), _mm256_permute2f128_pd(_t0_519, _t0_519, 49), 15), _t0_523)));

  // 4-BLAC: 1x4 - 1x4
  _t0_111 = _mm256_sub_pd(_t0_111, _t0_101);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_524 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_525 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_526 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t0_527 = _mm256_add_pd(_t0_525, _t0_526);

  // 4-BLAC: 1x4 / 1x4
  _t0_528 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_524), _mm256_castpd256_pd128(_t0_527)));

  // AVX Storer:
  _t0_80 = _t0_528;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_529 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_530 = _t0_80;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_531 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_532 = _mm256_mul_pd(_t0_530, _t0_531);

  // 4-BLAC: 1x4 - 1x4
  _t0_533 = _mm256_sub_pd(_t0_529, _t0_532);

  // AVX Storer:
  _t0_81 = _t0_533;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_534 = _t0_81;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_535 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_536 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t0_537 = _mm256_add_pd(_t0_535, _t0_536);

  // 4-BLAC: 1x4 / 1x4
  _t0_538 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_534), _mm256_castpd256_pd128(_t0_537)));

  // AVX Storer:
  _t0_81 = _t0_538;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 2), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_539 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_540 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_80, _t0_81), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_541 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t0_542 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_540, _t0_541), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_540, _t0_541), _mm256_mul_pd(_t0_540, _t0_541), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_540, _t0_541), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_540, _t0_541), _mm256_mul_pd(_t0_540, _t0_541), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_540, _t0_541), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_540, _t0_541), _mm256_mul_pd(_t0_540, _t0_541), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_543 = _mm256_sub_pd(_t0_539, _t0_542);

  // AVX Storer:
  _t0_82 = _t0_543;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_544 = _t0_82;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_545 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_546 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t0_547 = _mm256_add_pd(_t0_545, _t0_546);

  // 4-BLAC: 1x4 / 1x4
  _t0_548 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_544), _mm256_castpd256_pd128(_t0_547)));

  // AVX Storer:
  _t0_82 = _t0_548;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 2), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_549 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_111, _t0_111, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_550 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_80, _t0_81), _mm256_unpacklo_pd(_t0_82, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_551 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t0_552 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_550, _t0_551), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_550, _t0_551), _mm256_mul_pd(_t0_550, _t0_551), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_550, _t0_551), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_550, _t0_551), _mm256_mul_pd(_t0_550, _t0_551), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_550, _t0_551), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_550, _t0_551), _mm256_mul_pd(_t0_550, _t0_551), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_553 = _mm256_sub_pd(_t0_549, _t0_552);

  // AVX Storer:
  _t0_83 = _t0_553;

  // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_554 = _t0_83;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_555 = _t0_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_556 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t0_557 = _mm256_add_pd(_t0_555, _t0_556);

  // 4-BLAC: 1x4 / 1x4
  _t0_558 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_554), _mm256_castpd256_pd128(_t0_557)));

  // AVX Storer:
  _t0_83 = _t0_558;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(4, 52, fi879)) - ( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) * G(h(3, 52, 0), X[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_559 = _t0_27;

  // AVX Loader:

  // 3x4 -> 4x4
  _t0_560 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_72, _t0_73), _mm256_unpacklo_pd(_t0_74, _t0_75), 32);
  _t0_561 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_76, _t0_77), _mm256_unpacklo_pd(_t0_78, _t0_79), 32);
  _t0_562 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_80, _t0_81), _mm256_unpacklo_pd(_t0_82, _t0_83), 32);
  _t0_563 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t0_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 32), _mm256_permute2f128_pd(_t0_559, _t0_559, 32), 0), _t0_560), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 32), _mm256_permute2f128_pd(_t0_559, _t0_559, 32), 15), _t0_561)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 49), _mm256_permute2f128_pd(_t0_559, _t0_559, 49), 0), _t0_562), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_559, _t0_559, 49), _mm256_permute2f128_pd(_t0_559, _t0_559, 49), 15), _t0_563)));

  // 4-BLAC: 1x4 - 1x4
  _t0_112 = _mm256_sub_pd(_t0_112, _t0_102);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_564 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_565 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_566 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t0_567 = _mm256_add_pd(_t0_565, _t0_566);

  // 4-BLAC: 1x4 / 1x4
  _t0_568 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_564), _mm256_castpd256_pd128(_t0_567)));

  // AVX Storer:
  _t0_84 = _t0_568;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_569 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_570 = _t0_84;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_571 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t0_572 = _mm256_mul_pd(_t0_570, _t0_571);

  // 4-BLAC: 1x4 - 1x4
  _t0_120 = _mm256_sub_pd(_t0_569, _t0_572);

  // AVX Storer:
  _t0_85 = _t0_120;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_121 = _t0_85;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_122 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_123 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t0_124 = _mm256_add_pd(_t0_122, _t0_123);

  // 4-BLAC: 1x4 / 1x4
  _t0_125 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_121), _mm256_castpd256_pd128(_t0_124)));

  // AVX Storer:
  _t0_85 = _t0_125;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 3), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_126 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t0_127 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_84, _t0_85), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t0_128 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t0_129 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_127, _t0_128), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_127, _t0_128), _mm256_mul_pd(_t0_127, _t0_128), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_127, _t0_128), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_127, _t0_128), _mm256_mul_pd(_t0_127, _t0_128), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_127, _t0_128), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_127, _t0_128), _mm256_mul_pd(_t0_127, _t0_128), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_130 = _mm256_sub_pd(_t0_126, _t0_129);

  // AVX Storer:
  _t0_86 = _t0_130;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_131 = _t0_86;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_132 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_133 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t0_134 = _mm256_add_pd(_t0_132, _t0_133);

  // 4-BLAC: 1x4 / 1x4
  _t0_135 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_131), _mm256_castpd256_pd128(_t0_134)));

  // AVX Storer:
  _t0_86 = _t0_135;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 3), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_136 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_112, _t0_112, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t0_137 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_84, _t0_85), _mm256_unpacklo_pd(_t0_86, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t0_138 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t0_140 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_137, _t0_138), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_137, _t0_138), _mm256_mul_pd(_t0_137, _t0_138), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_137, _t0_138), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_137, _t0_138), _mm256_mul_pd(_t0_137, _t0_138), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_137, _t0_138), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_137, _t0_138), _mm256_mul_pd(_t0_137, _t0_138), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t0_141 = _mm256_sub_pd(_t0_136, _t0_140);

  // AVX Storer:
  _t0_87 = _t0_141;

  // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_142 = _t0_87;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_143 = _t0_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t0_144 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t0_146 = _mm256_add_pd(_t0_143, _t0_144);

  // 4-BLAC: 1x4 / 1x4
  _t0_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_142), _mm256_castpd256_pd128(_t0_146)));

  // AVX Storer:
  _t0_87 = _t0_147;

  _asm256_storeu_pd(C + 52, _t0_113);
  _asm256_storeu_pd(C + 104, _t0_114);
  _asm256_storeu_pd(C + 156, _t0_115);
  _mm_store_sd(&(C[4]), _mm256_castpd256_pd128(_t0_56));
  _mm_store_sd(&(C[5]), _mm256_castpd256_pd128(_t0_57));
  _mm_store_sd(&(C[6]), _mm256_castpd256_pd128(_t0_58));
  _mm_store_sd(&(C[7]), _mm256_castpd256_pd128(_t0_59));
  _mm_store_sd(&(C[56]), _mm256_castpd256_pd128(_t0_60));
  _mm_store_sd(&(C[57]), _mm256_castpd256_pd128(_t0_61));
  _mm_store_sd(&(C[58]), _mm256_castpd256_pd128(_t0_62));
  _mm_store_sd(&(C[59]), _mm256_castpd256_pd128(_t0_63));
  _mm_store_sd(&(C[108]), _mm256_castpd256_pd128(_t0_64));
  _mm_store_sd(&(C[109]), _mm256_castpd256_pd128(_t0_65));
  _mm_store_sd(&(C[110]), _mm256_castpd256_pd128(_t0_66));
  _mm_store_sd(&(C[111]), _mm256_castpd256_pd128(_t0_67));
  _mm_store_sd(&(C[160]), _mm256_castpd256_pd128(_t0_68));
  _mm_store_sd(&(C[161]), _mm256_castpd256_pd128(_t0_69));
  _mm_store_sd(&(C[162]), _mm256_castpd256_pd128(_t0_70));
  _mm_store_sd(&(C[163]), _mm256_castpd256_pd128(_t0_71));
  _mm_store_sd(&(C[8]), _mm256_castpd256_pd128(_t0_72));
  _mm_store_sd(&(C[9]), _mm256_castpd256_pd128(_t0_73));
  _mm_store_sd(&(C[10]), _mm256_castpd256_pd128(_t0_74));
  _mm_store_sd(&(C[11]), _mm256_castpd256_pd128(_t0_75));
  _mm_store_sd(&(C[60]), _mm256_castpd256_pd128(_t0_76));
  _mm_store_sd(&(C[61]), _mm256_castpd256_pd128(_t0_77));
  _mm_store_sd(&(C[62]), _mm256_castpd256_pd128(_t0_78));
  _mm_store_sd(&(C[63]), _mm256_castpd256_pd128(_t0_79));
  _mm_store_sd(&(C[112]), _mm256_castpd256_pd128(_t0_80));
  _mm_store_sd(&(C[113]), _mm256_castpd256_pd128(_t0_81));
  _mm_store_sd(&(C[114]), _mm256_castpd256_pd128(_t0_82));
  _mm_store_sd(&(C[115]), _mm256_castpd256_pd128(_t0_83));
  _mm_store_sd(&(C[164]), _mm256_castpd256_pd128(_t0_84));
  _mm_store_sd(&(C[165]), _mm256_castpd256_pd128(_t0_85));
  _mm_store_sd(&(C[166]), _mm256_castpd256_pd128(_t0_86));
  _mm_store_sd(&(C[167]), _mm256_castpd256_pd128(_t0_87));

  for( int fi879 = 12; fi879 <= 48; fi879+=4 ) {
    _t1_4 = _asm256_loadu_pd(C + fi879);
    _t1_5 = _asm256_loadu_pd(C + fi879 + 52);
    _t1_6 = _asm256_loadu_pd(C + fi879 + 104);
    _t1_7 = _asm256_loadu_pd(C + fi879 + 156);
    _t1_3 = _asm256_loadu_pd(U + fi879);
    _t1_2 = _asm256_loadu_pd(U + fi879 + 52);
    _t1_1 = _asm256_loadu_pd(U + fi879 + 104);
    _t1_0 = _asm256_loadu_pd(U + fi879 + 156);

    // Generating : X[52,52] = ( S(h(4, 52, 0), ( G(h(4, 52, 0), X[52,52],h(4, 52, fi879)) - ( G(h(4, 52, 0), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879)) + Sum_{k116} ( -$(h(4, 52, 0), ( G(h(4, 52, 0), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi879)) ),h(4, 52, fi879)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t0_93 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_40, _t0_40, 32), _mm256_permute2f128_pd(_t0_40, _t0_40, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_42, _t0_42, 32), _mm256_permute2f128_pd(_t0_42, _t0_42, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0), _t1_0)));
    _t0_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_44, _t0_44, 32), _mm256_permute2f128_pd(_t0_44, _t0_44, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_45, _t0_45, 32), _mm256_permute2f128_pd(_t0_45, _t0_45, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_46, _t0_46, 32), _mm256_permute2f128_pd(_t0_46, _t0_46, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_47, _t0_47, 32), _mm256_permute2f128_pd(_t0_47, _t0_47, 32), 0), _t1_0)));
    _t0_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_50, _t0_50, 32), _mm256_permute2f128_pd(_t0_50, _t0_50, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t1_0)));
    _t0_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_52, _t0_52, 32), _mm256_permute2f128_pd(_t0_52, _t0_52, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t1_0)));

    // 4-BLAC: 4x4 - 4x4
    _t1_4 = _mm256_sub_pd(_t1_4, _t0_93);
    _t1_5 = _mm256_sub_pd(_t1_5, _t0_94);
    _t1_6 = _mm256_sub_pd(_t1_6, _t0_95);
    _t1_7 = _mm256_sub_pd(_t1_7, _t0_96);

    // AVX Storer:
    _asm256_storeu_pd(C + fi879, _t1_4);
    _asm256_storeu_pd(C + fi879 + 52, _t1_5);
    _asm256_storeu_pd(C + fi879 + 104, _t1_6);
    _asm256_storeu_pd(C + fi879 + 156, _t1_7);

    for( int k116 = 4; k116 <= fi879 - 1; k116+=4 ) {
      _t2_19 = _mm256_broadcast_sd(C + k116);
      _t2_18 = _mm256_broadcast_sd(C + k116 + 1);
      _t2_17 = _mm256_broadcast_sd(C + k116 + 2);
      _t2_16 = _mm256_broadcast_sd(C + k116 + 3);
      _t2_15 = _mm256_broadcast_sd(C + k116 + 52);
      _t2_14 = _mm256_broadcast_sd(C + k116 + 53);
      _t2_13 = _mm256_broadcast_sd(C + k116 + 54);
      _t2_12 = _mm256_broadcast_sd(C + k116 + 55);
      _t2_11 = _mm256_broadcast_sd(C + k116 + 104);
      _t2_10 = _mm256_broadcast_sd(C + k116 + 105);
      _t2_9 = _mm256_broadcast_sd(C + k116 + 106);
      _t2_8 = _mm256_broadcast_sd(C + k116 + 107);
      _t2_7 = _mm256_broadcast_sd(C + k116 + 156);
      _t2_6 = _mm256_broadcast_sd(C + k116 + 157);
      _t2_5 = _mm256_broadcast_sd(C + k116 + 158);
      _t2_4 = _mm256_broadcast_sd(C + k116 + 159);
      _t2_3 = _asm256_loadu_pd(U + fi879 + 52*k116);
      _t2_2 = _asm256_loadu_pd(U + fi879 + 52*k116 + 52);
      _t2_1 = _asm256_loadu_pd(U + fi879 + 52*k116 + 104);
      _t2_0 = _asm256_loadu_pd(U + fi879 + 52*k116 + 156);
      _t2_20 = _asm256_loadu_pd(C + fi879);
      _t2_21 = _asm256_loadu_pd(C + fi879 + 52);
      _t2_22 = _asm256_loadu_pd(C + fi879 + 104);
      _t2_23 = _asm256_loadu_pd(C + fi879 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t0_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_19, _t2_3), _mm256_mul_pd(_t2_18, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_17, _t2_1), _mm256_mul_pd(_t2_16, _t2_0)));
      _t0_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_15, _t2_3), _mm256_mul_pd(_t2_14, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_13, _t2_1), _mm256_mul_pd(_t2_12, _t2_0)));
      _t0_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_11, _t2_3), _mm256_mul_pd(_t2_10, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_9, _t2_1), _mm256_mul_pd(_t2_8, _t2_0)));
      _t0_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_7, _t2_3), _mm256_mul_pd(_t2_6, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_5, _t2_1), _mm256_mul_pd(_t2_4, _t2_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t2_20 = _mm256_sub_pd(_t2_20, _t0_97);
      _t2_21 = _mm256_sub_pd(_t2_21, _t0_98);
      _t2_22 = _mm256_sub_pd(_t2_22, _t0_99);
      _t2_23 = _mm256_sub_pd(_t2_23, _t0_100);

      // AVX Storer:
      _asm256_storeu_pd(C + fi879, _t2_20);
      _asm256_storeu_pd(C + fi879 + 52, _t2_21);
      _asm256_storeu_pd(C + fi879 + 104, _t2_22);
      _asm256_storeu_pd(C + fi879 + 156, _t2_23);
    }
    _t1_6 = _asm256_loadu_pd(C + fi879 + 104);
    _t1_7 = _asm256_loadu_pd(C + fi879 + 156);
    _t1_5 = _asm256_loadu_pd(C + fi879 + 52);
    _t1_4 = _asm256_loadu_pd(C + fi879);
    _t3_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi879])));
    _t3_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi879 + 1])));
    _t3_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi879 + 53])));
    _t3_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi879 + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi879 + 54)), 0);
    _t3_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi879 + 106])));
    _t3_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi879 + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi879 + 55))), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi879 + 107)), 32);
    _t3_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi879 + 159])));

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_24 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_25 = _t3_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_451 = _mm256_add_pd(_t3_24, _t3_25);

    // 4-BLAC: 1x4 / 1x4
    _t3_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_23), _mm256_castpd256_pd128(_t0_451)));

    // AVX Storer:
    _t3_7 = _t3_26;

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_28 = _t3_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_29 = _t3_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_456 = _mm256_mul_pd(_t3_28, _t3_29);

    // 4-BLAC: 1x4 - 1x4
    _t3_30 = _mm256_sub_pd(_t3_27, _t0_456);

    // AVX Storer:
    _t3_8 = _t3_30;

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_31 = _t3_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_32 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_33 = _t3_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_461 = _mm256_add_pd(_t3_32, _t3_33);

    // 4-BLAC: 1x4 / 1x4
    _t3_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_31), _mm256_castpd256_pd128(_t0_461)));

    // AVX Storer:
    _t3_8 = _t3_34;

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 0), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_37 = _t3_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_466 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_mul_pd(_t3_36, _t3_37), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_mul_pd(_t3_36, _t3_37), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_mul_pd(_t3_36, _t3_37), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_38 = _mm256_sub_pd(_t3_35, _t0_466);

    // AVX Storer:
    _t3_9 = _t3_38;

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_39 = _t3_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_40 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_41 = _t3_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_471 = _mm256_add_pd(_t3_40, _t3_41);

    // 4-BLAC: 1x4 / 1x4
    _t3_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_39), _mm256_castpd256_pd128(_t0_471)));

    // AVX Storer:
    _t3_9 = _t3_42;

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 0), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_4, _t1_4, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_45 = _t3_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_476 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_mul_pd(_t3_44, _t3_45), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_mul_pd(_t3_44, _t3_45), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_mul_pd(_t3_44, _t3_45), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_46 = _mm256_sub_pd(_t3_43, _t0_476);

    // AVX Storer:
    _t3_10 = _t3_46;

    // Generating : X[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 0), L[52,52],h(1, 52, 0)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_47 = _t3_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_48 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_49 = _t3_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_481 = _mm256_add_pd(_t3_48, _t3_49);

    // 4-BLAC: 1x4 / 1x4
    _t3_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_47), _mm256_castpd256_pd128(_t0_481)));

    // AVX Storer:
    _t3_10 = _t3_50;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(4, 52, fi879)) - ( G(h(1, 52, 1), L[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), X[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_51 = _t0_31;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t0_88 = _mm256_mul_pd(_t3_51, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32));

    // 4-BLAC: 1x4 - 1x4
    _t1_5 = _mm256_sub_pd(_t1_5, _t0_88);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_52 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_53 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_54 = _t3_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_487 = _mm256_add_pd(_t3_53, _t3_54);

    // 4-BLAC: 1x4 / 1x4
    _t3_55 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_52), _mm256_castpd256_pd128(_t0_487)));

    // AVX Storer:
    _t3_11 = _t3_55;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_56 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_57 = _t3_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_58 = _t3_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_492 = _mm256_mul_pd(_t3_57, _t3_58);

    // 4-BLAC: 1x4 - 1x4
    _t3_59 = _mm256_sub_pd(_t3_56, _t0_492);

    // AVX Storer:
    _t3_12 = _t3_59;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_60 = _t3_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_61 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_62 = _t3_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_497 = _mm256_add_pd(_t3_61, _t3_62);

    // 4-BLAC: 1x4 / 1x4
    _t3_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_60), _mm256_castpd256_pd128(_t0_497)));

    // AVX Storer:
    _t3_12 = _t3_63;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 1), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_64 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_65 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_66 = _t3_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_502 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_65, _t3_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_65, _t3_66), _mm256_mul_pd(_t3_65, _t3_66), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_65, _t3_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_65, _t3_66), _mm256_mul_pd(_t3_65, _t3_66), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_65, _t3_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_65, _t3_66), _mm256_mul_pd(_t3_65, _t3_66), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_67 = _mm256_sub_pd(_t3_64, _t0_502);

    // AVX Storer:
    _t3_13 = _t3_67;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_68 = _t3_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_69 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_70 = _t3_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_507 = _mm256_add_pd(_t3_69, _t3_70);

    // 4-BLAC: 1x4 / 1x4
    _t3_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_68), _mm256_castpd256_pd128(_t0_507)));

    // AVX Storer:
    _t3_13 = _t3_71;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 1), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_72 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_5, _t1_5, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_73 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_74 = _t3_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_512 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_mul_pd(_t3_73, _t3_74), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_mul_pd(_t3_73, _t3_74), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_mul_pd(_t3_73, _t3_74), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_75 = _mm256_sub_pd(_t3_72, _t0_512);

    // AVX Storer:
    _t3_14 = _t3_75;

    // Generating : X[52,52] = S(h(1, 52, 1), ( G(h(1, 52, 1), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 1), L[52,52],h(1, 52, 1)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_76 = _t3_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_77 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_78 = _t3_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_517 = _mm256_add_pd(_t3_77, _t3_78);

    // 4-BLAC: 1x4 / 1x4
    _t3_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_76), _mm256_castpd256_pd128(_t0_517)));

    // AVX Storer:
    _t3_14 = _t3_79;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(4, 52, fi879)) - ( G(h(1, 52, 2), L[52,52],h(2, 52, 0)) * G(h(2, 52, 0), X[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_80 = _t0_29;

    // AVX Loader:

    // 2x4 -> 4x4
    _t3_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32);
    _t3_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _t3_14), 32);
    _t3_83 = _mm256_setzero_pd();
    _t3_84 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t0_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_80, _t3_80, 32), _mm256_permute2f128_pd(_t3_80, _t3_80, 32), 0), _t3_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_80, _t3_80, 32), _mm256_permute2f128_pd(_t3_80, _t3_80, 32), 15), _t3_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_80, _t3_80, 49), _mm256_permute2f128_pd(_t3_80, _t3_80, 49), 0), _t3_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_80, _t3_80, 49), _mm256_permute2f128_pd(_t3_80, _t3_80, 49), 15), _t3_84)));

    // 4-BLAC: 1x4 - 1x4
    _t1_6 = _mm256_sub_pd(_t1_6, _t0_101);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_85 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_86 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_87 = _t3_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_527 = _mm256_add_pd(_t3_86, _t3_87);

    // 4-BLAC: 1x4 / 1x4
    _t3_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_85), _mm256_castpd256_pd128(_t0_527)));

    // AVX Storer:
    _t3_15 = _t3_88;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_90 = _t3_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_91 = _t3_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_532 = _mm256_mul_pd(_t3_90, _t3_91);

    // 4-BLAC: 1x4 - 1x4
    _t3_92 = _mm256_sub_pd(_t3_89, _t0_532);

    // AVX Storer:
    _t3_16 = _t3_92;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_93 = _t3_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_94 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_95 = _t3_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_537 = _mm256_add_pd(_t3_94, _t3_95);

    // 4-BLAC: 1x4 / 1x4
    _t3_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_93), _mm256_castpd256_pd128(_t0_537)));

    // AVX Storer:
    _t3_16 = _t3_96;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 2), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_97 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_98 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_15, _t3_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_99 = _t3_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_542 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_98, _t3_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_98, _t3_99), _mm256_mul_pd(_t3_98, _t3_99), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_98, _t3_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_98, _t3_99), _mm256_mul_pd(_t3_98, _t3_99), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_98, _t3_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_98, _t3_99), _mm256_mul_pd(_t3_98, _t3_99), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_100 = _mm256_sub_pd(_t3_97, _t0_542);

    // AVX Storer:
    _t3_17 = _t3_100;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_101 = _t3_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_102 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_103 = _t3_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_547 = _mm256_add_pd(_t3_102, _t3_103);

    // 4-BLAC: 1x4 / 1x4
    _t3_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_101), _mm256_castpd256_pd128(_t0_547)));

    // AVX Storer:
    _t3_17 = _t3_104;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 2), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_6, _t1_6, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_16), _mm256_unpacklo_pd(_t3_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_107 = _t3_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_552 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_106, _t3_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_106, _t3_107), _mm256_mul_pd(_t3_106, _t3_107), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_106, _t3_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_106, _t3_107), _mm256_mul_pd(_t3_106, _t3_107), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_106, _t3_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_106, _t3_107), _mm256_mul_pd(_t3_106, _t3_107), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_108 = _mm256_sub_pd(_t3_105, _t0_552);

    // AVX Storer:
    _t3_18 = _t3_108;

    // Generating : X[52,52] = S(h(1, 52, 2), ( G(h(1, 52, 2), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 2), L[52,52],h(1, 52, 2)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_109 = _t3_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_110 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_111 = _t3_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_557 = _mm256_add_pd(_t3_110, _t3_111);

    // 4-BLAC: 1x4 / 1x4
    _t3_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_109), _mm256_castpd256_pd128(_t0_557)));

    // AVX Storer:
    _t3_18 = _t3_112;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(4, 52, fi879)) - ( G(h(1, 52, 3), L[52,52],h(3, 52, 0)) * G(h(3, 52, 0), X[52,52],h(4, 52, fi879)) ) ),h(4, 52, fi879))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_113 = _t0_27;

    // AVX Loader:

    // 3x4 -> 4x4
    _t3_114 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32);
    _t3_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _t3_14), 32);
    _t3_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_16), _mm256_unpacklo_pd(_t3_17, _t3_18), 32);
    _t3_117 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t0_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_113, _t3_113, 32), _mm256_permute2f128_pd(_t3_113, _t3_113, 32), 0), _t3_114), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_113, _t3_113, 32), _mm256_permute2f128_pd(_t3_113, _t3_113, 32), 15), _t3_115)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_113, _t3_113, 49), _mm256_permute2f128_pd(_t3_113, _t3_113, 49), 0), _t3_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_113, _t3_113, 49), _mm256_permute2f128_pd(_t3_113, _t3_113, 49), 15), _t3_117)));

    // 4-BLAC: 1x4 - 1x4
    _t1_7 = _mm256_sub_pd(_t1_7, _t0_102);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879), U[52,52],h(1, 52, fi879)) ) ),h(1, 52, fi879))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_118 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_119 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_120 = _t3_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_567 = _mm256_add_pd(_t3_119, _t3_120);

    // 4-BLAC: 1x4 / 1x4
    _t3_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_118), _mm256_castpd256_pd128(_t0_567)));

    // AVX Storer:
    _t3_19 = _t3_121;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) - ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_122 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_123 = _t3_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_124 = _t3_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_572 = _mm256_mul_pd(_t3_123, _t3_124);

    // 4-BLAC: 1x4 - 1x4
    _t3_125 = _mm256_sub_pd(_t3_122, _t0_572);

    // AVX Storer:
    _t3_20 = _t3_125;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 1)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879 + 1), U[52,52],h(1, 52, fi879 + 1)) ) ),h(1, 52, fi879 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_126 = _t3_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_127 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_128 = _t3_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_124 = _mm256_add_pd(_t3_127, _t3_128);

    // 4-BLAC: 1x4 / 1x4
    _t3_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_126), _mm256_castpd256_pd128(_t0_124)));

    // AVX Storer:
    _t3_20 = _t3_129;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) - ( G(h(1, 52, 3), X[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_130 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t3_131 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_19, _t3_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t3_132 = _t3_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_129 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_131, _t3_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_131, _t3_132), _mm256_mul_pd(_t3_131, _t3_132), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_131, _t3_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_131, _t3_132), _mm256_mul_pd(_t3_131, _t3_132), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_131, _t3_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_131, _t3_132), _mm256_mul_pd(_t3_131, _t3_132), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_133 = _mm256_sub_pd(_t3_130, _t0_129);

    // AVX Storer:
    _t3_21 = _t3_133;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 2)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879 + 2), U[52,52],h(1, 52, fi879 + 2)) ) ),h(1, 52, fi879 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_134 = _t3_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_135 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_136 = _t3_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_134 = _mm256_add_pd(_t3_135, _t3_136);

    // 4-BLAC: 1x4 / 1x4
    _t3_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_134), _mm256_castpd256_pd128(_t0_134)));

    // AVX Storer:
    _t3_21 = _t3_137;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) - ( G(h(1, 52, 3), X[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_138 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_7, _t1_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t3_139 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_19, _t3_20), _mm256_unpacklo_pd(_t3_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t3_140 = _t3_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_140 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_mul_pd(_t3_139, _t3_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_mul_pd(_t3_139, _t3_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_mul_pd(_t3_139, _t3_140), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t3_141 = _mm256_sub_pd(_t3_138, _t0_140);

    // AVX Storer:
    _t3_22 = _t3_141;

    // Generating : X[52,52] = S(h(1, 52, 3), ( G(h(1, 52, 3), X[52,52],h(1, 52, fi879 + 3)) Div ( G(h(1, 52, 3), L[52,52],h(1, 52, 3)) + G(h(1, 52, fi879 + 3), U[52,52],h(1, 52, fi879 + 3)) ) ),h(1, 52, fi879 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_142 = _t3_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_143 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t3_144 = _t3_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_146 = _mm256_add_pd(_t3_143, _t3_144);

    // 4-BLAC: 1x4 / 1x4
    _t3_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_142), _mm256_castpd256_pd128(_t0_146)));

    // AVX Storer:
    _t3_22 = _t3_145;
    _mm_store_sd(&(C[fi879]), _mm256_castpd256_pd128(_t3_7));
    _mm_store_sd(&(C[fi879 + 1]), _mm256_castpd256_pd128(_t3_8));
    _mm_store_sd(&(C[fi879 + 2]), _mm256_castpd256_pd128(_t3_9));
    _mm_store_sd(&(C[fi879 + 3]), _mm256_castpd256_pd128(_t3_10));
    _mm_store_sd(&(C[fi879 + 52]), _mm256_castpd256_pd128(_t3_11));
    _mm_store_sd(&(C[fi879 + 53]), _mm256_castpd256_pd128(_t3_12));
    _mm_store_sd(&(C[fi879 + 54]), _mm256_castpd256_pd128(_t3_13));
    _mm_store_sd(&(C[fi879 + 55]), _mm256_castpd256_pd128(_t3_14));
    _mm_store_sd(&(C[fi879 + 104]), _mm256_castpd256_pd128(_t3_15));
    _mm_store_sd(&(C[fi879 + 105]), _mm256_castpd256_pd128(_t3_16));
    _mm_store_sd(&(C[fi879 + 106]), _mm256_castpd256_pd128(_t3_17));
    _mm_store_sd(&(C[fi879 + 107]), _mm256_castpd256_pd128(_t3_18));
    _mm_store_sd(&(C[fi879 + 156]), _mm256_castpd256_pd128(_t3_19));
    _mm_store_sd(&(C[fi879 + 157]), _mm256_castpd256_pd128(_t3_20));
    _mm_store_sd(&(C[fi879 + 158]), _mm256_castpd256_pd128(_t3_21));
    _mm_store_sd(&(C[fi879 + 159]), _mm256_castpd256_pd128(_t3_22));
  }


  // Generating : X[52,52] = Sum_{k233} ( S(h(4, 52, 4), ( G(h(4, 52, 4), C[52,52],h(4, 52, k233)) - ( G(h(4, 52, 4), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, k233)) ) ),h(4, 52, k233)) )

  // AVX Loader:

  _mm_store_sd(&(C[0]), _mm256_castpd256_pd128(_t0_40));
  _mm_store_sd(&(C[1]), _mm256_castpd256_pd128(_t0_41));
  _mm_store_sd(&(C[2]), _mm256_castpd256_pd128(_t0_42));
  _mm_store_sd(&(C[3]), _mm256_castpd256_pd128(_t0_43));
  _mm_store_sd(&(C[52]), _mm256_castpd256_pd128(_t0_44));
  _mm_store_sd(&(C[53]), _mm256_castpd256_pd128(_t0_45));
  _mm_store_sd(&(C[54]), _mm256_castpd256_pd128(_t0_46));
  _mm_store_sd(&(C[55]), _mm256_castpd256_pd128(_t0_47));
  _mm_store_sd(&(C[104]), _mm256_castpd256_pd128(_t0_48));
  _mm_store_sd(&(C[105]), _mm256_castpd256_pd128(_t0_49));
  _mm_store_sd(&(C[106]), _mm256_castpd256_pd128(_t0_50));
  _mm_store_sd(&(C[107]), _mm256_castpd256_pd128(_t0_51));
  _mm_store_sd(&(C[156]), _mm256_castpd256_pd128(_t0_52));
  _mm_store_sd(&(C[157]), _mm256_castpd256_pd128(_t0_53));
  _mm_store_sd(&(C[158]), _mm256_castpd256_pd128(_t0_54));
  _mm_store_sd(&(C[159]), _mm256_castpd256_pd128(_t0_55));

  for( int k233 = 0; k233 <= 51; k233+=4 ) {
    _t4_24 = _asm256_loadu_pd(C + k233 + 208);
    _t4_25 = _asm256_loadu_pd(C + k233 + 260);
    _t4_26 = _asm256_loadu_pd(C + k233 + 312);
    _t4_27 = _asm256_loadu_pd(C + k233 + 364);
    _t4_19 = _mm256_broadcast_sd(L + 208);
    _t4_18 = _mm256_broadcast_sd(L + 209);
    _t4_17 = _mm256_broadcast_sd(L + 210);
    _t4_16 = _mm256_broadcast_sd(L + 211);
    _t4_15 = _mm256_broadcast_sd(L + 260);
    _t4_14 = _mm256_broadcast_sd(L + 261);
    _t4_13 = _mm256_broadcast_sd(L + 262);
    _t4_12 = _mm256_broadcast_sd(L + 263);
    _t4_11 = _mm256_broadcast_sd(L + 312);
    _t4_10 = _mm256_broadcast_sd(L + 313);
    _t4_9 = _mm256_broadcast_sd(L + 314);
    _t4_8 = _mm256_broadcast_sd(L + 315);
    _t4_7 = _mm256_broadcast_sd(L + 364);
    _t4_6 = _mm256_broadcast_sd(L + 365);
    _t4_5 = _mm256_broadcast_sd(L + 366);
    _t4_4 = _mm256_broadcast_sd(L + 367);
    _t4_3 = _asm256_loadu_pd(C + k233);
    _t4_2 = _asm256_loadu_pd(C + k233 + 52);
    _t4_1 = _asm256_loadu_pd(C + k233 + 104);
    _t4_0 = _asm256_loadu_pd(C + k233 + 156);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_19, _t4_3), _mm256_mul_pd(_t4_18, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_17, _t4_1), _mm256_mul_pd(_t4_16, _t4_0)));
    _t4_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t4_3), _mm256_mul_pd(_t4_14, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t4_1), _mm256_mul_pd(_t4_12, _t4_0)));
    _t4_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t4_3), _mm256_mul_pd(_t4_10, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t4_1), _mm256_mul_pd(_t4_8, _t4_0)));
    _t4_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t4_3), _mm256_mul_pd(_t4_6, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t4_1), _mm256_mul_pd(_t4_4, _t4_0)));

    // 4-BLAC: 4x4 - 4x4
    _t4_24 = _mm256_sub_pd(_t4_24, _t4_20);
    _t4_25 = _mm256_sub_pd(_t4_25, _t4_21);
    _t4_26 = _mm256_sub_pd(_t4_26, _t4_22);
    _t4_27 = _mm256_sub_pd(_t4_27, _t4_23);

    // AVX Storer:
    _asm256_storeu_pd(C + k233 + 208, _t4_24);
    _asm256_storeu_pd(C + k233 + 260, _t4_25);
    _asm256_storeu_pd(C + k233 + 312, _t4_26);
    _asm256_storeu_pd(C + k233 + 364, _t4_27);
  }

  _t5_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[208])));
  _t5_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[212])));
  _t5_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[209])));
  _t5_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[210])));
  _t5_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[211])));
  _t5_76 = _asm256_loadu_pd(C + 260);
  _t5_5 = _mm256_broadcast_sd(&(L[264]));
  _t5_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[265])));
  _t5_77 = _asm256_loadu_pd(C + 312);
  _t5_3 = _mm256_maskload_pd(L + 316, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t5_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[318])));
  _t5_78 = _asm256_loadu_pd(C + 364);
  _t5_1 = _mm256_maskload_pd(L + 368, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t5_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[371])));
  _t5_79 = _asm256_loadu_pd(C + 212);
  _t5_80 = _asm256_loadu_pd(C + 264);
  _t5_81 = _asm256_loadu_pd(C + 316);
  _t5_82 = _asm256_loadu_pd(C + 368);
  _t5_83 = _asm256_loadu_pd(C + 216);
  _t5_84 = _asm256_loadu_pd(C + 268);
  _t5_85 = _asm256_loadu_pd(C + 320);
  _t5_86 = _asm256_loadu_pd(C + 372);

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_87 = _t5_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_88 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_89 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_90 = _mm256_add_pd(_t5_88, _t5_89);

  // 4-BLAC: 1x4 / 1x4
  _t5_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_87), _mm256_castpd256_pd128(_t5_90)));

  // AVX Storer:
  _t5_7 = _t5_91;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 4), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_92 = _t5_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_93 = _t5_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_94 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_95 = _mm256_mul_pd(_t5_93, _t5_94);

  // 4-BLAC: 1x4 - 1x4
  _t5_96 = _mm256_sub_pd(_t5_92, _t5_95);

  // AVX Storer:
  _t5_8 = _t5_96;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_97 = _t5_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_98 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_99 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_100 = _mm256_add_pd(_t5_98, _t5_99);

  // 4-BLAC: 1x4 / 1x4
  _t5_101 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_97), _mm256_castpd256_pd128(_t5_100)));

  // AVX Storer:
  _t5_8 = _t5_101;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 4), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_102 = _t5_9;

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_103 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_7, _t5_8), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_104 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_105 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_103, _t5_104), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_103, _t5_104), _mm256_mul_pd(_t5_103, _t5_104), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_103, _t5_104), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_103, _t5_104), _mm256_mul_pd(_t5_103, _t5_104), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_103, _t5_104), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_103, _t5_104), _mm256_mul_pd(_t5_103, _t5_104), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_106 = _mm256_sub_pd(_t5_102, _t5_105);

  // AVX Storer:
  _t5_9 = _t5_106;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_107 = _t5_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_108 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_109 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_110 = _mm256_add_pd(_t5_108, _t5_109);

  // 4-BLAC: 1x4 / 1x4
  _t5_111 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_107), _mm256_castpd256_pd128(_t5_110)));

  // AVX Storer:
  _t5_9 = _t5_111;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 4), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_112 = _t5_10;

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_7, _t5_8), _mm256_unpacklo_pd(_t5_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_114 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_115 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_113, _t5_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_113, _t5_114), _mm256_mul_pd(_t5_113, _t5_114), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_113, _t5_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_113, _t5_114), _mm256_mul_pd(_t5_113, _t5_114), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_113, _t5_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_113, _t5_114), _mm256_mul_pd(_t5_113, _t5_114), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_116 = _mm256_sub_pd(_t5_112, _t5_115);

  // AVX Storer:
  _t5_10 = _t5_116;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_117 = _t5_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_118 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_119 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_120 = _mm256_add_pd(_t5_118, _t5_119);

  // 4-BLAC: 1x4 / 1x4
  _t5_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_117), _mm256_castpd256_pd128(_t5_120)));

  // AVX Storer:
  _t5_10 = _t5_121;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_122 = _t5_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t5_55 = _mm256_mul_pd(_t5_122, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_7, _t5_8), _mm256_unpacklo_pd(_t5_9, _t5_10), 32));

  // 4-BLAC: 1x4 - 1x4
  _t5_76 = _mm256_sub_pd(_t5_76, _t5_55);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_76, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_124 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_125 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_126 = _mm256_add_pd(_t5_124, _t5_125);

  // 4-BLAC: 1x4 / 1x4
  _t5_127 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_123), _mm256_castpd256_pd128(_t5_126)));

  // AVX Storer:
  _t5_11 = _t5_127;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 5), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_128 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_76, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_129 = _t5_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_130 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_131 = _mm256_mul_pd(_t5_129, _t5_130);

  // 4-BLAC: 1x4 - 1x4
  _t5_132 = _mm256_sub_pd(_t5_128, _t5_131);

  // AVX Storer:
  _t5_12 = _t5_132;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_133 = _t5_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_134 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_135 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_136 = _mm256_add_pd(_t5_134, _t5_135);

  // 4-BLAC: 1x4 / 1x4
  _t5_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_133), _mm256_castpd256_pd128(_t5_136)));

  // AVX Storer:
  _t5_12 = _t5_137;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 5), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_76, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_76, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_11, _t5_12), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_140 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_141 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_139, _t5_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_139, _t5_140), _mm256_mul_pd(_t5_139, _t5_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_139, _t5_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_139, _t5_140), _mm256_mul_pd(_t5_139, _t5_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_139, _t5_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_139, _t5_140), _mm256_mul_pd(_t5_139, _t5_140), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_142 = _mm256_sub_pd(_t5_138, _t5_141);

  // AVX Storer:
  _t5_13 = _t5_142;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_143 = _t5_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_144 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_145 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_146 = _mm256_add_pd(_t5_144, _t5_145);

  // 4-BLAC: 1x4 / 1x4
  _t5_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_143), _mm256_castpd256_pd128(_t5_146)));

  // AVX Storer:
  _t5_13 = _t5_147;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 5), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_148 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_76, _t5_76, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_149 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_11, _t5_12), _mm256_unpacklo_pd(_t5_13, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_150 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_151 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_149, _t5_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_149, _t5_150), _mm256_mul_pd(_t5_149, _t5_150), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_149, _t5_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_149, _t5_150), _mm256_mul_pd(_t5_149, _t5_150), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_149, _t5_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_149, _t5_150), _mm256_mul_pd(_t5_149, _t5_150), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_152 = _mm256_sub_pd(_t5_148, _t5_151);

  // AVX Storer:
  _t5_14 = _t5_152;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_153 = _t5_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_154 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_155 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_156 = _mm256_add_pd(_t5_154, _t5_155);

  // 4-BLAC: 1x4 / 1x4
  _t5_157 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_153), _mm256_castpd256_pd128(_t5_156)));

  // AVX Storer:
  _t5_14 = _t5_157;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) * G(h(2, 52, 4), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_158 = _t5_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t5_159 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_7, _t5_8), _mm256_unpacklo_pd(_t5_9, _t5_10), 32);
  _t5_160 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_11, _t5_12), _mm256_unpacklo_pd(_t5_13, _t5_14), 32);
  _t5_161 = _mm256_setzero_pd();
  _t5_162 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t5_58 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_158, _t5_158, 32), _mm256_permute2f128_pd(_t5_158, _t5_158, 32), 0), _t5_159), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_158, _t5_158, 32), _mm256_permute2f128_pd(_t5_158, _t5_158, 32), 15), _t5_160)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_158, _t5_158, 49), _mm256_permute2f128_pd(_t5_158, _t5_158, 49), 0), _t5_161), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_158, _t5_158, 49), _mm256_permute2f128_pd(_t5_158, _t5_158, 49), 15), _t5_162)));

  // 4-BLAC: 1x4 - 1x4
  _t5_77 = _mm256_sub_pd(_t5_77, _t5_58);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_163 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_77, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_164 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_165 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_166 = _mm256_add_pd(_t5_164, _t5_165);

  // 4-BLAC: 1x4 / 1x4
  _t5_167 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_163), _mm256_castpd256_pd128(_t5_166)));

  // AVX Storer:
  _t5_15 = _t5_167;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 6), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_168 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_77, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_169 = _t5_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_170 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_171 = _mm256_mul_pd(_t5_169, _t5_170);

  // 4-BLAC: 1x4 - 1x4
  _t5_172 = _mm256_sub_pd(_t5_168, _t5_171);

  // AVX Storer:
  _t5_16 = _t5_172;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_173 = _t5_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_174 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_175 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_176 = _mm256_add_pd(_t5_174, _t5_175);

  // 4-BLAC: 1x4 / 1x4
  _t5_177 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_173), _mm256_castpd256_pd128(_t5_176)));

  // AVX Storer:
  _t5_16 = _t5_177;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 6), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_178 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_77, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_77, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_179 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_15, _t5_16), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_180 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_181 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_179, _t5_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_179, _t5_180), _mm256_mul_pd(_t5_179, _t5_180), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_179, _t5_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_179, _t5_180), _mm256_mul_pd(_t5_179, _t5_180), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_179, _t5_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_179, _t5_180), _mm256_mul_pd(_t5_179, _t5_180), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_182 = _mm256_sub_pd(_t5_178, _t5_181);

  // AVX Storer:
  _t5_17 = _t5_182;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_183 = _t5_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_184 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_185 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_186 = _mm256_add_pd(_t5_184, _t5_185);

  // 4-BLAC: 1x4 / 1x4
  _t5_187 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_183), _mm256_castpd256_pd128(_t5_186)));

  // AVX Storer:
  _t5_17 = _t5_187;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 6), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_188 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_77, _t5_77, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_189 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_15, _t5_16), _mm256_unpacklo_pd(_t5_17, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_190 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_191 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_189, _t5_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_189, _t5_190), _mm256_mul_pd(_t5_189, _t5_190), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_189, _t5_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_189, _t5_190), _mm256_mul_pd(_t5_189, _t5_190), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_189, _t5_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_189, _t5_190), _mm256_mul_pd(_t5_189, _t5_190), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_192 = _mm256_sub_pd(_t5_188, _t5_191);

  // AVX Storer:
  _t5_18 = _t5_192;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_193 = _t5_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_194 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_195 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_196 = _mm256_add_pd(_t5_194, _t5_195);

  // 4-BLAC: 1x4 / 1x4
  _t5_197 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_193), _mm256_castpd256_pd128(_t5_196)));

  // AVX Storer:
  _t5_18 = _t5_197;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) * G(h(3, 52, 4), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_198 = _t5_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t5_199 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_7, _t5_8), _mm256_unpacklo_pd(_t5_9, _t5_10), 32);
  _t5_200 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_11, _t5_12), _mm256_unpacklo_pd(_t5_13, _t5_14), 32);
  _t5_201 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_15, _t5_16), _mm256_unpacklo_pd(_t5_17, _t5_18), 32);
  _t5_202 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t5_59 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_198, _t5_198, 32), _mm256_permute2f128_pd(_t5_198, _t5_198, 32), 0), _t5_199), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_198, _t5_198, 32), _mm256_permute2f128_pd(_t5_198, _t5_198, 32), 15), _t5_200)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_198, _t5_198, 49), _mm256_permute2f128_pd(_t5_198, _t5_198, 49), 0), _t5_201), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_198, _t5_198, 49), _mm256_permute2f128_pd(_t5_198, _t5_198, 49), 15), _t5_202)));

  // 4-BLAC: 1x4 - 1x4
  _t5_78 = _mm256_sub_pd(_t5_78, _t5_59);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_203 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_78, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_204 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_205 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_206 = _mm256_add_pd(_t5_204, _t5_205);

  // 4-BLAC: 1x4 / 1x4
  _t5_207 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_203), _mm256_castpd256_pd128(_t5_206)));

  // AVX Storer:
  _t5_19 = _t5_207;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, 7), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_208 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_78, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_209 = _t5_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_210 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_211 = _mm256_mul_pd(_t5_209, _t5_210);

  // 4-BLAC: 1x4 - 1x4
  _t5_212 = _mm256_sub_pd(_t5_208, _t5_211);

  // AVX Storer:
  _t5_20 = _t5_212;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_213 = _t5_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_214 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_215 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_216 = _mm256_add_pd(_t5_214, _t5_215);

  // 4-BLAC: 1x4 / 1x4
  _t5_217 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_213), _mm256_castpd256_pd128(_t5_216)));

  // AVX Storer:
  _t5_20 = _t5_217;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, 7), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_218 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_78, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_78, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_219 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_19, _t5_20), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_220 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_221 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_219, _t5_220), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_219, _t5_220), _mm256_mul_pd(_t5_219, _t5_220), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_219, _t5_220), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_219, _t5_220), _mm256_mul_pd(_t5_219, _t5_220), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_219, _t5_220), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_219, _t5_220), _mm256_mul_pd(_t5_219, _t5_220), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_222 = _mm256_sub_pd(_t5_218, _t5_221);

  // AVX Storer:
  _t5_21 = _t5_222;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_223 = _t5_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_224 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_225 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_226 = _mm256_add_pd(_t5_224, _t5_225);

  // 4-BLAC: 1x4 / 1x4
  _t5_227 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_223), _mm256_castpd256_pd128(_t5_226)));

  // AVX Storer:
  _t5_21 = _t5_227;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, 7), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_228 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_78, _t5_78, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_229 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_19, _t5_20), _mm256_unpacklo_pd(_t5_21, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_230 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_231 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_229, _t5_230), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_229, _t5_230), _mm256_mul_pd(_t5_229, _t5_230), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_229, _t5_230), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_229, _t5_230), _mm256_mul_pd(_t5_229, _t5_230), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_229, _t5_230), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_229, _t5_230), _mm256_mul_pd(_t5_229, _t5_230), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_232 = _mm256_sub_pd(_t5_228, _t5_231);

  // AVX Storer:
  _t5_22 = _t5_232;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_233 = _t5_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_234 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_235 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_236 = _mm256_add_pd(_t5_234, _t5_235);

  // 4-BLAC: 1x4 / 1x4
  _t5_237 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_233), _mm256_castpd256_pd128(_t5_236)));

  // AVX Storer:
  _t5_22 = _t5_237;

  // Generating : X[52,52] = S(h(4, 52, 4), ( G(h(4, 52, 4), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, 4), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t5_60 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_7, _t5_7, 32), _mm256_permute2f128_pd(_t5_7, _t5_7, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_8, _t5_8, 32), _mm256_permute2f128_pd(_t5_8, _t5_8, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_9, _t5_9, 32), _mm256_permute2f128_pd(_t5_9, _t5_9, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_10, _t5_10, 32), _mm256_permute2f128_pd(_t5_10, _t5_10, 32), 0), _t0_22)));
  _t5_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_11, _t5_11, 32), _mm256_permute2f128_pd(_t5_11, _t5_11, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_13, _t5_13, 32), _mm256_permute2f128_pd(_t5_13, _t5_13, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0), _t0_22)));
  _t5_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_15, _t5_15, 32), _mm256_permute2f128_pd(_t5_15, _t5_15, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_16, _t5_16, 32), _mm256_permute2f128_pd(_t5_16, _t5_16, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_17, _t5_17, 32), _mm256_permute2f128_pd(_t5_17, _t5_17, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0), _t0_22)));
  _t5_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_19, _t5_19, 32), _mm256_permute2f128_pd(_t5_19, _t5_19, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_21, _t5_21, 32), _mm256_permute2f128_pd(_t5_21, _t5_21, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_22, _t5_22, 32), _mm256_permute2f128_pd(_t5_22, _t5_22, 32), 0), _t0_22)));

  // 4-BLAC: 4x4 - 4x4
  _t5_79 = _mm256_sub_pd(_t5_79, _t5_60);
  _t5_80 = _mm256_sub_pd(_t5_80, _t5_61);
  _t5_81 = _mm256_sub_pd(_t5_81, _t5_62);
  _t5_82 = _mm256_sub_pd(_t5_82, _t5_63);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_238 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_79, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_239 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_240 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_241 = _mm256_add_pd(_t5_239, _t5_240);

  // 4-BLAC: 1x4 / 1x4
  _t5_242 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_238), _mm256_castpd256_pd128(_t5_241)));

  // AVX Storer:
  _t5_23 = _t5_242;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 4), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_243 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_79, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_244 = _t5_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_245 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_246 = _mm256_mul_pd(_t5_244, _t5_245);

  // 4-BLAC: 1x4 - 1x4
  _t5_247 = _mm256_sub_pd(_t5_243, _t5_246);

  // AVX Storer:
  _t5_24 = _t5_247;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_248 = _t5_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_249 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_250 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_251 = _mm256_add_pd(_t5_249, _t5_250);

  // 4-BLAC: 1x4 / 1x4
  _t5_252 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_248), _mm256_castpd256_pd128(_t5_251)));

  // AVX Storer:
  _t5_24 = _t5_252;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 4), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_253 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_79, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_79, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_254 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_23, _t5_24), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_255 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_256 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_254, _t5_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_254, _t5_255), _mm256_mul_pd(_t5_254, _t5_255), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_254, _t5_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_254, _t5_255), _mm256_mul_pd(_t5_254, _t5_255), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_254, _t5_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_254, _t5_255), _mm256_mul_pd(_t5_254, _t5_255), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_257 = _mm256_sub_pd(_t5_253, _t5_256);

  // AVX Storer:
  _t5_25 = _t5_257;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_258 = _t5_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_259 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_260 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_261 = _mm256_add_pd(_t5_259, _t5_260);

  // 4-BLAC: 1x4 / 1x4
  _t5_262 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_258), _mm256_castpd256_pd128(_t5_261)));

  // AVX Storer:
  _t5_25 = _t5_262;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 4), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_263 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_79, _t5_79, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_264 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_23, _t5_24), _mm256_unpacklo_pd(_t5_25, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_265 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_266 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_264, _t5_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_264, _t5_265), _mm256_mul_pd(_t5_264, _t5_265), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_264, _t5_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_264, _t5_265), _mm256_mul_pd(_t5_264, _t5_265), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_264, _t5_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_264, _t5_265), _mm256_mul_pd(_t5_264, _t5_265), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_267 = _mm256_sub_pd(_t5_263, _t5_266);

  // AVX Storer:
  _t5_26 = _t5_267;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_268 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_269 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_270 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_271 = _mm256_add_pd(_t5_269, _t5_270);

  // 4-BLAC: 1x4 / 1x4
  _t5_272 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_268), _mm256_castpd256_pd128(_t5_271)));

  // AVX Storer:
  _t5_26 = _t5_272;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_273 = _t5_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t5_56 = _mm256_mul_pd(_t5_273, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_23, _t5_24), _mm256_unpacklo_pd(_t5_25, _t5_26), 32));

  // 4-BLAC: 1x4 - 1x4
  _t5_80 = _mm256_sub_pd(_t5_80, _t5_56);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_274 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_80, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_275 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_276 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_277 = _mm256_add_pd(_t5_275, _t5_276);

  // 4-BLAC: 1x4 / 1x4
  _t5_278 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_274), _mm256_castpd256_pd128(_t5_277)));

  // AVX Storer:
  _t5_27 = _t5_278;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 5), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_279 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_80, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_280 = _t5_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_281 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_282 = _mm256_mul_pd(_t5_280, _t5_281);

  // 4-BLAC: 1x4 - 1x4
  _t5_283 = _mm256_sub_pd(_t5_279, _t5_282);

  // AVX Storer:
  _t5_28 = _t5_283;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_284 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_285 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_286 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_287 = _mm256_add_pd(_t5_285, _t5_286);

  // 4-BLAC: 1x4 / 1x4
  _t5_288 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_284), _mm256_castpd256_pd128(_t5_287)));

  // AVX Storer:
  _t5_28 = _t5_288;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 5), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_289 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_80, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_80, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_290 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_27, _t5_28), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_291 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_292 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_290, _t5_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_290, _t5_291), _mm256_mul_pd(_t5_290, _t5_291), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_290, _t5_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_290, _t5_291), _mm256_mul_pd(_t5_290, _t5_291), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_290, _t5_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_290, _t5_291), _mm256_mul_pd(_t5_290, _t5_291), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_293 = _mm256_sub_pd(_t5_289, _t5_292);

  // AVX Storer:
  _t5_29 = _t5_293;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_294 = _t5_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_295 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_296 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_297 = _mm256_add_pd(_t5_295, _t5_296);

  // 4-BLAC: 1x4 / 1x4
  _t5_298 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_294), _mm256_castpd256_pd128(_t5_297)));

  // AVX Storer:
  _t5_29 = _t5_298;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 5), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_299 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_80, _t5_80, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_300 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_27, _t5_28), _mm256_unpacklo_pd(_t5_29, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_301 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_302 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_300, _t5_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_300, _t5_301), _mm256_mul_pd(_t5_300, _t5_301), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_300, _t5_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_300, _t5_301), _mm256_mul_pd(_t5_300, _t5_301), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_300, _t5_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_300, _t5_301), _mm256_mul_pd(_t5_300, _t5_301), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_303 = _mm256_sub_pd(_t5_299, _t5_302);

  // AVX Storer:
  _t5_30 = _t5_303;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_304 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_305 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_306 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_307 = _mm256_add_pd(_t5_305, _t5_306);

  // 4-BLAC: 1x4 / 1x4
  _t5_308 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_304), _mm256_castpd256_pd128(_t5_307)));

  // AVX Storer:
  _t5_30 = _t5_308;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) * G(h(2, 52, 4), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_309 = _t5_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t5_310 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_23, _t5_24), _mm256_unpacklo_pd(_t5_25, _t5_26), 32);
  _t5_311 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_27, _t5_28), _mm256_unpacklo_pd(_t5_29, _t5_30), 32);
  _t5_312 = _mm256_setzero_pd();
  _t5_313 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t5_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_309, _t5_309, 32), _mm256_permute2f128_pd(_t5_309, _t5_309, 32), 0), _t5_310), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_309, _t5_309, 32), _mm256_permute2f128_pd(_t5_309, _t5_309, 32), 15), _t5_311)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_309, _t5_309, 49), _mm256_permute2f128_pd(_t5_309, _t5_309, 49), 0), _t5_312), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_309, _t5_309, 49), _mm256_permute2f128_pd(_t5_309, _t5_309, 49), 15), _t5_313)));

  // 4-BLAC: 1x4 - 1x4
  _t5_81 = _mm256_sub_pd(_t5_81, _t5_64);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_314 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_81, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_315 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_316 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_317 = _mm256_add_pd(_t5_315, _t5_316);

  // 4-BLAC: 1x4 / 1x4
  _t5_318 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_314), _mm256_castpd256_pd128(_t5_317)));

  // AVX Storer:
  _t5_31 = _t5_318;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 6), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_319 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_81, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_320 = _t5_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_321 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_322 = _mm256_mul_pd(_t5_320, _t5_321);

  // 4-BLAC: 1x4 - 1x4
  _t5_323 = _mm256_sub_pd(_t5_319, _t5_322);

  // AVX Storer:
  _t5_32 = _t5_323;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_324 = _t5_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_325 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_326 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_327 = _mm256_add_pd(_t5_325, _t5_326);

  // 4-BLAC: 1x4 / 1x4
  _t5_328 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_324), _mm256_castpd256_pd128(_t5_327)));

  // AVX Storer:
  _t5_32 = _t5_328;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 6), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_329 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_81, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_81, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_330 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_31, _t5_32), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_331 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_332 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_330, _t5_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_330, _t5_331), _mm256_mul_pd(_t5_330, _t5_331), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_330, _t5_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_330, _t5_331), _mm256_mul_pd(_t5_330, _t5_331), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_330, _t5_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_330, _t5_331), _mm256_mul_pd(_t5_330, _t5_331), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_333 = _mm256_sub_pd(_t5_329, _t5_332);

  // AVX Storer:
  _t5_33 = _t5_333;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_334 = _t5_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_335 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_336 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_337 = _mm256_add_pd(_t5_335, _t5_336);

  // 4-BLAC: 1x4 / 1x4
  _t5_338 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_334), _mm256_castpd256_pd128(_t5_337)));

  // AVX Storer:
  _t5_33 = _t5_338;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 6), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_339 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_81, _t5_81, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_340 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_31, _t5_32), _mm256_unpacklo_pd(_t5_33, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_341 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_342 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_340, _t5_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_340, _t5_341), _mm256_mul_pd(_t5_340, _t5_341), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_340, _t5_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_340, _t5_341), _mm256_mul_pd(_t5_340, _t5_341), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_340, _t5_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_340, _t5_341), _mm256_mul_pd(_t5_340, _t5_341), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_343 = _mm256_sub_pd(_t5_339, _t5_342);

  // AVX Storer:
  _t5_34 = _t5_343;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_344 = _t5_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_345 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_346 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_347 = _mm256_add_pd(_t5_345, _t5_346);

  // 4-BLAC: 1x4 / 1x4
  _t5_348 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_344), _mm256_castpd256_pd128(_t5_347)));

  // AVX Storer:
  _t5_34 = _t5_348;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) * G(h(3, 52, 4), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_349 = _t5_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t5_350 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_23, _t5_24), _mm256_unpacklo_pd(_t5_25, _t5_26), 32);
  _t5_351 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_27, _t5_28), _mm256_unpacklo_pd(_t5_29, _t5_30), 32);
  _t5_352 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_31, _t5_32), _mm256_unpacklo_pd(_t5_33, _t5_34), 32);
  _t5_353 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t5_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_349, _t5_349, 32), _mm256_permute2f128_pd(_t5_349, _t5_349, 32), 0), _t5_350), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_349, _t5_349, 32), _mm256_permute2f128_pd(_t5_349, _t5_349, 32), 15), _t5_351)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_349, _t5_349, 49), _mm256_permute2f128_pd(_t5_349, _t5_349, 49), 0), _t5_352), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_349, _t5_349, 49), _mm256_permute2f128_pd(_t5_349, _t5_349, 49), 15), _t5_353)));

  // 4-BLAC: 1x4 - 1x4
  _t5_82 = _mm256_sub_pd(_t5_82, _t5_65);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_354 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_82, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_355 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_356 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_357 = _mm256_add_pd(_t5_355, _t5_356);

  // 4-BLAC: 1x4 / 1x4
  _t5_358 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_354), _mm256_castpd256_pd128(_t5_357)));

  // AVX Storer:
  _t5_35 = _t5_358;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, 7), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_359 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_82, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_360 = _t5_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_361 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_362 = _mm256_mul_pd(_t5_360, _t5_361);

  // 4-BLAC: 1x4 - 1x4
  _t5_363 = _mm256_sub_pd(_t5_359, _t5_362);

  // AVX Storer:
  _t5_36 = _t5_363;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_364 = _t5_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_365 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_366 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_367 = _mm256_add_pd(_t5_365, _t5_366);

  // 4-BLAC: 1x4 / 1x4
  _t5_368 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_364), _mm256_castpd256_pd128(_t5_367)));

  // AVX Storer:
  _t5_36 = _t5_368;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, 7), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_369 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_82, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_82, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_370 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_35, _t5_36), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_371 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_372 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_370, _t5_371), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_370, _t5_371), _mm256_mul_pd(_t5_370, _t5_371), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_370, _t5_371), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_370, _t5_371), _mm256_mul_pd(_t5_370, _t5_371), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_370, _t5_371), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_370, _t5_371), _mm256_mul_pd(_t5_370, _t5_371), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_373 = _mm256_sub_pd(_t5_369, _t5_372);

  // AVX Storer:
  _t5_37 = _t5_373;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_374 = _t5_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_375 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_376 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_377 = _mm256_add_pd(_t5_375, _t5_376);

  // 4-BLAC: 1x4 / 1x4
  _t5_378 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_374), _mm256_castpd256_pd128(_t5_377)));

  // AVX Storer:
  _t5_37 = _t5_378;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, 7), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_379 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_82, _t5_82, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_380 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_35, _t5_36), _mm256_unpacklo_pd(_t5_37, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_381 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_382 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_380, _t5_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_380, _t5_381), _mm256_mul_pd(_t5_380, _t5_381), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_380, _t5_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_380, _t5_381), _mm256_mul_pd(_t5_380, _t5_381), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_380, _t5_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_380, _t5_381), _mm256_mul_pd(_t5_380, _t5_381), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_383 = _mm256_sub_pd(_t5_379, _t5_382);

  // AVX Storer:
  _t5_38 = _t5_383;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_384 = _t5_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_385 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_386 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_387 = _mm256_add_pd(_t5_385, _t5_386);

  // 4-BLAC: 1x4 / 1x4
  _t5_388 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_384), _mm256_castpd256_pd128(_t5_387)));

  // AVX Storer:
  _t5_38 = _t5_388;

  // Generating : X[52,52] = ( S(h(4, 52, 4), ( G(h(4, 52, 4), X[52,52],h(4, 52, fi1082)) - ( G(h(4, 52, 4), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082)) + Sum_{k116} ( -$(h(4, 52, 4), ( G(h(4, 52, 4), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi1082)) ),h(4, 52, fi1082)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t5_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_7, _t5_7, 32), _mm256_permute2f128_pd(_t5_7, _t5_7, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_8, _t5_8, 32), _mm256_permute2f128_pd(_t5_8, _t5_8, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_9, _t5_9, 32), _mm256_permute2f128_pd(_t5_9, _t5_9, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_10, _t5_10, 32), _mm256_permute2f128_pd(_t5_10, _t5_10, 32), 0), _t0_11)));
  _t5_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_11, _t5_11, 32), _mm256_permute2f128_pd(_t5_11, _t5_11, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_13, _t5_13, 32), _mm256_permute2f128_pd(_t5_13, _t5_13, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0), _t0_11)));
  _t5_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_15, _t5_15, 32), _mm256_permute2f128_pd(_t5_15, _t5_15, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_16, _t5_16, 32), _mm256_permute2f128_pd(_t5_16, _t5_16, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_17, _t5_17, 32), _mm256_permute2f128_pd(_t5_17, _t5_17, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0), _t0_11)));
  _t5_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_19, _t5_19, 32), _mm256_permute2f128_pd(_t5_19, _t5_19, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_21, _t5_21, 32), _mm256_permute2f128_pd(_t5_21, _t5_21, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_22, _t5_22, 32), _mm256_permute2f128_pd(_t5_22, _t5_22, 32), 0), _t0_11)));

  // 4-BLAC: 4x4 - 4x4
  _t5_83 = _mm256_sub_pd(_t5_83, _t5_66);
  _t5_84 = _mm256_sub_pd(_t5_84, _t5_67);
  _t5_85 = _mm256_sub_pd(_t5_85, _t5_68);
  _t5_86 = _mm256_sub_pd(_t5_86, _t5_69);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t5_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_23, _t5_23, 32), _mm256_permute2f128_pd(_t5_23, _t5_23, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_24, _t5_24, 32), _mm256_permute2f128_pd(_t5_24, _t5_24, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_25, _t5_25, 32), _mm256_permute2f128_pd(_t5_25, _t5_25, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_26, _t5_26, 32), _mm256_permute2f128_pd(_t5_26, _t5_26, 32), 0), _t0_7)));
  _t5_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_27, _t5_27, 32), _mm256_permute2f128_pd(_t5_27, _t5_27, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_28, _t5_28, 32), _mm256_permute2f128_pd(_t5_28, _t5_28, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_29, _t5_29, 32), _mm256_permute2f128_pd(_t5_29, _t5_29, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_30, _t5_30, 32), _mm256_permute2f128_pd(_t5_30, _t5_30, 32), 0), _t0_7)));
  _t5_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_31, _t5_31, 32), _mm256_permute2f128_pd(_t5_31, _t5_31, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_32, _t5_32, 32), _mm256_permute2f128_pd(_t5_32, _t5_32, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_33, _t5_33, 32), _mm256_permute2f128_pd(_t5_33, _t5_33, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_34, _t5_34, 32), _mm256_permute2f128_pd(_t5_34, _t5_34, 32), 0), _t0_7)));
  _t5_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_35, _t5_35, 32), _mm256_permute2f128_pd(_t5_35, _t5_35, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_36, _t5_36, 32), _mm256_permute2f128_pd(_t5_36, _t5_36, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_37, _t5_37, 32), _mm256_permute2f128_pd(_t5_37, _t5_37, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_38, _t5_38, 32), _mm256_permute2f128_pd(_t5_38, _t5_38, 32), 0), _t0_7)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t5_83 = _mm256_sub_pd(_t5_83, _t5_70);
  _t5_84 = _mm256_sub_pd(_t5_84, _t5_71);
  _t5_85 = _mm256_sub_pd(_t5_85, _t5_72);
  _t5_86 = _mm256_sub_pd(_t5_86, _t5_73);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_389 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_83, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_390 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_391 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_392 = _mm256_add_pd(_t5_390, _t5_391);

  // 4-BLAC: 1x4 / 1x4
  _t5_393 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_389), _mm256_castpd256_pd128(_t5_392)));

  // AVX Storer:
  _t5_39 = _t5_393;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_394 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_83, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_395 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_396 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_397 = _mm256_mul_pd(_t5_395, _t5_396);

  // 4-BLAC: 1x4 - 1x4
  _t5_398 = _mm256_sub_pd(_t5_394, _t5_397);

  // AVX Storer:
  _t5_40 = _t5_398;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_399 = _t5_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_400 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_401 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_402 = _mm256_add_pd(_t5_400, _t5_401);

  // 4-BLAC: 1x4 / 1x4
  _t5_403 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_399), _mm256_castpd256_pd128(_t5_402)));

  // AVX Storer:
  _t5_40 = _t5_403;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 4), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_404 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_83, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_83, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_405 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_39, _t5_40), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_406 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_405, _t5_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_405, _t5_406), _mm256_mul_pd(_t5_405, _t5_406), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_405, _t5_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_405, _t5_406), _mm256_mul_pd(_t5_405, _t5_406), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_405, _t5_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_405, _t5_406), _mm256_mul_pd(_t5_405, _t5_406), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_408 = _mm256_sub_pd(_t5_404, _t5_407);

  // AVX Storer:
  _t5_41 = _t5_408;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_409 = _t5_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_410 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_411 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_412 = _mm256_add_pd(_t5_410, _t5_411);

  // 4-BLAC: 1x4 / 1x4
  _t5_413 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_409), _mm256_castpd256_pd128(_t5_412)));

  // AVX Storer:
  _t5_41 = _t5_413;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 4), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_414 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_83, _t5_83, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_415 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_39, _t5_40), _mm256_unpacklo_pd(_t5_41, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_416 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_417 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_415, _t5_416), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_415, _t5_416), _mm256_mul_pd(_t5_415, _t5_416), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_415, _t5_416), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_415, _t5_416), _mm256_mul_pd(_t5_415, _t5_416), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_415, _t5_416), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_415, _t5_416), _mm256_mul_pd(_t5_415, _t5_416), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_418 = _mm256_sub_pd(_t5_414, _t5_417);

  // AVX Storer:
  _t5_42 = _t5_418;

  // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_419 = _t5_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_420 = _t5_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_421 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_422 = _mm256_add_pd(_t5_420, _t5_421);

  // 4-BLAC: 1x4 / 1x4
  _t5_423 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_419), _mm256_castpd256_pd128(_t5_422)));

  // AVX Storer:
  _t5_42 = _t5_423;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_424 = _t5_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t5_57 = _mm256_mul_pd(_t5_424, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_39, _t5_40), _mm256_unpacklo_pd(_t5_41, _t5_42), 32));

  // 4-BLAC: 1x4 - 1x4
  _t5_84 = _mm256_sub_pd(_t5_84, _t5_57);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_425 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_84, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_426 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_427 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_428 = _mm256_add_pd(_t5_426, _t5_427);

  // 4-BLAC: 1x4 / 1x4
  _t5_429 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_425), _mm256_castpd256_pd128(_t5_428)));

  // AVX Storer:
  _t5_43 = _t5_429;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_430 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_84, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_431 = _t5_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_432 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_433 = _mm256_mul_pd(_t5_431, _t5_432);

  // 4-BLAC: 1x4 - 1x4
  _t5_434 = _mm256_sub_pd(_t5_430, _t5_433);

  // AVX Storer:
  _t5_44 = _t5_434;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_435 = _t5_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_436 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_437 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_438 = _mm256_add_pd(_t5_436, _t5_437);

  // 4-BLAC: 1x4 / 1x4
  _t5_439 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_435), _mm256_castpd256_pd128(_t5_438)));

  // AVX Storer:
  _t5_44 = _t5_439;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 5), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_440 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_84, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_84, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_441 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_43, _t5_44), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_442 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_443 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_441, _t5_442), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_441, _t5_442), _mm256_mul_pd(_t5_441, _t5_442), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_441, _t5_442), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_441, _t5_442), _mm256_mul_pd(_t5_441, _t5_442), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_441, _t5_442), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_441, _t5_442), _mm256_mul_pd(_t5_441, _t5_442), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_444 = _mm256_sub_pd(_t5_440, _t5_443);

  // AVX Storer:
  _t5_45 = _t5_444;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_445 = _t5_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_446 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_447 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_448 = _mm256_add_pd(_t5_446, _t5_447);

  // 4-BLAC: 1x4 / 1x4
  _t5_449 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_445), _mm256_castpd256_pd128(_t5_448)));

  // AVX Storer:
  _t5_45 = _t5_449;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 5), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_450 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_84, _t5_84, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_451 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_43, _t5_44), _mm256_unpacklo_pd(_t5_45, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_452 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_453 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_451, _t5_452), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_451, _t5_452), _mm256_mul_pd(_t5_451, _t5_452), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_451, _t5_452), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_451, _t5_452), _mm256_mul_pd(_t5_451, _t5_452), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_451, _t5_452), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_451, _t5_452), _mm256_mul_pd(_t5_451, _t5_452), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_454 = _mm256_sub_pd(_t5_450, _t5_453);

  // AVX Storer:
  _t5_46 = _t5_454;

  // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_455 = _t5_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_456 = _t5_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_457 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_458 = _mm256_add_pd(_t5_456, _t5_457);

  // 4-BLAC: 1x4 / 1x4
  _t5_459 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_455), _mm256_castpd256_pd128(_t5_458)));

  // AVX Storer:
  _t5_46 = _t5_459;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) * G(h(2, 52, 4), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_460 = _t5_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t5_461 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_39, _t5_40), _mm256_unpacklo_pd(_t5_41, _t5_42), 32);
  _t5_462 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_43, _t5_44), _mm256_unpacklo_pd(_t5_45, _t5_46), 32);
  _t5_463 = _mm256_setzero_pd();
  _t5_464 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t5_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_460, _t5_460, 32), _mm256_permute2f128_pd(_t5_460, _t5_460, 32), 0), _t5_461), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_460, _t5_460, 32), _mm256_permute2f128_pd(_t5_460, _t5_460, 32), 15), _t5_462)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_460, _t5_460, 49), _mm256_permute2f128_pd(_t5_460, _t5_460, 49), 0), _t5_463), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_460, _t5_460, 49), _mm256_permute2f128_pd(_t5_460, _t5_460, 49), 15), _t5_464)));

  // 4-BLAC: 1x4 - 1x4
  _t5_85 = _mm256_sub_pd(_t5_85, _t5_74);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_465 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_85, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_466 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_467 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_468 = _mm256_add_pd(_t5_466, _t5_467);

  // 4-BLAC: 1x4 / 1x4
  _t5_469 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_465), _mm256_castpd256_pd128(_t5_468)));

  // AVX Storer:
  _t5_47 = _t5_469;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_470 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_85, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_471 = _t5_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_472 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_473 = _mm256_mul_pd(_t5_471, _t5_472);

  // 4-BLAC: 1x4 - 1x4
  _t5_474 = _mm256_sub_pd(_t5_470, _t5_473);

  // AVX Storer:
  _t5_48 = _t5_474;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_475 = _t5_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_476 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_477 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_478 = _mm256_add_pd(_t5_476, _t5_477);

  // 4-BLAC: 1x4 / 1x4
  _t5_479 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_475), _mm256_castpd256_pd128(_t5_478)));

  // AVX Storer:
  _t5_48 = _t5_479;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 6), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_480 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_85, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_85, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_481 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_482 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_481, _t5_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_481, _t5_482), _mm256_mul_pd(_t5_481, _t5_482), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_481, _t5_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_481, _t5_482), _mm256_mul_pd(_t5_481, _t5_482), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_481, _t5_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_481, _t5_482), _mm256_mul_pd(_t5_481, _t5_482), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_484 = _mm256_sub_pd(_t5_480, _t5_483);

  // AVX Storer:
  _t5_49 = _t5_484;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_485 = _t5_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_486 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_487 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_488 = _mm256_add_pd(_t5_486, _t5_487);

  // 4-BLAC: 1x4 / 1x4
  _t5_489 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_485), _mm256_castpd256_pd128(_t5_488)));

  // AVX Storer:
  _t5_49 = _t5_489;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 6), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_490 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_85, _t5_85, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_491 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_492 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_493 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_491, _t5_492), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_491, _t5_492), _mm256_mul_pd(_t5_491, _t5_492), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_491, _t5_492), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_491, _t5_492), _mm256_mul_pd(_t5_491, _t5_492), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_491, _t5_492), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_491, _t5_492), _mm256_mul_pd(_t5_491, _t5_492), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_494 = _mm256_sub_pd(_t5_490, _t5_493);

  // AVX Storer:
  _t5_50 = _t5_494;

  // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_495 = _t5_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_496 = _t5_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_497 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_498 = _mm256_add_pd(_t5_496, _t5_497);

  // 4-BLAC: 1x4 / 1x4
  _t5_499 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_495), _mm256_castpd256_pd128(_t5_498)));

  // AVX Storer:
  _t5_50 = _t5_499;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) * G(h(3, 52, 4), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_500 = _t5_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t5_501 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_39, _t5_40), _mm256_unpacklo_pd(_t5_41, _t5_42), 32);
  _t5_502 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_43, _t5_44), _mm256_unpacklo_pd(_t5_45, _t5_46), 32);
  _t5_503 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _t5_50), 32);
  _t5_504 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t5_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_500, _t5_500, 32), _mm256_permute2f128_pd(_t5_500, _t5_500, 32), 0), _t5_501), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_500, _t5_500, 32), _mm256_permute2f128_pd(_t5_500, _t5_500, 32), 15), _t5_502)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_500, _t5_500, 49), _mm256_permute2f128_pd(_t5_500, _t5_500, 49), 0), _t5_503), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_500, _t5_500, 49), _mm256_permute2f128_pd(_t5_500, _t5_500, 49), 15), _t5_504)));

  // 4-BLAC: 1x4 - 1x4
  _t5_86 = _mm256_sub_pd(_t5_86, _t5_75);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_505 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_86, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_506 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_507 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_508 = _mm256_add_pd(_t5_506, _t5_507);

  // 4-BLAC: 1x4 / 1x4
  _t5_509 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_505), _mm256_castpd256_pd128(_t5_508)));

  // AVX Storer:
  _t5_51 = _t5_509;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_510 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_86, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_511 = _t5_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_512 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_513 = _mm256_mul_pd(_t5_511, _t5_512);

  // 4-BLAC: 1x4 - 1x4
  _t5_514 = _mm256_sub_pd(_t5_510, _t5_513);

  // AVX Storer:
  _t5_52 = _t5_514;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_515 = _t5_52;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_516 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_517 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_518 = _mm256_add_pd(_t5_516, _t5_517);

  // 4-BLAC: 1x4 / 1x4
  _t5_519 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_515), _mm256_castpd256_pd128(_t5_518)));

  // AVX Storer:
  _t5_52 = _t5_519;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 7), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_520 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_86, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_86, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_521 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_51, _t5_52), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_522 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_523 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_521, _t5_522), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_521, _t5_522), _mm256_mul_pd(_t5_521, _t5_522), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_521, _t5_522), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_521, _t5_522), _mm256_mul_pd(_t5_521, _t5_522), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_521, _t5_522), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_521, _t5_522), _mm256_mul_pd(_t5_521, _t5_522), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_524 = _mm256_sub_pd(_t5_520, _t5_523);

  // AVX Storer:
  _t5_53 = _t5_524;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_525 = _t5_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_526 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_527 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_528 = _mm256_add_pd(_t5_526, _t5_527);

  // 4-BLAC: 1x4 / 1x4
  _t5_529 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_525), _mm256_castpd256_pd128(_t5_528)));

  // AVX Storer:
  _t5_53 = _t5_529;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 7), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_530 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_86, _t5_86, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_531 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_51, _t5_52), _mm256_unpacklo_pd(_t5_53, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_532 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_533 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_531, _t5_532), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_531, _t5_532), _mm256_mul_pd(_t5_531, _t5_532), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_531, _t5_532), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_531, _t5_532), _mm256_mul_pd(_t5_531, _t5_532), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_531, _t5_532), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_531, _t5_532), _mm256_mul_pd(_t5_531, _t5_532), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_534 = _mm256_sub_pd(_t5_530, _t5_533);

  // AVX Storer:
  _t5_54 = _t5_534;

  // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_535 = _t5_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_536 = _t5_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_537 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_538 = _mm256_add_pd(_t5_536, _t5_537);

  // 4-BLAC: 1x4 / 1x4
  _t5_539 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_535), _mm256_castpd256_pd128(_t5_538)));

  // AVX Storer:
  _t5_54 = _t5_539;

  _asm256_storeu_pd(C + 260, _t5_76);
  _asm256_storeu_pd(C + 312, _t5_77);
  _asm256_storeu_pd(C + 364, _t5_78);
  _mm_store_sd(&(C[212]), _mm256_castpd256_pd128(_t5_23));
  _mm_store_sd(&(C[213]), _mm256_castpd256_pd128(_t5_24));
  _mm_store_sd(&(C[214]), _mm256_castpd256_pd128(_t5_25));
  _mm_store_sd(&(C[215]), _mm256_castpd256_pd128(_t5_26));
  _mm_store_sd(&(C[264]), _mm256_castpd256_pd128(_t5_27));
  _mm_store_sd(&(C[265]), _mm256_castpd256_pd128(_t5_28));
  _mm_store_sd(&(C[266]), _mm256_castpd256_pd128(_t5_29));
  _mm_store_sd(&(C[267]), _mm256_castpd256_pd128(_t5_30));
  _mm_store_sd(&(C[316]), _mm256_castpd256_pd128(_t5_31));
  _mm_store_sd(&(C[317]), _mm256_castpd256_pd128(_t5_32));
  _mm_store_sd(&(C[318]), _mm256_castpd256_pd128(_t5_33));
  _mm_store_sd(&(C[319]), _mm256_castpd256_pd128(_t5_34));
  _mm_store_sd(&(C[368]), _mm256_castpd256_pd128(_t5_35));
  _mm_store_sd(&(C[369]), _mm256_castpd256_pd128(_t5_36));
  _mm_store_sd(&(C[370]), _mm256_castpd256_pd128(_t5_37));
  _mm_store_sd(&(C[371]), _mm256_castpd256_pd128(_t5_38));
  _mm_store_sd(&(C[216]), _mm256_castpd256_pd128(_t5_39));
  _mm_store_sd(&(C[217]), _mm256_castpd256_pd128(_t5_40));
  _mm_store_sd(&(C[218]), _mm256_castpd256_pd128(_t5_41));
  _mm_store_sd(&(C[219]), _mm256_castpd256_pd128(_t5_42));
  _mm_store_sd(&(C[268]), _mm256_castpd256_pd128(_t5_43));
  _mm_store_sd(&(C[269]), _mm256_castpd256_pd128(_t5_44));
  _mm_store_sd(&(C[270]), _mm256_castpd256_pd128(_t5_45));
  _mm_store_sd(&(C[271]), _mm256_castpd256_pd128(_t5_46));
  _mm_store_sd(&(C[320]), _mm256_castpd256_pd128(_t5_47));
  _mm_store_sd(&(C[321]), _mm256_castpd256_pd128(_t5_48));
  _mm_store_sd(&(C[322]), _mm256_castpd256_pd128(_t5_49));
  _mm_store_sd(&(C[323]), _mm256_castpd256_pd128(_t5_50));
  _mm_store_sd(&(C[372]), _mm256_castpd256_pd128(_t5_51));
  _mm_store_sd(&(C[373]), _mm256_castpd256_pd128(_t5_52));
  _mm_store_sd(&(C[374]), _mm256_castpd256_pd128(_t5_53));
  _mm_store_sd(&(C[375]), _mm256_castpd256_pd128(_t5_54));

  for( int fi1082 = 12; fi1082 <= 48; fi1082+=4 ) {
    _t6_4 = _asm256_loadu_pd(C + fi1082 + 208);
    _t6_5 = _asm256_loadu_pd(C + fi1082 + 260);
    _t6_6 = _asm256_loadu_pd(C + fi1082 + 312);
    _t6_7 = _asm256_loadu_pd(C + fi1082 + 364);
    _t6_3 = _asm256_loadu_pd(U + fi1082);
    _t6_2 = _asm256_loadu_pd(U + fi1082 + 52);
    _t6_1 = _asm256_loadu_pd(U + fi1082 + 104);
    _t6_0 = _asm256_loadu_pd(U + fi1082 + 156);

    // Generating : X[52,52] = ( S(h(4, 52, 4), ( G(h(4, 52, 4), X[52,52],h(4, 52, fi1082)) - ( G(h(4, 52, 4), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082)) + Sum_{k116} ( -$(h(4, 52, 4), ( G(h(4, 52, 4), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi1082)) ),h(4, 52, fi1082)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t5_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_7, _t5_7, 32), _mm256_permute2f128_pd(_t5_7, _t5_7, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_8, _t5_8, 32), _mm256_permute2f128_pd(_t5_8, _t5_8, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_9, _t5_9, 32), _mm256_permute2f128_pd(_t5_9, _t5_9, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_10, _t5_10, 32), _mm256_permute2f128_pd(_t5_10, _t5_10, 32), 0), _t6_0)));
    _t5_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_11, _t5_11, 32), _mm256_permute2f128_pd(_t5_11, _t5_11, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_12, _t5_12, 32), _mm256_permute2f128_pd(_t5_12, _t5_12, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_13, _t5_13, 32), _mm256_permute2f128_pd(_t5_13, _t5_13, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_14, _t5_14, 32), _mm256_permute2f128_pd(_t5_14, _t5_14, 32), 0), _t6_0)));
    _t5_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_15, _t5_15, 32), _mm256_permute2f128_pd(_t5_15, _t5_15, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_16, _t5_16, 32), _mm256_permute2f128_pd(_t5_16, _t5_16, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_17, _t5_17, 32), _mm256_permute2f128_pd(_t5_17, _t5_17, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_18, _t5_18, 32), _mm256_permute2f128_pd(_t5_18, _t5_18, 32), 0), _t6_0)));
    _t5_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_19, _t5_19, 32), _mm256_permute2f128_pd(_t5_19, _t5_19, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_20, _t5_20, 32), _mm256_permute2f128_pd(_t5_20, _t5_20, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_21, _t5_21, 32), _mm256_permute2f128_pd(_t5_21, _t5_21, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_22, _t5_22, 32), _mm256_permute2f128_pd(_t5_22, _t5_22, 32), 0), _t6_0)));

    // 4-BLAC: 4x4 - 4x4
    _t6_4 = _mm256_sub_pd(_t6_4, _t5_66);
    _t6_5 = _mm256_sub_pd(_t6_5, _t5_67);
    _t6_6 = _mm256_sub_pd(_t6_6, _t5_68);
    _t6_7 = _mm256_sub_pd(_t6_7, _t5_69);

    // AVX Storer:
    _asm256_storeu_pd(C + fi1082 + 208, _t6_4);
    _asm256_storeu_pd(C + fi1082 + 260, _t6_5);
    _asm256_storeu_pd(C + fi1082 + 312, _t6_6);
    _asm256_storeu_pd(C + fi1082 + 364, _t6_7);

    for( int k116 = 4; k116 <= fi1082 - 1; k116+=4 ) {
      _t7_19 = _mm256_broadcast_sd(C + k116 + 208);
      _t7_18 = _mm256_broadcast_sd(C + k116 + 209);
      _t7_17 = _mm256_broadcast_sd(C + k116 + 210);
      _t7_16 = _mm256_broadcast_sd(C + k116 + 211);
      _t7_15 = _mm256_broadcast_sd(C + k116 + 260);
      _t7_14 = _mm256_broadcast_sd(C + k116 + 261);
      _t7_13 = _mm256_broadcast_sd(C + k116 + 262);
      _t7_12 = _mm256_broadcast_sd(C + k116 + 263);
      _t7_11 = _mm256_broadcast_sd(C + k116 + 312);
      _t7_10 = _mm256_broadcast_sd(C + k116 + 313);
      _t7_9 = _mm256_broadcast_sd(C + k116 + 314);
      _t7_8 = _mm256_broadcast_sd(C + k116 + 315);
      _t7_7 = _mm256_broadcast_sd(C + k116 + 364);
      _t7_6 = _mm256_broadcast_sd(C + k116 + 365);
      _t7_5 = _mm256_broadcast_sd(C + k116 + 366);
      _t7_4 = _mm256_broadcast_sd(C + k116 + 367);
      _t7_3 = _asm256_loadu_pd(U + fi1082 + 52*k116);
      _t7_2 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 52);
      _t7_1 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 104);
      _t7_0 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 156);
      _t7_20 = _asm256_loadu_pd(C + fi1082 + 208);
      _t7_21 = _asm256_loadu_pd(C + fi1082 + 260);
      _t7_22 = _asm256_loadu_pd(C + fi1082 + 312);
      _t7_23 = _asm256_loadu_pd(C + fi1082 + 364);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_3), _mm256_mul_pd(_t7_18, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_1), _mm256_mul_pd(_t7_16, _t7_0)));
      _t5_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_3), _mm256_mul_pd(_t7_14, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_1), _mm256_mul_pd(_t7_12, _t7_0)));
      _t5_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_3), _mm256_mul_pd(_t7_10, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_1), _mm256_mul_pd(_t7_8, _t7_0)));
      _t5_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_3), _mm256_mul_pd(_t7_6, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_1), _mm256_mul_pd(_t7_4, _t7_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t7_20 = _mm256_sub_pd(_t7_20, _t5_70);
      _t7_21 = _mm256_sub_pd(_t7_21, _t5_71);
      _t7_22 = _mm256_sub_pd(_t7_22, _t5_72);
      _t7_23 = _mm256_sub_pd(_t7_23, _t5_73);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1082 + 208, _t7_20);
      _asm256_storeu_pd(C + fi1082 + 260, _t7_21);
      _asm256_storeu_pd(C + fi1082 + 312, _t7_22);
      _asm256_storeu_pd(C + fi1082 + 364, _t7_23);
    }
    _t6_6 = _asm256_loadu_pd(C + fi1082 + 312);
    _t6_5 = _asm256_loadu_pd(C + fi1082 + 260);
    _t6_4 = _asm256_loadu_pd(C + fi1082 + 208);
    _t6_7 = _asm256_loadu_pd(C + fi1082 + 364);
    _t8_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082])));
    _t8_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 1])));
    _t8_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 53])));
    _t8_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 54)), 0);
    _t8_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 106])));
    _t8_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 55))), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 107)), 32);
    _t8_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 159])));

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_24 = _t5_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_25 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_392 = _mm256_add_pd(_t8_24, _t8_25);

    // 4-BLAC: 1x4 / 1x4
    _t8_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_23), _mm256_castpd256_pd128(_t5_392)));

    // AVX Storer:
    _t8_7 = _t8_26;

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_28 = _t8_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_29 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_397 = _mm256_mul_pd(_t8_28, _t8_29);

    // 4-BLAC: 1x4 - 1x4
    _t8_30 = _mm256_sub_pd(_t8_27, _t5_397);

    // AVX Storer:
    _t8_8 = _t8_30;

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_31 = _t8_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_32 = _t5_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_33 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_402 = _mm256_add_pd(_t8_32, _t8_33);

    // 4-BLAC: 1x4 / 1x4
    _t8_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_31), _mm256_castpd256_pd128(_t5_402)));

    // AVX Storer:
    _t8_8 = _t8_34;

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 4), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_37 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_mul_pd(_t8_36, _t8_37), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_mul_pd(_t8_36, _t8_37), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_mul_pd(_t8_36, _t8_37), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_38 = _mm256_sub_pd(_t8_35, _t5_407);

    // AVX Storer:
    _t8_9 = _t8_38;

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_39 = _t8_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_40 = _t5_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_41 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_412 = _mm256_add_pd(_t8_40, _t8_41);

    // 4-BLAC: 1x4 / 1x4
    _t8_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_39), _mm256_castpd256_pd128(_t5_412)));

    // AVX Storer:
    _t8_9 = _t8_42;

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 4), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_4, _t6_4, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_45 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_417 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_mul_pd(_t8_44, _t8_45), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_mul_pd(_t8_44, _t8_45), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_mul_pd(_t8_44, _t8_45), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_46 = _mm256_sub_pd(_t8_43, _t5_417);

    // AVX Storer:
    _t8_10 = _t8_46;

    // Generating : X[52,52] = S(h(1, 52, 4), ( G(h(1, 52, 4), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 4), L[52,52],h(1, 52, 4)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_47 = _t8_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_48 = _t5_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_49 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_422 = _mm256_add_pd(_t8_48, _t8_49);

    // 4-BLAC: 1x4 / 1x4
    _t8_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_47), _mm256_castpd256_pd128(_t5_422)));

    // AVX Storer:
    _t8_10 = _t8_50;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, 5), L[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_51 = _t5_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t5_57 = _mm256_mul_pd(_t8_51, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));

    // 4-BLAC: 1x4 - 1x4
    _t6_5 = _mm256_sub_pd(_t6_5, _t5_57);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_52 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_53 = _t5_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_54 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_428 = _mm256_add_pd(_t8_53, _t8_54);

    // 4-BLAC: 1x4 / 1x4
    _t8_55 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_52), _mm256_castpd256_pd128(_t5_428)));

    // AVX Storer:
    _t8_11 = _t8_55;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_56 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_57 = _t8_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_58 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_433 = _mm256_mul_pd(_t8_57, _t8_58);

    // 4-BLAC: 1x4 - 1x4
    _t8_59 = _mm256_sub_pd(_t8_56, _t5_433);

    // AVX Storer:
    _t8_12 = _t8_59;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_60 = _t8_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_61 = _t5_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_62 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_438 = _mm256_add_pd(_t8_61, _t8_62);

    // 4-BLAC: 1x4 / 1x4
    _t8_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_60), _mm256_castpd256_pd128(_t5_438)));

    // AVX Storer:
    _t8_12 = _t8_63;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 5), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_64 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_65 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_66 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_443 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_65, _t8_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_65, _t8_66), _mm256_mul_pd(_t8_65, _t8_66), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_65, _t8_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_65, _t8_66), _mm256_mul_pd(_t8_65, _t8_66), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_65, _t8_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_65, _t8_66), _mm256_mul_pd(_t8_65, _t8_66), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_67 = _mm256_sub_pd(_t8_64, _t5_443);

    // AVX Storer:
    _t8_13 = _t8_67;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_68 = _t8_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_69 = _t5_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_70 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_448 = _mm256_add_pd(_t8_69, _t8_70);

    // 4-BLAC: 1x4 / 1x4
    _t8_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_68), _mm256_castpd256_pd128(_t5_448)));

    // AVX Storer:
    _t8_13 = _t8_71;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 5), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_72 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_5, _t6_5, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_73 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_74 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_453 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_mul_pd(_t8_73, _t8_74), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_mul_pd(_t8_73, _t8_74), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_mul_pd(_t8_73, _t8_74), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_75 = _mm256_sub_pd(_t8_72, _t5_453);

    // AVX Storer:
    _t8_14 = _t8_75;

    // Generating : X[52,52] = S(h(1, 52, 5), ( G(h(1, 52, 5), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 5), L[52,52],h(1, 52, 5)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_76 = _t8_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_77 = _t5_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_78 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_458 = _mm256_add_pd(_t8_77, _t8_78);

    // 4-BLAC: 1x4 / 1x4
    _t8_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_76), _mm256_castpd256_pd128(_t5_458)));

    // AVX Storer:
    _t8_14 = _t8_79;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, 6), L[52,52],h(2, 52, 4)) * G(h(2, 52, 4), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_80 = _t5_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t8_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32);
    _t8_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _t8_14), 32);
    _t8_83 = _mm256_setzero_pd();
    _t8_84 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t5_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_80, _t8_80, 32), _mm256_permute2f128_pd(_t8_80, _t8_80, 32), 0), _t8_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_80, _t8_80, 32), _mm256_permute2f128_pd(_t8_80, _t8_80, 32), 15), _t8_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_80, _t8_80, 49), _mm256_permute2f128_pd(_t8_80, _t8_80, 49), 0), _t8_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_80, _t8_80, 49), _mm256_permute2f128_pd(_t8_80, _t8_80, 49), 15), _t8_84)));

    // 4-BLAC: 1x4 - 1x4
    _t6_6 = _mm256_sub_pd(_t6_6, _t5_74);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_85 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_86 = _t5_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_87 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_468 = _mm256_add_pd(_t8_86, _t8_87);

    // 4-BLAC: 1x4 / 1x4
    _t8_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_85), _mm256_castpd256_pd128(_t5_468)));

    // AVX Storer:
    _t8_15 = _t8_88;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_90 = _t8_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_91 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_473 = _mm256_mul_pd(_t8_90, _t8_91);

    // 4-BLAC: 1x4 - 1x4
    _t8_92 = _mm256_sub_pd(_t8_89, _t5_473);

    // AVX Storer:
    _t8_16 = _t8_92;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_93 = _t8_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_94 = _t5_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_95 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_478 = _mm256_add_pd(_t8_94, _t8_95);

    // 4-BLAC: 1x4 / 1x4
    _t8_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_93), _mm256_castpd256_pd128(_t5_478)));

    // AVX Storer:
    _t8_16 = _t8_96;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 6), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_97 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_98 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_15, _t8_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_99 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_98, _t8_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_98, _t8_99), _mm256_mul_pd(_t8_98, _t8_99), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_98, _t8_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_98, _t8_99), _mm256_mul_pd(_t8_98, _t8_99), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_98, _t8_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_98, _t8_99), _mm256_mul_pd(_t8_98, _t8_99), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_100 = _mm256_sub_pd(_t8_97, _t5_483);

    // AVX Storer:
    _t8_17 = _t8_100;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_101 = _t8_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_102 = _t5_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_103 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_488 = _mm256_add_pd(_t8_102, _t8_103);

    // 4-BLAC: 1x4 / 1x4
    _t8_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_101), _mm256_castpd256_pd128(_t5_488)));

    // AVX Storer:
    _t8_17 = _t8_104;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 6), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_6, _t6_6, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_15, _t8_16), _mm256_unpacklo_pd(_t8_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_107 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_493 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_106, _t8_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_106, _t8_107), _mm256_mul_pd(_t8_106, _t8_107), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_106, _t8_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_106, _t8_107), _mm256_mul_pd(_t8_106, _t8_107), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_106, _t8_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_106, _t8_107), _mm256_mul_pd(_t8_106, _t8_107), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_108 = _mm256_sub_pd(_t8_105, _t5_493);

    // AVX Storer:
    _t8_18 = _t8_108;

    // Generating : X[52,52] = S(h(1, 52, 6), ( G(h(1, 52, 6), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 6), L[52,52],h(1, 52, 6)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_109 = _t8_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_110 = _t5_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_111 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_498 = _mm256_add_pd(_t8_110, _t8_111);

    // 4-BLAC: 1x4 / 1x4
    _t8_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_109), _mm256_castpd256_pd128(_t5_498)));

    // AVX Storer:
    _t8_18 = _t8_112;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, 7), L[52,52],h(3, 52, 4)) * G(h(3, 52, 4), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_113 = _t5_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t8_114 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32);
    _t8_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _t8_14), 32);
    _t8_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_15, _t8_16), _mm256_unpacklo_pd(_t8_17, _t8_18), 32);
    _t8_117 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t5_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_113, _t8_113, 32), _mm256_permute2f128_pd(_t8_113, _t8_113, 32), 0), _t8_114), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_113, _t8_113, 32), _mm256_permute2f128_pd(_t8_113, _t8_113, 32), 15), _t8_115)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_113, _t8_113, 49), _mm256_permute2f128_pd(_t8_113, _t8_113, 49), 0), _t8_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_113, _t8_113, 49), _mm256_permute2f128_pd(_t8_113, _t8_113, 49), 15), _t8_117)));

    // 4-BLAC: 1x4 - 1x4
    _t6_7 = _mm256_sub_pd(_t6_7, _t5_75);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_118 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_119 = _t5_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_120 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_508 = _mm256_add_pd(_t8_119, _t8_120);

    // 4-BLAC: 1x4 / 1x4
    _t8_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_118), _mm256_castpd256_pd128(_t5_508)));

    // AVX Storer:
    _t8_19 = _t8_121;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_122 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_123 = _t8_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_124 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_513 = _mm256_mul_pd(_t8_123, _t8_124);

    // 4-BLAC: 1x4 - 1x4
    _t8_125 = _mm256_sub_pd(_t8_122, _t5_513);

    // AVX Storer:
    _t8_20 = _t8_125;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_126 = _t8_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_127 = _t5_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_128 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_518 = _mm256_add_pd(_t8_127, _t8_128);

    // 4-BLAC: 1x4 / 1x4
    _t8_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_126), _mm256_castpd256_pd128(_t5_518)));

    // AVX Storer:
    _t8_20 = _t8_129;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, 7), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_130 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_131 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_19, _t8_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_132 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_523 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_131, _t8_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_131, _t8_132), _mm256_mul_pd(_t8_131, _t8_132), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_131, _t8_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_131, _t8_132), _mm256_mul_pd(_t8_131, _t8_132), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_131, _t8_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_131, _t8_132), _mm256_mul_pd(_t8_131, _t8_132), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_133 = _mm256_sub_pd(_t8_130, _t5_523);

    // AVX Storer:
    _t8_21 = _t8_133;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_134 = _t8_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_135 = _t5_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_136 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_528 = _mm256_add_pd(_t8_135, _t8_136);

    // 4-BLAC: 1x4 / 1x4
    _t8_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_134), _mm256_castpd256_pd128(_t5_528)));

    // AVX Storer:
    _t8_21 = _t8_137;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, 7), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_138 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_7, _t6_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_139 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_19, _t8_20), _mm256_unpacklo_pd(_t8_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_140 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_533 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_mul_pd(_t8_139, _t8_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_mul_pd(_t8_139, _t8_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_mul_pd(_t8_139, _t8_140), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_141 = _mm256_sub_pd(_t8_138, _t5_533);

    // AVX Storer:
    _t8_22 = _t8_141;

    // Generating : X[52,52] = S(h(1, 52, 7), ( G(h(1, 52, 7), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, 7), L[52,52],h(1, 52, 7)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_142 = _t8_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_143 = _t5_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_144 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_538 = _mm256_add_pd(_t8_143, _t8_144);

    // 4-BLAC: 1x4 / 1x4
    _t8_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_142), _mm256_castpd256_pd128(_t5_538)));

    // AVX Storer:
    _t8_22 = _t8_145;
    _mm_store_sd(&(C[fi1082 + 208]), _mm256_castpd256_pd128(_t8_7));
    _mm_store_sd(&(C[fi1082 + 209]), _mm256_castpd256_pd128(_t8_8));
    _mm_store_sd(&(C[fi1082 + 210]), _mm256_castpd256_pd128(_t8_9));
    _mm_store_sd(&(C[fi1082 + 211]), _mm256_castpd256_pd128(_t8_10));
    _mm_store_sd(&(C[fi1082 + 260]), _mm256_castpd256_pd128(_t8_11));
    _mm_store_sd(&(C[fi1082 + 261]), _mm256_castpd256_pd128(_t8_12));
    _mm_store_sd(&(C[fi1082 + 262]), _mm256_castpd256_pd128(_t8_13));
    _mm_store_sd(&(C[fi1082 + 263]), _mm256_castpd256_pd128(_t8_14));
    _mm_store_sd(&(C[fi1082 + 312]), _mm256_castpd256_pd128(_t8_15));
    _mm_store_sd(&(C[fi1082 + 313]), _mm256_castpd256_pd128(_t8_16));
    _mm_store_sd(&(C[fi1082 + 314]), _mm256_castpd256_pd128(_t8_17));
    _mm_store_sd(&(C[fi1082 + 315]), _mm256_castpd256_pd128(_t8_18));
    _mm_store_sd(&(C[fi1082 + 364]), _mm256_castpd256_pd128(_t8_19));
    _mm_store_sd(&(C[fi1082 + 365]), _mm256_castpd256_pd128(_t8_20));
    _mm_store_sd(&(C[fi1082 + 366]), _mm256_castpd256_pd128(_t8_21));
    _mm_store_sd(&(C[fi1082 + 367]), _mm256_castpd256_pd128(_t8_22));
  }


  // Generating : X[52,52] = ( Sum_{k233} ( S(h(4, 52, fi879), ( G(h(4, 52, fi879), C[52,52],h(4, 52, k233)) - ( G(h(4, 52, fi879), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, k233)) ) ),h(4, 52, k233)) ) + Sum_{k116} ( Sum_{k233} ( -$(h(4, 52, fi879), ( G(h(4, 52, fi879), L[52,52],h(4, 52, k116)) * G(h(4, 52, k116), X[52,52],h(4, 52, k233)) ),h(4, 52, k233)) ) ) )

  // AVX Loader:


  for( int k233 = 0; k233 <= 51; k233+=4 ) {
    _t9_24 = _asm256_loadu_pd(C + k233 + 416);
    _t9_25 = _asm256_loadu_pd(C + k233 + 468);
    _t9_26 = _asm256_loadu_pd(C + k233 + 520);
    _t9_27 = _asm256_loadu_pd(C + k233 + 572);
    _t9_19 = _mm256_broadcast_sd(L + 416);
    _t9_18 = _mm256_broadcast_sd(L + 417);
    _t9_17 = _mm256_broadcast_sd(L + 418);
    _t9_16 = _mm256_broadcast_sd(L + 419);
    _t9_15 = _mm256_broadcast_sd(L + 468);
    _t9_14 = _mm256_broadcast_sd(L + 469);
    _t9_13 = _mm256_broadcast_sd(L + 470);
    _t9_12 = _mm256_broadcast_sd(L + 471);
    _t9_11 = _mm256_broadcast_sd(L + 520);
    _t9_10 = _mm256_broadcast_sd(L + 521);
    _t9_9 = _mm256_broadcast_sd(L + 522);
    _t9_8 = _mm256_broadcast_sd(L + 523);
    _t9_7 = _mm256_broadcast_sd(L + 572);
    _t9_6 = _mm256_broadcast_sd(L + 573);
    _t9_5 = _mm256_broadcast_sd(L + 574);
    _t9_4 = _mm256_broadcast_sd(L + 575);
    _t9_3 = _asm256_loadu_pd(C + k233);
    _t9_2 = _asm256_loadu_pd(C + k233 + 52);
    _t9_1 = _asm256_loadu_pd(C + k233 + 104);
    _t9_0 = _asm256_loadu_pd(C + k233 + 156);

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_3), _mm256_mul_pd(_t9_18, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_1), _mm256_mul_pd(_t9_16, _t9_0)));
    _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_3), _mm256_mul_pd(_t9_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_1), _mm256_mul_pd(_t9_12, _t9_0)));
    _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_3), _mm256_mul_pd(_t9_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_1), _mm256_mul_pd(_t9_8, _t9_0)));
    _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_3), _mm256_mul_pd(_t9_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_1), _mm256_mul_pd(_t9_4, _t9_0)));

    // 4-BLAC: 4x4 - 4x4
    _t9_24 = _mm256_sub_pd(_t9_24, _t9_20);
    _t9_25 = _mm256_sub_pd(_t9_25, _t9_21);
    _t9_26 = _mm256_sub_pd(_t9_26, _t9_22);
    _t9_27 = _mm256_sub_pd(_t9_27, _t9_23);

    // AVX Storer:
    _asm256_storeu_pd(C + k233 + 416, _t9_24);
    _asm256_storeu_pd(C + k233 + 468, _t9_25);
    _asm256_storeu_pd(C + k233 + 520, _t9_26);
    _asm256_storeu_pd(C + k233 + 572, _t9_27);
  }


  // AVX Loader:

  _mm_store_sd(&(C[208]), _mm256_castpd256_pd128(_t5_7));
  _mm_store_sd(&(C[209]), _mm256_castpd256_pd128(_t5_8));
  _mm_store_sd(&(C[210]), _mm256_castpd256_pd128(_t5_9));
  _mm_store_sd(&(C[211]), _mm256_castpd256_pd128(_t5_10));
  _mm_store_sd(&(C[260]), _mm256_castpd256_pd128(_t5_11));
  _mm_store_sd(&(C[261]), _mm256_castpd256_pd128(_t5_12));
  _mm_store_sd(&(C[262]), _mm256_castpd256_pd128(_t5_13));
  _mm_store_sd(&(C[263]), _mm256_castpd256_pd128(_t5_14));
  _mm_store_sd(&(C[312]), _mm256_castpd256_pd128(_t5_15));
  _mm_store_sd(&(C[313]), _mm256_castpd256_pd128(_t5_16));
  _mm_store_sd(&(C[314]), _mm256_castpd256_pd128(_t5_17));
  _mm_store_sd(&(C[315]), _mm256_castpd256_pd128(_t5_18));
  _mm_store_sd(&(C[364]), _mm256_castpd256_pd128(_t5_19));
  _mm_store_sd(&(C[365]), _mm256_castpd256_pd128(_t5_20));
  _mm_store_sd(&(C[366]), _mm256_castpd256_pd128(_t5_21));
  _mm_store_sd(&(C[367]), _mm256_castpd256_pd128(_t5_22));

  for( int k233 = 0; k233 <= 51; k233+=4 ) {
    _t10_19 = _mm256_broadcast_sd(L + 420);
    _t10_18 = _mm256_broadcast_sd(L + 421);
    _t10_17 = _mm256_broadcast_sd(L + 422);
    _t10_16 = _mm256_broadcast_sd(L + 423);
    _t10_15 = _mm256_broadcast_sd(L + 472);
    _t10_14 = _mm256_broadcast_sd(L + 473);
    _t10_13 = _mm256_broadcast_sd(L + 474);
    _t10_12 = _mm256_broadcast_sd(L + 475);
    _t10_11 = _mm256_broadcast_sd(L + 524);
    _t10_10 = _mm256_broadcast_sd(L + 525);
    _t10_9 = _mm256_broadcast_sd(L + 526);
    _t10_8 = _mm256_broadcast_sd(L + 527);
    _t10_7 = _mm256_broadcast_sd(L + 576);
    _t10_6 = _mm256_broadcast_sd(L + 577);
    _t10_5 = _mm256_broadcast_sd(L + 578);
    _t10_4 = _mm256_broadcast_sd(L + 579);
    _t10_3 = _asm256_loadu_pd(C + k233 + 208);
    _t10_2 = _asm256_loadu_pd(C + k233 + 260);
    _t10_1 = _asm256_loadu_pd(C + k233 + 312);
    _t10_0 = _asm256_loadu_pd(C + k233 + 364);
    _t10_20 = _asm256_loadu_pd(C + k233 + 416);
    _t10_21 = _asm256_loadu_pd(C + k233 + 468);
    _t10_22 = _asm256_loadu_pd(C + k233 + 520);
    _t10_23 = _asm256_loadu_pd(C + k233 + 572);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t10_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_3), _mm256_mul_pd(_t10_18, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_1), _mm256_mul_pd(_t10_16, _t10_0)));
    _t10_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_3), _mm256_mul_pd(_t10_14, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_1), _mm256_mul_pd(_t10_12, _t10_0)));
    _t10_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_3), _mm256_mul_pd(_t10_10, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_1), _mm256_mul_pd(_t10_8, _t10_0)));
    _t10_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_3), _mm256_mul_pd(_t10_6, _t10_2)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t10_1), _mm256_mul_pd(_t10_4, _t10_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t10_20 = _mm256_sub_pd(_t10_20, _t10_24);
    _t10_21 = _mm256_sub_pd(_t10_21, _t10_25);
    _t10_22 = _mm256_sub_pd(_t10_22, _t10_26);
    _t10_23 = _mm256_sub_pd(_t10_23, _t10_27);

    // AVX Storer:
    _asm256_storeu_pd(C + k233 + 416, _t10_20);
    _asm256_storeu_pd(C + k233 + 468, _t10_21);
    _asm256_storeu_pd(C + k233 + 520, _t10_22);
    _asm256_storeu_pd(C + k233 + 572, _t10_23);
  }

  _t11_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[416])));
  _t11_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[424])));
  _t11_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[417])));
  _t11_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[418])));
  _t11_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[419])));
  _t11_76 = _asm256_loadu_pd(C + 468);
  _t11_5 = _mm256_broadcast_sd(&(L[476]));
  _t11_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[477])));
  _t11_77 = _asm256_loadu_pd(C + 520);
  _t11_3 = _mm256_maskload_pd(L + 528, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t11_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[530])));
  _t11_78 = _asm256_loadu_pd(C + 572);
  _t11_1 = _mm256_maskload_pd(L + 580, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t11_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[583])));
  _t11_79 = _asm256_loadu_pd(C + 420);
  _t11_80 = _asm256_loadu_pd(C + 472);
  _t11_81 = _asm256_loadu_pd(C + 524);
  _t11_82 = _asm256_loadu_pd(C + 576);
  _t11_83 = _asm256_loadu_pd(C + 424);
  _t11_84 = _asm256_loadu_pd(C + 476);
  _t11_85 = _asm256_loadu_pd(C + 528);
  _t11_86 = _asm256_loadu_pd(C + 580);

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_87 = _t11_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_88 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_89 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t11_90 = _mm256_add_pd(_t11_88, _t11_89);

  // 4-BLAC: 1x4 / 1x4
  _t11_91 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_87), _mm256_castpd256_pd128(_t11_90)));

  // AVX Storer:
  _t11_7 = _t11_91;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_92 = _t11_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_93 = _t11_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_94 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_95 = _mm256_mul_pd(_t11_93, _t11_94);

  // 4-BLAC: 1x4 - 1x4
  _t11_96 = _mm256_sub_pd(_t11_92, _t11_95);

  // AVX Storer:
  _t11_8 = _t11_96;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_97 = _t11_8;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_98 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_99 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t11_100 = _mm256_add_pd(_t11_98, _t11_99);

  // 4-BLAC: 1x4 / 1x4
  _t11_101 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_97), _mm256_castpd256_pd128(_t11_100)));

  // AVX Storer:
  _t11_8 = _t11_101;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_102 = _t11_9;

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_103 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_7, _t11_8), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_104 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t11_105 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_103, _t11_104), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_103, _t11_104), _mm256_mul_pd(_t11_103, _t11_104), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_103, _t11_104), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_103, _t11_104), _mm256_mul_pd(_t11_103, _t11_104), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_103, _t11_104), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_103, _t11_104), _mm256_mul_pd(_t11_103, _t11_104), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_106 = _mm256_sub_pd(_t11_102, _t11_105);

  // AVX Storer:
  _t11_9 = _t11_106;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_107 = _t11_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_108 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_109 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t11_110 = _mm256_add_pd(_t11_108, _t11_109);

  // 4-BLAC: 1x4 / 1x4
  _t11_111 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_107), _mm256_castpd256_pd128(_t11_110)));

  // AVX Storer:
  _t11_9 = _t11_111;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_112 = _t11_10;

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_113 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_8), _mm256_unpacklo_pd(_t11_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_114 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t11_115 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_113, _t11_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_113, _t11_114), _mm256_mul_pd(_t11_113, _t11_114), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_113, _t11_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_113, _t11_114), _mm256_mul_pd(_t11_113, _t11_114), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_113, _t11_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_113, _t11_114), _mm256_mul_pd(_t11_113, _t11_114), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_116 = _mm256_sub_pd(_t11_112, _t11_115);

  // AVX Storer:
  _t11_10 = _t11_116;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_117 = _t11_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_118 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_119 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t11_120 = _mm256_add_pd(_t11_118, _t11_119);

  // 4-BLAC: 1x4 / 1x4
  _t11_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_117), _mm256_castpd256_pd128(_t11_120)));

  // AVX Storer:
  _t11_10 = _t11_121;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_122 = _t11_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t11_55 = _mm256_mul_pd(_t11_122, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_8), _mm256_unpacklo_pd(_t11_9, _t11_10), 32));

  // 4-BLAC: 1x4 - 1x4
  _t11_76 = _mm256_sub_pd(_t11_76, _t11_55);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_76, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_124 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_125 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t11_126 = _mm256_add_pd(_t11_124, _t11_125);

  // 4-BLAC: 1x4 / 1x4
  _t11_127 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_123), _mm256_castpd256_pd128(_t11_126)));

  // AVX Storer:
  _t11_11 = _t11_127;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_128 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_76, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_129 = _t11_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_130 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_131 = _mm256_mul_pd(_t11_129, _t11_130);

  // 4-BLAC: 1x4 - 1x4
  _t11_132 = _mm256_sub_pd(_t11_128, _t11_131);

  // AVX Storer:
  _t11_12 = _t11_132;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_133 = _t11_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_134 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_135 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t11_136 = _mm256_add_pd(_t11_134, _t11_135);

  // 4-BLAC: 1x4 / 1x4
  _t11_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_133), _mm256_castpd256_pd128(_t11_136)));

  // AVX Storer:
  _t11_12 = _t11_137;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_76, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_76, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_11, _t11_12), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_140 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t11_141 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_139, _t11_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_139, _t11_140), _mm256_mul_pd(_t11_139, _t11_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_139, _t11_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_139, _t11_140), _mm256_mul_pd(_t11_139, _t11_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_139, _t11_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_139, _t11_140), _mm256_mul_pd(_t11_139, _t11_140), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_142 = _mm256_sub_pd(_t11_138, _t11_141);

  // AVX Storer:
  _t11_13 = _t11_142;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_143 = _t11_13;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_144 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_145 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t11_146 = _mm256_add_pd(_t11_144, _t11_145);

  // 4-BLAC: 1x4 / 1x4
  _t11_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_143), _mm256_castpd256_pd128(_t11_146)));

  // AVX Storer:
  _t11_13 = _t11_147;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_148 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_76, _t11_76, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_149 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_12), _mm256_unpacklo_pd(_t11_13, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_150 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t11_151 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_149, _t11_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_149, _t11_150), _mm256_mul_pd(_t11_149, _t11_150), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_149, _t11_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_149, _t11_150), _mm256_mul_pd(_t11_149, _t11_150), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_149, _t11_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_149, _t11_150), _mm256_mul_pd(_t11_149, _t11_150), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_152 = _mm256_sub_pd(_t11_148, _t11_151);

  // AVX Storer:
  _t11_14 = _t11_152;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_153 = _t11_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_154 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_155 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t11_156 = _mm256_add_pd(_t11_154, _t11_155);

  // 4-BLAC: 1x4 / 1x4
  _t11_157 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_153), _mm256_castpd256_pd128(_t11_156)));

  // AVX Storer:
  _t11_14 = _t11_157;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_158 = _t11_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t11_159 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_8), _mm256_unpacklo_pd(_t11_9, _t11_10), 32);
  _t11_160 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_12), _mm256_unpacklo_pd(_t11_13, _t11_14), 32);
  _t11_161 = _mm256_setzero_pd();
  _t11_162 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_58 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_158, _t11_158, 32), _mm256_permute2f128_pd(_t11_158, _t11_158, 32), 0), _t11_159), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_158, _t11_158, 32), _mm256_permute2f128_pd(_t11_158, _t11_158, 32), 15), _t11_160)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_158, _t11_158, 49), _mm256_permute2f128_pd(_t11_158, _t11_158, 49), 0), _t11_161), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_158, _t11_158, 49), _mm256_permute2f128_pd(_t11_158, _t11_158, 49), 15), _t11_162)));

  // 4-BLAC: 1x4 - 1x4
  _t11_77 = _mm256_sub_pd(_t11_77, _t11_58);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_163 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_77, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_164 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_165 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t11_166 = _mm256_add_pd(_t11_164, _t11_165);

  // 4-BLAC: 1x4 / 1x4
  _t11_167 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_163), _mm256_castpd256_pd128(_t11_166)));

  // AVX Storer:
  _t11_15 = _t11_167;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_168 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_77, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_169 = _t11_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_170 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_171 = _mm256_mul_pd(_t11_169, _t11_170);

  // 4-BLAC: 1x4 - 1x4
  _t11_172 = _mm256_sub_pd(_t11_168, _t11_171);

  // AVX Storer:
  _t11_16 = _t11_172;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_173 = _t11_16;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_174 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_175 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t11_176 = _mm256_add_pd(_t11_174, _t11_175);

  // 4-BLAC: 1x4 / 1x4
  _t11_177 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_173), _mm256_castpd256_pd128(_t11_176)));

  // AVX Storer:
  _t11_16 = _t11_177;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_178 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_77, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_77, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_179 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_15, _t11_16), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_180 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t11_181 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_179, _t11_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_179, _t11_180), _mm256_mul_pd(_t11_179, _t11_180), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_179, _t11_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_179, _t11_180), _mm256_mul_pd(_t11_179, _t11_180), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_179, _t11_180), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_179, _t11_180), _mm256_mul_pd(_t11_179, _t11_180), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_182 = _mm256_sub_pd(_t11_178, _t11_181);

  // AVX Storer:
  _t11_17 = _t11_182;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_183 = _t11_17;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_184 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_185 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t11_186 = _mm256_add_pd(_t11_184, _t11_185);

  // 4-BLAC: 1x4 / 1x4
  _t11_187 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_183), _mm256_castpd256_pd128(_t11_186)));

  // AVX Storer:
  _t11_17 = _t11_187;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_188 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_77, _t11_77, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_189 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_15, _t11_16), _mm256_unpacklo_pd(_t11_17, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_190 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t11_191 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_189, _t11_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_189, _t11_190), _mm256_mul_pd(_t11_189, _t11_190), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_189, _t11_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_189, _t11_190), _mm256_mul_pd(_t11_189, _t11_190), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_189, _t11_190), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_189, _t11_190), _mm256_mul_pd(_t11_189, _t11_190), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_192 = _mm256_sub_pd(_t11_188, _t11_191);

  // AVX Storer:
  _t11_18 = _t11_192;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_193 = _t11_18;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_194 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_195 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t11_196 = _mm256_add_pd(_t11_194, _t11_195);

  // 4-BLAC: 1x4 / 1x4
  _t11_197 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_193), _mm256_castpd256_pd128(_t11_196)));

  // AVX Storer:
  _t11_18 = _t11_197;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_198 = _t11_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t11_199 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_8), _mm256_unpacklo_pd(_t11_9, _t11_10), 32);
  _t11_200 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_11, _t11_12), _mm256_unpacklo_pd(_t11_13, _t11_14), 32);
  _t11_201 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_15, _t11_16), _mm256_unpacklo_pd(_t11_17, _t11_18), 32);
  _t11_202 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_59 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_198, _t11_198, 32), _mm256_permute2f128_pd(_t11_198, _t11_198, 32), 0), _t11_199), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_198, _t11_198, 32), _mm256_permute2f128_pd(_t11_198, _t11_198, 32), 15), _t11_200)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_198, _t11_198, 49), _mm256_permute2f128_pd(_t11_198, _t11_198, 49), 0), _t11_201), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_198, _t11_198, 49), _mm256_permute2f128_pd(_t11_198, _t11_198, 49), 15), _t11_202)));

  // 4-BLAC: 1x4 - 1x4
  _t11_78 = _mm256_sub_pd(_t11_78, _t11_59);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_203 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_78, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_204 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_205 = _t0_38;

  // 4-BLAC: 1x4 + 1x4
  _t11_206 = _mm256_add_pd(_t11_204, _t11_205);

  // 4-BLAC: 1x4 / 1x4
  _t11_207 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_203), _mm256_castpd256_pd128(_t11_206)));

  // AVX Storer:
  _t11_19 = _t11_207;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_208 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_78, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_209 = _t11_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_210 = _t0_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_211 = _mm256_mul_pd(_t11_209, _t11_210);

  // 4-BLAC: 1x4 - 1x4
  _t11_212 = _mm256_sub_pd(_t11_208, _t11_211);

  // AVX Storer:
  _t11_20 = _t11_212;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_213 = _t11_20;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_214 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_215 = _t0_36;

  // 4-BLAC: 1x4 + 1x4
  _t11_216 = _mm256_add_pd(_t11_214, _t11_215);

  // 4-BLAC: 1x4 / 1x4
  _t11_217 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_213), _mm256_castpd256_pd128(_t11_216)));

  // AVX Storer:
  _t11_20 = _t11_217;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_218 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_78, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_78, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_219 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_19, _t11_20), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_220 = _t0_35;

  // 4-BLAC: 1x4 * 4x1
  _t11_221 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_219, _t11_220), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_219, _t11_220), _mm256_mul_pd(_t11_219, _t11_220), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_219, _t11_220), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_219, _t11_220), _mm256_mul_pd(_t11_219, _t11_220), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_219, _t11_220), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_219, _t11_220), _mm256_mul_pd(_t11_219, _t11_220), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_222 = _mm256_sub_pd(_t11_218, _t11_221);

  // AVX Storer:
  _t11_21 = _t11_222;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_223 = _t11_21;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_224 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_225 = _t0_34;

  // 4-BLAC: 1x4 + 1x4
  _t11_226 = _mm256_add_pd(_t11_224, _t11_225);

  // 4-BLAC: 1x4 / 1x4
  _t11_227 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_223), _mm256_castpd256_pd128(_t11_226)));

  // AVX Storer:
  _t11_21 = _t11_227;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_228 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_78, _t11_78, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_229 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_19, _t11_20), _mm256_unpacklo_pd(_t11_21, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_230 = _t0_33;

  // 4-BLAC: 1x4 * 4x1
  _t11_231 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_229, _t11_230), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_229, _t11_230), _mm256_mul_pd(_t11_229, _t11_230), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_229, _t11_230), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_229, _t11_230), _mm256_mul_pd(_t11_229, _t11_230), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_229, _t11_230), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_229, _t11_230), _mm256_mul_pd(_t11_229, _t11_230), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_232 = _mm256_sub_pd(_t11_228, _t11_231);

  // AVX Storer:
  _t11_22 = _t11_232;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_233 = _t11_22;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_234 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_235 = _t0_32;

  // 4-BLAC: 1x4 + 1x4
  _t11_236 = _mm256_add_pd(_t11_234, _t11_235);

  // 4-BLAC: 1x4 / 1x4
  _t11_237 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_233), _mm256_castpd256_pd128(_t11_236)));

  // AVX Storer:
  _t11_22 = _t11_237;

  // Generating : X[52,52] = S(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, fi879), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t11_60 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_7, _t11_7, 32), _mm256_permute2f128_pd(_t11_7, _t11_7, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_8, _t11_8, 32), _mm256_permute2f128_pd(_t11_8, _t11_8, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_9, _t11_9, 32), _mm256_permute2f128_pd(_t11_9, _t11_9, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_10, _t11_10, 32), _mm256_permute2f128_pd(_t11_10, _t11_10, 32), 0), _t0_22)));
  _t11_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_11, _t11_11, 32), _mm256_permute2f128_pd(_t11_11, _t11_11, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_12, _t11_12, 32), _mm256_permute2f128_pd(_t11_12, _t11_12, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_13, _t11_13, 32), _mm256_permute2f128_pd(_t11_13, _t11_13, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_14, _t11_14, 32), _mm256_permute2f128_pd(_t11_14, _t11_14, 32), 0), _t0_22)));
  _t11_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_15, _t11_15, 32), _mm256_permute2f128_pd(_t11_15, _t11_15, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_16, _t11_16, 32), _mm256_permute2f128_pd(_t11_16, _t11_16, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_17, _t11_17, 32), _mm256_permute2f128_pd(_t11_17, _t11_17, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_18, _t11_18, 32), _mm256_permute2f128_pd(_t11_18, _t11_18, 32), 0), _t0_22)));
  _t11_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_19, _t11_19, 32), _mm256_permute2f128_pd(_t11_19, _t11_19, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_20, _t11_20, 32), _mm256_permute2f128_pd(_t11_20, _t11_20, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_21, _t11_21, 32), _mm256_permute2f128_pd(_t11_21, _t11_21, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_22, _t11_22, 32), _mm256_permute2f128_pd(_t11_22, _t11_22, 32), 0), _t0_22)));

  // 4-BLAC: 4x4 - 4x4
  _t11_79 = _mm256_sub_pd(_t11_79, _t11_60);
  _t11_80 = _mm256_sub_pd(_t11_80, _t11_61);
  _t11_81 = _mm256_sub_pd(_t11_81, _t11_62);
  _t11_82 = _mm256_sub_pd(_t11_82, _t11_63);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_238 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_79, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_239 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_240 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t11_241 = _mm256_add_pd(_t11_239, _t11_240);

  // 4-BLAC: 1x4 / 1x4
  _t11_242 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_238), _mm256_castpd256_pd128(_t11_241)));

  // AVX Storer:
  _t11_23 = _t11_242;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_243 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_79, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_244 = _t11_23;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_245 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_246 = _mm256_mul_pd(_t11_244, _t11_245);

  // 4-BLAC: 1x4 - 1x4
  _t11_247 = _mm256_sub_pd(_t11_243, _t11_246);

  // AVX Storer:
  _t11_24 = _t11_247;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_248 = _t11_24;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_249 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_250 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t11_251 = _mm256_add_pd(_t11_249, _t11_250);

  // 4-BLAC: 1x4 / 1x4
  _t11_252 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_248), _mm256_castpd256_pd128(_t11_251)));

  // AVX Storer:
  _t11_24 = _t11_252;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_253 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_79, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_79, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_254 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_23, _t11_24), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_255 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t11_256 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_254, _t11_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_254, _t11_255), _mm256_mul_pd(_t11_254, _t11_255), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_254, _t11_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_254, _t11_255), _mm256_mul_pd(_t11_254, _t11_255), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_254, _t11_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_254, _t11_255), _mm256_mul_pd(_t11_254, _t11_255), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_257 = _mm256_sub_pd(_t11_253, _t11_256);

  // AVX Storer:
  _t11_25 = _t11_257;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_258 = _t11_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_259 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_260 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t11_261 = _mm256_add_pd(_t11_259, _t11_260);

  // 4-BLAC: 1x4 / 1x4
  _t11_262 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_258), _mm256_castpd256_pd128(_t11_261)));

  // AVX Storer:
  _t11_25 = _t11_262;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_263 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_79, _t11_79, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_264 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_23, _t11_24), _mm256_unpacklo_pd(_t11_25, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_265 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t11_266 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_264, _t11_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_264, _t11_265), _mm256_mul_pd(_t11_264, _t11_265), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_264, _t11_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_264, _t11_265), _mm256_mul_pd(_t11_264, _t11_265), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_264, _t11_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_264, _t11_265), _mm256_mul_pd(_t11_264, _t11_265), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_267 = _mm256_sub_pd(_t11_263, _t11_266);

  // AVX Storer:
  _t11_26 = _t11_267;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_268 = _t11_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_269 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_270 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t11_271 = _mm256_add_pd(_t11_269, _t11_270);

  // 4-BLAC: 1x4 / 1x4
  _t11_272 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_268), _mm256_castpd256_pd128(_t11_271)));

  // AVX Storer:
  _t11_26 = _t11_272;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_273 = _t11_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t11_56 = _mm256_mul_pd(_t11_273, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_23, _t11_24), _mm256_unpacklo_pd(_t11_25, _t11_26), 32));

  // 4-BLAC: 1x4 - 1x4
  _t11_80 = _mm256_sub_pd(_t11_80, _t11_56);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_274 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_80, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_275 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_276 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t11_277 = _mm256_add_pd(_t11_275, _t11_276);

  // 4-BLAC: 1x4 / 1x4
  _t11_278 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_274), _mm256_castpd256_pd128(_t11_277)));

  // AVX Storer:
  _t11_27 = _t11_278;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_279 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_80, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_280 = _t11_27;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_281 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_282 = _mm256_mul_pd(_t11_280, _t11_281);

  // 4-BLAC: 1x4 - 1x4
  _t11_283 = _mm256_sub_pd(_t11_279, _t11_282);

  // AVX Storer:
  _t11_28 = _t11_283;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_284 = _t11_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_285 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_286 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t11_287 = _mm256_add_pd(_t11_285, _t11_286);

  // 4-BLAC: 1x4 / 1x4
  _t11_288 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_284), _mm256_castpd256_pd128(_t11_287)));

  // AVX Storer:
  _t11_28 = _t11_288;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_289 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_80, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_80, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_290 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_27, _t11_28), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_291 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t11_292 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_290, _t11_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_290, _t11_291), _mm256_mul_pd(_t11_290, _t11_291), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_290, _t11_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_290, _t11_291), _mm256_mul_pd(_t11_290, _t11_291), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_290, _t11_291), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_290, _t11_291), _mm256_mul_pd(_t11_290, _t11_291), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_293 = _mm256_sub_pd(_t11_289, _t11_292);

  // AVX Storer:
  _t11_29 = _t11_293;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_294 = _t11_29;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_295 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_296 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t11_297 = _mm256_add_pd(_t11_295, _t11_296);

  // 4-BLAC: 1x4 / 1x4
  _t11_298 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_294), _mm256_castpd256_pd128(_t11_297)));

  // AVX Storer:
  _t11_29 = _t11_298;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_299 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_80, _t11_80, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_300 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_27, _t11_28), _mm256_unpacklo_pd(_t11_29, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_301 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t11_302 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_300, _t11_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_300, _t11_301), _mm256_mul_pd(_t11_300, _t11_301), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_300, _t11_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_300, _t11_301), _mm256_mul_pd(_t11_300, _t11_301), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_300, _t11_301), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_300, _t11_301), _mm256_mul_pd(_t11_300, _t11_301), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_303 = _mm256_sub_pd(_t11_299, _t11_302);

  // AVX Storer:
  _t11_30 = _t11_303;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_304 = _t11_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_305 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_306 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t11_307 = _mm256_add_pd(_t11_305, _t11_306);

  // 4-BLAC: 1x4 / 1x4
  _t11_308 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_304), _mm256_castpd256_pd128(_t11_307)));

  // AVX Storer:
  _t11_30 = _t11_308;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_309 = _t11_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t11_310 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_23, _t11_24), _mm256_unpacklo_pd(_t11_25, _t11_26), 32);
  _t11_311 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_27, _t11_28), _mm256_unpacklo_pd(_t11_29, _t11_30), 32);
  _t11_312 = _mm256_setzero_pd();
  _t11_313 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_309, _t11_309, 32), _mm256_permute2f128_pd(_t11_309, _t11_309, 32), 0), _t11_310), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_309, _t11_309, 32), _mm256_permute2f128_pd(_t11_309, _t11_309, 32), 15), _t11_311)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_309, _t11_309, 49), _mm256_permute2f128_pd(_t11_309, _t11_309, 49), 0), _t11_312), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_309, _t11_309, 49), _mm256_permute2f128_pd(_t11_309, _t11_309, 49), 15), _t11_313)));

  // 4-BLAC: 1x4 - 1x4
  _t11_81 = _mm256_sub_pd(_t11_81, _t11_64);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_314 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_81, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_315 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_316 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t11_317 = _mm256_add_pd(_t11_315, _t11_316);

  // 4-BLAC: 1x4 / 1x4
  _t11_318 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_314), _mm256_castpd256_pd128(_t11_317)));

  // AVX Storer:
  _t11_31 = _t11_318;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_319 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_81, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_320 = _t11_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_321 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_322 = _mm256_mul_pd(_t11_320, _t11_321);

  // 4-BLAC: 1x4 - 1x4
  _t11_323 = _mm256_sub_pd(_t11_319, _t11_322);

  // AVX Storer:
  _t11_32 = _t11_323;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_324 = _t11_32;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_325 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_326 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t11_327 = _mm256_add_pd(_t11_325, _t11_326);

  // 4-BLAC: 1x4 / 1x4
  _t11_328 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_324), _mm256_castpd256_pd128(_t11_327)));

  // AVX Storer:
  _t11_32 = _t11_328;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_329 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_81, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_81, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_330 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_31, _t11_32), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_331 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t11_332 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_330, _t11_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_330, _t11_331), _mm256_mul_pd(_t11_330, _t11_331), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_330, _t11_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_330, _t11_331), _mm256_mul_pd(_t11_330, _t11_331), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_330, _t11_331), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_330, _t11_331), _mm256_mul_pd(_t11_330, _t11_331), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_333 = _mm256_sub_pd(_t11_329, _t11_332);

  // AVX Storer:
  _t11_33 = _t11_333;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_334 = _t11_33;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_335 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_336 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t11_337 = _mm256_add_pd(_t11_335, _t11_336);

  // 4-BLAC: 1x4 / 1x4
  _t11_338 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_334), _mm256_castpd256_pd128(_t11_337)));

  // AVX Storer:
  _t11_33 = _t11_338;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_339 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_81, _t11_81, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_340 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_31, _t11_32), _mm256_unpacklo_pd(_t11_33, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_341 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t11_342 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_340, _t11_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_340, _t11_341), _mm256_mul_pd(_t11_340, _t11_341), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_340, _t11_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_340, _t11_341), _mm256_mul_pd(_t11_340, _t11_341), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_340, _t11_341), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_340, _t11_341), _mm256_mul_pd(_t11_340, _t11_341), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_343 = _mm256_sub_pd(_t11_339, _t11_342);

  // AVX Storer:
  _t11_34 = _t11_343;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_344 = _t11_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_345 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_346 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t11_347 = _mm256_add_pd(_t11_345, _t11_346);

  // 4-BLAC: 1x4 / 1x4
  _t11_348 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_344), _mm256_castpd256_pd128(_t11_347)));

  // AVX Storer:
  _t11_34 = _t11_348;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_349 = _t11_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t11_350 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_23, _t11_24), _mm256_unpacklo_pd(_t11_25, _t11_26), 32);
  _t11_351 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_27, _t11_28), _mm256_unpacklo_pd(_t11_29, _t11_30), 32);
  _t11_352 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_31, _t11_32), _mm256_unpacklo_pd(_t11_33, _t11_34), 32);
  _t11_353 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_349, _t11_349, 32), _mm256_permute2f128_pd(_t11_349, _t11_349, 32), 0), _t11_350), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_349, _t11_349, 32), _mm256_permute2f128_pd(_t11_349, _t11_349, 32), 15), _t11_351)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_349, _t11_349, 49), _mm256_permute2f128_pd(_t11_349, _t11_349, 49), 0), _t11_352), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_349, _t11_349, 49), _mm256_permute2f128_pd(_t11_349, _t11_349, 49), 15), _t11_353)));

  // 4-BLAC: 1x4 - 1x4
  _t11_82 = _mm256_sub_pd(_t11_82, _t11_65);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_354 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_82, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_355 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_356 = _t0_21;

  // 4-BLAC: 1x4 + 1x4
  _t11_357 = _mm256_add_pd(_t11_355, _t11_356);

  // 4-BLAC: 1x4 / 1x4
  _t11_358 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_354), _mm256_castpd256_pd128(_t11_357)));

  // AVX Storer:
  _t11_35 = _t11_358;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_359 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_82, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_360 = _t11_35;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_361 = _t0_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_362 = _mm256_mul_pd(_t11_360, _t11_361);

  // 4-BLAC: 1x4 - 1x4
  _t11_363 = _mm256_sub_pd(_t11_359, _t11_362);

  // AVX Storer:
  _t11_36 = _t11_363;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_364 = _t11_36;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_365 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_366 = _t0_19;

  // 4-BLAC: 1x4 + 1x4
  _t11_367 = _mm256_add_pd(_t11_365, _t11_366);

  // 4-BLAC: 1x4 / 1x4
  _t11_368 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_364), _mm256_castpd256_pd128(_t11_367)));

  // AVX Storer:
  _t11_36 = _t11_368;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_369 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_82, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_82, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_370 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_35, _t11_36), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_371 = _t0_18;

  // 4-BLAC: 1x4 * 4x1
  _t11_372 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_370, _t11_371), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_370, _t11_371), _mm256_mul_pd(_t11_370, _t11_371), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_370, _t11_371), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_370, _t11_371), _mm256_mul_pd(_t11_370, _t11_371), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_370, _t11_371), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_370, _t11_371), _mm256_mul_pd(_t11_370, _t11_371), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_373 = _mm256_sub_pd(_t11_369, _t11_372);

  // AVX Storer:
  _t11_37 = _t11_373;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_374 = _t11_37;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_375 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_376 = _t0_17;

  // 4-BLAC: 1x4 + 1x4
  _t11_377 = _mm256_add_pd(_t11_375, _t11_376);

  // 4-BLAC: 1x4 / 1x4
  _t11_378 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_374), _mm256_castpd256_pd128(_t11_377)));

  // AVX Storer:
  _t11_37 = _t11_378;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_379 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_82, _t11_82, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_380 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_35, _t11_36), _mm256_unpacklo_pd(_t11_37, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_381 = _t0_16;

  // 4-BLAC: 1x4 * 4x1
  _t11_382 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_380, _t11_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_380, _t11_381), _mm256_mul_pd(_t11_380, _t11_381), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_380, _t11_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_380, _t11_381), _mm256_mul_pd(_t11_380, _t11_381), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_380, _t11_381), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_380, _t11_381), _mm256_mul_pd(_t11_380, _t11_381), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_383 = _mm256_sub_pd(_t11_379, _t11_382);

  // AVX Storer:
  _t11_38 = _t11_383;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_384 = _t11_38;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_385 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_386 = _t0_15;

  // 4-BLAC: 1x4 + 1x4
  _t11_387 = _mm256_add_pd(_t11_385, _t11_386);

  // 4-BLAC: 1x4 / 1x4
  _t11_388 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_384), _mm256_castpd256_pd128(_t11_387)));

  // AVX Storer:
  _t11_38 = _t11_388;

  // Generating : X[52,52] = ( S(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, fi1082)) - ( G(h(4, 52, fi879), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082)) + Sum_{k116} ( -$(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi1082)) ),h(4, 52, fi1082)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t11_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_7, _t11_7, 32), _mm256_permute2f128_pd(_t11_7, _t11_7, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_8, _t11_8, 32), _mm256_permute2f128_pd(_t11_8, _t11_8, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_9, _t11_9, 32), _mm256_permute2f128_pd(_t11_9, _t11_9, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_10, _t11_10, 32), _mm256_permute2f128_pd(_t11_10, _t11_10, 32), 0), _t0_11)));
  _t11_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_11, _t11_11, 32), _mm256_permute2f128_pd(_t11_11, _t11_11, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_12, _t11_12, 32), _mm256_permute2f128_pd(_t11_12, _t11_12, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_13, _t11_13, 32), _mm256_permute2f128_pd(_t11_13, _t11_13, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_14, _t11_14, 32), _mm256_permute2f128_pd(_t11_14, _t11_14, 32), 0), _t0_11)));
  _t11_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_15, _t11_15, 32), _mm256_permute2f128_pd(_t11_15, _t11_15, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_16, _t11_16, 32), _mm256_permute2f128_pd(_t11_16, _t11_16, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_17, _t11_17, 32), _mm256_permute2f128_pd(_t11_17, _t11_17, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_18, _t11_18, 32), _mm256_permute2f128_pd(_t11_18, _t11_18, 32), 0), _t0_11)));
  _t11_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_19, _t11_19, 32), _mm256_permute2f128_pd(_t11_19, _t11_19, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_20, _t11_20, 32), _mm256_permute2f128_pd(_t11_20, _t11_20, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_21, _t11_21, 32), _mm256_permute2f128_pd(_t11_21, _t11_21, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_22, _t11_22, 32), _mm256_permute2f128_pd(_t11_22, _t11_22, 32), 0), _t0_11)));

  // 4-BLAC: 4x4 - 4x4
  _t11_83 = _mm256_sub_pd(_t11_83, _t11_66);
  _t11_84 = _mm256_sub_pd(_t11_84, _t11_67);
  _t11_85 = _mm256_sub_pd(_t11_85, _t11_68);
  _t11_86 = _mm256_sub_pd(_t11_86, _t11_69);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t11_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_23, _t11_23, 32), _mm256_permute2f128_pd(_t11_23, _t11_23, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_24, _t11_24, 32), _mm256_permute2f128_pd(_t11_24, _t11_24, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_25, _t11_25, 32), _mm256_permute2f128_pd(_t11_25, _t11_25, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_26, _t11_26, 32), _mm256_permute2f128_pd(_t11_26, _t11_26, 32), 0), _t0_7)));
  _t11_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_27, _t11_27, 32), _mm256_permute2f128_pd(_t11_27, _t11_27, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_28, _t11_28, 32), _mm256_permute2f128_pd(_t11_28, _t11_28, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_29, _t11_29, 32), _mm256_permute2f128_pd(_t11_29, _t11_29, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_30, _t11_30, 32), _mm256_permute2f128_pd(_t11_30, _t11_30, 32), 0), _t0_7)));
  _t11_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_31, _t11_31, 32), _mm256_permute2f128_pd(_t11_31, _t11_31, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_32, _t11_32, 32), _mm256_permute2f128_pd(_t11_32, _t11_32, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_33, _t11_33, 32), _mm256_permute2f128_pd(_t11_33, _t11_33, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_34, _t11_34, 32), _mm256_permute2f128_pd(_t11_34, _t11_34, 32), 0), _t0_7)));
  _t11_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_35, _t11_35, 32), _mm256_permute2f128_pd(_t11_35, _t11_35, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_36, _t11_36, 32), _mm256_permute2f128_pd(_t11_36, _t11_36, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_37, _t11_37, 32), _mm256_permute2f128_pd(_t11_37, _t11_37, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_38, _t11_38, 32), _mm256_permute2f128_pd(_t11_38, _t11_38, 32), 0), _t0_7)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t11_83 = _mm256_sub_pd(_t11_83, _t11_70);
  _t11_84 = _mm256_sub_pd(_t11_84, _t11_71);
  _t11_85 = _mm256_sub_pd(_t11_85, _t11_72);
  _t11_86 = _mm256_sub_pd(_t11_86, _t11_73);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_389 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_83, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_390 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_391 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_392 = _mm256_add_pd(_t11_390, _t11_391);

  // 4-BLAC: 1x4 / 1x4
  _t11_393 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_389), _mm256_castpd256_pd128(_t11_392)));

  // AVX Storer:
  _t11_39 = _t11_393;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_394 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_83, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_395 = _t11_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_396 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_397 = _mm256_mul_pd(_t11_395, _t11_396);

  // 4-BLAC: 1x4 - 1x4
  _t11_398 = _mm256_sub_pd(_t11_394, _t11_397);

  // AVX Storer:
  _t11_40 = _t11_398;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_399 = _t11_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_400 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_401 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t11_402 = _mm256_add_pd(_t11_400, _t11_401);

  // 4-BLAC: 1x4 / 1x4
  _t11_403 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_399), _mm256_castpd256_pd128(_t11_402)));

  // AVX Storer:
  _t11_40 = _t11_403;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_404 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_83, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_83, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_405 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_39, _t11_40), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_406 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t11_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_405, _t11_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_405, _t11_406), _mm256_mul_pd(_t11_405, _t11_406), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_405, _t11_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_405, _t11_406), _mm256_mul_pd(_t11_405, _t11_406), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_405, _t11_406), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_405, _t11_406), _mm256_mul_pd(_t11_405, _t11_406), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_408 = _mm256_sub_pd(_t11_404, _t11_407);

  // AVX Storer:
  _t11_41 = _t11_408;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_409 = _t11_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_410 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_411 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t11_412 = _mm256_add_pd(_t11_410, _t11_411);

  // 4-BLAC: 1x4 / 1x4
  _t11_413 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_409), _mm256_castpd256_pd128(_t11_412)));

  // AVX Storer:
  _t11_41 = _t11_413;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_414 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_83, _t11_83, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_415 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_39, _t11_40), _mm256_unpacklo_pd(_t11_41, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_416 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t11_417 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_415, _t11_416), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_415, _t11_416), _mm256_mul_pd(_t11_415, _t11_416), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_415, _t11_416), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_415, _t11_416), _mm256_mul_pd(_t11_415, _t11_416), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_415, _t11_416), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_415, _t11_416), _mm256_mul_pd(_t11_415, _t11_416), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_418 = _mm256_sub_pd(_t11_414, _t11_417);

  // AVX Storer:
  _t11_42 = _t11_418;

  // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_419 = _t11_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_420 = _t11_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_421 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t11_422 = _mm256_add_pd(_t11_420, _t11_421);

  // 4-BLAC: 1x4 / 1x4
  _t11_423 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_419), _mm256_castpd256_pd128(_t11_422)));

  // AVX Storer:
  _t11_42 = _t11_423;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_424 = _t11_5;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t11_57 = _mm256_mul_pd(_t11_424, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_39, _t11_40), _mm256_unpacklo_pd(_t11_41, _t11_42), 32));

  // 4-BLAC: 1x4 - 1x4
  _t11_84 = _mm256_sub_pd(_t11_84, _t11_57);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_425 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_84, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_426 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_427 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_428 = _mm256_add_pd(_t11_426, _t11_427);

  // 4-BLAC: 1x4 / 1x4
  _t11_429 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_425), _mm256_castpd256_pd128(_t11_428)));

  // AVX Storer:
  _t11_43 = _t11_429;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_430 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_84, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_431 = _t11_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_432 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_433 = _mm256_mul_pd(_t11_431, _t11_432);

  // 4-BLAC: 1x4 - 1x4
  _t11_434 = _mm256_sub_pd(_t11_430, _t11_433);

  // AVX Storer:
  _t11_44 = _t11_434;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_435 = _t11_44;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_436 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_437 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t11_438 = _mm256_add_pd(_t11_436, _t11_437);

  // 4-BLAC: 1x4 / 1x4
  _t11_439 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_435), _mm256_castpd256_pd128(_t11_438)));

  // AVX Storer:
  _t11_44 = _t11_439;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_440 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_84, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_84, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_441 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_43, _t11_44), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_442 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t11_443 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_441, _t11_442), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_441, _t11_442), _mm256_mul_pd(_t11_441, _t11_442), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_441, _t11_442), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_441, _t11_442), _mm256_mul_pd(_t11_441, _t11_442), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_441, _t11_442), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_441, _t11_442), _mm256_mul_pd(_t11_441, _t11_442), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_444 = _mm256_sub_pd(_t11_440, _t11_443);

  // AVX Storer:
  _t11_45 = _t11_444;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_445 = _t11_45;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_446 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_447 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t11_448 = _mm256_add_pd(_t11_446, _t11_447);

  // 4-BLAC: 1x4 / 1x4
  _t11_449 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_445), _mm256_castpd256_pd128(_t11_448)));

  // AVX Storer:
  _t11_45 = _t11_449;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_450 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_84, _t11_84, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_451 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_43, _t11_44), _mm256_unpacklo_pd(_t11_45, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_452 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t11_453 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_451, _t11_452), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_451, _t11_452), _mm256_mul_pd(_t11_451, _t11_452), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_451, _t11_452), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_451, _t11_452), _mm256_mul_pd(_t11_451, _t11_452), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_451, _t11_452), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_451, _t11_452), _mm256_mul_pd(_t11_451, _t11_452), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_454 = _mm256_sub_pd(_t11_450, _t11_453);

  // AVX Storer:
  _t11_46 = _t11_454;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_455 = _t11_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_456 = _t11_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_457 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t11_458 = _mm256_add_pd(_t11_456, _t11_457);

  // 4-BLAC: 1x4 / 1x4
  _t11_459 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_455), _mm256_castpd256_pd128(_t11_458)));

  // AVX Storer:
  _t11_46 = _t11_459;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

  // AVX Loader:

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_460 = _t11_3;

  // AVX Loader:

  // 2x4 -> 4x4
  _t11_461 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_39, _t11_40), _mm256_unpacklo_pd(_t11_41, _t11_42), 32);
  _t11_462 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_43, _t11_44), _mm256_unpacklo_pd(_t11_45, _t11_46), 32);
  _t11_463 = _mm256_setzero_pd();
  _t11_464 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_460, _t11_460, 32), _mm256_permute2f128_pd(_t11_460, _t11_460, 32), 0), _t11_461), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_460, _t11_460, 32), _mm256_permute2f128_pd(_t11_460, _t11_460, 32), 15), _t11_462)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_460, _t11_460, 49), _mm256_permute2f128_pd(_t11_460, _t11_460, 49), 0), _t11_463), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_460, _t11_460, 49), _mm256_permute2f128_pd(_t11_460, _t11_460, 49), 15), _t11_464)));

  // 4-BLAC: 1x4 - 1x4
  _t11_85 = _mm256_sub_pd(_t11_85, _t11_74);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_465 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_85, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_466 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_467 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_468 = _mm256_add_pd(_t11_466, _t11_467);

  // 4-BLAC: 1x4 / 1x4
  _t11_469 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_465), _mm256_castpd256_pd128(_t11_468)));

  // AVX Storer:
  _t11_47 = _t11_469;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_470 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_85, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_471 = _t11_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_472 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_473 = _mm256_mul_pd(_t11_471, _t11_472);

  // 4-BLAC: 1x4 - 1x4
  _t11_474 = _mm256_sub_pd(_t11_470, _t11_473);

  // AVX Storer:
  _t11_48 = _t11_474;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_475 = _t11_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_476 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_477 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t11_478 = _mm256_add_pd(_t11_476, _t11_477);

  // 4-BLAC: 1x4 / 1x4
  _t11_479 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_475), _mm256_castpd256_pd128(_t11_478)));

  // AVX Storer:
  _t11_48 = _t11_479;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_480 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_85, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_85, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_481 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_47, _t11_48), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_482 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t11_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_481, _t11_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_481, _t11_482), _mm256_mul_pd(_t11_481, _t11_482), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_481, _t11_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_481, _t11_482), _mm256_mul_pd(_t11_481, _t11_482), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_481, _t11_482), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_481, _t11_482), _mm256_mul_pd(_t11_481, _t11_482), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_484 = _mm256_sub_pd(_t11_480, _t11_483);

  // AVX Storer:
  _t11_49 = _t11_484;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_485 = _t11_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_486 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_487 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t11_488 = _mm256_add_pd(_t11_486, _t11_487);

  // 4-BLAC: 1x4 / 1x4
  _t11_489 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_485), _mm256_castpd256_pd128(_t11_488)));

  // AVX Storer:
  _t11_49 = _t11_489;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_490 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_85, _t11_85, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_491 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_47, _t11_48), _mm256_unpacklo_pd(_t11_49, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_492 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t11_493 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_491, _t11_492), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_491, _t11_492), _mm256_mul_pd(_t11_491, _t11_492), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_491, _t11_492), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_491, _t11_492), _mm256_mul_pd(_t11_491, _t11_492), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_491, _t11_492), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_491, _t11_492), _mm256_mul_pd(_t11_491, _t11_492), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_494 = _mm256_sub_pd(_t11_490, _t11_493);

  // AVX Storer:
  _t11_50 = _t11_494;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_495 = _t11_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_496 = _t11_2;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_497 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t11_498 = _mm256_add_pd(_t11_496, _t11_497);

  // 4-BLAC: 1x4 / 1x4
  _t11_499 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_495), _mm256_castpd256_pd128(_t11_498)));

  // AVX Storer:
  _t11_50 = _t11_499;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

  // AVX Loader:

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_500 = _t11_1;

  // AVX Loader:

  // 3x4 -> 4x4
  _t11_501 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_39, _t11_40), _mm256_unpacklo_pd(_t11_41, _t11_42), 32);
  _t11_502 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_43, _t11_44), _mm256_unpacklo_pd(_t11_45, _t11_46), 32);
  _t11_503 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_47, _t11_48), _mm256_unpacklo_pd(_t11_49, _t11_50), 32);
  _t11_504 = _mm256_setzero_pd();

  // 4-BLAC: 1x4 * 4x4
  _t11_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_500, _t11_500, 32), _mm256_permute2f128_pd(_t11_500, _t11_500, 32), 0), _t11_501), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_500, _t11_500, 32), _mm256_permute2f128_pd(_t11_500, _t11_500, 32), 15), _t11_502)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_500, _t11_500, 49), _mm256_permute2f128_pd(_t11_500, _t11_500, 49), 0), _t11_503), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_500, _t11_500, 49), _mm256_permute2f128_pd(_t11_500, _t11_500, 49), 15), _t11_504)));

  // 4-BLAC: 1x4 - 1x4
  _t11_86 = _mm256_sub_pd(_t11_86, _t11_75);

  // AVX Storer:

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_505 = _mm256_blend_pd(_mm256_setzero_pd(), _t11_86, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_506 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_507 = _t0_6;

  // 4-BLAC: 1x4 + 1x4
  _t11_508 = _mm256_add_pd(_t11_506, _t11_507);

  // 4-BLAC: 1x4 / 1x4
  _t11_509 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_505), _mm256_castpd256_pd128(_t11_508)));

  // AVX Storer:
  _t11_51 = _t11_509;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_510 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_86, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_511 = _t11_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_512 = _t0_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t11_513 = _mm256_mul_pd(_t11_511, _t11_512);

  // 4-BLAC: 1x4 - 1x4
  _t11_514 = _mm256_sub_pd(_t11_510, _t11_513);

  // AVX Storer:
  _t11_52 = _t11_514;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_515 = _t11_52;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_516 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_517 = _t0_4;

  // 4-BLAC: 1x4 + 1x4
  _t11_518 = _mm256_add_pd(_t11_516, _t11_517);

  // 4-BLAC: 1x4 / 1x4
  _t11_519 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_515), _mm256_castpd256_pd128(_t11_518)));

  // AVX Storer:
  _t11_52 = _t11_519;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_520 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t11_86, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t11_86, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t11_521 = _mm256_blend_pd(_mm256_unpacklo_pd(_t11_51, _t11_52), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t11_522 = _t0_3;

  // 4-BLAC: 1x4 * 4x1
  _t11_523 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_521, _t11_522), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_521, _t11_522), _mm256_mul_pd(_t11_521, _t11_522), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_521, _t11_522), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_521, _t11_522), _mm256_mul_pd(_t11_521, _t11_522), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_521, _t11_522), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_521, _t11_522), _mm256_mul_pd(_t11_521, _t11_522), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_524 = _mm256_sub_pd(_t11_520, _t11_523);

  // AVX Storer:
  _t11_53 = _t11_524;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_525 = _t11_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_526 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_527 = _t0_2;

  // 4-BLAC: 1x4 + 1x4
  _t11_528 = _mm256_add_pd(_t11_526, _t11_527);

  // 4-BLAC: 1x4 / 1x4
  _t11_529 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_525), _mm256_castpd256_pd128(_t11_528)));

  // AVX Storer:
  _t11_53 = _t11_529;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_530 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t11_86, _t11_86, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t11_531 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_51, _t11_52), _mm256_unpacklo_pd(_t11_53, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t11_532 = _t0_1;

  // 4-BLAC: 1x4 * 4x1
  _t11_533 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t11_531, _t11_532), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_531, _t11_532), _mm256_mul_pd(_t11_531, _t11_532), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t11_531, _t11_532), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_531, _t11_532), _mm256_mul_pd(_t11_531, _t11_532), 129)), _mm256_add_pd(_mm256_mul_pd(_t11_531, _t11_532), _mm256_permute2f128_pd(_mm256_mul_pd(_t11_531, _t11_532), _mm256_mul_pd(_t11_531, _t11_532), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t11_534 = _mm256_sub_pd(_t11_530, _t11_533);

  // AVX Storer:
  _t11_54 = _t11_534;

  // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_535 = _t11_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_536 = _t11_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t11_537 = _t0_0;

  // 4-BLAC: 1x4 + 1x4
  _t11_538 = _mm256_add_pd(_t11_536, _t11_537);

  // 4-BLAC: 1x4 / 1x4
  _t11_539 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t11_535), _mm256_castpd256_pd128(_t11_538)));

  // AVX Storer:
  _t11_54 = _t11_539;

  _asm256_storeu_pd(C + 468, _t11_76);
  _asm256_storeu_pd(C + 520, _t11_77);
  _asm256_storeu_pd(C + 572, _t11_78);
  _mm_store_sd(&(C[420]), _mm256_castpd256_pd128(_t11_23));
  _mm_store_sd(&(C[421]), _mm256_castpd256_pd128(_t11_24));
  _mm_store_sd(&(C[422]), _mm256_castpd256_pd128(_t11_25));
  _mm_store_sd(&(C[423]), _mm256_castpd256_pd128(_t11_26));
  _mm_store_sd(&(C[472]), _mm256_castpd256_pd128(_t11_27));
  _mm_store_sd(&(C[473]), _mm256_castpd256_pd128(_t11_28));
  _mm_store_sd(&(C[474]), _mm256_castpd256_pd128(_t11_29));
  _mm_store_sd(&(C[475]), _mm256_castpd256_pd128(_t11_30));
  _mm_store_sd(&(C[524]), _mm256_castpd256_pd128(_t11_31));
  _mm_store_sd(&(C[525]), _mm256_castpd256_pd128(_t11_32));
  _mm_store_sd(&(C[526]), _mm256_castpd256_pd128(_t11_33));
  _mm_store_sd(&(C[527]), _mm256_castpd256_pd128(_t11_34));
  _mm_store_sd(&(C[576]), _mm256_castpd256_pd128(_t11_35));
  _mm_store_sd(&(C[577]), _mm256_castpd256_pd128(_t11_36));
  _mm_store_sd(&(C[578]), _mm256_castpd256_pd128(_t11_37));
  _mm_store_sd(&(C[579]), _mm256_castpd256_pd128(_t11_38));
  _mm_store_sd(&(C[424]), _mm256_castpd256_pd128(_t11_39));
  _mm_store_sd(&(C[425]), _mm256_castpd256_pd128(_t11_40));
  _mm_store_sd(&(C[426]), _mm256_castpd256_pd128(_t11_41));
  _mm_store_sd(&(C[427]), _mm256_castpd256_pd128(_t11_42));
  _mm_store_sd(&(C[476]), _mm256_castpd256_pd128(_t11_43));
  _mm_store_sd(&(C[477]), _mm256_castpd256_pd128(_t11_44));
  _mm_store_sd(&(C[478]), _mm256_castpd256_pd128(_t11_45));
  _mm_store_sd(&(C[479]), _mm256_castpd256_pd128(_t11_46));
  _mm_store_sd(&(C[528]), _mm256_castpd256_pd128(_t11_47));
  _mm_store_sd(&(C[529]), _mm256_castpd256_pd128(_t11_48));
  _mm_store_sd(&(C[530]), _mm256_castpd256_pd128(_t11_49));
  _mm_store_sd(&(C[531]), _mm256_castpd256_pd128(_t11_50));
  _mm_store_sd(&(C[580]), _mm256_castpd256_pd128(_t11_51));
  _mm_store_sd(&(C[581]), _mm256_castpd256_pd128(_t11_52));
  _mm_store_sd(&(C[582]), _mm256_castpd256_pd128(_t11_53));
  _mm_store_sd(&(C[583]), _mm256_castpd256_pd128(_t11_54));

  for( int fi1082 = 12; fi1082 <= 48; fi1082+=4 ) {
    _t12_4 = _asm256_loadu_pd(C + fi1082 + 416);
    _t12_5 = _asm256_loadu_pd(C + fi1082 + 468);
    _t12_6 = _asm256_loadu_pd(C + fi1082 + 520);
    _t12_7 = _asm256_loadu_pd(C + fi1082 + 572);
    _t12_3 = _asm256_loadu_pd(U + fi1082);
    _t12_2 = _asm256_loadu_pd(U + fi1082 + 52);
    _t12_1 = _asm256_loadu_pd(U + fi1082 + 104);
    _t12_0 = _asm256_loadu_pd(U + fi1082 + 156);

    // Generating : X[52,52] = ( S(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, fi1082)) - ( G(h(4, 52, fi879), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082)) + Sum_{k116} ( -$(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi1082)) ),h(4, 52, fi1082)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t11_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_7, _t11_7, 32), _mm256_permute2f128_pd(_t11_7, _t11_7, 32), 0), _t12_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_8, _t11_8, 32), _mm256_permute2f128_pd(_t11_8, _t11_8, 32), 0), _t12_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_9, _t11_9, 32), _mm256_permute2f128_pd(_t11_9, _t11_9, 32), 0), _t12_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_10, _t11_10, 32), _mm256_permute2f128_pd(_t11_10, _t11_10, 32), 0), _t12_0)));
    _t11_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_11, _t11_11, 32), _mm256_permute2f128_pd(_t11_11, _t11_11, 32), 0), _t12_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_12, _t11_12, 32), _mm256_permute2f128_pd(_t11_12, _t11_12, 32), 0), _t12_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_13, _t11_13, 32), _mm256_permute2f128_pd(_t11_13, _t11_13, 32), 0), _t12_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_14, _t11_14, 32), _mm256_permute2f128_pd(_t11_14, _t11_14, 32), 0), _t12_0)));
    _t11_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_15, _t11_15, 32), _mm256_permute2f128_pd(_t11_15, _t11_15, 32), 0), _t12_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_16, _t11_16, 32), _mm256_permute2f128_pd(_t11_16, _t11_16, 32), 0), _t12_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_17, _t11_17, 32), _mm256_permute2f128_pd(_t11_17, _t11_17, 32), 0), _t12_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_18, _t11_18, 32), _mm256_permute2f128_pd(_t11_18, _t11_18, 32), 0), _t12_0)));
    _t11_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_19, _t11_19, 32), _mm256_permute2f128_pd(_t11_19, _t11_19, 32), 0), _t12_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_20, _t11_20, 32), _mm256_permute2f128_pd(_t11_20, _t11_20, 32), 0), _t12_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_21, _t11_21, 32), _mm256_permute2f128_pd(_t11_21, _t11_21, 32), 0), _t12_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t11_22, _t11_22, 32), _mm256_permute2f128_pd(_t11_22, _t11_22, 32), 0), _t12_0)));

    // 4-BLAC: 4x4 - 4x4
    _t12_4 = _mm256_sub_pd(_t12_4, _t11_66);
    _t12_5 = _mm256_sub_pd(_t12_5, _t11_67);
    _t12_6 = _mm256_sub_pd(_t12_6, _t11_68);
    _t12_7 = _mm256_sub_pd(_t12_7, _t11_69);

    // AVX Storer:
    _asm256_storeu_pd(C + fi1082 + 416, _t12_4);
    _asm256_storeu_pd(C + fi1082 + 468, _t12_5);
    _asm256_storeu_pd(C + fi1082 + 520, _t12_6);
    _asm256_storeu_pd(C + fi1082 + 572, _t12_7);

    for( int k116 = 4; k116 <= fi1082 - 1; k116+=4 ) {
      _t13_19 = _mm256_broadcast_sd(C + k116 + 416);
      _t13_18 = _mm256_broadcast_sd(C + k116 + 417);
      _t13_17 = _mm256_broadcast_sd(C + k116 + 418);
      _t13_16 = _mm256_broadcast_sd(C + k116 + 419);
      _t13_15 = _mm256_broadcast_sd(C + k116 + 468);
      _t13_14 = _mm256_broadcast_sd(C + k116 + 469);
      _t13_13 = _mm256_broadcast_sd(C + k116 + 470);
      _t13_12 = _mm256_broadcast_sd(C + k116 + 471);
      _t13_11 = _mm256_broadcast_sd(C + k116 + 520);
      _t13_10 = _mm256_broadcast_sd(C + k116 + 521);
      _t13_9 = _mm256_broadcast_sd(C + k116 + 522);
      _t13_8 = _mm256_broadcast_sd(C + k116 + 523);
      _t13_7 = _mm256_broadcast_sd(C + k116 + 572);
      _t13_6 = _mm256_broadcast_sd(C + k116 + 573);
      _t13_5 = _mm256_broadcast_sd(C + k116 + 574);
      _t13_4 = _mm256_broadcast_sd(C + k116 + 575);
      _t13_3 = _asm256_loadu_pd(U + fi1082 + 52*k116);
      _t13_2 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 52);
      _t13_1 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 104);
      _t13_0 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 156);
      _t13_20 = _asm256_loadu_pd(C + fi1082 + 416);
      _t13_21 = _asm256_loadu_pd(C + fi1082 + 468);
      _t13_22 = _asm256_loadu_pd(C + fi1082 + 520);
      _t13_23 = _asm256_loadu_pd(C + fi1082 + 572);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t11_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_3), _mm256_mul_pd(_t13_18, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_1), _mm256_mul_pd(_t13_16, _t13_0)));
      _t11_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_3), _mm256_mul_pd(_t13_14, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_1), _mm256_mul_pd(_t13_12, _t13_0)));
      _t11_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_3), _mm256_mul_pd(_t13_10, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_1), _mm256_mul_pd(_t13_8, _t13_0)));
      _t11_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_3), _mm256_mul_pd(_t13_6, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_1), _mm256_mul_pd(_t13_4, _t13_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t13_20 = _mm256_sub_pd(_t13_20, _t11_70);
      _t13_21 = _mm256_sub_pd(_t13_21, _t11_71);
      _t13_22 = _mm256_sub_pd(_t13_22, _t11_72);
      _t13_23 = _mm256_sub_pd(_t13_23, _t11_73);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1082 + 416, _t13_20);
      _asm256_storeu_pd(C + fi1082 + 468, _t13_21);
      _asm256_storeu_pd(C + fi1082 + 520, _t13_22);
      _asm256_storeu_pd(C + fi1082 + 572, _t13_23);
    }
    _t12_6 = _asm256_loadu_pd(C + fi1082 + 520);
    _t12_4 = _asm256_loadu_pd(C + fi1082 + 416);
    _t12_7 = _asm256_loadu_pd(C + fi1082 + 572);
    _t12_5 = _asm256_loadu_pd(C + fi1082 + 468);
    _t14_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082])));
    _t14_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 1])));
    _t14_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 53])));
    _t14_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 54)), 0);
    _t14_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 106])));
    _t14_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 55))), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 107)), 32);
    _t14_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 159])));

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_24 = _t11_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_25 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_392 = _mm256_add_pd(_t14_24, _t14_25);

    // 4-BLAC: 1x4 / 1x4
    _t14_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_23), _mm256_castpd256_pd128(_t11_392)));

    // AVX Storer:
    _t14_7 = _t14_26;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_28 = _t14_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_29 = _t14_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_397 = _mm256_mul_pd(_t14_28, _t14_29);

    // 4-BLAC: 1x4 - 1x4
    _t14_30 = _mm256_sub_pd(_t14_27, _t11_397);

    // AVX Storer:
    _t14_8 = _t14_30;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_31 = _t14_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_32 = _t11_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_33 = _t14_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_402 = _mm256_add_pd(_t14_32, _t14_33);

    // 4-BLAC: 1x4 / 1x4
    _t14_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_31), _mm256_castpd256_pd128(_t11_402)));

    // AVX Storer:
    _t14_8 = _t14_34;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_4, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_7, _t14_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_37 = _t14_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_36, _t14_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_36, _t14_37), _mm256_mul_pd(_t14_36, _t14_37), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_36, _t14_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_36, _t14_37), _mm256_mul_pd(_t14_36, _t14_37), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_36, _t14_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_36, _t14_37), _mm256_mul_pd(_t14_36, _t14_37), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_38 = _mm256_sub_pd(_t14_35, _t11_407);

    // AVX Storer:
    _t14_9 = _t14_38;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_39 = _t14_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_40 = _t11_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_41 = _t14_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_412 = _mm256_add_pd(_t14_40, _t14_41);

    // 4-BLAC: 1x4 / 1x4
    _t14_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_39), _mm256_castpd256_pd128(_t11_412)));

    // AVX Storer:
    _t14_9 = _t14_42;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_4, _t12_4, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_8), _mm256_unpacklo_pd(_t14_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_45 = _t14_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_417 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_44, _t14_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_44, _t14_45), _mm256_mul_pd(_t14_44, _t14_45), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_44, _t14_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_44, _t14_45), _mm256_mul_pd(_t14_44, _t14_45), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_44, _t14_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_44, _t14_45), _mm256_mul_pd(_t14_44, _t14_45), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_46 = _mm256_sub_pd(_t14_43, _t11_417);

    // AVX Storer:
    _t14_10 = _t14_46;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_47 = _t14_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_48 = _t11_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_49 = _t14_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_422 = _mm256_add_pd(_t14_48, _t14_49);

    // 4-BLAC: 1x4 / 1x4
    _t14_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_47), _mm256_castpd256_pd128(_t11_422)));

    // AVX Storer:
    _t14_10 = _t14_50;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_51 = _t11_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t11_57 = _mm256_mul_pd(_t14_51, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_8), _mm256_unpacklo_pd(_t14_9, _t14_10), 32));

    // 4-BLAC: 1x4 - 1x4
    _t12_5 = _mm256_sub_pd(_t12_5, _t11_57);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_52 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_53 = _t11_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_54 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_428 = _mm256_add_pd(_t14_53, _t14_54);

    // 4-BLAC: 1x4 / 1x4
    _t14_55 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_52), _mm256_castpd256_pd128(_t11_428)));

    // AVX Storer:
    _t14_11 = _t14_55;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_56 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_57 = _t14_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_58 = _t14_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_433 = _mm256_mul_pd(_t14_57, _t14_58);

    // 4-BLAC: 1x4 - 1x4
    _t14_59 = _mm256_sub_pd(_t14_56, _t11_433);

    // AVX Storer:
    _t14_12 = _t14_59;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_60 = _t14_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_61 = _t11_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_62 = _t14_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_438 = _mm256_add_pd(_t14_61, _t14_62);

    // 4-BLAC: 1x4 / 1x4
    _t14_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_60), _mm256_castpd256_pd128(_t11_438)));

    // AVX Storer:
    _t14_12 = _t14_63;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_64 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_5, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_65 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_11, _t14_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_66 = _t14_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_443 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_65, _t14_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_65, _t14_66), _mm256_mul_pd(_t14_65, _t14_66), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_65, _t14_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_65, _t14_66), _mm256_mul_pd(_t14_65, _t14_66), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_65, _t14_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_65, _t14_66), _mm256_mul_pd(_t14_65, _t14_66), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_67 = _mm256_sub_pd(_t14_64, _t11_443);

    // AVX Storer:
    _t14_13 = _t14_67;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_68 = _t14_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_69 = _t11_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_70 = _t14_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_448 = _mm256_add_pd(_t14_69, _t14_70);

    // 4-BLAC: 1x4 / 1x4
    _t14_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_68), _mm256_castpd256_pd128(_t11_448)));

    // AVX Storer:
    _t14_13 = _t14_71;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_72 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_5, _t12_5, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_73 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_11, _t14_12), _mm256_unpacklo_pd(_t14_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_74 = _t14_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_453 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_73, _t14_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_73, _t14_74), _mm256_mul_pd(_t14_73, _t14_74), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_73, _t14_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_73, _t14_74), _mm256_mul_pd(_t14_73, _t14_74), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_73, _t14_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_73, _t14_74), _mm256_mul_pd(_t14_73, _t14_74), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_75 = _mm256_sub_pd(_t14_72, _t11_453);

    // AVX Storer:
    _t14_14 = _t14_75;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_76 = _t14_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_77 = _t11_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_78 = _t14_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_458 = _mm256_add_pd(_t14_77, _t14_78);

    // 4-BLAC: 1x4 / 1x4
    _t14_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_76), _mm256_castpd256_pd128(_t11_458)));

    // AVX Storer:
    _t14_14 = _t14_79;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_80 = _t11_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t14_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_8), _mm256_unpacklo_pd(_t14_9, _t14_10), 32);
    _t14_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_11, _t14_12), _mm256_unpacklo_pd(_t14_13, _t14_14), 32);
    _t14_83 = _mm256_setzero_pd();
    _t14_84 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_80, _t14_80, 32), _mm256_permute2f128_pd(_t14_80, _t14_80, 32), 0), _t14_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_80, _t14_80, 32), _mm256_permute2f128_pd(_t14_80, _t14_80, 32), 15), _t14_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_80, _t14_80, 49), _mm256_permute2f128_pd(_t14_80, _t14_80, 49), 0), _t14_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_80, _t14_80, 49), _mm256_permute2f128_pd(_t14_80, _t14_80, 49), 15), _t14_84)));

    // 4-BLAC: 1x4 - 1x4
    _t12_6 = _mm256_sub_pd(_t12_6, _t11_74);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_85 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_86 = _t11_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_87 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_468 = _mm256_add_pd(_t14_86, _t14_87);

    // 4-BLAC: 1x4 / 1x4
    _t14_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_85), _mm256_castpd256_pd128(_t11_468)));

    // AVX Storer:
    _t14_15 = _t14_88;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_90 = _t14_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_91 = _t14_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_473 = _mm256_mul_pd(_t14_90, _t14_91);

    // 4-BLAC: 1x4 - 1x4
    _t14_92 = _mm256_sub_pd(_t14_89, _t11_473);

    // AVX Storer:
    _t14_16 = _t14_92;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_93 = _t14_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_94 = _t11_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_95 = _t14_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_478 = _mm256_add_pd(_t14_94, _t14_95);

    // 4-BLAC: 1x4 / 1x4
    _t14_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_93), _mm256_castpd256_pd128(_t11_478)));

    // AVX Storer:
    _t14_16 = _t14_96;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_97 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_6, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_98 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_15, _t14_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_99 = _t14_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_98, _t14_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_98, _t14_99), _mm256_mul_pd(_t14_98, _t14_99), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_98, _t14_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_98, _t14_99), _mm256_mul_pd(_t14_98, _t14_99), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_98, _t14_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_98, _t14_99), _mm256_mul_pd(_t14_98, _t14_99), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_100 = _mm256_sub_pd(_t14_97, _t11_483);

    // AVX Storer:
    _t14_17 = _t14_100;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_101 = _t14_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_102 = _t11_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_103 = _t14_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_488 = _mm256_add_pd(_t14_102, _t14_103);

    // 4-BLAC: 1x4 / 1x4
    _t14_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_101), _mm256_castpd256_pd128(_t11_488)));

    // AVX Storer:
    _t14_17 = _t14_104;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_6, _t12_6, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_15, _t14_16), _mm256_unpacklo_pd(_t14_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_107 = _t14_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_493 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_106, _t14_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_106, _t14_107), _mm256_mul_pd(_t14_106, _t14_107), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_106, _t14_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_106, _t14_107), _mm256_mul_pd(_t14_106, _t14_107), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_106, _t14_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_106, _t14_107), _mm256_mul_pd(_t14_106, _t14_107), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_108 = _mm256_sub_pd(_t14_105, _t11_493);

    // AVX Storer:
    _t14_18 = _t14_108;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_109 = _t14_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_110 = _t11_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_111 = _t14_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_498 = _mm256_add_pd(_t14_110, _t14_111);

    // 4-BLAC: 1x4 / 1x4
    _t14_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_109), _mm256_castpd256_pd128(_t11_498)));

    // AVX Storer:
    _t14_18 = _t14_112;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_113 = _t11_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t14_114 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_8), _mm256_unpacklo_pd(_t14_9, _t14_10), 32);
    _t14_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_11, _t14_12), _mm256_unpacklo_pd(_t14_13, _t14_14), 32);
    _t14_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_15, _t14_16), _mm256_unpacklo_pd(_t14_17, _t14_18), 32);
    _t14_117 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_113, _t14_113, 32), _mm256_permute2f128_pd(_t14_113, _t14_113, 32), 0), _t14_114), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_113, _t14_113, 32), _mm256_permute2f128_pd(_t14_113, _t14_113, 32), 15), _t14_115)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_113, _t14_113, 49), _mm256_permute2f128_pd(_t14_113, _t14_113, 49), 0), _t14_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t14_113, _t14_113, 49), _mm256_permute2f128_pd(_t14_113, _t14_113, 49), 15), _t14_117)));

    // 4-BLAC: 1x4 - 1x4
    _t12_7 = _mm256_sub_pd(_t12_7, _t11_75);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_118 = _mm256_blend_pd(_mm256_setzero_pd(), _t12_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_119 = _t11_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_120 = _t14_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_508 = _mm256_add_pd(_t14_119, _t14_120);

    // 4-BLAC: 1x4 / 1x4
    _t14_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_118), _mm256_castpd256_pd128(_t11_508)));

    // AVX Storer:
    _t14_19 = _t14_121;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_122 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_7, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_123 = _t14_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_124 = _t14_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_513 = _mm256_mul_pd(_t14_123, _t14_124);

    // 4-BLAC: 1x4 - 1x4
    _t14_125 = _mm256_sub_pd(_t14_122, _t11_513);

    // AVX Storer:
    _t14_20 = _t14_125;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_126 = _t14_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_127 = _t11_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_128 = _t14_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_518 = _mm256_add_pd(_t14_127, _t14_128);

    // 4-BLAC: 1x4 / 1x4
    _t14_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_126), _mm256_castpd256_pd128(_t11_518)));

    // AVX Storer:
    _t14_20 = _t14_129;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_130 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t12_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t12_7, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t14_131 = _mm256_blend_pd(_mm256_unpacklo_pd(_t14_19, _t14_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t14_132 = _t14_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_523 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_131, _t14_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_131, _t14_132), _mm256_mul_pd(_t14_131, _t14_132), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_131, _t14_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_131, _t14_132), _mm256_mul_pd(_t14_131, _t14_132), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_131, _t14_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_131, _t14_132), _mm256_mul_pd(_t14_131, _t14_132), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_133 = _mm256_sub_pd(_t14_130, _t11_523);

    // AVX Storer:
    _t14_21 = _t14_133;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_134 = _t14_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_135 = _t11_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_136 = _t14_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_528 = _mm256_add_pd(_t14_135, _t14_136);

    // 4-BLAC: 1x4 / 1x4
    _t14_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_134), _mm256_castpd256_pd128(_t11_528)));

    // AVX Storer:
    _t14_21 = _t14_137;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_138 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t12_7, _t12_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t14_139 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_19, _t14_20), _mm256_unpacklo_pd(_t14_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t14_140 = _t14_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_533 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t14_139, _t14_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_139, _t14_140), _mm256_mul_pd(_t14_139, _t14_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t14_139, _t14_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_139, _t14_140), _mm256_mul_pd(_t14_139, _t14_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t14_139, _t14_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t14_139, _t14_140), _mm256_mul_pd(_t14_139, _t14_140), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t14_141 = _mm256_sub_pd(_t14_138, _t11_533);

    // AVX Storer:
    _t14_22 = _t14_141;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_142 = _t14_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_143 = _t11_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t14_144 = _t14_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_538 = _mm256_add_pd(_t14_143, _t14_144);

    // 4-BLAC: 1x4 / 1x4
    _t14_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t14_142), _mm256_castpd256_pd128(_t11_538)));

    // AVX Storer:
    _t14_22 = _t14_145;
    _mm_store_sd(&(C[fi1082 + 416]), _mm256_castpd256_pd128(_t14_7));
    _mm_store_sd(&(C[fi1082 + 417]), _mm256_castpd256_pd128(_t14_8));
    _mm_store_sd(&(C[fi1082 + 418]), _mm256_castpd256_pd128(_t14_9));
    _mm_store_sd(&(C[fi1082 + 419]), _mm256_castpd256_pd128(_t14_10));
    _mm_store_sd(&(C[fi1082 + 468]), _mm256_castpd256_pd128(_t14_11));
    _mm_store_sd(&(C[fi1082 + 469]), _mm256_castpd256_pd128(_t14_12));
    _mm_store_sd(&(C[fi1082 + 470]), _mm256_castpd256_pd128(_t14_13));
    _mm_store_sd(&(C[fi1082 + 471]), _mm256_castpd256_pd128(_t14_14));
    _mm_store_sd(&(C[fi1082 + 520]), _mm256_castpd256_pd128(_t14_15));
    _mm_store_sd(&(C[fi1082 + 521]), _mm256_castpd256_pd128(_t14_16));
    _mm_store_sd(&(C[fi1082 + 522]), _mm256_castpd256_pd128(_t14_17));
    _mm_store_sd(&(C[fi1082 + 523]), _mm256_castpd256_pd128(_t14_18));
    _mm_store_sd(&(C[fi1082 + 572]), _mm256_castpd256_pd128(_t14_19));
    _mm_store_sd(&(C[fi1082 + 573]), _mm256_castpd256_pd128(_t14_20));
    _mm_store_sd(&(C[fi1082 + 574]), _mm256_castpd256_pd128(_t14_21));
    _mm_store_sd(&(C[fi1082 + 575]), _mm256_castpd256_pd128(_t14_22));
  }

  _mm_store_sd(&(C[416]), _mm256_castpd256_pd128(_t11_7));
  _mm_store_sd(&(C[417]), _mm256_castpd256_pd128(_t11_8));
  _mm_store_sd(&(C[418]), _mm256_castpd256_pd128(_t11_9));
  _mm_store_sd(&(C[419]), _mm256_castpd256_pd128(_t11_10));
  _mm_store_sd(&(C[468]), _mm256_castpd256_pd128(_t11_11));
  _mm_store_sd(&(C[469]), _mm256_castpd256_pd128(_t11_12));
  _mm_store_sd(&(C[470]), _mm256_castpd256_pd128(_t11_13));
  _mm_store_sd(&(C[471]), _mm256_castpd256_pd128(_t11_14));
  _mm_store_sd(&(C[520]), _mm256_castpd256_pd128(_t11_15));
  _mm_store_sd(&(C[521]), _mm256_castpd256_pd128(_t11_16));
  _mm_store_sd(&(C[522]), _mm256_castpd256_pd128(_t11_17));
  _mm_store_sd(&(C[523]), _mm256_castpd256_pd128(_t11_18));
  _mm_store_sd(&(C[572]), _mm256_castpd256_pd128(_t11_19));
  _mm_store_sd(&(C[573]), _mm256_castpd256_pd128(_t11_20));
  _mm_store_sd(&(C[574]), _mm256_castpd256_pd128(_t11_21));
  _mm_store_sd(&(C[575]), _mm256_castpd256_pd128(_t11_22));

  for( int fi879 = 12; fi879 <= 48; fi879+=4 ) {

    // Generating : X[52,52] = ( Sum_{k233} ( S(h(4, 52, fi879), ( G(h(4, 52, fi879), C[52,52],h(4, 52, k233)) - ( G(h(4, 52, fi879), L[52,52],h(4, 52, 0)) * G(h(4, 52, 0), X[52,52],h(4, 52, k233)) ) ),h(4, 52, k233)) ) + Sum_{k116} ( Sum_{k233} ( -$(h(4, 52, fi879), ( G(h(4, 52, fi879), L[52,52],h(4, 52, k116)) * G(h(4, 52, k116), X[52,52],h(4, 52, k233)) ),h(4, 52, k233)) ) ) )

    // AVX Loader:

    for( int k233 = 0; k233 <= 51; k233+=4 ) {
      _t15_24 = _asm256_loadu_pd(C + 52*fi879 + k233);
      _t15_25 = _asm256_loadu_pd(C + 52*fi879 + k233 + 52);
      _t15_26 = _asm256_loadu_pd(C + 52*fi879 + k233 + 104);
      _t15_27 = _asm256_loadu_pd(C + 52*fi879 + k233 + 156);
      _t15_19 = _mm256_broadcast_sd(L + 52*fi879);
      _t15_18 = _mm256_broadcast_sd(L + 52*fi879 + 1);
      _t15_17 = _mm256_broadcast_sd(L + 52*fi879 + 2);
      _t15_16 = _mm256_broadcast_sd(L + 52*fi879 + 3);
      _t15_15 = _mm256_broadcast_sd(L + 52*fi879 + 52);
      _t15_14 = _mm256_broadcast_sd(L + 52*fi879 + 53);
      _t15_13 = _mm256_broadcast_sd(L + 52*fi879 + 54);
      _t15_12 = _mm256_broadcast_sd(L + 52*fi879 + 55);
      _t15_11 = _mm256_broadcast_sd(L + 52*fi879 + 104);
      _t15_10 = _mm256_broadcast_sd(L + 52*fi879 + 105);
      _t15_9 = _mm256_broadcast_sd(L + 52*fi879 + 106);
      _t15_8 = _mm256_broadcast_sd(L + 52*fi879 + 107);
      _t15_7 = _mm256_broadcast_sd(L + 52*fi879 + 156);
      _t15_6 = _mm256_broadcast_sd(L + 52*fi879 + 157);
      _t15_5 = _mm256_broadcast_sd(L + 52*fi879 + 158);
      _t15_4 = _mm256_broadcast_sd(L + 52*fi879 + 159);
      _t15_3 = _asm256_loadu_pd(C + k233);
      _t15_2 = _asm256_loadu_pd(C + k233 + 52);
      _t15_1 = _asm256_loadu_pd(C + k233 + 104);
      _t15_0 = _asm256_loadu_pd(C + k233 + 156);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t15_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_3), _mm256_mul_pd(_t15_18, _t15_2)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_1), _mm256_mul_pd(_t15_16, _t15_0)));
      _t15_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_3), _mm256_mul_pd(_t15_14, _t15_2)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_1), _mm256_mul_pd(_t15_12, _t15_0)));
      _t15_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_3), _mm256_mul_pd(_t15_10, _t15_2)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_1), _mm256_mul_pd(_t15_8, _t15_0)));
      _t15_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_7, _t15_3), _mm256_mul_pd(_t15_6, _t15_2)), _mm256_add_pd(_mm256_mul_pd(_t15_5, _t15_1), _mm256_mul_pd(_t15_4, _t15_0)));

      // 4-BLAC: 4x4 - 4x4
      _t15_24 = _mm256_sub_pd(_t15_24, _t15_20);
      _t15_25 = _mm256_sub_pd(_t15_25, _t15_21);
      _t15_26 = _mm256_sub_pd(_t15_26, _t15_22);
      _t15_27 = _mm256_sub_pd(_t15_27, _t15_23);

      // AVX Storer:
      _asm256_storeu_pd(C + 52*fi879 + k233, _t15_24);
      _asm256_storeu_pd(C + 52*fi879 + k233 + 52, _t15_25);
      _asm256_storeu_pd(C + 52*fi879 + k233 + 104, _t15_26);
      _asm256_storeu_pd(C + 52*fi879 + k233 + 156, _t15_27);
    }

    for( int k116 = 4; k116 <= fi879 - 1; k116+=4 ) {

      for( int k233 = 0; k233 <= 51; k233+=4 ) {
        _t16_19 = _mm256_broadcast_sd(L + 52*fi879 + k116);
        _t16_18 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 1);
        _t16_17 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 2);
        _t16_16 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 3);
        _t16_15 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 52);
        _t16_14 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 53);
        _t16_13 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 54);
        _t16_12 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 55);
        _t16_11 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 104);
        _t16_10 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 105);
        _t16_9 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 106);
        _t16_8 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 107);
        _t16_7 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 156);
        _t16_6 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 157);
        _t16_5 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 158);
        _t16_4 = _mm256_broadcast_sd(L + 52*fi879 + k116 + 159);
        _t16_3 = _asm256_loadu_pd(C + 52*k116 + k233);
        _t16_2 = _asm256_loadu_pd(C + 52*k116 + k233 + 52);
        _t16_1 = _asm256_loadu_pd(C + 52*k116 + k233 + 104);
        _t16_0 = _asm256_loadu_pd(C + 52*k116 + k233 + 156);
        _t16_20 = _asm256_loadu_pd(C + 52*fi879 + k233);
        _t16_21 = _asm256_loadu_pd(C + 52*fi879 + k233 + 52);
        _t16_22 = _asm256_loadu_pd(C + 52*fi879 + k233 + 104);
        _t16_23 = _asm256_loadu_pd(C + 52*fi879 + k233 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t16_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_3), _mm256_mul_pd(_t16_18, _t16_2)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_1), _mm256_mul_pd(_t16_16, _t16_0)));
        _t16_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_3), _mm256_mul_pd(_t16_14, _t16_2)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_1), _mm256_mul_pd(_t16_12, _t16_0)));
        _t16_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_3), _mm256_mul_pd(_t16_10, _t16_2)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_1), _mm256_mul_pd(_t16_8, _t16_0)));
        _t16_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_3), _mm256_mul_pd(_t16_6, _t16_2)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_1), _mm256_mul_pd(_t16_4, _t16_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t16_20 = _mm256_sub_pd(_t16_20, _t16_24);
        _t16_21 = _mm256_sub_pd(_t16_21, _t16_25);
        _t16_22 = _mm256_sub_pd(_t16_22, _t16_26);
        _t16_23 = _mm256_sub_pd(_t16_23, _t16_27);

        // AVX Storer:
        _asm256_storeu_pd(C + 52*fi879 + k233, _t16_20);
        _asm256_storeu_pd(C + 52*fi879 + k233 + 52, _t16_21);
        _asm256_storeu_pd(C + 52*fi879 + k233 + 104, _t16_22);
        _asm256_storeu_pd(C + 52*fi879 + k233 + 156, _t16_23);
      }
    }
    _t17_7 = _mm256_castpd128_pd256(_mm_load_sd(&(C[52*fi879])));
    _t17_6 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi879])));
    _t17_8 = _mm256_castpd128_pd256(_mm_load_sd(&(C[52*fi879 + 1])));
    _t17_9 = _mm256_castpd128_pd256(_mm_load_sd(&(C[52*fi879 + 2])));
    _t17_10 = _mm256_castpd128_pd256(_mm_load_sd(&(C[52*fi879 + 3])));
    _t17_55 = _asm256_loadu_pd(C + 52*fi879 + 52);
    _t17_5 = _mm256_broadcast_sd(&(L[53*fi879 + 52]));
    _t17_4 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi879 + 53])));
    _t17_56 = _asm256_loadu_pd(C + 52*fi879 + 104);
    _t17_3 = _mm256_maskload_pd(L + 53*fi879 + 104, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t17_2 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi879 + 106])));
    _t17_57 = _asm256_loadu_pd(C + 52*fi879 + 156);
    _t17_1 = _mm256_maskload_pd(L + 53*fi879 + 156, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t17_0 = _mm256_castpd128_pd256(_mm_load_sd(&(L[53*fi879 + 159])));
    _t17_58 = _asm256_loadu_pd(C + 52*fi879 + 4);
    _t17_59 = _asm256_loadu_pd(C + 52*fi879 + 56);
    _t17_60 = _asm256_loadu_pd(C + 52*fi879 + 108);
    _t17_61 = _asm256_loadu_pd(C + 52*fi879 + 160);
    _t17_62 = _asm256_loadu_pd(C + 52*fi879 + 8);
    _t17_63 = _asm256_loadu_pd(C + 52*fi879 + 60);
    _t17_64 = _asm256_loadu_pd(C + 52*fi879 + 112);
    _t17_65 = _asm256_loadu_pd(C + 52*fi879 + 164);

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_66 = _t17_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_67 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_68 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t11_90 = _mm256_add_pd(_t17_67, _t17_68);

    // 4-BLAC: 1x4 / 1x4
    _t17_69 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_66), _mm256_castpd256_pd128(_t11_90)));

    // AVX Storer:
    _t17_7 = _t17_69;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_70 = _t17_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_71 = _t17_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_72 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_95 = _mm256_mul_pd(_t17_71, _t17_72);

    // 4-BLAC: 1x4 - 1x4
    _t17_73 = _mm256_sub_pd(_t17_70, _t11_95);

    // AVX Storer:
    _t17_8 = _t17_73;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_74 = _t17_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_75 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_76 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t11_100 = _mm256_add_pd(_t17_75, _t17_76);

    // 4-BLAC: 1x4 / 1x4
    _t17_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_74), _mm256_castpd256_pd128(_t11_100)));

    // AVX Storer:
    _t17_8 = _t17_77;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_78 = _t17_9;

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_79 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_80 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t11_105 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_79, _t17_80), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_79, _t17_80), _mm256_mul_pd(_t17_79, _t17_80), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_79, _t17_80), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_79, _t17_80), _mm256_mul_pd(_t17_79, _t17_80), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_79, _t17_80), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_79, _t17_80), _mm256_mul_pd(_t17_79, _t17_80), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_81 = _mm256_sub_pd(_t17_78, _t11_105);

    // AVX Storer:
    _t17_9 = _t17_81;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_82 = _t17_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_83 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_84 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t11_110 = _mm256_add_pd(_t17_83, _t17_84);

    // 4-BLAC: 1x4 / 1x4
    _t17_85 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_82), _mm256_castpd256_pd128(_t11_110)));

    // AVX Storer:
    _t17_9 = _t17_85;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_86 = _t17_10;

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_87 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_88 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t11_115 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_87, _t17_88), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_87, _t17_88), _mm256_mul_pd(_t17_87, _t17_88), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_87, _t17_88), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_87, _t17_88), _mm256_mul_pd(_t17_87, _t17_88), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_87, _t17_88), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_87, _t17_88), _mm256_mul_pd(_t17_87, _t17_88), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_89 = _mm256_sub_pd(_t17_86, _t11_115);

    // AVX Storer:
    _t17_10 = _t17_89;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_90 = _t17_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_91 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_92 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t11_120 = _mm256_add_pd(_t17_91, _t17_92);

    // 4-BLAC: 1x4 / 1x4
    _t17_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_90), _mm256_castpd256_pd128(_t11_120)));

    // AVX Storer:
    _t17_10 = _t17_93;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_94 = _t17_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t11_55 = _mm256_mul_pd(_t17_94, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32));

    // 4-BLAC: 1x4 - 1x4
    _t17_55 = _mm256_sub_pd(_t17_55, _t11_55);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_95 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_55, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_96 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_97 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t11_126 = _mm256_add_pd(_t17_96, _t17_97);

    // 4-BLAC: 1x4 / 1x4
    _t17_98 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_95), _mm256_castpd256_pd128(_t11_126)));

    // AVX Storer:
    _t17_11 = _t17_98;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_99 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_55, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_100 = _t17_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_101 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_131 = _mm256_mul_pd(_t17_100, _t17_101);

    // 4-BLAC: 1x4 - 1x4
    _t17_102 = _mm256_sub_pd(_t17_99, _t11_131);

    // AVX Storer:
    _t17_12 = _t17_102;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_103 = _t17_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_104 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_105 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t11_136 = _mm256_add_pd(_t17_104, _t17_105);

    // 4-BLAC: 1x4 / 1x4
    _t17_106 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_103), _mm256_castpd256_pd128(_t11_136)));

    // AVX Storer:
    _t17_12 = _t17_106;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_107 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_55, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_55, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_108 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_109 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t11_141 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_108, _t17_109), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_108, _t17_109), _mm256_mul_pd(_t17_108, _t17_109), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_108, _t17_109), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_108, _t17_109), _mm256_mul_pd(_t17_108, _t17_109), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_108, _t17_109), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_108, _t17_109), _mm256_mul_pd(_t17_108, _t17_109), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_110 = _mm256_sub_pd(_t17_107, _t11_141);

    // AVX Storer:
    _t17_13 = _t17_110;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_111 = _t17_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_112 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_113 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t11_146 = _mm256_add_pd(_t17_112, _t17_113);

    // 4-BLAC: 1x4 / 1x4
    _t17_114 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_111), _mm256_castpd256_pd128(_t11_146)));

    // AVX Storer:
    _t17_13 = _t17_114;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_115 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_55, _t17_55, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_117 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t11_151 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_116, _t17_117), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_116, _t17_117), _mm256_mul_pd(_t17_116, _t17_117), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_116, _t17_117), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_116, _t17_117), _mm256_mul_pd(_t17_116, _t17_117), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_116, _t17_117), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_116, _t17_117), _mm256_mul_pd(_t17_116, _t17_117), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_118 = _mm256_sub_pd(_t17_115, _t11_151);

    // AVX Storer:
    _t17_14 = _t17_118;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_119 = _t17_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_120 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_121 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t11_156 = _mm256_add_pd(_t17_120, _t17_121);

    // 4-BLAC: 1x4 / 1x4
    _t17_122 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_119), _mm256_castpd256_pd128(_t11_156)));

    // AVX Storer:
    _t17_14 = _t17_122;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_123 = _t17_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t17_124 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32);
    _t17_125 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _t17_14), 32);
    _t17_126 = _mm256_setzero_pd();
    _t17_127 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_58 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_123, _t17_123, 32), _mm256_permute2f128_pd(_t17_123, _t17_123, 32), 0), _t17_124), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_123, _t17_123, 32), _mm256_permute2f128_pd(_t17_123, _t17_123, 32), 15), _t17_125)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_123, _t17_123, 49), _mm256_permute2f128_pd(_t17_123, _t17_123, 49), 0), _t17_126), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_123, _t17_123, 49), _mm256_permute2f128_pd(_t17_123, _t17_123, 49), 15), _t17_127)));

    // 4-BLAC: 1x4 - 1x4
    _t17_56 = _mm256_sub_pd(_t17_56, _t11_58);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_128 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_56, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_129 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_130 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t11_166 = _mm256_add_pd(_t17_129, _t17_130);

    // 4-BLAC: 1x4 / 1x4
    _t17_131 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_128), _mm256_castpd256_pd128(_t11_166)));

    // AVX Storer:
    _t17_15 = _t17_131;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_132 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_56, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_133 = _t17_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_134 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_171 = _mm256_mul_pd(_t17_133, _t17_134);

    // 4-BLAC: 1x4 - 1x4
    _t17_135 = _mm256_sub_pd(_t17_132, _t11_171);

    // AVX Storer:
    _t17_16 = _t17_135;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_136 = _t17_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_137 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_138 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t11_176 = _mm256_add_pd(_t17_137, _t17_138);

    // 4-BLAC: 1x4 / 1x4
    _t17_139 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_136), _mm256_castpd256_pd128(_t11_176)));

    // AVX Storer:
    _t17_16 = _t17_139;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_140 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_56, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_56, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_141 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_15, _t17_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_142 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t11_181 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_141, _t17_142), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_141, _t17_142), _mm256_mul_pd(_t17_141, _t17_142), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_141, _t17_142), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_141, _t17_142), _mm256_mul_pd(_t17_141, _t17_142), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_141, _t17_142), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_141, _t17_142), _mm256_mul_pd(_t17_141, _t17_142), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_143 = _mm256_sub_pd(_t17_140, _t11_181);

    // AVX Storer:
    _t17_17 = _t17_143;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_144 = _t17_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_145 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_146 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t11_186 = _mm256_add_pd(_t17_145, _t17_146);

    // 4-BLAC: 1x4 / 1x4
    _t17_147 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_144), _mm256_castpd256_pd128(_t11_186)));

    // AVX Storer:
    _t17_17 = _t17_147;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_148 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_56, _t17_56, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_149 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_15, _t17_16), _mm256_unpacklo_pd(_t17_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_150 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t11_191 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_149, _t17_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_149, _t17_150), _mm256_mul_pd(_t17_149, _t17_150), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_149, _t17_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_149, _t17_150), _mm256_mul_pd(_t17_149, _t17_150), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_149, _t17_150), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_149, _t17_150), _mm256_mul_pd(_t17_149, _t17_150), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_151 = _mm256_sub_pd(_t17_148, _t11_191);

    // AVX Storer:
    _t17_18 = _t17_151;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_152 = _t17_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_153 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_154 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t11_196 = _mm256_add_pd(_t17_153, _t17_154);

    // 4-BLAC: 1x4 / 1x4
    _t17_155 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_152), _mm256_castpd256_pd128(_t11_196)));

    // AVX Storer:
    _t17_18 = _t17_155;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, 0)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, 0)) ) ),h(4, 52, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_156 = _t17_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t17_157 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_8), _mm256_unpacklo_pd(_t17_9, _t17_10), 32);
    _t17_158 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_11, _t17_12), _mm256_unpacklo_pd(_t17_13, _t17_14), 32);
    _t17_159 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_15, _t17_16), _mm256_unpacklo_pd(_t17_17, _t17_18), 32);
    _t17_160 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_59 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_156, _t17_156, 32), _mm256_permute2f128_pd(_t17_156, _t17_156, 32), 0), _t17_157), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_156, _t17_156, 32), _mm256_permute2f128_pd(_t17_156, _t17_156, 32), 15), _t17_158)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_156, _t17_156, 49), _mm256_permute2f128_pd(_t17_156, _t17_156, 49), 0), _t17_159), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_156, _t17_156, 49), _mm256_permute2f128_pd(_t17_156, _t17_156, 49), 15), _t17_160)));

    // 4-BLAC: 1x4 - 1x4
    _t17_57 = _mm256_sub_pd(_t17_57, _t11_59);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 0)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 0), U[52,52],h(1, 52, 0)) ) ),h(1, 52, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_161 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_57, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_162 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_163 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t11_206 = _mm256_add_pd(_t17_162, _t17_163);

    // 4-BLAC: 1x4 / 1x4
    _t17_164 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_161), _mm256_castpd256_pd128(_t11_206)));

    // AVX Storer:
    _t17_19 = _t17_164;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 1)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 0)) Kro G(h(1, 52, 0), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_165 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_57, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_166 = _t17_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_167 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_211 = _mm256_mul_pd(_t17_166, _t17_167);

    // 4-BLAC: 1x4 - 1x4
    _t17_168 = _mm256_sub_pd(_t17_165, _t11_211);

    // AVX Storer:
    _t17_20 = _t17_168;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 1)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 1), U[52,52],h(1, 52, 1)) ) ),h(1, 52, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_169 = _t17_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_170 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_171 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t11_216 = _mm256_add_pd(_t17_170, _t17_171);

    // 4-BLAC: 1x4 / 1x4
    _t17_172 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_169), _mm256_castpd256_pd128(_t11_216)));

    // AVX Storer:
    _t17_20 = _t17_172;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 2)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, 0)) * G(h(2, 52, 0), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_173 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_57, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_57, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_174 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_19, _t17_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_175 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t11_221 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_174, _t17_175), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_174, _t17_175), _mm256_mul_pd(_t17_174, _t17_175), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_174, _t17_175), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_174, _t17_175), _mm256_mul_pd(_t17_174, _t17_175), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_174, _t17_175), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_174, _t17_175), _mm256_mul_pd(_t17_174, _t17_175), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_176 = _mm256_sub_pd(_t17_173, _t11_221);

    // AVX Storer:
    _t17_21 = _t17_176;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 2)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 2), U[52,52],h(1, 52, 2)) ) ),h(1, 52, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_177 = _t17_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_178 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_179 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t11_226 = _mm256_add_pd(_t17_178, _t17_179);

    // 4-BLAC: 1x4 / 1x4
    _t17_180 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_177), _mm256_castpd256_pd128(_t11_226)));

    // AVX Storer:
    _t17_21 = _t17_180;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 3)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, 0)) * G(h(3, 52, 0), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_181 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_57, _t17_57, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_182 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_19, _t17_20), _mm256_unpacklo_pd(_t17_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_183 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t11_231 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_182, _t17_183), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_182, _t17_183), _mm256_mul_pd(_t17_182, _t17_183), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_182, _t17_183), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_182, _t17_183), _mm256_mul_pd(_t17_182, _t17_183), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_182, _t17_183), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_182, _t17_183), _mm256_mul_pd(_t17_182, _t17_183), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_184 = _mm256_sub_pd(_t17_181, _t11_231);

    // AVX Storer:
    _t17_22 = _t17_184;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 3)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 3), U[52,52],h(1, 52, 3)) ) ),h(1, 52, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_185 = _t17_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_186 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_187 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t11_236 = _mm256_add_pd(_t17_186, _t17_187);

    // 4-BLAC: 1x4 / 1x4
    _t17_188 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_185), _mm256_castpd256_pd128(_t11_236)));

    // AVX Storer:
    _t17_22 = _t17_188;

    // Generating : X[52,52] = S(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, 4)) - ( G(h(4, 52, fi879), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t11_60 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_7, _t17_7, 32), _mm256_permute2f128_pd(_t17_7, _t17_7, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_8, _t17_8, 32), _mm256_permute2f128_pd(_t17_8, _t17_8, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_9, _t17_9, 32), _mm256_permute2f128_pd(_t17_9, _t17_9, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_10, _t17_10, 32), _mm256_permute2f128_pd(_t17_10, _t17_10, 32), 0), _t0_22)));
    _t11_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_11, _t17_11, 32), _mm256_permute2f128_pd(_t17_11, _t17_11, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_12, _t17_12, 32), _mm256_permute2f128_pd(_t17_12, _t17_12, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_13, _t17_13, 32), _mm256_permute2f128_pd(_t17_13, _t17_13, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_14, _t17_14, 32), _mm256_permute2f128_pd(_t17_14, _t17_14, 32), 0), _t0_22)));
    _t11_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_15, _t17_15, 32), _mm256_permute2f128_pd(_t17_15, _t17_15, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_16, _t17_16, 32), _mm256_permute2f128_pd(_t17_16, _t17_16, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_17, _t17_17, 32), _mm256_permute2f128_pd(_t17_17, _t17_17, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_18, _t17_18, 32), _mm256_permute2f128_pd(_t17_18, _t17_18, 32), 0), _t0_22)));
    _t11_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_19, _t17_19, 32), _mm256_permute2f128_pd(_t17_19, _t17_19, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 32), _mm256_permute2f128_pd(_t17_20, _t17_20, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 32), _mm256_permute2f128_pd(_t17_21, _t17_21, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 32), _mm256_permute2f128_pd(_t17_22, _t17_22, 32), 0), _t0_22)));

    // 4-BLAC: 4x4 - 4x4
    _t17_58 = _mm256_sub_pd(_t17_58, _t11_60);
    _t17_59 = _mm256_sub_pd(_t17_59, _t11_61);
    _t17_60 = _mm256_sub_pd(_t17_60, _t11_62);
    _t17_61 = _mm256_sub_pd(_t17_61, _t11_63);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_189 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_58, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_190 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_191 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t11_241 = _mm256_add_pd(_t17_190, _t17_191);

    // 4-BLAC: 1x4 / 1x4
    _t17_192 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_189), _mm256_castpd256_pd128(_t11_241)));

    // AVX Storer:
    _t17_23 = _t17_192;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_193 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_58, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_194 = _t17_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_195 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_246 = _mm256_mul_pd(_t17_194, _t17_195);

    // 4-BLAC: 1x4 - 1x4
    _t17_196 = _mm256_sub_pd(_t17_193, _t11_246);

    // AVX Storer:
    _t17_24 = _t17_196;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_197 = _t17_24;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_198 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_199 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t11_251 = _mm256_add_pd(_t17_198, _t17_199);

    // 4-BLAC: 1x4 / 1x4
    _t17_200 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_197), _mm256_castpd256_pd128(_t11_251)));

    // AVX Storer:
    _t17_24 = _t17_200;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_201 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_58, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_58, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_202 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_23, _t17_24), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_203 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t11_256 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_202, _t17_203), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_202, _t17_203), _mm256_mul_pd(_t17_202, _t17_203), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_202, _t17_203), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_202, _t17_203), _mm256_mul_pd(_t17_202, _t17_203), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_202, _t17_203), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_202, _t17_203), _mm256_mul_pd(_t17_202, _t17_203), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_204 = _mm256_sub_pd(_t17_201, _t11_256);

    // AVX Storer:
    _t17_25 = _t17_204;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_205 = _t17_25;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_206 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_207 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t11_261 = _mm256_add_pd(_t17_206, _t17_207);

    // 4-BLAC: 1x4 / 1x4
    _t17_208 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_205), _mm256_castpd256_pd128(_t11_261)));

    // AVX Storer:
    _t17_25 = _t17_208;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_209 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_58, _t17_58, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_210 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_23, _t17_24), _mm256_unpacklo_pd(_t17_25, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_211 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t11_266 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_210, _t17_211), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_210, _t17_211), _mm256_mul_pd(_t17_210, _t17_211), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_210, _t17_211), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_210, _t17_211), _mm256_mul_pd(_t17_210, _t17_211), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_210, _t17_211), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_210, _t17_211), _mm256_mul_pd(_t17_210, _t17_211), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_212 = _mm256_sub_pd(_t17_209, _t11_266);

    // AVX Storer:
    _t17_26 = _t17_212;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_213 = _t17_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_214 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_215 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t11_271 = _mm256_add_pd(_t17_214, _t17_215);

    // 4-BLAC: 1x4 / 1x4
    _t17_216 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_213), _mm256_castpd256_pd128(_t11_271)));

    // AVX Storer:
    _t17_26 = _t17_216;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_217 = _t17_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t11_56 = _mm256_mul_pd(_t17_217, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_23, _t17_24), _mm256_unpacklo_pd(_t17_25, _t17_26), 32));

    // 4-BLAC: 1x4 - 1x4
    _t17_59 = _mm256_sub_pd(_t17_59, _t11_56);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_218 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_59, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_219 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_220 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t11_277 = _mm256_add_pd(_t17_219, _t17_220);

    // 4-BLAC: 1x4 / 1x4
    _t17_221 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_218), _mm256_castpd256_pd128(_t11_277)));

    // AVX Storer:
    _t17_27 = _t17_221;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_222 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_59, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_223 = _t17_27;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_224 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_282 = _mm256_mul_pd(_t17_223, _t17_224);

    // 4-BLAC: 1x4 - 1x4
    _t17_225 = _mm256_sub_pd(_t17_222, _t11_282);

    // AVX Storer:
    _t17_28 = _t17_225;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_226 = _t17_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_227 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_228 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t11_287 = _mm256_add_pd(_t17_227, _t17_228);

    // 4-BLAC: 1x4 / 1x4
    _t17_229 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_226), _mm256_castpd256_pd128(_t11_287)));

    // AVX Storer:
    _t17_28 = _t17_229;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_230 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_59, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_59, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_231 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_27, _t17_28), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_232 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t11_292 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_231, _t17_232), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_231, _t17_232), _mm256_mul_pd(_t17_231, _t17_232), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_231, _t17_232), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_231, _t17_232), _mm256_mul_pd(_t17_231, _t17_232), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_231, _t17_232), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_231, _t17_232), _mm256_mul_pd(_t17_231, _t17_232), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_233 = _mm256_sub_pd(_t17_230, _t11_292);

    // AVX Storer:
    _t17_29 = _t17_233;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_234 = _t17_29;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_235 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_236 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t11_297 = _mm256_add_pd(_t17_235, _t17_236);

    // 4-BLAC: 1x4 / 1x4
    _t17_237 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_234), _mm256_castpd256_pd128(_t11_297)));

    // AVX Storer:
    _t17_29 = _t17_237;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_238 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_59, _t17_59, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_239 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_27, _t17_28), _mm256_unpacklo_pd(_t17_29, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_240 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t11_302 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_239, _t17_240), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_239, _t17_240), _mm256_mul_pd(_t17_239, _t17_240), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_239, _t17_240), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_239, _t17_240), _mm256_mul_pd(_t17_239, _t17_240), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_239, _t17_240), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_239, _t17_240), _mm256_mul_pd(_t17_239, _t17_240), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_241 = _mm256_sub_pd(_t17_238, _t11_302);

    // AVX Storer:
    _t17_30 = _t17_241;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_242 = _t17_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_243 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_244 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t11_307 = _mm256_add_pd(_t17_243, _t17_244);

    // 4-BLAC: 1x4 / 1x4
    _t17_245 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_242), _mm256_castpd256_pd128(_t11_307)));

    // AVX Storer:
    _t17_30 = _t17_245;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_246 = _t17_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t17_247 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_23, _t17_24), _mm256_unpacklo_pd(_t17_25, _t17_26), 32);
    _t17_248 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_27, _t17_28), _mm256_unpacklo_pd(_t17_29, _t17_30), 32);
    _t17_249 = _mm256_setzero_pd();
    _t17_250 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_246, _t17_246, 32), _mm256_permute2f128_pd(_t17_246, _t17_246, 32), 0), _t17_247), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_246, _t17_246, 32), _mm256_permute2f128_pd(_t17_246, _t17_246, 32), 15), _t17_248)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_246, _t17_246, 49), _mm256_permute2f128_pd(_t17_246, _t17_246, 49), 0), _t17_249), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_246, _t17_246, 49), _mm256_permute2f128_pd(_t17_246, _t17_246, 49), 15), _t17_250)));

    // 4-BLAC: 1x4 - 1x4
    _t17_60 = _mm256_sub_pd(_t17_60, _t11_64);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_251 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_60, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_252 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_253 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t11_317 = _mm256_add_pd(_t17_252, _t17_253);

    // 4-BLAC: 1x4 / 1x4
    _t17_254 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_251), _mm256_castpd256_pd128(_t11_317)));

    // AVX Storer:
    _t17_31 = _t17_254;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_255 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_60, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_256 = _t17_31;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_257 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_322 = _mm256_mul_pd(_t17_256, _t17_257);

    // 4-BLAC: 1x4 - 1x4
    _t17_258 = _mm256_sub_pd(_t17_255, _t11_322);

    // AVX Storer:
    _t17_32 = _t17_258;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_259 = _t17_32;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_260 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_261 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t11_327 = _mm256_add_pd(_t17_260, _t17_261);

    // 4-BLAC: 1x4 / 1x4
    _t17_262 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_259), _mm256_castpd256_pd128(_t11_327)));

    // AVX Storer:
    _t17_32 = _t17_262;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_263 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_60, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_60, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_264 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_31, _t17_32), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_265 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t11_332 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_264, _t17_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_264, _t17_265), _mm256_mul_pd(_t17_264, _t17_265), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_264, _t17_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_264, _t17_265), _mm256_mul_pd(_t17_264, _t17_265), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_264, _t17_265), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_264, _t17_265), _mm256_mul_pd(_t17_264, _t17_265), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_266 = _mm256_sub_pd(_t17_263, _t11_332);

    // AVX Storer:
    _t17_33 = _t17_266;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_267 = _t17_33;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_268 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_269 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t11_337 = _mm256_add_pd(_t17_268, _t17_269);

    // 4-BLAC: 1x4 / 1x4
    _t17_270 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_267), _mm256_castpd256_pd128(_t11_337)));

    // AVX Storer:
    _t17_33 = _t17_270;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_271 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_60, _t17_60, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_272 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_31, _t17_32), _mm256_unpacklo_pd(_t17_33, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_273 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t11_342 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_272, _t17_273), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_272, _t17_273), _mm256_mul_pd(_t17_272, _t17_273), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_272, _t17_273), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_272, _t17_273), _mm256_mul_pd(_t17_272, _t17_273), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_272, _t17_273), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_272, _t17_273), _mm256_mul_pd(_t17_272, _t17_273), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_274 = _mm256_sub_pd(_t17_271, _t11_342);

    // AVX Storer:
    _t17_34 = _t17_274;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_275 = _t17_34;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_276 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_277 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t11_347 = _mm256_add_pd(_t17_276, _t17_277);

    // 4-BLAC: 1x4 / 1x4
    _t17_278 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_275), _mm256_castpd256_pd128(_t11_347)));

    // AVX Storer:
    _t17_34 = _t17_278;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, 4)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, 4)) ) ),h(4, 52, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_279 = _t17_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t17_280 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_23, _t17_24), _mm256_unpacklo_pd(_t17_25, _t17_26), 32);
    _t17_281 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_27, _t17_28), _mm256_unpacklo_pd(_t17_29, _t17_30), 32);
    _t17_282 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_31, _t17_32), _mm256_unpacklo_pd(_t17_33, _t17_34), 32);
    _t17_283 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_279, _t17_279, 32), _mm256_permute2f128_pd(_t17_279, _t17_279, 32), 0), _t17_280), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_279, _t17_279, 32), _mm256_permute2f128_pd(_t17_279, _t17_279, 32), 15), _t17_281)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_279, _t17_279, 49), _mm256_permute2f128_pd(_t17_279, _t17_279, 49), 0), _t17_282), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_279, _t17_279, 49), _mm256_permute2f128_pd(_t17_279, _t17_279, 49), 15), _t17_283)));

    // 4-BLAC: 1x4 - 1x4
    _t17_61 = _mm256_sub_pd(_t17_61, _t11_65);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 4)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 4), U[52,52],h(1, 52, 4)) ) ),h(1, 52, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_284 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_61, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_285 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_286 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t11_357 = _mm256_add_pd(_t17_285, _t17_286);

    // 4-BLAC: 1x4 / 1x4
    _t17_287 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_284), _mm256_castpd256_pd128(_t11_357)));

    // AVX Storer:
    _t17_35 = _t17_287;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 5)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 4)) Kro G(h(1, 52, 4), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_288 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_61, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_289 = _t17_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_290 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_362 = _mm256_mul_pd(_t17_289, _t17_290);

    // 4-BLAC: 1x4 - 1x4
    _t17_291 = _mm256_sub_pd(_t17_288, _t11_362);

    // AVX Storer:
    _t17_36 = _t17_291;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 5)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 5), U[52,52],h(1, 52, 5)) ) ),h(1, 52, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_292 = _t17_36;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_293 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_294 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t11_367 = _mm256_add_pd(_t17_293, _t17_294);

    // 4-BLAC: 1x4 / 1x4
    _t17_295 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_292), _mm256_castpd256_pd128(_t11_367)));

    // AVX Storer:
    _t17_36 = _t17_295;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 6)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, 4)) * G(h(2, 52, 4), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_296 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_61, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_61, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_297 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_35, _t17_36), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_298 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t11_372 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_297, _t17_298), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_297, _t17_298), _mm256_mul_pd(_t17_297, _t17_298), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_297, _t17_298), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_297, _t17_298), _mm256_mul_pd(_t17_297, _t17_298), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_297, _t17_298), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_297, _t17_298), _mm256_mul_pd(_t17_297, _t17_298), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_299 = _mm256_sub_pd(_t17_296, _t11_372);

    // AVX Storer:
    _t17_37 = _t17_299;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 6)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 6), U[52,52],h(1, 52, 6)) ) ),h(1, 52, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_300 = _t17_37;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_301 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_302 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t11_377 = _mm256_add_pd(_t17_301, _t17_302);

    // 4-BLAC: 1x4 / 1x4
    _t17_303 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_300), _mm256_castpd256_pd128(_t11_377)));

    // AVX Storer:
    _t17_37 = _t17_303;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 7)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, 4)) * G(h(3, 52, 4), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_304 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_61, _t17_61, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_305 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_35, _t17_36), _mm256_unpacklo_pd(_t17_37, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_306 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t11_382 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_305, _t17_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_305, _t17_306), _mm256_mul_pd(_t17_305, _t17_306), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_305, _t17_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_305, _t17_306), _mm256_mul_pd(_t17_305, _t17_306), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_305, _t17_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_305, _t17_306), _mm256_mul_pd(_t17_305, _t17_306), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_307 = _mm256_sub_pd(_t17_304, _t11_382);

    // AVX Storer:
    _t17_38 = _t17_307;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, 7)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, 7), U[52,52],h(1, 52, 7)) ) ),h(1, 52, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_308 = _t17_38;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_309 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_310 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t11_387 = _mm256_add_pd(_t17_309, _t17_310);

    // 4-BLAC: 1x4 / 1x4
    _t17_311 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_308), _mm256_castpd256_pd128(_t11_387)));

    // AVX Storer:
    _t17_38 = _t17_311;

    // Generating : X[52,52] = ( S(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, fi1082)) - ( G(h(4, 52, fi879), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082)) + Sum_{k116} ( -$(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi1082)) ),h(4, 52, fi1082)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t11_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_7, _t17_7, 32), _mm256_permute2f128_pd(_t17_7, _t17_7, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_8, _t17_8, 32), _mm256_permute2f128_pd(_t17_8, _t17_8, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_9, _t17_9, 32), _mm256_permute2f128_pd(_t17_9, _t17_9, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_10, _t17_10, 32), _mm256_permute2f128_pd(_t17_10, _t17_10, 32), 0), _t0_11)));
    _t11_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_11, _t17_11, 32), _mm256_permute2f128_pd(_t17_11, _t17_11, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_12, _t17_12, 32), _mm256_permute2f128_pd(_t17_12, _t17_12, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_13, _t17_13, 32), _mm256_permute2f128_pd(_t17_13, _t17_13, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_14, _t17_14, 32), _mm256_permute2f128_pd(_t17_14, _t17_14, 32), 0), _t0_11)));
    _t11_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_15, _t17_15, 32), _mm256_permute2f128_pd(_t17_15, _t17_15, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_16, _t17_16, 32), _mm256_permute2f128_pd(_t17_16, _t17_16, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_17, _t17_17, 32), _mm256_permute2f128_pd(_t17_17, _t17_17, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_18, _t17_18, 32), _mm256_permute2f128_pd(_t17_18, _t17_18, 32), 0), _t0_11)));
    _t11_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_19, _t17_19, 32), _mm256_permute2f128_pd(_t17_19, _t17_19, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 32), _mm256_permute2f128_pd(_t17_20, _t17_20, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 32), _mm256_permute2f128_pd(_t17_21, _t17_21, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 32), _mm256_permute2f128_pd(_t17_22, _t17_22, 32), 0), _t0_11)));

    // 4-BLAC: 4x4 - 4x4
    _t17_62 = _mm256_sub_pd(_t17_62, _t11_66);
    _t17_63 = _mm256_sub_pd(_t17_63, _t11_67);
    _t17_64 = _mm256_sub_pd(_t17_64, _t11_68);
    _t17_65 = _mm256_sub_pd(_t17_65, _t11_69);

    // AVX Storer:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t11_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_23, _t17_23, 32), _mm256_permute2f128_pd(_t17_23, _t17_23, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_24, _t17_24, 32), _mm256_permute2f128_pd(_t17_24, _t17_24, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_25, _t17_25, 32), _mm256_permute2f128_pd(_t17_25, _t17_25, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_26, _t17_26, 32), _mm256_permute2f128_pd(_t17_26, _t17_26, 32), 0), _t0_7)));
    _t11_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_27, _t17_27, 32), _mm256_permute2f128_pd(_t17_27, _t17_27, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_28, _t17_28, 32), _mm256_permute2f128_pd(_t17_28, _t17_28, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_29, _t17_29, 32), _mm256_permute2f128_pd(_t17_29, _t17_29, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_30, _t17_30, 32), _mm256_permute2f128_pd(_t17_30, _t17_30, 32), 0), _t0_7)));
    _t11_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_31, _t17_31, 32), _mm256_permute2f128_pd(_t17_31, _t17_31, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_32, _t17_32, 32), _mm256_permute2f128_pd(_t17_32, _t17_32, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_33, _t17_33, 32), _mm256_permute2f128_pd(_t17_33, _t17_33, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_34, _t17_34, 32), _mm256_permute2f128_pd(_t17_34, _t17_34, 32), 0), _t0_7)));
    _t11_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_35, _t17_35, 32), _mm256_permute2f128_pd(_t17_35, _t17_35, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_36, _t17_36, 32), _mm256_permute2f128_pd(_t17_36, _t17_36, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_37, _t17_37, 32), _mm256_permute2f128_pd(_t17_37, _t17_37, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_38, _t17_38, 32), _mm256_permute2f128_pd(_t17_38, _t17_38, 32), 0), _t0_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t17_62 = _mm256_sub_pd(_t17_62, _t11_70);
    _t17_63 = _mm256_sub_pd(_t17_63, _t11_71);
    _t17_64 = _mm256_sub_pd(_t17_64, _t11_72);
    _t17_65 = _mm256_sub_pd(_t17_65, _t11_73);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_312 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_62, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_313 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_314 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_392 = _mm256_add_pd(_t17_313, _t17_314);

    // 4-BLAC: 1x4 / 1x4
    _t17_315 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_312), _mm256_castpd256_pd128(_t11_392)));

    // AVX Storer:
    _t17_39 = _t17_315;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_316 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_62, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_317 = _t17_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_318 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_397 = _mm256_mul_pd(_t17_317, _t17_318);

    // 4-BLAC: 1x4 - 1x4
    _t17_319 = _mm256_sub_pd(_t17_316, _t11_397);

    // AVX Storer:
    _t17_40 = _t17_319;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_320 = _t17_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_321 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_322 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_402 = _mm256_add_pd(_t17_321, _t17_322);

    // 4-BLAC: 1x4 / 1x4
    _t17_323 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_320), _mm256_castpd256_pd128(_t11_402)));

    // AVX Storer:
    _t17_40 = _t17_323;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_324 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_62, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_62, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_325 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_39, _t17_40), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_326 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_325, _t17_326), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_325, _t17_326), _mm256_mul_pd(_t17_325, _t17_326), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_325, _t17_326), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_325, _t17_326), _mm256_mul_pd(_t17_325, _t17_326), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_325, _t17_326), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_325, _t17_326), _mm256_mul_pd(_t17_325, _t17_326), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_327 = _mm256_sub_pd(_t17_324, _t11_407);

    // AVX Storer:
    _t17_41 = _t17_327;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_328 = _t17_41;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_329 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_330 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_412 = _mm256_add_pd(_t17_329, _t17_330);

    // 4-BLAC: 1x4 / 1x4
    _t17_331 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_328), _mm256_castpd256_pd128(_t11_412)));

    // AVX Storer:
    _t17_41 = _t17_331;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_332 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_62, _t17_62, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_333 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_39, _t17_40), _mm256_unpacklo_pd(_t17_41, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_334 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_417 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_333, _t17_334), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_333, _t17_334), _mm256_mul_pd(_t17_333, _t17_334), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_333, _t17_334), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_333, _t17_334), _mm256_mul_pd(_t17_333, _t17_334), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_333, _t17_334), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_333, _t17_334), _mm256_mul_pd(_t17_333, _t17_334), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_335 = _mm256_sub_pd(_t17_332, _t11_417);

    // AVX Storer:
    _t17_42 = _t17_335;

    // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_336 = _t17_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_337 = _t17_6;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_338 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_422 = _mm256_add_pd(_t17_337, _t17_338);

    // 4-BLAC: 1x4 / 1x4
    _t17_339 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_336), _mm256_castpd256_pd128(_t11_422)));

    // AVX Storer:
    _t17_42 = _t17_339;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_340 = _t17_5;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t11_57 = _mm256_mul_pd(_t17_340, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_39, _t17_40), _mm256_unpacklo_pd(_t17_41, _t17_42), 32));

    // 4-BLAC: 1x4 - 1x4
    _t17_63 = _mm256_sub_pd(_t17_63, _t11_57);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_341 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_63, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_342 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_343 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_428 = _mm256_add_pd(_t17_342, _t17_343);

    // 4-BLAC: 1x4 / 1x4
    _t17_344 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_341), _mm256_castpd256_pd128(_t11_428)));

    // AVX Storer:
    _t17_43 = _t17_344;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_345 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_63, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_346 = _t17_43;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_347 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_433 = _mm256_mul_pd(_t17_346, _t17_347);

    // 4-BLAC: 1x4 - 1x4
    _t17_348 = _mm256_sub_pd(_t17_345, _t11_433);

    // AVX Storer:
    _t17_44 = _t17_348;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_349 = _t17_44;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_350 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_351 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_438 = _mm256_add_pd(_t17_350, _t17_351);

    // 4-BLAC: 1x4 / 1x4
    _t17_352 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_349), _mm256_castpd256_pd128(_t11_438)));

    // AVX Storer:
    _t17_44 = _t17_352;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_353 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_63, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_63, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_354 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_43, _t17_44), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_355 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_443 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_354, _t17_355), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_354, _t17_355), _mm256_mul_pd(_t17_354, _t17_355), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_354, _t17_355), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_354, _t17_355), _mm256_mul_pd(_t17_354, _t17_355), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_354, _t17_355), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_354, _t17_355), _mm256_mul_pd(_t17_354, _t17_355), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_356 = _mm256_sub_pd(_t17_353, _t11_443);

    // AVX Storer:
    _t17_45 = _t17_356;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_357 = _t17_45;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_358 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_359 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_448 = _mm256_add_pd(_t17_358, _t17_359);

    // 4-BLAC: 1x4 / 1x4
    _t17_360 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_357), _mm256_castpd256_pd128(_t11_448)));

    // AVX Storer:
    _t17_45 = _t17_360;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_361 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_63, _t17_63, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_362 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_43, _t17_44), _mm256_unpacklo_pd(_t17_45, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_363 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_453 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_362, _t17_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_362, _t17_363), _mm256_mul_pd(_t17_362, _t17_363), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_362, _t17_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_362, _t17_363), _mm256_mul_pd(_t17_362, _t17_363), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_362, _t17_363), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_362, _t17_363), _mm256_mul_pd(_t17_362, _t17_363), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_364 = _mm256_sub_pd(_t17_361, _t11_453);

    // AVX Storer:
    _t17_46 = _t17_364;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_365 = _t17_46;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_366 = _t17_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_367 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_458 = _mm256_add_pd(_t17_366, _t17_367);

    // 4-BLAC: 1x4 / 1x4
    _t17_368 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_365), _mm256_castpd256_pd128(_t11_458)));

    // AVX Storer:
    _t17_46 = _t17_368;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_369 = _t17_3;

    // AVX Loader:

    // 2x4 -> 4x4
    _t17_370 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_39, _t17_40), _mm256_unpacklo_pd(_t17_41, _t17_42), 32);
    _t17_371 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_43, _t17_44), _mm256_unpacklo_pd(_t17_45, _t17_46), 32);
    _t17_372 = _mm256_setzero_pd();
    _t17_373 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_369, _t17_369, 32), _mm256_permute2f128_pd(_t17_369, _t17_369, 32), 0), _t17_370), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_369, _t17_369, 32), _mm256_permute2f128_pd(_t17_369, _t17_369, 32), 15), _t17_371)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_369, _t17_369, 49), _mm256_permute2f128_pd(_t17_369, _t17_369, 49), 0), _t17_372), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_369, _t17_369, 49), _mm256_permute2f128_pd(_t17_369, _t17_369, 49), 15), _t17_373)));

    // 4-BLAC: 1x4 - 1x4
    _t17_64 = _mm256_sub_pd(_t17_64, _t11_74);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_374 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_64, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_375 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_376 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_468 = _mm256_add_pd(_t17_375, _t17_376);

    // 4-BLAC: 1x4 / 1x4
    _t17_377 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_374), _mm256_castpd256_pd128(_t11_468)));

    // AVX Storer:
    _t17_47 = _t17_377;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_378 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_64, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_379 = _t17_47;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_380 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_473 = _mm256_mul_pd(_t17_379, _t17_380);

    // 4-BLAC: 1x4 - 1x4
    _t17_381 = _mm256_sub_pd(_t17_378, _t11_473);

    // AVX Storer:
    _t17_48 = _t17_381;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_382 = _t17_48;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_383 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_384 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_478 = _mm256_add_pd(_t17_383, _t17_384);

    // 4-BLAC: 1x4 / 1x4
    _t17_385 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_382), _mm256_castpd256_pd128(_t11_478)));

    // AVX Storer:
    _t17_48 = _t17_385;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_386 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_64, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_64, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_387 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_47, _t17_48), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_388 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_387, _t17_388), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_387, _t17_388), _mm256_mul_pd(_t17_387, _t17_388), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_387, _t17_388), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_387, _t17_388), _mm256_mul_pd(_t17_387, _t17_388), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_387, _t17_388), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_387, _t17_388), _mm256_mul_pd(_t17_387, _t17_388), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_389 = _mm256_sub_pd(_t17_386, _t11_483);

    // AVX Storer:
    _t17_49 = _t17_389;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_390 = _t17_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_391 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_392 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_488 = _mm256_add_pd(_t17_391, _t17_392);

    // 4-BLAC: 1x4 / 1x4
    _t17_393 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_390), _mm256_castpd256_pd128(_t11_488)));

    // AVX Storer:
    _t17_49 = _t17_393;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_394 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_64, _t17_64, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_395 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_47, _t17_48), _mm256_unpacklo_pd(_t17_49, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_396 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_493 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_395, _t17_396), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_395, _t17_396), _mm256_mul_pd(_t17_395, _t17_396), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_395, _t17_396), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_395, _t17_396), _mm256_mul_pd(_t17_395, _t17_396), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_395, _t17_396), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_395, _t17_396), _mm256_mul_pd(_t17_395, _t17_396), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_397 = _mm256_sub_pd(_t17_394, _t11_493);

    // AVX Storer:
    _t17_50 = _t17_397;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_398 = _t17_50;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_399 = _t17_2;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_400 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_498 = _mm256_add_pd(_t17_399, _t17_400);

    // 4-BLAC: 1x4 / 1x4
    _t17_401 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_398), _mm256_castpd256_pd128(_t11_498)));

    // AVX Storer:
    _t17_50 = _t17_401;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

    // AVX Loader:

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_402 = _t17_1;

    // AVX Loader:

    // 3x4 -> 4x4
    _t17_403 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_39, _t17_40), _mm256_unpacklo_pd(_t17_41, _t17_42), 32);
    _t17_404 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_43, _t17_44), _mm256_unpacklo_pd(_t17_45, _t17_46), 32);
    _t17_405 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_47, _t17_48), _mm256_unpacklo_pd(_t17_49, _t17_50), 32);
    _t17_406 = _mm256_setzero_pd();

    // 4-BLAC: 1x4 * 4x4
    _t11_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_402, _t17_402, 32), _mm256_permute2f128_pd(_t17_402, _t17_402, 32), 0), _t17_403), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_402, _t17_402, 32), _mm256_permute2f128_pd(_t17_402, _t17_402, 32), 15), _t17_404)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_402, _t17_402, 49), _mm256_permute2f128_pd(_t17_402, _t17_402, 49), 0), _t17_405), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_402, _t17_402, 49), _mm256_permute2f128_pd(_t17_402, _t17_402, 49), 15), _t17_406)));

    // 4-BLAC: 1x4 - 1x4
    _t17_65 = _mm256_sub_pd(_t17_65, _t11_75);

    // AVX Storer:

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_407 = _mm256_blend_pd(_mm256_setzero_pd(), _t17_65, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_408 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_409 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t11_508 = _mm256_add_pd(_t17_408, _t17_409);

    // 4-BLAC: 1x4 / 1x4
    _t17_410 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_407), _mm256_castpd256_pd128(_t11_508)));

    // AVX Storer:
    _t17_51 = _t17_410;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_411 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_65, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_412 = _t17_51;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_413 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t11_513 = _mm256_mul_pd(_t17_412, _t17_413);

    // 4-BLAC: 1x4 - 1x4
    _t17_414 = _mm256_sub_pd(_t17_411, _t11_513);

    // AVX Storer:
    _t17_52 = _t17_414;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_415 = _t17_52;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_416 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_417 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t11_518 = _mm256_add_pd(_t17_416, _t17_417);

    // 4-BLAC: 1x4 / 1x4
    _t17_418 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_415), _mm256_castpd256_pd128(_t11_518)));

    // AVX Storer:
    _t17_52 = _t17_418;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_419 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t17_65, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t17_65, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t17_420 = _mm256_blend_pd(_mm256_unpacklo_pd(_t17_51, _t17_52), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t17_421 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t11_523 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_420, _t17_421), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_420, _t17_421), _mm256_mul_pd(_t17_420, _t17_421), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_420, _t17_421), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_420, _t17_421), _mm256_mul_pd(_t17_420, _t17_421), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_420, _t17_421), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_420, _t17_421), _mm256_mul_pd(_t17_420, _t17_421), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_422 = _mm256_sub_pd(_t17_419, _t11_523);

    // AVX Storer:
    _t17_53 = _t17_422;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_423 = _t17_53;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_424 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_425 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t11_528 = _mm256_add_pd(_t17_424, _t17_425);

    // 4-BLAC: 1x4 / 1x4
    _t17_426 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_423), _mm256_castpd256_pd128(_t11_528)));

    // AVX Storer:
    _t17_53 = _t17_426;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_427 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t17_65, _t17_65, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t17_428 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_51, _t17_52), _mm256_unpacklo_pd(_t17_53, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t17_429 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t11_533 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t17_428, _t17_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_428, _t17_429), _mm256_mul_pd(_t17_428, _t17_429), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t17_428, _t17_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_428, _t17_429), _mm256_mul_pd(_t17_428, _t17_429), 129)), _mm256_add_pd(_mm256_mul_pd(_t17_428, _t17_429), _mm256_permute2f128_pd(_mm256_mul_pd(_t17_428, _t17_429), _mm256_mul_pd(_t17_428, _t17_429), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t17_430 = _mm256_sub_pd(_t17_427, _t11_533);

    // AVX Storer:
    _t17_54 = _t17_430;

    // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_431 = _t17_54;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_432 = _t17_0;

    // AVX Loader:

    // 1x1 -> 1x4
    _t17_433 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t11_538 = _mm256_add_pd(_t17_432, _t17_433);

    // 4-BLAC: 1x4 / 1x4
    _t17_434 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t17_431), _mm256_castpd256_pd128(_t11_538)));

    // AVX Storer:
    _t17_54 = _t17_434;
    _asm256_storeu_pd(C + 52*fi879 + 52, _t17_55);
    _asm256_storeu_pd(C + 52*fi879 + 104, _t17_56);
    _asm256_storeu_pd(C + 52*fi879 + 156, _t17_57);
    _mm_store_sd(&(C[52*fi879 + 4]), _mm256_castpd256_pd128(_t17_23));
    _mm_store_sd(&(C[52*fi879 + 5]), _mm256_castpd256_pd128(_t17_24));
    _mm_store_sd(&(C[52*fi879 + 6]), _mm256_castpd256_pd128(_t17_25));
    _mm_store_sd(&(C[52*fi879 + 7]), _mm256_castpd256_pd128(_t17_26));
    _mm_store_sd(&(C[52*fi879 + 56]), _mm256_castpd256_pd128(_t17_27));
    _mm_store_sd(&(C[52*fi879 + 57]), _mm256_castpd256_pd128(_t17_28));
    _mm_store_sd(&(C[52*fi879 + 58]), _mm256_castpd256_pd128(_t17_29));
    _mm_store_sd(&(C[52*fi879 + 59]), _mm256_castpd256_pd128(_t17_30));
    _mm_store_sd(&(C[52*fi879 + 108]), _mm256_castpd256_pd128(_t17_31));
    _mm_store_sd(&(C[52*fi879 + 109]), _mm256_castpd256_pd128(_t17_32));
    _mm_store_sd(&(C[52*fi879 + 110]), _mm256_castpd256_pd128(_t17_33));
    _mm_store_sd(&(C[52*fi879 + 111]), _mm256_castpd256_pd128(_t17_34));
    _mm_store_sd(&(C[52*fi879 + 160]), _mm256_castpd256_pd128(_t17_35));
    _mm_store_sd(&(C[52*fi879 + 161]), _mm256_castpd256_pd128(_t17_36));
    _mm_store_sd(&(C[52*fi879 + 162]), _mm256_castpd256_pd128(_t17_37));
    _mm_store_sd(&(C[52*fi879 + 163]), _mm256_castpd256_pd128(_t17_38));
    _mm_store_sd(&(C[52*fi879 + 8]), _mm256_castpd256_pd128(_t17_39));
    _mm_store_sd(&(C[52*fi879 + 9]), _mm256_castpd256_pd128(_t17_40));
    _mm_store_sd(&(C[52*fi879 + 10]), _mm256_castpd256_pd128(_t17_41));
    _mm_store_sd(&(C[52*fi879 + 11]), _mm256_castpd256_pd128(_t17_42));
    _mm_store_sd(&(C[52*fi879 + 60]), _mm256_castpd256_pd128(_t17_43));
    _mm_store_sd(&(C[52*fi879 + 61]), _mm256_castpd256_pd128(_t17_44));
    _mm_store_sd(&(C[52*fi879 + 62]), _mm256_castpd256_pd128(_t17_45));
    _mm_store_sd(&(C[52*fi879 + 63]), _mm256_castpd256_pd128(_t17_46));
    _mm_store_sd(&(C[52*fi879 + 112]), _mm256_castpd256_pd128(_t17_47));
    _mm_store_sd(&(C[52*fi879 + 113]), _mm256_castpd256_pd128(_t17_48));
    _mm_store_sd(&(C[52*fi879 + 114]), _mm256_castpd256_pd128(_t17_49));
    _mm_store_sd(&(C[52*fi879 + 115]), _mm256_castpd256_pd128(_t17_50));
    _mm_store_sd(&(C[52*fi879 + 164]), _mm256_castpd256_pd128(_t17_51));
    _mm_store_sd(&(C[52*fi879 + 165]), _mm256_castpd256_pd128(_t17_52));
    _mm_store_sd(&(C[52*fi879 + 166]), _mm256_castpd256_pd128(_t17_53));
    _mm_store_sd(&(C[52*fi879 + 167]), _mm256_castpd256_pd128(_t17_54));

    for( int fi1082 = 12; fi1082 <= 48; fi1082+=4 ) {
      _t18_4 = _asm256_loadu_pd(C + fi1082 + 52*fi879);
      _t18_5 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 52);
      _t18_6 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 104);
      _t18_7 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 156);
      _t18_3 = _asm256_loadu_pd(U + fi1082);
      _t18_2 = _asm256_loadu_pd(U + fi1082 + 52);
      _t18_1 = _asm256_loadu_pd(U + fi1082 + 104);
      _t18_0 = _asm256_loadu_pd(U + fi1082 + 156);

      // Generating : X[52,52] = ( S(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, fi1082)) - ( G(h(4, 52, fi879), X[52,52],h(4, 52, 0)) * G(h(4, 52, 0), U[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082)) + Sum_{k116} ( -$(h(4, 52, fi879), ( G(h(4, 52, fi879), X[52,52],h(4, 52, k116)) * G(h(4, 52, k116), U[52,52],h(4, 52, fi1082)) ),h(4, 52, fi1082)) ) )

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t11_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_7, _t17_7, 32), _mm256_permute2f128_pd(_t17_7, _t17_7, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_8, _t17_8, 32), _mm256_permute2f128_pd(_t17_8, _t17_8, 32), 0), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_9, _t17_9, 32), _mm256_permute2f128_pd(_t17_9, _t17_9, 32), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_10, _t17_10, 32), _mm256_permute2f128_pd(_t17_10, _t17_10, 32), 0), _t18_0)));
      _t11_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_11, _t17_11, 32), _mm256_permute2f128_pd(_t17_11, _t17_11, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_12, _t17_12, 32), _mm256_permute2f128_pd(_t17_12, _t17_12, 32), 0), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_13, _t17_13, 32), _mm256_permute2f128_pd(_t17_13, _t17_13, 32), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_14, _t17_14, 32), _mm256_permute2f128_pd(_t17_14, _t17_14, 32), 0), _t18_0)));
      _t11_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_15, _t17_15, 32), _mm256_permute2f128_pd(_t17_15, _t17_15, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_16, _t17_16, 32), _mm256_permute2f128_pd(_t17_16, _t17_16, 32), 0), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_17, _t17_17, 32), _mm256_permute2f128_pd(_t17_17, _t17_17, 32), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_18, _t17_18, 32), _mm256_permute2f128_pd(_t17_18, _t17_18, 32), 0), _t18_0)));
      _t11_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_19, _t17_19, 32), _mm256_permute2f128_pd(_t17_19, _t17_19, 32), 0), _t18_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_20, _t17_20, 32), _mm256_permute2f128_pd(_t17_20, _t17_20, 32), 0), _t18_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_21, _t17_21, 32), _mm256_permute2f128_pd(_t17_21, _t17_21, 32), 0), _t18_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t17_22, _t17_22, 32), _mm256_permute2f128_pd(_t17_22, _t17_22, 32), 0), _t18_0)));

      // 4-BLAC: 4x4 - 4x4
      _t18_4 = _mm256_sub_pd(_t18_4, _t11_66);
      _t18_5 = _mm256_sub_pd(_t18_5, _t11_67);
      _t18_6 = _mm256_sub_pd(_t18_6, _t11_68);
      _t18_7 = _mm256_sub_pd(_t18_7, _t11_69);

      // AVX Storer:
      _asm256_storeu_pd(C + fi1082 + 52*fi879, _t18_4);
      _asm256_storeu_pd(C + fi1082 + 52*fi879 + 52, _t18_5);
      _asm256_storeu_pd(C + fi1082 + 52*fi879 + 104, _t18_6);
      _asm256_storeu_pd(C + fi1082 + 52*fi879 + 156, _t18_7);

      for( int k116 = 4; k116 <= fi1082 - 1; k116+=4 ) {
        _t19_19 = _mm256_broadcast_sd(C + 52*fi879 + k116);
        _t19_18 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 1);
        _t19_17 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 2);
        _t19_16 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 3);
        _t19_15 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 52);
        _t19_14 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 53);
        _t19_13 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 54);
        _t19_12 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 55);
        _t19_11 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 104);
        _t19_10 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 105);
        _t19_9 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 106);
        _t19_8 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 107);
        _t19_7 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 156);
        _t19_6 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 157);
        _t19_5 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 158);
        _t19_4 = _mm256_broadcast_sd(C + 52*fi879 + k116 + 159);
        _t19_3 = _asm256_loadu_pd(U + fi1082 + 52*k116);
        _t19_2 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 52);
        _t19_1 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 104);
        _t19_0 = _asm256_loadu_pd(U + fi1082 + 52*k116 + 156);
        _t19_20 = _asm256_loadu_pd(C + fi1082 + 52*fi879);
        _t19_21 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 52);
        _t19_22 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 104);
        _t19_23 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t11_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_3), _mm256_mul_pd(_t19_18, _t19_2)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_1), _mm256_mul_pd(_t19_16, _t19_0)));
        _t11_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_3), _mm256_mul_pd(_t19_14, _t19_2)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_1), _mm256_mul_pd(_t19_12, _t19_0)));
        _t11_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_3), _mm256_mul_pd(_t19_10, _t19_2)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_1), _mm256_mul_pd(_t19_8, _t19_0)));
        _t11_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_7, _t19_3), _mm256_mul_pd(_t19_6, _t19_2)), _mm256_add_pd(_mm256_mul_pd(_t19_5, _t19_1), _mm256_mul_pd(_t19_4, _t19_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t19_20 = _mm256_sub_pd(_t19_20, _t11_70);
        _t19_21 = _mm256_sub_pd(_t19_21, _t11_71);
        _t19_22 = _mm256_sub_pd(_t19_22, _t11_72);
        _t19_23 = _mm256_sub_pd(_t19_23, _t11_73);

        // AVX Storer:
        _asm256_storeu_pd(C + fi1082 + 52*fi879, _t19_20);
        _asm256_storeu_pd(C + fi1082 + 52*fi879 + 52, _t19_21);
        _asm256_storeu_pd(C + fi1082 + 52*fi879 + 104, _t19_22);
        _asm256_storeu_pd(C + fi1082 + 52*fi879 + 156, _t19_23);
      }
      _t18_6 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 104);
      _t18_7 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 156);
      _t18_5 = _asm256_loadu_pd(C + fi1082 + 52*fi879 + 52);
      _t18_4 = _asm256_loadu_pd(C + fi1082 + 52*fi879);
      _t20_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082])));
      _t20_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 1])));
      _t20_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 53])));
      _t20_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 54)), 0);
      _t20_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 106])));
      _t20_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 55))), _mm256_castpd128_pd256(_mm_load_sd(U + 53*fi1082 + 107)), 32);
      _t20_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[53*fi1082 + 159])));

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_4, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_24 = _t17_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_25 = _t20_6;

      // 4-BLAC: 1x4 + 1x4
      _t11_392 = _mm256_add_pd(_t20_24, _t20_25);

      // 4-BLAC: 1x4 / 1x4
      _t20_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_23), _mm256_castpd256_pd128(_t11_392)));

      // AVX Storer:
      _t20_7 = _t20_26;

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_4, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_28 = _t20_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_29 = _t20_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t11_397 = _mm256_mul_pd(_t20_28, _t20_29);

      // 4-BLAC: 1x4 - 1x4
      _t20_30 = _mm256_sub_pd(_t20_27, _t11_397);

      // AVX Storer:
      _t20_8 = _t20_30;

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_31 = _t20_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_32 = _t17_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_33 = _t20_4;

      // 4-BLAC: 1x4 + 1x4
      _t11_402 = _mm256_add_pd(_t20_32, _t20_33);

      // 4-BLAC: 1x4 / 1x4
      _t20_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_31), _mm256_castpd256_pd128(_t11_402)));

      // AVX Storer:
      _t20_8 = _t20_34;

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_4, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t20_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t20_7, _t20_8), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t20_37 = _t20_3;

      // 4-BLAC: 1x4 * 4x1
      _t11_407 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_36, _t20_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_36, _t20_37), _mm256_mul_pd(_t20_36, _t20_37), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_36, _t20_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_36, _t20_37), _mm256_mul_pd(_t20_36, _t20_37), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_36, _t20_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_36, _t20_37), _mm256_mul_pd(_t20_36, _t20_37), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_38 = _mm256_sub_pd(_t20_35, _t11_407);

      // AVX Storer:
      _t20_9 = _t20_38;

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_39 = _t20_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_40 = _t17_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_41 = _t20_2;

      // 4-BLAC: 1x4 + 1x4
      _t11_412 = _mm256_add_pd(_t20_40, _t20_41);

      // 4-BLAC: 1x4 / 1x4
      _t20_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_39), _mm256_castpd256_pd128(_t11_412)));

      // AVX Storer:
      _t20_9 = _t20_42;

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t18_4, _t18_4, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t20_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_7, _t20_8), _mm256_unpacklo_pd(_t20_9, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t20_45 = _t20_1;

      // 4-BLAC: 1x4 * 4x1
      _t11_417 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_44, _t20_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_44, _t20_45), _mm256_mul_pd(_t20_44, _t20_45), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_44, _t20_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_44, _t20_45), _mm256_mul_pd(_t20_44, _t20_45), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_44, _t20_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_44, _t20_45), _mm256_mul_pd(_t20_44, _t20_45), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_46 = _mm256_sub_pd(_t20_43, _t11_417);

      // AVX Storer:
      _t20_10 = _t20_46;

      // Generating : X[52,52] = S(h(1, 52, fi879), ( G(h(1, 52, fi879), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879), L[52,52],h(1, 52, fi879)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_47 = _t20_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_48 = _t17_6;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_49 = _t20_0;

      // 4-BLAC: 1x4 + 1x4
      _t11_422 = _mm256_add_pd(_t20_48, _t20_49);

      // 4-BLAC: 1x4 / 1x4
      _t20_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_47), _mm256_castpd256_pd128(_t11_422)));

      // AVX Storer:
      _t20_10 = _t20_50;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879)) Kro G(h(1, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_51 = _t17_5;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t11_57 = _mm256_mul_pd(_t20_51, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_7, _t20_8), _mm256_unpacklo_pd(_t20_9, _t20_10), 32));

      // 4-BLAC: 1x4 - 1x4
      _t18_5 = _mm256_sub_pd(_t18_5, _t11_57);

      // AVX Storer:

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_52 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_53 = _t17_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_54 = _t20_6;

      // 4-BLAC: 1x4 + 1x4
      _t11_428 = _mm256_add_pd(_t20_53, _t20_54);

      // 4-BLAC: 1x4 / 1x4
      _t20_55 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_52), _mm256_castpd256_pd128(_t11_428)));

      // AVX Storer:
      _t20_11 = _t20_55;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_56 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_57 = _t20_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_58 = _t20_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t11_433 = _mm256_mul_pd(_t20_57, _t20_58);

      // 4-BLAC: 1x4 - 1x4
      _t20_59 = _mm256_sub_pd(_t20_56, _t11_433);

      // AVX Storer:
      _t20_12 = _t20_59;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_60 = _t20_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_61 = _t17_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_62 = _t20_4;

      // 4-BLAC: 1x4 + 1x4
      _t11_438 = _mm256_add_pd(_t20_61, _t20_62);

      // 4-BLAC: 1x4 / 1x4
      _t20_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_60), _mm256_castpd256_pd128(_t11_438)));

      // AVX Storer:
      _t20_12 = _t20_63;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_64 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_5, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t20_65 = _mm256_blend_pd(_mm256_unpacklo_pd(_t20_11, _t20_12), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t20_66 = _t20_3;

      // 4-BLAC: 1x4 * 4x1
      _t11_443 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_65, _t20_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_65, _t20_66), _mm256_mul_pd(_t20_65, _t20_66), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_65, _t20_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_65, _t20_66), _mm256_mul_pd(_t20_65, _t20_66), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_65, _t20_66), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_65, _t20_66), _mm256_mul_pd(_t20_65, _t20_66), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_67 = _mm256_sub_pd(_t20_64, _t11_443);

      // AVX Storer:
      _t20_13 = _t20_67;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_68 = _t20_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_69 = _t17_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_70 = _t20_2;

      // 4-BLAC: 1x4 + 1x4
      _t11_448 = _mm256_add_pd(_t20_69, _t20_70);

      // 4-BLAC: 1x4 / 1x4
      _t20_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_68), _mm256_castpd256_pd128(_t11_448)));

      // AVX Storer:
      _t20_13 = _t20_71;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 1), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_72 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t18_5, _t18_5, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t20_73 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_11, _t20_12), _mm256_unpacklo_pd(_t20_13, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t20_74 = _t20_1;

      // 4-BLAC: 1x4 * 4x1
      _t11_453 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_73, _t20_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_73, _t20_74), _mm256_mul_pd(_t20_73, _t20_74), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_73, _t20_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_73, _t20_74), _mm256_mul_pd(_t20_73, _t20_74), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_73, _t20_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_73, _t20_74), _mm256_mul_pd(_t20_73, _t20_74), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_75 = _mm256_sub_pd(_t20_72, _t11_453);

      // AVX Storer:
      _t20_14 = _t20_75;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 1), ( G(h(1, 52, fi879 + 1), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 1), L[52,52],h(1, 52, fi879 + 1)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_76 = _t20_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_77 = _t17_4;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_78 = _t20_0;

      // 4-BLAC: 1x4 + 1x4
      _t11_458 = _mm256_add_pd(_t20_77, _t20_78);

      // 4-BLAC: 1x4 / 1x4
      _t20_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_76), _mm256_castpd256_pd128(_t11_458)));

      // AVX Storer:
      _t20_14 = _t20_79;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 2), L[52,52],h(2, 52, fi879)) * G(h(2, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

      // AVX Loader:

      // AVX Loader:

      // 1x2 -> 1x4
      _t20_80 = _t17_3;

      // AVX Loader:

      // 2x4 -> 4x4
      _t20_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_7, _t20_8), _mm256_unpacklo_pd(_t20_9, _t20_10), 32);
      _t20_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_11, _t20_12), _mm256_unpacklo_pd(_t20_13, _t20_14), 32);
      _t20_83 = _mm256_setzero_pd();
      _t20_84 = _mm256_setzero_pd();

      // 4-BLAC: 1x4 * 4x4
      _t11_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_80, _t20_80, 32), _mm256_permute2f128_pd(_t20_80, _t20_80, 32), 0), _t20_81), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_80, _t20_80, 32), _mm256_permute2f128_pd(_t20_80, _t20_80, 32), 15), _t20_82)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_80, _t20_80, 49), _mm256_permute2f128_pd(_t20_80, _t20_80, 49), 0), _t20_83), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_80, _t20_80, 49), _mm256_permute2f128_pd(_t20_80, _t20_80, 49), 15), _t20_84)));

      // 4-BLAC: 1x4 - 1x4
      _t18_6 = _mm256_sub_pd(_t18_6, _t11_74);

      // AVX Storer:

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_85 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_86 = _t17_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_87 = _t20_6;

      // 4-BLAC: 1x4 + 1x4
      _t11_468 = _mm256_add_pd(_t20_86, _t20_87);

      // 4-BLAC: 1x4 / 1x4
      _t20_88 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_85), _mm256_castpd256_pd128(_t11_468)));

      // AVX Storer:
      _t20_15 = _t20_88;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_89 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_90 = _t20_15;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_91 = _t20_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t11_473 = _mm256_mul_pd(_t20_90, _t20_91);

      // 4-BLAC: 1x4 - 1x4
      _t20_92 = _mm256_sub_pd(_t20_89, _t11_473);

      // AVX Storer:
      _t20_16 = _t20_92;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_93 = _t20_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_94 = _t17_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_95 = _t20_4;

      // 4-BLAC: 1x4 + 1x4
      _t11_478 = _mm256_add_pd(_t20_94, _t20_95);

      // 4-BLAC: 1x4 / 1x4
      _t20_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_93), _mm256_castpd256_pd128(_t11_478)));

      // AVX Storer:
      _t20_16 = _t20_96;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_97 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_6, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t20_98 = _mm256_blend_pd(_mm256_unpacklo_pd(_t20_15, _t20_16), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t20_99 = _t20_3;

      // 4-BLAC: 1x4 * 4x1
      _t11_483 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_98, _t20_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_98, _t20_99), _mm256_mul_pd(_t20_98, _t20_99), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_98, _t20_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_98, _t20_99), _mm256_mul_pd(_t20_98, _t20_99), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_98, _t20_99), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_98, _t20_99), _mm256_mul_pd(_t20_98, _t20_99), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_100 = _mm256_sub_pd(_t20_97, _t11_483);

      // AVX Storer:
      _t20_17 = _t20_100;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_101 = _t20_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_102 = _t17_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_103 = _t20_2;

      // 4-BLAC: 1x4 + 1x4
      _t11_488 = _mm256_add_pd(_t20_102, _t20_103);

      // 4-BLAC: 1x4 / 1x4
      _t20_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_101), _mm256_castpd256_pd128(_t11_488)));

      // AVX Storer:
      _t20_17 = _t20_104;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 2), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_105 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t18_6, _t18_6, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t20_106 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_15, _t20_16), _mm256_unpacklo_pd(_t20_17, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t20_107 = _t20_1;

      // 4-BLAC: 1x4 * 4x1
      _t11_493 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_106, _t20_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_106, _t20_107), _mm256_mul_pd(_t20_106, _t20_107), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_106, _t20_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_106, _t20_107), _mm256_mul_pd(_t20_106, _t20_107), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_106, _t20_107), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_106, _t20_107), _mm256_mul_pd(_t20_106, _t20_107), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_108 = _mm256_sub_pd(_t20_105, _t11_493);

      // AVX Storer:
      _t20_18 = _t20_108;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 2), ( G(h(1, 52, fi879 + 2), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 2), L[52,52],h(1, 52, fi879 + 2)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_109 = _t20_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_110 = _t17_2;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_111 = _t20_0;

      // 4-BLAC: 1x4 + 1x4
      _t11_498 = _mm256_add_pd(_t20_110, _t20_111);

      // 4-BLAC: 1x4 / 1x4
      _t20_112 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_109), _mm256_castpd256_pd128(_t11_498)));

      // AVX Storer:
      _t20_18 = _t20_112;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(4, 52, fi1082)) - ( G(h(1, 52, fi879 + 3), L[52,52],h(3, 52, fi879)) * G(h(3, 52, fi879), X[52,52],h(4, 52, fi1082)) ) ),h(4, 52, fi1082))

      // AVX Loader:

      // AVX Loader:

      // 1x3 -> 1x4
      _t20_113 = _t17_1;

      // AVX Loader:

      // 3x4 -> 4x4
      _t20_114 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_7, _t20_8), _mm256_unpacklo_pd(_t20_9, _t20_10), 32);
      _t20_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_11, _t20_12), _mm256_unpacklo_pd(_t20_13, _t20_14), 32);
      _t20_116 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_15, _t20_16), _mm256_unpacklo_pd(_t20_17, _t20_18), 32);
      _t20_117 = _mm256_setzero_pd();

      // 4-BLAC: 1x4 * 4x4
      _t11_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_113, _t20_113, 32), _mm256_permute2f128_pd(_t20_113, _t20_113, 32), 0), _t20_114), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_113, _t20_113, 32), _mm256_permute2f128_pd(_t20_113, _t20_113, 32), 15), _t20_115)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_113, _t20_113, 49), _mm256_permute2f128_pd(_t20_113, _t20_113, 49), 0), _t20_116), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t20_113, _t20_113, 49), _mm256_permute2f128_pd(_t20_113, _t20_113, 49), 15), _t20_117)));

      // 4-BLAC: 1x4 - 1x4
      _t18_7 = _mm256_sub_pd(_t18_7, _t11_75);

      // AVX Storer:

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082)) ) ),h(1, 52, fi1082))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_118 = _mm256_blend_pd(_mm256_setzero_pd(), _t18_7, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_119 = _t17_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_120 = _t20_6;

      // 4-BLAC: 1x4 + 1x4
      _t11_508 = _mm256_add_pd(_t20_119, _t20_120);

      // 4-BLAC: 1x4 / 1x4
      _t20_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_118), _mm256_castpd256_pd128(_t11_508)));

      // AVX Storer:
      _t20_19 = _t20_121;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082)) Kro G(h(1, 52, fi1082), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_122 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_7, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_123 = _t20_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_124 = _t20_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t11_513 = _mm256_mul_pd(_t20_123, _t20_124);

      // 4-BLAC: 1x4 - 1x4
      _t20_125 = _mm256_sub_pd(_t20_122, _t11_513);

      // AVX Storer:
      _t20_20 = _t20_125;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 1)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 1), U[52,52],h(1, 52, fi1082 + 1)) ) ),h(1, 52, fi1082 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_126 = _t20_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_127 = _t17_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_128 = _t20_4;

      // 4-BLAC: 1x4 + 1x4
      _t11_518 = _mm256_add_pd(_t20_127, _t20_128);

      // 4-BLAC: 1x4 / 1x4
      _t20_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_126), _mm256_castpd256_pd128(_t11_518)));

      // AVX Storer:
      _t20_20 = _t20_129;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(2, 52, fi1082)) * G(h(2, 52, fi1082), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_130 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t18_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t18_7, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t20_131 = _mm256_blend_pd(_mm256_unpacklo_pd(_t20_19, _t20_20), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t20_132 = _t20_3;

      // 4-BLAC: 1x4 * 4x1
      _t11_523 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_131, _t20_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_131, _t20_132), _mm256_mul_pd(_t20_131, _t20_132), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_131, _t20_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_131, _t20_132), _mm256_mul_pd(_t20_131, _t20_132), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_131, _t20_132), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_131, _t20_132), _mm256_mul_pd(_t20_131, _t20_132), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_133 = _mm256_sub_pd(_t20_130, _t11_523);

      // AVX Storer:
      _t20_21 = _t20_133;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 2)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 2), U[52,52],h(1, 52, fi1082 + 2)) ) ),h(1, 52, fi1082 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_134 = _t20_21;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_135 = _t17_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_136 = _t20_2;

      // 4-BLAC: 1x4 + 1x4
      _t11_528 = _mm256_add_pd(_t20_135, _t20_136);

      // 4-BLAC: 1x4 / 1x4
      _t20_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_134), _mm256_castpd256_pd128(_t11_528)));

      // AVX Storer:
      _t20_21 = _t20_137;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) - ( G(h(1, 52, fi879 + 3), X[52,52],h(3, 52, fi1082)) * G(h(3, 52, fi1082), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_138 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t18_7, _t18_7, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t20_139 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_19, _t20_20), _mm256_unpacklo_pd(_t20_21, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t20_140 = _t20_1;

      // 4-BLAC: 1x4 * 4x1
      _t11_533 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t20_139, _t20_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_139, _t20_140), _mm256_mul_pd(_t20_139, _t20_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t20_139, _t20_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_139, _t20_140), _mm256_mul_pd(_t20_139, _t20_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t20_139, _t20_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t20_139, _t20_140), _mm256_mul_pd(_t20_139, _t20_140), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t20_141 = _mm256_sub_pd(_t20_138, _t11_533);

      // AVX Storer:
      _t20_22 = _t20_141;

      // Generating : X[52,52] = S(h(1, 52, fi879 + 3), ( G(h(1, 52, fi879 + 3), X[52,52],h(1, 52, fi1082 + 3)) Div ( G(h(1, 52, fi879 + 3), L[52,52],h(1, 52, fi879 + 3)) + G(h(1, 52, fi1082 + 3), U[52,52],h(1, 52, fi1082 + 3)) ) ),h(1, 52, fi1082 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_142 = _t20_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_143 = _t17_0;

      // AVX Loader:

      // 1x1 -> 1x4
      _t20_144 = _t20_0;

      // 4-BLAC: 1x4 + 1x4
      _t11_538 = _mm256_add_pd(_t20_143, _t20_144);

      // 4-BLAC: 1x4 / 1x4
      _t20_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t20_142), _mm256_castpd256_pd128(_t11_538)));

      // AVX Storer:
      _t20_22 = _t20_145;
      _mm_store_sd(&(C[fi1082 + 52*fi879]), _mm256_castpd256_pd128(_t20_7));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 1]), _mm256_castpd256_pd128(_t20_8));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 2]), _mm256_castpd256_pd128(_t20_9));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 3]), _mm256_castpd256_pd128(_t20_10));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 52]), _mm256_castpd256_pd128(_t20_11));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 53]), _mm256_castpd256_pd128(_t20_12));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 54]), _mm256_castpd256_pd128(_t20_13));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 55]), _mm256_castpd256_pd128(_t20_14));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 104]), _mm256_castpd256_pd128(_t20_15));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 105]), _mm256_castpd256_pd128(_t20_16));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 106]), _mm256_castpd256_pd128(_t20_17));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 107]), _mm256_castpd256_pd128(_t20_18));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 156]), _mm256_castpd256_pd128(_t20_19));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 157]), _mm256_castpd256_pd128(_t20_20));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 158]), _mm256_castpd256_pd128(_t20_21));
      _mm_store_sd(&(C[fi1082 + 52*fi879 + 159]), _mm256_castpd256_pd128(_t20_22));
    }
    _mm_store_sd(&(C[52*fi879]), _mm256_castpd256_pd128(_t17_7));
    _mm_store_sd(&(C[52*fi879 + 1]), _mm256_castpd256_pd128(_t17_8));
    _mm_store_sd(&(C[52*fi879 + 2]), _mm256_castpd256_pd128(_t17_9));
    _mm_store_sd(&(C[52*fi879 + 3]), _mm256_castpd256_pd128(_t17_10));
    _mm_store_sd(&(C[52*fi879 + 52]), _mm256_castpd256_pd128(_t17_11));
    _mm_store_sd(&(C[52*fi879 + 53]), _mm256_castpd256_pd128(_t17_12));
    _mm_store_sd(&(C[52*fi879 + 54]), _mm256_castpd256_pd128(_t17_13));
    _mm_store_sd(&(C[52*fi879 + 55]), _mm256_castpd256_pd128(_t17_14));
    _mm_store_sd(&(C[52*fi879 + 104]), _mm256_castpd256_pd128(_t17_15));
    _mm_store_sd(&(C[52*fi879 + 105]), _mm256_castpd256_pd128(_t17_16));
    _mm_store_sd(&(C[52*fi879 + 106]), _mm256_castpd256_pd128(_t17_17));
    _mm_store_sd(&(C[52*fi879 + 107]), _mm256_castpd256_pd128(_t17_18));
    _mm_store_sd(&(C[52*fi879 + 156]), _mm256_castpd256_pd128(_t17_19));
    _mm_store_sd(&(C[52*fi879 + 157]), _mm256_castpd256_pd128(_t17_20));
    _mm_store_sd(&(C[52*fi879 + 158]), _mm256_castpd256_pd128(_t17_21));
    _mm_store_sd(&(C[52*fi879 + 159]), _mm256_castpd256_pd128(_t17_22));
  }

}
