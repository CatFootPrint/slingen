/*
 * lusylp_kernel.h
 *
Decl { {u'X': SquaredMatrix[X, (100, 100), GenMatAccess], u'C': SquaredMatrix[C, (100, 100), GenMatAccess], u'U': UpperTriangular[U, (100, 100), GenMatAccess], u'L': LowerTriangular[L, (100, 100), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Add_Mul_LowerTriangular_SquaredMatrix_Mul_SquaredMatrix_UpperTriangular_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'Assign_Add_Mul_LowerTriangular_SquaredMatrix_Mul_SquaredMatrix_UpperTriangular_SquaredMatrix_opt_m04_m21'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
For_{fi4;0;95;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi4 + 1), L[100,100],h(1, 100, fi4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(4, 100, 0)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 1)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(4, 100, 0)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(4, 100, 0)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 31:
For_{fi25;4;96;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 100, fi4), X[100,100],h(4, 100, fi25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 100, fi4), X[100,100],h(4, 100, fi25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 100, fi4), X[100,100],h(fi25, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi25, 100, 0), U[100,100],h(4, 100, fi25)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(2, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(3, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi4 + 1), L[100,100],h(1, 100, fi4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4), X[100,100],h(4, 100, fi25)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(2, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(3, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 1)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(2, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(3, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, fi25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, fi25)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(2, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi4 + 3), X[100,100],h(3, 100, fi25)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) ) + Tile( (1, 1), G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ) )
Eq.ann: {}
 )Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 96, 100, fi4 + 4), X[100,100],h(100, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 96, 100, fi4 + 4), X[100,100],h(100, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi4 + 96, 100, fi4 + 4), L[100,100],h(4, 100, fi4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 100, fi4), X[100,100],h(100, 100, 0)) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 97), X[100,100],h(4, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 97), X[100,100],h(4, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 97), L[100,100],h(1, 100, 96)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(4, 100, 0)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 98), X[100,100],h(4, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 98), X[100,100],h(4, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 98), L[100,100],h(1, 100, 97)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(4, 100, 0)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(4, 100, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(4, 100, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), L[100,100],h(1, 100, 98)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(4, 100, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 0)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 0)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 1)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(2, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 2)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(3, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 3)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ) )
Eq.ann: {}
Entry 32:
For_{fi228;4;96;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 100, 96), X[100,100],h(4, 100, fi228)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 100, 96), X[100,100],h(4, 100, fi228)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 100, 96), X[100,100],h(fi228, 100, 0)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi228, 100, 0), U[100,100],h(4, 100, fi228)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228 + 1)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 1), U[100,100],h(1, 100, fi228 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(2, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi228), U[100,100],h(1, 100, fi228 + 2)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 2), U[100,100],h(1, 100, fi228 + 2)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(3, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi228), U[100,100],h(1, 100, fi228 + 3)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, 96), X[100,100],h(1, 100, fi228 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 96), L[100,100],h(1, 100, 96)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 3), U[100,100],h(1, 100, fi228 + 3)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 97), X[100,100],h(4, 100, fi228)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 97), X[100,100],h(4, 100, fi228)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 100, 97), L[100,100],h(1, 100, 96)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 96), X[100,100],h(4, 100, fi228)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228 + 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 1), U[100,100],h(1, 100, fi228 + 1)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(2, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi228), U[100,100],h(1, 100, fi228 + 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 2), U[100,100],h(1, 100, fi228 + 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(3, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi228), U[100,100],h(1, 100, fi228 + 3)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, 97), X[100,100],h(1, 100, fi228 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 97), L[100,100],h(1, 100, 97)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 3), U[100,100],h(1, 100, fi228 + 3)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 98), X[100,100],h(4, 100, fi228)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 98), X[100,100],h(4, 100, fi228)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 100, 98), L[100,100],h(1, 100, 97)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 97), X[100,100],h(4, 100, fi228)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228)) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228 + 1)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 1), U[100,100],h(1, 100, fi228 + 1)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(2, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi228), U[100,100],h(1, 100, fi228 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 2), U[100,100],h(1, 100, fi228 + 2)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(3, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi228), U[100,100],h(1, 100, fi228 + 3)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, 98), X[100,100],h(1, 100, fi228 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 98), L[100,100],h(1, 100, 98)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 3), U[100,100],h(1, 100, fi228 + 3)) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(4, 100, fi228)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(4, 100, fi228)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), L[100,100],h(1, 100, 98)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 98), X[100,100],h(4, 100, fi228)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228)) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 1)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 100, fi228), U[100,100],h(1, 100, fi228 + 1)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 1)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 1)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 1), U[100,100],h(1, 100, fi228 + 1)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 2)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(2, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 100, fi228), U[100,100],h(1, 100, fi228 + 2)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 2)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 2)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 2), U[100,100],h(1, 100, fi228 + 2)) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 3)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 100, 99), X[100,100],h(3, 100, fi228)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 100, fi228), U[100,100],h(1, 100, fi228 + 3)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 3)) ) = ( Tile( (1, 1), G(h(1, 100, 99), X[100,100],h(1, 100, fi228 + 3)) ) Div ( Tile( (1, 1), G(h(1, 100, 99), L[100,100],h(1, 100, 99)) ) + Tile( (1, 1), G(h(1, 100, fi228 + 3), U[100,100],h(1, 100, fi228 + 3)) ) ) )
Eq.ann: {}
 ) *
 * Created on: 2017-07-31
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 100
#define PARAM1 100

#define ERRTHRESH 1e-14

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * L, double const * U, double * C)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12, _t0_13, _t0_14, _t0_15,
	_t0_16, _t0_17, _t0_18, _t0_19, _t0_20, _t0_21, _t0_22, _t0_23,
	_t0_24, _t0_25, _t0_26, _t0_27, _t0_28, _t0_29, _t0_30, _t0_31,
	_t0_32, _t0_33, _t0_34, _t0_35, _t0_36, _t0_37, _t0_38, _t0_39,
	_t0_40, _t0_41, _t0_42, _t0_43, _t0_44, _t0_45, _t0_46, _t0_47,
	_t0_48, _t0_49, _t0_50, _t0_51, _t0_52, _t0_53, _t0_54, _t0_55,
	_t0_56, _t0_57, _t0_58, _t0_59, _t0_60, _t0_61, _t0_62, _t0_63,
	_t0_64, _t0_65, _t0_66, _t0_67, _t0_68, _t0_69, _t0_70, _t0_71,
	_t0_72, _t0_73, _t0_74, _t0_75, _t0_76, _t0_77, _t0_78, _t0_79,
	_t0_80, _t0_81, _t0_82, _t0_83, _t0_84, _t0_85, _t0_86, _t0_87,
	_t0_88, _t0_89, _t0_90, _t0_91, _t0_92, _t0_93, _t0_94, _t0_95,
	_t0_96, _t0_97, _t0_98, _t0_99, _t0_100, _t0_101, _t0_102, _t0_103,
	_t0_104, _t0_105, _t0_106, _t0_107, _t0_108, _t0_109, _t0_110, _t0_111,
	_t0_112, _t0_113, _t0_114, _t0_115, _t0_116, _t0_117, _t0_118, _t0_119,
	_t0_120, _t0_121, _t0_122, _t0_123, _t0_124, _t0_125, _t0_126, _t0_127,
	_t0_128, _t0_129, _t0_130, _t0_131, _t0_132, _t0_133, _t0_134, _t0_135,
	_t0_136, _t0_137, _t0_138, _t0_139, _t0_140, _t0_141, _t0_142, _t0_143,
	_t0_144, _t0_145, _t0_146, _t0_147, _t0_148, _t0_149, _t0_150, _t0_151,
	_t0_152, _t0_153, _t0_154, _t0_155, _t0_156, _t0_157, _t0_158, _t0_159,
	_t0_160, _t0_161, _t0_162, _t0_163, _t0_164, _t0_165, _t0_166, _t0_167,
	_t0_168, _t0_169, _t0_170, _t0_171, _t0_172, _t0_173, _t0_174, _t0_175,
	_t0_176, _t0_177, _t0_178, _t0_179, _t0_180, _t0_181, _t0_182, _t0_183,
	_t0_184, _t0_185, _t0_186, _t0_187, _t0_188, _t0_189, _t0_190, _t0_191,
	_t0_192, _t0_193, _t0_194, _t0_195, _t0_196, _t0_197, _t0_198, _t0_199,
	_t0_200, _t0_201, _t0_202, _t0_203, _t0_204, _t0_205, _t0_206, _t0_207,
	_t0_208, _t0_209, _t0_210, _t0_211, _t0_212, _t0_213, _t0_214, _t0_215,
	_t0_216, _t0_217, _t0_218, _t0_219, _t0_220, _t0_221, _t0_222, _t0_223,
	_t0_224, _t0_225, _t0_226, _t0_227, _t0_228, _t0_229, _t0_230, _t0_231,
	_t0_232, _t0_233, _t0_234, _t0_235, _t0_236, _t0_237, _t0_238, _t0_239,
	_t0_240, _t0_241, _t0_242, _t0_243, _t0_244, _t0_245, _t0_246, _t0_247,
	_t0_248, _t0_249, _t0_250, _t0_251, _t0_252, _t0_253, _t0_254, _t0_255,
	_t0_256, _t0_257, _t0_258, _t0_259, _t0_260, _t0_261, _t0_262, _t0_263,
	_t0_264, _t0_265, _t0_266, _t0_267, _t0_268, _t0_269, _t0_270, _t0_271,
	_t0_272, _t0_273, _t0_274, _t0_275, _t0_276, _t0_277, _t0_278, _t0_279,
	_t0_280, _t0_281, _t0_282, _t0_283, _t0_284, _t0_285, _t0_286, _t0_287,
	_t0_288, _t0_289, _t0_290, _t0_291, _t0_292, _t0_293, _t0_294, _t0_295,
	_t0_296, _t0_297, _t0_298, _t0_299, _t0_300, _t0_301, _t0_302, _t0_303,
	_t0_304, _t0_305, _t0_306, _t0_307, _t0_308, _t0_309, _t0_310, _t0_311,
	_t0_312, _t0_313, _t0_314, _t0_315, _t0_316, _t0_317, _t0_318, _t0_319,
	_t0_320, _t0_321, _t0_322, _t0_323, _t0_324, _t0_325, _t0_326, _t0_327,
	_t0_328, _t0_329, _t0_330, _t0_331, _t0_332, _t0_333, _t0_334, _t0_335,
	_t0_336, _t0_337, _t0_338, _t0_339, _t0_340, _t0_341, _t0_342, _t0_343,
	_t0_344, _t0_345, _t0_346, _t0_347, _t0_348, _t0_349, _t0_350, _t0_351,
	_t0_352, _t0_353, _t0_354, _t0_355, _t0_356, _t0_357, _t0_358, _t0_359,
	_t0_360, _t0_361, _t0_362, _t0_363, _t0_364, _t0_365, _t0_366, _t0_367,
	_t0_368, _t0_369, _t0_370, _t0_371, _t0_372, _t0_373, _t0_374, _t0_375,
	_t0_376, _t0_377, _t0_378, _t0_379, _t0_380, _t0_381, _t0_382, _t0_383,
	_t0_384, _t0_385, _t0_386, _t0_387, _t0_388, _t0_389, _t0_390, _t0_391,
	_t0_392, _t0_393, _t0_394, _t0_395, _t0_396, _t0_397, _t0_398, _t0_399,
	_t0_400, _t0_401, _t0_402, _t0_403, _t0_404, _t0_405, _t0_406, _t0_407,
	_t0_408, _t0_409, _t0_410, _t0_411, _t0_412, _t0_413, _t0_414, _t0_415,
	_t0_416, _t0_417, _t0_418, _t0_419, _t0_420, _t0_421, _t0_422, _t0_423,
	_t0_424, _t0_425, _t0_426, _t0_427, _t0_428, _t0_429, _t0_430, _t0_431,
	_t0_432, _t0_433, _t0_434, _t0_435, _t0_436, _t0_437, _t0_438, _t0_439,
	_t0_440, _t0_441, _t0_442, _t0_443, _t0_444, _t0_445, _t0_446, _t0_447,
	_t0_448, _t0_449, _t0_450, _t0_451, _t0_452, _t0_453, _t0_454, _t0_455,
	_t0_456, _t0_457, _t0_458, _t0_459, _t0_460, _t0_461, _t0_462, _t0_463,
	_t0_464, _t0_465, _t0_466, _t0_467, _t0_468, _t0_469, _t0_470, _t0_471,
	_t0_472, _t0_473, _t0_474, _t0_475, _t0_476, _t0_477, _t0_478, _t0_479,
	_t0_480, _t0_481, _t0_482, _t0_483, _t0_484, _t0_485, _t0_486, _t0_487,
	_t0_488, _t0_489, _t0_490, _t0_491, _t0_492, _t0_493, _t0_494, _t0_495,
	_t0_496, _t0_497, _t0_498, _t0_499, _t0_500, _t0_501, _t0_502, _t0_503,
	_t0_504, _t0_505, _t0_506, _t0_507, _t0_508, _t0_509, _t0_510, _t0_511,
	_t0_512, _t0_513, _t0_514, _t0_515, _t0_516, _t0_517, _t0_518, _t0_519,
	_t0_520, _t0_521, _t0_522, _t0_523, _t0_524, _t0_525, _t0_526, _t0_527,
	_t0_528, _t0_529, _t0_530, _t0_531, _t0_532, _t0_533, _t0_534, _t0_535,
	_t0_536, _t0_537, _t0_538, _t0_539, _t0_540, _t0_541, _t0_542, _t0_543,
	_t0_544, _t0_545, _t0_546, _t0_547, _t0_548, _t0_549, _t0_550, _t0_551,
	_t0_552, _t0_553, _t0_554, _t0_555, _t0_556, _t0_557, _t0_558, _t0_559,
	_t0_560, _t0_561, _t0_562, _t0_563, _t0_564, _t0_565, _t0_566, _t0_567,
	_t0_568, _t0_569, _t0_570, _t0_571, _t0_572, _t0_573, _t0_574, _t0_575,
	_t0_576, _t0_577, _t0_578, _t0_579, _t0_580, _t0_581, _t0_582, _t0_583,
	_t0_584, _t0_585, _t0_586, _t0_587, _t0_588, _t0_589, _t0_590, _t0_591,
	_t0_592, _t0_593, _t0_594, _t0_595, _t0_596, _t0_597, _t0_598, _t0_599,
	_t0_600, _t0_601, _t0_602, _t0_603, _t0_604, _t0_605, _t0_606, _t0_607,
	_t0_608, _t0_609, _t0_610, _t0_611, _t0_612, _t0_613, _t0_614;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6, _t1_7;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23,
	_t3_24, _t3_25, _t3_26, _t3_27, _t3_28, _t3_29, _t3_30, _t3_31,
	_t3_32, _t3_33, _t3_34, _t3_35, _t3_36, _t3_37, _t3_38, _t3_39,
	_t3_40, _t3_41, _t3_42, _t3_43, _t3_44, _t3_45, _t3_46, _t3_47,
	_t3_48, _t3_49, _t3_50, _t3_51, _t3_52, _t3_53, _t3_54, _t3_55,
	_t3_56, _t3_57, _t3_58, _t3_59, _t3_60, _t3_61, _t3_62, _t3_63,
	_t3_64, _t3_65, _t3_66, _t3_67, _t3_68, _t3_69, _t3_70, _t3_71,
	_t3_72, _t3_73, _t3_74, _t3_75, _t3_76, _t3_77, _t3_78, _t3_79,
	_t3_80, _t3_81, _t3_82, _t3_83, _t3_84, _t3_85, _t3_86, _t3_87,
	_t3_88, _t3_89, _t3_90, _t3_91, _t3_92, _t3_93, _t3_94, _t3_95,
	_t3_96, _t3_97, _t3_98, _t3_99, _t3_100, _t3_101, _t3_102, _t3_103,
	_t3_104, _t3_105, _t3_106, _t3_107, _t3_108, _t3_109, _t3_110, _t3_111,
	_t3_112, _t3_113, _t3_114, _t3_115, _t3_116, _t3_117, _t3_118, _t3_119,
	_t3_120, _t3_121, _t3_122, _t3_123, _t3_124, _t3_125, _t3_126, _t3_127,
	_t3_128, _t3_129, _t3_130, _t3_131, _t3_132, _t3_133, _t3_134, _t3_135,
	_t3_136, _t3_137, _t3_138, _t3_139, _t3_140, _t3_141, _t3_142, _t3_143,
	_t3_144, _t3_145, _t3_146, _t3_147, _t3_148, _t3_149, _t3_150, _t3_151,
	_t3_152, _t3_153;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27, _t5_28, _t5_29, _t5_30, _t5_31,
	_t5_32, _t5_33, _t5_34, _t5_35, _t5_36, _t5_37, _t5_38, _t5_39,
	_t5_40, _t5_41, _t5_42, _t5_43, _t5_44, _t5_45, _t5_46, _t5_47,
	_t5_48, _t5_49, _t5_50, _t5_51, _t5_52, _t5_53, _t5_54, _t5_55,
	_t5_56, _t5_57, _t5_58, _t5_59, _t5_60, _t5_61, _t5_62, _t5_63,
	_t5_64, _t5_65, _t5_66, _t5_67, _t5_68, _t5_69, _t5_70, _t5_71,
	_t5_72, _t5_73, _t5_74, _t5_75, _t5_76, _t5_77, _t5_78, _t5_79,
	_t5_80, _t5_81, _t5_82, _t5_83, _t5_84, _t5_85, _t5_86, _t5_87,
	_t5_88, _t5_89, _t5_90, _t5_91, _t5_92, _t5_93, _t5_94, _t5_95,
	_t5_96, _t5_97, _t5_98, _t5_99, _t5_100, _t5_101, _t5_102, _t5_103,
	_t5_104, _t5_105, _t5_106, _t5_107, _t5_108, _t5_109, _t5_110, _t5_111,
	_t5_112, _t5_113, _t5_114, _t5_115, _t5_116, _t5_117, _t5_118, _t5_119,
	_t5_120, _t5_121, _t5_122, _t5_123, _t5_124, _t5_125, _t5_126, _t5_127,
	_t5_128, _t5_129, _t5_130, _t5_131, _t5_132, _t5_133, _t5_134, _t5_135,
	_t5_136, _t5_137, _t5_138, _t5_139, _t5_140, _t5_141, _t5_142, _t5_143,
	_t5_144, _t5_145, _t5_146, _t5_147, _t5_148, _t5_149, _t5_150, _t5_151,
	_t5_152, _t5_153, _t5_154, _t5_155, _t5_156, _t5_157, _t5_158, _t5_159,
	_t5_160, _t5_161, _t5_162, _t5_163, _t5_164, _t5_165, _t5_166, _t5_167,
	_t5_168, _t5_169, _t5_170, _t5_171, _t5_172, _t5_173, _t5_174, _t5_175,
	_t5_176, _t5_177, _t5_178, _t5_179, _t5_180, _t5_181, _t5_182, _t5_183,
	_t5_184, _t5_185, _t5_186, _t5_187, _t5_188, _t5_189, _t5_190, _t5_191,
	_t5_192, _t5_193, _t5_194, _t5_195, _t5_196, _t5_197, _t5_198, _t5_199,
	_t5_200, _t5_201, _t5_202, _t5_203, _t5_204, _t5_205, _t5_206, _t5_207,
	_t5_208, _t5_209, _t5_210, _t5_211, _t5_212, _t5_213, _t5_214, _t5_215,
	_t5_216, _t5_217, _t5_218, _t5_219, _t5_220, _t5_221, _t5_222, _t5_223,
	_t5_224, _t5_225, _t5_226, _t5_227, _t5_228, _t5_229, _t5_230, _t5_231,
	_t5_232, _t5_233, _t5_234, _t5_235, _t5_236, _t5_237, _t5_238, _t5_239,
	_t5_240, _t5_241, _t5_242, _t5_243, _t5_244, _t5_245, _t5_246, _t5_247,
	_t5_248, _t5_249, _t5_250, _t5_251, _t5_252, _t5_253, _t5_254, _t5_255,
	_t5_256, _t5_257, _t5_258, _t5_259, _t5_260, _t5_261, _t5_262, _t5_263,
	_t5_264, _t5_265, _t5_266, _t5_267, _t5_268, _t5_269, _t5_270, _t5_271,
	_t5_272, _t5_273, _t5_274, _t5_275, _t5_276, _t5_277, _t5_278, _t5_279,
	_t5_280, _t5_281, _t5_282, _t5_283, _t5_284, _t5_285, _t5_286, _t5_287,
	_t5_288, _t5_289, _t5_290, _t5_291, _t5_292, _t5_293, _t5_294, _t5_295,
	_t5_296, _t5_297, _t5_298, _t5_299, _t5_300, _t5_301, _t5_302, _t5_303,
	_t5_304, _t5_305, _t5_306, _t5_307, _t5_308, _t5_309, _t5_310, _t5_311,
	_t5_312, _t5_313, _t5_314, _t5_315, _t5_316, _t5_317, _t5_318, _t5_319,
	_t5_320, _t5_321, _t5_322, _t5_323, _t5_324, _t5_325, _t5_326, _t5_327,
	_t5_328, _t5_329, _t5_330, _t5_331, _t5_332, _t5_333, _t5_334, _t5_335,
	_t5_336, _t5_337, _t5_338, _t5_339, _t5_340, _t5_341, _t5_342, _t5_343,
	_t5_344, _t5_345, _t5_346, _t5_347, _t5_348, _t5_349, _t5_350, _t5_351,
	_t5_352, _t5_353, _t5_354, _t5_355, _t5_356, _t5_357, _t5_358, _t5_359,
	_t5_360, _t5_361, _t5_362, _t5_363, _t5_364, _t5_365, _t5_366, _t5_367,
	_t5_368, _t5_369, _t5_370, _t5_371, _t5_372, _t5_373, _t5_374, _t5_375,
	_t5_376, _t5_377, _t5_378, _t5_379, _t5_380, _t5_381, _t5_382, _t5_383,
	_t5_384, _t5_385, _t5_386, _t5_387, _t5_388, _t5_389, _t5_390, _t5_391,
	_t5_392, _t5_393, _t5_394, _t5_395, _t5_396, _t5_397, _t5_398, _t5_399,
	_t5_400, _t5_401, _t5_402, _t5_403, _t5_404, _t5_405, _t5_406, _t5_407,
	_t5_408, _t5_409, _t5_410, _t5_411, _t5_412, _t5_413, _t5_414, _t5_415,
	_t5_416, _t5_417, _t5_418, _t5_419, _t5_420, _t5_421, _t5_422, _t5_423,
	_t5_424, _t5_425, _t5_426, _t5_427, _t5_428, _t5_429, _t5_430, _t5_431,
	_t5_432, _t5_433, _t5_434, _t5_435, _t5_436, _t5_437, _t5_438, _t5_439,
	_t5_440, _t5_441, _t5_442, _t5_443, _t5_444, _t5_445, _t5_446, _t5_447,
	_t5_448, _t5_449, _t5_450, _t5_451, _t5_452, _t5_453, _t5_454, _t5_455,
	_t5_456, _t5_457, _t5_458, _t5_459, _t5_460, _t5_461, _t5_462, _t5_463,
	_t5_464, _t5_465, _t5_466, _t5_467, _t5_468, _t5_469, _t5_470, _t5_471,
	_t5_472, _t5_473, _t5_474, _t5_475, _t5_476, _t5_477, _t5_478, _t5_479,
	_t5_480, _t5_481, _t5_482, _t5_483, _t5_484, _t5_485, _t5_486, _t5_487,
	_t5_488, _t5_489, _t5_490, _t5_491, _t5_492, _t5_493, _t5_494, _t5_495,
	_t5_496, _t5_497, _t5_498, _t5_499, _t5_500, _t5_501, _t5_502, _t5_503,
	_t5_504, _t5_505, _t5_506, _t5_507, _t5_508, _t5_509, _t5_510, _t5_511,
	_t5_512, _t5_513, _t5_514, _t5_515, _t5_516, _t5_517, _t5_518, _t5_519,
	_t5_520, _t5_521, _t5_522, _t5_523, _t5_524, _t5_525, _t5_526, _t5_527,
	_t5_528, _t5_529, _t5_530, _t5_531, _t5_532, _t5_533, _t5_534, _t5_535,
	_t5_536, _t5_537, _t5_538, _t5_539, _t5_540, _t5_541, _t5_542, _t5_543,
	_t5_544, _t5_545, _t5_546, _t5_547, _t5_548, _t5_549, _t5_550, _t5_551,
	_t5_552, _t5_553, _t5_554, _t5_555, _t5_556, _t5_557, _t5_558, _t5_559,
	_t5_560, _t5_561, _t5_562, _t5_563, _t5_564, _t5_565, _t5_566, _t5_567,
	_t5_568, _t5_569, _t5_570, _t5_571, _t5_572, _t5_573, _t5_574, _t5_575,
	_t5_576, _t5_577, _t5_578, _t5_579, _t5_580, _t5_581, _t5_582, _t5_583,
	_t5_584, _t5_585, _t5_586, _t5_587, _t5_588, _t5_589, _t5_590, _t5_591,
	_t5_592, _t5_593, _t5_594, _t5_595, _t5_596, _t5_597, _t5_598, _t5_599,
	_t5_600, _t5_601, _t5_602, _t5_603, _t5_604, _t5_605, _t5_606, _t5_607,
	_t5_608, _t5_609, _t5_610, _t5_611, _t5_612, _t5_613, _t5_614;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27, _t8_28, _t8_29, _t8_30, _t8_31,
	_t8_32, _t8_33, _t8_34, _t8_35, _t8_36, _t8_37, _t8_38, _t8_39,
	_t8_40, _t8_41, _t8_42, _t8_43, _t8_44, _t8_45, _t8_46, _t8_47,
	_t8_48, _t8_49, _t8_50, _t8_51, _t8_52, _t8_53, _t8_54, _t8_55,
	_t8_56, _t8_57, _t8_58, _t8_59, _t8_60, _t8_61, _t8_62, _t8_63,
	_t8_64, _t8_65, _t8_66, _t8_67, _t8_68, _t8_69, _t8_70, _t8_71,
	_t8_72, _t8_73, _t8_74, _t8_75, _t8_76, _t8_77, _t8_78, _t8_79,
	_t8_80, _t8_81, _t8_82, _t8_83, _t8_84, _t8_85, _t8_86, _t8_87,
	_t8_88, _t8_89, _t8_90, _t8_91, _t8_92, _t8_93, _t8_94, _t8_95,
	_t8_96, _t8_97, _t8_98, _t8_99, _t8_100, _t8_101, _t8_102, _t8_103,
	_t8_104, _t8_105, _t8_106, _t8_107, _t8_108, _t8_109, _t8_110, _t8_111,
	_t8_112, _t8_113, _t8_114, _t8_115, _t8_116, _t8_117, _t8_118, _t8_119,
	_t8_120, _t8_121, _t8_122, _t8_123, _t8_124, _t8_125, _t8_126, _t8_127,
	_t8_128, _t8_129, _t8_130, _t8_131, _t8_132, _t8_133, _t8_134, _t8_135,
	_t8_136, _t8_137, _t8_138, _t8_139, _t8_140, _t8_141, _t8_142, _t8_143,
	_t8_144, _t8_145, _t8_146, _t8_147, _t8_148, _t8_149, _t8_150, _t8_151,
	_t8_152, _t8_153;


  for( int fi4 = 0; fi4 <= 95; fi4+=4 ) {
    _t0_40 = _mm256_castpd128_pd256(_mm_load_sd(&(C[100*fi4])));
    _t0_39 = _mm256_castpd128_pd256(_mm_load_sd(&(L[101*fi4])));
    _t0_38 = _mm256_castpd128_pd256(_mm_load_sd(&(U[0])));
    _t0_41 = _mm256_castpd128_pd256(_mm_load_sd(&(C[100*fi4 + 1])));
    _t0_37 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1])));
    _t0_36 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101])));
    _t0_42 = _mm256_castpd128_pd256(_mm_load_sd(&(C[100*fi4 + 2])));
    _t0_35 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 102)), 0);
    _t0_34 = _mm256_castpd128_pd256(_mm_load_sd(&(U[202])));
    _t0_43 = _mm256_castpd128_pd256(_mm_load_sd(&(C[100*fi4 + 3])));
    _t0_33 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 103))), _mm256_castpd128_pd256(_mm_load_sd(U + 203)), 32);
    _t0_32 = _mm256_castpd128_pd256(_mm_load_sd(&(U[303])));
    _t0_44 = _asm256_loadu_pd(C + 100*fi4 + 100);
    _t0_45 = _asm256_loadu_pd(C + 100*fi4 + 200);
    _t0_46 = _asm256_loadu_pd(C + 100*fi4 + 300);
    _t0_31 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 101*fi4 + 100)), _mm256_castpd128_pd256(_mm_load_sd(L + 101*fi4 + 200))), _mm256_castpd128_pd256(_mm_load_sd(L + 101*fi4 + 300)), 32);
    _t0_30 = _mm256_castpd128_pd256(_mm_load_sd(&(L[101*fi4 + 101])));
    _t0_29 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 101*fi4 + 201)), _mm256_castpd128_pd256(_mm_load_sd(L + 101*fi4 + 301)), 0);
    _t0_28 = _mm256_castpd128_pd256(_mm_load_sd(&(L[101*fi4 + 202])));
    _t0_27 = _mm256_broadcast_sd(&(L[101*fi4 + 302]));
    _t0_26 = _mm256_castpd128_pd256(_mm_load_sd(&(L[101*fi4 + 303])));
    _t0_110 = _asm256_loadu_pd(C + 100*fi4 + 4);
    _t0_111 = _asm256_loadu_pd(C + 100*fi4 + 104);
    _t0_112 = _asm256_loadu_pd(C + 100*fi4 + 204);
    _t0_113 = _asm256_loadu_pd(C + 100*fi4 + 304);
    _t0_25 = _asm256_loadu_pd(U + 4);
    _t0_24 = _asm256_loadu_pd(U + 104);
    _t0_23 = _asm256_loadu_pd(U + 204);
    _t0_22 = _asm256_loadu_pd(U + 304);
    _t0_21 = _mm256_castpd128_pd256(_mm_load_sd(&(U[404])));
    _t0_20 = _mm256_castpd128_pd256(_mm_load_sd(&(U[405])));
    _t0_19 = _mm256_castpd128_pd256(_mm_load_sd(&(U[505])));
    _t0_18 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 406)), _mm256_castpd128_pd256(_mm_load_sd(U + 506)), 0);
    _t0_17 = _mm256_castpd128_pd256(_mm_load_sd(&(U[606])));
    _t0_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 407)), _mm256_castpd128_pd256(_mm_load_sd(U + 507))), _mm256_castpd128_pd256(_mm_load_sd(U + 607)), 32);
    _t0_15 = _mm256_castpd128_pd256(_mm_load_sd(&(U[707])));
    _t0_106 = _asm256_loadu_pd(C + 100*fi4 + 8);
    _t0_107 = _asm256_loadu_pd(C + 100*fi4 + 108);
    _t0_108 = _asm256_loadu_pd(C + 100*fi4 + 208);
    _t0_109 = _asm256_loadu_pd(C + 100*fi4 + 308);
    _t0_14 = _asm256_loadu_pd(U + 8);
    _t0_13 = _asm256_loadu_pd(U + 108);
    _t0_12 = _asm256_loadu_pd(U + 208);
    _t0_11 = _asm256_loadu_pd(U + 308);
    _t0_10 = _asm256_loadu_pd(U + 408);
    _t0_9 = _asm256_loadu_pd(U + 508);
    _t0_8 = _asm256_loadu_pd(U + 608);
    _t0_7 = _asm256_loadu_pd(U + 708);
    _t0_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[808])));
    _t0_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[809])));
    _t0_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[909])));
    _t0_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 810)), _mm256_castpd128_pd256(_mm_load_sd(U + 910)), 0);
    _t0_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1010])));
    _t0_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 811)), _mm256_castpd128_pd256(_mm_load_sd(U + 911))), _mm256_castpd128_pd256(_mm_load_sd(U + 1011)), 32);
    _t0_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1111])));

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_441 = _t0_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_446 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_452 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t0_467 = _mm256_add_pd(_t0_446, _t0_452);

    // 4-BLAC: 1x4 / 1x4
    _t0_481 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_441), _mm256_castpd256_pd128(_t0_467)));

    // AVX Storer:
    _t0_40 = _t0_481;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, fi4), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_496 = _t0_41;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_510 = _t0_40;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_516 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_531 = _mm256_mul_pd(_t0_510, _t0_516);

    // 4-BLAC: 1x4 - 1x4
    _t0_546 = _mm256_sub_pd(_t0_496, _t0_531);

    // AVX Storer:
    _t0_41 = _t0_546;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_569 = _t0_41;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_574 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_579 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t0_594 = _mm256_add_pd(_t0_574, _t0_579);

    // 4-BLAC: 1x4 / 1x4
    _t0_610 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_569), _mm256_castpd256_pd128(_t0_594)));

    // AVX Storer:
    _t0_41 = _t0_610;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, fi4), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_122 = _t0_42;

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_128 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_134 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t0_140 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_128, _t0_134), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_128, _t0_134), _mm256_mul_pd(_t0_128, _t0_134), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_128, _t0_134), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_128, _t0_134), _mm256_mul_pd(_t0_128, _t0_134), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_128, _t0_134), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_128, _t0_134), _mm256_mul_pd(_t0_128, _t0_134), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_141 = _mm256_sub_pd(_t0_122, _t0_140);

    // AVX Storer:
    _t0_42 = _t0_141;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_142 = _t0_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_143 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_144 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t0_145 = _mm256_add_pd(_t0_143, _t0_144);

    // 4-BLAC: 1x4 / 1x4
    _t0_146 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_142), _mm256_castpd256_pd128(_t0_145)));

    // AVX Storer:
    _t0_42 = _t0_146;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, fi4), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_147 = _t0_43;

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_148 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_149 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t0_150 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_148, _t0_149), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_148, _t0_149), _mm256_mul_pd(_t0_148, _t0_149), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_148, _t0_149), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_148, _t0_149), _mm256_mul_pd(_t0_148, _t0_149), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_148, _t0_149), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_148, _t0_149), _mm256_mul_pd(_t0_148, _t0_149), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_151 = _mm256_sub_pd(_t0_147, _t0_150);

    // AVX Storer:
    _t0_43 = _t0_151;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_152 = _t0_43;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_153 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_154 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t0_155 = _mm256_add_pd(_t0_153, _t0_154);

    // 4-BLAC: 1x4 / 1x4
    _t0_156 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_152), _mm256_castpd256_pd128(_t0_155)));

    // AVX Storer:
    _t0_43 = _t0_156;

    // Generating : X[100,100] = S(h(3, 100, fi4 + 1), ( G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, 0)) - ( G(h(3, 100, fi4 + 1), L[100,100],h(1, 100, fi4)) * G(h(1, 100, fi4), X[100,100],h(4, 100, 0)) ) ),h(4, 100, 0))

    // AVX Loader:

    // 3x4 -> 4x4
    _t0_157 = _t0_44;
    _t0_158 = _t0_45;
    _t0_159 = _t0_46;
    _t0_160 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_161 = _t0_31;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t0_162 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_161, _t0_161, 32), _mm256_permute2f128_pd(_t0_161, _t0_161, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32));
    _t0_163 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_161, _t0_161, 32), _mm256_permute2f128_pd(_t0_161, _t0_161, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32));
    _t0_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_161, _t0_161, 49), _mm256_permute2f128_pd(_t0_161, _t0_161, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32));
    _t0_165 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_161, _t0_161, 49), _mm256_permute2f128_pd(_t0_161, _t0_161, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_40, _t0_41), _mm256_unpacklo_pd(_t0_42, _t0_43), 32));

    // 4-BLAC: 4x4 - 4x4
    _t0_166 = _mm256_sub_pd(_t0_157, _t0_162);
    _t0_167 = _mm256_sub_pd(_t0_158, _t0_163);
    _t0_168 = _mm256_sub_pd(_t0_159, _t0_164);
    _t0_169 = _mm256_sub_pd(_t0_160, _t0_165);

    // AVX Storer:
    _t0_44 = _t0_166;
    _t0_45 = _t0_167;
    _t0_46 = _t0_168;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_170 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_171 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_172 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t0_173 = _mm256_add_pd(_t0_171, _t0_172);

    // 4-BLAC: 1x4 / 1x4
    _t0_174 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_170), _mm256_castpd256_pd128(_t0_173)));

    // AVX Storer:
    _t0_47 = _t0_174;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_175 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_176 = _t0_47;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_177 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_178 = _mm256_mul_pd(_t0_176, _t0_177);

    // 4-BLAC: 1x4 - 1x4
    _t0_179 = _mm256_sub_pd(_t0_175, _t0_178);

    // AVX Storer:
    _t0_48 = _t0_179;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_180 = _t0_48;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_181 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_182 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t0_183 = _mm256_add_pd(_t0_181, _t0_182);

    // 4-BLAC: 1x4 / 1x4
    _t0_184 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_180), _mm256_castpd256_pd128(_t0_183)));

    // AVX Storer:
    _t0_48 = _t0_184;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_185 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_44, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_186 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_47, _t0_48), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_187 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t0_188 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_186, _t0_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_186, _t0_187), _mm256_mul_pd(_t0_186, _t0_187), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_186, _t0_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_186, _t0_187), _mm256_mul_pd(_t0_186, _t0_187), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_186, _t0_187), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_186, _t0_187), _mm256_mul_pd(_t0_186, _t0_187), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_189 = _mm256_sub_pd(_t0_185, _t0_188);

    // AVX Storer:
    _t0_49 = _t0_189;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_190 = _t0_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_191 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_192 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t0_193 = _mm256_add_pd(_t0_191, _t0_192);

    // 4-BLAC: 1x4 / 1x4
    _t0_194 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_190), _mm256_castpd256_pd128(_t0_193)));

    // AVX Storer:
    _t0_49 = _t0_194;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_195 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_44, _t0_44, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_196 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_47, _t0_48), _mm256_unpacklo_pd(_t0_49, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_197 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t0_198 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_196, _t0_197), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_196, _t0_197), _mm256_mul_pd(_t0_196, _t0_197), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_196, _t0_197), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_196, _t0_197), _mm256_mul_pd(_t0_196, _t0_197), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_196, _t0_197), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_196, _t0_197), _mm256_mul_pd(_t0_196, _t0_197), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_199 = _mm256_sub_pd(_t0_195, _t0_198);

    // AVX Storer:
    _t0_50 = _t0_199;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_200 = _t0_50;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_201 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_202 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t0_203 = _mm256_add_pd(_t0_201, _t0_202);

    // 4-BLAC: 1x4 / 1x4
    _t0_204 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_200), _mm256_castpd256_pd128(_t0_203)));

    // AVX Storer:
    _t0_50 = _t0_204;

    // Generating : X[100,100] = S(h(2, 100, fi4 + 2), ( G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, 0)) - ( G(h(2, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 1)) * G(h(1, 100, fi4 + 1), X[100,100],h(4, 100, 0)) ) ),h(4, 100, 0))

    // AVX Loader:

    // 2x4 -> 4x4
    _t0_205 = _t0_45;
    _t0_206 = _t0_46;
    _t0_207 = _mm256_setzero_pd();
    _t0_208 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_209 = _t0_29;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t0_210 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_209, _t0_209, 32), _mm256_permute2f128_pd(_t0_209, _t0_209, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_47, _t0_48), _mm256_unpacklo_pd(_t0_49, _t0_50), 32));
    _t0_211 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_209, _t0_209, 32), _mm256_permute2f128_pd(_t0_209, _t0_209, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_47, _t0_48), _mm256_unpacklo_pd(_t0_49, _t0_50), 32));
    _t0_212 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_209, _t0_209, 49), _mm256_permute2f128_pd(_t0_209, _t0_209, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_47, _t0_48), _mm256_unpacklo_pd(_t0_49, _t0_50), 32));
    _t0_213 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_209, _t0_209, 49), _mm256_permute2f128_pd(_t0_209, _t0_209, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_47, _t0_48), _mm256_unpacklo_pd(_t0_49, _t0_50), 32));

    // 4-BLAC: 4x4 - 4x4
    _t0_214 = _mm256_sub_pd(_t0_205, _t0_210);
    _t0_215 = _mm256_sub_pd(_t0_206, _t0_211);
    _t0_216 = _mm256_sub_pd(_t0_207, _t0_212);
    _t0_217 = _mm256_sub_pd(_t0_208, _t0_213);

    // AVX Storer:
    _t0_45 = _t0_214;
    _t0_46 = _t0_215;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_218 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_45, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_219 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_220 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t0_221 = _mm256_add_pd(_t0_219, _t0_220);

    // 4-BLAC: 1x4 / 1x4
    _t0_222 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_218), _mm256_castpd256_pd128(_t0_221)));

    // AVX Storer:
    _t0_51 = _t0_222;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_223 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_45, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_224 = _t0_51;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_225 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_226 = _mm256_mul_pd(_t0_224, _t0_225);

    // 4-BLAC: 1x4 - 1x4
    _t0_227 = _mm256_sub_pd(_t0_223, _t0_226);

    // AVX Storer:
    _t0_52 = _t0_227;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_228 = _t0_52;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_229 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_230 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t0_231 = _mm256_add_pd(_t0_229, _t0_230);

    // 4-BLAC: 1x4 / 1x4
    _t0_232 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_228), _mm256_castpd256_pd128(_t0_231)));

    // AVX Storer:
    _t0_52 = _t0_232;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_233 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_45, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_45, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_234 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_51, _t0_52), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_235 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t0_236 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_234, _t0_235), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_234, _t0_235), _mm256_mul_pd(_t0_234, _t0_235), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_234, _t0_235), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_234, _t0_235), _mm256_mul_pd(_t0_234, _t0_235), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_234, _t0_235), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_234, _t0_235), _mm256_mul_pd(_t0_234, _t0_235), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_237 = _mm256_sub_pd(_t0_233, _t0_236);

    // AVX Storer:
    _t0_53 = _t0_237;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_238 = _t0_53;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_239 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_240 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t0_241 = _mm256_add_pd(_t0_239, _t0_240);

    // 4-BLAC: 1x4 / 1x4
    _t0_242 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_238), _mm256_castpd256_pd128(_t0_241)));

    // AVX Storer:
    _t0_53 = _t0_242;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_243 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_45, _t0_45, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_244 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_51, _t0_52), _mm256_unpacklo_pd(_t0_53, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_245 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t0_246 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_244, _t0_245), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_244, _t0_245), _mm256_mul_pd(_t0_244, _t0_245), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_244, _t0_245), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_244, _t0_245), _mm256_mul_pd(_t0_244, _t0_245), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_244, _t0_245), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_244, _t0_245), _mm256_mul_pd(_t0_244, _t0_245), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_247 = _mm256_sub_pd(_t0_243, _t0_246);

    // AVX Storer:
    _t0_54 = _t0_247;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_248 = _t0_54;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_249 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_250 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t0_251 = _mm256_add_pd(_t0_249, _t0_250);

    // 4-BLAC: 1x4 / 1x4
    _t0_252 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_248), _mm256_castpd256_pd128(_t0_251)));

    // AVX Storer:
    _t0_54 = _t0_252;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, 0)) - ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 2)) Kro G(h(1, 100, fi4 + 2), X[100,100],h(4, 100, 0)) ) ),h(4, 100, 0))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_253 = _t0_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t0_93 = _mm256_mul_pd(_t0_253, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_51, _t0_52), _mm256_unpacklo_pd(_t0_53, _t0_54), 32));

    // 4-BLAC: 1x4 - 1x4
    _t0_46 = _mm256_sub_pd(_t0_46, _t0_93);

    // AVX Storer:

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_254 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_46, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_255 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_256 = _t0_38;

    // 4-BLAC: 1x4 + 1x4
    _t0_257 = _mm256_add_pd(_t0_255, _t0_256);

    // 4-BLAC: 1x4 / 1x4
    _t0_258 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_254), _mm256_castpd256_pd128(_t0_257)));

    // AVX Storer:
    _t0_55 = _t0_258;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_259 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_46, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_260 = _t0_55;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_261 = _t0_37;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_262 = _mm256_mul_pd(_t0_260, _t0_261);

    // 4-BLAC: 1x4 - 1x4
    _t0_263 = _mm256_sub_pd(_t0_259, _t0_262);

    // AVX Storer:
    _t0_56 = _t0_263;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_264 = _t0_56;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_265 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_266 = _t0_36;

    // 4-BLAC: 1x4 + 1x4
    _t0_267 = _mm256_add_pd(_t0_265, _t0_266);

    // 4-BLAC: 1x4 / 1x4
    _t0_268 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_264), _mm256_castpd256_pd128(_t0_267)));

    // AVX Storer:
    _t0_56 = _t0_268;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_269 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_46, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_46, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_270 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_55, _t0_56), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_271 = _t0_35;

    // 4-BLAC: 1x4 * 4x1
    _t0_272 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_270, _t0_271), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_270, _t0_271), _mm256_mul_pd(_t0_270, _t0_271), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_270, _t0_271), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_270, _t0_271), _mm256_mul_pd(_t0_270, _t0_271), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_270, _t0_271), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_270, _t0_271), _mm256_mul_pd(_t0_270, _t0_271), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_273 = _mm256_sub_pd(_t0_269, _t0_272);

    // AVX Storer:
    _t0_57 = _t0_273;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_274 = _t0_57;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_275 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_276 = _t0_34;

    // 4-BLAC: 1x4 + 1x4
    _t0_277 = _mm256_add_pd(_t0_275, _t0_276);

    // 4-BLAC: 1x4 / 1x4
    _t0_278 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_274), _mm256_castpd256_pd128(_t0_277)));

    // AVX Storer:
    _t0_57 = _t0_278;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_279 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_46, _t0_46, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_280 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_55, _t0_56), _mm256_unpacklo_pd(_t0_57, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_281 = _t0_33;

    // 4-BLAC: 1x4 * 4x1
    _t0_282 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_280, _t0_281), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_280, _t0_281), _mm256_mul_pd(_t0_280, _t0_281), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_280, _t0_281), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_280, _t0_281), _mm256_mul_pd(_t0_280, _t0_281), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_280, _t0_281), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_280, _t0_281), _mm256_mul_pd(_t0_280, _t0_281), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_283 = _mm256_sub_pd(_t0_279, _t0_282);

    // AVX Storer:
    _t0_58 = _t0_283;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_284 = _t0_58;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_285 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_286 = _t0_32;

    // 4-BLAC: 1x4 + 1x4
    _t0_287 = _mm256_add_pd(_t0_285, _t0_286);

    // 4-BLAC: 1x4 / 1x4
    _t0_288 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_284), _mm256_castpd256_pd128(_t0_287)));

    // AVX Storer:
    _t0_58 = _t0_288;

    // Generating : X[100,100] = S(h(4, 100, fi4), ( G(h(4, 100, fi4), X[100,100],h(4, 100, 4)) - ( G(h(4, 100, fi4), X[100,100],h(4, 100, 0)) * G(h(4, 100, 0), U[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t0_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_40, _t0_40, 32), _mm256_permute2f128_pd(_t0_40, _t0_40, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_42, _t0_42, 32), _mm256_permute2f128_pd(_t0_42, _t0_42, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0), _t0_22)));
    _t0_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_47, _t0_47, 32), _mm256_permute2f128_pd(_t0_47, _t0_47, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_50, _t0_50, 32), _mm256_permute2f128_pd(_t0_50, _t0_50, 32), 0), _t0_22)));
    _t0_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_52, _t0_52, 32), _mm256_permute2f128_pd(_t0_52, _t0_52, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t0_22)));
    _t0_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_56, _t0_56, 32), _mm256_permute2f128_pd(_t0_56, _t0_56, 32), 0), _t0_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0), _t0_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_58, _t0_58, 32), _mm256_permute2f128_pd(_t0_58, _t0_58, 32), 0), _t0_22)));

    // 4-BLAC: 4x4 - 4x4
    _t0_110 = _mm256_sub_pd(_t0_110, _t0_102);
    _t0_111 = _mm256_sub_pd(_t0_111, _t0_103);
    _t0_112 = _mm256_sub_pd(_t0_112, _t0_104);
    _t0_113 = _mm256_sub_pd(_t0_113, _t0_105);

    // AVX Storer:

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_289 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_290 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_291 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t0_292 = _mm256_add_pd(_t0_290, _t0_291);

    // 4-BLAC: 1x4 / 1x4
    _t0_293 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_289), _mm256_castpd256_pd128(_t0_292)));

    // AVX Storer:
    _t0_59 = _t0_293;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, fi4), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_294 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_295 = _t0_59;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_296 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_297 = _mm256_mul_pd(_t0_295, _t0_296);

    // 4-BLAC: 1x4 - 1x4
    _t0_298 = _mm256_sub_pd(_t0_294, _t0_297);

    // AVX Storer:
    _t0_60 = _t0_298;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_299 = _t0_60;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_300 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_301 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t0_302 = _mm256_add_pd(_t0_300, _t0_301);

    // 4-BLAC: 1x4 / 1x4
    _t0_303 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_299), _mm256_castpd256_pd128(_t0_302)));

    // AVX Storer:
    _t0_60 = _t0_303;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, fi4), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_304 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_110, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_305 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_59, _t0_60), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_306 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t0_307 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_305, _t0_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_305, _t0_306), _mm256_mul_pd(_t0_305, _t0_306), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_305, _t0_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_305, _t0_306), _mm256_mul_pd(_t0_305, _t0_306), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_305, _t0_306), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_305, _t0_306), _mm256_mul_pd(_t0_305, _t0_306), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_308 = _mm256_sub_pd(_t0_304, _t0_307);

    // AVX Storer:
    _t0_61 = _t0_308;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_309 = _t0_61;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_310 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_311 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t0_312 = _mm256_add_pd(_t0_310, _t0_311);

    // 4-BLAC: 1x4 / 1x4
    _t0_313 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_309), _mm256_castpd256_pd128(_t0_312)));

    // AVX Storer:
    _t0_61 = _t0_313;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, fi4), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_314 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_110, _t0_110, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_315 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_59, _t0_60), _mm256_unpacklo_pd(_t0_61, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_316 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t0_317 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_315, _t0_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_315, _t0_316), _mm256_mul_pd(_t0_315, _t0_316), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_315, _t0_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_315, _t0_316), _mm256_mul_pd(_t0_315, _t0_316), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_315, _t0_316), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_315, _t0_316), _mm256_mul_pd(_t0_315, _t0_316), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_318 = _mm256_sub_pd(_t0_314, _t0_317);

    // AVX Storer:
    _t0_62 = _t0_318;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_319 = _t0_62;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_320 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_321 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t0_322 = _mm256_add_pd(_t0_320, _t0_321);

    // 4-BLAC: 1x4 / 1x4
    _t0_323 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_319), _mm256_castpd256_pd128(_t0_322)));

    // AVX Storer:
    _t0_62 = _t0_323;

    // Generating : X[100,100] = S(h(3, 100, fi4 + 1), ( G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, 4)) - ( G(h(3, 100, fi4 + 1), L[100,100],h(1, 100, fi4)) * G(h(1, 100, fi4), X[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t0_324 = _t0_111;
    _t0_325 = _t0_112;
    _t0_326 = _t0_113;
    _t0_327 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_328 = _t0_31;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t0_329 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_328, _t0_328, 32), _mm256_permute2f128_pd(_t0_328, _t0_328, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_59, _t0_60), _mm256_unpacklo_pd(_t0_61, _t0_62), 32));
    _t0_330 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_328, _t0_328, 32), _mm256_permute2f128_pd(_t0_328, _t0_328, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_59, _t0_60), _mm256_unpacklo_pd(_t0_61, _t0_62), 32));
    _t0_331 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_328, _t0_328, 49), _mm256_permute2f128_pd(_t0_328, _t0_328, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_59, _t0_60), _mm256_unpacklo_pd(_t0_61, _t0_62), 32));
    _t0_332 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_328, _t0_328, 49), _mm256_permute2f128_pd(_t0_328, _t0_328, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_59, _t0_60), _mm256_unpacklo_pd(_t0_61, _t0_62), 32));

    // 4-BLAC: 4x4 - 4x4
    _t0_333 = _mm256_sub_pd(_t0_324, _t0_329);
    _t0_334 = _mm256_sub_pd(_t0_325, _t0_330);
    _t0_335 = _mm256_sub_pd(_t0_326, _t0_331);
    _t0_336 = _mm256_sub_pd(_t0_327, _t0_332);

    // AVX Storer:
    _t0_111 = _t0_333;
    _t0_112 = _t0_334;
    _t0_113 = _t0_335;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_337 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_338 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_339 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t0_340 = _mm256_add_pd(_t0_338, _t0_339);

    // 4-BLAC: 1x4 / 1x4
    _t0_341 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_337), _mm256_castpd256_pd128(_t0_340)));

    // AVX Storer:
    _t0_63 = _t0_341;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_342 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_343 = _t0_63;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_344 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_345 = _mm256_mul_pd(_t0_343, _t0_344);

    // 4-BLAC: 1x4 - 1x4
    _t0_346 = _mm256_sub_pd(_t0_342, _t0_345);

    // AVX Storer:
    _t0_64 = _t0_346;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_347 = _t0_64;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_348 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_349 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t0_350 = _mm256_add_pd(_t0_348, _t0_349);

    // 4-BLAC: 1x4 / 1x4
    _t0_351 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_347), _mm256_castpd256_pd128(_t0_350)));

    // AVX Storer:
    _t0_64 = _t0_351;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_352 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_111, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_353 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_63, _t0_64), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_354 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t0_355 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_353, _t0_354), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_353, _t0_354), _mm256_mul_pd(_t0_353, _t0_354), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_353, _t0_354), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_353, _t0_354), _mm256_mul_pd(_t0_353, _t0_354), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_353, _t0_354), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_353, _t0_354), _mm256_mul_pd(_t0_353, _t0_354), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_356 = _mm256_sub_pd(_t0_352, _t0_355);

    // AVX Storer:
    _t0_65 = _t0_356;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_357 = _t0_65;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_358 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_359 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t0_360 = _mm256_add_pd(_t0_358, _t0_359);

    // 4-BLAC: 1x4 / 1x4
    _t0_361 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_357), _mm256_castpd256_pd128(_t0_360)));

    // AVX Storer:
    _t0_65 = _t0_361;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_362 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_111, _t0_111, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_363 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_63, _t0_64), _mm256_unpacklo_pd(_t0_65, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_364 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t0_365 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_363, _t0_364), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_363, _t0_364), _mm256_mul_pd(_t0_363, _t0_364), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_363, _t0_364), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_363, _t0_364), _mm256_mul_pd(_t0_363, _t0_364), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_363, _t0_364), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_363, _t0_364), _mm256_mul_pd(_t0_363, _t0_364), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_366 = _mm256_sub_pd(_t0_362, _t0_365);

    // AVX Storer:
    _t0_66 = _t0_366;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_367 = _t0_66;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_368 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_369 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t0_370 = _mm256_add_pd(_t0_368, _t0_369);

    // 4-BLAC: 1x4 / 1x4
    _t0_371 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_367), _mm256_castpd256_pd128(_t0_370)));

    // AVX Storer:
    _t0_66 = _t0_371;

    // Generating : X[100,100] = S(h(2, 100, fi4 + 2), ( G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, 4)) - ( G(h(2, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 1)) * G(h(1, 100, fi4 + 1), X[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t0_372 = _t0_112;
    _t0_373 = _t0_113;
    _t0_374 = _mm256_setzero_pd();
    _t0_375 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_376 = _t0_29;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t0_377 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_376, _t0_376, 32), _mm256_permute2f128_pd(_t0_376, _t0_376, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_63, _t0_64), _mm256_unpacklo_pd(_t0_65, _t0_66), 32));
    _t0_378 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_376, _t0_376, 32), _mm256_permute2f128_pd(_t0_376, _t0_376, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_63, _t0_64), _mm256_unpacklo_pd(_t0_65, _t0_66), 32));
    _t0_379 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_376, _t0_376, 49), _mm256_permute2f128_pd(_t0_376, _t0_376, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_63, _t0_64), _mm256_unpacklo_pd(_t0_65, _t0_66), 32));
    _t0_380 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_376, _t0_376, 49), _mm256_permute2f128_pd(_t0_376, _t0_376, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_63, _t0_64), _mm256_unpacklo_pd(_t0_65, _t0_66), 32));

    // 4-BLAC: 4x4 - 4x4
    _t0_381 = _mm256_sub_pd(_t0_372, _t0_377);
    _t0_382 = _mm256_sub_pd(_t0_373, _t0_378);
    _t0_383 = _mm256_sub_pd(_t0_374, _t0_379);
    _t0_384 = _mm256_sub_pd(_t0_375, _t0_380);

    // AVX Storer:
    _t0_112 = _t0_381;
    _t0_113 = _t0_382;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_385 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_386 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_387 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t0_388 = _mm256_add_pd(_t0_386, _t0_387);

    // 4-BLAC: 1x4 / 1x4
    _t0_389 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_385), _mm256_castpd256_pd128(_t0_388)));

    // AVX Storer:
    _t0_67 = _t0_389;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_390 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_391 = _t0_67;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_392 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_393 = _mm256_mul_pd(_t0_391, _t0_392);

    // 4-BLAC: 1x4 - 1x4
    _t0_394 = _mm256_sub_pd(_t0_390, _t0_393);

    // AVX Storer:
    _t0_68 = _t0_394;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_395 = _t0_68;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_396 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_397 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t0_398 = _mm256_add_pd(_t0_396, _t0_397);

    // 4-BLAC: 1x4 / 1x4
    _t0_399 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_395), _mm256_castpd256_pd128(_t0_398)));

    // AVX Storer:
    _t0_68 = _t0_399;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_400 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_112, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_401 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_67, _t0_68), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_402 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t0_403 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_401, _t0_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_401, _t0_402), _mm256_mul_pd(_t0_401, _t0_402), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_401, _t0_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_401, _t0_402), _mm256_mul_pd(_t0_401, _t0_402), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_401, _t0_402), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_401, _t0_402), _mm256_mul_pd(_t0_401, _t0_402), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_404 = _mm256_sub_pd(_t0_400, _t0_403);

    // AVX Storer:
    _t0_69 = _t0_404;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_405 = _t0_69;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_406 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_407 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t0_408 = _mm256_add_pd(_t0_406, _t0_407);

    // 4-BLAC: 1x4 / 1x4
    _t0_409 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_405), _mm256_castpd256_pd128(_t0_408)));

    // AVX Storer:
    _t0_69 = _t0_409;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_410 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_112, _t0_112, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_411 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_67, _t0_68), _mm256_unpacklo_pd(_t0_69, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_412 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t0_413 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_411, _t0_412), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_411, _t0_412), _mm256_mul_pd(_t0_411, _t0_412), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_411, _t0_412), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_411, _t0_412), _mm256_mul_pd(_t0_411, _t0_412), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_411, _t0_412), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_411, _t0_412), _mm256_mul_pd(_t0_411, _t0_412), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_414 = _mm256_sub_pd(_t0_410, _t0_413);

    // AVX Storer:
    _t0_70 = _t0_414;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_415 = _t0_70;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_416 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_417 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t0_418 = _mm256_add_pd(_t0_416, _t0_417);

    // 4-BLAC: 1x4 / 1x4
    _t0_419 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_415), _mm256_castpd256_pd128(_t0_418)));

    // AVX Storer:
    _t0_70 = _t0_419;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, 4)) - ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 2)) Kro G(h(1, 100, fi4 + 2), X[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_420 = _t0_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t0_91 = _mm256_mul_pd(_t0_420, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_67, _t0_68), _mm256_unpacklo_pd(_t0_69, _t0_70), 32));

    // 4-BLAC: 1x4 - 1x4
    _t0_113 = _mm256_sub_pd(_t0_113, _t0_91);

    // AVX Storer:

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_421 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_422 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_423 = _t0_21;

    // 4-BLAC: 1x4 + 1x4
    _t0_424 = _mm256_add_pd(_t0_422, _t0_423);

    // 4-BLAC: 1x4 / 1x4
    _t0_425 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_421), _mm256_castpd256_pd128(_t0_424)));

    // AVX Storer:
    _t0_71 = _t0_425;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_426 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_427 = _t0_71;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_428 = _t0_20;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_429 = _mm256_mul_pd(_t0_427, _t0_428);

    // 4-BLAC: 1x4 - 1x4
    _t0_430 = _mm256_sub_pd(_t0_426, _t0_429);

    // AVX Storer:
    _t0_72 = _t0_430;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_431 = _t0_72;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_432 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_433 = _t0_19;

    // 4-BLAC: 1x4 + 1x4
    _t0_434 = _mm256_add_pd(_t0_432, _t0_433);

    // 4-BLAC: 1x4 / 1x4
    _t0_435 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_431), _mm256_castpd256_pd128(_t0_434)));

    // AVX Storer:
    _t0_72 = _t0_435;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_436 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_113, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_437 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_71, _t0_72), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_438 = _t0_18;

    // 4-BLAC: 1x4 * 4x1
    _t0_439 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_437, _t0_438), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_437, _t0_438), _mm256_mul_pd(_t0_437, _t0_438), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_437, _t0_438), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_437, _t0_438), _mm256_mul_pd(_t0_437, _t0_438), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_437, _t0_438), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_437, _t0_438), _mm256_mul_pd(_t0_437, _t0_438), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_440 = _mm256_sub_pd(_t0_436, _t0_439);

    // AVX Storer:
    _t0_73 = _t0_440;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_442 = _t0_73;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_443 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_444 = _t0_17;

    // 4-BLAC: 1x4 + 1x4
    _t0_445 = _mm256_add_pd(_t0_443, _t0_444);

    // 4-BLAC: 1x4 / 1x4
    _t0_447 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_442), _mm256_castpd256_pd128(_t0_445)));

    // AVX Storer:
    _t0_73 = _t0_447;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_448 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_113, _t0_113, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_449 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_71, _t0_72), _mm256_unpacklo_pd(_t0_73, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_450 = _t0_16;

    // 4-BLAC: 1x4 * 4x1
    _t0_451 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_449, _t0_450), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_449, _t0_450), _mm256_mul_pd(_t0_449, _t0_450), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_449, _t0_450), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_449, _t0_450), _mm256_mul_pd(_t0_449, _t0_450), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_449, _t0_450), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_449, _t0_450), _mm256_mul_pd(_t0_449, _t0_450), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_453 = _mm256_sub_pd(_t0_448, _t0_451);

    // AVX Storer:
    _t0_74 = _t0_453;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_454 = _t0_74;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_455 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_456 = _t0_15;

    // 4-BLAC: 1x4 + 1x4
    _t0_457 = _mm256_add_pd(_t0_455, _t0_456);

    // 4-BLAC: 1x4 / 1x4
    _t0_458 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_454), _mm256_castpd256_pd128(_t0_457)));

    // AVX Storer:
    _t0_74 = _t0_458;

    // Generating : X[100,100] = ( S(h(4, 100, fi4), ( G(h(4, 100, fi4), X[100,100],h(4, 100, fi25)) - ( G(h(4, 100, fi4), X[100,100],h(4, 100, 0)) * G(h(4, 100, 0), U[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25)) + Sum_{k116} ( -$(h(4, 100, fi4), ( G(h(4, 100, fi4), X[100,100],h(4, 100, k116)) * G(h(4, 100, k116), U[100,100],h(4, 100, fi25)) ),h(4, 100, fi25)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t0_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_40, _t0_40, 32), _mm256_permute2f128_pd(_t0_40, _t0_40, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_42, _t0_42, 32), _mm256_permute2f128_pd(_t0_42, _t0_42, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0), _t0_11)));
    _t0_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_47, _t0_47, 32), _mm256_permute2f128_pd(_t0_47, _t0_47, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_50, _t0_50, 32), _mm256_permute2f128_pd(_t0_50, _t0_50, 32), 0), _t0_11)));
    _t0_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_52, _t0_52, 32), _mm256_permute2f128_pd(_t0_52, _t0_52, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t0_11)));
    _t0_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t0_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_56, _t0_56, 32), _mm256_permute2f128_pd(_t0_56, _t0_56, 32), 0), _t0_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0), _t0_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_58, _t0_58, 32), _mm256_permute2f128_pd(_t0_58, _t0_58, 32), 0), _t0_11)));

    // 4-BLAC: 4x4 - 4x4
    _t0_106 = _mm256_sub_pd(_t0_106, _t0_94);
    _t0_107 = _mm256_sub_pd(_t0_107, _t0_95);
    _t0_108 = _mm256_sub_pd(_t0_108, _t0_96);
    _t0_109 = _mm256_sub_pd(_t0_109, _t0_97);

    // AVX Storer:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t0_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_59, _t0_59, 32), _mm256_permute2f128_pd(_t0_59, _t0_59, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_60, _t0_60, 32), _mm256_permute2f128_pd(_t0_60, _t0_60, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_61, _t0_61, 32), _mm256_permute2f128_pd(_t0_61, _t0_61, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_62, _t0_62, 32), _mm256_permute2f128_pd(_t0_62, _t0_62, 32), 0), _t0_7)));
    _t0_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_63, _t0_63, 32), _mm256_permute2f128_pd(_t0_63, _t0_63, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_64, _t0_64, 32), _mm256_permute2f128_pd(_t0_64, _t0_64, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_65, _t0_65, 32), _mm256_permute2f128_pd(_t0_65, _t0_65, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_66, _t0_66, 32), _mm256_permute2f128_pd(_t0_66, _t0_66, 32), 0), _t0_7)));
    _t0_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_67, _t0_67, 32), _mm256_permute2f128_pd(_t0_67, _t0_67, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_68, _t0_68, 32), _mm256_permute2f128_pd(_t0_68, _t0_68, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_69, _t0_69, 32), _mm256_permute2f128_pd(_t0_69, _t0_69, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_70, _t0_70, 32), _mm256_permute2f128_pd(_t0_70, _t0_70, 32), 0), _t0_7)));
    _t0_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_71, _t0_71, 32), _mm256_permute2f128_pd(_t0_71, _t0_71, 32), 0), _t0_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_72, _t0_72, 32), _mm256_permute2f128_pd(_t0_72, _t0_72, 32), 0), _t0_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_73, _t0_73, 32), _mm256_permute2f128_pd(_t0_73, _t0_73, 32), 0), _t0_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_74, _t0_74, 32), _mm256_permute2f128_pd(_t0_74, _t0_74, 32), 0), _t0_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t0_106 = _mm256_sub_pd(_t0_106, _t0_98);
    _t0_107 = _mm256_sub_pd(_t0_107, _t0_99);
    _t0_108 = _mm256_sub_pd(_t0_108, _t0_100);
    _t0_109 = _mm256_sub_pd(_t0_109, _t0_101);

    // AVX Storer:

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_459 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_106, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_460 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_461 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_462 = _mm256_add_pd(_t0_460, _t0_461);

    // 4-BLAC: 1x4 / 1x4
    _t0_463 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_459), _mm256_castpd256_pd128(_t0_462)));

    // AVX Storer:
    _t0_75 = _t0_463;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_464 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_106, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_465 = _t0_75;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_466 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_468 = _mm256_mul_pd(_t0_465, _t0_466);

    // 4-BLAC: 1x4 - 1x4
    _t0_469 = _mm256_sub_pd(_t0_464, _t0_468);

    // AVX Storer:
    _t0_76 = _t0_469;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_470 = _t0_76;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_471 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_472 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_473 = _mm256_add_pd(_t0_471, _t0_472);

    // 4-BLAC: 1x4 / 1x4
    _t0_474 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_470), _mm256_castpd256_pd128(_t0_473)));

    // AVX Storer:
    _t0_76 = _t0_474;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_475 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_106, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_106, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_476 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_75, _t0_76), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_477 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_478 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_476, _t0_477), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_476, _t0_477), _mm256_mul_pd(_t0_476, _t0_477), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_476, _t0_477), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_476, _t0_477), _mm256_mul_pd(_t0_476, _t0_477), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_476, _t0_477), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_476, _t0_477), _mm256_mul_pd(_t0_476, _t0_477), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_479 = _mm256_sub_pd(_t0_475, _t0_478);

    // AVX Storer:
    _t0_77 = _t0_479;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_480 = _t0_77;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_482 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_483 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_484 = _mm256_add_pd(_t0_482, _t0_483);

    // 4-BLAC: 1x4 / 1x4
    _t0_485 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_480), _mm256_castpd256_pd128(_t0_484)));

    // AVX Storer:
    _t0_77 = _t0_485;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_486 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_106, _t0_106, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_487 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_75, _t0_76), _mm256_unpacklo_pd(_t0_77, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_488 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_489 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_487, _t0_488), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_487, _t0_488), _mm256_mul_pd(_t0_487, _t0_488), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_487, _t0_488), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_487, _t0_488), _mm256_mul_pd(_t0_487, _t0_488), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_487, _t0_488), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_487, _t0_488), _mm256_mul_pd(_t0_487, _t0_488), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_490 = _mm256_sub_pd(_t0_486, _t0_489);

    // AVX Storer:
    _t0_78 = _t0_490;

    // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_491 = _t0_78;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_492 = _t0_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_493 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_494 = _mm256_add_pd(_t0_492, _t0_493);

    // 4-BLAC: 1x4 / 1x4
    _t0_495 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_491), _mm256_castpd256_pd128(_t0_494)));

    // AVX Storer:
    _t0_78 = _t0_495;

    // Generating : X[100,100] = S(h(3, 100, fi4 + 1), ( G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) - ( G(h(3, 100, fi4 + 1), L[100,100],h(1, 100, fi4)) * G(h(1, 100, fi4), X[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25))

    // AVX Loader:

    // 3x4 -> 4x4
    _t0_497 = _t0_107;
    _t0_498 = _t0_108;
    _t0_499 = _t0_109;
    _t0_500 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_501 = _t0_31;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t0_502 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_501, _t0_501, 32), _mm256_permute2f128_pd(_t0_501, _t0_501, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_75, _t0_76), _mm256_unpacklo_pd(_t0_77, _t0_78), 32));
    _t0_503 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_501, _t0_501, 32), _mm256_permute2f128_pd(_t0_501, _t0_501, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_75, _t0_76), _mm256_unpacklo_pd(_t0_77, _t0_78), 32));
    _t0_504 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_501, _t0_501, 49), _mm256_permute2f128_pd(_t0_501, _t0_501, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_75, _t0_76), _mm256_unpacklo_pd(_t0_77, _t0_78), 32));
    _t0_505 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_501, _t0_501, 49), _mm256_permute2f128_pd(_t0_501, _t0_501, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_75, _t0_76), _mm256_unpacklo_pd(_t0_77, _t0_78), 32));

    // 4-BLAC: 4x4 - 4x4
    _t0_506 = _mm256_sub_pd(_t0_497, _t0_502);
    _t0_507 = _mm256_sub_pd(_t0_498, _t0_503);
    _t0_508 = _mm256_sub_pd(_t0_499, _t0_504);
    _t0_509 = _mm256_sub_pd(_t0_500, _t0_505);

    // AVX Storer:
    _t0_107 = _t0_506;
    _t0_108 = _t0_507;
    _t0_109 = _t0_508;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_511 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_107, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_512 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_513 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_514 = _mm256_add_pd(_t0_512, _t0_513);

    // 4-BLAC: 1x4 / 1x4
    _t0_515 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_511), _mm256_castpd256_pd128(_t0_514)));

    // AVX Storer:
    _t0_79 = _t0_515;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_517 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_107, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_518 = _t0_79;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_519 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_520 = _mm256_mul_pd(_t0_518, _t0_519);

    // 4-BLAC: 1x4 - 1x4
    _t0_521 = _mm256_sub_pd(_t0_517, _t0_520);

    // AVX Storer:
    _t0_80 = _t0_521;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_522 = _t0_80;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_523 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_524 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_525 = _mm256_add_pd(_t0_523, _t0_524);

    // 4-BLAC: 1x4 / 1x4
    _t0_526 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_522), _mm256_castpd256_pd128(_t0_525)));

    // AVX Storer:
    _t0_80 = _t0_526;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_527 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_107, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_107, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_528 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_79, _t0_80), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_529 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_530 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_528, _t0_529), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_528, _t0_529), _mm256_mul_pd(_t0_528, _t0_529), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_528, _t0_529), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_528, _t0_529), _mm256_mul_pd(_t0_528, _t0_529), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_528, _t0_529), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_528, _t0_529), _mm256_mul_pd(_t0_528, _t0_529), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_532 = _mm256_sub_pd(_t0_527, _t0_530);

    // AVX Storer:
    _t0_81 = _t0_532;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_533 = _t0_81;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_534 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_535 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_536 = _mm256_add_pd(_t0_534, _t0_535);

    // 4-BLAC: 1x4 / 1x4
    _t0_537 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_533), _mm256_castpd256_pd128(_t0_536)));

    // AVX Storer:
    _t0_81 = _t0_537;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_538 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_107, _t0_107, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_539 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_80), _mm256_unpacklo_pd(_t0_81, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_540 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_541 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_539, _t0_540), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_539, _t0_540), _mm256_mul_pd(_t0_539, _t0_540), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_539, _t0_540), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_539, _t0_540), _mm256_mul_pd(_t0_539, _t0_540), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_539, _t0_540), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_539, _t0_540), _mm256_mul_pd(_t0_539, _t0_540), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_542 = _mm256_sub_pd(_t0_538, _t0_541);

    // AVX Storer:
    _t0_82 = _t0_542;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_543 = _t0_82;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_544 = _t0_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_545 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_547 = _mm256_add_pd(_t0_544, _t0_545);

    // 4-BLAC: 1x4 / 1x4
    _t0_548 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_543), _mm256_castpd256_pd128(_t0_547)));

    // AVX Storer:
    _t0_82 = _t0_548;

    // Generating : X[100,100] = S(h(2, 100, fi4 + 2), ( G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) - ( G(h(2, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 1)) * G(h(1, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25))

    // AVX Loader:

    // 2x4 -> 4x4
    _t0_549 = _t0_108;
    _t0_550 = _t0_109;
    _t0_551 = _mm256_setzero_pd();
    _t0_552 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_553 = _t0_29;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t0_554 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_553, _t0_553, 32), _mm256_permute2f128_pd(_t0_553, _t0_553, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_80), _mm256_unpacklo_pd(_t0_81, _t0_82), 32));
    _t0_555 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_553, _t0_553, 32), _mm256_permute2f128_pd(_t0_553, _t0_553, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_80), _mm256_unpacklo_pd(_t0_81, _t0_82), 32));
    _t0_556 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_553, _t0_553, 49), _mm256_permute2f128_pd(_t0_553, _t0_553, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_80), _mm256_unpacklo_pd(_t0_81, _t0_82), 32));
    _t0_557 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_553, _t0_553, 49), _mm256_permute2f128_pd(_t0_553, _t0_553, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_79, _t0_80), _mm256_unpacklo_pd(_t0_81, _t0_82), 32));

    // 4-BLAC: 4x4 - 4x4
    _t0_558 = _mm256_sub_pd(_t0_549, _t0_554);
    _t0_559 = _mm256_sub_pd(_t0_550, _t0_555);
    _t0_560 = _mm256_sub_pd(_t0_551, _t0_556);
    _t0_561 = _mm256_sub_pd(_t0_552, _t0_557);

    // AVX Storer:
    _t0_108 = _t0_558;
    _t0_109 = _t0_559;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_562 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_108, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_563 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_564 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_565 = _mm256_add_pd(_t0_563, _t0_564);

    // 4-BLAC: 1x4 / 1x4
    _t0_566 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_562), _mm256_castpd256_pd128(_t0_565)));

    // AVX Storer:
    _t0_83 = _t0_566;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_567 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_108, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_568 = _t0_83;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_570 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_571 = _mm256_mul_pd(_t0_568, _t0_570);

    // 4-BLAC: 1x4 - 1x4
    _t0_572 = _mm256_sub_pd(_t0_567, _t0_571);

    // AVX Storer:
    _t0_84 = _t0_572;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_573 = _t0_84;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_575 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_576 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_577 = _mm256_add_pd(_t0_575, _t0_576);

    // 4-BLAC: 1x4 / 1x4
    _t0_578 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_573), _mm256_castpd256_pd128(_t0_577)));

    // AVX Storer:
    _t0_84 = _t0_578;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_580 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_108, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_108, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_581 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_83, _t0_84), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_582 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_583 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_581, _t0_582), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_581, _t0_582), _mm256_mul_pd(_t0_581, _t0_582), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_581, _t0_582), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_581, _t0_582), _mm256_mul_pd(_t0_581, _t0_582), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_581, _t0_582), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_581, _t0_582), _mm256_mul_pd(_t0_581, _t0_582), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_584 = _mm256_sub_pd(_t0_580, _t0_583);

    // AVX Storer:
    _t0_85 = _t0_584;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_585 = _t0_85;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_586 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_587 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_588 = _mm256_add_pd(_t0_586, _t0_587);

    // 4-BLAC: 1x4 / 1x4
    _t0_589 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_585), _mm256_castpd256_pd128(_t0_588)));

    // AVX Storer:
    _t0_85 = _t0_589;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_590 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_108, _t0_108, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_591 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_83, _t0_84), _mm256_unpacklo_pd(_t0_85, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_592 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_593 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_591, _t0_592), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_591, _t0_592), _mm256_mul_pd(_t0_591, _t0_592), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_591, _t0_592), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_591, _t0_592), _mm256_mul_pd(_t0_591, _t0_592), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_591, _t0_592), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_591, _t0_592), _mm256_mul_pd(_t0_591, _t0_592), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_595 = _mm256_sub_pd(_t0_590, _t0_593);

    // AVX Storer:
    _t0_86 = _t0_595;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_596 = _t0_86;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_597 = _t0_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_598 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_599 = _mm256_add_pd(_t0_597, _t0_598);

    // 4-BLAC: 1x4 / 1x4
    _t0_600 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_596), _mm256_castpd256_pd128(_t0_599)));

    // AVX Storer:
    _t0_86 = _t0_600;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, fi25)) - ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 2)) Kro G(h(1, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_601 = _t0_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t0_92 = _mm256_mul_pd(_t0_601, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_83, _t0_84), _mm256_unpacklo_pd(_t0_85, _t0_86), 32));

    // 4-BLAC: 1x4 - 1x4
    _t0_109 = _mm256_sub_pd(_t0_109, _t0_92);

    // AVX Storer:

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_602 = _mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_603 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_604 = _t0_6;

    // 4-BLAC: 1x4 + 1x4
    _t0_605 = _mm256_add_pd(_t0_603, _t0_604);

    // 4-BLAC: 1x4 / 1x4
    _t0_606 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_602), _mm256_castpd256_pd128(_t0_605)));

    // AVX Storer:
    _t0_87 = _t0_606;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_607 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_608 = _t0_87;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_609 = _t0_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t0_611 = _mm256_mul_pd(_t0_608, _t0_609);

    // 4-BLAC: 1x4 - 1x4
    _t0_612 = _mm256_sub_pd(_t0_607, _t0_611);

    // AVX Storer:
    _t0_88 = _t0_612;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_613 = _t0_88;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_614 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_114 = _t0_4;

    // 4-BLAC: 1x4 + 1x4
    _t0_115 = _mm256_add_pd(_t0_614, _t0_114);

    // 4-BLAC: 1x4 / 1x4
    _t0_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_613), _mm256_castpd256_pd128(_t0_115)));

    // AVX Storer:
    _t0_88 = _t0_116;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_117 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t0_109, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t0_118 = _mm256_blend_pd(_mm256_unpacklo_pd(_t0_87, _t0_88), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t0_119 = _t0_3;

    // 4-BLAC: 1x4 * 4x1
    _t0_120 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_118, _t0_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_118, _t0_119), _mm256_mul_pd(_t0_118, _t0_119), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_118, _t0_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_118, _t0_119), _mm256_mul_pd(_t0_118, _t0_119), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_118, _t0_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_118, _t0_119), _mm256_mul_pd(_t0_118, _t0_119), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_121 = _mm256_sub_pd(_t0_117, _t0_120);

    // AVX Storer:
    _t0_89 = _t0_121;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_123 = _t0_89;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_124 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_125 = _t0_2;

    // 4-BLAC: 1x4 + 1x4
    _t0_126 = _mm256_add_pd(_t0_124, _t0_125);

    // 4-BLAC: 1x4 / 1x4
    _t0_127 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_123), _mm256_castpd256_pd128(_t0_126)));

    // AVX Storer:
    _t0_89 = _t0_127;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_129 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t0_109, _t0_109, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t0_130 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t0_87, _t0_88), _mm256_unpacklo_pd(_t0_89, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t0_131 = _t0_1;

    // 4-BLAC: 1x4 * 4x1
    _t0_132 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t0_130, _t0_131), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_130, _t0_131), _mm256_mul_pd(_t0_130, _t0_131), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t0_130, _t0_131), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_130, _t0_131), _mm256_mul_pd(_t0_130, _t0_131), 129)), _mm256_add_pd(_mm256_mul_pd(_t0_130, _t0_131), _mm256_permute2f128_pd(_mm256_mul_pd(_t0_130, _t0_131), _mm256_mul_pd(_t0_130, _t0_131), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t0_133 = _mm256_sub_pd(_t0_129, _t0_132);

    // AVX Storer:
    _t0_90 = _t0_133;

    // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_135 = _t0_90;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_136 = _t0_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t0_137 = _t0_0;

    // 4-BLAC: 1x4 + 1x4
    _t0_138 = _mm256_add_pd(_t0_136, _t0_137);

    // 4-BLAC: 1x4 / 1x4
    _t0_139 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t0_135), _mm256_castpd256_pd128(_t0_138)));

    // AVX Storer:
    _t0_90 = _t0_139;
    _asm256_storeu_pd(C + 100*fi4 + 100, _t0_44);
    _asm256_storeu_pd(C + 100*fi4 + 200, _t0_45);
    _asm256_storeu_pd(C + 100*fi4 + 300, _t0_46);
    _mm_store_sd(&(C[100*fi4 + 4]), _mm256_castpd256_pd128(_t0_59));
    _mm_store_sd(&(C[100*fi4 + 5]), _mm256_castpd256_pd128(_t0_60));
    _mm_store_sd(&(C[100*fi4 + 6]), _mm256_castpd256_pd128(_t0_61));
    _mm_store_sd(&(C[100*fi4 + 7]), _mm256_castpd256_pd128(_t0_62));
    _mm_store_sd(&(C[100*fi4 + 104]), _mm256_castpd256_pd128(_t0_63));
    _mm_store_sd(&(C[100*fi4 + 105]), _mm256_castpd256_pd128(_t0_64));
    _mm_store_sd(&(C[100*fi4 + 106]), _mm256_castpd256_pd128(_t0_65));
    _mm_store_sd(&(C[100*fi4 + 107]), _mm256_castpd256_pd128(_t0_66));
    _mm_store_sd(&(C[100*fi4 + 204]), _mm256_castpd256_pd128(_t0_67));
    _mm_store_sd(&(C[100*fi4 + 205]), _mm256_castpd256_pd128(_t0_68));
    _mm_store_sd(&(C[100*fi4 + 206]), _mm256_castpd256_pd128(_t0_69));
    _mm_store_sd(&(C[100*fi4 + 207]), _mm256_castpd256_pd128(_t0_70));
    _mm_store_sd(&(C[100*fi4 + 304]), _mm256_castpd256_pd128(_t0_71));
    _mm_store_sd(&(C[100*fi4 + 305]), _mm256_castpd256_pd128(_t0_72));
    _mm_store_sd(&(C[100*fi4 + 306]), _mm256_castpd256_pd128(_t0_73));
    _mm_store_sd(&(C[100*fi4 + 307]), _mm256_castpd256_pd128(_t0_74));
    _mm_store_sd(&(C[100*fi4 + 8]), _mm256_castpd256_pd128(_t0_75));
    _mm_store_sd(&(C[100*fi4 + 9]), _mm256_castpd256_pd128(_t0_76));
    _mm_store_sd(&(C[100*fi4 + 10]), _mm256_castpd256_pd128(_t0_77));
    _mm_store_sd(&(C[100*fi4 + 11]), _mm256_castpd256_pd128(_t0_78));
    _mm_store_sd(&(C[100*fi4 + 108]), _mm256_castpd256_pd128(_t0_79));
    _mm_store_sd(&(C[100*fi4 + 109]), _mm256_castpd256_pd128(_t0_80));
    _mm_store_sd(&(C[100*fi4 + 110]), _mm256_castpd256_pd128(_t0_81));
    _mm_store_sd(&(C[100*fi4 + 111]), _mm256_castpd256_pd128(_t0_82));
    _mm_store_sd(&(C[100*fi4 + 208]), _mm256_castpd256_pd128(_t0_83));
    _mm_store_sd(&(C[100*fi4 + 209]), _mm256_castpd256_pd128(_t0_84));
    _mm_store_sd(&(C[100*fi4 + 210]), _mm256_castpd256_pd128(_t0_85));
    _mm_store_sd(&(C[100*fi4 + 211]), _mm256_castpd256_pd128(_t0_86));
    _mm_store_sd(&(C[100*fi4 + 308]), _mm256_castpd256_pd128(_t0_87));
    _mm_store_sd(&(C[100*fi4 + 309]), _mm256_castpd256_pd128(_t0_88));
    _mm_store_sd(&(C[100*fi4 + 310]), _mm256_castpd256_pd128(_t0_89));
    _mm_store_sd(&(C[100*fi4 + 311]), _mm256_castpd256_pd128(_t0_90));

    for( int fi25 = 12; fi25 <= 96; fi25+=4 ) {
      _t1_4 = _asm256_loadu_pd(C + fi25 + 100*fi4);
      _t1_5 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 100);
      _t1_6 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 200);
      _t1_7 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 300);
      _t1_3 = _asm256_loadu_pd(U + fi25);
      _t1_2 = _asm256_loadu_pd(U + fi25 + 100);
      _t1_1 = _asm256_loadu_pd(U + fi25 + 200);
      _t1_0 = _asm256_loadu_pd(U + fi25 + 300);

      // Generating : X[100,100] = ( S(h(4, 100, fi4), ( G(h(4, 100, fi4), X[100,100],h(4, 100, fi25)) - ( G(h(4, 100, fi4), X[100,100],h(4, 100, 0)) * G(h(4, 100, 0), U[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25)) + Sum_{k116} ( -$(h(4, 100, fi4), ( G(h(4, 100, fi4), X[100,100],h(4, 100, k116)) * G(h(4, 100, k116), U[100,100],h(4, 100, fi25)) ),h(4, 100, fi25)) ) )

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t0_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_40, _t0_40, 32), _mm256_permute2f128_pd(_t0_40, _t0_40, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_41, _t0_41, 32), _mm256_permute2f128_pd(_t0_41, _t0_41, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_42, _t0_42, 32), _mm256_permute2f128_pd(_t0_42, _t0_42, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_43, _t0_43, 32), _mm256_permute2f128_pd(_t0_43, _t0_43, 32), 0), _t1_0)));
      _t0_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_47, _t0_47, 32), _mm256_permute2f128_pd(_t0_47, _t0_47, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_48, _t0_48, 32), _mm256_permute2f128_pd(_t0_48, _t0_48, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_49, _t0_49, 32), _mm256_permute2f128_pd(_t0_49, _t0_49, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_50, _t0_50, 32), _mm256_permute2f128_pd(_t0_50, _t0_50, 32), 0), _t1_0)));
      _t0_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_51, _t0_51, 32), _mm256_permute2f128_pd(_t0_51, _t0_51, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_52, _t0_52, 32), _mm256_permute2f128_pd(_t0_52, _t0_52, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_53, _t0_53, 32), _mm256_permute2f128_pd(_t0_53, _t0_53, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_54, _t0_54, 32), _mm256_permute2f128_pd(_t0_54, _t0_54, 32), 0), _t1_0)));
      _t0_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_55, _t0_55, 32), _mm256_permute2f128_pd(_t0_55, _t0_55, 32), 0), _t1_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_56, _t0_56, 32), _mm256_permute2f128_pd(_t0_56, _t0_56, 32), 0), _t1_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_57, _t0_57, 32), _mm256_permute2f128_pd(_t0_57, _t0_57, 32), 0), _t1_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t0_58, _t0_58, 32), _mm256_permute2f128_pd(_t0_58, _t0_58, 32), 0), _t1_0)));

      // 4-BLAC: 4x4 - 4x4
      _t1_4 = _mm256_sub_pd(_t1_4, _t0_94);
      _t1_5 = _mm256_sub_pd(_t1_5, _t0_95);
      _t1_6 = _mm256_sub_pd(_t1_6, _t0_96);
      _t1_7 = _mm256_sub_pd(_t1_7, _t0_97);

      // AVX Storer:
      _asm256_storeu_pd(C + fi25 + 100*fi4, _t1_4);
      _asm256_storeu_pd(C + fi25 + 100*fi4 + 100, _t1_5);
      _asm256_storeu_pd(C + fi25 + 100*fi4 + 200, _t1_6);
      _asm256_storeu_pd(C + fi25 + 100*fi4 + 300, _t1_7);

      for( int k116 = 4; k116 <= fi25 - 1; k116+=4 ) {
        _t2_19 = _mm256_broadcast_sd(C + 100*fi4 + k116);
        _t2_18 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 1);
        _t2_17 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 2);
        _t2_16 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 3);
        _t2_15 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 100);
        _t2_14 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 101);
        _t2_13 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 102);
        _t2_12 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 103);
        _t2_11 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 200);
        _t2_10 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 201);
        _t2_9 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 202);
        _t2_8 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 203);
        _t2_7 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 300);
        _t2_6 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 301);
        _t2_5 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 302);
        _t2_4 = _mm256_broadcast_sd(C + 100*fi4 + k116 + 303);
        _t2_3 = _asm256_loadu_pd(U + fi25 + 100*k116);
        _t2_2 = _asm256_loadu_pd(U + fi25 + 100*k116 + 100);
        _t2_1 = _asm256_loadu_pd(U + fi25 + 100*k116 + 200);
        _t2_0 = _asm256_loadu_pd(U + fi25 + 100*k116 + 300);
        _t2_20 = _asm256_loadu_pd(C + fi25 + 100*fi4);
        _t2_21 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 100);
        _t2_22 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 200);
        _t2_23 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 300);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t0_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_19, _t2_3), _mm256_mul_pd(_t2_18, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_17, _t2_1), _mm256_mul_pd(_t2_16, _t2_0)));
        _t0_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_15, _t2_3), _mm256_mul_pd(_t2_14, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_13, _t2_1), _mm256_mul_pd(_t2_12, _t2_0)));
        _t0_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_11, _t2_3), _mm256_mul_pd(_t2_10, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_9, _t2_1), _mm256_mul_pd(_t2_8, _t2_0)));
        _t0_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t2_7, _t2_3), _mm256_mul_pd(_t2_6, _t2_2)), _mm256_add_pd(_mm256_mul_pd(_t2_5, _t2_1), _mm256_mul_pd(_t2_4, _t2_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t2_20 = _mm256_sub_pd(_t2_20, _t0_98);
        _t2_21 = _mm256_sub_pd(_t2_21, _t0_99);
        _t2_22 = _mm256_sub_pd(_t2_22, _t0_100);
        _t2_23 = _mm256_sub_pd(_t2_23, _t0_101);

        // AVX Storer:
        _asm256_storeu_pd(C + fi25 + 100*fi4, _t2_20);
        _asm256_storeu_pd(C + fi25 + 100*fi4 + 100, _t2_21);
        _asm256_storeu_pd(C + fi25 + 100*fi4 + 200, _t2_22);
        _asm256_storeu_pd(C + fi25 + 100*fi4 + 300, _t2_23);
      }
      _t1_5 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 100);
      _t1_4 = _asm256_loadu_pd(C + fi25 + 100*fi4);
      _t1_7 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 300);
      _t1_6 = _asm256_loadu_pd(C + fi25 + 100*fi4 + 200);
      _t3_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi25])));
      _t3_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi25 + 1])));
      _t3_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi25 + 101])));
      _t3_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 101*fi25 + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 101*fi25 + 102)), 0);
      _t3_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi25 + 202])));
      _t3_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 101*fi25 + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 101*fi25 + 103))), _mm256_castpd128_pd256(_mm_load_sd(U + 101*fi25 + 203)), 32);
      _t3_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi25 + 303])));

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_24 = _t0_39;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_25 = _t3_6;

      // 4-BLAC: 1x4 + 1x4
      _t0_462 = _mm256_add_pd(_t3_24, _t3_25);

      // 4-BLAC: 1x4 / 1x4
      _t3_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_23), _mm256_castpd256_pd128(_t0_462)));

      // AVX Storer:
      _t3_7 = _t3_26;

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_28 = _t3_7;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_29 = _t3_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_468 = _mm256_mul_pd(_t3_28, _t3_29);

      // 4-BLAC: 1x4 - 1x4
      _t3_30 = _mm256_sub_pd(_t3_27, _t0_468);

      // AVX Storer:
      _t3_8 = _t3_30;

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_31 = _t3_8;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_32 = _t0_39;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_33 = _t3_4;

      // 4-BLAC: 1x4 + 1x4
      _t0_473 = _mm256_add_pd(_t3_32, _t3_33);

      // 4-BLAC: 1x4 / 1x4
      _t3_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_31), _mm256_castpd256_pd128(_t0_473)));

      // AVX Storer:
      _t3_8 = _t3_34;

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_4, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t3_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t3_37 = _t3_3;

      // 4-BLAC: 1x4 * 4x1
      _t0_478 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_mul_pd(_t3_36, _t3_37), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_mul_pd(_t3_36, _t3_37), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_36, _t3_37), _mm256_mul_pd(_t3_36, _t3_37), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_38 = _mm256_sub_pd(_t3_35, _t0_478);

      // AVX Storer:
      _t3_9 = _t3_38;

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_39 = _t3_9;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_40 = _t0_39;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_41 = _t3_2;

      // 4-BLAC: 1x4 + 1x4
      _t0_484 = _mm256_add_pd(_t3_40, _t3_41);

      // 4-BLAC: 1x4 / 1x4
      _t3_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_39), _mm256_castpd256_pd128(_t0_484)));

      // AVX Storer:
      _t3_9 = _t3_42;

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_4, _t1_4, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t3_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t3_45 = _t3_1;

      // 4-BLAC: 1x4 * 4x1
      _t0_489 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_mul_pd(_t3_44, _t3_45), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_mul_pd(_t3_44, _t3_45), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_44, _t3_45), _mm256_mul_pd(_t3_44, _t3_45), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_46 = _mm256_sub_pd(_t3_43, _t0_489);

      // AVX Storer:
      _t3_10 = _t3_46;

      // Generating : X[100,100] = S(h(1, 100, fi4), ( G(h(1, 100, fi4), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4), L[100,100],h(1, 100, fi4)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_47 = _t3_10;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_48 = _t0_39;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_49 = _t3_0;

      // 4-BLAC: 1x4 + 1x4
      _t0_494 = _mm256_add_pd(_t3_48, _t3_49);

      // 4-BLAC: 1x4 / 1x4
      _t3_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_47), _mm256_castpd256_pd128(_t0_494)));

      // AVX Storer:
      _t3_10 = _t3_50;

      // Generating : X[100,100] = S(h(3, 100, fi4 + 1), ( G(h(3, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) - ( G(h(3, 100, fi4 + 1), L[100,100],h(1, 100, fi4)) * G(h(1, 100, fi4), X[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25))

      // AVX Loader:

      // 3x4 -> 4x4
      _t3_51 = _t1_5;
      _t3_52 = _t1_6;
      _t3_53 = _t1_7;
      _t3_54 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t3_55 = _t0_31;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t0_502 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_55, _t3_55, 32), _mm256_permute2f128_pd(_t3_55, _t3_55, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32));
      _t0_503 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_55, _t3_55, 32), _mm256_permute2f128_pd(_t3_55, _t3_55, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32));
      _t0_504 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_55, _t3_55, 49), _mm256_permute2f128_pd(_t3_55, _t3_55, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32));
      _t0_505 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_55, _t3_55, 49), _mm256_permute2f128_pd(_t3_55, _t3_55, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_7, _t3_8), _mm256_unpacklo_pd(_t3_9, _t3_10), 32));

      // 4-BLAC: 4x4 - 4x4
      _t3_56 = _mm256_sub_pd(_t3_51, _t0_502);
      _t3_57 = _mm256_sub_pd(_t3_52, _t0_503);
      _t3_58 = _mm256_sub_pd(_t3_53, _t0_504);
      _t3_59 = _mm256_sub_pd(_t3_54, _t0_505);

      // AVX Storer:
      _t1_5 = _t3_56;
      _t1_6 = _t3_57;
      _t1_7 = _t3_58;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_60 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_61 = _t0_30;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_62 = _t3_6;

      // 4-BLAC: 1x4 + 1x4
      _t0_514 = _mm256_add_pd(_t3_61, _t3_62);

      // 4-BLAC: 1x4 / 1x4
      _t3_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_60), _mm256_castpd256_pd128(_t0_514)));

      // AVX Storer:
      _t3_11 = _t3_63;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_64 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_65 = _t3_11;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_66 = _t3_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_520 = _mm256_mul_pd(_t3_65, _t3_66);

      // 4-BLAC: 1x4 - 1x4
      _t3_67 = _mm256_sub_pd(_t3_64, _t0_520);

      // AVX Storer:
      _t3_12 = _t3_67;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_68 = _t3_12;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_69 = _t0_30;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_70 = _t3_4;

      // 4-BLAC: 1x4 + 1x4
      _t0_525 = _mm256_add_pd(_t3_69, _t3_70);

      // 4-BLAC: 1x4 / 1x4
      _t3_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_68), _mm256_castpd256_pd128(_t0_525)));

      // AVX Storer:
      _t3_12 = _t3_71;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_72 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_5, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t3_73 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t3_74 = _t3_3;

      // 4-BLAC: 1x4 * 4x1
      _t0_530 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_mul_pd(_t3_73, _t3_74), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_mul_pd(_t3_73, _t3_74), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_73, _t3_74), _mm256_mul_pd(_t3_73, _t3_74), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_75 = _mm256_sub_pd(_t3_72, _t0_530);

      // AVX Storer:
      _t3_13 = _t3_75;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_76 = _t3_13;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_77 = _t0_30;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_78 = _t3_2;

      // 4-BLAC: 1x4 + 1x4
      _t0_536 = _mm256_add_pd(_t3_77, _t3_78);

      // 4-BLAC: 1x4 / 1x4
      _t3_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_76), _mm256_castpd256_pd128(_t0_536)));

      // AVX Storer:
      _t3_13 = _t3_79;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4 + 1), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_80 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_5, _t1_5, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t3_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t3_82 = _t3_1;

      // 4-BLAC: 1x4 * 4x1
      _t0_541 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_81, _t3_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_81, _t3_82), _mm256_mul_pd(_t3_81, _t3_82), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_81, _t3_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_81, _t3_82), _mm256_mul_pd(_t3_81, _t3_82), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_81, _t3_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_81, _t3_82), _mm256_mul_pd(_t3_81, _t3_82), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_83 = _mm256_sub_pd(_t3_80, _t0_541);

      // AVX Storer:
      _t3_14 = _t3_83;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 1), ( G(h(1, 100, fi4 + 1), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4 + 1), L[100,100],h(1, 100, fi4 + 1)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_84 = _t3_14;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_85 = _t0_30;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_86 = _t3_0;

      // 4-BLAC: 1x4 + 1x4
      _t0_547 = _mm256_add_pd(_t3_85, _t3_86);

      // 4-BLAC: 1x4 / 1x4
      _t3_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_84), _mm256_castpd256_pd128(_t0_547)));

      // AVX Storer:
      _t3_14 = _t3_87;

      // Generating : X[100,100] = S(h(2, 100, fi4 + 2), ( G(h(2, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) - ( G(h(2, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 1)) * G(h(1, 100, fi4 + 1), X[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25))

      // AVX Loader:

      // 2x4 -> 4x4
      _t3_88 = _t1_6;
      _t3_89 = _t1_7;
      _t3_90 = _mm256_setzero_pd();
      _t3_91 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t3_92 = _t0_29;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t0_554 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_92, _t3_92, 32), _mm256_permute2f128_pd(_t3_92, _t3_92, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _t3_14), 32));
      _t0_555 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_92, _t3_92, 32), _mm256_permute2f128_pd(_t3_92, _t3_92, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _t3_14), 32));
      _t0_556 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_92, _t3_92, 49), _mm256_permute2f128_pd(_t3_92, _t3_92, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _t3_14), 32));
      _t0_557 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t3_92, _t3_92, 49), _mm256_permute2f128_pd(_t3_92, _t3_92, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_11, _t3_12), _mm256_unpacklo_pd(_t3_13, _t3_14), 32));

      // 4-BLAC: 4x4 - 4x4
      _t3_93 = _mm256_sub_pd(_t3_88, _t0_554);
      _t3_94 = _mm256_sub_pd(_t3_89, _t0_555);
      _t3_95 = _mm256_sub_pd(_t3_90, _t0_556);
      _t3_96 = _mm256_sub_pd(_t3_91, _t0_557);

      // AVX Storer:
      _t1_6 = _t3_93;
      _t1_7 = _t3_94;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_97 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_98 = _t0_28;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_99 = _t3_6;

      // 4-BLAC: 1x4 + 1x4
      _t0_565 = _mm256_add_pd(_t3_98, _t3_99);

      // 4-BLAC: 1x4 / 1x4
      _t3_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_97), _mm256_castpd256_pd128(_t0_565)));

      // AVX Storer:
      _t3_15 = _t3_100;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_101 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_102 = _t3_15;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_103 = _t3_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_571 = _mm256_mul_pd(_t3_102, _t3_103);

      // 4-BLAC: 1x4 - 1x4
      _t3_104 = _mm256_sub_pd(_t3_101, _t0_571);

      // AVX Storer:
      _t3_16 = _t3_104;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_105 = _t3_16;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_106 = _t0_28;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_107 = _t3_4;

      // 4-BLAC: 1x4 + 1x4
      _t0_577 = _mm256_add_pd(_t3_106, _t3_107);

      // 4-BLAC: 1x4 / 1x4
      _t3_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_105), _mm256_castpd256_pd128(_t0_577)));

      // AVX Storer:
      _t3_16 = _t3_108;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_109 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_6, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t3_110 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_15, _t3_16), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t3_111 = _t3_3;

      // 4-BLAC: 1x4 * 4x1
      _t0_583 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_110, _t3_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_110, _t3_111), _mm256_mul_pd(_t3_110, _t3_111), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_110, _t3_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_110, _t3_111), _mm256_mul_pd(_t3_110, _t3_111), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_110, _t3_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_110, _t3_111), _mm256_mul_pd(_t3_110, _t3_111), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_112 = _mm256_sub_pd(_t3_109, _t0_583);

      // AVX Storer:
      _t3_17 = _t3_112;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_113 = _t3_17;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_114 = _t0_28;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_115 = _t3_2;

      // 4-BLAC: 1x4 + 1x4
      _t0_588 = _mm256_add_pd(_t3_114, _t3_115);

      // 4-BLAC: 1x4 / 1x4
      _t3_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_113), _mm256_castpd256_pd128(_t0_588)));

      // AVX Storer:
      _t3_17 = _t3_116;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4 + 2), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_117 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_6, _t1_6, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t3_118 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_16), _mm256_unpacklo_pd(_t3_17, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t3_119 = _t3_1;

      // 4-BLAC: 1x4 * 4x1
      _t0_593 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_118, _t3_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_118, _t3_119), _mm256_mul_pd(_t3_118, _t3_119), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_118, _t3_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_118, _t3_119), _mm256_mul_pd(_t3_118, _t3_119), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_118, _t3_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_118, _t3_119), _mm256_mul_pd(_t3_118, _t3_119), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_120 = _mm256_sub_pd(_t3_117, _t0_593);

      // AVX Storer:
      _t3_18 = _t3_120;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 2), ( G(h(1, 100, fi4 + 2), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4 + 2), L[100,100],h(1, 100, fi4 + 2)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_121 = _t3_18;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_122 = _t0_28;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_123 = _t3_0;

      // 4-BLAC: 1x4 + 1x4
      _t0_599 = _mm256_add_pd(_t3_122, _t3_123);

      // 4-BLAC: 1x4 / 1x4
      _t3_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_121), _mm256_castpd256_pd128(_t0_599)));

      // AVX Storer:
      _t3_18 = _t3_124;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(4, 100, fi25)) - ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 2)) Kro G(h(1, 100, fi4 + 2), X[100,100],h(4, 100, fi25)) ) ),h(4, 100, fi25))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_125 = _t0_27;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t0_92 = _mm256_mul_pd(_t3_125, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_15, _t3_16), _mm256_unpacklo_pd(_t3_17, _t3_18), 32));

      // 4-BLAC: 1x4 - 1x4
      _t1_7 = _mm256_sub_pd(_t1_7, _t0_92);

      // AVX Storer:

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25), U[100,100],h(1, 100, fi25)) ) ),h(1, 100, fi25))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_127 = _t0_26;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_128 = _t3_6;

      // 4-BLAC: 1x4 + 1x4
      _t0_605 = _mm256_add_pd(_t3_127, _t3_128);

      // 4-BLAC: 1x4 / 1x4
      _t3_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_126), _mm256_castpd256_pd128(_t0_605)));

      // AVX Storer:
      _t3_19 = _t3_129;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25)) Kro G(h(1, 100, fi25), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_130 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 2), _mm256_setzero_pd());

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_131 = _t3_19;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_132 = _t3_5;

      // 4-BLAC: 1x4 Kro 1x4
      _t0_611 = _mm256_mul_pd(_t3_131, _t3_132);

      // 4-BLAC: 1x4 - 1x4
      _t3_133 = _mm256_sub_pd(_t3_130, _t0_611);

      // AVX Storer:
      _t3_20 = _t3_133;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 1)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25 + 1), U[100,100],h(1, 100, fi25 + 1)) ) ),h(1, 100, fi25 + 1))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_134 = _t3_20;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_135 = _t0_26;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_136 = _t3_4;

      // 4-BLAC: 1x4 + 1x4
      _t0_115 = _mm256_add_pd(_t3_135, _t3_136);

      // 4-BLAC: 1x4 / 1x4
      _t3_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_134), _mm256_castpd256_pd128(_t0_115)));

      // AVX Storer:
      _t3_20 = _t3_137;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(2, 100, fi25)) * G(h(2, 100, fi25), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t1_7, 4), 129);

      // AVX Loader:

      // 1x2 -> 1x4
      _t3_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t3_19, _t3_20), _mm256_setzero_pd(), 12);

      // AVX Loader:

      // 2x1 -> 4x1
      _t3_140 = _t3_3;

      // 4-BLAC: 1x4 * 4x1
      _t0_120 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_mul_pd(_t3_139, _t3_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_mul_pd(_t3_139, _t3_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_139, _t3_140), _mm256_mul_pd(_t3_139, _t3_140), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_141 = _mm256_sub_pd(_t3_138, _t0_120);

      // AVX Storer:
      _t3_21 = _t3_141;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 2)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25 + 2), U[100,100],h(1, 100, fi25 + 2)) ) ),h(1, 100, fi25 + 2))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_142 = _t3_21;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_143 = _t0_26;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_144 = _t3_2;

      // 4-BLAC: 1x4 + 1x4
      _t0_126 = _mm256_add_pd(_t3_143, _t3_144);

      // 4-BLAC: 1x4 / 1x4
      _t3_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_142), _mm256_castpd256_pd128(_t0_126)));

      // AVX Storer:
      _t3_21 = _t3_145;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) - ( G(h(1, 100, fi4 + 3), X[100,100],h(3, 100, fi25)) * G(h(3, 100, fi25), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_146 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t1_7, _t1_7, 129), _mm256_setzero_pd());

      // AVX Loader:

      // 1x3 -> 1x4
      _t3_147 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t3_19, _t3_20), _mm256_unpacklo_pd(_t3_21, _mm256_setzero_pd()), 32);

      // AVX Loader:

      // 3x1 -> 4x1
      _t3_148 = _t3_1;

      // 4-BLAC: 1x4 * 4x1
      _t0_132 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t3_147, _t3_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_147, _t3_148), _mm256_mul_pd(_t3_147, _t3_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t3_147, _t3_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_147, _t3_148), _mm256_mul_pd(_t3_147, _t3_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t3_147, _t3_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t3_147, _t3_148), _mm256_mul_pd(_t3_147, _t3_148), 129)), 1));

      // 4-BLAC: 1x4 - 1x4
      _t3_149 = _mm256_sub_pd(_t3_146, _t0_132);

      // AVX Storer:
      _t3_22 = _t3_149;

      // Generating : X[100,100] = S(h(1, 100, fi4 + 3), ( G(h(1, 100, fi4 + 3), X[100,100],h(1, 100, fi25 + 3)) Div ( G(h(1, 100, fi4 + 3), L[100,100],h(1, 100, fi4 + 3)) + G(h(1, 100, fi25 + 3), U[100,100],h(1, 100, fi25 + 3)) ) ),h(1, 100, fi25 + 3))

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_150 = _t3_22;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_151 = _t0_26;

      // AVX Loader:

      // 1x1 -> 1x4
      _t3_152 = _t3_0;

      // 4-BLAC: 1x4 + 1x4
      _t0_138 = _mm256_add_pd(_t3_151, _t3_152);

      // 4-BLAC: 1x4 / 1x4
      _t3_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t3_150), _mm256_castpd256_pd128(_t0_138)));

      // AVX Storer:
      _t3_22 = _t3_153;
      _mm_store_sd(&(C[fi25 + 100*fi4]), _mm256_castpd256_pd128(_t3_7));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 1]), _mm256_castpd256_pd128(_t3_8));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 2]), _mm256_castpd256_pd128(_t3_9));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 3]), _mm256_castpd256_pd128(_t3_10));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 100]), _mm256_castpd256_pd128(_t3_11));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 101]), _mm256_castpd256_pd128(_t3_12));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 102]), _mm256_castpd256_pd128(_t3_13));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 103]), _mm256_castpd256_pd128(_t3_14));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 200]), _mm256_castpd256_pd128(_t3_15));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 201]), _mm256_castpd256_pd128(_t3_16));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 202]), _mm256_castpd256_pd128(_t3_17));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 203]), _mm256_castpd256_pd128(_t3_18));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 300]), _mm256_castpd256_pd128(_t3_19));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 301]), _mm256_castpd256_pd128(_t3_20));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 302]), _mm256_castpd256_pd128(_t3_21));
      _mm_store_sd(&(C[fi25 + 100*fi4 + 303]), _mm256_castpd256_pd128(_t3_22));
    }

    // Generating : X[100,100] = Sum_{k116} ( Sum_{i231} ( S(h(4, 100, fi4 + k116 + 4), ( G(h(4, 100, fi4 + k116 + 4), X[100,100],h(4, 100, i231)) - ( G(h(4, 100, fi4 + k116 + 4), L[100,100],h(4, 100, fi4)) * G(h(4, 100, fi4), X[100,100],h(4, 100, i231)) ) ),h(4, 100, i231)) ) )
    _mm_store_sd(&(C[100*fi4]), _mm256_castpd256_pd128(_t0_40));
    _mm_store_sd(&(C[100*fi4 + 1]), _mm256_castpd256_pd128(_t0_41));
    _mm_store_sd(&(C[100*fi4 + 2]), _mm256_castpd256_pd128(_t0_42));
    _mm_store_sd(&(C[100*fi4 + 3]), _mm256_castpd256_pd128(_t0_43));
    _mm_store_sd(&(C[100*fi4 + 100]), _mm256_castpd256_pd128(_t0_47));
    _mm_store_sd(&(C[100*fi4 + 101]), _mm256_castpd256_pd128(_t0_48));
    _mm_store_sd(&(C[100*fi4 + 102]), _mm256_castpd256_pd128(_t0_49));
    _mm_store_sd(&(C[100*fi4 + 103]), _mm256_castpd256_pd128(_t0_50));
    _mm_store_sd(&(C[100*fi4 + 200]), _mm256_castpd256_pd128(_t0_51));
    _mm_store_sd(&(C[100*fi4 + 201]), _mm256_castpd256_pd128(_t0_52));
    _mm_store_sd(&(C[100*fi4 + 202]), _mm256_castpd256_pd128(_t0_53));
    _mm_store_sd(&(C[100*fi4 + 203]), _mm256_castpd256_pd128(_t0_54));
    _mm_store_sd(&(C[100*fi4 + 300]), _mm256_castpd256_pd128(_t0_55));
    _mm_store_sd(&(C[100*fi4 + 301]), _mm256_castpd256_pd128(_t0_56));
    _mm_store_sd(&(C[100*fi4 + 302]), _mm256_castpd256_pd128(_t0_57));
    _mm_store_sd(&(C[100*fi4 + 303]), _mm256_castpd256_pd128(_t0_58));

    for( int k116 = 0; k116 <= -fi4 + 95; k116+=4 ) {

      for( int i231 = 0; i231 <= 99; i231+=4 ) {
        _t4_24 = _asm256_loadu_pd(C + 100*fi4 + i231 + 100*k116 + 400);
        _t4_25 = _asm256_loadu_pd(C + 100*fi4 + i231 + 100*k116 + 500);
        _t4_26 = _asm256_loadu_pd(C + 100*fi4 + i231 + 100*k116 + 600);
        _t4_27 = _asm256_loadu_pd(C + 100*fi4 + i231 + 100*k116 + 700);
        _t4_19 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 400);
        _t4_18 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 401);
        _t4_17 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 402);
        _t4_16 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 403);
        _t4_15 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 500);
        _t4_14 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 501);
        _t4_13 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 502);
        _t4_12 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 503);
        _t4_11 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 600);
        _t4_10 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 601);
        _t4_9 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 602);
        _t4_8 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 603);
        _t4_7 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 700);
        _t4_6 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 701);
        _t4_5 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 702);
        _t4_4 = _mm256_broadcast_sd(L + 101*fi4 + 100*k116 + 703);
        _t4_3 = _asm256_loadu_pd(C + 100*fi4 + i231);
        _t4_2 = _asm256_loadu_pd(C + 100*fi4 + i231 + 100);
        _t4_1 = _asm256_loadu_pd(C + 100*fi4 + i231 + 200);
        _t4_0 = _asm256_loadu_pd(C + 100*fi4 + i231 + 300);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t4_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_19, _t4_3), _mm256_mul_pd(_t4_18, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_17, _t4_1), _mm256_mul_pd(_t4_16, _t4_0)));
        _t4_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t4_3), _mm256_mul_pd(_t4_14, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t4_1), _mm256_mul_pd(_t4_12, _t4_0)));
        _t4_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t4_3), _mm256_mul_pd(_t4_10, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t4_1), _mm256_mul_pd(_t4_8, _t4_0)));
        _t4_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t4_3), _mm256_mul_pd(_t4_6, _t4_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t4_1), _mm256_mul_pd(_t4_4, _t4_0)));

        // 4-BLAC: 4x4 - 4x4
        _t4_24 = _mm256_sub_pd(_t4_24, _t4_20);
        _t4_25 = _mm256_sub_pd(_t4_25, _t4_21);
        _t4_26 = _mm256_sub_pd(_t4_26, _t4_22);
        _t4_27 = _mm256_sub_pd(_t4_27, _t4_23);

        // AVX Storer:
        _asm256_storeu_pd(C + 100*fi4 + i231 + 100*k116 + 400, _t4_24);
        _asm256_storeu_pd(C + 100*fi4 + i231 + 100*k116 + 500, _t4_25);
        _asm256_storeu_pd(C + 100*fi4 + i231 + 100*k116 + 600, _t4_26);
        _asm256_storeu_pd(C + 100*fi4 + i231 + 100*k116 + 700, _t4_27);
      }
    }
  }

  _t5_40 = _mm256_castpd128_pd256(_mm_load_sd(&(C[9600])));
  _t5_39 = _mm256_castpd128_pd256(_mm_load_sd(&(L[9696])));
  _t5_38 = _mm256_castpd128_pd256(_mm_load_sd(&(U[0])));
  _t5_41 = _mm256_castpd128_pd256(_mm_load_sd(&(C[9601])));
  _t5_37 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1])));
  _t5_36 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101])));
  _t5_42 = _mm256_castpd128_pd256(_mm_load_sd(&(C[9602])));
  _t5_35 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 102)), 0);
  _t5_34 = _mm256_castpd128_pd256(_mm_load_sd(&(U[202])));
  _t5_43 = _mm256_castpd128_pd256(_mm_load_sd(&(C[9603])));
  _t5_33 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 103))), _mm256_castpd128_pd256(_mm_load_sd(U + 203)), 32);
  _t5_32 = _mm256_castpd128_pd256(_mm_load_sd(&(U[303])));
  _t5_44 = _asm256_loadu_pd(C + 9700);
  _t5_45 = _asm256_loadu_pd(C + 9800);
  _t5_46 = _asm256_loadu_pd(C + 9900);
  _t5_31 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 9796)), _mm256_castpd128_pd256(_mm_load_sd(L + 9896))), _mm256_castpd128_pd256(_mm_load_sd(L + 9996)), 32);
  _t5_30 = _mm256_castpd128_pd256(_mm_load_sd(&(L[9797])));
  _t5_29 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(L + 9897)), _mm256_castpd128_pd256(_mm_load_sd(L + 9997)), 0);
  _t5_28 = _mm256_castpd128_pd256(_mm_load_sd(&(L[9898])));
  _t5_27 = _mm256_broadcast_sd(&(L[9998]));
  _t5_26 = _mm256_castpd128_pd256(_mm_load_sd(&(L[9999])));
  _t5_106 = _asm256_loadu_pd(C + 9604);
  _t5_107 = _asm256_loadu_pd(C + 9704);
  _t5_108 = _asm256_loadu_pd(C + 9804);
  _t5_109 = _asm256_loadu_pd(C + 9904);
  _t5_25 = _asm256_loadu_pd(U + 4);
  _t5_24 = _asm256_loadu_pd(U + 104);
  _t5_23 = _asm256_loadu_pd(U + 204);
  _t5_22 = _asm256_loadu_pd(U + 304);
  _t5_21 = _mm256_castpd128_pd256(_mm_load_sd(&(U[404])));
  _t5_20 = _mm256_castpd128_pd256(_mm_load_sd(&(U[405])));
  _t5_19 = _mm256_castpd128_pd256(_mm_load_sd(&(U[505])));
  _t5_18 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 406)), _mm256_castpd128_pd256(_mm_load_sd(U + 506)), 0);
  _t5_17 = _mm256_castpd128_pd256(_mm_load_sd(&(U[606])));
  _t5_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 407)), _mm256_castpd128_pd256(_mm_load_sd(U + 507))), _mm256_castpd128_pd256(_mm_load_sd(U + 607)), 32);
  _t5_15 = _mm256_castpd128_pd256(_mm_load_sd(&(U[707])));
  _t5_110 = _asm256_loadu_pd(C + 9608);
  _t5_111 = _asm256_loadu_pd(C + 9708);
  _t5_112 = _asm256_loadu_pd(C + 9808);
  _t5_113 = _asm256_loadu_pd(C + 9908);
  _t5_14 = _asm256_loadu_pd(U + 8);
  _t5_13 = _asm256_loadu_pd(U + 108);
  _t5_12 = _asm256_loadu_pd(U + 208);
  _t5_11 = _asm256_loadu_pd(U + 308);
  _t5_10 = _asm256_loadu_pd(U + 408);
  _t5_9 = _asm256_loadu_pd(U + 508);
  _t5_8 = _asm256_loadu_pd(U + 608);
  _t5_7 = _asm256_loadu_pd(U + 708);
  _t5_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[808])));
  _t5_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[809])));
  _t5_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[909])));
  _t5_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 810)), _mm256_castpd128_pd256(_mm_load_sd(U + 910)), 0);
  _t5_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1010])));
  _t5_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 811)), _mm256_castpd128_pd256(_mm_load_sd(U + 911))), _mm256_castpd128_pd256(_mm_load_sd(U + 1011)), 32);
  _t5_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[1111])));

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_114 = _t5_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_115 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_116 = _t5_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_117 = _mm256_add_pd(_t5_115, _t5_116);

  // 4-BLAC: 1x4 / 1x4
  _t5_118 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_114), _mm256_castpd256_pd128(_t5_117)));

  // AVX Storer:
  _t5_40 = _t5_118;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, 96), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_119 = _t5_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_120 = _t5_40;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_121 = _t5_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_122 = _mm256_mul_pd(_t5_120, _t5_121);

  // 4-BLAC: 1x4 - 1x4
  _t5_123 = _mm256_sub_pd(_t5_119, _t5_122);

  // AVX Storer:
  _t5_41 = _t5_123;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_124 = _t5_41;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_125 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_126 = _t5_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_127 = _mm256_add_pd(_t5_125, _t5_126);

  // 4-BLAC: 1x4 / 1x4
  _t5_128 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_124), _mm256_castpd256_pd128(_t5_127)));

  // AVX Storer:
  _t5_41 = _t5_128;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, 96), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_129 = _t5_42;

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_130 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_40, _t5_41), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_131 = _t5_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_132 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_130, _t5_131), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_130, _t5_131), _mm256_mul_pd(_t5_130, _t5_131), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_130, _t5_131), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_130, _t5_131), _mm256_mul_pd(_t5_130, _t5_131), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_130, _t5_131), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_130, _t5_131), _mm256_mul_pd(_t5_130, _t5_131), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_133 = _mm256_sub_pd(_t5_129, _t5_132);

  // AVX Storer:
  _t5_42 = _t5_133;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_134 = _t5_42;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_135 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_136 = _t5_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_137 = _mm256_add_pd(_t5_135, _t5_136);

  // 4-BLAC: 1x4 / 1x4
  _t5_138 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_134), _mm256_castpd256_pd128(_t5_137)));

  // AVX Storer:
  _t5_42 = _t5_138;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, 96), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_139 = _t5_43;

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_140 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_40, _t5_41), _mm256_unpacklo_pd(_t5_42, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_141 = _t5_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_142 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_140, _t5_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_140, _t5_141), _mm256_mul_pd(_t5_140, _t5_141), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_140, _t5_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_140, _t5_141), _mm256_mul_pd(_t5_140, _t5_141), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_140, _t5_141), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_140, _t5_141), _mm256_mul_pd(_t5_140, _t5_141), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_143 = _mm256_sub_pd(_t5_139, _t5_142);

  // AVX Storer:
  _t5_43 = _t5_143;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_144 = _t5_43;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_145 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_146 = _t5_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_147 = _mm256_add_pd(_t5_145, _t5_146);

  // 4-BLAC: 1x4 / 1x4
  _t5_148 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_144), _mm256_castpd256_pd128(_t5_147)));

  // AVX Storer:
  _t5_43 = _t5_148;

  // Generating : X[100,100] = S(h(3, 100, 97), ( G(h(3, 100, 97), X[100,100],h(4, 100, 0)) - ( G(h(3, 100, 97), L[100,100],h(1, 100, 96)) * G(h(1, 100, 96), X[100,100],h(4, 100, 0)) ) ),h(4, 100, 0))

  // AVX Loader:

  // 3x4 -> 4x4
  _t5_149 = _t5_44;
  _t5_150 = _t5_45;
  _t5_151 = _t5_46;
  _t5_152 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_153 = _t5_31;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t5_154 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_153, _t5_153, 32), _mm256_permute2f128_pd(_t5_153, _t5_153, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_40, _t5_41), _mm256_unpacklo_pd(_t5_42, _t5_43), 32));
  _t5_155 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_153, _t5_153, 32), _mm256_permute2f128_pd(_t5_153, _t5_153, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_40, _t5_41), _mm256_unpacklo_pd(_t5_42, _t5_43), 32));
  _t5_156 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_153, _t5_153, 49), _mm256_permute2f128_pd(_t5_153, _t5_153, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_40, _t5_41), _mm256_unpacklo_pd(_t5_42, _t5_43), 32));
  _t5_157 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_153, _t5_153, 49), _mm256_permute2f128_pd(_t5_153, _t5_153, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_40, _t5_41), _mm256_unpacklo_pd(_t5_42, _t5_43), 32));

  // 4-BLAC: 4x4 - 4x4
  _t5_158 = _mm256_sub_pd(_t5_149, _t5_154);
  _t5_159 = _mm256_sub_pd(_t5_150, _t5_155);
  _t5_160 = _mm256_sub_pd(_t5_151, _t5_156);
  _t5_161 = _mm256_sub_pd(_t5_152, _t5_157);

  // AVX Storer:
  _t5_44 = _t5_158;
  _t5_45 = _t5_159;
  _t5_46 = _t5_160;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_162 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_44, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_163 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_164 = _t5_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_165 = _mm256_add_pd(_t5_163, _t5_164);

  // 4-BLAC: 1x4 / 1x4
  _t5_166 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_162), _mm256_castpd256_pd128(_t5_165)));

  // AVX Storer:
  _t5_47 = _t5_166;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, 97), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_167 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_44, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_168 = _t5_47;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_169 = _t5_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_170 = _mm256_mul_pd(_t5_168, _t5_169);

  // 4-BLAC: 1x4 - 1x4
  _t5_171 = _mm256_sub_pd(_t5_167, _t5_170);

  // AVX Storer:
  _t5_48 = _t5_171;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_172 = _t5_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_173 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_174 = _t5_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_175 = _mm256_add_pd(_t5_173, _t5_174);

  // 4-BLAC: 1x4 / 1x4
  _t5_176 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_172), _mm256_castpd256_pd128(_t5_175)));

  // AVX Storer:
  _t5_48 = _t5_176;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, 97), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_177 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_44, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_44, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_178 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_179 = _t5_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_180 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_178, _t5_179), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_178, _t5_179), _mm256_mul_pd(_t5_178, _t5_179), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_178, _t5_179), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_178, _t5_179), _mm256_mul_pd(_t5_178, _t5_179), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_178, _t5_179), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_178, _t5_179), _mm256_mul_pd(_t5_178, _t5_179), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_181 = _mm256_sub_pd(_t5_177, _t5_180);

  // AVX Storer:
  _t5_49 = _t5_181;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_182 = _t5_49;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_183 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_184 = _t5_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_185 = _mm256_add_pd(_t5_183, _t5_184);

  // 4-BLAC: 1x4 / 1x4
  _t5_186 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_182), _mm256_castpd256_pd128(_t5_185)));

  // AVX Storer:
  _t5_49 = _t5_186;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, 97), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_187 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_44, _t5_44, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_188 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_189 = _t5_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_190 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_188, _t5_189), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_188, _t5_189), _mm256_mul_pd(_t5_188, _t5_189), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_188, _t5_189), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_188, _t5_189), _mm256_mul_pd(_t5_188, _t5_189), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_188, _t5_189), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_188, _t5_189), _mm256_mul_pd(_t5_188, _t5_189), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_191 = _mm256_sub_pd(_t5_187, _t5_190);

  // AVX Storer:
  _t5_50 = _t5_191;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_192 = _t5_50;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_193 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_194 = _t5_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_195 = _mm256_add_pd(_t5_193, _t5_194);

  // 4-BLAC: 1x4 / 1x4
  _t5_196 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_192), _mm256_castpd256_pd128(_t5_195)));

  // AVX Storer:
  _t5_50 = _t5_196;

  // Generating : X[100,100] = S(h(2, 100, 98), ( G(h(2, 100, 98), X[100,100],h(4, 100, 0)) - ( G(h(2, 100, 98), L[100,100],h(1, 100, 97)) * G(h(1, 100, 97), X[100,100],h(4, 100, 0)) ) ),h(4, 100, 0))

  // AVX Loader:

  // 2x4 -> 4x4
  _t5_197 = _t5_45;
  _t5_198 = _t5_46;
  _t5_199 = _mm256_setzero_pd();
  _t5_200 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_201 = _t5_29;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t5_202 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_201, _t5_201, 32), _mm256_permute2f128_pd(_t5_201, _t5_201, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _t5_50), 32));
  _t5_203 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_201, _t5_201, 32), _mm256_permute2f128_pd(_t5_201, _t5_201, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _t5_50), 32));
  _t5_204 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_201, _t5_201, 49), _mm256_permute2f128_pd(_t5_201, _t5_201, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _t5_50), 32));
  _t5_205 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_201, _t5_201, 49), _mm256_permute2f128_pd(_t5_201, _t5_201, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_47, _t5_48), _mm256_unpacklo_pd(_t5_49, _t5_50), 32));

  // 4-BLAC: 4x4 - 4x4
  _t5_206 = _mm256_sub_pd(_t5_197, _t5_202);
  _t5_207 = _mm256_sub_pd(_t5_198, _t5_203);
  _t5_208 = _mm256_sub_pd(_t5_199, _t5_204);
  _t5_209 = _mm256_sub_pd(_t5_200, _t5_205);

  // AVX Storer:
  _t5_45 = _t5_206;
  _t5_46 = _t5_207;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_210 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_45, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_211 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_212 = _t5_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_213 = _mm256_add_pd(_t5_211, _t5_212);

  // 4-BLAC: 1x4 / 1x4
  _t5_214 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_210), _mm256_castpd256_pd128(_t5_213)));

  // AVX Storer:
  _t5_51 = _t5_214;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, 98), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_215 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_45, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_216 = _t5_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_217 = _t5_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_218 = _mm256_mul_pd(_t5_216, _t5_217);

  // 4-BLAC: 1x4 - 1x4
  _t5_219 = _mm256_sub_pd(_t5_215, _t5_218);

  // AVX Storer:
  _t5_52 = _t5_219;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_220 = _t5_52;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_221 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_222 = _t5_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_223 = _mm256_add_pd(_t5_221, _t5_222);

  // 4-BLAC: 1x4 / 1x4
  _t5_224 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_220), _mm256_castpd256_pd128(_t5_223)));

  // AVX Storer:
  _t5_52 = _t5_224;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, 98), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_225 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_45, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_45, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_226 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_51, _t5_52), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_227 = _t5_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_228 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_226, _t5_227), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_226, _t5_227), _mm256_mul_pd(_t5_226, _t5_227), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_226, _t5_227), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_226, _t5_227), _mm256_mul_pd(_t5_226, _t5_227), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_226, _t5_227), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_226, _t5_227), _mm256_mul_pd(_t5_226, _t5_227), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_229 = _mm256_sub_pd(_t5_225, _t5_228);

  // AVX Storer:
  _t5_53 = _t5_229;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_230 = _t5_53;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_231 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_232 = _t5_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_233 = _mm256_add_pd(_t5_231, _t5_232);

  // 4-BLAC: 1x4 / 1x4
  _t5_234 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_230), _mm256_castpd256_pd128(_t5_233)));

  // AVX Storer:
  _t5_53 = _t5_234;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, 98), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_235 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_45, _t5_45, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_236 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_51, _t5_52), _mm256_unpacklo_pd(_t5_53, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_237 = _t5_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_238 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_236, _t5_237), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_236, _t5_237), _mm256_mul_pd(_t5_236, _t5_237), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_236, _t5_237), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_236, _t5_237), _mm256_mul_pd(_t5_236, _t5_237), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_236, _t5_237), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_236, _t5_237), _mm256_mul_pd(_t5_236, _t5_237), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_239 = _mm256_sub_pd(_t5_235, _t5_238);

  // AVX Storer:
  _t5_54 = _t5_239;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_240 = _t5_54;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_241 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_242 = _t5_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_243 = _mm256_add_pd(_t5_241, _t5_242);

  // 4-BLAC: 1x4 / 1x4
  _t5_244 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_240), _mm256_castpd256_pd128(_t5_243)));

  // AVX Storer:
  _t5_54 = _t5_244;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(4, 100, 0)) - ( G(h(1, 100, 99), L[100,100],h(1, 100, 98)) Kro G(h(1, 100, 98), X[100,100],h(4, 100, 0)) ) ),h(4, 100, 0))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_245 = _t5_27;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t5_91 = _mm256_mul_pd(_t5_245, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_51, _t5_52), _mm256_unpacklo_pd(_t5_53, _t5_54), 32));

  // 4-BLAC: 1x4 - 1x4
  _t5_46 = _mm256_sub_pd(_t5_46, _t5_91);

  // AVX Storer:

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 0)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 0), U[100,100],h(1, 100, 0)) ) ),h(1, 100, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_246 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_46, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_247 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_248 = _t5_38;

  // 4-BLAC: 1x4 + 1x4
  _t5_249 = _mm256_add_pd(_t5_247, _t5_248);

  // 4-BLAC: 1x4 / 1x4
  _t5_250 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_246), _mm256_castpd256_pd128(_t5_249)));

  // AVX Storer:
  _t5_55 = _t5_250;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 1)) - ( G(h(1, 100, 99), X[100,100],h(1, 100, 0)) Kro G(h(1, 100, 0), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_251 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_46, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_252 = _t5_55;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_253 = _t5_37;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_254 = _mm256_mul_pd(_t5_252, _t5_253);

  // 4-BLAC: 1x4 - 1x4
  _t5_255 = _mm256_sub_pd(_t5_251, _t5_254);

  // AVX Storer:
  _t5_56 = _t5_255;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 1)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 1), U[100,100],h(1, 100, 1)) ) ),h(1, 100, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_256 = _t5_56;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_257 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_258 = _t5_36;

  // 4-BLAC: 1x4 + 1x4
  _t5_259 = _mm256_add_pd(_t5_257, _t5_258);

  // 4-BLAC: 1x4 / 1x4
  _t5_260 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_256), _mm256_castpd256_pd128(_t5_259)));

  // AVX Storer:
  _t5_56 = _t5_260;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 2)) - ( G(h(1, 100, 99), X[100,100],h(2, 100, 0)) * G(h(2, 100, 0), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_261 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_46, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_46, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_262 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_55, _t5_56), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_263 = _t5_35;

  // 4-BLAC: 1x4 * 4x1
  _t5_264 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_262, _t5_263), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_262, _t5_263), _mm256_mul_pd(_t5_262, _t5_263), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_262, _t5_263), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_262, _t5_263), _mm256_mul_pd(_t5_262, _t5_263), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_262, _t5_263), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_262, _t5_263), _mm256_mul_pd(_t5_262, _t5_263), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_265 = _mm256_sub_pd(_t5_261, _t5_264);

  // AVX Storer:
  _t5_57 = _t5_265;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 2)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 2), U[100,100],h(1, 100, 2)) ) ),h(1, 100, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_266 = _t5_57;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_267 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_268 = _t5_34;

  // 4-BLAC: 1x4 + 1x4
  _t5_269 = _mm256_add_pd(_t5_267, _t5_268);

  // 4-BLAC: 1x4 / 1x4
  _t5_270 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_266), _mm256_castpd256_pd128(_t5_269)));

  // AVX Storer:
  _t5_57 = _t5_270;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 3)) - ( G(h(1, 100, 99), X[100,100],h(3, 100, 0)) * G(h(3, 100, 0), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_271 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_46, _t5_46, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_272 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_55, _t5_56), _mm256_unpacklo_pd(_t5_57, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_273 = _t5_33;

  // 4-BLAC: 1x4 * 4x1
  _t5_274 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_272, _t5_273), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_272, _t5_273), _mm256_mul_pd(_t5_272, _t5_273), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_272, _t5_273), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_272, _t5_273), _mm256_mul_pd(_t5_272, _t5_273), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_272, _t5_273), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_272, _t5_273), _mm256_mul_pd(_t5_272, _t5_273), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_275 = _mm256_sub_pd(_t5_271, _t5_274);

  // AVX Storer:
  _t5_58 = _t5_275;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 3)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 3), U[100,100],h(1, 100, 3)) ) ),h(1, 100, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_276 = _t5_58;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_277 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_278 = _t5_32;

  // 4-BLAC: 1x4 + 1x4
  _t5_279 = _mm256_add_pd(_t5_277, _t5_278);

  // 4-BLAC: 1x4 / 1x4
  _t5_280 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_276), _mm256_castpd256_pd128(_t5_279)));

  // AVX Storer:
  _t5_58 = _t5_280;

  // Generating : X[100,100] = S(h(4, 100, 96), ( G(h(4, 100, 96), X[100,100],h(4, 100, 4)) - ( G(h(4, 100, 96), X[100,100],h(4, 100, 0)) * G(h(4, 100, 0), U[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t5_94 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_40, _t5_40, 32), _mm256_permute2f128_pd(_t5_40, _t5_40, 32), 0), _t5_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_41, _t5_41, 32), _mm256_permute2f128_pd(_t5_41, _t5_41, 32), 0), _t5_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_42, _t5_42, 32), _mm256_permute2f128_pd(_t5_42, _t5_42, 32), 0), _t5_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_43, _t5_43, 32), _mm256_permute2f128_pd(_t5_43, _t5_43, 32), 0), _t5_22)));
  _t5_95 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_47, _t5_47, 32), _mm256_permute2f128_pd(_t5_47, _t5_47, 32), 0), _t5_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_48, _t5_48, 32), _mm256_permute2f128_pd(_t5_48, _t5_48, 32), 0), _t5_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_49, _t5_49, 32), _mm256_permute2f128_pd(_t5_49, _t5_49, 32), 0), _t5_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_50, _t5_50, 32), _mm256_permute2f128_pd(_t5_50, _t5_50, 32), 0), _t5_22)));
  _t5_96 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_51, _t5_51, 32), _mm256_permute2f128_pd(_t5_51, _t5_51, 32), 0), _t5_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_52, _t5_52, 32), _mm256_permute2f128_pd(_t5_52, _t5_52, 32), 0), _t5_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_53, _t5_53, 32), _mm256_permute2f128_pd(_t5_53, _t5_53, 32), 0), _t5_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_54, _t5_54, 32), _mm256_permute2f128_pd(_t5_54, _t5_54, 32), 0), _t5_22)));
  _t5_97 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_55, _t5_55, 32), _mm256_permute2f128_pd(_t5_55, _t5_55, 32), 0), _t5_25), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_56, _t5_56, 32), _mm256_permute2f128_pd(_t5_56, _t5_56, 32), 0), _t5_24)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_57, _t5_57, 32), _mm256_permute2f128_pd(_t5_57, _t5_57, 32), 0), _t5_23), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_58, _t5_58, 32), _mm256_permute2f128_pd(_t5_58, _t5_58, 32), 0), _t5_22)));

  // 4-BLAC: 4x4 - 4x4
  _t5_106 = _mm256_sub_pd(_t5_106, _t5_94);
  _t5_107 = _mm256_sub_pd(_t5_107, _t5_95);
  _t5_108 = _mm256_sub_pd(_t5_108, _t5_96);
  _t5_109 = _mm256_sub_pd(_t5_109, _t5_97);

  // AVX Storer:

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_281 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_106, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_282 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_283 = _t5_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_284 = _mm256_add_pd(_t5_282, _t5_283);

  // 4-BLAC: 1x4 / 1x4
  _t5_285 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_281), _mm256_castpd256_pd128(_t5_284)));

  // AVX Storer:
  _t5_59 = _t5_285;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, 96), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_286 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_106, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_287 = _t5_59;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_288 = _t5_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_289 = _mm256_mul_pd(_t5_287, _t5_288);

  // 4-BLAC: 1x4 - 1x4
  _t5_290 = _mm256_sub_pd(_t5_286, _t5_289);

  // AVX Storer:
  _t5_60 = _t5_290;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_291 = _t5_60;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_292 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_293 = _t5_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_294 = _mm256_add_pd(_t5_292, _t5_293);

  // 4-BLAC: 1x4 / 1x4
  _t5_295 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_291), _mm256_castpd256_pd128(_t5_294)));

  // AVX Storer:
  _t5_60 = _t5_295;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, 96), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_296 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_106, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_106, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_297 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_59, _t5_60), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_298 = _t5_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_299 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_297, _t5_298), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_297, _t5_298), _mm256_mul_pd(_t5_297, _t5_298), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_297, _t5_298), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_297, _t5_298), _mm256_mul_pd(_t5_297, _t5_298), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_297, _t5_298), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_297, _t5_298), _mm256_mul_pd(_t5_297, _t5_298), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_300 = _mm256_sub_pd(_t5_296, _t5_299);

  // AVX Storer:
  _t5_61 = _t5_300;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_301 = _t5_61;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_302 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_303 = _t5_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_304 = _mm256_add_pd(_t5_302, _t5_303);

  // 4-BLAC: 1x4 / 1x4
  _t5_305 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_301), _mm256_castpd256_pd128(_t5_304)));

  // AVX Storer:
  _t5_61 = _t5_305;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, 96), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_306 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_106, _t5_106, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_307 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_59, _t5_60), _mm256_unpacklo_pd(_t5_61, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_308 = _t5_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_309 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_307, _t5_308), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_307, _t5_308), _mm256_mul_pd(_t5_307, _t5_308), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_307, _t5_308), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_307, _t5_308), _mm256_mul_pd(_t5_307, _t5_308), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_307, _t5_308), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_307, _t5_308), _mm256_mul_pd(_t5_307, _t5_308), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_310 = _mm256_sub_pd(_t5_306, _t5_309);

  // AVX Storer:
  _t5_62 = _t5_310;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_311 = _t5_62;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_312 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_313 = _t5_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_314 = _mm256_add_pd(_t5_312, _t5_313);

  // 4-BLAC: 1x4 / 1x4
  _t5_315 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_311), _mm256_castpd256_pd128(_t5_314)));

  // AVX Storer:
  _t5_62 = _t5_315;

  // Generating : X[100,100] = S(h(3, 100, 97), ( G(h(3, 100, 97), X[100,100],h(4, 100, 4)) - ( G(h(3, 100, 97), L[100,100],h(1, 100, 96)) * G(h(1, 100, 96), X[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t5_316 = _t5_107;
  _t5_317 = _t5_108;
  _t5_318 = _t5_109;
  _t5_319 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_320 = _t5_31;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t5_321 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_320, _t5_320, 32), _mm256_permute2f128_pd(_t5_320, _t5_320, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_59, _t5_60), _mm256_unpacklo_pd(_t5_61, _t5_62), 32));
  _t5_322 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_320, _t5_320, 32), _mm256_permute2f128_pd(_t5_320, _t5_320, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_59, _t5_60), _mm256_unpacklo_pd(_t5_61, _t5_62), 32));
  _t5_323 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_320, _t5_320, 49), _mm256_permute2f128_pd(_t5_320, _t5_320, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_59, _t5_60), _mm256_unpacklo_pd(_t5_61, _t5_62), 32));
  _t5_324 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_320, _t5_320, 49), _mm256_permute2f128_pd(_t5_320, _t5_320, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_59, _t5_60), _mm256_unpacklo_pd(_t5_61, _t5_62), 32));

  // 4-BLAC: 4x4 - 4x4
  _t5_325 = _mm256_sub_pd(_t5_316, _t5_321);
  _t5_326 = _mm256_sub_pd(_t5_317, _t5_322);
  _t5_327 = _mm256_sub_pd(_t5_318, _t5_323);
  _t5_328 = _mm256_sub_pd(_t5_319, _t5_324);

  // AVX Storer:
  _t5_107 = _t5_325;
  _t5_108 = _t5_326;
  _t5_109 = _t5_327;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_329 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_107, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_330 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_331 = _t5_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_332 = _mm256_add_pd(_t5_330, _t5_331);

  // 4-BLAC: 1x4 / 1x4
  _t5_333 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_329), _mm256_castpd256_pd128(_t5_332)));

  // AVX Storer:
  _t5_63 = _t5_333;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, 97), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_334 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_107, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_335 = _t5_63;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_336 = _t5_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_337 = _mm256_mul_pd(_t5_335, _t5_336);

  // 4-BLAC: 1x4 - 1x4
  _t5_338 = _mm256_sub_pd(_t5_334, _t5_337);

  // AVX Storer:
  _t5_64 = _t5_338;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_339 = _t5_64;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_340 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_341 = _t5_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_342 = _mm256_add_pd(_t5_340, _t5_341);

  // 4-BLAC: 1x4 / 1x4
  _t5_343 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_339), _mm256_castpd256_pd128(_t5_342)));

  // AVX Storer:
  _t5_64 = _t5_343;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, 97), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_344 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_107, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_107, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_345 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_63, _t5_64), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_346 = _t5_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_347 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_345, _t5_346), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_345, _t5_346), _mm256_mul_pd(_t5_345, _t5_346), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_345, _t5_346), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_345, _t5_346), _mm256_mul_pd(_t5_345, _t5_346), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_345, _t5_346), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_345, _t5_346), _mm256_mul_pd(_t5_345, _t5_346), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_348 = _mm256_sub_pd(_t5_344, _t5_347);

  // AVX Storer:
  _t5_65 = _t5_348;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_349 = _t5_65;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_350 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_351 = _t5_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_352 = _mm256_add_pd(_t5_350, _t5_351);

  // 4-BLAC: 1x4 / 1x4
  _t5_353 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_349), _mm256_castpd256_pd128(_t5_352)));

  // AVX Storer:
  _t5_65 = _t5_353;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, 97), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_354 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_107, _t5_107, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_355 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_63, _t5_64), _mm256_unpacklo_pd(_t5_65, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_356 = _t5_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_357 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_355, _t5_356), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_355, _t5_356), _mm256_mul_pd(_t5_355, _t5_356), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_355, _t5_356), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_355, _t5_356), _mm256_mul_pd(_t5_355, _t5_356), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_355, _t5_356), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_355, _t5_356), _mm256_mul_pd(_t5_355, _t5_356), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_358 = _mm256_sub_pd(_t5_354, _t5_357);

  // AVX Storer:
  _t5_66 = _t5_358;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_359 = _t5_66;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_360 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_361 = _t5_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_362 = _mm256_add_pd(_t5_360, _t5_361);

  // 4-BLAC: 1x4 / 1x4
  _t5_363 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_359), _mm256_castpd256_pd128(_t5_362)));

  // AVX Storer:
  _t5_66 = _t5_363;

  // Generating : X[100,100] = S(h(2, 100, 98), ( G(h(2, 100, 98), X[100,100],h(4, 100, 4)) - ( G(h(2, 100, 98), L[100,100],h(1, 100, 97)) * G(h(1, 100, 97), X[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t5_364 = _t5_108;
  _t5_365 = _t5_109;
  _t5_366 = _mm256_setzero_pd();
  _t5_367 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_368 = _t5_29;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t5_369 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_368, _t5_368, 32), _mm256_permute2f128_pd(_t5_368, _t5_368, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_63, _t5_64), _mm256_unpacklo_pd(_t5_65, _t5_66), 32));
  _t5_370 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_368, _t5_368, 32), _mm256_permute2f128_pd(_t5_368, _t5_368, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_63, _t5_64), _mm256_unpacklo_pd(_t5_65, _t5_66), 32));
  _t5_371 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_368, _t5_368, 49), _mm256_permute2f128_pd(_t5_368, _t5_368, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_63, _t5_64), _mm256_unpacklo_pd(_t5_65, _t5_66), 32));
  _t5_372 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_368, _t5_368, 49), _mm256_permute2f128_pd(_t5_368, _t5_368, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_63, _t5_64), _mm256_unpacklo_pd(_t5_65, _t5_66), 32));

  // 4-BLAC: 4x4 - 4x4
  _t5_373 = _mm256_sub_pd(_t5_364, _t5_369);
  _t5_374 = _mm256_sub_pd(_t5_365, _t5_370);
  _t5_375 = _mm256_sub_pd(_t5_366, _t5_371);
  _t5_376 = _mm256_sub_pd(_t5_367, _t5_372);

  // AVX Storer:
  _t5_108 = _t5_373;
  _t5_109 = _t5_374;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_377 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_108, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_378 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_379 = _t5_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_380 = _mm256_add_pd(_t5_378, _t5_379);

  // 4-BLAC: 1x4 / 1x4
  _t5_381 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_377), _mm256_castpd256_pd128(_t5_380)));

  // AVX Storer:
  _t5_67 = _t5_381;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, 98), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_382 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_108, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_383 = _t5_67;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_384 = _t5_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_385 = _mm256_mul_pd(_t5_383, _t5_384);

  // 4-BLAC: 1x4 - 1x4
  _t5_386 = _mm256_sub_pd(_t5_382, _t5_385);

  // AVX Storer:
  _t5_68 = _t5_386;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_387 = _t5_68;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_388 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_389 = _t5_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_390 = _mm256_add_pd(_t5_388, _t5_389);

  // 4-BLAC: 1x4 / 1x4
  _t5_391 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_387), _mm256_castpd256_pd128(_t5_390)));

  // AVX Storer:
  _t5_68 = _t5_391;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, 98), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_392 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_108, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_108, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_393 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_67, _t5_68), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_394 = _t5_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_395 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_393, _t5_394), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_393, _t5_394), _mm256_mul_pd(_t5_393, _t5_394), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_393, _t5_394), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_393, _t5_394), _mm256_mul_pd(_t5_393, _t5_394), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_393, _t5_394), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_393, _t5_394), _mm256_mul_pd(_t5_393, _t5_394), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_396 = _mm256_sub_pd(_t5_392, _t5_395);

  // AVX Storer:
  _t5_69 = _t5_396;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_397 = _t5_69;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_398 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_399 = _t5_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_400 = _mm256_add_pd(_t5_398, _t5_399);

  // 4-BLAC: 1x4 / 1x4
  _t5_401 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_397), _mm256_castpd256_pd128(_t5_400)));

  // AVX Storer:
  _t5_69 = _t5_401;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, 98), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_402 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_108, _t5_108, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_403 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_67, _t5_68), _mm256_unpacklo_pd(_t5_69, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_404 = _t5_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_405 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_403, _t5_404), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_403, _t5_404), _mm256_mul_pd(_t5_403, _t5_404), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_403, _t5_404), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_403, _t5_404), _mm256_mul_pd(_t5_403, _t5_404), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_403, _t5_404), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_403, _t5_404), _mm256_mul_pd(_t5_403, _t5_404), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_406 = _mm256_sub_pd(_t5_402, _t5_405);

  // AVX Storer:
  _t5_70 = _t5_406;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_407 = _t5_70;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_408 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_409 = _t5_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_410 = _mm256_add_pd(_t5_408, _t5_409);

  // 4-BLAC: 1x4 / 1x4
  _t5_411 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_407), _mm256_castpd256_pd128(_t5_410)));

  // AVX Storer:
  _t5_70 = _t5_411;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(4, 100, 4)) - ( G(h(1, 100, 99), L[100,100],h(1, 100, 98)) Kro G(h(1, 100, 98), X[100,100],h(4, 100, 4)) ) ),h(4, 100, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_412 = _t5_27;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t5_92 = _mm256_mul_pd(_t5_412, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_67, _t5_68), _mm256_unpacklo_pd(_t5_69, _t5_70), 32));

  // 4-BLAC: 1x4 - 1x4
  _t5_109 = _mm256_sub_pd(_t5_109, _t5_92);

  // AVX Storer:

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 4)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 4), U[100,100],h(1, 100, 4)) ) ),h(1, 100, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_413 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_109, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_414 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_415 = _t5_21;

  // 4-BLAC: 1x4 + 1x4
  _t5_416 = _mm256_add_pd(_t5_414, _t5_415);

  // 4-BLAC: 1x4 / 1x4
  _t5_417 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_413), _mm256_castpd256_pd128(_t5_416)));

  // AVX Storer:
  _t5_71 = _t5_417;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 5)) - ( G(h(1, 100, 99), X[100,100],h(1, 100, 4)) Kro G(h(1, 100, 4), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_418 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_109, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_419 = _t5_71;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_420 = _t5_20;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_421 = _mm256_mul_pd(_t5_419, _t5_420);

  // 4-BLAC: 1x4 - 1x4
  _t5_422 = _mm256_sub_pd(_t5_418, _t5_421);

  // AVX Storer:
  _t5_72 = _t5_422;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 5)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 5), U[100,100],h(1, 100, 5)) ) ),h(1, 100, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_423 = _t5_72;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_424 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_425 = _t5_19;

  // 4-BLAC: 1x4 + 1x4
  _t5_426 = _mm256_add_pd(_t5_424, _t5_425);

  // 4-BLAC: 1x4 / 1x4
  _t5_427 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_423), _mm256_castpd256_pd128(_t5_426)));

  // AVX Storer:
  _t5_72 = _t5_427;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 6)) - ( G(h(1, 100, 99), X[100,100],h(2, 100, 4)) * G(h(2, 100, 4), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_428 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_109, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_109, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_429 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_71, _t5_72), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_430 = _t5_18;

  // 4-BLAC: 1x4 * 4x1
  _t5_431 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_429, _t5_430), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_429, _t5_430), _mm256_mul_pd(_t5_429, _t5_430), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_429, _t5_430), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_429, _t5_430), _mm256_mul_pd(_t5_429, _t5_430), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_429, _t5_430), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_429, _t5_430), _mm256_mul_pd(_t5_429, _t5_430), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_432 = _mm256_sub_pd(_t5_428, _t5_431);

  // AVX Storer:
  _t5_73 = _t5_432;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 6)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 6), U[100,100],h(1, 100, 6)) ) ),h(1, 100, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_433 = _t5_73;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_434 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_435 = _t5_17;

  // 4-BLAC: 1x4 + 1x4
  _t5_436 = _mm256_add_pd(_t5_434, _t5_435);

  // 4-BLAC: 1x4 / 1x4
  _t5_437 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_433), _mm256_castpd256_pd128(_t5_436)));

  // AVX Storer:
  _t5_73 = _t5_437;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 7)) - ( G(h(1, 100, 99), X[100,100],h(3, 100, 4)) * G(h(3, 100, 4), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_438 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_109, _t5_109, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_439 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_71, _t5_72), _mm256_unpacklo_pd(_t5_73, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_440 = _t5_16;

  // 4-BLAC: 1x4 * 4x1
  _t5_441 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_439, _t5_440), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_439, _t5_440), _mm256_mul_pd(_t5_439, _t5_440), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_439, _t5_440), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_439, _t5_440), _mm256_mul_pd(_t5_439, _t5_440), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_439, _t5_440), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_439, _t5_440), _mm256_mul_pd(_t5_439, _t5_440), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_442 = _mm256_sub_pd(_t5_438, _t5_441);

  // AVX Storer:
  _t5_74 = _t5_442;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, 7)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, 7), U[100,100],h(1, 100, 7)) ) ),h(1, 100, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_443 = _t5_74;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_444 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_445 = _t5_15;

  // 4-BLAC: 1x4 + 1x4
  _t5_446 = _mm256_add_pd(_t5_444, _t5_445);

  // 4-BLAC: 1x4 / 1x4
  _t5_447 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_443), _mm256_castpd256_pd128(_t5_446)));

  // AVX Storer:
  _t5_74 = _t5_447;

  // Generating : X[100,100] = ( S(h(4, 100, 96), ( G(h(4, 100, 96), X[100,100],h(4, 100, fi4)) - ( G(h(4, 100, 96), X[100,100],h(4, 100, 0)) * G(h(4, 100, 0), U[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4)) + Sum_{k116} ( -$(h(4, 100, 96), ( G(h(4, 100, 96), X[100,100],h(4, 100, k116)) * G(h(4, 100, k116), U[100,100],h(4, 100, fi4)) ),h(4, 100, fi4)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t5_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_40, _t5_40, 32), _mm256_permute2f128_pd(_t5_40, _t5_40, 32), 0), _t5_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_41, _t5_41, 32), _mm256_permute2f128_pd(_t5_41, _t5_41, 32), 0), _t5_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_42, _t5_42, 32), _mm256_permute2f128_pd(_t5_42, _t5_42, 32), 0), _t5_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_43, _t5_43, 32), _mm256_permute2f128_pd(_t5_43, _t5_43, 32), 0), _t5_11)));
  _t5_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_47, _t5_47, 32), _mm256_permute2f128_pd(_t5_47, _t5_47, 32), 0), _t5_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_48, _t5_48, 32), _mm256_permute2f128_pd(_t5_48, _t5_48, 32), 0), _t5_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_49, _t5_49, 32), _mm256_permute2f128_pd(_t5_49, _t5_49, 32), 0), _t5_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_50, _t5_50, 32), _mm256_permute2f128_pd(_t5_50, _t5_50, 32), 0), _t5_11)));
  _t5_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_51, _t5_51, 32), _mm256_permute2f128_pd(_t5_51, _t5_51, 32), 0), _t5_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_52, _t5_52, 32), _mm256_permute2f128_pd(_t5_52, _t5_52, 32), 0), _t5_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_53, _t5_53, 32), _mm256_permute2f128_pd(_t5_53, _t5_53, 32), 0), _t5_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_54, _t5_54, 32), _mm256_permute2f128_pd(_t5_54, _t5_54, 32), 0), _t5_11)));
  _t5_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_55, _t5_55, 32), _mm256_permute2f128_pd(_t5_55, _t5_55, 32), 0), _t5_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_56, _t5_56, 32), _mm256_permute2f128_pd(_t5_56, _t5_56, 32), 0), _t5_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_57, _t5_57, 32), _mm256_permute2f128_pd(_t5_57, _t5_57, 32), 0), _t5_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_58, _t5_58, 32), _mm256_permute2f128_pd(_t5_58, _t5_58, 32), 0), _t5_11)));

  // 4-BLAC: 4x4 - 4x4
  _t5_110 = _mm256_sub_pd(_t5_110, _t5_98);
  _t5_111 = _mm256_sub_pd(_t5_111, _t5_99);
  _t5_112 = _mm256_sub_pd(_t5_112, _t5_100);
  _t5_113 = _mm256_sub_pd(_t5_113, _t5_101);

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t5_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_59, _t5_59, 32), _mm256_permute2f128_pd(_t5_59, _t5_59, 32), 0), _t5_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_60, _t5_60, 32), _mm256_permute2f128_pd(_t5_60, _t5_60, 32), 0), _t5_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_61, _t5_61, 32), _mm256_permute2f128_pd(_t5_61, _t5_61, 32), 0), _t5_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_62, _t5_62, 32), _mm256_permute2f128_pd(_t5_62, _t5_62, 32), 0), _t5_7)));
  _t5_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_63, _t5_63, 32), _mm256_permute2f128_pd(_t5_63, _t5_63, 32), 0), _t5_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_64, _t5_64, 32), _mm256_permute2f128_pd(_t5_64, _t5_64, 32), 0), _t5_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_65, _t5_65, 32), _mm256_permute2f128_pd(_t5_65, _t5_65, 32), 0), _t5_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_66, _t5_66, 32), _mm256_permute2f128_pd(_t5_66, _t5_66, 32), 0), _t5_7)));
  _t5_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_67, _t5_67, 32), _mm256_permute2f128_pd(_t5_67, _t5_67, 32), 0), _t5_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_68, _t5_68, 32), _mm256_permute2f128_pd(_t5_68, _t5_68, 32), 0), _t5_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_69, _t5_69, 32), _mm256_permute2f128_pd(_t5_69, _t5_69, 32), 0), _t5_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_70, _t5_70, 32), _mm256_permute2f128_pd(_t5_70, _t5_70, 32), 0), _t5_7)));
  _t5_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_71, _t5_71, 32), _mm256_permute2f128_pd(_t5_71, _t5_71, 32), 0), _t5_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_72, _t5_72, 32), _mm256_permute2f128_pd(_t5_72, _t5_72, 32), 0), _t5_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_73, _t5_73, 32), _mm256_permute2f128_pd(_t5_73, _t5_73, 32), 0), _t5_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_74, _t5_74, 32), _mm256_permute2f128_pd(_t5_74, _t5_74, 32), 0), _t5_7)));

  // AVX Loader:

  // 4-BLAC: 4x4 - 4x4
  _t5_110 = _mm256_sub_pd(_t5_110, _t5_102);
  _t5_111 = _mm256_sub_pd(_t5_111, _t5_103);
  _t5_112 = _mm256_sub_pd(_t5_112, _t5_104);
  _t5_113 = _mm256_sub_pd(_t5_113, _t5_105);

  // AVX Storer:

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_448 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_110, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_449 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_450 = _t5_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_451 = _mm256_add_pd(_t5_449, _t5_450);

  // 4-BLAC: 1x4 / 1x4
  _t5_452 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_448), _mm256_castpd256_pd128(_t5_451)));

  // AVX Storer:
  _t5_75 = _t5_452;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_453 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_110, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_454 = _t5_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_455 = _t5_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_456 = _mm256_mul_pd(_t5_454, _t5_455);

  // 4-BLAC: 1x4 - 1x4
  _t5_457 = _mm256_sub_pd(_t5_453, _t5_456);

  // AVX Storer:
  _t5_76 = _t5_457;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_458 = _t5_76;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_459 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_460 = _t5_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_461 = _mm256_add_pd(_t5_459, _t5_460);

  // 4-BLAC: 1x4 / 1x4
  _t5_462 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_458), _mm256_castpd256_pd128(_t5_461)));

  // AVX Storer:
  _t5_76 = _t5_462;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 96), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_463 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_110, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_110, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_464 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_75, _t5_76), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_465 = _t5_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_466 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_464, _t5_465), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_464, _t5_465), _mm256_mul_pd(_t5_464, _t5_465), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_464, _t5_465), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_464, _t5_465), _mm256_mul_pd(_t5_464, _t5_465), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_464, _t5_465), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_464, _t5_465), _mm256_mul_pd(_t5_464, _t5_465), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_467 = _mm256_sub_pd(_t5_463, _t5_466);

  // AVX Storer:
  _t5_77 = _t5_467;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_468 = _t5_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_469 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_470 = _t5_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_471 = _mm256_add_pd(_t5_469, _t5_470);

  // 4-BLAC: 1x4 / 1x4
  _t5_472 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_468), _mm256_castpd256_pd128(_t5_471)));

  // AVX Storer:
  _t5_77 = _t5_472;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 96), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_473 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_110, _t5_110, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_474 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_75, _t5_76), _mm256_unpacklo_pd(_t5_77, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_475 = _t5_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_476 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_474, _t5_475), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_474, _t5_475), _mm256_mul_pd(_t5_474, _t5_475), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_474, _t5_475), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_474, _t5_475), _mm256_mul_pd(_t5_474, _t5_475), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_474, _t5_475), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_474, _t5_475), _mm256_mul_pd(_t5_474, _t5_475), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_477 = _mm256_sub_pd(_t5_473, _t5_476);

  // AVX Storer:
  _t5_78 = _t5_477;

  // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_478 = _t5_78;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_479 = _t5_39;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_480 = _t5_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_481 = _mm256_add_pd(_t5_479, _t5_480);

  // 4-BLAC: 1x4 / 1x4
  _t5_482 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_478), _mm256_castpd256_pd128(_t5_481)));

  // AVX Storer:
  _t5_78 = _t5_482;

  // Generating : X[100,100] = S(h(3, 100, 97), ( G(h(3, 100, 97), X[100,100],h(4, 100, fi4)) - ( G(h(3, 100, 97), L[100,100],h(1, 100, 96)) * G(h(1, 100, 96), X[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t5_483 = _t5_111;
  _t5_484 = _t5_112;
  _t5_485 = _t5_113;
  _t5_486 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_487 = _t5_31;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t5_488 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_487, _t5_487, 32), _mm256_permute2f128_pd(_t5_487, _t5_487, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_75, _t5_76), _mm256_unpacklo_pd(_t5_77, _t5_78), 32));
  _t5_489 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_487, _t5_487, 32), _mm256_permute2f128_pd(_t5_487, _t5_487, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_75, _t5_76), _mm256_unpacklo_pd(_t5_77, _t5_78), 32));
  _t5_490 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_487, _t5_487, 49), _mm256_permute2f128_pd(_t5_487, _t5_487, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_75, _t5_76), _mm256_unpacklo_pd(_t5_77, _t5_78), 32));
  _t5_491 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_487, _t5_487, 49), _mm256_permute2f128_pd(_t5_487, _t5_487, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_75, _t5_76), _mm256_unpacklo_pd(_t5_77, _t5_78), 32));

  // 4-BLAC: 4x4 - 4x4
  _t5_492 = _mm256_sub_pd(_t5_483, _t5_488);
  _t5_493 = _mm256_sub_pd(_t5_484, _t5_489);
  _t5_494 = _mm256_sub_pd(_t5_485, _t5_490);
  _t5_495 = _mm256_sub_pd(_t5_486, _t5_491);

  // AVX Storer:
  _t5_111 = _t5_492;
  _t5_112 = _t5_493;
  _t5_113 = _t5_494;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_496 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_111, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_497 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_498 = _t5_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_499 = _mm256_add_pd(_t5_497, _t5_498);

  // 4-BLAC: 1x4 / 1x4
  _t5_500 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_496), _mm256_castpd256_pd128(_t5_499)));

  // AVX Storer:
  _t5_79 = _t5_500;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_501 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_111, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_502 = _t5_79;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_503 = _t5_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_504 = _mm256_mul_pd(_t5_502, _t5_503);

  // 4-BLAC: 1x4 - 1x4
  _t5_505 = _mm256_sub_pd(_t5_501, _t5_504);

  // AVX Storer:
  _t5_80 = _t5_505;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_506 = _t5_80;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_507 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_508 = _t5_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_509 = _mm256_add_pd(_t5_507, _t5_508);

  // 4-BLAC: 1x4 / 1x4
  _t5_510 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_506), _mm256_castpd256_pd128(_t5_509)));

  // AVX Storer:
  _t5_80 = _t5_510;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 97), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_511 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_111, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_111, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_512 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_79, _t5_80), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_513 = _t5_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_514 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_512, _t5_513), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_512, _t5_513), _mm256_mul_pd(_t5_512, _t5_513), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_512, _t5_513), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_512, _t5_513), _mm256_mul_pd(_t5_512, _t5_513), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_512, _t5_513), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_512, _t5_513), _mm256_mul_pd(_t5_512, _t5_513), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_515 = _mm256_sub_pd(_t5_511, _t5_514);

  // AVX Storer:
  _t5_81 = _t5_515;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_516 = _t5_81;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_517 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_518 = _t5_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_519 = _mm256_add_pd(_t5_517, _t5_518);

  // 4-BLAC: 1x4 / 1x4
  _t5_520 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_516), _mm256_castpd256_pd128(_t5_519)));

  // AVX Storer:
  _t5_81 = _t5_520;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 97), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_521 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_111, _t5_111, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_522 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_79, _t5_80), _mm256_unpacklo_pd(_t5_81, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_523 = _t5_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_524 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_522, _t5_523), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_522, _t5_523), _mm256_mul_pd(_t5_522, _t5_523), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_522, _t5_523), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_522, _t5_523), _mm256_mul_pd(_t5_522, _t5_523), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_522, _t5_523), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_522, _t5_523), _mm256_mul_pd(_t5_522, _t5_523), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_525 = _mm256_sub_pd(_t5_521, _t5_524);

  // AVX Storer:
  _t5_82 = _t5_525;

  // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_526 = _t5_82;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_527 = _t5_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_528 = _t5_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_529 = _mm256_add_pd(_t5_527, _t5_528);

  // 4-BLAC: 1x4 / 1x4
  _t5_530 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_526), _mm256_castpd256_pd128(_t5_529)));

  // AVX Storer:
  _t5_82 = _t5_530;

  // Generating : X[100,100] = S(h(2, 100, 98), ( G(h(2, 100, 98), X[100,100],h(4, 100, fi4)) - ( G(h(2, 100, 98), L[100,100],h(1, 100, 97)) * G(h(1, 100, 97), X[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t5_531 = _t5_112;
  _t5_532 = _t5_113;
  _t5_533 = _mm256_setzero_pd();
  _t5_534 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_535 = _t5_29;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t5_536 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_535, _t5_535, 32), _mm256_permute2f128_pd(_t5_535, _t5_535, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_79, _t5_80), _mm256_unpacklo_pd(_t5_81, _t5_82), 32));
  _t5_537 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_535, _t5_535, 32), _mm256_permute2f128_pd(_t5_535, _t5_535, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_79, _t5_80), _mm256_unpacklo_pd(_t5_81, _t5_82), 32));
  _t5_538 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_535, _t5_535, 49), _mm256_permute2f128_pd(_t5_535, _t5_535, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_79, _t5_80), _mm256_unpacklo_pd(_t5_81, _t5_82), 32));
  _t5_539 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_535, _t5_535, 49), _mm256_permute2f128_pd(_t5_535, _t5_535, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_79, _t5_80), _mm256_unpacklo_pd(_t5_81, _t5_82), 32));

  // 4-BLAC: 4x4 - 4x4
  _t5_540 = _mm256_sub_pd(_t5_531, _t5_536);
  _t5_541 = _mm256_sub_pd(_t5_532, _t5_537);
  _t5_542 = _mm256_sub_pd(_t5_533, _t5_538);
  _t5_543 = _mm256_sub_pd(_t5_534, _t5_539);

  // AVX Storer:
  _t5_112 = _t5_540;
  _t5_113 = _t5_541;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_544 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_112, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_545 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_546 = _t5_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_547 = _mm256_add_pd(_t5_545, _t5_546);

  // 4-BLAC: 1x4 / 1x4
  _t5_548 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_544), _mm256_castpd256_pd128(_t5_547)));

  // AVX Storer:
  _t5_83 = _t5_548;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_549 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_112, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_550 = _t5_83;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_551 = _t5_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_552 = _mm256_mul_pd(_t5_550, _t5_551);

  // 4-BLAC: 1x4 - 1x4
  _t5_553 = _mm256_sub_pd(_t5_549, _t5_552);

  // AVX Storer:
  _t5_84 = _t5_553;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_554 = _t5_84;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_555 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_556 = _t5_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_557 = _mm256_add_pd(_t5_555, _t5_556);

  // 4-BLAC: 1x4 / 1x4
  _t5_558 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_554), _mm256_castpd256_pd128(_t5_557)));

  // AVX Storer:
  _t5_84 = _t5_558;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 98), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_559 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_112, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_112, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_560 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_83, _t5_84), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_561 = _t5_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_562 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_560, _t5_561), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_560, _t5_561), _mm256_mul_pd(_t5_560, _t5_561), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_560, _t5_561), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_560, _t5_561), _mm256_mul_pd(_t5_560, _t5_561), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_560, _t5_561), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_560, _t5_561), _mm256_mul_pd(_t5_560, _t5_561), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_563 = _mm256_sub_pd(_t5_559, _t5_562);

  // AVX Storer:
  _t5_85 = _t5_563;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_564 = _t5_85;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_565 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_566 = _t5_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_567 = _mm256_add_pd(_t5_565, _t5_566);

  // 4-BLAC: 1x4 / 1x4
  _t5_568 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_564), _mm256_castpd256_pd128(_t5_567)));

  // AVX Storer:
  _t5_85 = _t5_568;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 98), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_569 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_112, _t5_112, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_570 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_83, _t5_84), _mm256_unpacklo_pd(_t5_85, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_571 = _t5_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_572 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_570, _t5_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_570, _t5_571), _mm256_mul_pd(_t5_570, _t5_571), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_570, _t5_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_570, _t5_571), _mm256_mul_pd(_t5_570, _t5_571), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_570, _t5_571), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_570, _t5_571), _mm256_mul_pd(_t5_570, _t5_571), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_573 = _mm256_sub_pd(_t5_569, _t5_572);

  // AVX Storer:
  _t5_86 = _t5_573;

  // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_574 = _t5_86;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_575 = _t5_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_576 = _t5_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_577 = _mm256_add_pd(_t5_575, _t5_576);

  // 4-BLAC: 1x4 / 1x4
  _t5_578 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_574), _mm256_castpd256_pd128(_t5_577)));

  // AVX Storer:
  _t5_86 = _t5_578;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(4, 100, fi4)) - ( G(h(1, 100, 99), L[100,100],h(1, 100, 98)) Kro G(h(1, 100, 98), X[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_579 = _t5_27;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t5_93 = _mm256_mul_pd(_t5_579, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_83, _t5_84), _mm256_unpacklo_pd(_t5_85, _t5_86), 32));

  // 4-BLAC: 1x4 - 1x4
  _t5_113 = _mm256_sub_pd(_t5_113, _t5_93);

  // AVX Storer:

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_580 = _mm256_blend_pd(_mm256_setzero_pd(), _t5_113, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_581 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_582 = _t5_6;

  // 4-BLAC: 1x4 + 1x4
  _t5_583 = _mm256_add_pd(_t5_581, _t5_582);

  // 4-BLAC: 1x4 / 1x4
  _t5_584 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_580), _mm256_castpd256_pd128(_t5_583)));

  // AVX Storer:
  _t5_87 = _t5_584;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_585 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_113, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_586 = _t5_87;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_587 = _t5_5;

  // 4-BLAC: 1x4 Kro 1x4
  _t5_588 = _mm256_mul_pd(_t5_586, _t5_587);

  // 4-BLAC: 1x4 - 1x4
  _t5_589 = _mm256_sub_pd(_t5_585, _t5_588);

  // AVX Storer:
  _t5_88 = _t5_589;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_590 = _t5_88;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_591 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_592 = _t5_4;

  // 4-BLAC: 1x4 + 1x4
  _t5_593 = _mm256_add_pd(_t5_591, _t5_592);

  // 4-BLAC: 1x4 / 1x4
  _t5_594 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_590), _mm256_castpd256_pd128(_t5_593)));

  // AVX Storer:
  _t5_88 = _t5_594;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 99), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_595 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t5_113, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t5_113, 4), 129);

  // AVX Loader:

  // 1x2 -> 1x4
  _t5_596 = _mm256_blend_pd(_mm256_unpacklo_pd(_t5_87, _t5_88), _mm256_setzero_pd(), 12);

  // AVX Loader:

  // 2x1 -> 4x1
  _t5_597 = _t5_3;

  // 4-BLAC: 1x4 * 4x1
  _t5_598 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_596, _t5_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_596, _t5_597), _mm256_mul_pd(_t5_596, _t5_597), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_596, _t5_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_596, _t5_597), _mm256_mul_pd(_t5_596, _t5_597), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_596, _t5_597), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_596, _t5_597), _mm256_mul_pd(_t5_596, _t5_597), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_599 = _mm256_sub_pd(_t5_595, _t5_598);

  // AVX Storer:
  _t5_89 = _t5_599;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_600 = _t5_89;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_601 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_602 = _t5_2;

  // 4-BLAC: 1x4 + 1x4
  _t5_603 = _mm256_add_pd(_t5_601, _t5_602);

  // 4-BLAC: 1x4 / 1x4
  _t5_604 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_600), _mm256_castpd256_pd128(_t5_603)));

  // AVX Storer:
  _t5_89 = _t5_604;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 99), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_605 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t5_113, _t5_113, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x3 -> 1x4
  _t5_606 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_87, _t5_88), _mm256_unpacklo_pd(_t5_89, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t5_607 = _t5_1;

  // 4-BLAC: 1x4 * 4x1
  _t5_608 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t5_606, _t5_607), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_606, _t5_607), _mm256_mul_pd(_t5_606, _t5_607), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t5_606, _t5_607), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_606, _t5_607), _mm256_mul_pd(_t5_606, _t5_607), 129)), _mm256_add_pd(_mm256_mul_pd(_t5_606, _t5_607), _mm256_permute2f128_pd(_mm256_mul_pd(_t5_606, _t5_607), _mm256_mul_pd(_t5_606, _t5_607), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t5_609 = _mm256_sub_pd(_t5_605, _t5_608);

  // AVX Storer:
  _t5_90 = _t5_609;

  // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_610 = _t5_90;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_611 = _t5_26;

  // AVX Loader:

  // 1x1 -> 1x4
  _t5_612 = _t5_0;

  // 4-BLAC: 1x4 + 1x4
  _t5_613 = _mm256_add_pd(_t5_611, _t5_612);

  // 4-BLAC: 1x4 / 1x4
  _t5_614 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t5_610), _mm256_castpd256_pd128(_t5_613)));

  // AVX Storer:
  _t5_90 = _t5_614;

  _asm256_storeu_pd(C + 9700, _t5_44);
  _asm256_storeu_pd(C + 9800, _t5_45);
  _asm256_storeu_pd(C + 9900, _t5_46);
  _mm_store_sd(&(C[9604]), _mm256_castpd256_pd128(_t5_59));
  _mm_store_sd(&(C[9605]), _mm256_castpd256_pd128(_t5_60));
  _mm_store_sd(&(C[9606]), _mm256_castpd256_pd128(_t5_61));
  _mm_store_sd(&(C[9607]), _mm256_castpd256_pd128(_t5_62));
  _mm_store_sd(&(C[9704]), _mm256_castpd256_pd128(_t5_63));
  _mm_store_sd(&(C[9705]), _mm256_castpd256_pd128(_t5_64));
  _mm_store_sd(&(C[9706]), _mm256_castpd256_pd128(_t5_65));
  _mm_store_sd(&(C[9707]), _mm256_castpd256_pd128(_t5_66));
  _mm_store_sd(&(C[9804]), _mm256_castpd256_pd128(_t5_67));
  _mm_store_sd(&(C[9805]), _mm256_castpd256_pd128(_t5_68));
  _mm_store_sd(&(C[9806]), _mm256_castpd256_pd128(_t5_69));
  _mm_store_sd(&(C[9807]), _mm256_castpd256_pd128(_t5_70));
  _mm_store_sd(&(C[9904]), _mm256_castpd256_pd128(_t5_71));
  _mm_store_sd(&(C[9905]), _mm256_castpd256_pd128(_t5_72));
  _mm_store_sd(&(C[9906]), _mm256_castpd256_pd128(_t5_73));
  _mm_store_sd(&(C[9907]), _mm256_castpd256_pd128(_t5_74));
  _mm_store_sd(&(C[9608]), _mm256_castpd256_pd128(_t5_75));
  _mm_store_sd(&(C[9609]), _mm256_castpd256_pd128(_t5_76));
  _mm_store_sd(&(C[9610]), _mm256_castpd256_pd128(_t5_77));
  _mm_store_sd(&(C[9611]), _mm256_castpd256_pd128(_t5_78));
  _mm_store_sd(&(C[9708]), _mm256_castpd256_pd128(_t5_79));
  _mm_store_sd(&(C[9709]), _mm256_castpd256_pd128(_t5_80));
  _mm_store_sd(&(C[9710]), _mm256_castpd256_pd128(_t5_81));
  _mm_store_sd(&(C[9711]), _mm256_castpd256_pd128(_t5_82));
  _mm_store_sd(&(C[9808]), _mm256_castpd256_pd128(_t5_83));
  _mm_store_sd(&(C[9809]), _mm256_castpd256_pd128(_t5_84));
  _mm_store_sd(&(C[9810]), _mm256_castpd256_pd128(_t5_85));
  _mm_store_sd(&(C[9811]), _mm256_castpd256_pd128(_t5_86));
  _mm_store_sd(&(C[9908]), _mm256_castpd256_pd128(_t5_87));
  _mm_store_sd(&(C[9909]), _mm256_castpd256_pd128(_t5_88));
  _mm_store_sd(&(C[9910]), _mm256_castpd256_pd128(_t5_89));
  _mm_store_sd(&(C[9911]), _mm256_castpd256_pd128(_t5_90));

  for( int fi4 = 12; fi4 <= 96; fi4+=4 ) {
    _t6_4 = _asm256_loadu_pd(C + fi4 + 9600);
    _t6_5 = _asm256_loadu_pd(C + fi4 + 9700);
    _t6_6 = _asm256_loadu_pd(C + fi4 + 9800);
    _t6_7 = _asm256_loadu_pd(C + fi4 + 9900);
    _t6_3 = _asm256_loadu_pd(U + fi4);
    _t6_2 = _asm256_loadu_pd(U + fi4 + 100);
    _t6_1 = _asm256_loadu_pd(U + fi4 + 200);
    _t6_0 = _asm256_loadu_pd(U + fi4 + 300);

    // Generating : X[100,100] = ( S(h(4, 100, 96), ( G(h(4, 100, 96), X[100,100],h(4, 100, fi4)) - ( G(h(4, 100, 96), X[100,100],h(4, 100, 0)) * G(h(4, 100, 0), U[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4)) + Sum_{k116} ( -$(h(4, 100, 96), ( G(h(4, 100, 96), X[100,100],h(4, 100, k116)) * G(h(4, 100, k116), U[100,100],h(4, 100, fi4)) ),h(4, 100, fi4)) ) )

    // AVX Loader:

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t5_98 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_40, _t5_40, 32), _mm256_permute2f128_pd(_t5_40, _t5_40, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_41, _t5_41, 32), _mm256_permute2f128_pd(_t5_41, _t5_41, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_42, _t5_42, 32), _mm256_permute2f128_pd(_t5_42, _t5_42, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_43, _t5_43, 32), _mm256_permute2f128_pd(_t5_43, _t5_43, 32), 0), _t6_0)));
    _t5_99 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_47, _t5_47, 32), _mm256_permute2f128_pd(_t5_47, _t5_47, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_48, _t5_48, 32), _mm256_permute2f128_pd(_t5_48, _t5_48, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_49, _t5_49, 32), _mm256_permute2f128_pd(_t5_49, _t5_49, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_50, _t5_50, 32), _mm256_permute2f128_pd(_t5_50, _t5_50, 32), 0), _t6_0)));
    _t5_100 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_51, _t5_51, 32), _mm256_permute2f128_pd(_t5_51, _t5_51, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_52, _t5_52, 32), _mm256_permute2f128_pd(_t5_52, _t5_52, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_53, _t5_53, 32), _mm256_permute2f128_pd(_t5_53, _t5_53, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_54, _t5_54, 32), _mm256_permute2f128_pd(_t5_54, _t5_54, 32), 0), _t6_0)));
    _t5_101 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_55, _t5_55, 32), _mm256_permute2f128_pd(_t5_55, _t5_55, 32), 0), _t6_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_56, _t5_56, 32), _mm256_permute2f128_pd(_t5_56, _t5_56, 32), 0), _t6_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_57, _t5_57, 32), _mm256_permute2f128_pd(_t5_57, _t5_57, 32), 0), _t6_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t5_58, _t5_58, 32), _mm256_permute2f128_pd(_t5_58, _t5_58, 32), 0), _t6_0)));

    // 4-BLAC: 4x4 - 4x4
    _t6_4 = _mm256_sub_pd(_t6_4, _t5_98);
    _t6_5 = _mm256_sub_pd(_t6_5, _t5_99);
    _t6_6 = _mm256_sub_pd(_t6_6, _t5_100);
    _t6_7 = _mm256_sub_pd(_t6_7, _t5_101);

    // AVX Storer:
    _asm256_storeu_pd(C + fi4 + 9600, _t6_4);
    _asm256_storeu_pd(C + fi4 + 9700, _t6_5);
    _asm256_storeu_pd(C + fi4 + 9800, _t6_6);
    _asm256_storeu_pd(C + fi4 + 9900, _t6_7);

    for( int k116 = 4; k116 <= fi4 - 1; k116+=4 ) {
      _t7_19 = _mm256_broadcast_sd(C + k116 + 9600);
      _t7_18 = _mm256_broadcast_sd(C + k116 + 9601);
      _t7_17 = _mm256_broadcast_sd(C + k116 + 9602);
      _t7_16 = _mm256_broadcast_sd(C + k116 + 9603);
      _t7_15 = _mm256_broadcast_sd(C + k116 + 9700);
      _t7_14 = _mm256_broadcast_sd(C + k116 + 9701);
      _t7_13 = _mm256_broadcast_sd(C + k116 + 9702);
      _t7_12 = _mm256_broadcast_sd(C + k116 + 9703);
      _t7_11 = _mm256_broadcast_sd(C + k116 + 9800);
      _t7_10 = _mm256_broadcast_sd(C + k116 + 9801);
      _t7_9 = _mm256_broadcast_sd(C + k116 + 9802);
      _t7_8 = _mm256_broadcast_sd(C + k116 + 9803);
      _t7_7 = _mm256_broadcast_sd(C + k116 + 9900);
      _t7_6 = _mm256_broadcast_sd(C + k116 + 9901);
      _t7_5 = _mm256_broadcast_sd(C + k116 + 9902);
      _t7_4 = _mm256_broadcast_sd(C + k116 + 9903);
      _t7_3 = _asm256_loadu_pd(U + fi4 + 100*k116);
      _t7_2 = _asm256_loadu_pd(U + fi4 + 100*k116 + 100);
      _t7_1 = _asm256_loadu_pd(U + fi4 + 100*k116 + 200);
      _t7_0 = _asm256_loadu_pd(U + fi4 + 100*k116 + 300);
      _t7_20 = _asm256_loadu_pd(C + fi4 + 9600);
      _t7_21 = _asm256_loadu_pd(C + fi4 + 9700);
      _t7_22 = _asm256_loadu_pd(C + fi4 + 9800);
      _t7_23 = _asm256_loadu_pd(C + fi4 + 9900);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_102 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_3), _mm256_mul_pd(_t7_18, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_1), _mm256_mul_pd(_t7_16, _t7_0)));
      _t5_103 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_3), _mm256_mul_pd(_t7_14, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_1), _mm256_mul_pd(_t7_12, _t7_0)));
      _t5_104 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_3), _mm256_mul_pd(_t7_10, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_1), _mm256_mul_pd(_t7_8, _t7_0)));
      _t5_105 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_3), _mm256_mul_pd(_t7_6, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_1), _mm256_mul_pd(_t7_4, _t7_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t7_20 = _mm256_sub_pd(_t7_20, _t5_102);
      _t7_21 = _mm256_sub_pd(_t7_21, _t5_103);
      _t7_22 = _mm256_sub_pd(_t7_22, _t5_104);
      _t7_23 = _mm256_sub_pd(_t7_23, _t5_105);

      // AVX Storer:
      _asm256_storeu_pd(C + fi4 + 9600, _t7_20);
      _asm256_storeu_pd(C + fi4 + 9700, _t7_21);
      _asm256_storeu_pd(C + fi4 + 9800, _t7_22);
      _asm256_storeu_pd(C + fi4 + 9900, _t7_23);
    }
    _t6_6 = _asm256_loadu_pd(C + fi4 + 9800);
    _t6_4 = _asm256_loadu_pd(C + fi4 + 9600);
    _t6_7 = _asm256_loadu_pd(C + fi4 + 9900);
    _t6_5 = _asm256_loadu_pd(C + fi4 + 9700);
    _t8_6 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi4])));
    _t8_5 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi4 + 1])));
    _t8_4 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi4 + 101])));
    _t8_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 101*fi4 + 2)), _mm256_castpd128_pd256(_mm_load_sd(U + 101*fi4 + 102)), 0);
    _t8_2 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi4 + 202])));
    _t8_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(U + 101*fi4 + 3)), _mm256_castpd128_pd256(_mm_load_sd(U + 101*fi4 + 103))), _mm256_castpd128_pd256(_mm_load_sd(U + 101*fi4 + 203)), 32);
    _t8_0 = _mm256_castpd128_pd256(_mm_load_sd(&(U[101*fi4 + 303])));

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_23 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_24 = _t5_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_25 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_451 = _mm256_add_pd(_t8_24, _t8_25);

    // 4-BLAC: 1x4 / 1x4
    _t8_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_23), _mm256_castpd256_pd128(_t5_451)));

    // AVX Storer:
    _t8_7 = _t8_26;

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_28 = _t8_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_29 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_456 = _mm256_mul_pd(_t8_28, _t8_29);

    // 4-BLAC: 1x4 - 1x4
    _t8_30 = _mm256_sub_pd(_t8_27, _t5_456);

    // AVX Storer:
    _t8_8 = _t8_30;

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_31 = _t8_8;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_32 = _t5_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_33 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_461 = _mm256_add_pd(_t8_32, _t8_33);

    // 4-BLAC: 1x4 / 1x4
    _t8_34 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_31), _mm256_castpd256_pd128(_t5_461)));

    // AVX Storer:
    _t8_8 = _t8_34;

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 96), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_35 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_4, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_36 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_37 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_466 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_mul_pd(_t8_36, _t8_37), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_mul_pd(_t8_36, _t8_37), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_36, _t8_37), _mm256_mul_pd(_t8_36, _t8_37), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_38 = _mm256_sub_pd(_t8_35, _t5_466);

    // AVX Storer:
    _t8_9 = _t8_38;

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_39 = _t8_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_40 = _t5_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_41 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_471 = _mm256_add_pd(_t8_40, _t8_41);

    // 4-BLAC: 1x4 / 1x4
    _t8_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_39), _mm256_castpd256_pd128(_t5_471)));

    // AVX Storer:
    _t8_9 = _t8_42;

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 96), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_43 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_4, _t6_4, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_44 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_45 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_476 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_mul_pd(_t8_44, _t8_45), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_mul_pd(_t8_44, _t8_45), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_44, _t8_45), _mm256_mul_pd(_t8_44, _t8_45), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_46 = _mm256_sub_pd(_t8_43, _t5_476);

    // AVX Storer:
    _t8_10 = _t8_46;

    // Generating : X[100,100] = S(h(1, 100, 96), ( G(h(1, 100, 96), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 96), L[100,100],h(1, 100, 96)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_47 = _t8_10;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_48 = _t5_39;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_49 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_481 = _mm256_add_pd(_t8_48, _t8_49);

    // 4-BLAC: 1x4 / 1x4
    _t8_50 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_47), _mm256_castpd256_pd128(_t5_481)));

    // AVX Storer:
    _t8_10 = _t8_50;

    // Generating : X[100,100] = S(h(3, 100, 97), ( G(h(3, 100, 97), X[100,100],h(4, 100, fi4)) - ( G(h(3, 100, 97), L[100,100],h(1, 100, 96)) * G(h(1, 100, 96), X[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t8_51 = _t6_5;
    _t8_52 = _t6_6;
    _t8_53 = _t6_7;
    _t8_54 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_55 = _t5_31;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t5_488 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_55, _t8_55, 32), _mm256_permute2f128_pd(_t8_55, _t8_55, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));
    _t5_489 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_55, _t8_55, 32), _mm256_permute2f128_pd(_t8_55, _t8_55, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));
    _t5_490 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_55, _t8_55, 49), _mm256_permute2f128_pd(_t8_55, _t8_55, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));
    _t5_491 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_55, _t8_55, 49), _mm256_permute2f128_pd(_t8_55, _t8_55, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_7, _t8_8), _mm256_unpacklo_pd(_t8_9, _t8_10), 32));

    // 4-BLAC: 4x4 - 4x4
    _t8_56 = _mm256_sub_pd(_t8_51, _t5_488);
    _t8_57 = _mm256_sub_pd(_t8_52, _t5_489);
    _t8_58 = _mm256_sub_pd(_t8_53, _t5_490);
    _t8_59 = _mm256_sub_pd(_t8_54, _t5_491);

    // AVX Storer:
    _t6_5 = _t8_56;
    _t6_6 = _t8_57;
    _t6_7 = _t8_58;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_60 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_61 = _t5_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_62 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_499 = _mm256_add_pd(_t8_61, _t8_62);

    // 4-BLAC: 1x4 / 1x4
    _t8_63 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_60), _mm256_castpd256_pd128(_t5_499)));

    // AVX Storer:
    _t8_11 = _t8_63;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_64 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_65 = _t8_11;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_66 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_504 = _mm256_mul_pd(_t8_65, _t8_66);

    // 4-BLAC: 1x4 - 1x4
    _t8_67 = _mm256_sub_pd(_t8_64, _t5_504);

    // AVX Storer:
    _t8_12 = _t8_67;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_68 = _t8_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_69 = _t5_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_70 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_509 = _mm256_add_pd(_t8_69, _t8_70);

    // 4-BLAC: 1x4 / 1x4
    _t8_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_68), _mm256_castpd256_pd128(_t5_509)));

    // AVX Storer:
    _t8_12 = _t8_71;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 97), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_72 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_5, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_73 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_74 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_514 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_mul_pd(_t8_73, _t8_74), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_mul_pd(_t8_73, _t8_74), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_73, _t8_74), _mm256_mul_pd(_t8_73, _t8_74), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_75 = _mm256_sub_pd(_t8_72, _t5_514);

    // AVX Storer:
    _t8_13 = _t8_75;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_76 = _t8_13;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_77 = _t5_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_78 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_519 = _mm256_add_pd(_t8_77, _t8_78);

    // 4-BLAC: 1x4 / 1x4
    _t8_79 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_76), _mm256_castpd256_pd128(_t5_519)));

    // AVX Storer:
    _t8_13 = _t8_79;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 97), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_80 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_5, _t6_5, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_81 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_82 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_524 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_81, _t8_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_81, _t8_82), _mm256_mul_pd(_t8_81, _t8_82), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_81, _t8_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_81, _t8_82), _mm256_mul_pd(_t8_81, _t8_82), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_81, _t8_82), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_81, _t8_82), _mm256_mul_pd(_t8_81, _t8_82), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_83 = _mm256_sub_pd(_t8_80, _t5_524);

    // AVX Storer:
    _t8_14 = _t8_83;

    // Generating : X[100,100] = S(h(1, 100, 97), ( G(h(1, 100, 97), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 97), L[100,100],h(1, 100, 97)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_84 = _t8_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_85 = _t5_30;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_86 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_529 = _mm256_add_pd(_t8_85, _t8_86);

    // 4-BLAC: 1x4 / 1x4
    _t8_87 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_84), _mm256_castpd256_pd128(_t5_529)));

    // AVX Storer:
    _t8_14 = _t8_87;

    // Generating : X[100,100] = S(h(2, 100, 98), ( G(h(2, 100, 98), X[100,100],h(4, 100, fi4)) - ( G(h(2, 100, 98), L[100,100],h(1, 100, 97)) * G(h(1, 100, 97), X[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t8_88 = _t6_6;
    _t8_89 = _t6_7;
    _t8_90 = _mm256_setzero_pd();
    _t8_91 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_92 = _t5_29;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t5_536 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_92, _t8_92, 32), _mm256_permute2f128_pd(_t8_92, _t8_92, 32), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _t8_14), 32));
    _t5_537 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_92, _t8_92, 32), _mm256_permute2f128_pd(_t8_92, _t8_92, 32), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _t8_14), 32));
    _t5_538 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_92, _t8_92, 49), _mm256_permute2f128_pd(_t8_92, _t8_92, 49), 0), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _t8_14), 32));
    _t5_539 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t8_92, _t8_92, 49), _mm256_permute2f128_pd(_t8_92, _t8_92, 49), 15), _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_11, _t8_12), _mm256_unpacklo_pd(_t8_13, _t8_14), 32));

    // 4-BLAC: 4x4 - 4x4
    _t8_93 = _mm256_sub_pd(_t8_88, _t5_536);
    _t8_94 = _mm256_sub_pd(_t8_89, _t5_537);
    _t8_95 = _mm256_sub_pd(_t8_90, _t5_538);
    _t8_96 = _mm256_sub_pd(_t8_91, _t5_539);

    // AVX Storer:
    _t6_6 = _t8_93;
    _t6_7 = _t8_94;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_97 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_98 = _t5_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_99 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_547 = _mm256_add_pd(_t8_98, _t8_99);

    // 4-BLAC: 1x4 / 1x4
    _t8_100 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_97), _mm256_castpd256_pd128(_t5_547)));

    // AVX Storer:
    _t8_15 = _t8_100;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_101 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_102 = _t8_15;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_103 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_552 = _mm256_mul_pd(_t8_102, _t8_103);

    // 4-BLAC: 1x4 - 1x4
    _t8_104 = _mm256_sub_pd(_t8_101, _t5_552);

    // AVX Storer:
    _t8_16 = _t8_104;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_105 = _t8_16;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_106 = _t5_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_107 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_557 = _mm256_add_pd(_t8_106, _t8_107);

    // 4-BLAC: 1x4 / 1x4
    _t8_108 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_105), _mm256_castpd256_pd128(_t5_557)));

    // AVX Storer:
    _t8_16 = _t8_108;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 98), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_109 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_6, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_110 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_15, _t8_16), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_111 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_562 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_110, _t8_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_110, _t8_111), _mm256_mul_pd(_t8_110, _t8_111), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_110, _t8_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_110, _t8_111), _mm256_mul_pd(_t8_110, _t8_111), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_110, _t8_111), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_110, _t8_111), _mm256_mul_pd(_t8_110, _t8_111), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_112 = _mm256_sub_pd(_t8_109, _t5_562);

    // AVX Storer:
    _t8_17 = _t8_112;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_113 = _t8_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_114 = _t5_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_115 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_567 = _mm256_add_pd(_t8_114, _t8_115);

    // 4-BLAC: 1x4 / 1x4
    _t8_116 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_113), _mm256_castpd256_pd128(_t5_567)));

    // AVX Storer:
    _t8_17 = _t8_116;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 98), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_117 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_6, _t6_6, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_118 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_15, _t8_16), _mm256_unpacklo_pd(_t8_17, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_119 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_572 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_118, _t8_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_118, _t8_119), _mm256_mul_pd(_t8_118, _t8_119), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_118, _t8_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_118, _t8_119), _mm256_mul_pd(_t8_118, _t8_119), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_118, _t8_119), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_118, _t8_119), _mm256_mul_pd(_t8_118, _t8_119), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_120 = _mm256_sub_pd(_t8_117, _t5_572);

    // AVX Storer:
    _t8_18 = _t8_120;

    // Generating : X[100,100] = S(h(1, 100, 98), ( G(h(1, 100, 98), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 98), L[100,100],h(1, 100, 98)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_121 = _t8_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_122 = _t5_28;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_123 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_577 = _mm256_add_pd(_t8_122, _t8_123);

    // 4-BLAC: 1x4 / 1x4
    _t8_124 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_121), _mm256_castpd256_pd128(_t5_577)));

    // AVX Storer:
    _t8_18 = _t8_124;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(4, 100, fi4)) - ( G(h(1, 100, 99), L[100,100],h(1, 100, 98)) Kro G(h(1, 100, 98), X[100,100],h(4, 100, fi4)) ) ),h(4, 100, fi4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_125 = _t5_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t5_93 = _mm256_mul_pd(_t8_125, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_15, _t8_16), _mm256_unpacklo_pd(_t8_17, _t8_18), 32));

    // 4-BLAC: 1x4 - 1x4
    _t6_7 = _mm256_sub_pd(_t6_7, _t5_93);

    // AVX Storer:

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4), U[100,100],h(1, 100, fi4)) ) ),h(1, 100, fi4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_126 = _mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_127 = _t5_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_128 = _t8_6;

    // 4-BLAC: 1x4 + 1x4
    _t5_583 = _mm256_add_pd(_t8_127, _t8_128);

    // 4-BLAC: 1x4 / 1x4
    _t8_129 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_126), _mm256_castpd256_pd128(_t5_583)));

    // AVX Storer:
    _t8_19 = _t8_129;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 1)) - ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4)) Kro G(h(1, 100, fi4), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_130 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_131 = _t8_19;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_132 = _t8_5;

    // 4-BLAC: 1x4 Kro 1x4
    _t5_588 = _mm256_mul_pd(_t8_131, _t8_132);

    // 4-BLAC: 1x4 - 1x4
    _t8_133 = _mm256_sub_pd(_t8_130, _t5_588);

    // AVX Storer:
    _t8_20 = _t8_133;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 1)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4 + 1), U[100,100],h(1, 100, fi4 + 1)) ) ),h(1, 100, fi4 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_134 = _t8_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_135 = _t5_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_136 = _t8_4;

    // 4-BLAC: 1x4 + 1x4
    _t5_593 = _mm256_add_pd(_t8_135, _t8_136);

    // 4-BLAC: 1x4 / 1x4
    _t8_137 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_134), _mm256_castpd256_pd128(_t5_593)));

    // AVX Storer:
    _t8_20 = _t8_137;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 2)) - ( G(h(1, 100, 99), X[100,100],h(2, 100, fi4)) * G(h(2, 100, fi4), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_138 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t6_7, 4), 129);

    // AVX Loader:

    // 1x2 -> 1x4
    _t8_139 = _mm256_blend_pd(_mm256_unpacklo_pd(_t8_19, _t8_20), _mm256_setzero_pd(), 12);

    // AVX Loader:

    // 2x1 -> 4x1
    _t8_140 = _t8_3;

    // 4-BLAC: 1x4 * 4x1
    _t5_598 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_mul_pd(_t8_139, _t8_140), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_mul_pd(_t8_139, _t8_140), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_139, _t8_140), _mm256_mul_pd(_t8_139, _t8_140), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_141 = _mm256_sub_pd(_t8_138, _t5_598);

    // AVX Storer:
    _t8_21 = _t8_141;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 2)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4 + 2), U[100,100],h(1, 100, fi4 + 2)) ) ),h(1, 100, fi4 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_142 = _t8_21;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_143 = _t5_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_144 = _t8_2;

    // 4-BLAC: 1x4 + 1x4
    _t5_603 = _mm256_add_pd(_t8_143, _t8_144);

    // 4-BLAC: 1x4 / 1x4
    _t8_145 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_142), _mm256_castpd256_pd128(_t5_603)));

    // AVX Storer:
    _t8_21 = _t8_145;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 3)) - ( G(h(1, 100, 99), X[100,100],h(3, 100, fi4)) * G(h(3, 100, fi4), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_146 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t6_7, _t6_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x3 -> 1x4
    _t8_147 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t8_19, _t8_20), _mm256_unpacklo_pd(_t8_21, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 3x1 -> 4x1
    _t8_148 = _t8_1;

    // 4-BLAC: 1x4 * 4x1
    _t5_608 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t8_147, _t8_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_147, _t8_148), _mm256_mul_pd(_t8_147, _t8_148), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t8_147, _t8_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_147, _t8_148), _mm256_mul_pd(_t8_147, _t8_148), 129)), _mm256_add_pd(_mm256_mul_pd(_t8_147, _t8_148), _mm256_permute2f128_pd(_mm256_mul_pd(_t8_147, _t8_148), _mm256_mul_pd(_t8_147, _t8_148), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t8_149 = _mm256_sub_pd(_t8_146, _t5_608);

    // AVX Storer:
    _t8_22 = _t8_149;

    // Generating : X[100,100] = S(h(1, 100, 99), ( G(h(1, 100, 99), X[100,100],h(1, 100, fi4 + 3)) Div ( G(h(1, 100, 99), L[100,100],h(1, 100, 99)) + G(h(1, 100, fi4 + 3), U[100,100],h(1, 100, fi4 + 3)) ) ),h(1, 100, fi4 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_150 = _t8_22;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_151 = _t5_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t8_152 = _t8_0;

    // 4-BLAC: 1x4 + 1x4
    _t5_613 = _mm256_add_pd(_t8_151, _t8_152);

    // 4-BLAC: 1x4 / 1x4
    _t8_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t8_150), _mm256_castpd256_pd128(_t5_613)));

    // AVX Storer:
    _t8_22 = _t8_153;
    _mm_store_sd(&(C[fi4 + 9600]), _mm256_castpd256_pd128(_t8_7));
    _mm_store_sd(&(C[fi4 + 9601]), _mm256_castpd256_pd128(_t8_8));
    _mm_store_sd(&(C[fi4 + 9602]), _mm256_castpd256_pd128(_t8_9));
    _mm_store_sd(&(C[fi4 + 9603]), _mm256_castpd256_pd128(_t8_10));
    _mm_store_sd(&(C[fi4 + 9700]), _mm256_castpd256_pd128(_t8_11));
    _mm_store_sd(&(C[fi4 + 9701]), _mm256_castpd256_pd128(_t8_12));
    _mm_store_sd(&(C[fi4 + 9702]), _mm256_castpd256_pd128(_t8_13));
    _mm_store_sd(&(C[fi4 + 9703]), _mm256_castpd256_pd128(_t8_14));
    _mm_store_sd(&(C[fi4 + 9800]), _mm256_castpd256_pd128(_t8_15));
    _mm_store_sd(&(C[fi4 + 9801]), _mm256_castpd256_pd128(_t8_16));
    _mm_store_sd(&(C[fi4 + 9802]), _mm256_castpd256_pd128(_t8_17));
    _mm_store_sd(&(C[fi4 + 9803]), _mm256_castpd256_pd128(_t8_18));
    _mm_store_sd(&(C[fi4 + 9900]), _mm256_castpd256_pd128(_t8_19));
    _mm_store_sd(&(C[fi4 + 9901]), _mm256_castpd256_pd128(_t8_20));
    _mm_store_sd(&(C[fi4 + 9902]), _mm256_castpd256_pd128(_t8_21));
    _mm_store_sd(&(C[fi4 + 9903]), _mm256_castpd256_pd128(_t8_22));
  }

  _mm_store_sd(&(C[9600]), _mm256_castpd256_pd128(_t5_40));
  _mm_store_sd(&(C[9601]), _mm256_castpd256_pd128(_t5_41));
  _mm_store_sd(&(C[9602]), _mm256_castpd256_pd128(_t5_42));
  _mm_store_sd(&(C[9603]), _mm256_castpd256_pd128(_t5_43));
  _mm_store_sd(&(C[9700]), _mm256_castpd256_pd128(_t5_47));
  _mm_store_sd(&(C[9701]), _mm256_castpd256_pd128(_t5_48));
  _mm_store_sd(&(C[9702]), _mm256_castpd256_pd128(_t5_49));
  _mm_store_sd(&(C[9703]), _mm256_castpd256_pd128(_t5_50));
  _mm_store_sd(&(C[9800]), _mm256_castpd256_pd128(_t5_51));
  _mm_store_sd(&(C[9801]), _mm256_castpd256_pd128(_t5_52));
  _mm_store_sd(&(C[9802]), _mm256_castpd256_pd128(_t5_53));
  _mm_store_sd(&(C[9803]), _mm256_castpd256_pd128(_t5_54));
  _mm_store_sd(&(C[9900]), _mm256_castpd256_pd128(_t5_55));
  _mm_store_sd(&(C[9901]), _mm256_castpd256_pd128(_t5_56));
  _mm_store_sd(&(C[9902]), _mm256_castpd256_pd128(_t5_57));
  _mm_store_sd(&(C[9903]), _mm256_castpd256_pd128(_t5_58));

}
