/*
 * kf_kernel.h
 *
Decl { {'T2019': Matrix[T2019, (1, 52), GenMatAccess], u'B': SquaredMatrix[B, (52, 52), GenMatAccess], u'F': SquaredMatrix[F, (52, 52), GenMatAccess], 'T1982': Matrix[T1982, (1, 52), GenMatAccess], u'H': SquaredMatrix[H, (52, 52), GenMatAccess], u'U0': UpperTriangular[U0, (52, 52), GenMatAccess], u'M5': SquaredMatrix[M5, (52, 52), GenMatAccess], u'M4': Symmetric[M4, (52, 52), USMatAccess], u'M7': SquaredMatrix[M7, (52, 52), GenMatAccess], u'M6': SquaredMatrix[M6, (52, 52), GenMatAccess], u'v4': Matrix[v4, (52, 1), GenMatAccess], u'M0': SquaredMatrix[M0, (52, 52), GenMatAccess], u'M3': Symmetric[M3, (52, 52), USMatAccess], u'M2': SquaredMatrix[M2, (52, 52), GenMatAccess], u'Y': Symmetric[Y, (52, 52), USMatAccess], u'R': Symmetric[R, (52, 52), USMatAccess], u'U': UpperTriangular[U, (52, 52), GenMatAccess], u'M8': SquaredMatrix[M8, (52, 52), GenMatAccess], u'v0': Matrix[v0, (52, 1), GenMatAccess], u'u': Matrix[u, (52, 1), GenMatAccess], u'P': Symmetric[P, (52, 52), USMatAccess], u'v2': Matrix[v2, (52, 1), GenMatAccess], u'v1': Matrix[v1, (52, 1), GenMatAccess], u'v3': Matrix[v3, (52, 1), GenMatAccess], u'Q': Symmetric[Q, (52, 52), USMatAccess], u'x': Matrix[x, (52, 1), GenMatAccess], u'y': Matrix[y, (52, 1), GenMatAccess], u'M1': SquaredMatrix[M1, (52, 52), GenMatAccess], u'z': Matrix[z, (52, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt': {'m0': 'm03.ll'}, 'ldiv_utn_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt_m03_Assign_Mul_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_ldiv_utn_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[52,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), x[52,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), u[52,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[52,52] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), P[52,52] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[52,52] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[52,52] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[52,52] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[52,52] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[52,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[52,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), y[52,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[52,52] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), Y[52,52] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[52,52] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[52,52] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[52,52] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[52,52] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[52,52] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[52,52] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[52,52] ) ) )
Eq.ann: {}
Entry 7:
For_{fi35;0;47;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 52, fi35), U[52,52],h(1, 52, fi35)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi35), U[52,52],h(1, 52, fi35)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi35), U[52,52],h(1, 52, fi35)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi35 + 1), U[52,52],h(3, 52, fi35 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi35 + 1), U[52,52],h(3, 52, fi35 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 52, fi35 + 1), U[52,52],h(1, 52, fi35 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi35 + 1), U[52,52],h(1, 52, fi35 + 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi35 + 1), U[52,52],h(1, 52, fi35 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi35 + 2), U[52,52],h(2, 52, fi35 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi35 + 2), U[52,52],h(2, 52, fi35 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) = ( Tile( (1, 1), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) Div Tile( (1, 1), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ) )
Eq.ann: {}
Entry 14:
For_{fi96;0;-fi35 + 44;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 3), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 3), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 3), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi35 + 3), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
 )Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi35 + 48, 52, fi35 + 4), U[52,52],h(-fi35 + 48, 52, fi35 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi35 + 48, 52, fi35 + 4), U[52,52],h(-fi35 + 48, 52, fi35 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi35), U[52,52],h(-fi35 + 48, 52, fi35 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi35), U[52,52],h(-fi35 + 48, 52, fi35 + 4)) ) ) ) )
Eq.ann: {}
 )Entry 8:
Eq: Tile( (1, 1), G(h(1, 52, 48), U[52,52],h(1, 52, 48)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 48), U[52,52],h(1, 52, 48)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1982[1,52],h(1, 52, 48)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 48), U[52,52],h(1, 52, 48)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, 48)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 49), U[52,52],h(3, 52, 49)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 49), U[52,52],h(3, 52, 49)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 52, 49), U[52,52],h(1, 52, 49)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 49), U[52,52],h(1, 52, 49)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1982[1,52],h(1, 52, 49)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 49), U[52,52],h(1, 52, 49)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1982[1,52],h(1, 52, 49)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 50), U[52,52],h(2, 52, 50)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 50), U[52,52],h(2, 52, 50)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 52, 50), U[52,52],h(1, 52, 50)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 50), U[52,52],h(1, 52, 50)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 52, 50), U[52,52],h(1, 52, 51)) ) = ( Tile( (1, 1), G(h(1, 52, 50), U[52,52],h(1, 52, 51)) ) Div Tile( (1, 1), G(h(1, 52, 50), U[52,52],h(1, 52, 50)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), U[52,52],h(1, 52, 51)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), U[52,52],h(1, 52, 51)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), U[52,52],h(1, 52, 51)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), U[52,52],h(1, 52, 51)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 52, 51), U[52,52],h(1, 52, 51)) ) = Sqrt( Tile( (1, 1), G(h(1, 52, 51), U[52,52],h(1, 52, 51)) ) )
Eq.ann: {}
Entry 20:
For_{fi182;0;47;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 52, fi182), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi182), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, fi182), U0[52,52],h(1, 52, fi182)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi182 + 1), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi182 + 1), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182), U0[52,52],h(3, 52, fi182 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 52, fi182 + 1), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi182 + 1), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, fi182 + 1), U0[52,52],h(1, 52, fi182 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi182 + 2), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi182 + 2), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182 + 1), U0[52,52],h(2, 52, fi182 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182 + 1), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 52, fi182 + 2), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi182 + 2), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, fi182 + 2), U0[52,52],h(1, 52, fi182 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182 + 3), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182 + 3), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182 + 2), U0[52,52],h(1, 52, fi182 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi182 + 2), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 52, fi182 + 3), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, fi182 + 3), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, fi182 + 3), U0[52,52],h(1, 52, fi182 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi182 + 48, 52, fi182 + 4), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi182 + 48, 52, fi182 + 4), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi182), U0[52,52],h(-fi182 + 48, 52, fi182 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi182), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 21:
Eq: Tile( (1, 1), G(h(1, 52, 48), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 48), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 48), U0[52,52],h(1, 52, 48)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 49), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 49), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), U0[52,52],h(3, 52, 49)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 52, 49), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 49), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 49), U0[52,52],h(1, 52, 49)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 50), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 50), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), U0[52,52],h(2, 52, 50)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 52, 50), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 50), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 50), U0[52,52],h(1, 52, 50)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), v2[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), v2[52,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), U0[52,52],h(1, 52, 51)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), v2[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 52, 51), v2[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 51), v2[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 51), U0[52,52],h(1, 52, 51)) ) )
Eq.ann: {}
Entry 28:
For_{fi259;0;47;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 52, -fi259 + 51), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, -fi259 + 51), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, -fi259 + 51), U0[52,52],h(1, 52, -fi259 + 51)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, -fi259 + 48), U0[52,52],h(1, 52, -fi259 + 51)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi259 + 51), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 52, -fi259 + 50), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, -fi259 + 50), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, -fi259 + 50), U0[52,52],h(1, 52, -fi259 + 50)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, -fi259 + 48), U0[52,52],h(1, 52, -fi259 + 50)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi259 + 50), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 52, -fi259 + 49), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, -fi259 + 49), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, -fi259 + 49), U0[52,52],h(1, 52, -fi259 + 49)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi259 + 48), U0[52,52],h(1, 52, -fi259 + 49)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi259 + 49), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, -fi259 + 48), U0[52,52],h(1, 52, -fi259 + 48)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi259 + 48, 52, 0), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi259 + 48, 52, 0), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi259 + 48, 52, 0), U0[52,52],h(4, 52, -fi259 + 48)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 52, -fi259 + 48), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 29:
Eq: Tile( (1, 1), G(h(1, 52, 3), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 3), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 3), U0[52,52],h(1, 52, 3)) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U0[52,52],h(1, 52, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 52, 2), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 2), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 2), U0[52,52],h(1, 52, 2)) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U0[52,52],h(1, 52, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 52, 1), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 1), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 1), U0[52,52],h(1, 52, 1)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), v4[52,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), v4[52,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U0[52,52],h(1, 52, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), v4[52,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 52, 0), v4[52,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 52, 0), v4[52,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 52, 0), U0[52,52],h(1, 52, 0)) ) )
Eq.ann: {}
Entry 36:
For_{fi336;0;47;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi336), U0[52,52],h(1, 52, fi336)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi336 + 1), U0[52,52],h(1, 52, fi336 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi336 + 2), U0[52,52],h(1, 52, fi336 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, fi336 + 3), U0[52,52],h(1, 52, fi336 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi355;0;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336), M6[52,52],h(4, 52, fi355)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi336 + 1), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, fi336 + 1), M6[52,52],h(4, 52, fi355)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336), U0[52,52],h(3, 52, fi336 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336), M6[52,52],h(4, 52, fi355)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 1), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 1), M6[52,52],h(4, 52, fi355)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi336 + 2), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, fi336 + 2), M6[52,52],h(4, 52, fi355)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 1), U0[52,52],h(2, 52, fi336 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 1), M6[52,52],h(4, 52, fi355)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 2), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 2), M6[52,52],h(4, 52, fi355)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 3), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 3), M6[52,52],h(4, 52, fi355)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 2), U0[52,52],h(1, 52, fi336 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 2), M6[52,52],h(4, 52, fi355)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 3), M6[52,52],h(4, 52, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, fi336 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, fi336 + 3), M6[52,52],h(4, 52, fi355)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi336 + 48, 52, fi336 + 4), M6[52,52],h(52, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi336 + 48, 52, fi336 + 4), M6[52,52],h(52, 52, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi336), U0[52,52],h(-fi336 + 48, 52, fi336 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 52, fi336), M6[52,52],h(52, 52, 0)) ) ) ) )
Eq.ann: {}
 )Entry 37:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 48)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 48), U0[52,52],h(1, 52, 48)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 49)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 49), U0[52,52],h(1, 52, 49)) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 50)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 50), U0[52,52],h(1, 52, 50)) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 51)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 51), U0[52,52],h(1, 52, 51)) ) )
Eq.ann: {}
Entry 41:
For_{fi402;0;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 48)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), M6[52,52],h(4, 52, fi402)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 49), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 49), M6[52,52],h(4, 52, fi402)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), U0[52,52],h(3, 52, 49)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 48), M6[52,52],h(4, 52, fi402)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 49)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), M6[52,52],h(4, 52, fi402)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 50), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 50), M6[52,52],h(4, 52, fi402)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), U0[52,52],h(2, 52, 50)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 49), M6[52,52],h(4, 52, fi402)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 50)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), M6[52,52],h(4, 52, fi402)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), M6[52,52],h(4, 52, fi402)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), U0[52,52],h(1, 52, 51)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 50), M6[52,52],h(4, 52, fi402)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), M6[52,52],h(4, 52, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 51)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 51), M6[52,52],h(4, 52, fi402)) ) ) )
Eq.ann: {}
 )Entry 42:
For_{fi449;0;47;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 51)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, -fi449 + 51), U0[52,52],h(1, 52, -fi449 + 51)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 50)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, -fi449 + 50), U0[52,52],h(1, 52, -fi449 + 50)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 49)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, -fi449 + 49), U0[52,52],h(1, 52, -fi449 + 49)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 48)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, -fi449 + 48), U0[52,52],h(1, 52, -fi449 + 48)) ) )
Eq.ann: {}
Entry 4:
For_{fi468;0;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 51), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 51)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 51), M8[52,52],h(4, 52, fi468)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, -fi449 + 48), U0[52,52],h(1, 52, -fi449 + 51)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 51), M8[52,52],h(4, 52, fi468)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 50), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 50)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 50), M8[52,52],h(4, 52, fi468)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, -fi449 + 48), U0[52,52],h(1, 52, -fi449 + 50)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 50), M8[52,52],h(4, 52, fi468)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 49), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 49)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 49), M8[52,52],h(4, 52, fi468)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 48), U0[52,52],h(1, 52, -fi449 + 49)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 49), M8[52,52],h(4, 52, fi468)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi449 + 48)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, -fi449 + 48), M8[52,52],h(4, 52, fi468)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi449 + 48, 52, 0), M8[52,52],h(52, 52, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi449 + 48, 52, 0), M8[52,52],h(52, 52, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi449 + 48, 52, 0), U0[52,52],h(4, 52, -fi449 + 48)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 52, -fi449 + 48), M8[52,52],h(52, 52, 0)) ) ) ) )
Eq.ann: {}
 )Entry 43:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 3), U0[52,52],h(1, 52, 3)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 2), U0[52,52],h(1, 52, 2)) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 1), U0[52,52],h(1, 52, 1)) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2019[1,52],h(1, 52, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 52, 0), U0[52,52],h(1, 52, 0)) ) )
Eq.ann: {}
Entry 47:
For_{fi515;0;48;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), M8[52,52],h(4, 52, fi515)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 52, 0), U0[52,52],h(1, 52, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 3), M8[52,52],h(4, 52, fi515)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), M8[52,52],h(4, 52, fi515)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 52, 0), U0[52,52],h(1, 52, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 2), M8[52,52],h(4, 52, fi515)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), M8[52,52],h(4, 52, fi515)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), U0[52,52],h(1, 52, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 1), M8[52,52],h(4, 52, fi515)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2019[1,52],h(1, 52, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 52, 0), M8[52,52],h(4, 52, fi515)) ) ) )
Eq.ann: {}
 )Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), x[52,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[52,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), v0[52,1] ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), Tile( (4, 4), P[52,52] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[52,52] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[52,52] ) ) * Tile( (1, 1), Tile( (4, 4), M1[52,52] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 52
#define PARAM1 52
#define PARAM2 52

#define ERRTHRESH 1e-5

#define SOFTERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27, _t13_28, _t13_29, _t13_30, _t13_31,
	_t13_32, _t13_33, _t13_34, _t13_35, _t13_36, _t13_37, _t13_38, _t13_39,
	_t13_40, _t13_41, _t13_42, _t13_43;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27, _t25_28, _t25_29, _t25_30, _t25_31;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27, _t35_28, _t35_29, _t35_30, _t35_31;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11, _t38_12, _t38_13, _t38_14, _t38_15,
	_t38_16, _t38_17, _t38_18, _t38_19, _t38_20, _t38_21, _t38_22, _t38_23;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31,
	_t42_32, _t42_33, _t42_34, _t42_35, _t42_36, _t42_37, _t42_38, _t42_39,
	_t42_40, _t42_41, _t42_42, _t42_43;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31,
	_t44_32, _t44_33, _t44_34, _t44_35, _t44_36, _t44_37, _t44_38, _t44_39,
	_t44_40, _t44_41, _t44_42, _t44_43;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27, _t45_28, _t45_29, _t45_30, _t45_31,
	_t45_32, _t45_33, _t45_34, _t45_35;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11, _t46_12, _t46_13, _t46_14, _t46_15;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27, _t47_28, _t47_29, _t47_30, _t47_31;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35, _t48_36, _t48_37, _t48_38, _t48_39,
	_t48_40, _t48_41, _t48_42, _t48_43, _t48_44, _t48_45, _t48_46, _t48_47,
	_t48_48, _t48_49, _t48_50, _t48_51, _t48_52, _t48_53, _t48_54, _t48_55,
	_t48_56, _t48_57, _t48_58, _t48_59, _t48_60, _t48_61, _t48_62, _t48_63,
	_t48_64, _t48_65, _t48_66, _t48_67, _t48_68, _t48_69, _t48_70, _t48_71,
	_t48_72, _t48_73, _t48_74, _t48_75, _t48_76, _t48_77, _t48_78, _t48_79,
	_t48_80;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26, _t49_27, _t49_28, _t49_29, _t49_30, _t49_31,
	_t49_32, _t49_33, _t49_34, _t49_35, _t49_36, _t49_37, _t49_38;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26, _t53_27, _t53_28, _t53_29, _t53_30, _t53_31,
	_t53_32, _t53_33, _t53_34, _t53_35, _t53_36, _t53_37, _t53_38, _t53_39,
	_t53_40, _t53_41, _t53_42, _t53_43, _t53_44, _t53_45, _t53_46, _t53_47,
	_t53_48, _t53_49, _t53_50, _t53_51, _t53_52, _t53_53, _t53_54, _t53_55,
	_t53_56, _t53_57, _t53_58, _t53_59, _t53_60, _t53_61, _t53_62, _t53_63,
	_t53_64, _t53_65, _t53_66, _t53_67, _t53_68, _t53_69, _t53_70, _t53_71,
	_t53_72, _t53_73, _t53_74, _t53_75, _t53_76, _t53_77, _t53_78, _t53_79,
	_t53_80, _t53_81, _t53_82, _t53_83, _t53_84, _t53_85, _t53_86, _t53_87,
	_t53_88, _t53_89, _t53_90, _t53_91, _t53_92, _t53_93, _t53_94, _t53_95,
	_t53_96, _t53_97, _t53_98, _t53_99, _t53_100, _t53_101, _t53_102, _t53_103,
	_t53_104, _t53_105, _t53_106, _t53_107, _t53_108, _t53_109, _t53_110, _t53_111,
	_t53_112, _t53_113, _t53_114, _t53_115, _t53_116, _t53_117, _t53_118, _t53_119,
	_t53_120, _t53_121, _t53_122, _t53_123, _t53_124, _t53_125, _t53_126, _t53_127,
	_t53_128, _t53_129, _t53_130, _t53_131, _t53_132, _t53_133, _t53_134, _t53_135,
	_t53_136, _t53_137, _t53_138, _t53_139, _t53_140, _t53_141, _t53_142, _t53_143,
	_t53_144, _t53_145, _t53_146, _t53_147, _t53_148, _t53_149, _t53_150, _t53_151,
	_t53_152, _t53_153, _t53_154, _t53_155, _t53_156, _t53_157, _t53_158, _t53_159,
	_t53_160, _t53_161, _t53_162, _t53_163, _t53_164, _t53_165, _t53_166, _t53_167,
	_t53_168, _t53_169, _t53_170, _t53_171, _t53_172, _t53_173, _t53_174, _t53_175,
	_t53_176, _t53_177, _t53_178, _t53_179, _t53_180, _t53_181, _t53_182, _t53_183,
	_t53_184, _t53_185, _t53_186, _t53_187, _t53_188, _t53_189, _t53_190, _t53_191,
	_t53_192, _t53_193, _t53_194, _t53_195, _t53_196, _t53_197, _t53_198, _t53_199,
	_t53_200, _t53_201, _t53_202, _t53_203, _t53_204, _t53_205, _t53_206, _t53_207,
	_t53_208;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39,
	_t54_40, _t54_41, _t54_42;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11, _t55_12, _t55_13;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3, _t56_4, _t56_5, _t56_6, _t56_7,
	_t56_8, _t56_9, _t56_10, _t56_11, _t56_12, _t56_13, _t56_14, _t56_15,
	_t56_16, _t56_17, _t56_18, _t56_19, _t56_20, _t56_21, _t56_22, _t56_23,
	_t56_24, _t56_25, _t56_26, _t56_27, _t56_28, _t56_29, _t56_30, _t56_31,
	_t56_32, _t56_33, _t56_34, _t56_35;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11, _t57_12, _t57_13, _t57_14, _t57_15,
	_t57_16, _t57_17, _t57_18, _t57_19, _t57_20, _t57_21, _t57_22, _t57_23,
	_t57_24, _t57_25, _t57_26, _t57_27, _t57_28, _t57_29, _t57_30, _t57_31,
	_t57_32, _t57_33, _t57_34, _t57_35, _t57_36, _t57_37, _t57_38, _t57_39;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26, _t59_27, _t59_28, _t59_29, _t59_30, _t59_31,
	_t59_32, _t59_33, _t59_34, _t59_35, _t59_36, _t59_37, _t59_38, _t59_39;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11, _t60_12, _t60_13, _t60_14, _t60_15,
	_t60_16, _t60_17, _t60_18, _t60_19, _t60_20, _t60_21, _t60_22, _t60_23,
	_t60_24, _t60_25, _t60_26, _t60_27, _t60_28, _t60_29, _t60_30, _t60_31,
	_t60_32, _t60_33, _t60_34, _t60_35, _t60_36, _t60_37, _t60_38, _t60_39,
	_t60_40, _t60_41, _t60_42, _t60_43, _t60_44, _t60_45, _t60_46, _t60_47,
	_t60_48, _t60_49, _t60_50, _t60_51, _t60_52, _t60_53, _t60_54, _t60_55,
	_t60_56, _t60_57, _t60_58, _t60_59, _t60_60, _t60_61;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11, _t61_12, _t61_13, _t61_14, _t61_15,
	_t61_16, _t61_17, _t61_18, _t61_19, _t61_20, _t61_21, _t61_22, _t61_23,
	_t61_24, _t61_25, _t61_26;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6, _t63_7,
	_t63_8, _t63_9, _t63_10, _t63_11, _t63_12, _t63_13, _t63_14, _t63_15,
	_t63_16, _t63_17, _t63_18, _t63_19, _t63_20, _t63_21, _t63_22, _t63_23,
	_t63_24, _t63_25, _t63_26, _t63_27, _t63_28, _t63_29, _t63_30, _t63_31,
	_t63_32, _t63_33, _t63_34, _t63_35, _t63_36, _t63_37, _t63_38, _t63_39,
	_t63_40, _t63_41, _t63_42, _t63_43, _t63_44, _t63_45, _t63_46, _t63_47,
	_t63_48, _t63_49, _t63_50, _t63_51, _t63_52, _t63_53, _t63_54;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15,
	_t64_16, _t64_17, _t64_18, _t64_19, _t64_20, _t64_21, _t64_22, _t64_23,
	_t64_24, _t64_25, _t64_26;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15,
	_t65_16, _t65_17, _t65_18, _t65_19, _t65_20, _t65_21, _t65_22, _t65_23,
	_t65_24, _t65_25, _t65_26, _t65_27, _t65_28, _t65_29, _t65_30, _t65_31,
	_t65_32, _t65_33, _t65_34, _t65_35, _t65_36, _t65_37, _t65_38, _t65_39,
	_t65_40, _t65_41, _t65_42, _t65_43, _t65_44, _t65_45, _t65_46, _t65_47,
	_t65_48, _t65_49, _t65_50, _t65_51, _t65_52, _t65_53, _t65_54, _t65_55,
	_t65_56, _t65_57, _t65_58;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15,
	_t67_16, _t67_17, _t67_18, _t67_19, _t67_20, _t67_21, _t67_22, _t67_23,
	_t67_24, _t67_25, _t67_26, _t67_27;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15,
	_t68_16, _t68_17, _t68_18, _t68_19, _t68_20, _t68_21, _t68_22, _t68_23,
	_t68_24, _t68_25, _t68_26, _t68_27, _t68_28, _t68_29, _t68_30, _t68_31,
	_t68_32, _t68_33, _t68_34, _t68_35, _t68_36, _t68_37, _t68_38, _t68_39,
	_t68_40, _t68_41, _t68_42, _t68_43, _t68_44, _t68_45, _t68_46, _t68_47,
	_t68_48, _t68_49, _t68_50, _t68_51, _t68_52;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23,
	_t69_24, _t69_25, _t69_26;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6;
  __m256d _t71_0, _t71_1, _t71_2, _t71_3, _t71_4, _t71_5, _t71_6;
  __m256d _t72_0, _t72_1, _t72_2, _t72_3, _t72_4, _t72_5, _t72_6, _t72_7,
	_t72_8, _t72_9, _t72_10, _t72_11, _t72_12, _t72_13, _t72_14, _t72_15,
	_t72_16, _t72_17, _t72_18, _t72_19, _t72_20, _t72_21, _t72_22, _t72_23,
	_t72_24, _t72_25, _t72_26, _t72_27, _t72_28, _t72_29, _t72_30, _t72_31,
	_t72_32, _t72_33, _t72_34, _t72_35, _t72_36, _t72_37, _t72_38, _t72_39;
  __m256d _t73_0, _t73_1, _t73_2, _t73_3, _t73_4, _t73_5, _t73_6, _t73_7,
	_t73_8, _t73_9, _t73_10, _t73_11, _t73_12, _t73_13, _t73_14, _t73_15;
  __m256d _t74_0, _t74_1, _t74_2, _t74_3, _t74_4, _t74_5, _t74_6, _t74_7,
	_t74_8, _t74_9, _t74_10, _t74_11, _t74_12, _t74_13, _t74_14, _t74_15,
	_t74_16, _t74_17, _t74_18, _t74_19, _t74_20, _t74_21, _t74_22, _t74_23,
	_t74_24, _t74_25, _t74_26, _t74_27, _t74_28, _t74_29, _t74_30, _t74_31;
  __m256d _t75_0, _t75_1, _t75_2, _t75_3, _t75_4, _t75_5, _t75_6, _t75_7,
	_t75_8, _t75_9, _t75_10, _t75_11, _t75_12, _t75_13, _t75_14, _t75_15,
	_t75_16, _t75_17, _t75_18, _t75_19, _t75_20, _t75_21, _t75_22, _t75_23,
	_t75_24, _t75_25, _t75_26, _t75_27, _t75_28, _t75_29, _t75_30, _t75_31;
  __m256d _t76_0, _t76_1, _t76_2, _t76_3, _t76_4, _t76_5, _t76_6, _t76_7,
	_t76_8, _t76_9, _t76_10, _t76_11;
  __m256d _t77_0, _t77_1, _t77_2, _t77_3, _t77_4, _t77_5, _t77_6, _t77_7,
	_t77_8, _t77_9, _t77_10, _t77_11, _t77_12, _t77_13, _t77_14, _t77_15,
	_t77_16, _t77_17, _t77_18, _t77_19, _t77_20, _t77_21, _t77_22, _t77_23,
	_t77_24, _t77_25, _t77_26, _t77_27;


  // Generating : y[52,1] = ( ( Sum_{i0} ( S(h(4, 52, i0), ( ( G(h(4, 52, i0), F[52,52],h(4, 52, 0)) * G(h(4, 52, 0), x[52,1],h(1, 1, 0)) ) + ( G(h(4, 52, i0), B[52,52],h(4, 52, 0)) * G(h(4, 52, 0), u[52,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 52, i0), ( G(h(4, 52, i0), F[52,52],h(4, 52, k2)) * G(h(4, 52, k2), x[52,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) ) + Sum_{k3} ( Sum_{i0} ( $(h(4, 52, i0), ( G(h(4, 52, i0), B[52,52],h(4, 52, k3)) * G(h(4, 52, k3), u[52,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 51; i0+=4 ) {
    _t0_9 = _asm256_loadu_pd(F + 52*i0);
    _t0_8 = _asm256_loadu_pd(F + 52*i0 + 52);
    _t0_7 = _asm256_loadu_pd(F + 52*i0 + 104);
    _t0_6 = _asm256_loadu_pd(F + 52*i0 + 156);
    _t0_5 = _asm256_loadu_pd(x);
    _t0_4 = _asm256_loadu_pd(B + 52*i0);
    _t0_3 = _asm256_loadu_pd(B + 52*i0 + 52);
    _t0_2 = _asm256_loadu_pd(B + 52*i0 + 104);
    _t0_1 = _asm256_loadu_pd(B + 52*i0 + 156);
    _t0_0 = _asm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:
    _asm256_storeu_pd(y + i0, _t0_10);
  }


  for( int k2 = 4; k2 <= 51; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 51; i0+=4 ) {
      _t1_4 = _asm256_loadu_pd(F + 52*i0 + k2);
      _t1_3 = _asm256_loadu_pd(F + 52*i0 + k2 + 52);
      _t1_2 = _asm256_loadu_pd(F + 52*i0 + k2 + 104);
      _t1_1 = _asm256_loadu_pd(F + 52*i0 + k2 + 156);
      _t1_0 = _asm256_loadu_pd(x + k2);
      _t1_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t1_5 = _mm256_add_pd(_t1_5, _t1_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t1_5);
    }
  }


  for( int k3 = 4; k3 <= 51; k3+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 51; i0+=4 ) {
      _t2_4 = _asm256_loadu_pd(B + 52*i0 + k3);
      _t2_3 = _asm256_loadu_pd(B + 52*i0 + k3 + 52);
      _t2_2 = _asm256_loadu_pd(B + 52*i0 + k3 + 104);
      _t2_1 = _asm256_loadu_pd(B + 52*i0 + k3 + 156);
      _t2_0 = _asm256_loadu_pd(u + k3);
      _t2_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t2_5 = _mm256_add_pd(_t2_5, _t2_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t2_5);
    }
  }

  _t3_3 = _asm256_loadu_pd(P);
  _t3_2 = _mm256_maskload_pd(P + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[52,52] = ( ( Sum_{k2} ( ( S(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, 0)) * G(h(4, 52, 0), P[52,52],h(4, 52, 0)) ),h(4, 52, 0)) + Sum_{i0} ( S(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, 0)) * G(h(4, 52, 0), P[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, k3)) * T( G(h(4, 52, i0), P[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) + $(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, k3)) * G(h(4, 52, k3), P[52,52],h(4, 52, k3)) ),h(4, 52, k3)) ) + Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, k3)) * G(h(4, 52, k3), P[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, 48)) * T( G(h(4, 52, i0), P[52,52],h(4, 52, 48)) ) ),h(4, 52, i0)) ) + $(h(4, 52, k2), ( G(h(4, 52, k2), F[52,52],h(4, 52, 48)) * G(h(4, 52, 48), P[52,52],h(4, 52, 48)) ),h(4, 52, 48)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_4 = _t3_3;
  _t3_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int k2 = 0; k2 <= 51; k2+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 52*k2);
    _t4_14 = _mm256_broadcast_sd(F + 52*k2 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 52*k2 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 52*k2 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 52*k2 + 52);
    _t4_10 = _mm256_broadcast_sd(F + 52*k2 + 53);
    _t4_9 = _mm256_broadcast_sd(F + 52*k2 + 54);
    _t4_8 = _mm256_broadcast_sd(F + 52*k2 + 55);
    _t4_7 = _mm256_broadcast_sd(F + 52*k2 + 104);
    _t4_6 = _mm256_broadcast_sd(F + 52*k2 + 105);
    _t4_5 = _mm256_broadcast_sd(F + 52*k2 + 106);
    _t4_4 = _mm256_broadcast_sd(F + 52*k2 + 107);
    _t4_3 = _mm256_broadcast_sd(F + 52*k2 + 156);
    _t4_2 = _mm256_broadcast_sd(F + 52*k2 + 157);
    _t4_1 = _mm256_broadcast_sd(F + 52*k2 + 158);
    _t4_0 = _mm256_broadcast_sd(F + 52*k2 + 159);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_4), _mm256_mul_pd(_t4_14, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_6), _mm256_mul_pd(_t4_12, _t3_7)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_4), _mm256_mul_pd(_t4_10, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_6), _mm256_mul_pd(_t4_8, _t3_7)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_4), _mm256_mul_pd(_t4_6, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_6), _mm256_mul_pd(_t4_4, _t3_7)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_4), _mm256_mul_pd(_t4_2, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_6), _mm256_mul_pd(_t4_0, _t3_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 51; i0+=4 ) {
      _t5_3 = _asm256_loadu_pd(P + i0);
      _t5_2 = _asm256_loadu_pd(P + i0 + 52);
      _t5_1 = _asm256_loadu_pd(P + i0 + 104);
      _t5_0 = _asm256_loadu_pd(P + i0 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t5_3), _mm256_mul_pd(_t4_14, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t5_1), _mm256_mul_pd(_t4_12, _t5_0)));
      _t5_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t5_3), _mm256_mul_pd(_t4_10, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t5_1), _mm256_mul_pd(_t4_8, _t5_0)));
      _t5_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t5_3), _mm256_mul_pd(_t4_6, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t5_1), _mm256_mul_pd(_t4_4, _t5_0)));
      _t5_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t5_3), _mm256_mul_pd(_t4_2, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t5_1), _mm256_mul_pd(_t4_0, _t5_0)));

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 52*k2, _t5_4);
      _asm256_storeu_pd(M0 + i0 + 52*k2 + 52, _t5_5);
      _asm256_storeu_pd(M0 + i0 + 52*k2 + 104, _t5_6);
      _asm256_storeu_pd(M0 + i0 + 52*k2 + 156, _t5_7);
    }
    _asm256_storeu_pd(M0 + 52*k2, _t4_16);
    _asm256_storeu_pd(M0 + 52*k2 + 52, _t4_17);
    _asm256_storeu_pd(M0 + 52*k2 + 104, _t4_18);
    _asm256_storeu_pd(M0 + 52*k2 + 156, _t4_19);
  }


  for( int k3 = 4; k3 <= 47; k3+=4 ) {
    _t6_3 = _asm256_loadu_pd(P + 53*k3);
    _t6_2 = _mm256_maskload_pd(P + 53*k3 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t6_1 = _mm256_maskload_pd(P + 53*k3 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t6_0 = _mm256_maskload_pd(P + 53*k3 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t6_4 = _t6_3;
    _t6_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
    _t6_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
    _t6_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

    for( int k2 = 0; k2 <= 51; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t7_19 = _mm256_broadcast_sd(F + 52*k2 + k3);
        _t7_18 = _mm256_broadcast_sd(F + 52*k2 + k3 + 1);
        _t7_17 = _mm256_broadcast_sd(F + 52*k2 + k3 + 2);
        _t7_16 = _mm256_broadcast_sd(F + 52*k2 + k3 + 3);
        _t7_15 = _mm256_broadcast_sd(F + 52*k2 + k3 + 52);
        _t7_14 = _mm256_broadcast_sd(F + 52*k2 + k3 + 53);
        _t7_13 = _mm256_broadcast_sd(F + 52*k2 + k3 + 54);
        _t7_12 = _mm256_broadcast_sd(F + 52*k2 + k3 + 55);
        _t7_11 = _mm256_broadcast_sd(F + 52*k2 + k3 + 104);
        _t7_10 = _mm256_broadcast_sd(F + 52*k2 + k3 + 105);
        _t7_9 = _mm256_broadcast_sd(F + 52*k2 + k3 + 106);
        _t7_8 = _mm256_broadcast_sd(F + 52*k2 + k3 + 107);
        _t7_7 = _mm256_broadcast_sd(F + 52*k2 + k3 + 156);
        _t7_6 = _mm256_broadcast_sd(F + 52*k2 + k3 + 157);
        _t7_5 = _mm256_broadcast_sd(F + 52*k2 + k3 + 158);
        _t7_4 = _mm256_broadcast_sd(F + 52*k2 + k3 + 159);
        _t7_3 = _asm256_loadu_pd(P + 52*i0 + k3);
        _t7_2 = _asm256_loadu_pd(P + 52*i0 + k3 + 52);
        _t7_1 = _asm256_loadu_pd(P + 52*i0 + k3 + 104);
        _t7_0 = _asm256_loadu_pd(P + 52*i0 + k3 + 156);
        _t7_20 = _asm256_loadu_pd(M0 + i0 + 52*k2);
        _t7_21 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 52);
        _t7_22 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 104);
        _t7_23 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t7_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
        _t7_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
        _t7_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
        _t7_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t7_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_28), _mm256_mul_pd(_t7_18, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_30), _mm256_mul_pd(_t7_16, _t7_31)));
        _t7_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_28), _mm256_mul_pd(_t7_14, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_30), _mm256_mul_pd(_t7_12, _t7_31)));
        _t7_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_28), _mm256_mul_pd(_t7_10, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_30), _mm256_mul_pd(_t7_8, _t7_31)));
        _t7_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_28), _mm256_mul_pd(_t7_6, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_30), _mm256_mul_pd(_t7_4, _t7_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t7_20 = _mm256_add_pd(_t7_20, _t7_24);
        _t7_21 = _mm256_add_pd(_t7_21, _t7_25);
        _t7_22 = _mm256_add_pd(_t7_22, _t7_26);
        _t7_23 = _mm256_add_pd(_t7_23, _t7_27);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 52*k2, _t7_20);
        _asm256_storeu_pd(M0 + i0 + 52*k2 + 52, _t7_21);
        _asm256_storeu_pd(M0 + i0 + 52*k2 + 104, _t7_22);
        _asm256_storeu_pd(M0 + i0 + 52*k2 + 156, _t7_23);
      }
      _t8_15 = _mm256_broadcast_sd(F + 52*k2 + k3);
      _t8_14 = _mm256_broadcast_sd(F + 52*k2 + k3 + 1);
      _t8_13 = _mm256_broadcast_sd(F + 52*k2 + k3 + 2);
      _t8_12 = _mm256_broadcast_sd(F + 52*k2 + k3 + 3);
      _t8_11 = _mm256_broadcast_sd(F + 52*k2 + k3 + 52);
      _t8_10 = _mm256_broadcast_sd(F + 52*k2 + k3 + 53);
      _t8_9 = _mm256_broadcast_sd(F + 52*k2 + k3 + 54);
      _t8_8 = _mm256_broadcast_sd(F + 52*k2 + k3 + 55);
      _t8_7 = _mm256_broadcast_sd(F + 52*k2 + k3 + 104);
      _t8_6 = _mm256_broadcast_sd(F + 52*k2 + k3 + 105);
      _t8_5 = _mm256_broadcast_sd(F + 52*k2 + k3 + 106);
      _t8_4 = _mm256_broadcast_sd(F + 52*k2 + k3 + 107);
      _t8_3 = _mm256_broadcast_sd(F + 52*k2 + k3 + 156);
      _t8_2 = _mm256_broadcast_sd(F + 52*k2 + k3 + 157);
      _t8_1 = _mm256_broadcast_sd(F + 52*k2 + k3 + 158);
      _t8_0 = _mm256_broadcast_sd(F + 52*k2 + k3 + 159);
      _t8_16 = _asm256_loadu_pd(M0 + 52*k2 + k3);
      _t8_17 = _asm256_loadu_pd(M0 + 52*k2 + k3 + 52);
      _t8_18 = _asm256_loadu_pd(M0 + 52*k2 + k3 + 104);
      _t8_19 = _asm256_loadu_pd(M0 + 52*k2 + k3 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t8_24 = _t6_3;
      _t8_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
      _t8_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
      _t8_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t8_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_24), _mm256_mul_pd(_t8_14, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_26), _mm256_mul_pd(_t8_12, _t8_27)));
      _t8_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_24), _mm256_mul_pd(_t8_10, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_26), _mm256_mul_pd(_t8_8, _t8_27)));
      _t8_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_24), _mm256_mul_pd(_t8_6, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_26), _mm256_mul_pd(_t8_4, _t8_27)));
      _t8_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t8_24), _mm256_mul_pd(_t8_2, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t8_26), _mm256_mul_pd(_t8_0, _t8_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_16 = _mm256_add_pd(_t8_16, _t8_20);
      _t8_17 = _mm256_add_pd(_t8_17, _t8_21);
      _t8_18 = _mm256_add_pd(_t8_18, _t8_22);
      _t8_19 = _mm256_add_pd(_t8_19, _t8_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M0 + 52*k2 + k3, _t8_16);
      _asm256_storeu_pd(M0 + 52*k2 + k3 + 52, _t8_17);
      _asm256_storeu_pd(M0 + 52*k2 + k3 + 104, _t8_18);
      _asm256_storeu_pd(M0 + 52*k2 + k3 + 156, _t8_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
        _t9_3 = _asm256_loadu_pd(P + i0 + 52*k3);
        _t9_2 = _asm256_loadu_pd(P + i0 + 52*k3 + 52);
        _t9_1 = _asm256_loadu_pd(P + i0 + 52*k3 + 104);
        _t9_0 = _asm256_loadu_pd(P + i0 + 52*k3 + 156);
        _t9_4 = _asm256_loadu_pd(M0 + i0 + 52*k2);
        _t9_5 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 52);
        _t9_6 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 104);
        _t9_7 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t9_3), _mm256_mul_pd(_t8_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t9_1), _mm256_mul_pd(_t8_12, _t9_0)));
        _t9_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t9_3), _mm256_mul_pd(_t8_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t9_1), _mm256_mul_pd(_t8_8, _t9_0)));
        _t9_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t9_3), _mm256_mul_pd(_t8_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t9_1), _mm256_mul_pd(_t8_4, _t9_0)));
        _t9_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t9_3), _mm256_mul_pd(_t8_2, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t9_1), _mm256_mul_pd(_t8_0, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t9_4 = _mm256_add_pd(_t9_4, _t9_8);
        _t9_5 = _mm256_add_pd(_t9_5, _t9_9);
        _t9_6 = _mm256_add_pd(_t9_6, _t9_10);
        _t9_7 = _mm256_add_pd(_t9_7, _t9_11);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 52*k2, _t9_4);
        _asm256_storeu_pd(M0 + i0 + 52*k2 + 52, _t9_5);
        _asm256_storeu_pd(M0 + i0 + 52*k2 + 104, _t9_6);
        _asm256_storeu_pd(M0 + i0 + 52*k2 + 156, _t9_7);
      }
    }
  }

  _t10_3 = _asm256_loadu_pd(P + 2544);
  _t10_2 = _mm256_maskload_pd(P + 2596, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t10_1 = _mm256_maskload_pd(P + 2648, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t10_0 = _mm256_maskload_pd(P + 2700, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t10_4 = _t10_3;
  _t10_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
  _t10_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
  _t10_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);


  for( int k2 = 0; k2 <= 51; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 47; i0+=4 ) {
      _t11_19 = _mm256_broadcast_sd(F + 52*k2 + 48);
      _t11_18 = _mm256_broadcast_sd(F + 52*k2 + 49);
      _t11_17 = _mm256_broadcast_sd(F + 52*k2 + 50);
      _t11_16 = _mm256_broadcast_sd(F + 52*k2 + 51);
      _t11_15 = _mm256_broadcast_sd(F + 52*k2 + 100);
      _t11_14 = _mm256_broadcast_sd(F + 52*k2 + 101);
      _t11_13 = _mm256_broadcast_sd(F + 52*k2 + 102);
      _t11_12 = _mm256_broadcast_sd(F + 52*k2 + 103);
      _t11_11 = _mm256_broadcast_sd(F + 52*k2 + 152);
      _t11_10 = _mm256_broadcast_sd(F + 52*k2 + 153);
      _t11_9 = _mm256_broadcast_sd(F + 52*k2 + 154);
      _t11_8 = _mm256_broadcast_sd(F + 52*k2 + 155);
      _t11_7 = _mm256_broadcast_sd(F + 52*k2 + 204);
      _t11_6 = _mm256_broadcast_sd(F + 52*k2 + 205);
      _t11_5 = _mm256_broadcast_sd(F + 52*k2 + 206);
      _t11_4 = _mm256_broadcast_sd(F + 52*k2 + 207);
      _t11_3 = _asm256_loadu_pd(P + 52*i0 + 48);
      _t11_2 = _asm256_loadu_pd(P + 52*i0 + 100);
      _t11_1 = _asm256_loadu_pd(P + 52*i0 + 152);
      _t11_0 = _asm256_loadu_pd(P + 52*i0 + 204);
      _t11_20 = _asm256_loadu_pd(M0 + i0 + 52*k2);
      _t11_21 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 52);
      _t11_22 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 104);
      _t11_23 = _asm256_loadu_pd(M0 + i0 + 52*k2 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t11_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
      _t11_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
      _t11_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
      _t11_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t11_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_28), _mm256_mul_pd(_t11_18, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_30), _mm256_mul_pd(_t11_16, _t11_31)));
      _t11_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_28), _mm256_mul_pd(_t11_14, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_30), _mm256_mul_pd(_t11_12, _t11_31)));
      _t11_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_28), _mm256_mul_pd(_t11_10, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_30), _mm256_mul_pd(_t11_8, _t11_31)));
      _t11_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_28), _mm256_mul_pd(_t11_6, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_30), _mm256_mul_pd(_t11_4, _t11_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t11_20 = _mm256_add_pd(_t11_20, _t11_24);
      _t11_21 = _mm256_add_pd(_t11_21, _t11_25);
      _t11_22 = _mm256_add_pd(_t11_22, _t11_26);
      _t11_23 = _mm256_add_pd(_t11_23, _t11_27);

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 52*k2, _t11_20);
      _asm256_storeu_pd(M0 + i0 + 52*k2 + 52, _t11_21);
      _asm256_storeu_pd(M0 + i0 + 52*k2 + 104, _t11_22);
      _asm256_storeu_pd(M0 + i0 + 52*k2 + 156, _t11_23);
    }
    _t12_15 = _mm256_broadcast_sd(F + 52*k2 + 48);
    _t12_14 = _mm256_broadcast_sd(F + 52*k2 + 49);
    _t12_13 = _mm256_broadcast_sd(F + 52*k2 + 50);
    _t12_12 = _mm256_broadcast_sd(F + 52*k2 + 51);
    _t12_11 = _mm256_broadcast_sd(F + 52*k2 + 100);
    _t12_10 = _mm256_broadcast_sd(F + 52*k2 + 101);
    _t12_9 = _mm256_broadcast_sd(F + 52*k2 + 102);
    _t12_8 = _mm256_broadcast_sd(F + 52*k2 + 103);
    _t12_7 = _mm256_broadcast_sd(F + 52*k2 + 152);
    _t12_6 = _mm256_broadcast_sd(F + 52*k2 + 153);
    _t12_5 = _mm256_broadcast_sd(F + 52*k2 + 154);
    _t12_4 = _mm256_broadcast_sd(F + 52*k2 + 155);
    _t12_3 = _mm256_broadcast_sd(F + 52*k2 + 204);
    _t12_2 = _mm256_broadcast_sd(F + 52*k2 + 205);
    _t12_1 = _mm256_broadcast_sd(F + 52*k2 + 206);
    _t12_0 = _mm256_broadcast_sd(F + 52*k2 + 207);
    _t12_16 = _asm256_loadu_pd(M0 + 52*k2 + 48);
    _t12_17 = _asm256_loadu_pd(M0 + 52*k2 + 100);
    _t12_18 = _asm256_loadu_pd(M0 + 52*k2 + 152);
    _t12_19 = _asm256_loadu_pd(M0 + 52*k2 + 204);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t10_4), _mm256_mul_pd(_t12_14, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t10_6), _mm256_mul_pd(_t12_12, _t10_7)));
    _t12_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t10_4), _mm256_mul_pd(_t12_10, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t10_6), _mm256_mul_pd(_t12_8, _t10_7)));
    _t12_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t10_4), _mm256_mul_pd(_t12_6, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t10_6), _mm256_mul_pd(_t12_4, _t10_7)));
    _t12_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_3, _t10_4), _mm256_mul_pd(_t12_2, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_1, _t10_6), _mm256_mul_pd(_t12_0, _t10_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_16 = _mm256_add_pd(_t12_16, _t12_20);
    _t12_17 = _mm256_add_pd(_t12_17, _t12_21);
    _t12_18 = _mm256_add_pd(_t12_18, _t12_22);
    _t12_19 = _mm256_add_pd(_t12_19, _t12_23);

    // AVX Storer:
    _asm256_storeu_pd(M0 + 52*k2 + 48, _t12_16);
    _asm256_storeu_pd(M0 + 52*k2 + 100, _t12_17);
    _asm256_storeu_pd(M0 + 52*k2 + 152, _t12_18);
    _asm256_storeu_pd(M0 + 52*k2 + 204, _t12_19);
  }


  // Generating : Y[52,52] = ( ( Sum_{k2} ( ( S(h(4, 52, k2), ( ( G(h(4, 52, k2), M0[52,52],h(4, 52, 0)) * T( G(h(4, 52, k2), F[52,52],h(4, 52, 0)) ) ) + G(h(4, 52, k2), Q[52,52],h(4, 52, k2)) ),h(4, 52, k2)) + Sum_{i0} ( S(h(4, 52, k2), ( ( G(h(4, 52, k2), M0[52,52],h(4, 52, 0)) * T( G(h(4, 52, i0), F[52,52],h(4, 52, 0)) ) ) + G(h(4, 52, k2), Q[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) + S(h(4, 52, 48), ( ( G(h(4, 52, 48), M0[52,52],h(4, 52, 0)) * T( G(h(4, 52, 48), F[52,52],h(4, 52, 0)) ) ) + G(h(4, 52, 48), Q[52,52],h(4, 52, 48)) ),h(4, 52, 48)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 52, k2), ( G(h(4, 52, k2), M0[52,52],h(4, 52, k3)) * T( G(h(4, 52, k2), F[52,52],h(4, 52, k3)) ) ),h(4, 52, k2)) + Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), M0[52,52],h(4, 52, k3)) * T( G(h(4, 52, i0), F[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) ) ) + $(h(4, 52, 48), ( G(h(4, 52, 48), M0[52,52],h(4, 52, k3)) * T( G(h(4, 52, 48), F[52,52],h(4, 52, k3)) ) ),h(4, 52, 48)) ) ) )


  for( int k2 = 0; k2 <= 47; k2+=4 ) {
    _t13_23 = _mm256_broadcast_sd(M0 + 52*k2);
    _t13_22 = _mm256_broadcast_sd(M0 + 52*k2 + 1);
    _t13_21 = _mm256_broadcast_sd(M0 + 52*k2 + 2);
    _t13_20 = _mm256_broadcast_sd(M0 + 52*k2 + 3);
    _t13_19 = _mm256_broadcast_sd(M0 + 52*k2 + 52);
    _t13_18 = _mm256_broadcast_sd(M0 + 52*k2 + 53);
    _t13_17 = _mm256_broadcast_sd(M0 + 52*k2 + 54);
    _t13_16 = _mm256_broadcast_sd(M0 + 52*k2 + 55);
    _t13_15 = _mm256_broadcast_sd(M0 + 52*k2 + 104);
    _t13_14 = _mm256_broadcast_sd(M0 + 52*k2 + 105);
    _t13_13 = _mm256_broadcast_sd(M0 + 52*k2 + 106);
    _t13_12 = _mm256_broadcast_sd(M0 + 52*k2 + 107);
    _t13_11 = _mm256_broadcast_sd(M0 + 52*k2 + 156);
    _t13_10 = _mm256_broadcast_sd(M0 + 52*k2 + 157);
    _t13_9 = _mm256_broadcast_sd(M0 + 52*k2 + 158);
    _t13_8 = _mm256_broadcast_sd(M0 + 52*k2 + 159);
    _t13_7 = _asm256_loadu_pd(F + 52*k2);
    _t13_6 = _asm256_loadu_pd(F + 52*k2 + 52);
    _t13_5 = _asm256_loadu_pd(F + 52*k2 + 104);
    _t13_4 = _asm256_loadu_pd(F + 52*k2 + 156);
    _t13_3 = _asm256_loadu_pd(Q + 53*k2);
    _t13_2 = _mm256_maskload_pd(Q + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t13_1 = _mm256_maskload_pd(Q + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t13_0 = _mm256_maskload_pd(Q + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t13_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 32);
    _t13_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 32);
    _t13_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 49);
    _t13_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t13_40), _mm256_mul_pd(_t13_22, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t13_42), _mm256_mul_pd(_t13_20, _t13_43)));
    _t13_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_40), _mm256_mul_pd(_t13_18, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_42), _mm256_mul_pd(_t13_16, _t13_43)));
    _t13_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_40), _mm256_mul_pd(_t13_14, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_42), _mm256_mul_pd(_t13_12, _t13_43)));
    _t13_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_40), _mm256_mul_pd(_t13_10, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_42), _mm256_mul_pd(_t13_8, _t13_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t13_36 = _t13_3;
    _t13_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 3), _t13_2, 12);
    _t13_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 0), _t13_1, 49);
    _t13_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 12), _mm256_shuffle_pd(_t13_1, _t13_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t13_24 = _mm256_add_pd(_t13_32, _t13_36);
    _t13_25 = _mm256_add_pd(_t13_33, _t13_37);
    _t13_26 = _mm256_add_pd(_t13_34, _t13_38);
    _t13_27 = _mm256_add_pd(_t13_35, _t13_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t13_28 = _t13_24;
    _t13_29 = _t13_25;
    _t13_30 = _t13_26;
    _t13_31 = _t13_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
      _t14_7 = _asm256_loadu_pd(F + 52*i0);
      _t14_6 = _asm256_loadu_pd(F + 52*i0 + 52);
      _t14_5 = _asm256_loadu_pd(F + 52*i0 + 104);
      _t14_4 = _asm256_loadu_pd(F + 52*i0 + 156);
      _t14_3 = _asm256_loadu_pd(Q + i0 + 52*k2);
      _t14_2 = _asm256_loadu_pd(Q + i0 + 52*k2 + 52);
      _t14_1 = _asm256_loadu_pd(Q + i0 + 52*k2 + 104);
      _t14_0 = _asm256_loadu_pd(Q + i0 + 52*k2 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t14_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 32);
      _t14_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 32);
      _t14_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 49);
      _t14_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t14_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t14_16), _mm256_mul_pd(_t13_22, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t14_18), _mm256_mul_pd(_t13_20, _t14_19)));
      _t14_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t14_16), _mm256_mul_pd(_t13_18, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t14_18), _mm256_mul_pd(_t13_16, _t14_19)));
      _t14_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t14_16), _mm256_mul_pd(_t13_14, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t14_18), _mm256_mul_pd(_t13_12, _t14_19)));
      _t14_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t14_16), _mm256_mul_pd(_t13_10, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t14_18), _mm256_mul_pd(_t13_8, _t14_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t14_8 = _mm256_add_pd(_t14_12, _t14_3);
      _t14_9 = _mm256_add_pd(_t14_13, _t14_2);
      _t14_10 = _mm256_add_pd(_t14_14, _t14_1);
      _t14_11 = _mm256_add_pd(_t14_15, _t14_0);

      // AVX Storer:
      _asm256_storeu_pd(Y + i0 + 52*k2, _t14_8);
      _asm256_storeu_pd(Y + i0 + 52*k2 + 52, _t14_9);
      _asm256_storeu_pd(Y + i0 + 52*k2 + 104, _t14_10);
      _asm256_storeu_pd(Y + i0 + 52*k2 + 156, _t14_11);
    }
    _asm256_storeu_pd(Y + 53*k2, _t13_28);
    _mm256_maskstore_pd(Y + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t13_29);
    _mm256_maskstore_pd(Y + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t13_30);
    _mm256_maskstore_pd(Y + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t13_31);
  }

  _t15_23 = _mm256_broadcast_sd(M0 + 2496);
  _t15_22 = _mm256_broadcast_sd(M0 + 2497);
  _t15_21 = _mm256_broadcast_sd(M0 + 2498);
  _t15_20 = _mm256_broadcast_sd(M0 + 2499);
  _t15_19 = _mm256_broadcast_sd(M0 + 2548);
  _t15_18 = _mm256_broadcast_sd(M0 + 2549);
  _t15_17 = _mm256_broadcast_sd(M0 + 2550);
  _t15_16 = _mm256_broadcast_sd(M0 + 2551);
  _t15_15 = _mm256_broadcast_sd(M0 + 2600);
  _t15_14 = _mm256_broadcast_sd(M0 + 2601);
  _t15_13 = _mm256_broadcast_sd(M0 + 2602);
  _t15_12 = _mm256_broadcast_sd(M0 + 2603);
  _t15_11 = _mm256_broadcast_sd(M0 + 2652);
  _t15_10 = _mm256_broadcast_sd(M0 + 2653);
  _t15_9 = _mm256_broadcast_sd(M0 + 2654);
  _t15_8 = _mm256_broadcast_sd(M0 + 2655);
  _t15_7 = _asm256_loadu_pd(F + 2496);
  _t15_6 = _asm256_loadu_pd(F + 2548);
  _t15_5 = _asm256_loadu_pd(F + 2600);
  _t15_4 = _asm256_loadu_pd(F + 2652);
  _t15_3 = _asm256_loadu_pd(Q + 2544);
  _t15_2 = _mm256_maskload_pd(Q + 2596, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t15_1 = _mm256_maskload_pd(Q + 2648, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t15_0 = _mm256_maskload_pd(Q + 2700, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
  _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
  _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
  _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
  _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
  _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
  _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t15_36 = _t15_3;
  _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
  _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
  _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
  _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
  _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
  _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t15_28 = _t15_24;
  _t15_29 = _t15_25;
  _t15_30 = _t15_26;
  _t15_31 = _t15_27;


  for( int k3 = 4; k3 <= 51; k3+=4 ) {

    for( int k2 = 0; k2 <= 47; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 52*k2 + k3);
      _t16_18 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 52);
      _t16_14 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 53);
      _t16_13 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 54);
      _t16_12 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 55);
      _t16_11 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 104);
      _t16_10 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 105);
      _t16_9 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 106);
      _t16_8 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 107);
      _t16_7 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 156);
      _t16_6 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 157);
      _t16_5 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 158);
      _t16_4 = _mm256_broadcast_sd(M0 + 52*k2 + k3 + 159);
      _t16_3 = _asm256_loadu_pd(F + 52*k2 + k3);
      _t16_2 = _asm256_loadu_pd(F + 52*k2 + k3 + 52);
      _t16_1 = _asm256_loadu_pd(F + 52*k2 + k3 + 104);
      _t16_0 = _asm256_loadu_pd(F + 52*k2 + k3 + 156);
      _t16_20 = _asm256_loadu_pd(Y + 53*k2);
      _t16_21 = _mm256_maskload_pd(Y + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t16_22 = _mm256_maskload_pd(Y + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t16_23 = _mm256_maskload_pd(Y + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_32), _mm256_mul_pd(_t16_18, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_34), _mm256_mul_pd(_t16_16, _t16_35)));
      _t16_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_32), _mm256_mul_pd(_t16_14, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_34), _mm256_mul_pd(_t16_12, _t16_35)));
      _t16_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_32), _mm256_mul_pd(_t16_10, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_34), _mm256_mul_pd(_t16_8, _t16_35)));
      _t16_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_32), _mm256_mul_pd(_t16_6, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_34), _mm256_mul_pd(_t16_4, _t16_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_28 = _t16_20;
      _t16_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 3), _t16_21, 12);
      _t16_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 0), _t16_22, 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 12), _mm256_shuffle_pd(_t16_22, _t16_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_28 = _mm256_add_pd(_t16_28, _t16_24);
      _t16_29 = _mm256_add_pd(_t16_29, _t16_25);
      _t16_30 = _mm256_add_pd(_t16_30, _t16_26);
      _t16_31 = _mm256_add_pd(_t16_31, _t16_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t16_20 = _t16_28;
      _t16_21 = _t16_29;
      _t16_22 = _t16_30;
      _t16_23 = _t16_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
        _t17_3 = _asm256_loadu_pd(F + 52*i0 + k3);
        _t17_2 = _asm256_loadu_pd(F + 52*i0 + k3 + 52);
        _t17_1 = _asm256_loadu_pd(F + 52*i0 + k3 + 104);
        _t17_0 = _asm256_loadu_pd(F + 52*i0 + k3 + 156);
        _t17_4 = _asm256_loadu_pd(Y + i0 + 52*k2);
        _t17_5 = _asm256_loadu_pd(Y + i0 + 52*k2 + 52);
        _t17_6 = _asm256_loadu_pd(Y + i0 + 52*k2 + 104);
        _t17_7 = _asm256_loadu_pd(Y + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t17_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
        _t17_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
        _t17_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
        _t17_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t17_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t17_12), _mm256_mul_pd(_t16_18, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t17_14), _mm256_mul_pd(_t16_16, _t17_15)));
        _t17_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t17_12), _mm256_mul_pd(_t16_14, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t17_14), _mm256_mul_pd(_t16_12, _t17_15)));
        _t17_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t17_12), _mm256_mul_pd(_t16_10, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t17_14), _mm256_mul_pd(_t16_8, _t17_15)));
        _t17_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t17_12), _mm256_mul_pd(_t16_6, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t17_14), _mm256_mul_pd(_t16_4, _t17_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_4 = _mm256_add_pd(_t17_4, _t17_8);
        _t17_5 = _mm256_add_pd(_t17_5, _t17_9);
        _t17_6 = _mm256_add_pd(_t17_6, _t17_10);
        _t17_7 = _mm256_add_pd(_t17_7, _t17_11);

        // AVX Storer:
        _asm256_storeu_pd(Y + i0 + 52*k2, _t17_4);
        _asm256_storeu_pd(Y + i0 + 52*k2 + 52, _t17_5);
        _asm256_storeu_pd(Y + i0 + 52*k2 + 104, _t17_6);
        _asm256_storeu_pd(Y + i0 + 52*k2 + 156, _t17_7);
      }
      _asm256_storeu_pd(Y + 53*k2, _t16_20);
      _mm256_maskstore_pd(Y + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t16_21);
      _mm256_maskstore_pd(Y + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t16_22);
      _mm256_maskstore_pd(Y + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t16_23);
    }
    _t18_19 = _mm256_broadcast_sd(M0 + k3 + 2496);
    _t18_18 = _mm256_broadcast_sd(M0 + k3 + 2497);
    _t18_17 = _mm256_broadcast_sd(M0 + k3 + 2498);
    _t18_16 = _mm256_broadcast_sd(M0 + k3 + 2499);
    _t18_15 = _mm256_broadcast_sd(M0 + k3 + 2548);
    _t18_14 = _mm256_broadcast_sd(M0 + k3 + 2549);
    _t18_13 = _mm256_broadcast_sd(M0 + k3 + 2550);
    _t18_12 = _mm256_broadcast_sd(M0 + k3 + 2551);
    _t18_11 = _mm256_broadcast_sd(M0 + k3 + 2600);
    _t18_10 = _mm256_broadcast_sd(M0 + k3 + 2601);
    _t18_9 = _mm256_broadcast_sd(M0 + k3 + 2602);
    _t18_8 = _mm256_broadcast_sd(M0 + k3 + 2603);
    _t18_7 = _mm256_broadcast_sd(M0 + k3 + 2652);
    _t18_6 = _mm256_broadcast_sd(M0 + k3 + 2653);
    _t18_5 = _mm256_broadcast_sd(M0 + k3 + 2654);
    _t18_4 = _mm256_broadcast_sd(M0 + k3 + 2655);
    _t18_3 = _asm256_loadu_pd(F + k3 + 2496);
    _t18_2 = _asm256_loadu_pd(F + k3 + 2548);
    _t18_1 = _asm256_loadu_pd(F + k3 + 2600);
    _t18_0 = _asm256_loadu_pd(F + k3 + 2652);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t18_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
    _t18_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
    _t18_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
    _t18_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_28), _mm256_mul_pd(_t18_18, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_30), _mm256_mul_pd(_t18_16, _t18_31)));
    _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_28), _mm256_mul_pd(_t18_14, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_30), _mm256_mul_pd(_t18_12, _t18_31)));
    _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_28), _mm256_mul_pd(_t18_10, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_30), _mm256_mul_pd(_t18_8, _t18_31)));
    _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_28), _mm256_mul_pd(_t18_6, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_30), _mm256_mul_pd(_t18_4, _t18_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t18_24 = _t15_28;
    _t18_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
    _t18_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
    _t18_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t18_24 = _mm256_add_pd(_t18_24, _t18_20);
    _t18_25 = _mm256_add_pd(_t18_25, _t18_21);
    _t18_26 = _mm256_add_pd(_t18_26, _t18_22);
    _t18_27 = _mm256_add_pd(_t18_27, _t18_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t18_24;
    _t15_29 = _t18_25;
    _t15_30 = _t18_26;
    _t15_31 = _t18_27;
  }


  // Generating : v0[52,1] = ( Sum_{k2} ( S(h(4, 52, k2), ( G(h(4, 52, k2), z[52,1],h(1, 1, 0)) - ( G(h(4, 52, k2), H[52,52],h(4, 52, 0)) * G(h(4, 52, 0), y[52,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, k3)) * G(h(4, 52, k3), y[52,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k2 = 0; k2 <= 51; k2+=4 ) {
    _t19_5 = _asm256_loadu_pd(z + k2);
    _t19_4 = _asm256_loadu_pd(H + 52*k2);
    _t19_3 = _asm256_loadu_pd(H + 52*k2 + 52);
    _t19_2 = _asm256_loadu_pd(H + 52*k2 + 104);
    _t19_1 = _asm256_loadu_pd(H + 52*k2 + 156);
    _t19_0 = _asm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t19_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t19_7 = _mm256_sub_pd(_t19_5, _t19_6);

    // AVX Storer:
    _asm256_storeu_pd(v0 + k2, _t19_7);
  }


  for( int k3 = 4; k3 <= 51; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 51; k2+=4 ) {
      _t20_4 = _asm256_loadu_pd(H + 52*k2 + k3);
      _t20_3 = _asm256_loadu_pd(H + 52*k2 + k3 + 52);
      _t20_2 = _asm256_loadu_pd(H + 52*k2 + k3 + 104);
      _t20_1 = _asm256_loadu_pd(H + 52*k2 + k3 + 156);
      _t20_0 = _asm256_loadu_pd(y + k3);
      _t20_5 = _asm256_loadu_pd(v0 + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t20_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t20_5 = _mm256_sub_pd(_t20_5, _t20_6);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k2, _t20_5);
    }
  }

  _t21_3 = _asm256_loadu_pd(Y);
  _t21_2 = _mm256_maskload_pd(Y + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t21_1 = _mm256_maskload_pd(Y + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t21_0 = _mm256_maskload_pd(Y + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[52,52] = ( ( Sum_{k2} ( ( S(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, 0)) * G(h(4, 52, 0), Y[52,52],h(4, 52, 0)) ),h(4, 52, 0)) + Sum_{i0} ( S(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, 0)) * G(h(4, 52, 0), Y[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, k3)) * T( G(h(4, 52, i0), Y[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) + $(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, k3)) * G(h(4, 52, k3), Y[52,52],h(4, 52, k3)) ),h(4, 52, k3)) ) + Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, k3)) * G(h(4, 52, k3), Y[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, 48)) * T( G(h(4, 52, i0), Y[52,52],h(4, 52, 48)) ) ),h(4, 52, i0)) ) + $(h(4, 52, k2), ( G(h(4, 52, k2), H[52,52],h(4, 52, 48)) * G(h(4, 52, 48), Y[52,52],h(4, 52, 48)) ),h(4, 52, 48)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t21_4 = _t21_3;
  _t21_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t21_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t21_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int k2 = 0; k2 <= 51; k2+=4 ) {
    _t22_15 = _mm256_broadcast_sd(H + 52*k2);
    _t22_14 = _mm256_broadcast_sd(H + 52*k2 + 1);
    _t22_13 = _mm256_broadcast_sd(H + 52*k2 + 2);
    _t22_12 = _mm256_broadcast_sd(H + 52*k2 + 3);
    _t22_11 = _mm256_broadcast_sd(H + 52*k2 + 52);
    _t22_10 = _mm256_broadcast_sd(H + 52*k2 + 53);
    _t22_9 = _mm256_broadcast_sd(H + 52*k2 + 54);
    _t22_8 = _mm256_broadcast_sd(H + 52*k2 + 55);
    _t22_7 = _mm256_broadcast_sd(H + 52*k2 + 104);
    _t22_6 = _mm256_broadcast_sd(H + 52*k2 + 105);
    _t22_5 = _mm256_broadcast_sd(H + 52*k2 + 106);
    _t22_4 = _mm256_broadcast_sd(H + 52*k2 + 107);
    _t22_3 = _mm256_broadcast_sd(H + 52*k2 + 156);
    _t22_2 = _mm256_broadcast_sd(H + 52*k2 + 157);
    _t22_1 = _mm256_broadcast_sd(H + 52*k2 + 158);
    _t22_0 = _mm256_broadcast_sd(H + 52*k2 + 159);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t22_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t21_4), _mm256_mul_pd(_t22_14, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t21_6), _mm256_mul_pd(_t22_12, _t21_7)));
    _t22_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t21_4), _mm256_mul_pd(_t22_10, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t21_6), _mm256_mul_pd(_t22_8, _t21_7)));
    _t22_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t21_4), _mm256_mul_pd(_t22_6, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t21_6), _mm256_mul_pd(_t22_4, _t21_7)));
    _t22_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t21_4), _mm256_mul_pd(_t22_2, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t21_6), _mm256_mul_pd(_t22_0, _t21_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 51; i0+=4 ) {
      _t23_3 = _asm256_loadu_pd(Y + i0);
      _t23_2 = _asm256_loadu_pd(Y + i0 + 52);
      _t23_1 = _asm256_loadu_pd(Y + i0 + 104);
      _t23_0 = _asm256_loadu_pd(Y + i0 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t23_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t23_3), _mm256_mul_pd(_t22_14, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t23_1), _mm256_mul_pd(_t22_12, _t23_0)));
      _t23_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t23_3), _mm256_mul_pd(_t22_10, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t23_1), _mm256_mul_pd(_t22_8, _t23_0)));
      _t23_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t23_3), _mm256_mul_pd(_t22_6, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t23_1), _mm256_mul_pd(_t22_4, _t23_0)));
      _t23_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t23_3), _mm256_mul_pd(_t22_2, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t23_1), _mm256_mul_pd(_t22_0, _t23_0)));

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 52*k2, _t23_4);
      _asm256_storeu_pd(M1 + i0 + 52*k2 + 52, _t23_5);
      _asm256_storeu_pd(M1 + i0 + 52*k2 + 104, _t23_6);
      _asm256_storeu_pd(M1 + i0 + 52*k2 + 156, _t23_7);
    }
    _asm256_storeu_pd(M1 + 52*k2, _t22_16);
    _asm256_storeu_pd(M1 + 52*k2 + 52, _t22_17);
    _asm256_storeu_pd(M1 + 52*k2 + 104, _t22_18);
    _asm256_storeu_pd(M1 + 52*k2 + 156, _t22_19);
  }


  for( int k3 = 4; k3 <= 47; k3+=4 ) {
    _t24_3 = _asm256_loadu_pd(Y + 53*k3);
    _t24_2 = _mm256_maskload_pd(Y + 53*k3 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t24_1 = _mm256_maskload_pd(Y + 53*k3 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t24_0 = _mm256_maskload_pd(Y + 53*k3 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t24_4 = _t24_3;
    _t24_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
    _t24_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
    _t24_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

    for( int k2 = 0; k2 <= 51; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t25_19 = _mm256_broadcast_sd(H + 52*k2 + k3);
        _t25_18 = _mm256_broadcast_sd(H + 52*k2 + k3 + 1);
        _t25_17 = _mm256_broadcast_sd(H + 52*k2 + k3 + 2);
        _t25_16 = _mm256_broadcast_sd(H + 52*k2 + k3 + 3);
        _t25_15 = _mm256_broadcast_sd(H + 52*k2 + k3 + 52);
        _t25_14 = _mm256_broadcast_sd(H + 52*k2 + k3 + 53);
        _t25_13 = _mm256_broadcast_sd(H + 52*k2 + k3 + 54);
        _t25_12 = _mm256_broadcast_sd(H + 52*k2 + k3 + 55);
        _t25_11 = _mm256_broadcast_sd(H + 52*k2 + k3 + 104);
        _t25_10 = _mm256_broadcast_sd(H + 52*k2 + k3 + 105);
        _t25_9 = _mm256_broadcast_sd(H + 52*k2 + k3 + 106);
        _t25_8 = _mm256_broadcast_sd(H + 52*k2 + k3 + 107);
        _t25_7 = _mm256_broadcast_sd(H + 52*k2 + k3 + 156);
        _t25_6 = _mm256_broadcast_sd(H + 52*k2 + k3 + 157);
        _t25_5 = _mm256_broadcast_sd(H + 52*k2 + k3 + 158);
        _t25_4 = _mm256_broadcast_sd(H + 52*k2 + k3 + 159);
        _t25_3 = _asm256_loadu_pd(Y + 52*i0 + k3);
        _t25_2 = _asm256_loadu_pd(Y + 52*i0 + k3 + 52);
        _t25_1 = _asm256_loadu_pd(Y + 52*i0 + k3 + 104);
        _t25_0 = _asm256_loadu_pd(Y + 52*i0 + k3 + 156);
        _t25_20 = _asm256_loadu_pd(M1 + i0 + 52*k2);
        _t25_21 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 52);
        _t25_22 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 104);
        _t25_23 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t25_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
        _t25_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
        _t25_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
        _t25_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t25_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_28), _mm256_mul_pd(_t25_18, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_30), _mm256_mul_pd(_t25_16, _t25_31)));
        _t25_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_28), _mm256_mul_pd(_t25_14, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_30), _mm256_mul_pd(_t25_12, _t25_31)));
        _t25_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_28), _mm256_mul_pd(_t25_10, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_30), _mm256_mul_pd(_t25_8, _t25_31)));
        _t25_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_28), _mm256_mul_pd(_t25_6, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_30), _mm256_mul_pd(_t25_4, _t25_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t25_20 = _mm256_add_pd(_t25_20, _t25_24);
        _t25_21 = _mm256_add_pd(_t25_21, _t25_25);
        _t25_22 = _mm256_add_pd(_t25_22, _t25_26);
        _t25_23 = _mm256_add_pd(_t25_23, _t25_27);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 52*k2, _t25_20);
        _asm256_storeu_pd(M1 + i0 + 52*k2 + 52, _t25_21);
        _asm256_storeu_pd(M1 + i0 + 52*k2 + 104, _t25_22);
        _asm256_storeu_pd(M1 + i0 + 52*k2 + 156, _t25_23);
      }
      _t26_15 = _mm256_broadcast_sd(H + 52*k2 + k3);
      _t26_14 = _mm256_broadcast_sd(H + 52*k2 + k3 + 1);
      _t26_13 = _mm256_broadcast_sd(H + 52*k2 + k3 + 2);
      _t26_12 = _mm256_broadcast_sd(H + 52*k2 + k3 + 3);
      _t26_11 = _mm256_broadcast_sd(H + 52*k2 + k3 + 52);
      _t26_10 = _mm256_broadcast_sd(H + 52*k2 + k3 + 53);
      _t26_9 = _mm256_broadcast_sd(H + 52*k2 + k3 + 54);
      _t26_8 = _mm256_broadcast_sd(H + 52*k2 + k3 + 55);
      _t26_7 = _mm256_broadcast_sd(H + 52*k2 + k3 + 104);
      _t26_6 = _mm256_broadcast_sd(H + 52*k2 + k3 + 105);
      _t26_5 = _mm256_broadcast_sd(H + 52*k2 + k3 + 106);
      _t26_4 = _mm256_broadcast_sd(H + 52*k2 + k3 + 107);
      _t26_3 = _mm256_broadcast_sd(H + 52*k2 + k3 + 156);
      _t26_2 = _mm256_broadcast_sd(H + 52*k2 + k3 + 157);
      _t26_1 = _mm256_broadcast_sd(H + 52*k2 + k3 + 158);
      _t26_0 = _mm256_broadcast_sd(H + 52*k2 + k3 + 159);
      _t26_16 = _asm256_loadu_pd(M1 + 52*k2 + k3);
      _t26_17 = _asm256_loadu_pd(M1 + 52*k2 + k3 + 52);
      _t26_18 = _asm256_loadu_pd(M1 + 52*k2 + k3 + 104);
      _t26_19 = _asm256_loadu_pd(M1 + 52*k2 + k3 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t26_24 = _t24_3;
      _t26_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
      _t26_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
      _t26_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t26_24), _mm256_mul_pd(_t26_14, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t26_26), _mm256_mul_pd(_t26_12, _t26_27)));
      _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t26_24), _mm256_mul_pd(_t26_10, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t26_26), _mm256_mul_pd(_t26_8, _t26_27)));
      _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t26_24), _mm256_mul_pd(_t26_6, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t26_26), _mm256_mul_pd(_t26_4, _t26_27)));
      _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t26_24), _mm256_mul_pd(_t26_2, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t26_26), _mm256_mul_pd(_t26_0, _t26_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_16 = _mm256_add_pd(_t26_16, _t26_20);
      _t26_17 = _mm256_add_pd(_t26_17, _t26_21);
      _t26_18 = _mm256_add_pd(_t26_18, _t26_22);
      _t26_19 = _mm256_add_pd(_t26_19, _t26_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M1 + 52*k2 + k3, _t26_16);
      _asm256_storeu_pd(M1 + 52*k2 + k3 + 52, _t26_17);
      _asm256_storeu_pd(M1 + 52*k2 + k3 + 104, _t26_18);
      _asm256_storeu_pd(M1 + 52*k2 + k3 + 156, _t26_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
        _t27_3 = _asm256_loadu_pd(Y + i0 + 52*k3);
        _t27_2 = _asm256_loadu_pd(Y + i0 + 52*k3 + 52);
        _t27_1 = _asm256_loadu_pd(Y + i0 + 52*k3 + 104);
        _t27_0 = _asm256_loadu_pd(Y + i0 + 52*k3 + 156);
        _t27_4 = _asm256_loadu_pd(M1 + i0 + 52*k2);
        _t27_5 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 52);
        _t27_6 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 104);
        _t27_7 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t27_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t27_3), _mm256_mul_pd(_t26_14, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t27_1), _mm256_mul_pd(_t26_12, _t27_0)));
        _t27_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t27_3), _mm256_mul_pd(_t26_10, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t27_1), _mm256_mul_pd(_t26_8, _t27_0)));
        _t27_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t27_3), _mm256_mul_pd(_t26_6, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t27_1), _mm256_mul_pd(_t26_4, _t27_0)));
        _t27_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t27_3), _mm256_mul_pd(_t26_2, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t27_1), _mm256_mul_pd(_t26_0, _t27_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t27_4 = _mm256_add_pd(_t27_4, _t27_8);
        _t27_5 = _mm256_add_pd(_t27_5, _t27_9);
        _t27_6 = _mm256_add_pd(_t27_6, _t27_10);
        _t27_7 = _mm256_add_pd(_t27_7, _t27_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 52*k2, _t27_4);
        _asm256_storeu_pd(M1 + i0 + 52*k2 + 52, _t27_5);
        _asm256_storeu_pd(M1 + i0 + 52*k2 + 104, _t27_6);
        _asm256_storeu_pd(M1 + i0 + 52*k2 + 156, _t27_7);
      }
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t28_0 = _t15_28;
  _t28_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t28_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t28_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int k2 = 0; k2 <= 51; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 47; i0+=4 ) {
      _t29_19 = _mm256_broadcast_sd(H + 52*k2 + 48);
      _t29_18 = _mm256_broadcast_sd(H + 52*k2 + 49);
      _t29_17 = _mm256_broadcast_sd(H + 52*k2 + 50);
      _t29_16 = _mm256_broadcast_sd(H + 52*k2 + 51);
      _t29_15 = _mm256_broadcast_sd(H + 52*k2 + 100);
      _t29_14 = _mm256_broadcast_sd(H + 52*k2 + 101);
      _t29_13 = _mm256_broadcast_sd(H + 52*k2 + 102);
      _t29_12 = _mm256_broadcast_sd(H + 52*k2 + 103);
      _t29_11 = _mm256_broadcast_sd(H + 52*k2 + 152);
      _t29_10 = _mm256_broadcast_sd(H + 52*k2 + 153);
      _t29_9 = _mm256_broadcast_sd(H + 52*k2 + 154);
      _t29_8 = _mm256_broadcast_sd(H + 52*k2 + 155);
      _t29_7 = _mm256_broadcast_sd(H + 52*k2 + 204);
      _t29_6 = _mm256_broadcast_sd(H + 52*k2 + 205);
      _t29_5 = _mm256_broadcast_sd(H + 52*k2 + 206);
      _t29_4 = _mm256_broadcast_sd(H + 52*k2 + 207);
      _t29_3 = _asm256_loadu_pd(Y + 52*i0 + 48);
      _t29_2 = _asm256_loadu_pd(Y + 52*i0 + 100);
      _t29_1 = _asm256_loadu_pd(Y + 52*i0 + 152);
      _t29_0 = _asm256_loadu_pd(Y + 52*i0 + 204);
      _t29_20 = _asm256_loadu_pd(M1 + i0 + 52*k2);
      _t29_21 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 52);
      _t29_22 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 104);
      _t29_23 = _asm256_loadu_pd(M1 + i0 + 52*k2 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
      _t29_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
      _t29_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
      _t29_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t29_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_28), _mm256_mul_pd(_t29_18, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_30), _mm256_mul_pd(_t29_16, _t29_31)));
      _t29_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_28), _mm256_mul_pd(_t29_14, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_30), _mm256_mul_pd(_t29_12, _t29_31)));
      _t29_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_28), _mm256_mul_pd(_t29_10, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_30), _mm256_mul_pd(_t29_8, _t29_31)));
      _t29_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_28), _mm256_mul_pd(_t29_6, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_30), _mm256_mul_pd(_t29_4, _t29_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t29_20 = _mm256_add_pd(_t29_20, _t29_24);
      _t29_21 = _mm256_add_pd(_t29_21, _t29_25);
      _t29_22 = _mm256_add_pd(_t29_22, _t29_26);
      _t29_23 = _mm256_add_pd(_t29_23, _t29_27);

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 52*k2, _t29_20);
      _asm256_storeu_pd(M1 + i0 + 52*k2 + 52, _t29_21);
      _asm256_storeu_pd(M1 + i0 + 52*k2 + 104, _t29_22);
      _asm256_storeu_pd(M1 + i0 + 52*k2 + 156, _t29_23);
    }
    _t30_15 = _mm256_broadcast_sd(H + 52*k2 + 48);
    _t30_14 = _mm256_broadcast_sd(H + 52*k2 + 49);
    _t30_13 = _mm256_broadcast_sd(H + 52*k2 + 50);
    _t30_12 = _mm256_broadcast_sd(H + 52*k2 + 51);
    _t30_11 = _mm256_broadcast_sd(H + 52*k2 + 100);
    _t30_10 = _mm256_broadcast_sd(H + 52*k2 + 101);
    _t30_9 = _mm256_broadcast_sd(H + 52*k2 + 102);
    _t30_8 = _mm256_broadcast_sd(H + 52*k2 + 103);
    _t30_7 = _mm256_broadcast_sd(H + 52*k2 + 152);
    _t30_6 = _mm256_broadcast_sd(H + 52*k2 + 153);
    _t30_5 = _mm256_broadcast_sd(H + 52*k2 + 154);
    _t30_4 = _mm256_broadcast_sd(H + 52*k2 + 155);
    _t30_3 = _mm256_broadcast_sd(H + 52*k2 + 204);
    _t30_2 = _mm256_broadcast_sd(H + 52*k2 + 205);
    _t30_1 = _mm256_broadcast_sd(H + 52*k2 + 206);
    _t30_0 = _mm256_broadcast_sd(H + 52*k2 + 207);
    _t30_16 = _asm256_loadu_pd(M1 + 52*k2 + 48);
    _t30_17 = _asm256_loadu_pd(M1 + 52*k2 + 100);
    _t30_18 = _asm256_loadu_pd(M1 + 52*k2 + 152);
    _t30_19 = _asm256_loadu_pd(M1 + 52*k2 + 204);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t28_0), _mm256_mul_pd(_t30_14, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t28_2), _mm256_mul_pd(_t30_12, _t28_3)));
    _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t28_0), _mm256_mul_pd(_t30_10, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t28_2), _mm256_mul_pd(_t30_8, _t28_3)));
    _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t28_0), _mm256_mul_pd(_t30_6, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t28_2), _mm256_mul_pd(_t30_4, _t28_3)));
    _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_3, _t28_0), _mm256_mul_pd(_t30_2, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_1, _t28_2), _mm256_mul_pd(_t30_0, _t28_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t30_16 = _mm256_add_pd(_t30_16, _t30_20);
    _t30_17 = _mm256_add_pd(_t30_17, _t30_21);
    _t30_18 = _mm256_add_pd(_t30_18, _t30_22);
    _t30_19 = _mm256_add_pd(_t30_19, _t30_23);

    // AVX Storer:
    _asm256_storeu_pd(M1 + 52*k2 + 48, _t30_16);
    _asm256_storeu_pd(M1 + 52*k2 + 100, _t30_17);
    _asm256_storeu_pd(M1 + 52*k2 + 152, _t30_18);
    _asm256_storeu_pd(M1 + 52*k2 + 204, _t30_19);
  }


  // Generating : M2[52,52] = ( ( ( ( Sum_{i0} ( S(h(4, 52, 0), ( G(h(4, 52, 0), Y[52,52],h(4, 52, 0)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, 0)) ) ),h(4, 52, i0)) ) + Sum_{k2} ( Sum_{i0} ( S(h(4, 52, k2), ( T( G(h(4, 52, 0), Y[52,52],h(4, 52, k2)) ) * T( G(h(4, 52, i0), H[52,52],h(4, 52, 0)) ) ),h(4, 52, i0)) ) ) ) + Sum_{k3} ( ( ( Sum_{k2} ( Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), Y[52,52],h(4, 52, k3)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) ) + Sum_{i0} ( $(h(4, 52, k3), ( G(h(4, 52, k3), Y[52,52],h(4, 52, k3)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 52, k2), ( T( G(h(4, 52, k3), Y[52,52],h(4, 52, k2)) ) * T( G(h(4, 52, i0), H[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) ) ) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), Y[52,52],h(4, 52, 48)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, 48)) ) ),h(4, 52, i0)) ) ) ) + Sum_{i0} ( $(h(4, 52, 48), ( G(h(4, 52, 48), Y[52,52],h(4, 52, 48)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, 48)) ) ),h(4, 52, i0)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t31_0 = _t21_3;
  _t31_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t31_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t31_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int i0 = 0; i0 <= 51; i0+=4 ) {
    _t32_3 = _asm256_loadu_pd(H + 52*i0);
    _t32_2 = _asm256_loadu_pd(H + 52*i0 + 52);
    _t32_1 = _asm256_loadu_pd(H + 52*i0 + 104);
    _t32_0 = _asm256_loadu_pd(H + 52*i0 + 156);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t32_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32);
    _t32_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 32);
    _t32_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 49);
    _t32_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 15), _t32_11)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 15), _t32_11)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 15), _t32_11)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 15), _t32_11)));

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0, _t32_4);
    _asm256_storeu_pd(M2 + i0 + 52, _t32_5);
    _asm256_storeu_pd(M2 + i0 + 104, _t32_6);
    _asm256_storeu_pd(M2 + i0 + 156, _t32_7);
  }


  for( int k2 = 4; k2 <= 51; k2+=4 ) {
    _t33_3 = _asm256_loadu_pd(Y + k2);
    _t33_2 = _asm256_loadu_pd(Y + k2 + 52);
    _t33_1 = _asm256_loadu_pd(Y + k2 + 104);
    _t33_0 = _asm256_loadu_pd(Y + k2 + 156);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    for( int i0 = 0; i0 <= 51; i0+=4 ) {
      _t34_3 = _asm256_loadu_pd(H + 52*i0);
      _t34_2 = _asm256_loadu_pd(H + 52*i0 + 52);
      _t34_1 = _asm256_loadu_pd(H + 52*i0 + 104);
      _t34_0 = _asm256_loadu_pd(H + 52*i0 + 156);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
      _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
      _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
      _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t34_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
      _t34_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
      _t34_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
      _t34_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t34_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 15), _t34_11)));
      _t34_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 15), _t34_11)));
      _t34_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 15), _t34_11)));
      _t34_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 15), _t34_11)));

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 52*k2, _t34_4);
      _asm256_storeu_pd(M2 + i0 + 52*k2 + 52, _t34_5);
      _asm256_storeu_pd(M2 + i0 + 52*k2 + 104, _t34_6);
      _asm256_storeu_pd(M2 + i0 + 52*k2 + 156, _t34_7);
    }
  }


  for( int k3 = 4; k3 <= 47; k3+=4 ) {

    for( int k2 = 0; k2 <= k3 - 1; k2+=4 ) {

      for( int i0 = 0; i0 <= 51; i0+=4 ) {
        _t35_19 = _mm256_broadcast_sd(Y + 52*k2 + k3);
        _t35_18 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 1);
        _t35_17 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 2);
        _t35_16 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 3);
        _t35_15 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 52);
        _t35_14 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 53);
        _t35_13 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 54);
        _t35_12 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 55);
        _t35_11 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 104);
        _t35_10 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 105);
        _t35_9 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 106);
        _t35_8 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 107);
        _t35_7 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 156);
        _t35_6 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 157);
        _t35_5 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 158);
        _t35_4 = _mm256_broadcast_sd(Y + 52*k2 + k3 + 159);
        _t35_3 = _asm256_loadu_pd(H + 52*i0 + k3);
        _t35_2 = _asm256_loadu_pd(H + 52*i0 + k3 + 52);
        _t35_1 = _asm256_loadu_pd(H + 52*i0 + k3 + 104);
        _t35_0 = _asm256_loadu_pd(H + 52*i0 + k3 + 156);
        _t35_20 = _asm256_loadu_pd(M2 + i0 + 52*k2);
        _t35_21 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 52);
        _t35_22 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 104);
        _t35_23 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t35_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
        _t35_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
        _t35_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
        _t35_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t35_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_28), _mm256_mul_pd(_t35_18, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_30), _mm256_mul_pd(_t35_16, _t35_31)));
        _t35_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_28), _mm256_mul_pd(_t35_14, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_30), _mm256_mul_pd(_t35_12, _t35_31)));
        _t35_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_28), _mm256_mul_pd(_t35_10, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_30), _mm256_mul_pd(_t35_8, _t35_31)));
        _t35_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_28), _mm256_mul_pd(_t35_6, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_30), _mm256_mul_pd(_t35_4, _t35_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t35_20 = _mm256_add_pd(_t35_20, _t35_24);
        _t35_21 = _mm256_add_pd(_t35_21, _t35_25);
        _t35_22 = _mm256_add_pd(_t35_22, _t35_26);
        _t35_23 = _mm256_add_pd(_t35_23, _t35_27);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 52*k2, _t35_20);
        _asm256_storeu_pd(M2 + i0 + 52*k2 + 52, _t35_21);
        _asm256_storeu_pd(M2 + i0 + 52*k2 + 104, _t35_22);
        _asm256_storeu_pd(M2 + i0 + 52*k2 + 156, _t35_23);
      }
    }
    _t36_3 = _asm256_loadu_pd(Y + 53*k3);
    _t36_2 = _mm256_maskload_pd(Y + 53*k3 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t36_1 = _mm256_maskload_pd(Y + 53*k3 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t36_0 = _mm256_maskload_pd(Y + 53*k3 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t36_4 = _t36_3;
    _t36_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
    _t36_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
    _t36_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

    for( int i0 = 0; i0 <= 51; i0+=4 ) {
      _t37_3 = _asm256_loadu_pd(H + 52*i0 + k3);
      _t37_2 = _asm256_loadu_pd(H + 52*i0 + k3 + 52);
      _t37_1 = _asm256_loadu_pd(H + 52*i0 + k3 + 104);
      _t37_0 = _asm256_loadu_pd(H + 52*i0 + k3 + 156);
      _t37_4 = _asm256_loadu_pd(M2 + i0 + 52*k3);
      _t37_5 = _asm256_loadu_pd(M2 + i0 + 52*k3 + 52);
      _t37_6 = _asm256_loadu_pd(M2 + i0 + 52*k3 + 104);
      _t37_7 = _asm256_loadu_pd(M2 + i0 + 52*k3 + 156);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t37_12 = _t36_3;
      _t37_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
      _t37_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
      _t37_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 15), _t37_19)));
      _t37_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 15), _t37_19)));
      _t37_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 15), _t37_19)));
      _t37_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 15), _t37_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t37_4 = _mm256_add_pd(_t37_4, _t37_8);
      _t37_5 = _mm256_add_pd(_t37_5, _t37_9);
      _t37_6 = _mm256_add_pd(_t37_6, _t37_10);
      _t37_7 = _mm256_add_pd(_t37_7, _t37_11);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 52*k3, _t37_4);
      _asm256_storeu_pd(M2 + i0 + 52*k3 + 52, _t37_5);
      _asm256_storeu_pd(M2 + i0 + 52*k3 + 104, _t37_6);
      _asm256_storeu_pd(M2 + i0 + 52*k3 + 156, _t37_7);
    }

    for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 51; k2+=4 ) {

      for( int i0 = 0; i0 <= 51; i0+=4 ) {
        _t38_7 = _asm256_loadu_pd(Y + k2 + 52*k3);
        _t38_6 = _asm256_loadu_pd(Y + k2 + 52*k3 + 52);
        _t38_5 = _asm256_loadu_pd(Y + k2 + 52*k3 + 104);
        _t38_4 = _asm256_loadu_pd(Y + k2 + 52*k3 + 156);
        _t38_3 = _asm256_loadu_pd(H + 52*i0 + k3);
        _t38_2 = _asm256_loadu_pd(H + 52*i0 + k3 + 52);
        _t38_1 = _asm256_loadu_pd(H + 52*i0 + k3 + 104);
        _t38_0 = _asm256_loadu_pd(H + 52*i0 + k3 + 156);
        _t38_8 = _asm256_loadu_pd(M2 + i0 + 52*k2);
        _t38_9 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 52);
        _t38_10 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 104);
        _t38_11 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 156);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 32);
        _t38_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 32);
        _t38_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 49);
        _t38_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
        _t38_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
        _t38_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
        _t38_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t38_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 15), _t38_23)));
        _t38_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 15), _t38_23)));
        _t38_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 15), _t38_23)));
        _t38_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 15), _t38_23)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t38_8 = _mm256_add_pd(_t38_8, _t38_12);
        _t38_9 = _mm256_add_pd(_t38_9, _t38_13);
        _t38_10 = _mm256_add_pd(_t38_10, _t38_14);
        _t38_11 = _mm256_add_pd(_t38_11, _t38_15);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 52*k2, _t38_8);
        _asm256_storeu_pd(M2 + i0 + 52*k2 + 52, _t38_9);
        _asm256_storeu_pd(M2 + i0 + 52*k2 + 104, _t38_10);
        _asm256_storeu_pd(M2 + i0 + 52*k2 + 156, _t38_11);
      }
    }
  }


  for( int k2 = 0; k2 <= 47; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 51; i0+=4 ) {
      _t39_19 = _mm256_broadcast_sd(Y + 52*k2 + 48);
      _t39_18 = _mm256_broadcast_sd(Y + 52*k2 + 49);
      _t39_17 = _mm256_broadcast_sd(Y + 52*k2 + 50);
      _t39_16 = _mm256_broadcast_sd(Y + 52*k2 + 51);
      _t39_15 = _mm256_broadcast_sd(Y + 52*k2 + 100);
      _t39_14 = _mm256_broadcast_sd(Y + 52*k2 + 101);
      _t39_13 = _mm256_broadcast_sd(Y + 52*k2 + 102);
      _t39_12 = _mm256_broadcast_sd(Y + 52*k2 + 103);
      _t39_11 = _mm256_broadcast_sd(Y + 52*k2 + 152);
      _t39_10 = _mm256_broadcast_sd(Y + 52*k2 + 153);
      _t39_9 = _mm256_broadcast_sd(Y + 52*k2 + 154);
      _t39_8 = _mm256_broadcast_sd(Y + 52*k2 + 155);
      _t39_7 = _mm256_broadcast_sd(Y + 52*k2 + 204);
      _t39_6 = _mm256_broadcast_sd(Y + 52*k2 + 205);
      _t39_5 = _mm256_broadcast_sd(Y + 52*k2 + 206);
      _t39_4 = _mm256_broadcast_sd(Y + 52*k2 + 207);
      _t39_3 = _asm256_loadu_pd(H + 52*i0 + 48);
      _t39_2 = _asm256_loadu_pd(H + 52*i0 + 100);
      _t39_1 = _asm256_loadu_pd(H + 52*i0 + 152);
      _t39_0 = _asm256_loadu_pd(H + 52*i0 + 204);
      _t39_20 = _asm256_loadu_pd(M2 + i0 + 52*k2);
      _t39_21 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 52);
      _t39_22 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 104);
      _t39_23 = _asm256_loadu_pd(M2 + i0 + 52*k2 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t39_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 32);
      _t39_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 32);
      _t39_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 49);
      _t39_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t39_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_28), _mm256_mul_pd(_t39_18, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_30), _mm256_mul_pd(_t39_16, _t39_31)));
      _t39_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_28), _mm256_mul_pd(_t39_14, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_30), _mm256_mul_pd(_t39_12, _t39_31)));
      _t39_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_28), _mm256_mul_pd(_t39_10, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_30), _mm256_mul_pd(_t39_8, _t39_31)));
      _t39_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_28), _mm256_mul_pd(_t39_6, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_30), _mm256_mul_pd(_t39_4, _t39_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_20 = _mm256_add_pd(_t39_20, _t39_24);
      _t39_21 = _mm256_add_pd(_t39_21, _t39_25);
      _t39_22 = _mm256_add_pd(_t39_22, _t39_26);
      _t39_23 = _mm256_add_pd(_t39_23, _t39_27);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 52*k2, _t39_20);
      _asm256_storeu_pd(M2 + i0 + 52*k2 + 52, _t39_21);
      _asm256_storeu_pd(M2 + i0 + 52*k2 + 104, _t39_22);
      _asm256_storeu_pd(M2 + i0 + 52*k2 + 156, _t39_23);
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t40_0 = _t15_28;
  _t40_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t40_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t40_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int i0 = 0; i0 <= 51; i0+=4 ) {
    _t41_3 = _asm256_loadu_pd(H + 52*i0 + 48);
    _t41_2 = _asm256_loadu_pd(H + 52*i0 + 100);
    _t41_1 = _asm256_loadu_pd(H + 52*i0 + 152);
    _t41_0 = _asm256_loadu_pd(H + 52*i0 + 204);
    _t41_4 = _asm256_loadu_pd(M2 + i0 + 2496);
    _t41_5 = _asm256_loadu_pd(M2 + i0 + 2548);
    _t41_6 = _asm256_loadu_pd(M2 + i0 + 2600);
    _t41_7 = _asm256_loadu_pd(M2 + i0 + 2652);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t41_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 15), _t41_15)));
    _t41_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 15), _t41_15)));
    _t41_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 15), _t41_15)));
    _t41_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 15), _t41_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t41_4 = _mm256_add_pd(_t41_4, _t41_8);
    _t41_5 = _mm256_add_pd(_t41_5, _t41_9);
    _t41_6 = _mm256_add_pd(_t41_6, _t41_10);
    _t41_7 = _mm256_add_pd(_t41_7, _t41_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 2496, _t41_4);
    _asm256_storeu_pd(M2 + i0 + 2548, _t41_5);
    _asm256_storeu_pd(M2 + i0 + 2600, _t41_6);
    _asm256_storeu_pd(M2 + i0 + 2652, _t41_7);
  }


  // Generating : M3[52,52] = ( ( Sum_{k2} ( ( S(h(4, 52, k2), ( ( G(h(4, 52, k2), M1[52,52],h(4, 52, 0)) * T( G(h(4, 52, k2), H[52,52],h(4, 52, 0)) ) ) + G(h(4, 52, k2), R[52,52],h(4, 52, k2)) ),h(4, 52, k2)) + Sum_{i0} ( S(h(4, 52, k2), ( ( G(h(4, 52, k2), M1[52,52],h(4, 52, 0)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, 0)) ) ) + G(h(4, 52, k2), R[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) + S(h(4, 52, 48), ( ( G(h(4, 52, 48), M1[52,52],h(4, 52, 0)) * T( G(h(4, 52, 48), H[52,52],h(4, 52, 0)) ) ) + G(h(4, 52, 48), R[52,52],h(4, 52, 48)) ),h(4, 52, 48)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 52, k2), ( G(h(4, 52, k2), M1[52,52],h(4, 52, k3)) * T( G(h(4, 52, k2), H[52,52],h(4, 52, k3)) ) ),h(4, 52, k2)) + Sum_{i0} ( $(h(4, 52, k2), ( G(h(4, 52, k2), M1[52,52],h(4, 52, k3)) * T( G(h(4, 52, i0), H[52,52],h(4, 52, k3)) ) ),h(4, 52, i0)) ) ) ) + $(h(4, 52, 48), ( G(h(4, 52, 48), M1[52,52],h(4, 52, k3)) * T( G(h(4, 52, 48), H[52,52],h(4, 52, k3)) ) ),h(4, 52, 48)) ) ) )


  for( int k2 = 0; k2 <= 47; k2+=4 ) {
    _t42_23 = _mm256_broadcast_sd(M1 + 52*k2);
    _t42_22 = _mm256_broadcast_sd(M1 + 52*k2 + 1);
    _t42_21 = _mm256_broadcast_sd(M1 + 52*k2 + 2);
    _t42_20 = _mm256_broadcast_sd(M1 + 52*k2 + 3);
    _t42_19 = _mm256_broadcast_sd(M1 + 52*k2 + 52);
    _t42_18 = _mm256_broadcast_sd(M1 + 52*k2 + 53);
    _t42_17 = _mm256_broadcast_sd(M1 + 52*k2 + 54);
    _t42_16 = _mm256_broadcast_sd(M1 + 52*k2 + 55);
    _t42_15 = _mm256_broadcast_sd(M1 + 52*k2 + 104);
    _t42_14 = _mm256_broadcast_sd(M1 + 52*k2 + 105);
    _t42_13 = _mm256_broadcast_sd(M1 + 52*k2 + 106);
    _t42_12 = _mm256_broadcast_sd(M1 + 52*k2 + 107);
    _t42_11 = _mm256_broadcast_sd(M1 + 52*k2 + 156);
    _t42_10 = _mm256_broadcast_sd(M1 + 52*k2 + 157);
    _t42_9 = _mm256_broadcast_sd(M1 + 52*k2 + 158);
    _t42_8 = _mm256_broadcast_sd(M1 + 52*k2 + 159);
    _t42_7 = _asm256_loadu_pd(H + 52*k2);
    _t42_6 = _asm256_loadu_pd(H + 52*k2 + 52);
    _t42_5 = _asm256_loadu_pd(H + 52*k2 + 104);
    _t42_4 = _asm256_loadu_pd(H + 52*k2 + 156);
    _t42_3 = _asm256_loadu_pd(R + 53*k2);
    _t42_2 = _mm256_maskload_pd(R + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t42_1 = _mm256_maskload_pd(R + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t42_0 = _mm256_maskload_pd(R + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t42_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 32);
    _t42_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 32);
    _t42_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 49);
    _t42_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t42_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t42_40), _mm256_mul_pd(_t42_22, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t42_42), _mm256_mul_pd(_t42_20, _t42_43)));
    _t42_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t42_40), _mm256_mul_pd(_t42_18, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t42_42), _mm256_mul_pd(_t42_16, _t42_43)));
    _t42_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_40), _mm256_mul_pd(_t42_14, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_42), _mm256_mul_pd(_t42_12, _t42_43)));
    _t42_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_40), _mm256_mul_pd(_t42_10, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_42), _mm256_mul_pd(_t42_8, _t42_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t42_36 = _t42_3;
    _t42_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 3), _t42_2, 12);
    _t42_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 0), _t42_1, 49);
    _t42_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 12), _mm256_shuffle_pd(_t42_1, _t42_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t42_24 = _mm256_add_pd(_t42_32, _t42_36);
    _t42_25 = _mm256_add_pd(_t42_33, _t42_37);
    _t42_26 = _mm256_add_pd(_t42_34, _t42_38);
    _t42_27 = _mm256_add_pd(_t42_35, _t42_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t42_28 = _t42_24;
    _t42_29 = _t42_25;
    _t42_30 = _t42_26;
    _t42_31 = _t42_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
      _t43_7 = _asm256_loadu_pd(H + 52*i0);
      _t43_6 = _asm256_loadu_pd(H + 52*i0 + 52);
      _t43_5 = _asm256_loadu_pd(H + 52*i0 + 104);
      _t43_4 = _asm256_loadu_pd(H + 52*i0 + 156);
      _t43_3 = _asm256_loadu_pd(R + i0 + 52*k2);
      _t43_2 = _asm256_loadu_pd(R + i0 + 52*k2 + 52);
      _t43_1 = _asm256_loadu_pd(R + i0 + 52*k2 + 104);
      _t43_0 = _asm256_loadu_pd(R + i0 + 52*k2 + 156);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t43_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 32);
      _t43_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 32);
      _t43_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 49);
      _t43_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t43_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t43_16), _mm256_mul_pd(_t42_22, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t43_18), _mm256_mul_pd(_t42_20, _t43_19)));
      _t43_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t43_16), _mm256_mul_pd(_t42_18, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t43_18), _mm256_mul_pd(_t42_16, _t43_19)));
      _t43_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t43_16), _mm256_mul_pd(_t42_14, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t43_18), _mm256_mul_pd(_t42_12, _t43_19)));
      _t43_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t43_16), _mm256_mul_pd(_t42_10, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t43_18), _mm256_mul_pd(_t42_8, _t43_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t43_8 = _mm256_add_pd(_t43_12, _t43_3);
      _t43_9 = _mm256_add_pd(_t43_13, _t43_2);
      _t43_10 = _mm256_add_pd(_t43_14, _t43_1);
      _t43_11 = _mm256_add_pd(_t43_15, _t43_0);

      // AVX Storer:
      _asm256_storeu_pd(M3 + i0 + 52*k2, _t43_8);
      _asm256_storeu_pd(M3 + i0 + 52*k2 + 52, _t43_9);
      _asm256_storeu_pd(M3 + i0 + 52*k2 + 104, _t43_10);
      _asm256_storeu_pd(M3 + i0 + 52*k2 + 156, _t43_11);
    }
    _asm256_storeu_pd(M3 + 53*k2, _t42_28);
    _mm256_maskstore_pd(M3 + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t42_29);
    _mm256_maskstore_pd(M3 + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t42_30);
    _mm256_maskstore_pd(M3 + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t42_31);
  }

  _t44_23 = _mm256_broadcast_sd(M1 + 2496);
  _t44_22 = _mm256_broadcast_sd(M1 + 2497);
  _t44_21 = _mm256_broadcast_sd(M1 + 2498);
  _t44_20 = _mm256_broadcast_sd(M1 + 2499);
  _t44_19 = _mm256_broadcast_sd(M1 + 2548);
  _t44_18 = _mm256_broadcast_sd(M1 + 2549);
  _t44_17 = _mm256_broadcast_sd(M1 + 2550);
  _t44_16 = _mm256_broadcast_sd(M1 + 2551);
  _t44_15 = _mm256_broadcast_sd(M1 + 2600);
  _t44_14 = _mm256_broadcast_sd(M1 + 2601);
  _t44_13 = _mm256_broadcast_sd(M1 + 2602);
  _t44_12 = _mm256_broadcast_sd(M1 + 2603);
  _t44_11 = _mm256_broadcast_sd(M1 + 2652);
  _t44_10 = _mm256_broadcast_sd(M1 + 2653);
  _t44_9 = _mm256_broadcast_sd(M1 + 2654);
  _t44_8 = _mm256_broadcast_sd(M1 + 2655);
  _t44_7 = _asm256_loadu_pd(H + 2496);
  _t44_6 = _asm256_loadu_pd(H + 2548);
  _t44_5 = _asm256_loadu_pd(H + 2600);
  _t44_4 = _asm256_loadu_pd(H + 2652);
  _t44_3 = _asm256_loadu_pd(R + 2544);
  _t44_2 = _mm256_maskload_pd(R + 2596, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t44_1 = _mm256_maskload_pd(R + 2648, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t44_0 = _mm256_maskload_pd(R + 2700, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t44_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 32);
  _t44_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 32);
  _t44_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 49);
  _t44_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t44_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_23, _t44_40), _mm256_mul_pd(_t44_22, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_21, _t44_42), _mm256_mul_pd(_t44_20, _t44_43)));
  _t44_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_19, _t44_40), _mm256_mul_pd(_t44_18, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_17, _t44_42), _mm256_mul_pd(_t44_16, _t44_43)));
  _t44_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_40), _mm256_mul_pd(_t44_14, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_42), _mm256_mul_pd(_t44_12, _t44_43)));
  _t44_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_40), _mm256_mul_pd(_t44_10, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_42), _mm256_mul_pd(_t44_8, _t44_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t44_36 = _t44_3;
  _t44_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 3), _t44_2, 12);
  _t44_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 0), _t44_1, 49);
  _t44_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 12), _mm256_shuffle_pd(_t44_1, _t44_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t44_24 = _mm256_add_pd(_t44_32, _t44_36);
  _t44_25 = _mm256_add_pd(_t44_33, _t44_37);
  _t44_26 = _mm256_add_pd(_t44_34, _t44_38);
  _t44_27 = _mm256_add_pd(_t44_35, _t44_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t44_28 = _t44_24;
  _t44_29 = _t44_25;
  _t44_30 = _t44_26;
  _t44_31 = _t44_27;


  for( int k3 = 4; k3 <= 51; k3+=4 ) {

    for( int k2 = 0; k2 <= 47; k2+=4 ) {
      _t45_19 = _mm256_broadcast_sd(M1 + 52*k2 + k3);
      _t45_18 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 1);
      _t45_17 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 2);
      _t45_16 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 3);
      _t45_15 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 52);
      _t45_14 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 53);
      _t45_13 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 54);
      _t45_12 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 55);
      _t45_11 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 104);
      _t45_10 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 105);
      _t45_9 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 106);
      _t45_8 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 107);
      _t45_7 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 156);
      _t45_6 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 157);
      _t45_5 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 158);
      _t45_4 = _mm256_broadcast_sd(M1 + 52*k2 + k3 + 159);
      _t45_3 = _asm256_loadu_pd(H + 52*k2 + k3);
      _t45_2 = _asm256_loadu_pd(H + 52*k2 + k3 + 52);
      _t45_1 = _asm256_loadu_pd(H + 52*k2 + k3 + 104);
      _t45_0 = _asm256_loadu_pd(H + 52*k2 + k3 + 156);
      _t45_20 = _asm256_loadu_pd(M3 + 53*k2);
      _t45_21 = _mm256_maskload_pd(M3 + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t45_22 = _mm256_maskload_pd(M3 + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t45_23 = _mm256_maskload_pd(M3 + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t45_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
      _t45_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
      _t45_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
      _t45_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t45_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_32), _mm256_mul_pd(_t45_18, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_34), _mm256_mul_pd(_t45_16, _t45_35)));
      _t45_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_32), _mm256_mul_pd(_t45_14, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_34), _mm256_mul_pd(_t45_12, _t45_35)));
      _t45_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_32), _mm256_mul_pd(_t45_10, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_34), _mm256_mul_pd(_t45_8, _t45_35)));
      _t45_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_32), _mm256_mul_pd(_t45_6, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_34), _mm256_mul_pd(_t45_4, _t45_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t45_28 = _t45_20;
      _t45_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 3), _t45_21, 12);
      _t45_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 0), _t45_22, 49);
      _t45_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 12), _mm256_shuffle_pd(_t45_22, _t45_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t45_28 = _mm256_add_pd(_t45_28, _t45_24);
      _t45_29 = _mm256_add_pd(_t45_29, _t45_25);
      _t45_30 = _mm256_add_pd(_t45_30, _t45_26);
      _t45_31 = _mm256_add_pd(_t45_31, _t45_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t45_20 = _t45_28;
      _t45_21 = _t45_29;
      _t45_22 = _t45_30;
      _t45_23 = _t45_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
        _t46_3 = _asm256_loadu_pd(H + 52*i0 + k3);
        _t46_2 = _asm256_loadu_pd(H + 52*i0 + k3 + 52);
        _t46_1 = _asm256_loadu_pd(H + 52*i0 + k3 + 104);
        _t46_0 = _asm256_loadu_pd(H + 52*i0 + k3 + 156);
        _t46_4 = _asm256_loadu_pd(M3 + i0 + 52*k2);
        _t46_5 = _asm256_loadu_pd(M3 + i0 + 52*k2 + 52);
        _t46_6 = _asm256_loadu_pd(M3 + i0 + 52*k2 + 104);
        _t46_7 = _asm256_loadu_pd(M3 + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t46_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
        _t46_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
        _t46_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
        _t46_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t46_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t46_12), _mm256_mul_pd(_t45_18, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t46_14), _mm256_mul_pd(_t45_16, _t46_15)));
        _t46_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t46_12), _mm256_mul_pd(_t45_14, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t46_14), _mm256_mul_pd(_t45_12, _t46_15)));
        _t46_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t46_12), _mm256_mul_pd(_t45_10, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t46_14), _mm256_mul_pd(_t45_8, _t46_15)));
        _t46_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t46_12), _mm256_mul_pd(_t45_6, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t46_14), _mm256_mul_pd(_t45_4, _t46_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t46_4 = _mm256_add_pd(_t46_4, _t46_8);
        _t46_5 = _mm256_add_pd(_t46_5, _t46_9);
        _t46_6 = _mm256_add_pd(_t46_6, _t46_10);
        _t46_7 = _mm256_add_pd(_t46_7, _t46_11);

        // AVX Storer:
        _asm256_storeu_pd(M3 + i0 + 52*k2, _t46_4);
        _asm256_storeu_pd(M3 + i0 + 52*k2 + 52, _t46_5);
        _asm256_storeu_pd(M3 + i0 + 52*k2 + 104, _t46_6);
        _asm256_storeu_pd(M3 + i0 + 52*k2 + 156, _t46_7);
      }
      _asm256_storeu_pd(M3 + 53*k2, _t45_20);
      _mm256_maskstore_pd(M3 + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t45_21);
      _mm256_maskstore_pd(M3 + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t45_22);
      _mm256_maskstore_pd(M3 + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t45_23);
    }
    _t47_19 = _mm256_broadcast_sd(M1 + k3 + 2496);
    _t47_18 = _mm256_broadcast_sd(M1 + k3 + 2497);
    _t47_17 = _mm256_broadcast_sd(M1 + k3 + 2498);
    _t47_16 = _mm256_broadcast_sd(M1 + k3 + 2499);
    _t47_15 = _mm256_broadcast_sd(M1 + k3 + 2548);
    _t47_14 = _mm256_broadcast_sd(M1 + k3 + 2549);
    _t47_13 = _mm256_broadcast_sd(M1 + k3 + 2550);
    _t47_12 = _mm256_broadcast_sd(M1 + k3 + 2551);
    _t47_11 = _mm256_broadcast_sd(M1 + k3 + 2600);
    _t47_10 = _mm256_broadcast_sd(M1 + k3 + 2601);
    _t47_9 = _mm256_broadcast_sd(M1 + k3 + 2602);
    _t47_8 = _mm256_broadcast_sd(M1 + k3 + 2603);
    _t47_7 = _mm256_broadcast_sd(M1 + k3 + 2652);
    _t47_6 = _mm256_broadcast_sd(M1 + k3 + 2653);
    _t47_5 = _mm256_broadcast_sd(M1 + k3 + 2654);
    _t47_4 = _mm256_broadcast_sd(M1 + k3 + 2655);
    _t47_3 = _asm256_loadu_pd(H + k3 + 2496);
    _t47_2 = _asm256_loadu_pd(H + k3 + 2548);
    _t47_1 = _asm256_loadu_pd(H + k3 + 2600);
    _t47_0 = _asm256_loadu_pd(H + k3 + 2652);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32);
    _t47_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 32);
    _t47_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 49);
    _t47_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_19, _t47_28), _mm256_mul_pd(_t47_18, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_17, _t47_30), _mm256_mul_pd(_t47_16, _t47_31)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_28), _mm256_mul_pd(_t47_14, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_30), _mm256_mul_pd(_t47_12, _t47_31)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_28), _mm256_mul_pd(_t47_10, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_30), _mm256_mul_pd(_t47_8, _t47_31)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_28), _mm256_mul_pd(_t47_6, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_30), _mm256_mul_pd(_t47_4, _t47_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t47_24 = _t44_28;
    _t47_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 3), _t44_29, 12);
    _t47_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 0), _t44_30, 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 12), _mm256_shuffle_pd(_t44_30, _t44_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t47_24 = _mm256_add_pd(_t47_24, _t47_20);
    _t47_25 = _mm256_add_pd(_t47_25, _t47_21);
    _t47_26 = _mm256_add_pd(_t47_26, _t47_22);
    _t47_27 = _mm256_add_pd(_t47_27, _t47_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t44_28 = _t47_24;
    _t44_29 = _t47_25;
    _t44_30 = _t47_26;
    _t44_31 = _t47_27;
  }


  for( int fi35 = 0; fi35 <= 43; fi35+=4 ) {
    _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35])));
    _t48_2 = _mm256_maskload_pd(M3 + 53*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t48_3 = _mm256_maskload_pd(M3 + 53*fi35 + 53, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t48_4 = _mm256_maskload_pd(M3 + 53*fi35 + 105, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t48_5 = _mm256_maskload_pd(M3 + 53*fi35 + 157, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));

    // Generating : U[52,52] = S(h(1, 52, fi35), Sqrt( G(h(1, 52, fi35), U[52,52],h(1, 52, fi35)) ),h(1, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_16 = _t48_0;

    // 4-BLAC: sqrt(1x4)
    _t48_17 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_16)));

    // AVX Storer:
    _t48_0 = _t48_17;

    // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35), U[52,52],h(1, 52, fi35)) ),h(1, 52, fi35))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t48_18 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_19 = _t48_0;

    // 4-BLAC: 1x4 / 1x4
    _t48_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_18), _mm256_castpd256_pd128(_t48_19)));

    // AVX Storer:
    _t48_1 = _t48_20;

    // Generating : U[52,52] = S(h(1, 52, fi35), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35)) Kro G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ),h(3, 52, fi35 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t48_22 = _t48_2;

    // 4-BLAC: 1x4 Kro 1x4
    _t48_23 = _mm256_mul_pd(_t48_21, _t48_22);

    // AVX Storer:
    _t48_2 = _t48_23;

    // Generating : U[52,52] = S(h(3, 52, fi35 + 1), ( G(h(3, 52, fi35 + 1), U[52,52],h(3, 52, fi35 + 1)) - ( T( G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) * G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) ),h(3, 52, fi35 + 1))

    // AVX Loader:

    // 3x3 -> 4x4 - UpperTriang
    _t48_24 = _t48_3;
    _t48_25 = _t48_4;
    _t48_26 = _t48_5;
    _t48_27 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t48_28 = _t48_2;

    // 4-BLAC: (1x4)^T
    _t48_29 = _t48_28;

    // AVX Loader:

    // 1x3 -> 1x4
    _t48_30 = _t48_2;

    // 4-BLAC: 4x1 * 1x4
    _t48_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_29, _t48_29, 32), _mm256_permute2f128_pd(_t48_29, _t48_29, 32), 0), _t48_30);
    _t48_32 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_29, _t48_29, 32), _mm256_permute2f128_pd(_t48_29, _t48_29, 32), 15), _t48_30);
    _t48_33 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_29, _t48_29, 49), _mm256_permute2f128_pd(_t48_29, _t48_29, 49), 0), _t48_30);
    _t48_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_29, _t48_29, 49), _mm256_permute2f128_pd(_t48_29, _t48_29, 49), 15), _t48_30);

    // 4-BLAC: 4x4 - 4x4
    _t48_35 = _mm256_sub_pd(_t48_24, _t48_31);
    _t48_36 = _mm256_sub_pd(_t48_25, _t48_32);
    _t48_37 = _mm256_sub_pd(_t48_26, _t48_33);
    _t48_38 = _mm256_sub_pd(_t48_27, _t48_34);

    // AVX Storer:

    // 4x4 -> 3x3 - UpTriang
    _t48_3 = _t48_35;
    _t48_4 = _t48_36;
    _t48_5 = _t48_37;

    // Generating : U[52,52] = S(h(1, 52, fi35 + 1), Sqrt( G(h(1, 52, fi35 + 1), U[52,52],h(1, 52, fi35 + 1)) ),h(1, 52, fi35 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_39 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_3, 1);

    // 4-BLAC: sqrt(1x4)
    _t48_40 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_39)));

    // AVX Storer:
    _t48_6 = _t48_40;

    // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 1), U[52,52],h(1, 52, fi35 + 1)) ),h(1, 52, fi35 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t48_41 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_42 = _t48_6;

    // 4-BLAC: 1x4 / 1x4
    _t48_43 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_41), _mm256_castpd256_pd128(_t48_42)));

    // AVX Storer:
    _t48_7 = _t48_43;

    // Generating : U[52,52] = S(h(1, 52, fi35 + 1), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 1)) Kro G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ),h(2, 52, fi35 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_7, _t48_7, 32), _mm256_permute2f128_pd(_t48_7, _t48_7, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t48_45 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_3, 6), _mm256_permute2f128_pd(_t48_3, _t48_3, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t48_46 = _mm256_mul_pd(_t48_44, _t48_45);

    // AVX Storer:
    _t48_8 = _t48_46;

    // Generating : U[52,52] = S(h(2, 52, fi35 + 2), ( G(h(2, 52, fi35 + 2), U[52,52],h(2, 52, fi35 + 2)) - ( T( G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) * G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) ),h(2, 52, fi35 + 2))

    // AVX Loader:

    // 2x2 -> 4x4 - UpperTriang
    _t48_47 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 6), _mm256_permute2f128_pd(_t48_4, _t48_4, 129), 5);
    _t48_48 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_5, 4), _mm256_permute2f128_pd(_t48_5, _t48_5, 129), 5);
    _t48_49 = _mm256_setzero_pd();
    _t48_50 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t48_51 = _t48_8;

    // 4-BLAC: (1x4)^T
    _t48_52 = _t48_51;

    // AVX Loader:

    // 1x2 -> 1x4
    _t48_53 = _t48_8;

    // 4-BLAC: 4x1 * 1x4
    _t48_54 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_52, _t48_52, 32), _mm256_permute2f128_pd(_t48_52, _t48_52, 32), 0), _t48_53);
    _t48_55 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_52, _t48_52, 32), _mm256_permute2f128_pd(_t48_52, _t48_52, 32), 15), _t48_53);
    _t48_56 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_52, _t48_52, 49), _mm256_permute2f128_pd(_t48_52, _t48_52, 49), 0), _t48_53);
    _t48_57 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_52, _t48_52, 49), _mm256_permute2f128_pd(_t48_52, _t48_52, 49), 15), _t48_53);

    // 4-BLAC: 4x4 - 4x4
    _t48_58 = _mm256_sub_pd(_t48_47, _t48_54);
    _t48_59 = _mm256_sub_pd(_t48_48, _t48_55);
    _t48_60 = _mm256_sub_pd(_t48_49, _t48_56);
    _t48_61 = _mm256_sub_pd(_t48_50, _t48_57);

    // AVX Storer:

    // 4x4 -> 2x2 - UpTriang
    _t48_9 = _t48_58;
    _t48_10 = _t48_59;

    // Generating : U[52,52] = S(h(1, 52, fi35 + 2), Sqrt( G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ),h(1, 52, fi35 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_62 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_9, 1);

    // 4-BLAC: sqrt(1x4)
    _t48_63 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_62)));

    // AVX Storer:
    _t48_11 = _t48_63;

    // Generating : U[52,52] = S(h(1, 52, fi35 + 2), ( G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) Div G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ),h(1, 52, fi35 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_64 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_9, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_65 = _t48_11;

    // 4-BLAC: 1x4 / 1x4
    _t48_66 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_64), _mm256_castpd256_pd128(_t48_65)));

    // AVX Storer:
    _t48_12 = _t48_66;

    // Generating : U[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) - ( T( G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) Kro G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) ),h(1, 52, fi35 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_67 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_68 = _t48_12;

    // 4-BLAC: (4x1)^T
    _t48_69 = _t48_68;

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_70 = _t48_12;

    // 4-BLAC: 1x4 Kro 1x4
    _t48_71 = _mm256_mul_pd(_t48_69, _t48_70);

    // 4-BLAC: 1x4 - 1x4
    _t48_72 = _mm256_sub_pd(_t48_67, _t48_71);

    // AVX Storer:
    _t48_13 = _t48_72;

    // Generating : U[52,52] = S(h(1, 52, fi35 + 3), Sqrt( G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ),h(1, 52, fi35 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_73 = _t48_13;

    // 4-BLAC: sqrt(1x4)
    _t48_74 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_73)));

    // AVX Storer:
    _t48_13 = _t48_74;

    // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 2)) ),h(1, 52, fi35 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t48_75 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_76 = _t48_11;

    // 4-BLAC: 1x4 / 1x4
    _t48_77 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_75), _mm256_castpd256_pd128(_t48_76)));

    // AVX Storer:
    _t48_14 = _t48_77;

    // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 3), U[52,52],h(1, 52, fi35 + 3)) ),h(1, 52, fi35 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t48_78 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_79 = _t48_13;

    // 4-BLAC: 1x4 / 1x4
    _t48_80 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_78), _mm256_castpd256_pd128(_t48_79)));

    // AVX Storer:
    _t48_15 = _t48_80;
    _mm256_maskstore_pd(M3 + 53*fi35 + 53, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t48_3);
    _mm256_maskstore_pd(M3 + 53*fi35 + 106, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_9);

    for( int fi96 = 0; fi96 <= -fi35 + 44; fi96+=4 ) {
      _t49_3 = _asm256_loadu_pd(M3 + 53*fi35 + fi96 + 4);
      _t49_0 = _asm256_loadu_pd(M3 + 53*fi35 + fi96 + 56);
      _t49_1 = _asm256_loadu_pd(M3 + 53*fi35 + fi96 + 108);
      _t49_2 = _asm256_loadu_pd(M3 + 53*fi35 + fi96 + 160);

      // Generating : U[52,52] = S(h(1, 52, fi35), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35)) Kro G(h(1, 52, fi35), U[52,52],h(4, 52, fi35 + fi96 + 4)) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_5 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_3 = _mm256_mul_pd(_t49_5, _t49_3);

      // AVX Storer:

      // Generating : U[52,52] = S(h(3, 52, fi35 + 1), ( G(h(3, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) - ( T( G(h(1, 52, fi35), U[52,52],h(3, 52, fi35 + 1)) ) * G(h(1, 52, fi35), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // 3x4 -> 4x4
      _t49_6 = _t49_0;
      _t49_7 = _t49_1;
      _t49_8 = _t49_2;
      _t49_9 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t49_10 = _t48_2;

      // 4-BLAC: (1x4)^T
      _t49_11 = _t49_10;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t49_12 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_11, _t49_11, 32), _mm256_permute2f128_pd(_t49_11, _t49_11, 32), 0), _t49_3);
      _t49_13 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_11, _t49_11, 32), _mm256_permute2f128_pd(_t49_11, _t49_11, 32), 15), _t49_3);
      _t49_14 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_11, _t49_11, 49), _mm256_permute2f128_pd(_t49_11, _t49_11, 49), 0), _t49_3);
      _t49_15 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_11, _t49_11, 49), _mm256_permute2f128_pd(_t49_11, _t49_11, 49), 15), _t49_3);

      // 4-BLAC: 4x4 - 4x4
      _t49_16 = _mm256_sub_pd(_t49_6, _t49_12);
      _t49_17 = _mm256_sub_pd(_t49_7, _t49_13);
      _t49_18 = _mm256_sub_pd(_t49_8, _t49_14);
      _t49_19 = _mm256_sub_pd(_t49_9, _t49_15);

      // AVX Storer:
      _t49_0 = _t49_16;
      _t49_1 = _t49_17;
      _t49_2 = _t49_18;

      // Generating : U[52,52] = S(h(1, 52, fi35 + 1), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 1)) Kro G(h(1, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_20 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_7, _t48_7, 32), _mm256_permute2f128_pd(_t48_7, _t48_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_0 = _mm256_mul_pd(_t49_20, _t49_0);

      // AVX Storer:

      // Generating : U[52,52] = S(h(2, 52, fi35 + 2), ( G(h(2, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) - ( T( G(h(1, 52, fi35 + 1), U[52,52],h(2, 52, fi35 + 2)) ) * G(h(1, 52, fi35 + 1), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // 2x4 -> 4x4
      _t49_21 = _t49_1;
      _t49_22 = _t49_2;
      _t49_23 = _mm256_setzero_pd();
      _t49_24 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t49_25 = _t48_8;

      // 4-BLAC: (1x4)^T
      _t49_26 = _t49_25;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t49_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_26, _t49_26, 32), _mm256_permute2f128_pd(_t49_26, _t49_26, 32), 0), _t49_0);
      _t49_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_26, _t49_26, 32), _mm256_permute2f128_pd(_t49_26, _t49_26, 32), 15), _t49_0);
      _t49_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_26, _t49_26, 49), _mm256_permute2f128_pd(_t49_26, _t49_26, 49), 0), _t49_0);
      _t49_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_26, _t49_26, 49), _mm256_permute2f128_pd(_t49_26, _t49_26, 49), 15), _t49_0);

      // 4-BLAC: 4x4 - 4x4
      _t49_31 = _mm256_sub_pd(_t49_21, _t49_27);
      _t49_32 = _mm256_sub_pd(_t49_22, _t49_28);
      _t49_33 = _mm256_sub_pd(_t49_23, _t49_29);
      _t49_34 = _mm256_sub_pd(_t49_24, _t49_30);

      // AVX Storer:
      _t49_1 = _t49_31;
      _t49_2 = _t49_32;

      // Generating : U[52,52] = S(h(1, 52, fi35 + 2), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 2)) Kro G(h(1, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_14, _t48_14, 32), _mm256_permute2f128_pd(_t48_14, _t48_14, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_1 = _mm256_mul_pd(_t49_35, _t49_1);

      // AVX Storer:

      // Generating : U[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 52, fi35 + 3), U[52,52],h(4, 52, fi35 + fi96 + 4)) - ( T( G(h(1, 52, fi35 + 2), U[52,52],h(1, 52, fi35 + 3)) ) Kro G(h(1, 52, fi35 + 2), U[52,52],h(4, 52, fi35 + fi96 + 4)) ) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_12, _t48_12, 32), _mm256_permute2f128_pd(_t48_12, _t48_12, 32), 0);

      // 4-BLAC: (4x1)^T
      _t49_37 = _t49_36;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_4 = _mm256_mul_pd(_t49_37, _t49_1);

      // 4-BLAC: 1x4 - 1x4
      _t49_2 = _mm256_sub_pd(_t49_2, _t49_4);

      // AVX Storer:

      // Generating : U[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, fi35 + 3)) Kro G(h(1, 52, fi35 + 3), U[52,52],h(4, 52, fi35 + fi96 + 4)) ),h(4, 52, fi35 + fi96 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_38 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_15, _t48_15, 32), _mm256_permute2f128_pd(_t48_15, _t48_15, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_2 = _mm256_mul_pd(_t49_38, _t49_2);

      // AVX Storer:
      _asm256_storeu_pd(M3 + 53*fi35 + fi96 + 4, _t49_3);
      _asm256_storeu_pd(M3 + 53*fi35 + fi96 + 56, _t49_0);
      _asm256_storeu_pd(M3 + 53*fi35 + fi96 + 108, _t49_1);
      _asm256_storeu_pd(M3 + 53*fi35 + fi96 + 160, _t49_2);
    }

    // Generating : U[52,52] = ( Sum_{k3} ( ( S(h(4, 52, fi35 + k3 + 4), ( G(h(4, 52, fi35 + k3 + 4), U[52,52],h(4, 52, fi35 + k3 + 4)) - ( T( G(h(4, 52, fi35), U[52,52],h(4, 52, fi35 + k3 + 4)) ) * G(h(4, 52, fi35), U[52,52],h(4, 52, fi35 + k3 + 4)) ) ),h(4, 52, fi35 + k3 + 4)) + Sum_{k2} ( S(h(4, 52, fi35 + k3 + 4), ( G(h(4, 52, fi35 + k3 + 4), U[52,52],h(4, 52, fi35 + k2 + 4)) - ( T( G(h(4, 52, fi35), U[52,52],h(4, 52, fi35 + k3 + 4)) ) * G(h(4, 52, fi35), U[52,52],h(4, 52, fi35 + k2 + 4)) ) ),h(4, 52, fi35 + k2 + 4)) ) ) ) + S(h(4, 52, 48), ( G(h(4, 52, 48), U[52,52],h(4, 52, 48)) - ( T( G(h(4, 52, fi35), U[52,52],h(4, 52, 48)) ) * G(h(4, 52, fi35), U[52,52],h(4, 52, 48)) ) ),h(4, 52, 48)) )
    _mm_store_sd(&(M3[53*fi35]), _mm256_castpd256_pd128(_t48_0));
    _mm256_maskstore_pd(M3 + 53*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t48_2);
    _mm_store_sd(&(M3[53*fi35 + 53]), _mm256_castpd256_pd128(_t48_6));
    _mm256_maskstore_pd(M3 + 53*fi35 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_8);
    _mm_store_sd(&(M3[53*fi35 + 106]), _mm256_castpd256_pd128(_t48_11));
    _mm_store_sd(&(M3[53*fi35 + 107]), _mm256_castpd256_pd128(_t48_12));
    _mm_store_sd(&(M3[53*fi35 + 159]), _mm256_castpd256_pd128(_t48_13));

    for( int k3 = 0; k3 <= -fi35 + 40; k3+=4 ) {
      _t50_4 = _asm256_loadu_pd(M3 + 53*fi35 + 53*k3 + 212);
      _t50_5 = _mm256_maskload_pd(M3 + 53*fi35 + 53*k3 + 264, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t50_6 = _mm256_maskload_pd(M3 + 53*fi35 + 53*k3 + 316, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t50_7 = _mm256_maskload_pd(M3 + 53*fi35 + 53*k3 + 368, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
      _t50_3 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 4);
      _t50_2 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 56);
      _t50_1 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 108);
      _t50_0 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 160);

      // AVX Loader:

      // 4x4 -> 4x4 - UpperTriang
      _t50_16 = _t50_4;
      _t50_17 = _t50_5;
      _t50_18 = _t50_6;
      _t50_19 = _t50_7;

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t50_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_3, _t50_2), _mm256_unpacklo_pd(_t50_1, _t50_0), 32);
      _t50_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_3, _t50_2), _mm256_unpackhi_pd(_t50_1, _t50_0), 32);
      _t50_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_3, _t50_2), _mm256_unpacklo_pd(_t50_1, _t50_0), 49);
      _t50_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_3, _t50_2), _mm256_unpackhi_pd(_t50_1, _t50_0), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t50_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_20, _t50_20, 32), _mm256_permute2f128_pd(_t50_20, _t50_20, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_20, _t50_20, 32), _mm256_permute2f128_pd(_t50_20, _t50_20, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_20, _t50_20, 49), _mm256_permute2f128_pd(_t50_20, _t50_20, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_20, _t50_20, 49), _mm256_permute2f128_pd(_t50_20, _t50_20, 49), 15), _t50_0)));
      _t50_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_21, _t50_21, 32), _mm256_permute2f128_pd(_t50_21, _t50_21, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_21, _t50_21, 32), _mm256_permute2f128_pd(_t50_21, _t50_21, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_21, _t50_21, 49), _mm256_permute2f128_pd(_t50_21, _t50_21, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_21, _t50_21, 49), _mm256_permute2f128_pd(_t50_21, _t50_21, 49), 15), _t50_0)));
      _t50_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_22, _t50_22, 32), _mm256_permute2f128_pd(_t50_22, _t50_22, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_22, _t50_22, 32), _mm256_permute2f128_pd(_t50_22, _t50_22, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_22, _t50_22, 49), _mm256_permute2f128_pd(_t50_22, _t50_22, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_22, _t50_22, 49), _mm256_permute2f128_pd(_t50_22, _t50_22, 49), 15), _t50_0)));
      _t50_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_23, _t50_23, 32), _mm256_permute2f128_pd(_t50_23, _t50_23, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_23, _t50_23, 32), _mm256_permute2f128_pd(_t50_23, _t50_23, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_23, _t50_23, 49), _mm256_permute2f128_pd(_t50_23, _t50_23, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_23, _t50_23, 49), _mm256_permute2f128_pd(_t50_23, _t50_23, 49), 15), _t50_0)));

      // 4-BLAC: 4x4 - 4x4
      _t50_12 = _mm256_sub_pd(_t50_16, _t50_8);
      _t50_13 = _mm256_sub_pd(_t50_17, _t50_9);
      _t50_14 = _mm256_sub_pd(_t50_18, _t50_10);
      _t50_15 = _mm256_sub_pd(_t50_19, _t50_11);

      // AVX Storer:

      // 4x4 -> 4x4 - UpTriang
      _t50_4 = _t50_12;
      _t50_5 = _t50_13;
      _t50_6 = _t50_14;
      _t50_7 = _t50_15;

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= -fi35 + 47; k2+=4 ) {
        _t51_8 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 52*k3 + 212);
        _t51_9 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 52*k3 + 264);
        _t51_10 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 52*k3 + 316);
        _t51_11 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 52*k3 + 368);
        _t51_3 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 4);
        _t51_2 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 56);
        _t51_1 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 108);
        _t51_0 = _asm256_loadu_pd(M3 + 53*fi35 + k2 + 160);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t51_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_3, _t50_2), _mm256_unpacklo_pd(_t50_1, _t50_0), 32);
        _t51_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_3, _t50_2), _mm256_unpackhi_pd(_t50_1, _t50_0), 32);
        _t51_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_3, _t50_2), _mm256_unpacklo_pd(_t50_1, _t50_0), 49);
        _t51_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_3, _t50_2), _mm256_unpackhi_pd(_t50_1, _t50_0), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t51_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_12, _t51_12, 32), _mm256_permute2f128_pd(_t51_12, _t51_12, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_12, _t51_12, 32), _mm256_permute2f128_pd(_t51_12, _t51_12, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_12, _t51_12, 49), _mm256_permute2f128_pd(_t51_12, _t51_12, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_12, _t51_12, 49), _mm256_permute2f128_pd(_t51_12, _t51_12, 49), 15), _t51_0)));
        _t51_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_13, _t51_13, 32), _mm256_permute2f128_pd(_t51_13, _t51_13, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_13, _t51_13, 32), _mm256_permute2f128_pd(_t51_13, _t51_13, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_13, _t51_13, 49), _mm256_permute2f128_pd(_t51_13, _t51_13, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_13, _t51_13, 49), _mm256_permute2f128_pd(_t51_13, _t51_13, 49), 15), _t51_0)));
        _t51_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_14, _t51_14, 32), _mm256_permute2f128_pd(_t51_14, _t51_14, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_14, _t51_14, 32), _mm256_permute2f128_pd(_t51_14, _t51_14, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_14, _t51_14, 49), _mm256_permute2f128_pd(_t51_14, _t51_14, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_14, _t51_14, 49), _mm256_permute2f128_pd(_t51_14, _t51_14, 49), 15), _t51_0)));
        _t51_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_15, _t51_15, 32), _mm256_permute2f128_pd(_t51_15, _t51_15, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_15, _t51_15, 32), _mm256_permute2f128_pd(_t51_15, _t51_15, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_15, _t51_15, 49), _mm256_permute2f128_pd(_t51_15, _t51_15, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_15, _t51_15, 49), _mm256_permute2f128_pd(_t51_15, _t51_15, 49), 15), _t51_0)));

        // 4-BLAC: 4x4 - 4x4
        _t51_8 = _mm256_sub_pd(_t51_8, _t51_4);
        _t51_9 = _mm256_sub_pd(_t51_9, _t51_5);
        _t51_10 = _mm256_sub_pd(_t51_10, _t51_6);
        _t51_11 = _mm256_sub_pd(_t51_11, _t51_7);

        // AVX Storer:
        _asm256_storeu_pd(M3 + 53*fi35 + k2 + 52*k3 + 212, _t51_8);
        _asm256_storeu_pd(M3 + 53*fi35 + k2 + 52*k3 + 264, _t51_9);
        _asm256_storeu_pd(M3 + 53*fi35 + k2 + 52*k3 + 316, _t51_10);
        _asm256_storeu_pd(M3 + 53*fi35 + k2 + 52*k3 + 368, _t51_11);
      }
      _asm256_storeu_pd(M3 + 53*fi35 + 53*k3 + 212, _t50_4);
      _mm256_maskstore_pd(M3 + 53*fi35 + 53*k3 + 264, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t50_5);
      _mm256_maskstore_pd(M3 + 53*fi35 + 53*k3 + 316, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t50_6);
      _mm256_maskstore_pd(M3 + 53*fi35 + 53*k3 + 368, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t50_7);
    }
    _t52_3 = _asm256_loadu_pd(M3 + 52*fi35 + 48);
    _t52_2 = _asm256_loadu_pd(M3 + 52*fi35 + 100);
    _t52_1 = _asm256_loadu_pd(M3 + 52*fi35 + 152);
    _t52_0 = _asm256_loadu_pd(M3 + 52*fi35 + 204);

    // AVX Loader:

    // 4x4 -> 4x4 - UpperTriang
    _t52_12 = _t44_28;
    _t52_13 = _t44_29;
    _t52_14 = _t44_30;
    _t52_15 = _t44_31;

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t52_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_3, _t52_2), _mm256_unpacklo_pd(_t52_1, _t52_0), 32);
    _t52_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_3, _t52_2), _mm256_unpackhi_pd(_t52_1, _t52_0), 32);
    _t52_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_3, _t52_2), _mm256_unpacklo_pd(_t52_1, _t52_0), 49);
    _t52_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_3, _t52_2), _mm256_unpackhi_pd(_t52_1, _t52_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t52_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_16, _t52_16, 32), _mm256_permute2f128_pd(_t52_16, _t52_16, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_16, _t52_16, 32), _mm256_permute2f128_pd(_t52_16, _t52_16, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_16, _t52_16, 49), _mm256_permute2f128_pd(_t52_16, _t52_16, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_16, _t52_16, 49), _mm256_permute2f128_pd(_t52_16, _t52_16, 49), 15), _t52_0)));
    _t52_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_17, _t52_17, 32), _mm256_permute2f128_pd(_t52_17, _t52_17, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_17, _t52_17, 32), _mm256_permute2f128_pd(_t52_17, _t52_17, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_17, _t52_17, 49), _mm256_permute2f128_pd(_t52_17, _t52_17, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_17, _t52_17, 49), _mm256_permute2f128_pd(_t52_17, _t52_17, 49), 15), _t52_0)));
    _t52_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_18, _t52_18, 32), _mm256_permute2f128_pd(_t52_18, _t52_18, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_18, _t52_18, 32), _mm256_permute2f128_pd(_t52_18, _t52_18, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_18, _t52_18, 49), _mm256_permute2f128_pd(_t52_18, _t52_18, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_18, _t52_18, 49), _mm256_permute2f128_pd(_t52_18, _t52_18, 49), 15), _t52_0)));
    _t52_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 15), _t52_0)));

    // 4-BLAC: 4x4 - 4x4
    _t52_8 = _mm256_sub_pd(_t52_12, _t52_4);
    _t52_9 = _mm256_sub_pd(_t52_13, _t52_5);
    _t52_10 = _mm256_sub_pd(_t52_14, _t52_6);
    _t52_11 = _mm256_sub_pd(_t52_15, _t52_7);

    // AVX Storer:

    // 4x4 -> 4x4 - UpTriang
    _t44_28 = _t52_8;
    _t44_29 = _t52_9;
    _t44_30 = _t52_10;
    _t44_31 = _t52_11;
  }

  _t53_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[2332])));
  _t53_2 = _mm256_maskload_pd(M3 + 2333, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t53_3 = _mm256_maskload_pd(M3 + 2385, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t53_4 = _mm256_maskload_pd(M3 + 2437, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t53_5 = _mm256_maskload_pd(M3 + 2489, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t53_33 = _asm256_loadu_pd(M3 + 2336);
  _t53_16 = _asm256_loadu_pd(M3 + 2388);
  _t53_17 = _asm256_loadu_pd(M3 + 2440);
  _t53_18 = _asm256_loadu_pd(M3 + 2492);

  // Generating : U[52,52] = S(h(1, 52, 44), Sqrt( G(h(1, 52, 44), U[52,52],h(1, 52, 44)) ),h(1, 52, 44))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_43 = _t53_0;

  // 4-BLAC: sqrt(1x4)
  _t53_44 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_43)));

  // AVX Storer:
  _t53_0 = _t53_44;

  // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 44), U[52,52],h(1, 52, 44)) ),h(1, 52, 44))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t53_45 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_46 = _t53_0;

  // 4-BLAC: 1x4 / 1x4
  _t53_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_45), _mm256_castpd256_pd128(_t53_46)));

  // AVX Storer:
  _t53_1 = _t53_47;

  // Generating : U[52,52] = S(h(1, 52, 44), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 44)) Kro G(h(1, 52, 44), U[52,52],h(3, 52, 45)) ),h(3, 52, 45))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_48 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_1, _t53_1, 32), _mm256_permute2f128_pd(_t53_1, _t53_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_49 = _t53_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t53_50 = _mm256_mul_pd(_t53_48, _t53_49);

  // AVX Storer:
  _t53_2 = _t53_50;

  // Generating : U[52,52] = S(h(3, 52, 45), ( G(h(3, 52, 45), U[52,52],h(3, 52, 45)) - ( T( G(h(1, 52, 44), U[52,52],h(3, 52, 45)) ) * G(h(1, 52, 44), U[52,52],h(3, 52, 45)) ) ),h(3, 52, 45))

  // AVX Loader:

  // 3x3 -> 4x4 - UpperTriang
  _t53_51 = _t53_3;
  _t53_52 = _t53_4;
  _t53_53 = _t53_5;
  _t53_54 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_55 = _t53_2;

  // 4-BLAC: (1x4)^T
  _t53_56 = _t53_55;

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_57 = _t53_2;

  // 4-BLAC: 4x1 * 1x4
  _t53_58 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_56, _t53_56, 32), _mm256_permute2f128_pd(_t53_56, _t53_56, 32), 0), _t53_57);
  _t53_59 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_56, _t53_56, 32), _mm256_permute2f128_pd(_t53_56, _t53_56, 32), 15), _t53_57);
  _t53_60 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_56, _t53_56, 49), _mm256_permute2f128_pd(_t53_56, _t53_56, 49), 0), _t53_57);
  _t53_61 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_56, _t53_56, 49), _mm256_permute2f128_pd(_t53_56, _t53_56, 49), 15), _t53_57);

  // 4-BLAC: 4x4 - 4x4
  _t53_62 = _mm256_sub_pd(_t53_51, _t53_58);
  _t53_63 = _mm256_sub_pd(_t53_52, _t53_59);
  _t53_64 = _mm256_sub_pd(_t53_53, _t53_60);
  _t53_65 = _mm256_sub_pd(_t53_54, _t53_61);

  // AVX Storer:

  // 4x4 -> 3x3 - UpTriang
  _t53_3 = _t53_62;
  _t53_4 = _t53_63;
  _t53_5 = _t53_64;

  // Generating : U[52,52] = S(h(1, 52, 45), Sqrt( G(h(1, 52, 45), U[52,52],h(1, 52, 45)) ),h(1, 52, 45))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_66 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_3, 1);

  // 4-BLAC: sqrt(1x4)
  _t53_67 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_66)));

  // AVX Storer:
  _t53_6 = _t53_67;

  // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 45), U[52,52],h(1, 52, 45)) ),h(1, 52, 45))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t53_68 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_69 = _t53_6;

  // 4-BLAC: 1x4 / 1x4
  _t53_70 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_68), _mm256_castpd256_pd128(_t53_69)));

  // AVX Storer:
  _t53_7 = _t53_70;

  // Generating : U[52,52] = S(h(1, 52, 45), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 45)) Kro G(h(1, 52, 45), U[52,52],h(2, 52, 46)) ),h(2, 52, 46))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_71 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_7, _t53_7, 32), _mm256_permute2f128_pd(_t53_7, _t53_7, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_72 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_3, 6), _mm256_permute2f128_pd(_t53_3, _t53_3, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t53_73 = _mm256_mul_pd(_t53_71, _t53_72);

  // AVX Storer:
  _t53_8 = _t53_73;

  // Generating : U[52,52] = S(h(2, 52, 46), ( G(h(2, 52, 46), U[52,52],h(2, 52, 46)) - ( T( G(h(1, 52, 45), U[52,52],h(2, 52, 46)) ) * G(h(1, 52, 45), U[52,52],h(2, 52, 46)) ) ),h(2, 52, 46))

  // AVX Loader:

  // 2x2 -> 4x4 - UpperTriang
  _t53_74 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_4, 6), _mm256_permute2f128_pd(_t53_4, _t53_4, 129), 5);
  _t53_75 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_5, 4), _mm256_permute2f128_pd(_t53_5, _t53_5, 129), 5);
  _t53_76 = _mm256_setzero_pd();
  _t53_77 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_78 = _t53_8;

  // 4-BLAC: (1x4)^T
  _t53_79 = _t53_78;

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_80 = _t53_8;

  // 4-BLAC: 4x1 * 1x4
  _t53_81 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_79, _t53_79, 32), _mm256_permute2f128_pd(_t53_79, _t53_79, 32), 0), _t53_80);
  _t53_82 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_79, _t53_79, 32), _mm256_permute2f128_pd(_t53_79, _t53_79, 32), 15), _t53_80);
  _t53_83 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_79, _t53_79, 49), _mm256_permute2f128_pd(_t53_79, _t53_79, 49), 0), _t53_80);
  _t53_84 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_79, _t53_79, 49), _mm256_permute2f128_pd(_t53_79, _t53_79, 49), 15), _t53_80);

  // 4-BLAC: 4x4 - 4x4
  _t53_85 = _mm256_sub_pd(_t53_74, _t53_81);
  _t53_86 = _mm256_sub_pd(_t53_75, _t53_82);
  _t53_87 = _mm256_sub_pd(_t53_76, _t53_83);
  _t53_88 = _mm256_sub_pd(_t53_77, _t53_84);

  // AVX Storer:

  // 4x4 -> 2x2 - UpTriang
  _t53_9 = _t53_85;
  _t53_10 = _t53_86;

  // Generating : U[52,52] = S(h(1, 52, 46), Sqrt( G(h(1, 52, 46), U[52,52],h(1, 52, 46)) ),h(1, 52, 46))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_89 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_9, 1);

  // 4-BLAC: sqrt(1x4)
  _t53_90 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_89)));

  // AVX Storer:
  _t53_11 = _t53_90;

  // Generating : U[52,52] = S(h(1, 52, 46), ( G(h(1, 52, 46), U[52,52],h(1, 52, 47)) Div G(h(1, 52, 46), U[52,52],h(1, 52, 46)) ),h(1, 52, 47))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_91 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_92 = _t53_11;

  // 4-BLAC: 1x4 / 1x4
  _t53_93 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_91), _mm256_castpd256_pd128(_t53_92)));

  // AVX Storer:
  _t53_12 = _t53_93;

  // Generating : U[52,52] = S(h(1, 52, 47), ( G(h(1, 52, 47), U[52,52],h(1, 52, 47)) - ( T( G(h(1, 52, 46), U[52,52],h(1, 52, 47)) ) Kro G(h(1, 52, 46), U[52,52],h(1, 52, 47)) ) ),h(1, 52, 47))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_94 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_95 = _t53_12;

  // 4-BLAC: (4x1)^T
  _t53_96 = _t53_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_97 = _t53_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t53_98 = _mm256_mul_pd(_t53_96, _t53_97);

  // 4-BLAC: 1x4 - 1x4
  _t53_99 = _mm256_sub_pd(_t53_94, _t53_98);

  // AVX Storer:
  _t53_13 = _t53_99;

  // Generating : U[52,52] = S(h(1, 52, 47), Sqrt( G(h(1, 52, 47), U[52,52],h(1, 52, 47)) ),h(1, 52, 47))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_100 = _t53_13;

  // 4-BLAC: sqrt(1x4)
  _t53_101 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_100)));

  // AVX Storer:
  _t53_13 = _t53_101;

  // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 46), U[52,52],h(1, 52, 46)) ),h(1, 52, 46))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t53_102 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_103 = _t53_11;

  // 4-BLAC: 1x4 / 1x4
  _t53_104 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_102), _mm256_castpd256_pd128(_t53_103)));

  // AVX Storer:
  _t53_14 = _t53_104;

  // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 47), U[52,52],h(1, 52, 47)) ),h(1, 52, 47))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t53_105 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_106 = _t53_13;

  // 4-BLAC: 1x4 / 1x4
  _t53_107 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_105), _mm256_castpd256_pd128(_t53_106)));

  // AVX Storer:
  _t53_15 = _t53_107;

  // Generating : U[52,52] = S(h(1, 52, 44), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 44)) Kro G(h(1, 52, 44), U[52,52],h(4, 52, 48)) ),h(4, 52, 48))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_108 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_1, _t53_1, 32), _mm256_permute2f128_pd(_t53_1, _t53_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t53_33 = _mm256_mul_pd(_t53_108, _t53_33);

  // AVX Storer:

  // Generating : U[52,52] = S(h(3, 52, 45), ( G(h(3, 52, 45), U[52,52],h(4, 52, 48)) - ( T( G(h(1, 52, 44), U[52,52],h(3, 52, 45)) ) * G(h(1, 52, 44), U[52,52],h(4, 52, 48)) ) ),h(4, 52, 48))

  // AVX Loader:

  // 3x4 -> 4x4
  _t53_109 = _t53_16;
  _t53_110 = _t53_17;
  _t53_111 = _t53_18;
  _t53_112 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_113 = _t53_2;

  // 4-BLAC: (1x4)^T
  _t53_114 = _t53_113;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t53_115 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_114, _t53_114, 32), _mm256_permute2f128_pd(_t53_114, _t53_114, 32), 0), _t53_33);
  _t53_116 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_114, _t53_114, 32), _mm256_permute2f128_pd(_t53_114, _t53_114, 32), 15), _t53_33);
  _t53_117 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_114, _t53_114, 49), _mm256_permute2f128_pd(_t53_114, _t53_114, 49), 0), _t53_33);
  _t53_118 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_114, _t53_114, 49), _mm256_permute2f128_pd(_t53_114, _t53_114, 49), 15), _t53_33);

  // 4-BLAC: 4x4 - 4x4
  _t53_119 = _mm256_sub_pd(_t53_109, _t53_115);
  _t53_120 = _mm256_sub_pd(_t53_110, _t53_116);
  _t53_121 = _mm256_sub_pd(_t53_111, _t53_117);
  _t53_122 = _mm256_sub_pd(_t53_112, _t53_118);

  // AVX Storer:
  _t53_16 = _t53_119;
  _t53_17 = _t53_120;
  _t53_18 = _t53_121;

  // Generating : U[52,52] = S(h(1, 52, 45), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 45)) Kro G(h(1, 52, 45), U[52,52],h(4, 52, 48)) ),h(4, 52, 48))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_123 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_7, _t53_7, 32), _mm256_permute2f128_pd(_t53_7, _t53_7, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t53_16 = _mm256_mul_pd(_t53_123, _t53_16);

  // AVX Storer:

  // Generating : U[52,52] = S(h(2, 52, 46), ( G(h(2, 52, 46), U[52,52],h(4, 52, 48)) - ( T( G(h(1, 52, 45), U[52,52],h(2, 52, 46)) ) * G(h(1, 52, 45), U[52,52],h(4, 52, 48)) ) ),h(4, 52, 48))

  // AVX Loader:

  // 2x4 -> 4x4
  _t53_124 = _t53_17;
  _t53_125 = _t53_18;
  _t53_126 = _mm256_setzero_pd();
  _t53_127 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_128 = _t53_8;

  // 4-BLAC: (1x4)^T
  _t53_129 = _t53_128;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t53_130 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_129, _t53_129, 32), _mm256_permute2f128_pd(_t53_129, _t53_129, 32), 0), _t53_16);
  _t53_131 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_129, _t53_129, 32), _mm256_permute2f128_pd(_t53_129, _t53_129, 32), 15), _t53_16);
  _t53_132 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_129, _t53_129, 49), _mm256_permute2f128_pd(_t53_129, _t53_129, 49), 0), _t53_16);
  _t53_133 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_129, _t53_129, 49), _mm256_permute2f128_pd(_t53_129, _t53_129, 49), 15), _t53_16);

  // 4-BLAC: 4x4 - 4x4
  _t53_134 = _mm256_sub_pd(_t53_124, _t53_130);
  _t53_135 = _mm256_sub_pd(_t53_125, _t53_131);
  _t53_136 = _mm256_sub_pd(_t53_126, _t53_132);
  _t53_137 = _mm256_sub_pd(_t53_127, _t53_133);

  // AVX Storer:
  _t53_17 = _t53_134;
  _t53_18 = _t53_135;

  // Generating : U[52,52] = S(h(1, 52, 46), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 46)) Kro G(h(1, 52, 46), U[52,52],h(4, 52, 48)) ),h(4, 52, 48))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_138 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_14, _t53_14, 32), _mm256_permute2f128_pd(_t53_14, _t53_14, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t53_17 = _mm256_mul_pd(_t53_138, _t53_17);

  // AVX Storer:

  // Generating : U[52,52] = S(h(1, 52, 47), ( G(h(1, 52, 47), U[52,52],h(4, 52, 48)) - ( T( G(h(1, 52, 46), U[52,52],h(1, 52, 47)) ) Kro G(h(1, 52, 46), U[52,52],h(4, 52, 48)) ) ),h(4, 52, 48))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_139 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_12, _t53_12, 32), _mm256_permute2f128_pd(_t53_12, _t53_12, 32), 0);

  // 4-BLAC: (4x1)^T
  _t53_140 = _t53_139;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t53_34 = _mm256_mul_pd(_t53_140, _t53_17);

  // 4-BLAC: 1x4 - 1x4
  _t53_18 = _mm256_sub_pd(_t53_18, _t53_34);

  // AVX Storer:

  // Generating : U[52,52] = S(h(1, 52, 47), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 47)) Kro G(h(1, 52, 47), U[52,52],h(4, 52, 48)) ),h(4, 52, 48))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_141 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_15, _t53_15, 32), _mm256_permute2f128_pd(_t53_15, _t53_15, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t53_18 = _mm256_mul_pd(_t53_141, _t53_18);

  // AVX Storer:

  // Generating : U[52,52] = S(h(4, 52, 48), ( G(h(4, 52, 48), U[52,52],h(4, 52, 48)) - ( T( G(h(4, 52, 44), U[52,52],h(4, 52, 48)) ) * G(h(4, 52, 44), U[52,52],h(4, 52, 48)) ) ),h(4, 52, 48))

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t53_142 = _t44_28;
  _t53_143 = _t44_29;
  _t53_144 = _t44_30;
  _t53_145 = _t44_31;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t53_205 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_33, _t53_16), _mm256_unpacklo_pd(_t53_17, _t53_18), 32);
  _t53_206 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_33, _t53_16), _mm256_unpackhi_pd(_t53_17, _t53_18), 32);
  _t53_207 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_33, _t53_16), _mm256_unpacklo_pd(_t53_17, _t53_18), 49);
  _t53_208 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_33, _t53_16), _mm256_unpackhi_pd(_t53_17, _t53_18), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t53_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_205, _t53_205, 32), _mm256_permute2f128_pd(_t53_205, _t53_205, 32), 0), _t53_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_205, _t53_205, 32), _mm256_permute2f128_pd(_t53_205, _t53_205, 32), 15), _t53_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_205, _t53_205, 49), _mm256_permute2f128_pd(_t53_205, _t53_205, 49), 0), _t53_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_205, _t53_205, 49), _mm256_permute2f128_pd(_t53_205, _t53_205, 49), 15), _t53_18)));
  _t53_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_206, _t53_206, 32), _mm256_permute2f128_pd(_t53_206, _t53_206, 32), 0), _t53_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_206, _t53_206, 32), _mm256_permute2f128_pd(_t53_206, _t53_206, 32), 15), _t53_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_206, _t53_206, 49), _mm256_permute2f128_pd(_t53_206, _t53_206, 49), 0), _t53_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_206, _t53_206, 49), _mm256_permute2f128_pd(_t53_206, _t53_206, 49), 15), _t53_18)));
  _t53_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_207, _t53_207, 32), _mm256_permute2f128_pd(_t53_207, _t53_207, 32), 0), _t53_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_207, _t53_207, 32), _mm256_permute2f128_pd(_t53_207, _t53_207, 32), 15), _t53_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_207, _t53_207, 49), _mm256_permute2f128_pd(_t53_207, _t53_207, 49), 0), _t53_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_207, _t53_207, 49), _mm256_permute2f128_pd(_t53_207, _t53_207, 49), 15), _t53_18)));
  _t53_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_208, _t53_208, 32), _mm256_permute2f128_pd(_t53_208, _t53_208, 32), 0), _t53_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_208, _t53_208, 32), _mm256_permute2f128_pd(_t53_208, _t53_208, 32), 15), _t53_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_208, _t53_208, 49), _mm256_permute2f128_pd(_t53_208, _t53_208, 49), 0), _t53_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_208, _t53_208, 49), _mm256_permute2f128_pd(_t53_208, _t53_208, 49), 15), _t53_18)));

  // 4-BLAC: 4x4 - 4x4
  _t53_39 = _mm256_sub_pd(_t53_142, _t53_35);
  _t53_40 = _mm256_sub_pd(_t53_143, _t53_36);
  _t53_41 = _mm256_sub_pd(_t53_144, _t53_37);
  _t53_42 = _mm256_sub_pd(_t53_145, _t53_38);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t44_28 = _t53_39;
  _t44_29 = _t53_40;
  _t44_30 = _t53_41;
  _t44_31 = _t53_42;

  // Generating : U[52,52] = S(h(1, 52, 48), Sqrt( G(h(1, 52, 48), U[52,52],h(1, 52, 48)) ),h(1, 52, 48))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_146 = _mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 1);

  // 4-BLAC: sqrt(1x4)
  _t53_147 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_146)));

  // AVX Storer:
  _t53_19 = _t53_147;

  // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 48), U[52,52],h(1, 52, 48)) ),h(1, 52, 48))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t53_148 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_149 = _t53_19;

  // 4-BLAC: 1x4 / 1x4
  _t53_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_148), _mm256_castpd256_pd128(_t53_149)));

  // AVX Storer:
  _t53_20 = _t53_150;

  // Generating : U[52,52] = S(h(1, 52, 48), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 48)) Kro G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ),h(3, 52, 49))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_151 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_20, _t53_20, 32), _mm256_permute2f128_pd(_t53_20, _t53_20, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_152 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 14), _mm256_permute2f128_pd(_t44_28, _t44_28, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t53_153 = _mm256_mul_pd(_t53_151, _t53_152);

  // AVX Storer:
  _t53_21 = _t53_153;

  // Generating : U[52,52] = S(h(3, 52, 49), ( G(h(3, 52, 49), U[52,52],h(3, 52, 49)) - ( T( G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ) * G(h(1, 52, 48), U[52,52],h(3, 52, 49)) ) ),h(3, 52, 49))

  // AVX Loader:

  // 3x3 -> 4x4 - UpperTriang
  _t53_154 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_29, 14), _mm256_permute2f128_pd(_t44_29, _t44_29, 129), 5);
  _t53_155 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 12), _mm256_permute2f128_pd(_t44_30, _t44_30, 129), 5);
  _t53_156 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_31, 8), _mm256_setzero_pd());
  _t53_157 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_158 = _t53_21;

  // 4-BLAC: (1x4)^T
  _t53_159 = _t53_158;

  // AVX Loader:

  // 1x3 -> 1x4
  _t53_160 = _t53_21;

  // 4-BLAC: 4x1 * 1x4
  _t53_161 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_159, _t53_159, 32), _mm256_permute2f128_pd(_t53_159, _t53_159, 32), 0), _t53_160);
  _t53_162 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_159, _t53_159, 32), _mm256_permute2f128_pd(_t53_159, _t53_159, 32), 15), _t53_160);
  _t53_163 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_159, _t53_159, 49), _mm256_permute2f128_pd(_t53_159, _t53_159, 49), 0), _t53_160);
  _t53_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_159, _t53_159, 49), _mm256_permute2f128_pd(_t53_159, _t53_159, 49), 15), _t53_160);

  // 4-BLAC: 4x4 - 4x4
  _t53_165 = _mm256_sub_pd(_t53_154, _t53_161);
  _t53_166 = _mm256_sub_pd(_t53_155, _t53_162);
  _t53_167 = _mm256_sub_pd(_t53_156, _t53_163);
  _t53_168 = _mm256_sub_pd(_t53_157, _t53_164);

  // AVX Storer:

  // 4x4 -> 3x3 - UpTriang
  _t53_22 = _t53_165;
  _t53_23 = _t53_166;
  _t53_24 = _t53_167;

  // Generating : U[52,52] = S(h(1, 52, 49), Sqrt( G(h(1, 52, 49), U[52,52],h(1, 52, 49)) ),h(1, 52, 49))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_169 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_22, 1);

  // 4-BLAC: sqrt(1x4)
  _t53_170 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_169)));

  // AVX Storer:
  _t53_25 = _t53_170;

  // Generating : T1982[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 49), U[52,52],h(1, 52, 49)) ),h(1, 52, 49))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t53_171 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_172 = _t53_25;

  // 4-BLAC: 1x4 / 1x4
  _t53_173 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_171), _mm256_castpd256_pd128(_t53_172)));

  // AVX Storer:
  _t53_26 = _t53_173;

  // Generating : U[52,52] = S(h(1, 52, 49), ( G(h(1, 1, 0), T1982[1,52],h(1, 52, 49)) Kro G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ),h(2, 52, 50))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_174 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_26, _t53_26, 32), _mm256_permute2f128_pd(_t53_26, _t53_26, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_175 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_22, 6), _mm256_permute2f128_pd(_t53_22, _t53_22, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t53_176 = _mm256_mul_pd(_t53_174, _t53_175);

  // AVX Storer:
  _t53_27 = _t53_176;

  // Generating : U[52,52] = S(h(2, 52, 50), ( G(h(2, 52, 50), U[52,52],h(2, 52, 50)) - ( T( G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ) * G(h(1, 52, 49), U[52,52],h(2, 52, 50)) ) ),h(2, 52, 50))

  // AVX Loader:

  // 2x2 -> 4x4 - UpperTriang
  _t53_177 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_23, 6), _mm256_permute2f128_pd(_t53_23, _t53_23, 129), 5);
  _t53_178 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_24, 4), _mm256_permute2f128_pd(_t53_24, _t53_24, 129), 5);
  _t53_179 = _mm256_setzero_pd();
  _t53_180 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_181 = _t53_27;

  // 4-BLAC: (1x4)^T
  _t53_182 = _t53_181;

  // AVX Loader:

  // 1x2 -> 1x4
  _t53_183 = _t53_27;

  // 4-BLAC: 4x1 * 1x4
  _t53_184 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_182, _t53_182, 32), _mm256_permute2f128_pd(_t53_182, _t53_182, 32), 0), _t53_183);
  _t53_185 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_182, _t53_182, 32), _mm256_permute2f128_pd(_t53_182, _t53_182, 32), 15), _t53_183);
  _t53_186 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_182, _t53_182, 49), _mm256_permute2f128_pd(_t53_182, _t53_182, 49), 0), _t53_183);
  _t53_187 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_182, _t53_182, 49), _mm256_permute2f128_pd(_t53_182, _t53_182, 49), 15), _t53_183);

  // 4-BLAC: 4x4 - 4x4
  _t53_188 = _mm256_sub_pd(_t53_177, _t53_184);
  _t53_189 = _mm256_sub_pd(_t53_178, _t53_185);
  _t53_190 = _mm256_sub_pd(_t53_179, _t53_186);
  _t53_191 = _mm256_sub_pd(_t53_180, _t53_187);

  // AVX Storer:

  // 4x4 -> 2x2 - UpTriang
  _t53_28 = _t53_188;
  _t53_29 = _t53_189;

  // Generating : U[52,52] = S(h(1, 52, 50), Sqrt( G(h(1, 52, 50), U[52,52],h(1, 52, 50)) ),h(1, 52, 50))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_192 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_28, 1);

  // 4-BLAC: sqrt(1x4)
  _t53_193 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_192)));

  // AVX Storer:
  _t53_30 = _t53_193;

  // Generating : U[52,52] = S(h(1, 52, 50), ( G(h(1, 52, 50), U[52,52],h(1, 52, 51)) Div G(h(1, 52, 50), U[52,52],h(1, 52, 50)) ),h(1, 52, 51))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_194 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_28, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_195 = _t53_30;

  // 4-BLAC: 1x4 / 1x4
  _t53_196 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_194), _mm256_castpd256_pd128(_t53_195)));

  // AVX Storer:
  _t53_31 = _t53_196;

  // Generating : U[52,52] = S(h(1, 52, 51), ( G(h(1, 52, 51), U[52,52],h(1, 52, 51)) - ( T( G(h(1, 52, 50), U[52,52],h(1, 52, 51)) ) Kro G(h(1, 52, 50), U[52,52],h(1, 52, 51)) ) ),h(1, 52, 51))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_197 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_198 = _t53_31;

  // 4-BLAC: (4x1)^T
  _t53_199 = _t53_198;

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_200 = _t53_31;

  // 4-BLAC: 1x4 Kro 1x4
  _t53_201 = _mm256_mul_pd(_t53_199, _t53_200);

  // 4-BLAC: 1x4 - 1x4
  _t53_202 = _mm256_sub_pd(_t53_197, _t53_201);

  // AVX Storer:
  _t53_32 = _t53_202;

  // Generating : U[52,52] = S(h(1, 52, 51), Sqrt( G(h(1, 52, 51), U[52,52],h(1, 52, 51)) ),h(1, 52, 51))

  // AVX Loader:

  // 1x1 -> 1x4
  _t53_203 = _t53_32;

  // 4-BLAC: sqrt(1x4)
  _t53_204 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_203)));

  // AVX Storer:
  _t53_32 = _t53_204;

  _mm_store_sd(&(M3[2332]), _mm256_castpd256_pd128(_t53_0));
  _mm256_maskstore_pd(M3 + 2333, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t53_2);
  _mm_store_sd(&(M3[2385]), _mm256_castpd256_pd128(_t53_6));
  _mm256_maskstore_pd(M3 + 2386, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t53_8);
  _mm_store_sd(&(M3[2438]), _mm256_castpd256_pd128(_t53_11));
  _mm_store_sd(&(M3[2439]), _mm256_castpd256_pd128(_t53_12));
  _mm_store_sd(&(M3[2491]), _mm256_castpd256_pd128(_t53_13));
  _asm256_storeu_pd(M3 + 2336, _t53_33);
  _asm256_storeu_pd(M3 + 2388, _t53_16);
  _asm256_storeu_pd(M3 + 2440, _t53_17);
  _asm256_storeu_pd(M3 + 2492, _t53_18);

  for( int fi35 = 0; fi35 <= 47; fi35+=4 ) {
    _t54_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 159])));
    _t54_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 106])));
    _t54_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 53])));
    _t54_5 = _mm256_maskload_pd(M3 + 53*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t54_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 107])));
    _t54_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35])));
    _t54_3 = _mm256_maskload_pd(M3 + 53*fi35 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t54_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35])));
    _t54_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35])));
    _t54_8 = _mm256_maskload_pd(v0 + fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t54_5 = _mm256_maskload_pd(M3 + 53*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t54_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 53])));
    _t54_3 = _mm256_maskload_pd(M3 + 53*fi35 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t54_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 106])));
    _t54_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 107])));
    _t54_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 159])));

    // Generating : v2[52,1] = S(h(1, 52, fi35), ( G(h(1, 52, fi35), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, fi35), U0[52,52],h(1, 52, fi35)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_13 = _t54_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_14 = _t54_6;

    // 4-BLAC: 1x4 / 1x4
    _t54_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_13), _mm256_castpd256_pd128(_t54_14)));

    // AVX Storer:
    _t54_7 = _t54_15;

    // Generating : v2[52,1] = S(h(3, 52, fi35 + 1), ( G(h(3, 52, fi35 + 1), v2[52,1],h(1, 1, 0)) - ( T( G(h(1, 52, fi35), U0[52,52],h(3, 52, fi35 + 1)) ) Kro G(h(1, 52, fi35), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t54_16 = _t54_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t54_17 = _t54_5;

    // 4-BLAC: (1x4)^T
    _t54_18 = _t54_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_7, _t54_7, 32), _mm256_permute2f128_pd(_t54_7, _t54_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t54_20 = _mm256_mul_pd(_t54_18, _t54_19);

    // 4-BLAC: 4x1 - 4x1
    _t54_21 = _mm256_sub_pd(_t54_16, _t54_20);

    // AVX Storer:
    _t54_8 = _t54_21;

    // Generating : v2[52,1] = S(h(1, 52, fi35 + 1), ( G(h(1, 52, fi35 + 1), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 1), U0[52,52],h(1, 52, fi35 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_23 = _t54_4;

    // 4-BLAC: 1x4 / 1x4
    _t54_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_22), _mm256_castpd256_pd128(_t54_23)));

    // AVX Storer:
    _t54_9 = _t54_24;

    // Generating : v2[52,1] = S(h(2, 52, fi35 + 2), ( G(h(2, 52, fi35 + 2), v2[52,1],h(1, 1, 0)) - ( T( G(h(1, 52, fi35 + 1), U0[52,52],h(2, 52, fi35 + 2)) ) Kro G(h(1, 52, fi35 + 1), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t54_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_8, 6), _mm256_permute2f128_pd(_t54_8, _t54_8, 129), 5);

    // AVX Loader:

    // 1x2 -> 1x4
    _t54_26 = _t54_3;

    // 4-BLAC: (1x4)^T
    _t54_27 = _t54_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t54_29 = _mm256_mul_pd(_t54_27, _t54_28);

    // 4-BLAC: 4x1 - 4x1
    _t54_30 = _mm256_sub_pd(_t54_25, _t54_29);

    // AVX Storer:
    _t54_10 = _t54_30;

    // Generating : v2[52,1] = S(h(1, 52, fi35 + 2), ( G(h(1, 52, fi35 + 2), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 2), U0[52,52],h(1, 52, fi35 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_32 = _t54_2;

    // 4-BLAC: 1x4 / 1x4
    _t54_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_31), _mm256_castpd256_pd128(_t54_32)));

    // AVX Storer:
    _t54_11 = _t54_33;

    // Generating : v2[52,1] = S(h(1, 52, fi35 + 3), ( G(h(1, 52, fi35 + 3), v2[52,1],h(1, 1, 0)) - ( T( G(h(1, 52, fi35 + 2), U0[52,52],h(1, 52, fi35 + 3)) ) Kro G(h(1, 52, fi35 + 2), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_34 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_35 = _t54_1;

    // 4-BLAC: (4x1)^T
    _t54_36 = _t54_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_37 = _t54_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t54_38 = _mm256_mul_pd(_t54_36, _t54_37);

    // 4-BLAC: 1x4 - 1x4
    _t54_39 = _mm256_sub_pd(_t54_34, _t54_38);

    // AVX Storer:
    _t54_12 = _t54_39;

    // Generating : v2[52,1] = S(h(1, 52, fi35 + 3), ( G(h(1, 52, fi35 + 3), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 3), U0[52,52],h(1, 52, fi35 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_40 = _t54_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_41 = _t54_0;

    // 4-BLAC: 1x4 / 1x4
    _t54_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_40), _mm256_castpd256_pd128(_t54_41)));

    // AVX Storer:
    _t54_12 = _t54_42;

    // Generating : v2[52,1] = Sum_{k3} ( S(h(4, 52, fi35 + k3 + 4), ( G(h(4, 52, fi35 + k3 + 4), v2[52,1],h(1, 1, 0)) - ( T( G(h(4, 52, fi35), U0[52,52],h(4, 52, fi35 + k3 + 4)) ) * G(h(4, 52, fi35), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[fi35]), _mm256_castpd256_pd128(_t54_7));
    _mm_store_sd(&(v0[fi35 + 1]), _mm256_castpd256_pd128(_t54_9));
    _mm_store_sd(&(v0[fi35 + 2]), _mm256_castpd256_pd128(_t54_11));
    _mm_store_sd(&(v0[fi35 + 3]), _mm256_castpd256_pd128(_t54_12));

    for( int k3 = 0; k3 <= -fi35 + 47; k3+=4 ) {
      _t55_9 = _asm256_loadu_pd(v0 + fi35 + k3 + 4);
      _t55_7 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 4);
      _t55_6 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 56);
      _t55_5 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 108);
      _t55_4 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 160);
      _t55_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35])));
      _t55_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35 + 1])));
      _t55_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35 + 2])));
      _t55_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35 + 3])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t55_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_7, _t55_6), _mm256_unpacklo_pd(_t55_5, _t55_4), 32);
      _t55_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_7, _t55_6), _mm256_unpackhi_pd(_t55_5, _t55_4), 32);
      _t55_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_7, _t55_6), _mm256_unpacklo_pd(_t55_5, _t55_4), 49);
      _t55_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_7, _t55_6), _mm256_unpackhi_pd(_t55_5, _t55_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t55_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t55_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t55_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t55_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t55_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t55_9 = _mm256_sub_pd(_t55_9, _t55_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + fi35 + k3 + 4, _t55_9);
    }
  }

  _t56_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[48])));
  _t56_1 = _mm256_maskload_pd(v0 + 49, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v2[52,1] = S(h(1, 52, 48), ( G(h(1, 52, 48), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, 48), U0[52,52],h(1, 52, 48)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_6 = _t56_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_7 = _t53_19;

  // 4-BLAC: 1x4 / 1x4
  _t56_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_6), _mm256_castpd256_pd128(_t56_7)));

  // AVX Storer:
  _t56_0 = _t56_8;

  // Generating : v2[52,1] = S(h(3, 52, 49), ( G(h(3, 52, 49), v2[52,1],h(1, 1, 0)) - ( T( G(h(1, 52, 48), U0[52,52],h(3, 52, 49)) ) Kro G(h(1, 52, 48), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_9 = _t56_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_10 = _t53_21;

  // 4-BLAC: (1x4)^T
  _t56_11 = _t56_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t56_13 = _mm256_mul_pd(_t56_11, _t56_12);

  // 4-BLAC: 4x1 - 4x1
  _t56_14 = _mm256_sub_pd(_t56_9, _t56_13);

  // AVX Storer:
  _t56_1 = _t56_14;

  // Generating : v2[52,1] = S(h(1, 52, 49), ( G(h(1, 52, 49), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, 49), U0[52,52],h(1, 52, 49)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_15 = _mm256_blend_pd(_mm256_setzero_pd(), _t56_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_16 = _t53_25;

  // 4-BLAC: 1x4 / 1x4
  _t56_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_15), _mm256_castpd256_pd128(_t56_16)));

  // AVX Storer:
  _t56_2 = _t56_17;

  // Generating : v2[52,1] = S(h(2, 52, 50), ( G(h(2, 52, 50), v2[52,1],h(1, 1, 0)) - ( T( G(h(1, 52, 49), U0[52,52],h(2, 52, 50)) ) Kro G(h(1, 52, 49), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_18 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t56_1, 6), _mm256_permute2f128_pd(_t56_1, _t56_1, 129), 5);

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_19 = _t53_27;

  // 4-BLAC: (1x4)^T
  _t56_20 = _t56_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t56_22 = _mm256_mul_pd(_t56_20, _t56_21);

  // 4-BLAC: 4x1 - 4x1
  _t56_23 = _mm256_sub_pd(_t56_18, _t56_22);

  // AVX Storer:
  _t56_3 = _t56_23;

  // Generating : v2[52,1] = S(h(1, 52, 50), ( G(h(1, 52, 50), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, 50), U0[52,52],h(1, 52, 50)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t56_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_25 = _t53_30;

  // 4-BLAC: 1x4 / 1x4
  _t56_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_24), _mm256_castpd256_pd128(_t56_25)));

  // AVX Storer:
  _t56_4 = _t56_26;

  // Generating : v2[52,1] = S(h(1, 52, 51), ( G(h(1, 52, 51), v2[52,1],h(1, 1, 0)) - ( T( G(h(1, 52, 50), U0[52,52],h(1, 52, 51)) ) Kro G(h(1, 52, 50), v2[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t56_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_28 = _t53_31;

  // 4-BLAC: (4x1)^T
  _t56_29 = _t56_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_30 = _t56_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_31 = _mm256_mul_pd(_t56_29, _t56_30);

  // 4-BLAC: 1x4 - 1x4
  _t56_32 = _mm256_sub_pd(_t56_27, _t56_31);

  // AVX Storer:
  _t56_5 = _t56_32;

  // Generating : v2[52,1] = S(h(1, 52, 51), ( G(h(1, 52, 51), v2[52,1],h(1, 1, 0)) Div G(h(1, 52, 51), U0[52,52],h(1, 52, 51)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_33 = _t56_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_34 = _t53_32;

  // 4-BLAC: 1x4 / 1x4
  _t56_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_33), _mm256_castpd256_pd128(_t56_34)));

  // AVX Storer:
  _t56_5 = _t56_35;

  _mm_store_sd(&(M3[2544]), _mm256_castpd256_pd128(_t53_19));
  _mm256_maskstore_pd(M3 + 2545, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t53_21);
  _mm_store_sd(&(M3[2597]), _mm256_castpd256_pd128(_t53_25));
  _mm256_maskstore_pd(M3 + 2598, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t53_27);
  _mm_store_sd(&(M3[2650]), _mm256_castpd256_pd128(_t53_30));
  _mm_store_sd(&(M3[2651]), _mm256_castpd256_pd128(_t53_31));
  _mm_store_sd(&(M3[2703]), _mm256_castpd256_pd128(_t53_32));
  _mm_store_sd(&(v0[48]), _mm256_castpd256_pd128(_t56_0));
  _mm_store_sd(&(v0[49]), _mm256_castpd256_pd128(_t56_2));
  _mm_store_sd(&(v0[50]), _mm256_castpd256_pd128(_t56_4));
  _mm_store_sd(&(v0[51]), _mm256_castpd256_pd128(_t56_5));

  for( int fi35 = 0; fi35 <= 47; fi35+=4 ) {
    _t57_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 51])));
    _t57_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2703])));
    _t57_8 = _mm256_maskload_pd(v0 + -fi35 + 48, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t57_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2547)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2599))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2651)), 32);
    _t57_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2650])));
    _t57_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2546)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2598)), 0);
    _t57_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2597])));
    _t57_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2545])));
    _t57_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2544])));

    // Generating : v4[52,1] = S(h(1, 52, -fi35 + 51), ( G(h(1, 52, -fi35 + 51), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 51), U0[52,52],h(1, 52, -fi35 + 51)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_13 = _t57_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_14 = _t57_6;

    // 4-BLAC: 1x4 / 1x4
    _t57_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_13), _mm256_castpd256_pd128(_t57_14)));

    // AVX Storer:
    _t57_7 = _t57_15;

    // Generating : v4[52,1] = S(h(3, 52, -fi35 + 48), ( G(h(3, 52, -fi35 + 48), v4[52,1],h(1, 1, 0)) - ( G(h(3, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 51)) Kro G(h(1, 52, -fi35 + 51), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t57_16 = _t57_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t57_17 = _t57_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_7, _t57_7, 32), _mm256_permute2f128_pd(_t57_7, _t57_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t57_19 = _mm256_mul_pd(_t57_17, _t57_18);

    // 4-BLAC: 4x1 - 4x1
    _t57_20 = _mm256_sub_pd(_t57_16, _t57_19);

    // AVX Storer:
    _t57_8 = _t57_20;

    // Generating : v4[52,1] = S(h(1, 52, -fi35 + 50), ( G(h(1, 52, -fi35 + 50), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 50), U0[52,52],h(1, 52, -fi35 + 50)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t57_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t57_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_22 = _t57_4;

    // 4-BLAC: 1x4 / 1x4
    _t57_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_21), _mm256_castpd256_pd128(_t57_22)));

    // AVX Storer:
    _t57_9 = _t57_23;

    // Generating : v4[52,1] = S(h(2, 52, -fi35 + 48), ( G(h(2, 52, -fi35 + 48), v4[52,1],h(1, 1, 0)) - ( G(h(2, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 50)) Kro G(h(1, 52, -fi35 + 50), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t57_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t57_8, 3);

    // AVX Loader:

    // 2x1 -> 4x1
    _t57_25 = _t57_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_9, _t57_9, 32), _mm256_permute2f128_pd(_t57_9, _t57_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t57_27 = _mm256_mul_pd(_t57_25, _t57_26);

    // 4-BLAC: 4x1 - 4x1
    _t57_28 = _mm256_sub_pd(_t57_24, _t57_27);

    // AVX Storer:
    _t57_10 = _t57_28;

    // Generating : v4[52,1] = S(h(1, 52, -fi35 + 49), ( G(h(1, 52, -fi35 + 49), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 49), U0[52,52],h(1, 52, -fi35 + 49)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t57_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_30 = _t57_2;

    // 4-BLAC: 1x4 / 1x4
    _t57_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_29), _mm256_castpd256_pd128(_t57_30)));

    // AVX Storer:
    _t57_11 = _t57_31;

    // Generating : v4[52,1] = S(h(1, 52, -fi35 + 48), ( G(h(1, 52, -fi35 + 48), v4[52,1],h(1, 1, 0)) - ( G(h(1, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 49)) Kro G(h(1, 52, -fi35 + 49), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_32 = _mm256_blend_pd(_mm256_setzero_pd(), _t57_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_33 = _t57_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_34 = _t57_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t57_35 = _mm256_mul_pd(_t57_33, _t57_34);

    // 4-BLAC: 1x4 - 1x4
    _t57_36 = _mm256_sub_pd(_t57_32, _t57_35);

    // AVX Storer:
    _t57_12 = _t57_36;

    // Generating : v4[52,1] = S(h(1, 52, -fi35 + 48), ( G(h(1, 52, -fi35 + 48), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 48)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_37 = _t57_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_38 = _t57_0;

    // 4-BLAC: 1x4 / 1x4
    _t57_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_37), _mm256_castpd256_pd128(_t57_38)));

    // AVX Storer:
    _t57_12 = _t57_39;

    // Generating : v4[52,1] = Sum_{k3} ( S(h(4, 52, k3), ( G(h(4, 52, k3), v4[52,1],h(1, 1, 0)) - ( G(h(4, 52, k3), U0[52,52],h(4, 52, -fi35 + 48)) * G(h(4, 52, -fi35 + 48), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[-fi35 + 51]), _mm256_castpd256_pd128(_t57_7));
    _mm_store_sd(&(v0[-fi35 + 50]), _mm256_castpd256_pd128(_t57_9));
    _mm_store_sd(&(v0[-fi35 + 49]), _mm256_castpd256_pd128(_t57_11));
    _mm_store_sd(&(v0[-fi35 + 48]), _mm256_castpd256_pd128(_t57_12));

    for( int k3 = 0; k3 <= -fi35 + 47; k3+=4 ) {
      _t58_9 = _asm256_loadu_pd(v0 + k3);
      _t58_7 = _asm256_loadu_pd(M3 + -fi35 + 52*k3 + 48);
      _t58_6 = _asm256_loadu_pd(M3 + -fi35 + 52*k3 + 100);
      _t58_5 = _asm256_loadu_pd(M3 + -fi35 + 52*k3 + 152);
      _t58_4 = _asm256_loadu_pd(M3 + -fi35 + 52*k3 + 204);
      _t58_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 51])));
      _t58_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 50])));
      _t58_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 49])));
      _t58_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 48])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t58_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t58_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t58_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t58_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t58_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t58_9 = _mm256_sub_pd(_t58_9, _t58_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k3, _t58_9);
    }
  }

  _t59_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[3])));
  _t59_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[159])));
  _t59_8 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t59_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + 3)), _mm256_castpd128_pd256(_mm_load_sd(M3 + 55))), _mm256_castpd128_pd256(_mm_load_sd(M3 + 107)), 32);
  _t59_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[106])));
  _t59_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + 2)), _mm256_castpd128_pd256(_mm_load_sd(M3 + 54)), 0);
  _t59_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53])));
  _t59_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[1])));
  _t59_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));

  // Generating : v4[52,1] = S(h(1, 52, 3), ( G(h(1, 52, 3), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, 3), U0[52,52],h(1, 52, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_13 = _t59_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_14 = _t59_6;

  // 4-BLAC: 1x4 / 1x4
  _t59_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_13), _mm256_castpd256_pd128(_t59_14)));

  // AVX Storer:
  _t59_7 = _t59_15;

  // Generating : v4[52,1] = S(h(3, 52, 0), ( G(h(3, 52, 0), v4[52,1],h(1, 1, 0)) - ( G(h(3, 52, 0), U0[52,52],h(1, 52, 3)) Kro G(h(1, 52, 3), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t59_16 = _t59_8;

  // AVX Loader:

  // 3x1 -> 4x1
  _t59_17 = _t59_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t59_7, _t59_7, 32), _mm256_permute2f128_pd(_t59_7, _t59_7, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t59_19 = _mm256_mul_pd(_t59_17, _t59_18);

  // 4-BLAC: 4x1 - 4x1
  _t59_20 = _mm256_sub_pd(_t59_16, _t59_19);

  // AVX Storer:
  _t59_8 = _t59_20;

  // Generating : v4[52,1] = S(h(1, 52, 2), ( G(h(1, 52, 2), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, 2), U0[52,52],h(1, 52, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t59_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t59_8, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_22 = _t59_4;

  // 4-BLAC: 1x4 / 1x4
  _t59_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_21), _mm256_castpd256_pd128(_t59_22)));

  // AVX Storer:
  _t59_9 = _t59_23;

  // Generating : v4[52,1] = S(h(2, 52, 0), ( G(h(2, 52, 0), v4[52,1],h(1, 1, 0)) - ( G(h(2, 52, 0), U0[52,52],h(1, 52, 2)) Kro G(h(1, 52, 2), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t59_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t59_8, 3);

  // AVX Loader:

  // 2x1 -> 4x1
  _t59_25 = _t59_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t59_9, _t59_9, 32), _mm256_permute2f128_pd(_t59_9, _t59_9, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t59_27 = _mm256_mul_pd(_t59_25, _t59_26);

  // 4-BLAC: 4x1 - 4x1
  _t59_28 = _mm256_sub_pd(_t59_24, _t59_27);

  // AVX Storer:
  _t59_10 = _t59_28;

  // Generating : v4[52,1] = S(h(1, 52, 1), ( G(h(1, 52, 1), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, 1), U0[52,52],h(1, 52, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t59_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_30 = _t59_2;

  // 4-BLAC: 1x4 / 1x4
  _t59_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_29), _mm256_castpd256_pd128(_t59_30)));

  // AVX Storer:
  _t59_11 = _t59_31;

  // Generating : v4[52,1] = S(h(1, 52, 0), ( G(h(1, 52, 0), v4[52,1],h(1, 1, 0)) - ( G(h(1, 52, 0), U0[52,52],h(1, 52, 1)) Kro G(h(1, 52, 1), v4[52,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_32 = _mm256_blend_pd(_mm256_setzero_pd(), _t59_10, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_33 = _t59_1;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_34 = _t59_11;

  // 4-BLAC: 1x4 Kro 1x4
  _t59_35 = _mm256_mul_pd(_t59_33, _t59_34);

  // 4-BLAC: 1x4 - 1x4
  _t59_36 = _mm256_sub_pd(_t59_32, _t59_35);

  // AVX Storer:
  _t59_12 = _t59_36;

  // Generating : v4[52,1] = S(h(1, 52, 0), ( G(h(1, 52, 0), v4[52,1],h(1, 1, 0)) Div G(h(1, 52, 0), U0[52,52],h(1, 52, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_37 = _t59_12;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_38 = _t59_0;

  // 4-BLAC: 1x4 / 1x4
  _t59_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_37), _mm256_castpd256_pd128(_t59_38)));

  // AVX Storer:
  _t59_12 = _t59_39;


  for( int fi35 = 0; fi35 <= 47; fi35+=4 ) {
    _t60_1 = _mm256_maskload_pd(M3 + 53*fi35 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t60_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35])));
    _t60_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 159])));
    _t60_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 106])));
    _t60_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 53])));
    _t60_2 = _mm256_maskload_pd(M3 + 53*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t60_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35])));
    _t60_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 53])));
    _t60_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 106])));
    _t60_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[53*fi35 + 159])));
    _t60_14 = _asm256_loadu_pd(M1 + 52*fi35);
    _t60_11 = _asm256_loadu_pd(M1 + 52*fi35 + 52);
    _t60_12 = _asm256_loadu_pd(M1 + 52*fi35 + 104);
    _t60_13 = _asm256_loadu_pd(M1 + 52*fi35 + 156);
    _t60_2 = _mm256_maskload_pd(M3 + 53*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t60_1 = _mm256_maskload_pd(M3 + 53*fi35 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t60_0 = _mm256_broadcast_sd(&(M3[53*fi35 + 107]));

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35), U0[52,52],h(1, 52, fi35)) ),h(1, 52, fi35))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_17 = _t60_6;

    // 4-BLAC: 1x4 / 1x4
    _t60_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_16), _mm256_castpd256_pd128(_t60_17)));

    // AVX Storer:
    _t60_7 = _t60_18;

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 1), U0[52,52],h(1, 52, fi35 + 1)) ),h(1, 52, fi35 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_20 = _t60_5;

    // 4-BLAC: 1x4 / 1x4
    _t60_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_19), _mm256_castpd256_pd128(_t60_20)));

    // AVX Storer:
    _t60_8 = _t60_21;

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 2), U0[52,52],h(1, 52, fi35 + 2)) ),h(1, 52, fi35 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_23 = _t60_4;

    // 4-BLAC: 1x4 / 1x4
    _t60_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_22), _mm256_castpd256_pd128(_t60_23)));

    // AVX Storer:
    _t60_9 = _t60_24;

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, fi35 + 3), U0[52,52],h(1, 52, fi35 + 3)) ),h(1, 52, fi35 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_26 = _t60_3;

    // 4-BLAC: 1x4 / 1x4
    _t60_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_25), _mm256_castpd256_pd128(_t60_26)));

    // AVX Storer:
    _t60_10 = _t60_27;

    // Generating : M6[52,52] = S(h(1, 52, fi35), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35)) Kro G(h(1, 52, fi35), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_7, _t60_7, 32), _mm256_permute2f128_pd(_t60_7, _t60_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_14 = _mm256_mul_pd(_t60_28, _t60_14);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(3, 52, fi35 + 1), ( G(h(3, 52, fi35 + 1), M6[52,52],h(4, 52, fi96)) - ( T( G(h(1, 52, fi35), U0[52,52],h(3, 52, fi35 + 1)) ) * G(h(1, 52, fi35), M6[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

    // AVX Loader:

    // 3x4 -> 4x4
    _t60_29 = _t60_11;
    _t60_30 = _t60_12;
    _t60_31 = _t60_13;
    _t60_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t60_33 = _t60_2;

    // 4-BLAC: (1x4)^T
    _t60_34 = _t60_33;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 0), _t60_14);
    _t60_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 15), _t60_14);
    _t60_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 0), _t60_14);
    _t60_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 15), _t60_14);

    // 4-BLAC: 4x4 - 4x4
    _t60_39 = _mm256_sub_pd(_t60_29, _t60_35);
    _t60_40 = _mm256_sub_pd(_t60_30, _t60_36);
    _t60_41 = _mm256_sub_pd(_t60_31, _t60_37);
    _t60_42 = _mm256_sub_pd(_t60_32, _t60_38);

    // AVX Storer:
    _t60_11 = _t60_39;
    _t60_12 = _t60_40;
    _t60_13 = _t60_41;

    // Generating : M6[52,52] = S(h(1, 52, fi35 + 1), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35 + 1)) Kro G(h(1, 52, fi35 + 1), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_8, _t60_8, 32), _mm256_permute2f128_pd(_t60_8, _t60_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_11 = _mm256_mul_pd(_t60_43, _t60_11);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(2, 52, fi35 + 2), ( G(h(2, 52, fi35 + 2), M6[52,52],h(4, 52, fi96)) - ( T( G(h(1, 52, fi35 + 1), U0[52,52],h(2, 52, fi35 + 2)) ) * G(h(1, 52, fi35 + 1), M6[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

    // AVX Loader:

    // 2x4 -> 4x4
    _t60_44 = _t60_12;
    _t60_45 = _t60_13;
    _t60_46 = _mm256_setzero_pd();
    _t60_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t60_48 = _t60_1;

    // 4-BLAC: (1x4)^T
    _t60_49 = _t60_48;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 0), _t60_11);
    _t60_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 15), _t60_11);
    _t60_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 0), _t60_11);
    _t60_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 15), _t60_11);

    // 4-BLAC: 4x4 - 4x4
    _t60_54 = _mm256_sub_pd(_t60_44, _t60_50);
    _t60_55 = _mm256_sub_pd(_t60_45, _t60_51);
    _t60_56 = _mm256_sub_pd(_t60_46, _t60_52);
    _t60_57 = _mm256_sub_pd(_t60_47, _t60_53);

    // AVX Storer:
    _t60_12 = _t60_54;
    _t60_13 = _t60_55;

    // Generating : M6[52,52] = S(h(1, 52, fi35 + 2), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35 + 2)) Kro G(h(1, 52, fi35 + 2), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_9, _t60_9, 32), _mm256_permute2f128_pd(_t60_9, _t60_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_12 = _mm256_mul_pd(_t60_58, _t60_12);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 52, fi35 + 3), M6[52,52],h(4, 52, fi96)) - ( T( G(h(1, 52, fi35 + 2), U0[52,52],h(1, 52, fi35 + 3)) ) Kro G(h(1, 52, fi35 + 2), M6[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_59 = _t60_0;

    // 4-BLAC: (4x1)^T
    _t60_60 = _t60_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_15 = _mm256_mul_pd(_t60_60, _t60_12);

    // 4-BLAC: 1x4 - 1x4
    _t60_13 = _mm256_sub_pd(_t60_13, _t60_15);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35 + 3)) Kro G(h(1, 52, fi35 + 3), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_10, _t60_10, 32), _mm256_permute2f128_pd(_t60_10, _t60_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_13 = _mm256_mul_pd(_t60_61, _t60_13);

    // AVX Storer:

    for( int fi96 = 4; fi96 <= 48; fi96+=4 ) {
      _t61_3 = _asm256_loadu_pd(M1 + 52*fi35 + fi96);
      _t61_0 = _asm256_loadu_pd(M1 + 52*fi35 + fi96 + 52);
      _t61_1 = _asm256_loadu_pd(M1 + 52*fi35 + fi96 + 104);
      _t61_2 = _asm256_loadu_pd(M1 + 52*fi35 + fi96 + 156);

      // Generating : M6[52,52] = S(h(1, 52, fi35), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35)) Kro G(h(1, 52, fi35), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_7, _t60_7, 32), _mm256_permute2f128_pd(_t60_7, _t60_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_3 = _mm256_mul_pd(_t61_4, _t61_3);

      // AVX Storer:

      // Generating : M6[52,52] = S(h(3, 52, fi35 + 1), ( G(h(3, 52, fi35 + 1), M6[52,52],h(4, 52, fi96)) - ( T( G(h(1, 52, fi35), U0[52,52],h(3, 52, fi35 + 1)) ) * G(h(1, 52, fi35), M6[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

      // AVX Loader:

      // 3x4 -> 4x4
      _t61_5 = _t61_0;
      _t61_6 = _t61_1;
      _t61_7 = _t61_2;
      _t61_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t61_9 = _t60_2;

      // 4-BLAC: (1x4)^T
      _t60_34 = _t61_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t60_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 0), _t61_3);
      _t60_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 15), _t61_3);
      _t60_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 0), _t61_3);
      _t60_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 15), _t61_3);

      // 4-BLAC: 4x4 - 4x4
      _t61_10 = _mm256_sub_pd(_t61_5, _t60_35);
      _t61_11 = _mm256_sub_pd(_t61_6, _t60_36);
      _t61_12 = _mm256_sub_pd(_t61_7, _t60_37);
      _t61_13 = _mm256_sub_pd(_t61_8, _t60_38);

      // AVX Storer:
      _t61_0 = _t61_10;
      _t61_1 = _t61_11;
      _t61_2 = _t61_12;

      // Generating : M6[52,52] = S(h(1, 52, fi35 + 1), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35 + 1)) Kro G(h(1, 52, fi35 + 1), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_8, _t60_8, 32), _mm256_permute2f128_pd(_t60_8, _t60_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_0 = _mm256_mul_pd(_t61_14, _t61_0);

      // AVX Storer:

      // Generating : M6[52,52] = S(h(2, 52, fi35 + 2), ( G(h(2, 52, fi35 + 2), M6[52,52],h(4, 52, fi96)) - ( T( G(h(1, 52, fi35 + 1), U0[52,52],h(2, 52, fi35 + 2)) ) * G(h(1, 52, fi35 + 1), M6[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

      // AVX Loader:

      // 2x4 -> 4x4
      _t61_15 = _t61_1;
      _t61_16 = _t61_2;
      _t61_17 = _mm256_setzero_pd();
      _t61_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t61_19 = _t60_1;

      // 4-BLAC: (1x4)^T
      _t60_49 = _t61_19;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t60_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 0), _t61_0);
      _t60_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 15), _t61_0);
      _t60_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 0), _t61_0);
      _t60_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 15), _t61_0);

      // 4-BLAC: 4x4 - 4x4
      _t61_20 = _mm256_sub_pd(_t61_15, _t60_50);
      _t61_21 = _mm256_sub_pd(_t61_16, _t60_51);
      _t61_22 = _mm256_sub_pd(_t61_17, _t60_52);
      _t61_23 = _mm256_sub_pd(_t61_18, _t60_53);

      // AVX Storer:
      _t61_1 = _t61_20;
      _t61_2 = _t61_21;

      // Generating : M6[52,52] = S(h(1, 52, fi35 + 2), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35 + 2)) Kro G(h(1, 52, fi35 + 2), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_9, _t60_9, 32), _mm256_permute2f128_pd(_t60_9, _t60_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_1 = _mm256_mul_pd(_t61_24, _t61_1);

      // AVX Storer:

      // Generating : M6[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 52, fi35 + 3), M6[52,52],h(4, 52, fi96)) - ( T( G(h(1, 52, fi35 + 2), U0[52,52],h(1, 52, fi35 + 3)) ) Kro G(h(1, 52, fi35 + 2), M6[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_25 = _t60_0;

      // 4-BLAC: (4x1)^T
      _t60_60 = _t61_25;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t60_15 = _mm256_mul_pd(_t60_60, _t61_1);

      // 4-BLAC: 1x4 - 1x4
      _t61_2 = _mm256_sub_pd(_t61_2, _t60_15);

      // AVX Storer:

      // Generating : M6[52,52] = S(h(1, 52, fi35 + 3), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, fi35 + 3)) Kro G(h(1, 52, fi35 + 3), M6[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_10, _t60_10, 32), _mm256_permute2f128_pd(_t60_10, _t60_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_2 = _mm256_mul_pd(_t61_26, _t61_2);

      // AVX Storer:
      _asm256_storeu_pd(M1 + 52*fi35 + fi96, _t61_3);
      _asm256_storeu_pd(M1 + 52*fi35 + fi96 + 52, _t61_0);
      _asm256_storeu_pd(M1 + 52*fi35 + fi96 + 104, _t61_1);
      _asm256_storeu_pd(M1 + 52*fi35 + fi96 + 156, _t61_2);
    }

    // Generating : M6[52,52] = Sum_{k3} ( Sum_{k2} ( S(h(4, 52, fi35 + k3 + 4), ( G(h(4, 52, fi35 + k3 + 4), M6[52,52],h(4, 52, k2)) - ( T( G(h(4, 52, fi35), U0[52,52],h(4, 52, fi35 + k3 + 4)) ) * G(h(4, 52, fi35), M6[52,52],h(4, 52, k2)) ) ),h(4, 52, k2)) ) )
    _asm256_storeu_pd(M1 + 52*fi35, _t60_14);
    _asm256_storeu_pd(M1 + 52*fi35 + 52, _t60_11);
    _asm256_storeu_pd(M1 + 52*fi35 + 104, _t60_12);
    _asm256_storeu_pd(M1 + 52*fi35 + 156, _t60_13);

    for( int k3 = 0; k3 <= -fi35 + 47; k3+=4 ) {

      for( int k2 = 0; k2 <= 51; k2+=4 ) {
        _t62_12 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 52*k3 + 208);
        _t62_13 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 52*k3 + 260);
        _t62_14 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 52*k3 + 312);
        _t62_15 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 52*k3 + 364);
        _t62_7 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 4);
        _t62_6 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 56);
        _t62_5 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 108);
        _t62_4 = _asm256_loadu_pd(M3 + 53*fi35 + k3 + 160);
        _t62_3 = _asm256_loadu_pd(M1 + 52*fi35 + k2);
        _t62_2 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 52);
        _t62_1 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 104);
        _t62_0 = _asm256_loadu_pd(M1 + 52*fi35 + k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t62_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t62_7, _t62_6), _mm256_unpacklo_pd(_t62_5, _t62_4), 32);
        _t62_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t62_7, _t62_6), _mm256_unpackhi_pd(_t62_5, _t62_4), 32);
        _t62_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t62_7, _t62_6), _mm256_unpacklo_pd(_t62_5, _t62_4), 49);
        _t62_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t62_7, _t62_6), _mm256_unpackhi_pd(_t62_5, _t62_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t62_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 32), _mm256_permute2f128_pd(_t62_16, _t62_16, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 32), _mm256_permute2f128_pd(_t62_16, _t62_16, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 49), _mm256_permute2f128_pd(_t62_16, _t62_16, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 49), _mm256_permute2f128_pd(_t62_16, _t62_16, 49), 15), _t62_0)));
        _t62_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 32), _mm256_permute2f128_pd(_t62_17, _t62_17, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 32), _mm256_permute2f128_pd(_t62_17, _t62_17, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 49), _mm256_permute2f128_pd(_t62_17, _t62_17, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 49), _mm256_permute2f128_pd(_t62_17, _t62_17, 49), 15), _t62_0)));
        _t62_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 32), _mm256_permute2f128_pd(_t62_18, _t62_18, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 32), _mm256_permute2f128_pd(_t62_18, _t62_18, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 49), _mm256_permute2f128_pd(_t62_18, _t62_18, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 49), _mm256_permute2f128_pd(_t62_18, _t62_18, 49), 15), _t62_0)));
        _t62_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 32), _mm256_permute2f128_pd(_t62_19, _t62_19, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 32), _mm256_permute2f128_pd(_t62_19, _t62_19, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 49), _mm256_permute2f128_pd(_t62_19, _t62_19, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 49), _mm256_permute2f128_pd(_t62_19, _t62_19, 49), 15), _t62_0)));

        // 4-BLAC: 4x4 - 4x4
        _t62_12 = _mm256_sub_pd(_t62_12, _t62_8);
        _t62_13 = _mm256_sub_pd(_t62_13, _t62_9);
        _t62_14 = _mm256_sub_pd(_t62_14, _t62_10);
        _t62_15 = _mm256_sub_pd(_t62_15, _t62_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + 52*fi35 + k2 + 52*k3 + 208, _t62_12);
        _asm256_storeu_pd(M1 + 52*fi35 + k2 + 52*k3 + 260, _t62_13);
        _asm256_storeu_pd(M1 + 52*fi35 + k2 + 52*k3 + 312, _t62_14);
        _asm256_storeu_pd(M1 + 52*fi35 + k2 + 52*k3 + 364, _t62_15);
      }
    }
  }

  _t53_32 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[2703])));
  _t53_30 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[2650])));
  _t53_21 = _mm256_maskload_pd(M3 + 2545, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t53_27 = _mm256_maskload_pd(M3 + 2598, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t53_19 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[2544])));
  _t53_31 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[2651])));
  _t53_25 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[2597])));
  _t63_7 = _asm256_loadu_pd(M1 + 2496);
  _t63_4 = _asm256_loadu_pd(M1 + 2548);
  _t63_5 = _asm256_loadu_pd(M1 + 2600);
  _t63_6 = _asm256_loadu_pd(M1 + 2652);

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 48), U0[52,52],h(1, 52, 48)) ),h(1, 52, 48))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_10 = _t53_19;

  // 4-BLAC: 1x4 / 1x4
  _t63_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_9), _mm256_castpd256_pd128(_t63_10)));

  // AVX Storer:
  _t63_0 = _t63_11;

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 49), U0[52,52],h(1, 52, 49)) ),h(1, 52, 49))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_13 = _t53_25;

  // 4-BLAC: 1x4 / 1x4
  _t63_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_12), _mm256_castpd256_pd128(_t63_13)));

  // AVX Storer:
  _t63_1 = _t63_14;

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 50), U0[52,52],h(1, 52, 50)) ),h(1, 52, 50))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_16 = _t53_30;

  // 4-BLAC: 1x4 / 1x4
  _t63_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_15), _mm256_castpd256_pd128(_t63_16)));

  // AVX Storer:
  _t63_2 = _t63_17;

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 51), U0[52,52],h(1, 52, 51)) ),h(1, 52, 51))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_19 = _t53_32;

  // 4-BLAC: 1x4 / 1x4
  _t63_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_18), _mm256_castpd256_pd128(_t63_19)));

  // AVX Storer:
  _t63_3 = _t63_20;

  // Generating : M6[52,52] = S(h(1, 52, 48), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 48)) Kro G(h(1, 52, 48), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_0, _t63_0, 32), _mm256_permute2f128_pd(_t63_0, _t63_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_7 = _mm256_mul_pd(_t63_21, _t63_7);

  // AVX Storer:

  // Generating : M6[52,52] = S(h(3, 52, 49), ( G(h(3, 52, 49), M6[52,52],h(4, 52, fi35)) - ( T( G(h(1, 52, 48), U0[52,52],h(3, 52, 49)) ) * G(h(1, 52, 48), M6[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

  // AVX Loader:

  // 3x4 -> 4x4
  _t63_22 = _t63_4;
  _t63_23 = _t63_5;
  _t63_24 = _t63_6;
  _t63_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t63_26 = _t53_21;

  // 4-BLAC: (1x4)^T
  _t63_27 = _t63_26;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t63_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 0), _t63_7);
  _t63_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 15), _t63_7);
  _t63_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 0), _t63_7);
  _t63_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 15), _t63_7);

  // 4-BLAC: 4x4 - 4x4
  _t63_32 = _mm256_sub_pd(_t63_22, _t63_28);
  _t63_33 = _mm256_sub_pd(_t63_23, _t63_29);
  _t63_34 = _mm256_sub_pd(_t63_24, _t63_30);
  _t63_35 = _mm256_sub_pd(_t63_25, _t63_31);

  // AVX Storer:
  _t63_4 = _t63_32;
  _t63_5 = _t63_33;
  _t63_6 = _t63_34;

  // Generating : M6[52,52] = S(h(1, 52, 49), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 49)) Kro G(h(1, 52, 49), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_1, _t63_1, 32), _mm256_permute2f128_pd(_t63_1, _t63_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_4 = _mm256_mul_pd(_t63_36, _t63_4);

  // AVX Storer:

  // Generating : M6[52,52] = S(h(2, 52, 50), ( G(h(2, 52, 50), M6[52,52],h(4, 52, fi35)) - ( T( G(h(1, 52, 49), U0[52,52],h(2, 52, 50)) ) * G(h(1, 52, 49), M6[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

  // AVX Loader:

  // 2x4 -> 4x4
  _t63_37 = _t63_5;
  _t63_38 = _t63_6;
  _t63_39 = _mm256_setzero_pd();
  _t63_40 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t63_41 = _t53_27;

  // 4-BLAC: (1x4)^T
  _t63_42 = _t63_41;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t63_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 0), _t63_4);
  _t63_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 15), _t63_4);
  _t63_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 0), _t63_4);
  _t63_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 15), _t63_4);

  // 4-BLAC: 4x4 - 4x4
  _t63_47 = _mm256_sub_pd(_t63_37, _t63_43);
  _t63_48 = _mm256_sub_pd(_t63_38, _t63_44);
  _t63_49 = _mm256_sub_pd(_t63_39, _t63_45);
  _t63_50 = _mm256_sub_pd(_t63_40, _t63_46);

  // AVX Storer:
  _t63_5 = _t63_47;
  _t63_6 = _t63_48;

  // Generating : M6[52,52] = S(h(1, 52, 50), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 50)) Kro G(h(1, 52, 50), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_2, _t63_2, 32), _mm256_permute2f128_pd(_t63_2, _t63_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_5 = _mm256_mul_pd(_t63_51, _t63_5);

  // AVX Storer:

  // Generating : M6[52,52] = S(h(1, 52, 51), ( G(h(1, 52, 51), M6[52,52],h(4, 52, fi35)) - ( T( G(h(1, 52, 50), U0[52,52],h(1, 52, 51)) ) Kro G(h(1, 52, 50), M6[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_31, _t53_31, 32), _mm256_permute2f128_pd(_t53_31, _t53_31, 32), 0);

  // 4-BLAC: (4x1)^T
  _t63_53 = _t63_52;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_8 = _mm256_mul_pd(_t63_53, _t63_5);

  // 4-BLAC: 1x4 - 1x4
  _t63_6 = _mm256_sub_pd(_t63_6, _t63_8);

  // AVX Storer:

  // Generating : M6[52,52] = S(h(1, 52, 51), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 51)) Kro G(h(1, 52, 51), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_3, _t63_3, 32), _mm256_permute2f128_pd(_t63_3, _t63_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_6 = _mm256_mul_pd(_t63_54, _t63_6);

  // AVX Storer:


  for( int fi35 = 4; fi35 <= 48; fi35+=4 ) {
    _t64_3 = _asm256_loadu_pd(M1 + fi35 + 2496);
    _t64_0 = _asm256_loadu_pd(M1 + fi35 + 2548);
    _t64_1 = _asm256_loadu_pd(M1 + fi35 + 2600);
    _t64_2 = _asm256_loadu_pd(M1 + fi35 + 2652);

    // Generating : M6[52,52] = S(h(1, 52, 48), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 48)) Kro G(h(1, 52, 48), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_0, _t63_0, 32), _mm256_permute2f128_pd(_t63_0, _t63_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_3 = _mm256_mul_pd(_t64_4, _t64_3);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(3, 52, 49), ( G(h(3, 52, 49), M6[52,52],h(4, 52, fi35)) - ( T( G(h(1, 52, 48), U0[52,52],h(3, 52, 49)) ) * G(h(1, 52, 48), M6[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

    // AVX Loader:

    // 3x4 -> 4x4
    _t64_5 = _t64_0;
    _t64_6 = _t64_1;
    _t64_7 = _t64_2;
    _t64_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t64_9 = _t53_21;

    // 4-BLAC: (1x4)^T
    _t63_27 = _t64_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t63_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 0), _t64_3);
    _t63_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 15), _t64_3);
    _t63_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 0), _t64_3);
    _t63_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 15), _t64_3);

    // 4-BLAC: 4x4 - 4x4
    _t64_10 = _mm256_sub_pd(_t64_5, _t63_28);
    _t64_11 = _mm256_sub_pd(_t64_6, _t63_29);
    _t64_12 = _mm256_sub_pd(_t64_7, _t63_30);
    _t64_13 = _mm256_sub_pd(_t64_8, _t63_31);

    // AVX Storer:
    _t64_0 = _t64_10;
    _t64_1 = _t64_11;
    _t64_2 = _t64_12;

    // Generating : M6[52,52] = S(h(1, 52, 49), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 49)) Kro G(h(1, 52, 49), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_1, _t63_1, 32), _mm256_permute2f128_pd(_t63_1, _t63_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_0 = _mm256_mul_pd(_t64_14, _t64_0);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(2, 52, 50), ( G(h(2, 52, 50), M6[52,52],h(4, 52, fi35)) - ( T( G(h(1, 52, 49), U0[52,52],h(2, 52, 50)) ) * G(h(1, 52, 49), M6[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

    // AVX Loader:

    // 2x4 -> 4x4
    _t64_15 = _t64_1;
    _t64_16 = _t64_2;
    _t64_17 = _mm256_setzero_pd();
    _t64_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t64_19 = _t53_27;

    // 4-BLAC: (1x4)^T
    _t63_42 = _t64_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t63_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 0), _t64_0);
    _t63_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 15), _t64_0);
    _t63_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 0), _t64_0);
    _t63_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 15), _t64_0);

    // 4-BLAC: 4x4 - 4x4
    _t64_20 = _mm256_sub_pd(_t64_15, _t63_43);
    _t64_21 = _mm256_sub_pd(_t64_16, _t63_44);
    _t64_22 = _mm256_sub_pd(_t64_17, _t63_45);
    _t64_23 = _mm256_sub_pd(_t64_18, _t63_46);

    // AVX Storer:
    _t64_1 = _t64_20;
    _t64_2 = _t64_21;

    // Generating : M6[52,52] = S(h(1, 52, 50), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 50)) Kro G(h(1, 52, 50), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_2, _t63_2, 32), _mm256_permute2f128_pd(_t63_2, _t63_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_1 = _mm256_mul_pd(_t64_24, _t64_1);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(1, 52, 51), ( G(h(1, 52, 51), M6[52,52],h(4, 52, fi35)) - ( T( G(h(1, 52, 50), U0[52,52],h(1, 52, 51)) ) Kro G(h(1, 52, 50), M6[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_31, _t53_31, 32), _mm256_permute2f128_pd(_t53_31, _t53_31, 32), 0);

    // 4-BLAC: (4x1)^T
    _t63_53 = _t64_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t63_8 = _mm256_mul_pd(_t63_53, _t64_1);

    // 4-BLAC: 1x4 - 1x4
    _t64_2 = _mm256_sub_pd(_t64_2, _t63_8);

    // AVX Storer:

    // Generating : M6[52,52] = S(h(1, 52, 51), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 51)) Kro G(h(1, 52, 51), M6[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_3, _t63_3, 32), _mm256_permute2f128_pd(_t63_3, _t63_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_2 = _mm256_mul_pd(_t64_26, _t64_2);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi35 + 2496, _t64_3);
    _asm256_storeu_pd(M1 + fi35 + 2548, _t64_0);
    _asm256_storeu_pd(M1 + fi35 + 2600, _t64_1);
    _asm256_storeu_pd(M1 + fi35 + 2652, _t64_2);
  }

  _asm256_storeu_pd(M1 + 2496, _t63_7);
  _asm256_storeu_pd(M1 + 2548, _t63_4);
  _asm256_storeu_pd(M1 + 2600, _t63_5);
  _asm256_storeu_pd(M1 + 2652, _t63_6);

  for( int fi35 = 0; fi35 <= 47; fi35+=4 ) {
    _t65_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2703])));
    _t65_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2650])));
    _t65_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2597])));
    _t65_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-53*fi35 + 2544])));
    _t65_14 = _asm256_loadu_pd(M1 + -52*fi35 + 2652);
    _t65_11 = _asm256_loadu_pd(M1 + -52*fi35 + 2496);
    _t65_12 = _asm256_loadu_pd(M1 + -52*fi35 + 2548);
    _t65_13 = _asm256_loadu_pd(M1 + -52*fi35 + 2600);
    _t65_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2547)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2599))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2651)), 32);
    _t65_1 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2546)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -53*fi35 + 2598)), 0);
    _t65_0 = _mm256_broadcast_sd(&(M3[-53*fi35 + 2545]));

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 51), U0[52,52],h(1, 52, -fi35 + 51)) ),h(1, 52, -fi35 + 51))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_17 = _t65_6;

    // 4-BLAC: 1x4 / 1x4
    _t65_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_16), _mm256_castpd256_pd128(_t65_17)));

    // AVX Storer:
    _t65_7 = _t65_18;

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 50), U0[52,52],h(1, 52, -fi35 + 50)) ),h(1, 52, -fi35 + 50))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_20 = _t65_5;

    // 4-BLAC: 1x4 / 1x4
    _t65_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_19), _mm256_castpd256_pd128(_t65_20)));

    // AVX Storer:
    _t65_8 = _t65_21;

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 49), U0[52,52],h(1, 52, -fi35 + 49)) ),h(1, 52, -fi35 + 49))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_23 = _t65_4;

    // 4-BLAC: 1x4 / 1x4
    _t65_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_22), _mm256_castpd256_pd128(_t65_23)));

    // AVX Storer:
    _t65_9 = _t65_24;

    // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 48)) ),h(1, 52, -fi35 + 48))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_26 = _t65_3;

    // 4-BLAC: 1x4 / 1x4
    _t65_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_25), _mm256_castpd256_pd128(_t65_26)));

    // AVX Storer:
    _t65_10 = _t65_27;

    // Generating : M8[52,52] = S(h(1, 52, -fi35 + 51), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 51)) Kro G(h(1, 52, -fi35 + 51), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_7, _t65_7, 32), _mm256_permute2f128_pd(_t65_7, _t65_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_14 = _mm256_mul_pd(_t65_28, _t65_14);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(3, 52, -fi35 + 48), ( G(h(3, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) - ( G(h(3, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 51)) * G(h(1, 52, -fi35 + 51), M8[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

    // AVX Loader:

    // 3x4 -> 4x4
    _t65_29 = _t65_11;
    _t65_30 = _t65_12;
    _t65_31 = _t65_13;
    _t65_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t65_33 = _t65_2;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 32), _mm256_permute2f128_pd(_t65_33, _t65_33, 32), 0), _t65_14);
    _t65_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 32), _mm256_permute2f128_pd(_t65_33, _t65_33, 32), 15), _t65_14);
    _t65_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 49), _mm256_permute2f128_pd(_t65_33, _t65_33, 49), 0), _t65_14);
    _t65_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 49), _mm256_permute2f128_pd(_t65_33, _t65_33, 49), 15), _t65_14);

    // 4-BLAC: 4x4 - 4x4
    _t65_38 = _mm256_sub_pd(_t65_29, _t65_34);
    _t65_39 = _mm256_sub_pd(_t65_30, _t65_35);
    _t65_40 = _mm256_sub_pd(_t65_31, _t65_36);
    _t65_41 = _mm256_sub_pd(_t65_32, _t65_37);

    // AVX Storer:
    _t65_11 = _t65_38;
    _t65_12 = _t65_39;
    _t65_13 = _t65_40;

    // Generating : M8[52,52] = S(h(1, 52, -fi35 + 50), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 50)) Kro G(h(1, 52, -fi35 + 50), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_8, _t65_8, 32), _mm256_permute2f128_pd(_t65_8, _t65_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_13 = _mm256_mul_pd(_t65_42, _t65_13);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(2, 52, -fi35 + 48), ( G(h(2, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) - ( G(h(2, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 50)) * G(h(1, 52, -fi35 + 50), M8[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

    // AVX Loader:

    // 2x4 -> 4x4
    _t65_43 = _t65_11;
    _t65_44 = _t65_12;
    _t65_45 = _mm256_setzero_pd();
    _t65_46 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t65_47 = _t65_1;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 32), _mm256_permute2f128_pd(_t65_47, _t65_47, 32), 0), _t65_13);
    _t65_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 32), _mm256_permute2f128_pd(_t65_47, _t65_47, 32), 15), _t65_13);
    _t65_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 49), _mm256_permute2f128_pd(_t65_47, _t65_47, 49), 0), _t65_13);
    _t65_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 49), _mm256_permute2f128_pd(_t65_47, _t65_47, 49), 15), _t65_13);

    // 4-BLAC: 4x4 - 4x4
    _t65_52 = _mm256_sub_pd(_t65_43, _t65_48);
    _t65_53 = _mm256_sub_pd(_t65_44, _t65_49);
    _t65_54 = _mm256_sub_pd(_t65_45, _t65_50);
    _t65_55 = _mm256_sub_pd(_t65_46, _t65_51);

    // AVX Storer:
    _t65_11 = _t65_52;
    _t65_12 = _t65_53;

    // Generating : M8[52,52] = S(h(1, 52, -fi35 + 49), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 49)) Kro G(h(1, 52, -fi35 + 49), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_56 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_9, _t65_9, 32), _mm256_permute2f128_pd(_t65_9, _t65_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_12 = _mm256_mul_pd(_t65_56, _t65_12);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(1, 52, -fi35 + 48), ( G(h(1, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) - ( G(h(1, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 49)) Kro G(h(1, 52, -fi35 + 49), M8[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_57 = _t65_0;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_15 = _mm256_mul_pd(_t65_57, _t65_12);

    // 4-BLAC: 1x4 - 1x4
    _t65_11 = _mm256_sub_pd(_t65_11, _t65_15);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(1, 52, -fi35 + 48), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 48)) Kro G(h(1, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_10, _t65_10, 32), _mm256_permute2f128_pd(_t65_10, _t65_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_11 = _mm256_mul_pd(_t65_58, _t65_11);

    // AVX Storer:

    for( int fi96 = 4; fi96 <= 48; fi96+=4 ) {
      _t66_3 = _asm256_loadu_pd(M1 + -52*fi35 + fi96 + 2652);
      _t66_0 = _asm256_loadu_pd(M1 + -52*fi35 + fi96 + 2496);
      _t66_1 = _asm256_loadu_pd(M1 + -52*fi35 + fi96 + 2548);
      _t66_2 = _asm256_loadu_pd(M1 + -52*fi35 + fi96 + 2600);

      // Generating : M8[52,52] = S(h(1, 52, -fi35 + 51), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 51)) Kro G(h(1, 52, -fi35 + 51), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_7, _t65_7, 32), _mm256_permute2f128_pd(_t65_7, _t65_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_3 = _mm256_mul_pd(_t66_4, _t66_3);

      // AVX Storer:

      // Generating : M8[52,52] = S(h(3, 52, -fi35 + 48), ( G(h(3, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) - ( G(h(3, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 51)) * G(h(1, 52, -fi35 + 51), M8[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

      // AVX Loader:

      // 3x4 -> 4x4
      _t66_5 = _t66_0;
      _t66_6 = _t66_1;
      _t66_7 = _t66_2;
      _t66_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t66_9 = _t65_2;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t65_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 32), _mm256_permute2f128_pd(_t66_9, _t66_9, 32), 0), _t66_3);
      _t65_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 32), _mm256_permute2f128_pd(_t66_9, _t66_9, 32), 15), _t66_3);
      _t65_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 49), _mm256_permute2f128_pd(_t66_9, _t66_9, 49), 0), _t66_3);
      _t65_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 49), _mm256_permute2f128_pd(_t66_9, _t66_9, 49), 15), _t66_3);

      // 4-BLAC: 4x4 - 4x4
      _t66_10 = _mm256_sub_pd(_t66_5, _t65_34);
      _t66_11 = _mm256_sub_pd(_t66_6, _t65_35);
      _t66_12 = _mm256_sub_pd(_t66_7, _t65_36);
      _t66_13 = _mm256_sub_pd(_t66_8, _t65_37);

      // AVX Storer:
      _t66_0 = _t66_10;
      _t66_1 = _t66_11;
      _t66_2 = _t66_12;

      // Generating : M8[52,52] = S(h(1, 52, -fi35 + 50), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 50)) Kro G(h(1, 52, -fi35 + 50), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_8, _t65_8, 32), _mm256_permute2f128_pd(_t65_8, _t65_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_2 = _mm256_mul_pd(_t66_14, _t66_2);

      // AVX Storer:

      // Generating : M8[52,52] = S(h(2, 52, -fi35 + 48), ( G(h(2, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) - ( G(h(2, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 50)) * G(h(1, 52, -fi35 + 50), M8[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

      // AVX Loader:

      // 2x4 -> 4x4
      _t66_15 = _t66_0;
      _t66_16 = _t66_1;
      _t66_17 = _mm256_setzero_pd();
      _t66_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t66_19 = _t65_1;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t65_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 32), _mm256_permute2f128_pd(_t66_19, _t66_19, 32), 0), _t66_2);
      _t65_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 32), _mm256_permute2f128_pd(_t66_19, _t66_19, 32), 15), _t66_2);
      _t65_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 49), _mm256_permute2f128_pd(_t66_19, _t66_19, 49), 0), _t66_2);
      _t65_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 49), _mm256_permute2f128_pd(_t66_19, _t66_19, 49), 15), _t66_2);

      // 4-BLAC: 4x4 - 4x4
      _t66_20 = _mm256_sub_pd(_t66_15, _t65_48);
      _t66_21 = _mm256_sub_pd(_t66_16, _t65_49);
      _t66_22 = _mm256_sub_pd(_t66_17, _t65_50);
      _t66_23 = _mm256_sub_pd(_t66_18, _t65_51);

      // AVX Storer:
      _t66_0 = _t66_20;
      _t66_1 = _t66_21;

      // Generating : M8[52,52] = S(h(1, 52, -fi35 + 49), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 49)) Kro G(h(1, 52, -fi35 + 49), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_9, _t65_9, 32), _mm256_permute2f128_pd(_t65_9, _t65_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_1 = _mm256_mul_pd(_t66_24, _t66_1);

      // AVX Storer:

      // Generating : M8[52,52] = S(h(1, 52, -fi35 + 48), ( G(h(1, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) - ( G(h(1, 52, -fi35 + 48), U0[52,52],h(1, 52, -fi35 + 49)) Kro G(h(1, 52, -fi35 + 49), M8[52,52],h(4, 52, fi96)) ) ),h(4, 52, fi96))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_25 = _t65_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t65_15 = _mm256_mul_pd(_t66_25, _t66_1);

      // 4-BLAC: 1x4 - 1x4
      _t66_0 = _mm256_sub_pd(_t66_0, _t65_15);

      // AVX Storer:

      // Generating : M8[52,52] = S(h(1, 52, -fi35 + 48), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, -fi35 + 48)) Kro G(h(1, 52, -fi35 + 48), M8[52,52],h(4, 52, fi96)) ),h(4, 52, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_10, _t65_10, 32), _mm256_permute2f128_pd(_t65_10, _t65_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_0 = _mm256_mul_pd(_t66_26, _t66_0);

      // AVX Storer:
      _asm256_storeu_pd(M1 + -52*fi35 + fi96 + 2652, _t66_3);
      _asm256_storeu_pd(M1 + -52*fi35 + fi96 + 2600, _t66_2);
      _asm256_storeu_pd(M1 + -52*fi35 + fi96 + 2548, _t66_1);
      _asm256_storeu_pd(M1 + -52*fi35 + fi96 + 2496, _t66_0);
    }

    // Generating : M8[52,52] = Sum_{k3} ( Sum_{k2} ( S(h(4, 52, k3), ( G(h(4, 52, k3), M8[52,52],h(4, 52, k2)) - ( G(h(4, 52, k3), U0[52,52],h(4, 52, -fi35 + 48)) * G(h(4, 52, -fi35 + 48), M8[52,52],h(4, 52, k2)) ) ),h(4, 52, k2)) ) )
    _asm256_storeu_pd(M1 + -52*fi35 + 2652, _t65_14);
    _asm256_storeu_pd(M1 + -52*fi35 + 2600, _t65_13);
    _asm256_storeu_pd(M1 + -52*fi35 + 2548, _t65_12);
    _asm256_storeu_pd(M1 + -52*fi35 + 2496, _t65_11);

    for( int k3 = 0; k3 <= -fi35 + 47; k3+=4 ) {

      for( int k2 = 0; k2 <= 51; k2+=4 ) {
        _t67_24 = _asm256_loadu_pd(M1 + k2 + 52*k3);
        _t67_25 = _asm256_loadu_pd(M1 + k2 + 52*k3 + 52);
        _t67_26 = _asm256_loadu_pd(M1 + k2 + 52*k3 + 104);
        _t67_27 = _asm256_loadu_pd(M1 + k2 + 52*k3 + 156);
        _t67_19 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 48);
        _t67_18 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 49);
        _t67_17 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 50);
        _t67_16 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 51);
        _t67_15 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 100);
        _t67_14 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 101);
        _t67_13 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 102);
        _t67_12 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 103);
        _t67_11 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 152);
        _t67_10 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 153);
        _t67_9 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 154);
        _t67_8 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 155);
        _t67_7 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 204);
        _t67_6 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 205);
        _t67_5 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 206);
        _t67_4 = _mm256_broadcast_sd(M3 + -fi35 + 52*k3 + 207);
        _t67_3 = _asm256_loadu_pd(M1 + -52*fi35 + k2 + 2496);
        _t67_2 = _asm256_loadu_pd(M1 + -52*fi35 + k2 + 2548);
        _t67_1 = _asm256_loadu_pd(M1 + -52*fi35 + k2 + 2600);
        _t67_0 = _asm256_loadu_pd(M1 + -52*fi35 + k2 + 2652);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t67_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_19, _t67_3), _mm256_mul_pd(_t67_18, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_17, _t67_1), _mm256_mul_pd(_t67_16, _t67_0)));
        _t67_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_15, _t67_3), _mm256_mul_pd(_t67_14, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_13, _t67_1), _mm256_mul_pd(_t67_12, _t67_0)));
        _t67_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_11, _t67_3), _mm256_mul_pd(_t67_10, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_9, _t67_1), _mm256_mul_pd(_t67_8, _t67_0)));
        _t67_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_7, _t67_3), _mm256_mul_pd(_t67_6, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_5, _t67_1), _mm256_mul_pd(_t67_4, _t67_0)));

        // 4-BLAC: 4x4 - 4x4
        _t67_24 = _mm256_sub_pd(_t67_24, _t67_20);
        _t67_25 = _mm256_sub_pd(_t67_25, _t67_21);
        _t67_26 = _mm256_sub_pd(_t67_26, _t67_22);
        _t67_27 = _mm256_sub_pd(_t67_27, _t67_23);

        // AVX Storer:
        _asm256_storeu_pd(M1 + k2 + 52*k3, _t67_24);
        _asm256_storeu_pd(M1 + k2 + 52*k3 + 52, _t67_25);
        _asm256_storeu_pd(M1 + k2 + 52*k3 + 104, _t67_26);
        _asm256_storeu_pd(M1 + k2 + 52*k3 + 156, _t67_27);
      }
    }
  }

  _t68_8 = _asm256_loadu_pd(M1 + 156);
  _t68_5 = _asm256_loadu_pd(M1);
  _t68_6 = _asm256_loadu_pd(M1 + 52);
  _t68_7 = _asm256_loadu_pd(M1 + 104);
  _t68_0 = _mm256_broadcast_sd(&(M3[1]));

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 3), U0[52,52],h(1, 52, 3)) ),h(1, 52, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_11 = _t59_6;

  // 4-BLAC: 1x4 / 1x4
  _t68_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_10), _mm256_castpd256_pd128(_t68_11)));

  // AVX Storer:
  _t68_1 = _t68_12;

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 2), U0[52,52],h(1, 52, 2)) ),h(1, 52, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_13 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_14 = _t59_4;

  // 4-BLAC: 1x4 / 1x4
  _t68_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_13), _mm256_castpd256_pd128(_t68_14)));

  // AVX Storer:
  _t68_2 = _t68_15;

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 1), U0[52,52],h(1, 52, 1)) ),h(1, 52, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_16 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_17 = _t59_2;

  // 4-BLAC: 1x4 / 1x4
  _t68_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_16), _mm256_castpd256_pd128(_t68_17)));

  // AVX Storer:
  _t68_3 = _t68_18;

  // Generating : T2019[1,52] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 52, 0), U0[52,52],h(1, 52, 0)) ),h(1, 52, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_19 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_20 = _t59_0;

  // 4-BLAC: 1x4 / 1x4
  _t68_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_19), _mm256_castpd256_pd128(_t68_20)));

  // AVX Storer:
  _t68_4 = _t68_21;

  // Generating : M8[52,52] = S(h(1, 52, 3), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 3)) Kro G(h(1, 52, 3), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_22 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_1, _t68_1, 32), _mm256_permute2f128_pd(_t68_1, _t68_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_8 = _mm256_mul_pd(_t68_22, _t68_8);

  // AVX Storer:

  // Generating : M8[52,52] = S(h(3, 52, 0), ( G(h(3, 52, 0), M8[52,52],h(4, 52, fi35)) - ( G(h(3, 52, 0), U0[52,52],h(1, 52, 3)) * G(h(1, 52, 3), M8[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

  // AVX Loader:

  // 3x4 -> 4x4
  _t68_23 = _t68_5;
  _t68_24 = _t68_6;
  _t68_25 = _t68_7;
  _t68_26 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t68_27 = _t59_5;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t68_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_27, _t68_27, 32), _mm256_permute2f128_pd(_t68_27, _t68_27, 32), 0), _t68_8);
  _t68_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_27, _t68_27, 32), _mm256_permute2f128_pd(_t68_27, _t68_27, 32), 15), _t68_8);
  _t68_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_27, _t68_27, 49), _mm256_permute2f128_pd(_t68_27, _t68_27, 49), 0), _t68_8);
  _t68_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_27, _t68_27, 49), _mm256_permute2f128_pd(_t68_27, _t68_27, 49), 15), _t68_8);

  // 4-BLAC: 4x4 - 4x4
  _t68_32 = _mm256_sub_pd(_t68_23, _t68_28);
  _t68_33 = _mm256_sub_pd(_t68_24, _t68_29);
  _t68_34 = _mm256_sub_pd(_t68_25, _t68_30);
  _t68_35 = _mm256_sub_pd(_t68_26, _t68_31);

  // AVX Storer:
  _t68_5 = _t68_32;
  _t68_6 = _t68_33;
  _t68_7 = _t68_34;

  // Generating : M8[52,52] = S(h(1, 52, 2), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 2)) Kro G(h(1, 52, 2), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_2, _t68_2, 32), _mm256_permute2f128_pd(_t68_2, _t68_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_7 = _mm256_mul_pd(_t68_36, _t68_7);

  // AVX Storer:

  // Generating : M8[52,52] = S(h(2, 52, 0), ( G(h(2, 52, 0), M8[52,52],h(4, 52, fi35)) - ( G(h(2, 52, 0), U0[52,52],h(1, 52, 2)) * G(h(1, 52, 2), M8[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

  // AVX Loader:

  // 2x4 -> 4x4
  _t68_37 = _t68_5;
  _t68_38 = _t68_6;
  _t68_39 = _mm256_setzero_pd();
  _t68_40 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t68_41 = _t59_3;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t68_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_41, _t68_41, 32), _mm256_permute2f128_pd(_t68_41, _t68_41, 32), 0), _t68_7);
  _t68_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_41, _t68_41, 32), _mm256_permute2f128_pd(_t68_41, _t68_41, 32), 15), _t68_7);
  _t68_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_41, _t68_41, 49), _mm256_permute2f128_pd(_t68_41, _t68_41, 49), 0), _t68_7);
  _t68_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_41, _t68_41, 49), _mm256_permute2f128_pd(_t68_41, _t68_41, 49), 15), _t68_7);

  // 4-BLAC: 4x4 - 4x4
  _t68_46 = _mm256_sub_pd(_t68_37, _t68_42);
  _t68_47 = _mm256_sub_pd(_t68_38, _t68_43);
  _t68_48 = _mm256_sub_pd(_t68_39, _t68_44);
  _t68_49 = _mm256_sub_pd(_t68_40, _t68_45);

  // AVX Storer:
  _t68_5 = _t68_46;
  _t68_6 = _t68_47;

  // Generating : M8[52,52] = S(h(1, 52, 1), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 1)) Kro G(h(1, 52, 1), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_3, _t68_3, 32), _mm256_permute2f128_pd(_t68_3, _t68_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_6 = _mm256_mul_pd(_t68_50, _t68_6);

  // AVX Storer:

  // Generating : M8[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), M8[52,52],h(4, 52, fi35)) - ( G(h(1, 52, 0), U0[52,52],h(1, 52, 1)) Kro G(h(1, 52, 1), M8[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_51 = _t68_0;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_9 = _mm256_mul_pd(_t68_51, _t68_6);

  // 4-BLAC: 1x4 - 1x4
  _t68_5 = _mm256_sub_pd(_t68_5, _t68_9);

  // AVX Storer:

  // Generating : M8[52,52] = S(h(1, 52, 0), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 0)) Kro G(h(1, 52, 0), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_4, _t68_4, 32), _mm256_permute2f128_pd(_t68_4, _t68_4, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_5 = _mm256_mul_pd(_t68_52, _t68_5);

  // AVX Storer:


  for( int fi35 = 4; fi35 <= 48; fi35+=4 ) {
    _t69_3 = _asm256_loadu_pd(M1 + fi35 + 156);
    _t69_0 = _asm256_loadu_pd(M1 + fi35);
    _t69_1 = _asm256_loadu_pd(M1 + fi35 + 52);
    _t69_2 = _asm256_loadu_pd(M1 + fi35 + 104);

    // Generating : M8[52,52] = S(h(1, 52, 3), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 3)) Kro G(h(1, 52, 3), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_1, _t68_1, 32), _mm256_permute2f128_pd(_t68_1, _t68_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_3 = _mm256_mul_pd(_t69_4, _t69_3);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(3, 52, 0), ( G(h(3, 52, 0), M8[52,52],h(4, 52, fi35)) - ( G(h(3, 52, 0), U0[52,52],h(1, 52, 3)) * G(h(1, 52, 3), M8[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

    // AVX Loader:

    // 3x4 -> 4x4
    _t69_5 = _t69_0;
    _t69_6 = _t69_1;
    _t69_7 = _t69_2;
    _t69_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t69_9 = _t59_5;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t68_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 32), _mm256_permute2f128_pd(_t69_9, _t69_9, 32), 0), _t69_3);
    _t68_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 32), _mm256_permute2f128_pd(_t69_9, _t69_9, 32), 15), _t69_3);
    _t68_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 49), _mm256_permute2f128_pd(_t69_9, _t69_9, 49), 0), _t69_3);
    _t68_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 49), _mm256_permute2f128_pd(_t69_9, _t69_9, 49), 15), _t69_3);

    // 4-BLAC: 4x4 - 4x4
    _t69_10 = _mm256_sub_pd(_t69_5, _t68_28);
    _t69_11 = _mm256_sub_pd(_t69_6, _t68_29);
    _t69_12 = _mm256_sub_pd(_t69_7, _t68_30);
    _t69_13 = _mm256_sub_pd(_t69_8, _t68_31);

    // AVX Storer:
    _t69_0 = _t69_10;
    _t69_1 = _t69_11;
    _t69_2 = _t69_12;

    // Generating : M8[52,52] = S(h(1, 52, 2), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 2)) Kro G(h(1, 52, 2), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_2, _t68_2, 32), _mm256_permute2f128_pd(_t68_2, _t68_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_2 = _mm256_mul_pd(_t69_14, _t69_2);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(2, 52, 0), ( G(h(2, 52, 0), M8[52,52],h(4, 52, fi35)) - ( G(h(2, 52, 0), U0[52,52],h(1, 52, 2)) * G(h(1, 52, 2), M8[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

    // AVX Loader:

    // 2x4 -> 4x4
    _t69_15 = _t69_0;
    _t69_16 = _t69_1;
    _t69_17 = _mm256_setzero_pd();
    _t69_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t69_19 = _t59_3;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t68_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 32), _mm256_permute2f128_pd(_t69_19, _t69_19, 32), 0), _t69_2);
    _t68_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 32), _mm256_permute2f128_pd(_t69_19, _t69_19, 32), 15), _t69_2);
    _t68_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 49), _mm256_permute2f128_pd(_t69_19, _t69_19, 49), 0), _t69_2);
    _t68_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 49), _mm256_permute2f128_pd(_t69_19, _t69_19, 49), 15), _t69_2);

    // 4-BLAC: 4x4 - 4x4
    _t69_20 = _mm256_sub_pd(_t69_15, _t68_42);
    _t69_21 = _mm256_sub_pd(_t69_16, _t68_43);
    _t69_22 = _mm256_sub_pd(_t69_17, _t68_44);
    _t69_23 = _mm256_sub_pd(_t69_18, _t68_45);

    // AVX Storer:
    _t69_0 = _t69_20;
    _t69_1 = _t69_21;

    // Generating : M8[52,52] = S(h(1, 52, 1), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 1)) Kro G(h(1, 52, 1), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_3, _t68_3, 32), _mm256_permute2f128_pd(_t68_3, _t68_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_1 = _mm256_mul_pd(_t69_24, _t69_1);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(1, 52, 0), ( G(h(1, 52, 0), M8[52,52],h(4, 52, fi35)) - ( G(h(1, 52, 0), U0[52,52],h(1, 52, 1)) Kro G(h(1, 52, 1), M8[52,52],h(4, 52, fi35)) ) ),h(4, 52, fi35))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_25 = _t68_0;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t68_9 = _mm256_mul_pd(_t69_25, _t69_1);

    // 4-BLAC: 1x4 - 1x4
    _t69_0 = _mm256_sub_pd(_t69_0, _t68_9);

    // AVX Storer:

    // Generating : M8[52,52] = S(h(1, 52, 0), ( G(h(1, 1, 0), T2019[1,52],h(1, 52, 0)) Kro G(h(1, 52, 0), M8[52,52],h(4, 52, fi35)) ),h(4, 52, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_4, _t68_4, 32), _mm256_permute2f128_pd(_t68_4, _t68_4, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_0 = _mm256_mul_pd(_t69_26, _t69_0);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi35 + 156, _t69_3);
    _asm256_storeu_pd(M1 + fi35 + 104, _t69_2);
    _asm256_storeu_pd(M1 + fi35 + 52, _t69_1);
    _asm256_storeu_pd(M1 + fi35, _t69_0);
  }


  // Generating : x[52,1] = ( Sum_{k2} ( S(h(4, 52, k2), ( G(h(4, 52, k2), y[52,1],h(1, 1, 0)) + ( G(h(4, 52, k2), M2[52,52],h(4, 52, 0)) * G(h(4, 52, 0), v0[52,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( $(h(4, 52, k2), ( G(h(4, 52, k2), M2[52,52],h(4, 52, k3)) * G(h(4, 52, k3), v0[52,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t59_8);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t59_10);

  for( int k2 = 0; k2 <= 51; k2+=4 ) {
    _t70_4 = _asm256_loadu_pd(y + k2);
    _t70_3 = _asm256_loadu_pd(M2 + 52*k2);
    _t70_2 = _asm256_loadu_pd(M2 + 52*k2 + 52);
    _t70_1 = _asm256_loadu_pd(M2 + 52*k2 + 104);
    _t70_0 = _asm256_loadu_pd(M2 + 52*k2 + 156);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t70_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t70_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32)), _mm256_mul_pd(_t70_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t70_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32)), _mm256_mul_pd(_t70_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t70_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32)), _mm256_mul_pd(_t70_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t70_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32)), _mm256_mul_pd(_t70_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_12, _t59_11), _mm256_unpacklo_pd(_t59_9, _t59_7), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t70_5 = _mm256_add_pd(_t70_4, _t70_6);

    // AVX Storer:
    _asm256_storeu_pd(x + k2, _t70_5);
  }


  for( int k3 = 4; k3 <= 51; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 51; k2+=4 ) {
      _t71_4 = _asm256_loadu_pd(M2 + 52*k2 + k3);
      _t71_3 = _asm256_loadu_pd(M2 + 52*k2 + k3 + 52);
      _t71_2 = _asm256_loadu_pd(M2 + 52*k2 + k3 + 104);
      _t71_1 = _asm256_loadu_pd(M2 + 52*k2 + k3 + 156);
      _t71_0 = _asm256_loadu_pd(v0 + k3);
      _t71_5 = _asm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t71_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t71_4, _t71_0), _mm256_mul_pd(_t71_3, _t71_0)), _mm256_hadd_pd(_mm256_mul_pd(_t71_2, _t71_0), _mm256_mul_pd(_t71_1, _t71_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t71_4, _t71_0), _mm256_mul_pd(_t71_3, _t71_0)), _mm256_hadd_pd(_mm256_mul_pd(_t71_2, _t71_0), _mm256_mul_pd(_t71_1, _t71_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t71_5 = _mm256_add_pd(_t71_5, _t71_6);

      // AVX Storer:
      _asm256_storeu_pd(x + k2, _t71_5);
    }
  }


  // Generating : P[52,52] = ( ( Sum_{k2} ( ( S(h(4, 52, k2), ( G(h(4, 52, k2), Y[52,52],h(4, 52, k2)) - ( G(h(4, 52, k2), M2[52,52],h(4, 52, 0)) * G(h(4, 52, 0), M1[52,52],h(4, 52, k2)) ) ),h(4, 52, k2)) + Sum_{i0} ( S(h(4, 52, k2), ( G(h(4, 52, k2), Y[52,52],h(4, 52, i0)) - ( G(h(4, 52, k2), M2[52,52],h(4, 52, 0)) * G(h(4, 52, 0), M1[52,52],h(4, 52, i0)) ) ),h(4, 52, i0)) ) ) ) + S(h(4, 52, 48), ( G(h(4, 52, 48), Y[52,52],h(4, 52, 48)) - ( G(h(4, 52, 48), M2[52,52],h(4, 52, 0)) * G(h(4, 52, 0), M1[52,52],h(4, 52, 48)) ) ),h(4, 52, 48)) ) + Sum_{k3} ( ( Sum_{k2} ( ( -$(h(4, 52, k2), ( G(h(4, 52, k2), M2[52,52],h(4, 52, k3)) * G(h(4, 52, k3), M1[52,52],h(4, 52, k2)) ),h(4, 52, k2)) + Sum_{i0} ( -$(h(4, 52, k2), ( G(h(4, 52, k2), M2[52,52],h(4, 52, k3)) * G(h(4, 52, k3), M1[52,52],h(4, 52, i0)) ),h(4, 52, i0)) ) ) ) + -$(h(4, 52, 48), ( G(h(4, 52, 48), M2[52,52],h(4, 52, k3)) * G(h(4, 52, k3), M1[52,52],h(4, 52, 48)) ),h(4, 52, 48)) ) ) )

  _asm256_storeu_pd(M1 + 156, _t68_8);
  _asm256_storeu_pd(M1 + 104, _t68_7);
  _asm256_storeu_pd(M1 + 52, _t68_6);
  _asm256_storeu_pd(M1, _t68_5);

  for( int k2 = 0; k2 <= 47; k2+=4 ) {
    _t72_23 = _asm256_loadu_pd(Y + 53*k2);
    _t72_22 = _mm256_maskload_pd(Y + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t72_21 = _mm256_maskload_pd(Y + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t72_20 = _mm256_maskload_pd(Y + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t72_19 = _mm256_broadcast_sd(M2 + 52*k2);
    _t72_18 = _mm256_broadcast_sd(M2 + 52*k2 + 1);
    _t72_17 = _mm256_broadcast_sd(M2 + 52*k2 + 2);
    _t72_16 = _mm256_broadcast_sd(M2 + 52*k2 + 3);
    _t72_15 = _mm256_broadcast_sd(M2 + 52*k2 + 52);
    _t72_14 = _mm256_broadcast_sd(M2 + 52*k2 + 53);
    _t72_13 = _mm256_broadcast_sd(M2 + 52*k2 + 54);
    _t72_12 = _mm256_broadcast_sd(M2 + 52*k2 + 55);
    _t72_11 = _mm256_broadcast_sd(M2 + 52*k2 + 104);
    _t72_10 = _mm256_broadcast_sd(M2 + 52*k2 + 105);
    _t72_9 = _mm256_broadcast_sd(M2 + 52*k2 + 106);
    _t72_8 = _mm256_broadcast_sd(M2 + 52*k2 + 107);
    _t72_7 = _mm256_broadcast_sd(M2 + 52*k2 + 156);
    _t72_6 = _mm256_broadcast_sd(M2 + 52*k2 + 157);
    _t72_5 = _mm256_broadcast_sd(M2 + 52*k2 + 158);
    _t72_4 = _mm256_broadcast_sd(M2 + 52*k2 + 159);
    _t72_3 = _asm256_loadu_pd(M1 + k2);
    _t72_2 = _asm256_loadu_pd(M1 + k2 + 52);
    _t72_1 = _asm256_loadu_pd(M1 + k2 + 104);
    _t72_0 = _asm256_loadu_pd(M1 + k2 + 156);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t72_36 = _t72_23;
    _t72_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t72_23, _t72_22, 3), _t72_22, 12);
    _t72_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t72_23, _t72_22, 0), _t72_21, 49);
    _t72_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t72_23, _t72_22, 12), _mm256_shuffle_pd(_t72_21, _t72_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t72_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_19, _t72_3), _mm256_mul_pd(_t72_18, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_17, _t72_1), _mm256_mul_pd(_t72_16, _t72_0)));
    _t72_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_15, _t72_3), _mm256_mul_pd(_t72_14, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_13, _t72_1), _mm256_mul_pd(_t72_12, _t72_0)));
    _t72_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_11, _t72_3), _mm256_mul_pd(_t72_10, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_9, _t72_1), _mm256_mul_pd(_t72_8, _t72_0)));
    _t72_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_7, _t72_3), _mm256_mul_pd(_t72_6, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_5, _t72_1), _mm256_mul_pd(_t72_4, _t72_0)));

    // 4-BLAC: 4x4 - 4x4
    _t72_32 = _mm256_sub_pd(_t72_36, _t72_28);
    _t72_33 = _mm256_sub_pd(_t72_37, _t72_29);
    _t72_34 = _mm256_sub_pd(_t72_38, _t72_30);
    _t72_35 = _mm256_sub_pd(_t72_39, _t72_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t72_24 = _t72_32;
    _t72_25 = _t72_33;
    _t72_26 = _t72_34;
    _t72_27 = _t72_35;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
      _t73_7 = _asm256_loadu_pd(Y + i0 + 52*k2);
      _t73_6 = _asm256_loadu_pd(Y + i0 + 52*k2 + 52);
      _t73_5 = _asm256_loadu_pd(Y + i0 + 52*k2 + 104);
      _t73_4 = _asm256_loadu_pd(Y + i0 + 52*k2 + 156);
      _t73_3 = _asm256_loadu_pd(M1 + i0);
      _t73_2 = _asm256_loadu_pd(M1 + i0 + 52);
      _t73_1 = _asm256_loadu_pd(M1 + i0 + 104);
      _t73_0 = _asm256_loadu_pd(M1 + i0 + 156);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t73_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_19, _t73_3), _mm256_mul_pd(_t72_18, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_17, _t73_1), _mm256_mul_pd(_t72_16, _t73_0)));
      _t73_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_15, _t73_3), _mm256_mul_pd(_t72_14, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_13, _t73_1), _mm256_mul_pd(_t72_12, _t73_0)));
      _t73_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_11, _t73_3), _mm256_mul_pd(_t72_10, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_9, _t73_1), _mm256_mul_pd(_t72_8, _t73_0)));
      _t73_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_7, _t73_3), _mm256_mul_pd(_t72_6, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_5, _t73_1), _mm256_mul_pd(_t72_4, _t73_0)));

      // 4-BLAC: 4x4 - 4x4
      _t73_12 = _mm256_sub_pd(_t73_7, _t73_8);
      _t73_13 = _mm256_sub_pd(_t73_6, _t73_9);
      _t73_14 = _mm256_sub_pd(_t73_5, _t73_10);
      _t73_15 = _mm256_sub_pd(_t73_4, _t73_11);

      // AVX Storer:
      _asm256_storeu_pd(P + i0 + 52*k2, _t73_12);
      _asm256_storeu_pd(P + i0 + 52*k2 + 52, _t73_13);
      _asm256_storeu_pd(P + i0 + 52*k2 + 104, _t73_14);
      _asm256_storeu_pd(P + i0 + 52*k2 + 156, _t73_15);
    }
    _asm256_storeu_pd(P + 53*k2, _t72_24);
    _mm256_maskstore_pd(P + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t72_25);
    _mm256_maskstore_pd(P + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t72_26);
    _mm256_maskstore_pd(P + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t72_27);
  }

  _t74_19 = _mm256_broadcast_sd(M2 + 2496);
  _t74_18 = _mm256_broadcast_sd(M2 + 2497);
  _t74_17 = _mm256_broadcast_sd(M2 + 2498);
  _t74_16 = _mm256_broadcast_sd(M2 + 2499);
  _t74_15 = _mm256_broadcast_sd(M2 + 2548);
  _t74_14 = _mm256_broadcast_sd(M2 + 2549);
  _t74_13 = _mm256_broadcast_sd(M2 + 2550);
  _t74_12 = _mm256_broadcast_sd(M2 + 2551);
  _t74_11 = _mm256_broadcast_sd(M2 + 2600);
  _t74_10 = _mm256_broadcast_sd(M2 + 2601);
  _t74_9 = _mm256_broadcast_sd(M2 + 2602);
  _t74_8 = _mm256_broadcast_sd(M2 + 2603);
  _t74_7 = _mm256_broadcast_sd(M2 + 2652);
  _t74_6 = _mm256_broadcast_sd(M2 + 2653);
  _t74_5 = _mm256_broadcast_sd(M2 + 2654);
  _t74_4 = _mm256_broadcast_sd(M2 + 2655);
  _t74_3 = _asm256_loadu_pd(M1 + 48);
  _t74_2 = _asm256_loadu_pd(M1 + 100);
  _t74_1 = _asm256_loadu_pd(M1 + 152);
  _t74_0 = _asm256_loadu_pd(M1 + 204);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t74_28 = _t15_28;
  _t74_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t74_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t74_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t74_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_19, _t74_3), _mm256_mul_pd(_t74_18, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_17, _t74_1), _mm256_mul_pd(_t74_16, _t74_0)));
  _t74_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_15, _t74_3), _mm256_mul_pd(_t74_14, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_13, _t74_1), _mm256_mul_pd(_t74_12, _t74_0)));
  _t74_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_11, _t74_3), _mm256_mul_pd(_t74_10, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_9, _t74_1), _mm256_mul_pd(_t74_8, _t74_0)));
  _t74_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_7, _t74_3), _mm256_mul_pd(_t74_6, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_5, _t74_1), _mm256_mul_pd(_t74_4, _t74_0)));

  // 4-BLAC: 4x4 - 4x4
  _t74_24 = _mm256_sub_pd(_t74_28, _t74_20);
  _t74_25 = _mm256_sub_pd(_t74_29, _t74_21);
  _t74_26 = _mm256_sub_pd(_t74_30, _t74_22);
  _t74_27 = _mm256_sub_pd(_t74_31, _t74_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t10_3 = _t74_24;
  _t10_2 = _t74_25;
  _t10_1 = _t74_26;
  _t10_0 = _t74_27;


  for( int k3 = 4; k3 <= 51; k3+=4 ) {

    for( int k2 = 0; k2 <= 47; k2+=4 ) {
      _t75_19 = _mm256_broadcast_sd(M2 + 52*k2 + k3);
      _t75_18 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 1);
      _t75_17 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 2);
      _t75_16 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 3);
      _t75_15 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 52);
      _t75_14 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 53);
      _t75_13 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 54);
      _t75_12 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 55);
      _t75_11 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 104);
      _t75_10 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 105);
      _t75_9 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 106);
      _t75_8 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 107);
      _t75_7 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 156);
      _t75_6 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 157);
      _t75_5 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 158);
      _t75_4 = _mm256_broadcast_sd(M2 + 52*k2 + k3 + 159);
      _t75_3 = _asm256_loadu_pd(M1 + k2 + 52*k3);
      _t75_2 = _asm256_loadu_pd(M1 + k2 + 52*k3 + 52);
      _t75_1 = _asm256_loadu_pd(M1 + k2 + 52*k3 + 104);
      _t75_0 = _asm256_loadu_pd(M1 + k2 + 52*k3 + 156);
      _t75_20 = _asm256_loadu_pd(P + 53*k2);
      _t75_21 = _mm256_maskload_pd(P + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t75_22 = _mm256_maskload_pd(P + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t75_23 = _mm256_maskload_pd(P + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t75_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_19, _t75_3), _mm256_mul_pd(_t75_18, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_17, _t75_1), _mm256_mul_pd(_t75_16, _t75_0)));
      _t75_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_15, _t75_3), _mm256_mul_pd(_t75_14, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_13, _t75_1), _mm256_mul_pd(_t75_12, _t75_0)));
      _t75_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_11, _t75_3), _mm256_mul_pd(_t75_10, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_9, _t75_1), _mm256_mul_pd(_t75_8, _t75_0)));
      _t75_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_7, _t75_3), _mm256_mul_pd(_t75_6, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_5, _t75_1), _mm256_mul_pd(_t75_4, _t75_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t75_28 = _t75_20;
      _t75_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t75_20, _t75_21, 3), _t75_21, 12);
      _t75_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t75_20, _t75_21, 0), _t75_22, 49);
      _t75_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t75_20, _t75_21, 12), _mm256_shuffle_pd(_t75_22, _t75_23, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t75_28 = _mm256_sub_pd(_t75_28, _t75_24);
      _t75_29 = _mm256_sub_pd(_t75_29, _t75_25);
      _t75_30 = _mm256_sub_pd(_t75_30, _t75_26);
      _t75_31 = _mm256_sub_pd(_t75_31, _t75_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t75_20 = _t75_28;
      _t75_21 = _t75_29;
      _t75_22 = _t75_30;
      _t75_23 = _t75_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 51; i0+=4 ) {
        _t76_3 = _asm256_loadu_pd(M1 + i0 + 52*k3);
        _t76_2 = _asm256_loadu_pd(M1 + i0 + 52*k3 + 52);
        _t76_1 = _asm256_loadu_pd(M1 + i0 + 52*k3 + 104);
        _t76_0 = _asm256_loadu_pd(M1 + i0 + 52*k3 + 156);
        _t76_4 = _asm256_loadu_pd(P + i0 + 52*k2);
        _t76_5 = _asm256_loadu_pd(P + i0 + 52*k2 + 52);
        _t76_6 = _asm256_loadu_pd(P + i0 + 52*k2 + 104);
        _t76_7 = _asm256_loadu_pd(P + i0 + 52*k2 + 156);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t76_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_19, _t76_3), _mm256_mul_pd(_t75_18, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_17, _t76_1), _mm256_mul_pd(_t75_16, _t76_0)));
        _t76_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_15, _t76_3), _mm256_mul_pd(_t75_14, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_13, _t76_1), _mm256_mul_pd(_t75_12, _t76_0)));
        _t76_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_11, _t76_3), _mm256_mul_pd(_t75_10, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_9, _t76_1), _mm256_mul_pd(_t75_8, _t76_0)));
        _t76_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_7, _t76_3), _mm256_mul_pd(_t75_6, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_5, _t76_1), _mm256_mul_pd(_t75_4, _t76_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t76_4 = _mm256_sub_pd(_t76_4, _t76_8);
        _t76_5 = _mm256_sub_pd(_t76_5, _t76_9);
        _t76_6 = _mm256_sub_pd(_t76_6, _t76_10);
        _t76_7 = _mm256_sub_pd(_t76_7, _t76_11);

        // AVX Storer:
        _asm256_storeu_pd(P + i0 + 52*k2, _t76_4);
        _asm256_storeu_pd(P + i0 + 52*k2 + 52, _t76_5);
        _asm256_storeu_pd(P + i0 + 52*k2 + 104, _t76_6);
        _asm256_storeu_pd(P + i0 + 52*k2 + 156, _t76_7);
      }
      _asm256_storeu_pd(P + 53*k2, _t75_20);
      _mm256_maskstore_pd(P + 53*k2 + 52, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t75_21);
      _mm256_maskstore_pd(P + 53*k2 + 104, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t75_22);
      _mm256_maskstore_pd(P + 53*k2 + 156, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t75_23);
    }
    _t77_19 = _mm256_broadcast_sd(M2 + k3 + 2496);
    _t77_18 = _mm256_broadcast_sd(M2 + k3 + 2497);
    _t77_17 = _mm256_broadcast_sd(M2 + k3 + 2498);
    _t77_16 = _mm256_broadcast_sd(M2 + k3 + 2499);
    _t77_15 = _mm256_broadcast_sd(M2 + k3 + 2548);
    _t77_14 = _mm256_broadcast_sd(M2 + k3 + 2549);
    _t77_13 = _mm256_broadcast_sd(M2 + k3 + 2550);
    _t77_12 = _mm256_broadcast_sd(M2 + k3 + 2551);
    _t77_11 = _mm256_broadcast_sd(M2 + k3 + 2600);
    _t77_10 = _mm256_broadcast_sd(M2 + k3 + 2601);
    _t77_9 = _mm256_broadcast_sd(M2 + k3 + 2602);
    _t77_8 = _mm256_broadcast_sd(M2 + k3 + 2603);
    _t77_7 = _mm256_broadcast_sd(M2 + k3 + 2652);
    _t77_6 = _mm256_broadcast_sd(M2 + k3 + 2653);
    _t77_5 = _mm256_broadcast_sd(M2 + k3 + 2654);
    _t77_4 = _mm256_broadcast_sd(M2 + k3 + 2655);
    _t77_3 = _asm256_loadu_pd(M1 + 52*k3 + 48);
    _t77_2 = _asm256_loadu_pd(M1 + 52*k3 + 100);
    _t77_1 = _asm256_loadu_pd(M1 + 52*k3 + 152);
    _t77_0 = _asm256_loadu_pd(M1 + 52*k3 + 204);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t77_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_19, _t77_3), _mm256_mul_pd(_t77_18, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_17, _t77_1), _mm256_mul_pd(_t77_16, _t77_0)));
    _t77_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_15, _t77_3), _mm256_mul_pd(_t77_14, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_13, _t77_1), _mm256_mul_pd(_t77_12, _t77_0)));
    _t77_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_11, _t77_3), _mm256_mul_pd(_t77_10, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_9, _t77_1), _mm256_mul_pd(_t77_8, _t77_0)));
    _t77_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_7, _t77_3), _mm256_mul_pd(_t77_6, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_5, _t77_1), _mm256_mul_pd(_t77_4, _t77_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t77_24 = _t10_3;
    _t77_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
    _t77_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
    _t77_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t77_24 = _mm256_sub_pd(_t77_24, _t77_20);
    _t77_25 = _mm256_sub_pd(_t77_25, _t77_21);
    _t77_26 = _mm256_sub_pd(_t77_26, _t77_22);
    _t77_27 = _mm256_sub_pd(_t77_27, _t77_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t10_3 = _t77_24;
    _t10_2 = _t77_25;
    _t10_1 = _t77_26;
    _t10_0 = _t77_27;
    _asm256_storeu_pd(P + 2544, _t10_3);
    _mm256_maskstore_pd(P + 2596, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
    _mm256_maskstore_pd(P + 2648, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
    _mm256_maskstore_pd(P + 2700, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);
  }

  _asm256_storeu_pd(Y + 2544, _t15_28);
  _mm256_maskstore_pd(Y + 2596, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
  _mm256_maskstore_pd(Y + 2648, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
  _mm256_maskstore_pd(Y + 2700, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  _mm_store_sd(&(v0[3]), _mm256_castpd256_pd128(_t59_7));
  _mm_store_sd(&(v0[2]), _mm256_castpd256_pd128(_t59_9));
  _mm_store_sd(&(v0[1]), _mm256_castpd256_pd128(_t59_11));
  _mm_store_sd(&(v0[0]), _mm256_castpd256_pd128(_t59_12));
  _asm256_storeu_pd(P + 2544, _t10_3);
  _mm256_maskstore_pd(P + 2596, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
  _mm256_maskstore_pd(P + 2648, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
  _mm256_maskstore_pd(P + 2700, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);

}
