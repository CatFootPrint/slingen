/*
 * kf_kernel.h
 *
Decl { {u'B': SquaredMatrix[B, (12, 12), GenMatAccess], u'F': SquaredMatrix[F, (12, 12), GenMatAccess], u'H': SquaredMatrix[H, (12, 12), GenMatAccess], 'T2013': Matrix[T2013, (1, 12), GenMatAccess], u'U0': UpperTriangular[U0, (12, 12), GenMatAccess], u'M5': SquaredMatrix[M5, (12, 12), GenMatAccess], u'P': Symmetric[P, (12, 12), USMatAccess], u'M7': SquaredMatrix[M7, (12, 12), GenMatAccess], u'M6': SquaredMatrix[M6, (12, 12), GenMatAccess], u'v4': Matrix[v4, (12, 1), GenMatAccess], u'M0': SquaredMatrix[M0, (12, 12), GenMatAccess], u'M3': Symmetric[M3, (12, 12), USMatAccess], u'M2': SquaredMatrix[M2, (12, 12), GenMatAccess], u'Y': Symmetric[Y, (12, 12), USMatAccess], u'R': Symmetric[R, (12, 12), USMatAccess], u'U': UpperTriangular[U, (12, 12), GenMatAccess], u'M8': SquaredMatrix[M8, (12, 12), GenMatAccess], u'v0': Matrix[v0, (12, 1), GenMatAccess], u'u': Matrix[u, (12, 1), GenMatAccess], u'M4': Symmetric[M4, (12, 12), USMatAccess], u'Q': Symmetric[Q, (12, 12), USMatAccess], u'v2': Matrix[v2, (12, 1), GenMatAccess], u'v1': Matrix[v1, (12, 1), GenMatAccess], u'v3': Matrix[v3, (12, 1), GenMatAccess], 'T1976': Matrix[T1976, (1, 12), GenMatAccess], u'x': Matrix[x, (12, 1), GenMatAccess], u'y': Matrix[y, (12, 1), GenMatAccess], u'M1': SquaredMatrix[M1, (12, 12), GenMatAccess], u'z': Matrix[z, (12, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt': {'m0': 'm03.ll'}, 'ldiv_utn_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt_m03_Assign_Mul_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_ldiv_utn_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[12,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), x[12,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), u[12,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[12,12] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), P[12,12] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[12,12] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[12,12] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[12,12] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[12,12] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[12,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[12,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), y[12,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[12,12] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), Y[12,12] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[12,12] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[12,12] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[12,12] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[12,12] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[12,12] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[12,12] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[12,12] ) ) )
Eq.ann: {}
Entry 7:
For_{fi35;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 12, fi35), U[12,12],h(1, 12, fi35)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi35), U[12,12],h(1, 12, fi35)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi35), U[12,12],h(1, 12, fi35)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(3, 12, fi35 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(3, 12, fi35 + 1)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi35 + 1), U[12,12],h(3, 12, fi35 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi35 + 1), U[12,12],h(3, 12, fi35 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(3, 12, fi35 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(3, 12, fi35 + 1)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 12, fi35 + 1), U[12,12],h(1, 12, fi35 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi35 + 1), U[12,12],h(1, 12, fi35 + 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi35 + 1), U[12,12],h(1, 12, fi35 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(2, 12, fi35 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(2, 12, fi35 + 2)) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi35 + 2), U[12,12],h(2, 12, fi35 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi35 + 2), U[12,12],h(2, 12, fi35 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(2, 12, fi35 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(2, 12, fi35 + 2)) ) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 2)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 3)) ) = ( Tile( (1, 1), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 3)) ) Div Tile( (1, 1), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 2)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 3), U[12,12],h(1, 12, fi35 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 3), U[12,12],h(1, 12, fi35 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 3)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 12, fi35 + 3), U[12,12],h(1, 12, fi35 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, fi35 + 3), U[12,12],h(1, 12, fi35 + 3)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi35 + 3), U[12,12],h(1, 12, fi35 + 3)) ) )
Eq.ann: {}
Entry 14:
For_{fi96;0;-fi35 + 4;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi35 + 1), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi35 + 1), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(3, 12, fi35 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi35 + 2), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi35 + 2), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(2, 12, fi35 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 1), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 2), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 2), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 3), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 3), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 2), U[12,12],h(1, 12, fi35 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 2), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 3), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, fi35 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi35 + 3), U[12,12],h(4, 12, fi35 + fi96 + 4)) ) ) )
Eq.ann: {}
 )Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi35 + 8, 12, fi35 + 4), U[12,12],h(-fi35 + 8, 12, fi35 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi35 + 8, 12, fi35 + 4), U[12,12],h(-fi35 + 8, 12, fi35 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi35), U[12,12],h(-fi35 + 8, 12, fi35 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi35), U[12,12],h(-fi35 + 8, 12, fi35 + 4)) ) ) ) )
Eq.ann: {}
 )Entry 8:
Eq: Tile( (1, 1), G(h(1, 12, 8), U[12,12],h(1, 12, 8)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 8), U[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1976[1,12],h(1, 12, 8)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 8), U[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, 8)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), U[12,12],h(3, 12, 9)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), U[12,12],h(3, 12, 9)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 12, 9), U[12,12],h(1, 12, 9)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 9), U[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T1976[1,12],h(1, 12, 9)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 9), U[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T1976[1,12],h(1, 12, 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), U[12,12],h(2, 12, 10)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), U[12,12],h(2, 12, 10)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 12, 10), U[12,12],h(1, 12, 10)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 10), U[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 12, 10), U[12,12],h(1, 12, 11)) ) = ( Tile( (1, 1), G(h(1, 12, 10), U[12,12],h(1, 12, 11)) ) Div Tile( (1, 1), G(h(1, 12, 10), U[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), U[12,12],h(1, 12, 11)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), U[12,12],h(1, 12, 11)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), U[12,12],h(1, 12, 11)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), U[12,12],h(1, 12, 11)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 12, 11), U[12,12],h(1, 12, 11)) ) = Sqrt( Tile( (1, 1), G(h(1, 12, 11), U[12,12],h(1, 12, 11)) ) )
Eq.ann: {}
Entry 20:
For_{fi182;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 12, fi182), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi182), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi182), U0[12,12],h(1, 12, fi182)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi182 + 1), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi182 + 1), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182), U0[12,12],h(3, 12, fi182 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 12, fi182 + 1), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi182 + 1), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi182 + 1), U0[12,12],h(1, 12, fi182 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi182 + 2), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi182 + 2), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182 + 1), U0[12,12],h(2, 12, fi182 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182 + 1), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 12, fi182 + 2), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi182 + 2), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi182 + 2), U0[12,12],h(1, 12, fi182 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182 + 3), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182 + 3), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182 + 2), U0[12,12],h(1, 12, fi182 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi182 + 2), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 12, fi182 + 3), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, fi182 + 3), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, fi182 + 3), U0[12,12],h(1, 12, fi182 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi182 + 8, 12, fi182 + 4), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi182 + 8, 12, fi182 + 4), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi182), U0[12,12],h(-fi182 + 8, 12, fi182 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi182), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 21:
Eq: Tile( (1, 1), G(h(1, 12, 8), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 8), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 8), U0[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), U0[12,12],h(3, 12, 9)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 12, 9), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 9), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 9), U0[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), U0[12,12],h(2, 12, 10)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 12, 10), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 10), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 10), U0[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), v2[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), v2[12,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), U0[12,12],h(1, 12, 11)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), v2[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 12, 11), v2[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 11), v2[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 11), U0[12,12],h(1, 12, 11)) ) )
Eq.ann: {}
Entry 28:
For_{fi259;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 12, -fi259 + 11), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi259 + 11), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi259 + 11), U0[12,12],h(1, 12, -fi259 + 11)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi259 + 8), U0[12,12],h(1, 12, -fi259 + 11)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi259 + 11), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 12, -fi259 + 10), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi259 + 10), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi259 + 10), U0[12,12],h(1, 12, -fi259 + 10)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi259 + 8), U0[12,12],h(1, 12, -fi259 + 10)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi259 + 10), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 12, -fi259 + 9), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi259 + 9), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi259 + 9), U0[12,12],h(1, 12, -fi259 + 9)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi259 + 8), U0[12,12],h(1, 12, -fi259 + 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi259 + 9), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, -fi259 + 8), U0[12,12],h(1, 12, -fi259 + 8)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi259 + 8, 12, 0), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi259 + 8, 12, 0), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi259 + 8, 12, 0), U0[12,12],h(4, 12, -fi259 + 8)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, -fi259 + 8), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 29:
Eq: Tile( (1, 1), G(h(1, 12, 3), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 3), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 3), U0[12,12],h(1, 12, 3)) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), U0[12,12],h(1, 12, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 12, 2), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 2), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 2), U0[12,12],h(1, 12, 2)) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), U0[12,12],h(1, 12, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 12, 1), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 1), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 1), U0[12,12],h(1, 12, 1)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), v4[12,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), v4[12,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), U0[12,12],h(1, 12, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), v4[12,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 12, 0), v4[12,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 12, 0), v4[12,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 12, 0), U0[12,12],h(1, 12, 0)) ) )
Eq.ann: {}
Entry 36:
For_{fi336;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi336), U0[12,12],h(1, 12, fi336)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi336 + 1), U0[12,12],h(1, 12, fi336 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi336 + 2), U0[12,12],h(1, 12, fi336 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, fi336 + 3), U0[12,12],h(1, 12, fi336 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi355;0;8;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336), M6[12,12],h(4, 12, fi355)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi336 + 1), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, fi336 + 1), M6[12,12],h(4, 12, fi355)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336), U0[12,12],h(3, 12, fi336 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336), M6[12,12],h(4, 12, fi355)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 1), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 1), M6[12,12],h(4, 12, fi355)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi336 + 2), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, fi336 + 2), M6[12,12],h(4, 12, fi355)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 1), U0[12,12],h(2, 12, fi336 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 1), M6[12,12],h(4, 12, fi355)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 2), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 2), M6[12,12],h(4, 12, fi355)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 3), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 3), M6[12,12],h(4, 12, fi355)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 2), U0[12,12],h(1, 12, fi336 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 2), M6[12,12],h(4, 12, fi355)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 3), M6[12,12],h(4, 12, fi355)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, fi336 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, fi336 + 3), M6[12,12],h(4, 12, fi355)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi336 + 8, 12, fi336 + 4), M6[12,12],h(12, 12, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi336 + 8, 12, fi336 + 4), M6[12,12],h(12, 12, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi336), U0[12,12],h(-fi336 + 8, 12, fi336 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, fi336), M6[12,12],h(12, 12, 0)) ) ) ) )
Eq.ann: {}
 )Entry 37:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 8)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 8), U0[12,12],h(1, 12, 8)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 9)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 9), U0[12,12],h(1, 12, 9)) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 10)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 10), U0[12,12],h(1, 12, 10)) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 11)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 11), U0[12,12],h(1, 12, 11)) ) )
Eq.ann: {}
Entry 41:
For_{fi402;0;8;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 8)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M6[12,12],h(4, 12, fi402)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 9), M6[12,12],h(4, 12, fi402)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), U0[12,12],h(3, 12, 9)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 8), M6[12,12],h(4, 12, fi402)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M6[12,12],h(4, 12, fi402)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 10), M6[12,12],h(4, 12, fi402)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), U0[12,12],h(2, 12, 10)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 9), M6[12,12],h(4, 12, fi402)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 10)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M6[12,12],h(4, 12, fi402)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M6[12,12],h(4, 12, fi402)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), U0[12,12],h(1, 12, 11)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 10), M6[12,12],h(4, 12, fi402)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M6[12,12],h(4, 12, fi402)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 11)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 11), M6[12,12],h(4, 12, fi402)) ) ) )
Eq.ann: {}
 )Entry 42:
For_{fi449;0;7;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 11)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi449 + 11), U0[12,12],h(1, 12, -fi449 + 11)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 10)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi449 + 10), U0[12,12],h(1, 12, -fi449 + 10)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 9)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi449 + 9), U0[12,12],h(1, 12, -fi449 + 9)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 8)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, -fi449 + 8), U0[12,12],h(1, 12, -fi449 + 8)) ) )
Eq.ann: {}
Entry 4:
For_{fi468;0;8;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 11), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 11)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 11), M8[12,12],h(4, 12, fi468)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, -fi449 + 8), U0[12,12],h(1, 12, -fi449 + 11)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 11), M8[12,12],h(4, 12, fi468)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 10), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 10)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 10), M8[12,12],h(4, 12, fi468)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, -fi449 + 8), U0[12,12],h(1, 12, -fi449 + 10)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 10), M8[12,12],h(4, 12, fi468)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 9), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 9), M8[12,12],h(4, 12, fi468)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 8), U0[12,12],h(1, 12, -fi449 + 9)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 9), M8[12,12],h(4, 12, fi468)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi449 + 8)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, -fi449 + 8), M8[12,12],h(4, 12, fi468)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi449 + 8, 12, 0), M8[12,12],h(12, 12, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi449 + 8, 12, 0), M8[12,12],h(12, 12, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi449 + 8, 12, 0), U0[12,12],h(4, 12, -fi449 + 8)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 12, -fi449 + 8), M8[12,12],h(12, 12, 0)) ) ) ) )
Eq.ann: {}
 )Entry 43:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 3), U0[12,12],h(1, 12, 3)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 2), U0[12,12],h(1, 12, 2)) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 1), U0[12,12],h(1, 12, 1)) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2013[1,12],h(1, 12, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 12, 0), U0[12,12],h(1, 12, 0)) ) )
Eq.ann: {}
Entry 47:
For_{fi515;0;8;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), M8[12,12],h(4, 12, fi515)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 12, 0), U0[12,12],h(1, 12, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 3), M8[12,12],h(4, 12, fi515)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), M8[12,12],h(4, 12, fi515)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 12, 0), U0[12,12],h(1, 12, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 2), M8[12,12],h(4, 12, fi515)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), M8[12,12],h(4, 12, fi515)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), U0[12,12],h(1, 12, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 1), M8[12,12],h(4, 12, fi515)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2013[1,12],h(1, 12, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 12, 0), M8[12,12],h(4, 12, fi515)) ) ) )
Eq.ann: {}
 )Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), x[12,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[12,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), v0[12,1] ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), Tile( (4, 4), P[12,12] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[12,12] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[12,12] ) ) * Tile( (1, 1), Tile( (4, 4), M1[12,12] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 12
#define PARAM1 12
#define PARAM2 12

#define ERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31,
	_t7_32, _t7_33, _t7_34, _t7_35, _t7_36, _t7_37, _t7_38, _t7_39,
	_t7_40, _t7_41, _t7_42, _t7_43;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27, _t9_28, _t9_29, _t9_30, _t9_31;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31,
	_t11_32, _t11_33, _t11_34, _t11_35, _t11_36, _t11_37, _t11_38, _t11_39,
	_t11_40, _t11_41, _t11_42, _t11_43;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27, _t13_28, _t13_29, _t13_30, _t13_31,
	_t13_32, _t13_33, _t13_34, _t13_35, _t13_36, _t13_37, _t13_38, _t13_39,
	_t13_40, _t13_41, _t13_42, _t13_43;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19, _t23_20, _t23_21, _t23_22, _t23_23,
	_t23_24, _t23_25, _t23_26, _t23_27, _t23_28, _t23_29, _t23_30, _t23_31,
	_t23_32, _t23_33, _t23_34, _t23_35, _t23_36, _t23_37, _t23_38, _t23_39,
	_t23_40, _t23_41, _t23_42, _t23_43;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27, _t25_28, _t25_29, _t25_30, _t25_31;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7,
	_t28_8, _t28_9, _t28_10, _t28_11;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23,
	_t31_24, _t31_25, _t31_26, _t31_27, _t31_28, _t31_29, _t31_30, _t31_31;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7,
	_t36_8, _t36_9, _t36_10, _t36_11, _t36_12, _t36_13, _t36_14, _t36_15,
	_t36_16, _t36_17, _t36_18, _t36_19, _t36_20, _t36_21, _t36_22, _t36_23,
	_t36_24, _t36_25, _t36_26, _t36_27, _t36_28, _t36_29, _t36_30, _t36_31;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11, _t38_12, _t38_13, _t38_14, _t38_15;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31,
	_t39_32, _t39_33, _t39_34, _t39_35, _t39_36, _t39_37, _t39_38, _t39_39,
	_t39_40, _t39_41, _t39_42, _t39_43;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3, _t40_4, _t40_5, _t40_6, _t40_7,
	_t40_8, _t40_9, _t40_10, _t40_11, _t40_12, _t40_13, _t40_14, _t40_15,
	_t40_16, _t40_17, _t40_18, _t40_19;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15,
	_t41_16, _t41_17, _t41_18, _t41_19, _t41_20, _t41_21, _t41_22, _t41_23,
	_t41_24, _t41_25, _t41_26, _t41_27, _t41_28, _t41_29, _t41_30, _t41_31,
	_t41_32, _t41_33, _t41_34, _t41_35, _t41_36, _t41_37, _t41_38, _t41_39,
	_t41_40, _t41_41, _t41_42, _t41_43;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31,
	_t42_32, _t42_33, _t42_34, _t42_35;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27, _t45_28, _t45_29, _t45_30, _t45_31,
	_t45_32, _t45_33, _t45_34, _t45_35, _t45_36, _t45_37, _t45_38, _t45_39,
	_t45_40, _t45_41, _t45_42, _t45_43, _t45_44, _t45_45, _t45_46, _t45_47,
	_t45_48, _t45_49, _t45_50, _t45_51, _t45_52, _t45_53, _t45_54, _t45_55,
	_t45_56, _t45_57, _t45_58, _t45_59, _t45_60, _t45_61, _t45_62, _t45_63,
	_t45_64, _t45_65, _t45_66, _t45_67, _t45_68, _t45_69, _t45_70, _t45_71,
	_t45_72, _t45_73, _t45_74, _t45_75, _t45_76, _t45_77, _t45_78, _t45_79,
	_t45_80, _t45_81, _t45_82, _t45_83, _t45_84, _t45_85, _t45_86, _t45_87,
	_t45_88, _t45_89, _t45_90, _t45_91, _t45_92, _t45_93, _t45_94, _t45_95,
	_t45_96, _t45_97, _t45_98, _t45_99, _t45_100, _t45_101, _t45_102, _t45_103,
	_t45_104, _t45_105, _t45_106, _t45_107, _t45_108, _t45_109, _t45_110, _t45_111,
	_t45_112, _t45_113, _t45_114, _t45_115, _t45_116, _t45_117, _t45_118, _t45_119,
	_t45_120, _t45_121, _t45_122, _t45_123, _t45_124, _t45_125, _t45_126, _t45_127,
	_t45_128, _t45_129, _t45_130, _t45_131, _t45_132, _t45_133, _t45_134, _t45_135,
	_t45_136, _t45_137, _t45_138, _t45_139, _t45_140, _t45_141, _t45_142, _t45_143,
	_t45_144, _t45_145, _t45_146, _t45_147, _t45_148, _t45_149, _t45_150, _t45_151,
	_t45_152, _t45_153, _t45_154, _t45_155, _t45_156, _t45_157, _t45_158, _t45_159,
	_t45_160, _t45_161, _t45_162, _t45_163, _t45_164, _t45_165, _t45_166, _t45_167,
	_t45_168, _t45_169, _t45_170, _t45_171, _t45_172, _t45_173, _t45_174, _t45_175,
	_t45_176, _t45_177, _t45_178, _t45_179, _t45_180, _t45_181, _t45_182, _t45_183,
	_t45_184, _t45_185, _t45_186, _t45_187, _t45_188, _t45_189, _t45_190, _t45_191,
	_t45_192, _t45_193, _t45_194, _t45_195, _t45_196, _t45_197, _t45_198, _t45_199,
	_t45_200, _t45_201, _t45_202, _t45_203, _t45_204, _t45_205, _t45_206, _t45_207,
	_t45_208, _t45_209, _t45_210, _t45_211, _t45_212, _t45_213, _t45_214, _t45_215,
	_t45_216, _t45_217, _t45_218, _t45_219, _t45_220, _t45_221, _t45_222, _t45_223,
	_t45_224, _t45_225, _t45_226, _t45_227, _t45_228, _t45_229, _t45_230, _t45_231,
	_t45_232, _t45_233, _t45_234, _t45_235, _t45_236, _t45_237, _t45_238, _t45_239,
	_t45_240, _t45_241, _t45_242, _t45_243, _t45_244, _t45_245, _t45_246, _t45_247,
	_t45_248, _t45_249, _t45_250, _t45_251, _t45_252, _t45_253, _t45_254, _t45_255,
	_t45_256, _t45_257, _t45_258, _t45_259, _t45_260, _t45_261, _t45_262, _t45_263,
	_t45_264, _t45_265, _t45_266, _t45_267, _t45_268, _t45_269, _t45_270, _t45_271,
	_t45_272, _t45_273, _t45_274, _t45_275, _t45_276, _t45_277, _t45_278, _t45_279,
	_t45_280, _t45_281, _t45_282, _t45_283, _t45_284, _t45_285, _t45_286, _t45_287,
	_t45_288, _t45_289, _t45_290, _t45_291, _t45_292, _t45_293, _t45_294, _t45_295,
	_t45_296, _t45_297, _t45_298, _t45_299, _t45_300, _t45_301, _t45_302, _t45_303,
	_t45_304, _t45_305, _t45_306, _t45_307, _t45_308, _t45_309, _t45_310, _t45_311,
	_t45_312, _t45_313, _t45_314, _t45_315, _t45_316, _t45_317, _t45_318, _t45_319,
	_t45_320, _t45_321, _t45_322, _t45_323, _t45_324, _t45_325, _t45_326, _t45_327,
	_t45_328, _t45_329, _t45_330, _t45_331, _t45_332, _t45_333, _t45_334, _t45_335,
	_t45_336, _t45_337, _t45_338, _t45_339, _t45_340, _t45_341, _t45_342, _t45_343,
	_t45_344, _t45_345, _t45_346, _t45_347, _t45_348, _t45_349, _t45_350, _t45_351,
	_t45_352, _t45_353, _t45_354, _t45_355, _t45_356, _t45_357, _t45_358, _t45_359,
	_t45_360, _t45_361, _t45_362, _t45_363, _t45_364, _t45_365, _t45_366, _t45_367,
	_t45_368, _t45_369, _t45_370, _t45_371, _t45_372, _t45_373, _t45_374, _t45_375,
	_t45_376, _t45_377, _t45_378, _t45_379, _t45_380, _t45_381, _t45_382, _t45_383,
	_t45_384, _t45_385, _t45_386, _t45_387, _t45_388, _t45_389, _t45_390, _t45_391,
	_t45_392, _t45_393, _t45_394, _t45_395, _t45_396, _t45_397, _t45_398, _t45_399;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11, _t46_12, _t46_13, _t46_14, _t46_15,
	_t46_16, _t46_17, _t46_18, _t46_19, _t46_20, _t46_21, _t46_22, _t46_23,
	_t46_24, _t46_25, _t46_26, _t46_27, _t46_28, _t46_29, _t46_30, _t46_31,
	_t46_32, _t46_33, _t46_34, _t46_35, _t46_36, _t46_37, _t46_38, _t46_39,
	_t46_40, _t46_41, _t46_42;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26, _t49_27, _t49_28, _t49_29, _t49_30, _t49_31,
	_t49_32, _t49_33, _t49_34, _t49_35, _t49_36, _t49_37, _t49_38, _t49_39;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15,
	_t51_16, _t51_17, _t51_18, _t51_19, _t51_20, _t51_21, _t51_22, _t51_23,
	_t51_24, _t51_25, _t51_26, _t51_27, _t51_28, _t51_29, _t51_30, _t51_31,
	_t51_32;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19, _t52_20, _t52_21, _t52_22, _t52_23,
	_t52_24, _t52_25, _t52_26, _t52_27, _t52_28, _t52_29, _t52_30, _t52_31,
	_t52_32, _t52_33, _t52_34, _t52_35, _t52_36, _t52_37, _t52_38, _t52_39,
	_t52_40, _t52_41, _t52_42, _t52_43, _t52_44, _t52_45, _t52_46, _t52_47,
	_t52_48, _t52_49, _t52_50, _t52_51, _t52_52, _t52_53, _t52_54, _t52_55,
	_t52_56, _t52_57, _t52_58, _t52_59, _t52_60, _t52_61;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11, _t55_12, _t55_13, _t55_14, _t55_15,
	_t55_16, _t55_17, _t55_18, _t55_19, _t55_20, _t55_21, _t55_22, _t55_23,
	_t55_24, _t55_25, _t55_26, _t55_27, _t55_28, _t55_29, _t55_30, _t55_31,
	_t55_32, _t55_33, _t55_34, _t55_35, _t55_36, _t55_37, _t55_38, _t55_39,
	_t55_40, _t55_41, _t55_42, _t55_43, _t55_44, _t55_45, _t55_46, _t55_47,
	_t55_48, _t55_49, _t55_50, _t55_51, _t55_52, _t55_53, _t55_54;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3, _t56_4, _t56_5, _t56_6, _t56_7,
	_t56_8, _t56_9, _t56_10, _t56_11, _t56_12, _t56_13, _t56_14, _t56_15,
	_t56_16, _t56_17, _t56_18, _t56_19, _t56_20, _t56_21, _t56_22, _t56_23,
	_t56_24, _t56_25, _t56_26;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11, _t57_12, _t57_13, _t57_14, _t57_15,
	_t57_16, _t57_17, _t57_18, _t57_19, _t57_20, _t57_21, _t57_22, _t57_23,
	_t57_24, _t57_25, _t57_26, _t57_27, _t57_28, _t57_29, _t57_30, _t57_31,
	_t57_32, _t57_33, _t57_34, _t57_35, _t57_36, _t57_37, _t57_38, _t57_39,
	_t57_40, _t57_41, _t57_42, _t57_43, _t57_44, _t57_45, _t57_46, _t57_47,
	_t57_48, _t57_49, _t57_50, _t57_51, _t57_52, _t57_53, _t57_54, _t57_55,
	_t57_56, _t57_57, _t57_58;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9, _t58_10, _t58_11, _t58_12, _t58_13, _t58_14, _t58_15,
	_t58_16, _t58_17, _t58_18, _t58_19, _t58_20, _t58_21, _t58_22, _t58_23,
	_t58_24, _t58_25, _t58_26;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26, _t59_27;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11, _t60_12, _t60_13, _t60_14, _t60_15,
	_t60_16, _t60_17, _t60_18, _t60_19, _t60_20, _t60_21, _t60_22, _t60_23,
	_t60_24, _t60_25, _t60_26, _t60_27, _t60_28, _t60_29, _t60_30, _t60_31,
	_t60_32, _t60_33, _t60_34, _t60_35, _t60_36, _t60_37, _t60_38, _t60_39,
	_t60_40, _t60_41, _t60_42, _t60_43, _t60_44, _t60_45, _t60_46, _t60_47,
	_t60_48, _t60_49, _t60_50, _t60_51;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11, _t61_12, _t61_13, _t61_14, _t61_15,
	_t61_16, _t61_17, _t61_18, _t61_19, _t61_20, _t61_21, _t61_22, _t61_23,
	_t61_24, _t61_25, _t61_26;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15,
	_t64_16, _t64_17, _t64_18, _t64_19, _t64_20, _t64_21, _t64_22, _t64_23,
	_t64_24, _t64_25, _t64_26, _t64_27, _t64_28, _t64_29, _t64_30, _t64_31,
	_t64_32, _t64_33, _t64_34, _t64_35, _t64_36, _t64_37, _t64_38, _t64_39;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26, _t66_27, _t66_28, _t66_29, _t66_30, _t66_31;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15,
	_t67_16, _t67_17, _t67_18, _t67_19, _t67_20, _t67_21, _t67_22, _t67_23,
	_t67_24, _t67_25, _t67_26, _t67_27, _t67_28, _t67_29, _t67_30, _t67_31;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23,
	_t69_24, _t69_25, _t69_26, _t69_27;


  // Generating : y[12,1] = ( ( Sum_{i0} ( S(h(4, 12, i0), ( ( G(h(4, 12, i0), F[12,12],h(4, 12, 0)) * G(h(4, 12, 0), x[12,1],h(1, 1, 0)) ) + ( G(h(4, 12, i0), B[12,12],h(4, 12, 0)) * G(h(4, 12, 0), u[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 12, i0), ( G(h(4, 12, i0), F[12,12],h(4, 12, k2)) * G(h(4, 12, k2), x[12,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) ) + Sum_{k3} ( Sum_{i0} ( $(h(4, 12, i0), ( G(h(4, 12, i0), B[12,12],h(4, 12, k3)) * G(h(4, 12, k3), u[12,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t0_9 = _asm256_loadu_pd(F + 12*i0);
    _t0_8 = _asm256_loadu_pd(F + 12*i0 + 12);
    _t0_7 = _asm256_loadu_pd(F + 12*i0 + 24);
    _t0_6 = _asm256_loadu_pd(F + 12*i0 + 36);
    _t0_5 = _asm256_loadu_pd(x);
    _t0_4 = _asm256_loadu_pd(B + 12*i0);
    _t0_3 = _asm256_loadu_pd(B + 12*i0 + 12);
    _t0_2 = _asm256_loadu_pd(B + 12*i0 + 24);
    _t0_1 = _asm256_loadu_pd(B + 12*i0 + 36);
    _t0_0 = _asm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:
    _asm256_storeu_pd(y + i0, _t0_10);
  }


  for( int k2 = 4; k2 <= 11; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 11; i0+=4 ) {
      _t1_4 = _asm256_loadu_pd(F + 12*i0 + k2);
      _t1_3 = _asm256_loadu_pd(F + 12*i0 + k2 + 12);
      _t1_2 = _asm256_loadu_pd(F + 12*i0 + k2 + 24);
      _t1_1 = _asm256_loadu_pd(F + 12*i0 + k2 + 36);
      _t1_0 = _asm256_loadu_pd(x + k2);
      _t1_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t1_5 = _mm256_add_pd(_t1_5, _t1_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t1_5);
    }
  }


  for( int k3 = 4; k3 <= 11; k3+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 11; i0+=4 ) {
      _t2_4 = _asm256_loadu_pd(B + 12*i0 + k3);
      _t2_3 = _asm256_loadu_pd(B + 12*i0 + k3 + 12);
      _t2_2 = _asm256_loadu_pd(B + 12*i0 + k3 + 24);
      _t2_1 = _asm256_loadu_pd(B + 12*i0 + k3 + 36);
      _t2_0 = _asm256_loadu_pd(u + k3);
      _t2_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t2_5 = _mm256_add_pd(_t2_5, _t2_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t2_5);
    }
  }

  _t3_3 = _asm256_loadu_pd(P);
  _t3_2 = _mm256_maskload_pd(P + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[12,12] = ( ( Sum_{k2} ( ( S(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 0)) * G(h(4, 12, 0), P[12,12],h(4, 12, 0)) ),h(4, 12, 0)) + Sum_{i0} ( S(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 0)) * G(h(4, 12, 0), P[12,12],h(4, 12, i0)) ),h(4, 12, i0)) ) ) ) + Sum_{k2} ( ( ( $(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 4)) * T( G(h(4, 12, 0), P[12,12],h(4, 12, 4)) ) ),h(4, 12, 0)) + $(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 4)) * G(h(4, 12, 4), P[12,12],h(4, 12, 4)) ),h(4, 12, 4)) ) + $(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 4)) * G(h(4, 12, 4), P[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 8)) * T( G(h(4, 12, i0), P[12,12],h(4, 12, 8)) ) ),h(4, 12, i0)) ) + $(h(4, 12, k2), ( G(h(4, 12, k2), F[12,12],h(4, 12, 8)) * G(h(4, 12, 8), P[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_4 = _t3_3;
  _t3_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 12*k2);
    _t4_14 = _mm256_broadcast_sd(F + 12*k2 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 12*k2 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 12*k2 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 12*k2 + 12);
    _t4_10 = _mm256_broadcast_sd(F + 12*k2 + 13);
    _t4_9 = _mm256_broadcast_sd(F + 12*k2 + 14);
    _t4_8 = _mm256_broadcast_sd(F + 12*k2 + 15);
    _t4_7 = _mm256_broadcast_sd(F + 12*k2 + 24);
    _t4_6 = _mm256_broadcast_sd(F + 12*k2 + 25);
    _t4_5 = _mm256_broadcast_sd(F + 12*k2 + 26);
    _t4_4 = _mm256_broadcast_sd(F + 12*k2 + 27);
    _t4_3 = _mm256_broadcast_sd(F + 12*k2 + 36);
    _t4_2 = _mm256_broadcast_sd(F + 12*k2 + 37);
    _t4_1 = _mm256_broadcast_sd(F + 12*k2 + 38);
    _t4_0 = _mm256_broadcast_sd(F + 12*k2 + 39);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_4), _mm256_mul_pd(_t4_14, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_6), _mm256_mul_pd(_t4_12, _t3_7)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_4), _mm256_mul_pd(_t4_10, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_6), _mm256_mul_pd(_t4_8, _t3_7)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_4), _mm256_mul_pd(_t4_6, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_6), _mm256_mul_pd(_t4_4, _t3_7)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_4), _mm256_mul_pd(_t4_2, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_6), _mm256_mul_pd(_t4_0, _t3_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 11; i0+=4 ) {
      _t5_3 = _asm256_loadu_pd(P + i0);
      _t5_2 = _asm256_loadu_pd(P + i0 + 12);
      _t5_1 = _asm256_loadu_pd(P + i0 + 24);
      _t5_0 = _asm256_loadu_pd(P + i0 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t5_3), _mm256_mul_pd(_t4_14, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t5_1), _mm256_mul_pd(_t4_12, _t5_0)));
      _t5_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t5_3), _mm256_mul_pd(_t4_10, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t5_1), _mm256_mul_pd(_t4_8, _t5_0)));
      _t5_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t5_3), _mm256_mul_pd(_t4_6, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t5_1), _mm256_mul_pd(_t4_4, _t5_0)));
      _t5_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t5_3), _mm256_mul_pd(_t4_2, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t5_1), _mm256_mul_pd(_t4_0, _t5_0)));

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 12*k2, _t5_4);
      _asm256_storeu_pd(M0 + i0 + 12*k2 + 12, _t5_5);
      _asm256_storeu_pd(M0 + i0 + 12*k2 + 24, _t5_6);
      _asm256_storeu_pd(M0 + i0 + 12*k2 + 36, _t5_7);
    }
    _asm256_storeu_pd(M0 + 12*k2, _t4_16);
    _asm256_storeu_pd(M0 + 12*k2 + 12, _t4_17);
    _asm256_storeu_pd(M0 + 12*k2 + 24, _t4_18);
    _asm256_storeu_pd(M0 + 12*k2 + 36, _t4_19);
  }

  _t6_7 = _asm256_loadu_pd(P + 4);
  _t6_6 = _asm256_loadu_pd(P + 16);
  _t6_5 = _asm256_loadu_pd(P + 28);
  _t6_4 = _asm256_loadu_pd(P + 40);
  _t6_3 = _asm256_loadu_pd(P + 52);
  _t6_2 = _mm256_maskload_pd(P + 64, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t6_1 = _mm256_maskload_pd(P + 76, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t6_0 = _mm256_maskload_pd(P + 88, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t6_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_7, _t6_6), _mm256_unpacklo_pd(_t6_5, _t6_4), 32);
  _t6_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_7, _t6_6), _mm256_unpackhi_pd(_t6_5, _t6_4), 32);
  _t6_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_7, _t6_6), _mm256_unpacklo_pd(_t6_5, _t6_4), 49);
  _t6_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_7, _t6_6), _mm256_unpackhi_pd(_t6_5, _t6_4), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t6_8 = _t6_3;
  _t6_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
  _t6_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
  _t6_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

  // AVX Loader:


  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t7_19 = _mm256_broadcast_sd(F + 12*k2 + 4);
    _t7_18 = _mm256_broadcast_sd(F + 12*k2 + 5);
    _t7_17 = _mm256_broadcast_sd(F + 12*k2 + 6);
    _t7_16 = _mm256_broadcast_sd(F + 12*k2 + 7);
    _t7_15 = _mm256_broadcast_sd(F + 12*k2 + 16);
    _t7_14 = _mm256_broadcast_sd(F + 12*k2 + 17);
    _t7_13 = _mm256_broadcast_sd(F + 12*k2 + 18);
    _t7_12 = _mm256_broadcast_sd(F + 12*k2 + 19);
    _t7_11 = _mm256_broadcast_sd(F + 12*k2 + 28);
    _t7_10 = _mm256_broadcast_sd(F + 12*k2 + 29);
    _t7_9 = _mm256_broadcast_sd(F + 12*k2 + 30);
    _t7_8 = _mm256_broadcast_sd(F + 12*k2 + 31);
    _t7_7 = _mm256_broadcast_sd(F + 12*k2 + 40);
    _t7_6 = _mm256_broadcast_sd(F + 12*k2 + 41);
    _t7_5 = _mm256_broadcast_sd(F + 12*k2 + 42);
    _t7_4 = _mm256_broadcast_sd(F + 12*k2 + 43);
    _t7_3 = _asm256_loadu_pd(P + 56);
    _t7_2 = _asm256_loadu_pd(P + 68);
    _t7_1 = _asm256_loadu_pd(P + 80);
    _t7_0 = _asm256_loadu_pd(P + 92);
    _t7_20 = _asm256_loadu_pd(M0 + 12*k2);
    _t7_21 = _asm256_loadu_pd(M0 + 12*k2 + 12);
    _t7_22 = _asm256_loadu_pd(M0 + 12*k2 + 24);
    _t7_23 = _asm256_loadu_pd(M0 + 12*k2 + 36);
    _t7_24 = _asm256_loadu_pd(M0 + 12*k2 + 4);
    _t7_25 = _asm256_loadu_pd(M0 + 12*k2 + 16);
    _t7_26 = _asm256_loadu_pd(M0 + 12*k2 + 28);
    _t7_27 = _asm256_loadu_pd(M0 + 12*k2 + 40);
    _t7_28 = _asm256_loadu_pd(M0 + 12*k2 + 8);
    _t7_29 = _asm256_loadu_pd(M0 + 12*k2 + 20);
    _t7_30 = _asm256_loadu_pd(M0 + 12*k2 + 32);
    _t7_31 = _asm256_loadu_pd(M0 + 12*k2 + 44);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t7_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t6_12), _mm256_mul_pd(_t7_18, _t6_13)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t6_14), _mm256_mul_pd(_t7_16, _t6_15)));
    _t7_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t6_12), _mm256_mul_pd(_t7_14, _t6_13)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t6_14), _mm256_mul_pd(_t7_12, _t6_15)));
    _t7_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t6_12), _mm256_mul_pd(_t7_10, _t6_13)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t6_14), _mm256_mul_pd(_t7_8, _t6_15)));
    _t7_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t6_12), _mm256_mul_pd(_t7_6, _t6_13)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t6_14), _mm256_mul_pd(_t7_4, _t6_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t7_20 = _mm256_add_pd(_t7_20, _t7_32);
    _t7_21 = _mm256_add_pd(_t7_21, _t7_33);
    _t7_22 = _mm256_add_pd(_t7_22, _t7_34);
    _t7_23 = _mm256_add_pd(_t7_23, _t7_35);

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t7_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t6_8), _mm256_mul_pd(_t7_18, _t6_9)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t6_10), _mm256_mul_pd(_t7_16, _t6_11)));
    _t7_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t6_8), _mm256_mul_pd(_t7_14, _t6_9)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t6_10), _mm256_mul_pd(_t7_12, _t6_11)));
    _t7_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t6_8), _mm256_mul_pd(_t7_10, _t6_9)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t6_10), _mm256_mul_pd(_t7_8, _t6_11)));
    _t7_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t6_8), _mm256_mul_pd(_t7_6, _t6_9)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t6_10), _mm256_mul_pd(_t7_4, _t6_11)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t7_24 = _mm256_add_pd(_t7_24, _t7_36);
    _t7_25 = _mm256_add_pd(_t7_25, _t7_37);
    _t7_26 = _mm256_add_pd(_t7_26, _t7_38);
    _t7_27 = _mm256_add_pd(_t7_27, _t7_39);

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t7_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_3), _mm256_mul_pd(_t7_18, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_1), _mm256_mul_pd(_t7_16, _t7_0)));
    _t7_41 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_3), _mm256_mul_pd(_t7_14, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_1), _mm256_mul_pd(_t7_12, _t7_0)));
    _t7_42 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_3), _mm256_mul_pd(_t7_10, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_1), _mm256_mul_pd(_t7_8, _t7_0)));
    _t7_43 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_3), _mm256_mul_pd(_t7_6, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_1), _mm256_mul_pd(_t7_4, _t7_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t7_28 = _mm256_add_pd(_t7_28, _t7_40);
    _t7_29 = _mm256_add_pd(_t7_29, _t7_41);
    _t7_30 = _mm256_add_pd(_t7_30, _t7_42);
    _t7_31 = _mm256_add_pd(_t7_31, _t7_43);

    // AVX Storer:
    _asm256_storeu_pd(M0 + 12*k2, _t7_20);
    _asm256_storeu_pd(M0 + 12*k2 + 12, _t7_21);
    _asm256_storeu_pd(M0 + 12*k2 + 24, _t7_22);
    _asm256_storeu_pd(M0 + 12*k2 + 36, _t7_23);
    _asm256_storeu_pd(M0 + 12*k2 + 4, _t7_24);
    _asm256_storeu_pd(M0 + 12*k2 + 16, _t7_25);
    _asm256_storeu_pd(M0 + 12*k2 + 28, _t7_26);
    _asm256_storeu_pd(M0 + 12*k2 + 40, _t7_27);
    _asm256_storeu_pd(M0 + 12*k2 + 8, _t7_28);
    _asm256_storeu_pd(M0 + 12*k2 + 20, _t7_29);
    _asm256_storeu_pd(M0 + 12*k2 + 32, _t7_30);
    _asm256_storeu_pd(M0 + 12*k2 + 44, _t7_31);
  }

  _t8_3 = _asm256_loadu_pd(P + 104);
  _t8_2 = _mm256_maskload_pd(P + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t8_1 = _mm256_maskload_pd(P + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t8_0 = _mm256_maskload_pd(P + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t8_4 = _t8_3;
  _t8_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t8_3, _t8_2, 3), _t8_2, 12);
  _t8_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t8_3, _t8_2, 0), _t8_1, 49);
  _t8_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t8_3, _t8_2, 12), _mm256_shuffle_pd(_t8_1, _t8_0, 12), 49);


  for( int k2 = 0; k2 <= 11; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 7; i0+=4 ) {
      _t9_19 = _mm256_broadcast_sd(F + 12*k2 + 8);
      _t9_18 = _mm256_broadcast_sd(F + 12*k2 + 9);
      _t9_17 = _mm256_broadcast_sd(F + 12*k2 + 10);
      _t9_16 = _mm256_broadcast_sd(F + 12*k2 + 11);
      _t9_15 = _mm256_broadcast_sd(F + 12*k2 + 20);
      _t9_14 = _mm256_broadcast_sd(F + 12*k2 + 21);
      _t9_13 = _mm256_broadcast_sd(F + 12*k2 + 22);
      _t9_12 = _mm256_broadcast_sd(F + 12*k2 + 23);
      _t9_11 = _mm256_broadcast_sd(F + 12*k2 + 32);
      _t9_10 = _mm256_broadcast_sd(F + 12*k2 + 33);
      _t9_9 = _mm256_broadcast_sd(F + 12*k2 + 34);
      _t9_8 = _mm256_broadcast_sd(F + 12*k2 + 35);
      _t9_7 = _mm256_broadcast_sd(F + 12*k2 + 44);
      _t9_6 = _mm256_broadcast_sd(F + 12*k2 + 45);
      _t9_5 = _mm256_broadcast_sd(F + 12*k2 + 46);
      _t9_4 = _mm256_broadcast_sd(F + 12*k2 + 47);
      _t9_3 = _asm256_loadu_pd(P + 12*i0 + 8);
      _t9_2 = _asm256_loadu_pd(P + 12*i0 + 20);
      _t9_1 = _asm256_loadu_pd(P + 12*i0 + 32);
      _t9_0 = _asm256_loadu_pd(P + 12*i0 + 44);
      _t9_20 = _asm256_loadu_pd(M0 + i0 + 12*k2);
      _t9_21 = _asm256_loadu_pd(M0 + i0 + 12*k2 + 12);
      _t9_22 = _asm256_loadu_pd(M0 + i0 + 12*k2 + 24);
      _t9_23 = _asm256_loadu_pd(M0 + i0 + 12*k2 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t9_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_3, _t9_2), _mm256_unpacklo_pd(_t9_1, _t9_0), 32);
      _t9_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t9_3, _t9_2), _mm256_unpackhi_pd(_t9_1, _t9_0), 32);
      _t9_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t9_3, _t9_2), _mm256_unpacklo_pd(_t9_1, _t9_0), 49);
      _t9_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t9_3, _t9_2), _mm256_unpackhi_pd(_t9_1, _t9_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t9_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_28), _mm256_mul_pd(_t9_18, _t9_29)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_30), _mm256_mul_pd(_t9_16, _t9_31)));
      _t9_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_28), _mm256_mul_pd(_t9_14, _t9_29)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_30), _mm256_mul_pd(_t9_12, _t9_31)));
      _t9_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_28), _mm256_mul_pd(_t9_10, _t9_29)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_30), _mm256_mul_pd(_t9_8, _t9_31)));
      _t9_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_28), _mm256_mul_pd(_t9_6, _t9_29)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_30), _mm256_mul_pd(_t9_4, _t9_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t9_20 = _mm256_add_pd(_t9_20, _t9_24);
      _t9_21 = _mm256_add_pd(_t9_21, _t9_25);
      _t9_22 = _mm256_add_pd(_t9_22, _t9_26);
      _t9_23 = _mm256_add_pd(_t9_23, _t9_27);

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 12*k2, _t9_20);
      _asm256_storeu_pd(M0 + i0 + 12*k2 + 12, _t9_21);
      _asm256_storeu_pd(M0 + i0 + 12*k2 + 24, _t9_22);
      _asm256_storeu_pd(M0 + i0 + 12*k2 + 36, _t9_23);
    }
    _t10_15 = _mm256_broadcast_sd(F + 12*k2 + 8);
    _t10_14 = _mm256_broadcast_sd(F + 12*k2 + 9);
    _t10_13 = _mm256_broadcast_sd(F + 12*k2 + 10);
    _t10_12 = _mm256_broadcast_sd(F + 12*k2 + 11);
    _t10_11 = _mm256_broadcast_sd(F + 12*k2 + 20);
    _t10_10 = _mm256_broadcast_sd(F + 12*k2 + 21);
    _t10_9 = _mm256_broadcast_sd(F + 12*k2 + 22);
    _t10_8 = _mm256_broadcast_sd(F + 12*k2 + 23);
    _t10_7 = _mm256_broadcast_sd(F + 12*k2 + 32);
    _t10_6 = _mm256_broadcast_sd(F + 12*k2 + 33);
    _t10_5 = _mm256_broadcast_sd(F + 12*k2 + 34);
    _t10_4 = _mm256_broadcast_sd(F + 12*k2 + 35);
    _t10_3 = _mm256_broadcast_sd(F + 12*k2 + 44);
    _t10_2 = _mm256_broadcast_sd(F + 12*k2 + 45);
    _t10_1 = _mm256_broadcast_sd(F + 12*k2 + 46);
    _t10_0 = _mm256_broadcast_sd(F + 12*k2 + 47);
    _t10_16 = _asm256_loadu_pd(M0 + 12*k2 + 8);
    _t10_17 = _asm256_loadu_pd(M0 + 12*k2 + 20);
    _t10_18 = _asm256_loadu_pd(M0 + 12*k2 + 32);
    _t10_19 = _asm256_loadu_pd(M0 + 12*k2 + 44);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t10_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t8_4), _mm256_mul_pd(_t10_14, _t8_5)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t8_6), _mm256_mul_pd(_t10_12, _t8_7)));
    _t10_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t8_4), _mm256_mul_pd(_t10_10, _t8_5)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t8_6), _mm256_mul_pd(_t10_8, _t8_7)));
    _t10_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t8_4), _mm256_mul_pd(_t10_6, _t8_5)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t8_6), _mm256_mul_pd(_t10_4, _t8_7)));
    _t10_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_3, _t8_4), _mm256_mul_pd(_t10_2, _t8_5)), _mm256_add_pd(_mm256_mul_pd(_t10_1, _t8_6), _mm256_mul_pd(_t10_0, _t8_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t10_16 = _mm256_add_pd(_t10_16, _t10_20);
    _t10_17 = _mm256_add_pd(_t10_17, _t10_21);
    _t10_18 = _mm256_add_pd(_t10_18, _t10_22);
    _t10_19 = _mm256_add_pd(_t10_19, _t10_23);

    // AVX Storer:
    _asm256_storeu_pd(M0 + 12*k2 + 8, _t10_16);
    _asm256_storeu_pd(M0 + 12*k2 + 20, _t10_17);
    _asm256_storeu_pd(M0 + 12*k2 + 32, _t10_18);
    _asm256_storeu_pd(M0 + 12*k2 + 44, _t10_19);
  }


  // Generating : Y[12,12] = ( ( Sum_{k2} ( ( S(h(4, 12, k2), ( ( G(h(4, 12, k2), M0[12,12],h(4, 12, 0)) * T( G(h(4, 12, k2), F[12,12],h(4, 12, 0)) ) ) + G(h(4, 12, k2), Q[12,12],h(4, 12, k2)) ),h(4, 12, k2)) + Sum_{i0} ( S(h(4, 12, k2), ( ( G(h(4, 12, k2), M0[12,12],h(4, 12, 0)) * T( G(h(4, 12, i0), F[12,12],h(4, 12, 0)) ) ) + G(h(4, 12, k2), Q[12,12],h(4, 12, i0)) ),h(4, 12, i0)) ) ) ) + S(h(4, 12, 8), ( ( G(h(4, 12, 8), M0[12,12],h(4, 12, 0)) * T( G(h(4, 12, 8), F[12,12],h(4, 12, 0)) ) ) + G(h(4, 12, 8), Q[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 12, k2), ( G(h(4, 12, k2), M0[12,12],h(4, 12, k3)) * T( G(h(4, 12, k2), F[12,12],h(4, 12, k3)) ) ),h(4, 12, k2)) + Sum_{i0} ( $(h(4, 12, k2), ( G(h(4, 12, k2), M0[12,12],h(4, 12, k3)) * T( G(h(4, 12, i0), F[12,12],h(4, 12, k3)) ) ),h(4, 12, i0)) ) ) ) + $(h(4, 12, 8), ( G(h(4, 12, 8), M0[12,12],h(4, 12, k3)) * T( G(h(4, 12, 8), F[12,12],h(4, 12, k3)) ) ),h(4, 12, 8)) ) ) )


  for( int k2 = 0; k2 <= 7; k2+=4 ) {
    _t11_23 = _mm256_broadcast_sd(M0 + 12*k2);
    _t11_22 = _mm256_broadcast_sd(M0 + 12*k2 + 1);
    _t11_21 = _mm256_broadcast_sd(M0 + 12*k2 + 2);
    _t11_20 = _mm256_broadcast_sd(M0 + 12*k2 + 3);
    _t11_19 = _mm256_broadcast_sd(M0 + 12*k2 + 12);
    _t11_18 = _mm256_broadcast_sd(M0 + 12*k2 + 13);
    _t11_17 = _mm256_broadcast_sd(M0 + 12*k2 + 14);
    _t11_16 = _mm256_broadcast_sd(M0 + 12*k2 + 15);
    _t11_15 = _mm256_broadcast_sd(M0 + 12*k2 + 24);
    _t11_14 = _mm256_broadcast_sd(M0 + 12*k2 + 25);
    _t11_13 = _mm256_broadcast_sd(M0 + 12*k2 + 26);
    _t11_12 = _mm256_broadcast_sd(M0 + 12*k2 + 27);
    _t11_11 = _mm256_broadcast_sd(M0 + 12*k2 + 36);
    _t11_10 = _mm256_broadcast_sd(M0 + 12*k2 + 37);
    _t11_9 = _mm256_broadcast_sd(M0 + 12*k2 + 38);
    _t11_8 = _mm256_broadcast_sd(M0 + 12*k2 + 39);
    _t11_7 = _asm256_loadu_pd(F + 12*k2);
    _t11_6 = _asm256_loadu_pd(F + 12*k2 + 12);
    _t11_5 = _asm256_loadu_pd(F + 12*k2 + 24);
    _t11_4 = _asm256_loadu_pd(F + 12*k2 + 36);
    _t11_3 = _asm256_loadu_pd(Q + 13*k2);
    _t11_2 = _mm256_maskload_pd(Q + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t11_1 = _mm256_maskload_pd(Q + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t11_0 = _mm256_maskload_pd(Q + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t11_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_6), _mm256_unpacklo_pd(_t11_5, _t11_4), 32);
    _t11_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_7, _t11_6), _mm256_unpackhi_pd(_t11_5, _t11_4), 32);
    _t11_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_7, _t11_6), _mm256_unpacklo_pd(_t11_5, _t11_4), 49);
    _t11_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_7, _t11_6), _mm256_unpackhi_pd(_t11_5, _t11_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t11_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_23, _t11_40), _mm256_mul_pd(_t11_22, _t11_41)), _mm256_add_pd(_mm256_mul_pd(_t11_21, _t11_42), _mm256_mul_pd(_t11_20, _t11_43)));
    _t11_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_40), _mm256_mul_pd(_t11_18, _t11_41)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_42), _mm256_mul_pd(_t11_16, _t11_43)));
    _t11_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_40), _mm256_mul_pd(_t11_14, _t11_41)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_42), _mm256_mul_pd(_t11_12, _t11_43)));
    _t11_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_40), _mm256_mul_pd(_t11_10, _t11_41)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_42), _mm256_mul_pd(_t11_8, _t11_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t11_36 = _t11_3;
    _t11_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t11_3, _t11_2, 3), _t11_2, 12);
    _t11_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t11_3, _t11_2, 0), _t11_1, 49);
    _t11_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t11_3, _t11_2, 12), _mm256_shuffle_pd(_t11_1, _t11_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t11_24 = _mm256_add_pd(_t11_32, _t11_36);
    _t11_25 = _mm256_add_pd(_t11_33, _t11_37);
    _t11_26 = _mm256_add_pd(_t11_34, _t11_38);
    _t11_27 = _mm256_add_pd(_t11_35, _t11_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t11_28 = _t11_24;
    _t11_29 = _t11_25;
    _t11_30 = _t11_26;
    _t11_31 = _t11_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 11; i0+=4 ) {
      _t12_7 = _asm256_loadu_pd(F + 12*i0);
      _t12_6 = _asm256_loadu_pd(F + 12*i0 + 12);
      _t12_5 = _asm256_loadu_pd(F + 12*i0 + 24);
      _t12_4 = _asm256_loadu_pd(F + 12*i0 + 36);
      _t12_3 = _asm256_loadu_pd(Q + i0 + 12*k2);
      _t12_2 = _asm256_loadu_pd(Q + i0 + 12*k2 + 12);
      _t12_1 = _asm256_loadu_pd(Q + i0 + 12*k2 + 24);
      _t12_0 = _asm256_loadu_pd(Q + i0 + 12*k2 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t12_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_6), _mm256_unpacklo_pd(_t12_5, _t12_4), 32);
      _t12_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_7, _t12_6), _mm256_unpackhi_pd(_t12_5, _t12_4), 32);
      _t12_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t12_7, _t12_6), _mm256_unpacklo_pd(_t12_5, _t12_4), 49);
      _t12_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t12_7, _t12_6), _mm256_unpackhi_pd(_t12_5, _t12_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t12_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_23, _t12_16), _mm256_mul_pd(_t11_22, _t12_17)), _mm256_add_pd(_mm256_mul_pd(_t11_21, _t12_18), _mm256_mul_pd(_t11_20, _t12_19)));
      _t12_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t12_16), _mm256_mul_pd(_t11_18, _t12_17)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t12_18), _mm256_mul_pd(_t11_16, _t12_19)));
      _t12_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t12_16), _mm256_mul_pd(_t11_14, _t12_17)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t12_18), _mm256_mul_pd(_t11_12, _t12_19)));
      _t12_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t12_16), _mm256_mul_pd(_t11_10, _t12_17)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t12_18), _mm256_mul_pd(_t11_8, _t12_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t12_8 = _mm256_add_pd(_t12_12, _t12_3);
      _t12_9 = _mm256_add_pd(_t12_13, _t12_2);
      _t12_10 = _mm256_add_pd(_t12_14, _t12_1);
      _t12_11 = _mm256_add_pd(_t12_15, _t12_0);

      // AVX Storer:
      _asm256_storeu_pd(Y + i0 + 12*k2, _t12_8);
      _asm256_storeu_pd(Y + i0 + 12*k2 + 12, _t12_9);
      _asm256_storeu_pd(Y + i0 + 12*k2 + 24, _t12_10);
      _asm256_storeu_pd(Y + i0 + 12*k2 + 36, _t12_11);
    }
    _asm256_storeu_pd(Y + 13*k2, _t11_28);
    _mm256_maskstore_pd(Y + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t11_29);
    _mm256_maskstore_pd(Y + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t11_30);
    _mm256_maskstore_pd(Y + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t11_31);
  }

  _t13_23 = _mm256_broadcast_sd(M0 + 96);
  _t13_22 = _mm256_broadcast_sd(M0 + 97);
  _t13_21 = _mm256_broadcast_sd(M0 + 98);
  _t13_20 = _mm256_broadcast_sd(M0 + 99);
  _t13_19 = _mm256_broadcast_sd(M0 + 108);
  _t13_18 = _mm256_broadcast_sd(M0 + 109);
  _t13_17 = _mm256_broadcast_sd(M0 + 110);
  _t13_16 = _mm256_broadcast_sd(M0 + 111);
  _t13_15 = _mm256_broadcast_sd(M0 + 120);
  _t13_14 = _mm256_broadcast_sd(M0 + 121);
  _t13_13 = _mm256_broadcast_sd(M0 + 122);
  _t13_12 = _mm256_broadcast_sd(M0 + 123);
  _t13_11 = _mm256_broadcast_sd(M0 + 132);
  _t13_10 = _mm256_broadcast_sd(M0 + 133);
  _t13_9 = _mm256_broadcast_sd(M0 + 134);
  _t13_8 = _mm256_broadcast_sd(M0 + 135);
  _t13_7 = _asm256_loadu_pd(F + 96);
  _t13_6 = _asm256_loadu_pd(F + 108);
  _t13_5 = _asm256_loadu_pd(F + 120);
  _t13_4 = _asm256_loadu_pd(F + 132);
  _t13_3 = _asm256_loadu_pd(Q + 104);
  _t13_2 = _mm256_maskload_pd(Q + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t13_1 = _mm256_maskload_pd(Q + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t13_0 = _mm256_maskload_pd(Q + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t13_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 32);
  _t13_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 32);
  _t13_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 49);
  _t13_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t13_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t13_40), _mm256_mul_pd(_t13_22, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t13_42), _mm256_mul_pd(_t13_20, _t13_43)));
  _t13_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_40), _mm256_mul_pd(_t13_18, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_42), _mm256_mul_pd(_t13_16, _t13_43)));
  _t13_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_40), _mm256_mul_pd(_t13_14, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_42), _mm256_mul_pd(_t13_12, _t13_43)));
  _t13_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_40), _mm256_mul_pd(_t13_10, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_42), _mm256_mul_pd(_t13_8, _t13_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t13_36 = _t13_3;
  _t13_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 3), _t13_2, 12);
  _t13_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 0), _t13_1, 49);
  _t13_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 12), _mm256_shuffle_pd(_t13_1, _t13_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t13_24 = _mm256_add_pd(_t13_32, _t13_36);
  _t13_25 = _mm256_add_pd(_t13_33, _t13_37);
  _t13_26 = _mm256_add_pd(_t13_34, _t13_38);
  _t13_27 = _mm256_add_pd(_t13_35, _t13_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t13_28 = _t13_24;
  _t13_29 = _t13_25;
  _t13_30 = _t13_26;
  _t13_31 = _t13_27;


  for( int k3 = 4; k3 <= 11; k3+=4 ) {

    for( int k2 = 0; k2 <= 7; k2+=4 ) {
      _t14_19 = _mm256_broadcast_sd(M0 + 12*k2 + k3);
      _t14_18 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 1);
      _t14_17 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 2);
      _t14_16 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 3);
      _t14_15 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 12);
      _t14_14 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 13);
      _t14_13 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 14);
      _t14_12 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 15);
      _t14_11 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 24);
      _t14_10 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 25);
      _t14_9 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 26);
      _t14_8 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 27);
      _t14_7 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 36);
      _t14_6 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 37);
      _t14_5 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 38);
      _t14_4 = _mm256_broadcast_sd(M0 + 12*k2 + k3 + 39);
      _t14_3 = _asm256_loadu_pd(F + 12*k2 + k3);
      _t14_2 = _asm256_loadu_pd(F + 12*k2 + k3 + 12);
      _t14_1 = _asm256_loadu_pd(F + 12*k2 + k3 + 24);
      _t14_0 = _asm256_loadu_pd(F + 12*k2 + k3 + 36);
      _t14_20 = _asm256_loadu_pd(Y + 13*k2);
      _t14_21 = _mm256_maskload_pd(Y + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t14_22 = _mm256_maskload_pd(Y + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t14_23 = _mm256_maskload_pd(Y + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t14_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_3, _t14_2), _mm256_unpacklo_pd(_t14_1, _t14_0), 32);
      _t14_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_3, _t14_2), _mm256_unpackhi_pd(_t14_1, _t14_0), 32);
      _t14_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_3, _t14_2), _mm256_unpacklo_pd(_t14_1, _t14_0), 49);
      _t14_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_3, _t14_2), _mm256_unpackhi_pd(_t14_1, _t14_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t14_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_19, _t14_32), _mm256_mul_pd(_t14_18, _t14_33)), _mm256_add_pd(_mm256_mul_pd(_t14_17, _t14_34), _mm256_mul_pd(_t14_16, _t14_35)));
      _t14_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t14_32), _mm256_mul_pd(_t14_14, _t14_33)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t14_34), _mm256_mul_pd(_t14_12, _t14_35)));
      _t14_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t14_32), _mm256_mul_pd(_t14_10, _t14_33)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t14_34), _mm256_mul_pd(_t14_8, _t14_35)));
      _t14_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_7, _t14_32), _mm256_mul_pd(_t14_6, _t14_33)), _mm256_add_pd(_mm256_mul_pd(_t14_5, _t14_34), _mm256_mul_pd(_t14_4, _t14_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t14_28 = _t14_20;
      _t14_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t14_20, _t14_21, 3), _t14_21, 12);
      _t14_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_20, _t14_21, 0), _t14_22, 49);
      _t14_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_20, _t14_21, 12), _mm256_shuffle_pd(_t14_22, _t14_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t14_28 = _mm256_add_pd(_t14_28, _t14_24);
      _t14_29 = _mm256_add_pd(_t14_29, _t14_25);
      _t14_30 = _mm256_add_pd(_t14_30, _t14_26);
      _t14_31 = _mm256_add_pd(_t14_31, _t14_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t14_20 = _t14_28;
      _t14_21 = _t14_29;
      _t14_22 = _t14_30;
      _t14_23 = _t14_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 11; i0+=4 ) {
        _t15_3 = _asm256_loadu_pd(F + 12*i0 + k3);
        _t15_2 = _asm256_loadu_pd(F + 12*i0 + k3 + 12);
        _t15_1 = _asm256_loadu_pd(F + 12*i0 + k3 + 24);
        _t15_0 = _asm256_loadu_pd(F + 12*i0 + k3 + 36);
        _t15_4 = _asm256_loadu_pd(Y + i0 + 12*k2);
        _t15_5 = _asm256_loadu_pd(Y + i0 + 12*k2 + 12);
        _t15_6 = _asm256_loadu_pd(Y + i0 + 12*k2 + 24);
        _t15_7 = _asm256_loadu_pd(Y + i0 + 12*k2 + 36);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t15_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 32);
        _t15_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 32);
        _t15_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 49);
        _t15_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t15_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_19, _t15_12), _mm256_mul_pd(_t14_18, _t15_13)), _mm256_add_pd(_mm256_mul_pd(_t14_17, _t15_14), _mm256_mul_pd(_t14_16, _t15_15)));
        _t15_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t15_12), _mm256_mul_pd(_t14_14, _t15_13)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t15_14), _mm256_mul_pd(_t14_12, _t15_15)));
        _t15_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t15_12), _mm256_mul_pd(_t14_10, _t15_13)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t15_14), _mm256_mul_pd(_t14_8, _t15_15)));
        _t15_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_7, _t15_12), _mm256_mul_pd(_t14_6, _t15_13)), _mm256_add_pd(_mm256_mul_pd(_t14_5, _t15_14), _mm256_mul_pd(_t14_4, _t15_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t15_4 = _mm256_add_pd(_t15_4, _t15_8);
        _t15_5 = _mm256_add_pd(_t15_5, _t15_9);
        _t15_6 = _mm256_add_pd(_t15_6, _t15_10);
        _t15_7 = _mm256_add_pd(_t15_7, _t15_11);

        // AVX Storer:
        _asm256_storeu_pd(Y + i0 + 12*k2, _t15_4);
        _asm256_storeu_pd(Y + i0 + 12*k2 + 12, _t15_5);
        _asm256_storeu_pd(Y + i0 + 12*k2 + 24, _t15_6);
        _asm256_storeu_pd(Y + i0 + 12*k2 + 36, _t15_7);
      }
      _asm256_storeu_pd(Y + 13*k2, _t14_20);
      _mm256_maskstore_pd(Y + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t14_21);
      _mm256_maskstore_pd(Y + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t14_22);
      _mm256_maskstore_pd(Y + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t14_23);
    }
    _t16_19 = _mm256_broadcast_sd(M0 + k3 + 96);
    _t16_18 = _mm256_broadcast_sd(M0 + k3 + 97);
    _t16_17 = _mm256_broadcast_sd(M0 + k3 + 98);
    _t16_16 = _mm256_broadcast_sd(M0 + k3 + 99);
    _t16_15 = _mm256_broadcast_sd(M0 + k3 + 108);
    _t16_14 = _mm256_broadcast_sd(M0 + k3 + 109);
    _t16_13 = _mm256_broadcast_sd(M0 + k3 + 110);
    _t16_12 = _mm256_broadcast_sd(M0 + k3 + 111);
    _t16_11 = _mm256_broadcast_sd(M0 + k3 + 120);
    _t16_10 = _mm256_broadcast_sd(M0 + k3 + 121);
    _t16_9 = _mm256_broadcast_sd(M0 + k3 + 122);
    _t16_8 = _mm256_broadcast_sd(M0 + k3 + 123);
    _t16_7 = _mm256_broadcast_sd(M0 + k3 + 132);
    _t16_6 = _mm256_broadcast_sd(M0 + k3 + 133);
    _t16_5 = _mm256_broadcast_sd(M0 + k3 + 134);
    _t16_4 = _mm256_broadcast_sd(M0 + k3 + 135);
    _t16_3 = _asm256_loadu_pd(F + k3 + 96);
    _t16_2 = _asm256_loadu_pd(F + k3 + 108);
    _t16_1 = _asm256_loadu_pd(F + k3 + 120);
    _t16_0 = _asm256_loadu_pd(F + k3 + 132);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t16_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
    _t16_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
    _t16_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
    _t16_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t16_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_28), _mm256_mul_pd(_t16_18, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_30), _mm256_mul_pd(_t16_16, _t16_31)));
    _t16_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_28), _mm256_mul_pd(_t16_14, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_30), _mm256_mul_pd(_t16_12, _t16_31)));
    _t16_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_28), _mm256_mul_pd(_t16_10, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_30), _mm256_mul_pd(_t16_8, _t16_31)));
    _t16_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_28), _mm256_mul_pd(_t16_6, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_30), _mm256_mul_pd(_t16_4, _t16_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t16_24 = _t13_28;
    _t16_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 3), _t13_29, 12);
    _t16_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 0), _t13_30, 49);
    _t16_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 12), _mm256_shuffle_pd(_t13_30, _t13_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t16_24 = _mm256_add_pd(_t16_24, _t16_20);
    _t16_25 = _mm256_add_pd(_t16_25, _t16_21);
    _t16_26 = _mm256_add_pd(_t16_26, _t16_22);
    _t16_27 = _mm256_add_pd(_t16_27, _t16_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t13_28 = _t16_24;
    _t13_29 = _t16_25;
    _t13_30 = _t16_26;
    _t13_31 = _t16_27;
  }


  // Generating : v0[12,1] = ( Sum_{k2} ( S(h(4, 12, k2), ( G(h(4, 12, k2), z[12,1],h(1, 1, 0)) - ( G(h(4, 12, k2), H[12,12],h(4, 12, 0)) * G(h(4, 12, 0), y[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, k3)) * G(h(4, 12, k3), y[12,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t17_5 = _asm256_loadu_pd(z + k2);
    _t17_4 = _asm256_loadu_pd(H + 12*k2);
    _t17_3 = _asm256_loadu_pd(H + 12*k2 + 12);
    _t17_2 = _asm256_loadu_pd(H + 12*k2 + 24);
    _t17_1 = _asm256_loadu_pd(H + 12*k2 + 36);
    _t17_0 = _asm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t17_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t17_4, _t17_0), _mm256_mul_pd(_t17_3, _t17_0)), _mm256_hadd_pd(_mm256_mul_pd(_t17_2, _t17_0), _mm256_mul_pd(_t17_1, _t17_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t17_4, _t17_0), _mm256_mul_pd(_t17_3, _t17_0)), _mm256_hadd_pd(_mm256_mul_pd(_t17_2, _t17_0), _mm256_mul_pd(_t17_1, _t17_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t17_7 = _mm256_sub_pd(_t17_5, _t17_6);

    // AVX Storer:
    _asm256_storeu_pd(v0 + k2, _t17_7);
  }


  for( int k3 = 4; k3 <= 11; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 11; k2+=4 ) {
      _t18_4 = _asm256_loadu_pd(H + 12*k2 + k3);
      _t18_3 = _asm256_loadu_pd(H + 12*k2 + k3 + 12);
      _t18_2 = _asm256_loadu_pd(H + 12*k2 + k3 + 24);
      _t18_1 = _asm256_loadu_pd(H + 12*k2 + k3 + 36);
      _t18_0 = _asm256_loadu_pd(y + k3);
      _t18_5 = _asm256_loadu_pd(v0 + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t18_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t18_4, _t18_0), _mm256_mul_pd(_t18_3, _t18_0)), _mm256_hadd_pd(_mm256_mul_pd(_t18_2, _t18_0), _mm256_mul_pd(_t18_1, _t18_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t18_4, _t18_0), _mm256_mul_pd(_t18_3, _t18_0)), _mm256_hadd_pd(_mm256_mul_pd(_t18_2, _t18_0), _mm256_mul_pd(_t18_1, _t18_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t18_5 = _mm256_sub_pd(_t18_5, _t18_6);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k2, _t18_5);
    }
  }

  _t19_3 = _asm256_loadu_pd(Y);
  _t19_2 = _mm256_maskload_pd(Y + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t19_1 = _mm256_maskload_pd(Y + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t19_0 = _mm256_maskload_pd(Y + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[12,12] = ( ( Sum_{k2} ( ( S(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 0)) * G(h(4, 12, 0), Y[12,12],h(4, 12, 0)) ),h(4, 12, 0)) + Sum_{i0} ( S(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 0)) * G(h(4, 12, 0), Y[12,12],h(4, 12, i0)) ),h(4, 12, i0)) ) ) ) + Sum_{k2} ( ( ( $(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 4)) * T( G(h(4, 12, 0), Y[12,12],h(4, 12, 4)) ) ),h(4, 12, 0)) + $(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 4)) * G(h(4, 12, 4), Y[12,12],h(4, 12, 4)) ),h(4, 12, 4)) ) + $(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 4)) * G(h(4, 12, 4), Y[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 8)) * T( G(h(4, 12, i0), Y[12,12],h(4, 12, 8)) ) ),h(4, 12, i0)) ) + $(h(4, 12, k2), ( G(h(4, 12, k2), H[12,12],h(4, 12, 8)) * G(h(4, 12, 8), Y[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t19_4 = _t19_3;
  _t19_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 3), _t19_2, 12);
  _t19_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 0), _t19_1, 49);
  _t19_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 12), _mm256_shuffle_pd(_t19_1, _t19_0, 12), 49);


  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t20_15 = _mm256_broadcast_sd(H + 12*k2);
    _t20_14 = _mm256_broadcast_sd(H + 12*k2 + 1);
    _t20_13 = _mm256_broadcast_sd(H + 12*k2 + 2);
    _t20_12 = _mm256_broadcast_sd(H + 12*k2 + 3);
    _t20_11 = _mm256_broadcast_sd(H + 12*k2 + 12);
    _t20_10 = _mm256_broadcast_sd(H + 12*k2 + 13);
    _t20_9 = _mm256_broadcast_sd(H + 12*k2 + 14);
    _t20_8 = _mm256_broadcast_sd(H + 12*k2 + 15);
    _t20_7 = _mm256_broadcast_sd(H + 12*k2 + 24);
    _t20_6 = _mm256_broadcast_sd(H + 12*k2 + 25);
    _t20_5 = _mm256_broadcast_sd(H + 12*k2 + 26);
    _t20_4 = _mm256_broadcast_sd(H + 12*k2 + 27);
    _t20_3 = _mm256_broadcast_sd(H + 12*k2 + 36);
    _t20_2 = _mm256_broadcast_sd(H + 12*k2 + 37);
    _t20_1 = _mm256_broadcast_sd(H + 12*k2 + 38);
    _t20_0 = _mm256_broadcast_sd(H + 12*k2 + 39);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t20_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t19_4), _mm256_mul_pd(_t20_14, _t19_5)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t19_6), _mm256_mul_pd(_t20_12, _t19_7)));
    _t20_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t19_4), _mm256_mul_pd(_t20_10, _t19_5)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t19_6), _mm256_mul_pd(_t20_8, _t19_7)));
    _t20_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t19_4), _mm256_mul_pd(_t20_6, _t19_5)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t19_6), _mm256_mul_pd(_t20_4, _t19_7)));
    _t20_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t19_4), _mm256_mul_pd(_t20_2, _t19_5)), _mm256_add_pd(_mm256_mul_pd(_t20_1, _t19_6), _mm256_mul_pd(_t20_0, _t19_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 11; i0+=4 ) {
      _t21_3 = _asm256_loadu_pd(Y + i0);
      _t21_2 = _asm256_loadu_pd(Y + i0 + 12);
      _t21_1 = _asm256_loadu_pd(Y + i0 + 24);
      _t21_0 = _asm256_loadu_pd(Y + i0 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t21_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t21_3), _mm256_mul_pd(_t20_14, _t21_2)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t21_1), _mm256_mul_pd(_t20_12, _t21_0)));
      _t21_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t21_3), _mm256_mul_pd(_t20_10, _t21_2)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t21_1), _mm256_mul_pd(_t20_8, _t21_0)));
      _t21_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t21_3), _mm256_mul_pd(_t20_6, _t21_2)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t21_1), _mm256_mul_pd(_t20_4, _t21_0)));
      _t21_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_3, _t21_3), _mm256_mul_pd(_t20_2, _t21_2)), _mm256_add_pd(_mm256_mul_pd(_t20_1, _t21_1), _mm256_mul_pd(_t20_0, _t21_0)));

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 12*k2, _t21_4);
      _asm256_storeu_pd(M1 + i0 + 12*k2 + 12, _t21_5);
      _asm256_storeu_pd(M1 + i0 + 12*k2 + 24, _t21_6);
      _asm256_storeu_pd(M1 + i0 + 12*k2 + 36, _t21_7);
    }
    _asm256_storeu_pd(M1 + 12*k2, _t20_16);
    _asm256_storeu_pd(M1 + 12*k2 + 12, _t20_17);
    _asm256_storeu_pd(M1 + 12*k2 + 24, _t20_18);
    _asm256_storeu_pd(M1 + 12*k2 + 36, _t20_19);
  }

  _t22_7 = _asm256_loadu_pd(Y + 4);
  _t22_6 = _asm256_loadu_pd(Y + 16);
  _t22_5 = _asm256_loadu_pd(Y + 28);
  _t22_4 = _asm256_loadu_pd(Y + 40);
  _t22_3 = _asm256_loadu_pd(Y + 52);
  _t22_2 = _mm256_maskload_pd(Y + 64, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t22_1 = _mm256_maskload_pd(Y + 76, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t22_0 = _mm256_maskload_pd(Y + 88, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t22_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_7, _t22_6), _mm256_unpacklo_pd(_t22_5, _t22_4), 32);
  _t22_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t22_7, _t22_6), _mm256_unpackhi_pd(_t22_5, _t22_4), 32);
  _t22_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t22_7, _t22_6), _mm256_unpacklo_pd(_t22_5, _t22_4), 49);
  _t22_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t22_7, _t22_6), _mm256_unpackhi_pd(_t22_5, _t22_4), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t22_8 = _t22_3;
  _t22_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 3), _t22_2, 12);
  _t22_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 0), _t22_1, 49);
  _t22_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 12), _mm256_shuffle_pd(_t22_1, _t22_0, 12), 49);

  // AVX Loader:


  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t23_19 = _mm256_broadcast_sd(H + 12*k2 + 4);
    _t23_18 = _mm256_broadcast_sd(H + 12*k2 + 5);
    _t23_17 = _mm256_broadcast_sd(H + 12*k2 + 6);
    _t23_16 = _mm256_broadcast_sd(H + 12*k2 + 7);
    _t23_15 = _mm256_broadcast_sd(H + 12*k2 + 16);
    _t23_14 = _mm256_broadcast_sd(H + 12*k2 + 17);
    _t23_13 = _mm256_broadcast_sd(H + 12*k2 + 18);
    _t23_12 = _mm256_broadcast_sd(H + 12*k2 + 19);
    _t23_11 = _mm256_broadcast_sd(H + 12*k2 + 28);
    _t23_10 = _mm256_broadcast_sd(H + 12*k2 + 29);
    _t23_9 = _mm256_broadcast_sd(H + 12*k2 + 30);
    _t23_8 = _mm256_broadcast_sd(H + 12*k2 + 31);
    _t23_7 = _mm256_broadcast_sd(H + 12*k2 + 40);
    _t23_6 = _mm256_broadcast_sd(H + 12*k2 + 41);
    _t23_5 = _mm256_broadcast_sd(H + 12*k2 + 42);
    _t23_4 = _mm256_broadcast_sd(H + 12*k2 + 43);
    _t23_3 = _asm256_loadu_pd(Y + 56);
    _t23_2 = _asm256_loadu_pd(Y + 68);
    _t23_1 = _asm256_loadu_pd(Y + 80);
    _t23_0 = _asm256_loadu_pd(Y + 92);
    _t23_20 = _asm256_loadu_pd(M1 + 12*k2);
    _t23_21 = _asm256_loadu_pd(M1 + 12*k2 + 12);
    _t23_22 = _asm256_loadu_pd(M1 + 12*k2 + 24);
    _t23_23 = _asm256_loadu_pd(M1 + 12*k2 + 36);
    _t23_24 = _asm256_loadu_pd(M1 + 12*k2 + 4);
    _t23_25 = _asm256_loadu_pd(M1 + 12*k2 + 16);
    _t23_26 = _asm256_loadu_pd(M1 + 12*k2 + 28);
    _t23_27 = _asm256_loadu_pd(M1 + 12*k2 + 40);
    _t23_28 = _asm256_loadu_pd(M1 + 12*k2 + 8);
    _t23_29 = _asm256_loadu_pd(M1 + 12*k2 + 20);
    _t23_30 = _asm256_loadu_pd(M1 + 12*k2 + 32);
    _t23_31 = _asm256_loadu_pd(M1 + 12*k2 + 44);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t23_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_19, _t22_12), _mm256_mul_pd(_t23_18, _t22_13)), _mm256_add_pd(_mm256_mul_pd(_t23_17, _t22_14), _mm256_mul_pd(_t23_16, _t22_15)));
    _t23_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_15, _t22_12), _mm256_mul_pd(_t23_14, _t22_13)), _mm256_add_pd(_mm256_mul_pd(_t23_13, _t22_14), _mm256_mul_pd(_t23_12, _t22_15)));
    _t23_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_11, _t22_12), _mm256_mul_pd(_t23_10, _t22_13)), _mm256_add_pd(_mm256_mul_pd(_t23_9, _t22_14), _mm256_mul_pd(_t23_8, _t22_15)));
    _t23_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_7, _t22_12), _mm256_mul_pd(_t23_6, _t22_13)), _mm256_add_pd(_mm256_mul_pd(_t23_5, _t22_14), _mm256_mul_pd(_t23_4, _t22_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t23_20 = _mm256_add_pd(_t23_20, _t23_32);
    _t23_21 = _mm256_add_pd(_t23_21, _t23_33);
    _t23_22 = _mm256_add_pd(_t23_22, _t23_34);
    _t23_23 = _mm256_add_pd(_t23_23, _t23_35);

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t23_36 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_19, _t22_8), _mm256_mul_pd(_t23_18, _t22_9)), _mm256_add_pd(_mm256_mul_pd(_t23_17, _t22_10), _mm256_mul_pd(_t23_16, _t22_11)));
    _t23_37 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_15, _t22_8), _mm256_mul_pd(_t23_14, _t22_9)), _mm256_add_pd(_mm256_mul_pd(_t23_13, _t22_10), _mm256_mul_pd(_t23_12, _t22_11)));
    _t23_38 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_11, _t22_8), _mm256_mul_pd(_t23_10, _t22_9)), _mm256_add_pd(_mm256_mul_pd(_t23_9, _t22_10), _mm256_mul_pd(_t23_8, _t22_11)));
    _t23_39 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_7, _t22_8), _mm256_mul_pd(_t23_6, _t22_9)), _mm256_add_pd(_mm256_mul_pd(_t23_5, _t22_10), _mm256_mul_pd(_t23_4, _t22_11)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t23_24 = _mm256_add_pd(_t23_24, _t23_36);
    _t23_25 = _mm256_add_pd(_t23_25, _t23_37);
    _t23_26 = _mm256_add_pd(_t23_26, _t23_38);
    _t23_27 = _mm256_add_pd(_t23_27, _t23_39);

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t23_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_19, _t23_3), _mm256_mul_pd(_t23_18, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t23_17, _t23_1), _mm256_mul_pd(_t23_16, _t23_0)));
    _t23_41 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_15, _t23_3), _mm256_mul_pd(_t23_14, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t23_13, _t23_1), _mm256_mul_pd(_t23_12, _t23_0)));
    _t23_42 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_11, _t23_3), _mm256_mul_pd(_t23_10, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t23_9, _t23_1), _mm256_mul_pd(_t23_8, _t23_0)));
    _t23_43 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_7, _t23_3), _mm256_mul_pd(_t23_6, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t23_5, _t23_1), _mm256_mul_pd(_t23_4, _t23_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t23_28 = _mm256_add_pd(_t23_28, _t23_40);
    _t23_29 = _mm256_add_pd(_t23_29, _t23_41);
    _t23_30 = _mm256_add_pd(_t23_30, _t23_42);
    _t23_31 = _mm256_add_pd(_t23_31, _t23_43);

    // AVX Storer:
    _asm256_storeu_pd(M1 + 12*k2, _t23_20);
    _asm256_storeu_pd(M1 + 12*k2 + 12, _t23_21);
    _asm256_storeu_pd(M1 + 12*k2 + 24, _t23_22);
    _asm256_storeu_pd(M1 + 12*k2 + 36, _t23_23);
    _asm256_storeu_pd(M1 + 12*k2 + 4, _t23_24);
    _asm256_storeu_pd(M1 + 12*k2 + 16, _t23_25);
    _asm256_storeu_pd(M1 + 12*k2 + 28, _t23_26);
    _asm256_storeu_pd(M1 + 12*k2 + 40, _t23_27);
    _asm256_storeu_pd(M1 + 12*k2 + 8, _t23_28);
    _asm256_storeu_pd(M1 + 12*k2 + 20, _t23_29);
    _asm256_storeu_pd(M1 + 12*k2 + 32, _t23_30);
    _asm256_storeu_pd(M1 + 12*k2 + 44, _t23_31);
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t24_0 = _t13_28;
  _t24_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 3), _t13_29, 12);
  _t24_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 0), _t13_30, 49);
  _t24_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 12), _mm256_shuffle_pd(_t13_30, _t13_31, 12), 49);


  for( int k2 = 0; k2 <= 11; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 7; i0+=4 ) {
      _t25_19 = _mm256_broadcast_sd(H + 12*k2 + 8);
      _t25_18 = _mm256_broadcast_sd(H + 12*k2 + 9);
      _t25_17 = _mm256_broadcast_sd(H + 12*k2 + 10);
      _t25_16 = _mm256_broadcast_sd(H + 12*k2 + 11);
      _t25_15 = _mm256_broadcast_sd(H + 12*k2 + 20);
      _t25_14 = _mm256_broadcast_sd(H + 12*k2 + 21);
      _t25_13 = _mm256_broadcast_sd(H + 12*k2 + 22);
      _t25_12 = _mm256_broadcast_sd(H + 12*k2 + 23);
      _t25_11 = _mm256_broadcast_sd(H + 12*k2 + 32);
      _t25_10 = _mm256_broadcast_sd(H + 12*k2 + 33);
      _t25_9 = _mm256_broadcast_sd(H + 12*k2 + 34);
      _t25_8 = _mm256_broadcast_sd(H + 12*k2 + 35);
      _t25_7 = _mm256_broadcast_sd(H + 12*k2 + 44);
      _t25_6 = _mm256_broadcast_sd(H + 12*k2 + 45);
      _t25_5 = _mm256_broadcast_sd(H + 12*k2 + 46);
      _t25_4 = _mm256_broadcast_sd(H + 12*k2 + 47);
      _t25_3 = _asm256_loadu_pd(Y + 12*i0 + 8);
      _t25_2 = _asm256_loadu_pd(Y + 12*i0 + 20);
      _t25_1 = _asm256_loadu_pd(Y + 12*i0 + 32);
      _t25_0 = _asm256_loadu_pd(Y + 12*i0 + 44);
      _t25_20 = _asm256_loadu_pd(M1 + i0 + 12*k2);
      _t25_21 = _asm256_loadu_pd(M1 + i0 + 12*k2 + 12);
      _t25_22 = _asm256_loadu_pd(M1 + i0 + 12*k2 + 24);
      _t25_23 = _asm256_loadu_pd(M1 + i0 + 12*k2 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t25_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
      _t25_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
      _t25_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
      _t25_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t25_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_28), _mm256_mul_pd(_t25_18, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_30), _mm256_mul_pd(_t25_16, _t25_31)));
      _t25_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_28), _mm256_mul_pd(_t25_14, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_30), _mm256_mul_pd(_t25_12, _t25_31)));
      _t25_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_28), _mm256_mul_pd(_t25_10, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_30), _mm256_mul_pd(_t25_8, _t25_31)));
      _t25_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_28), _mm256_mul_pd(_t25_6, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_30), _mm256_mul_pd(_t25_4, _t25_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t25_20 = _mm256_add_pd(_t25_20, _t25_24);
      _t25_21 = _mm256_add_pd(_t25_21, _t25_25);
      _t25_22 = _mm256_add_pd(_t25_22, _t25_26);
      _t25_23 = _mm256_add_pd(_t25_23, _t25_27);

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 12*k2, _t25_20);
      _asm256_storeu_pd(M1 + i0 + 12*k2 + 12, _t25_21);
      _asm256_storeu_pd(M1 + i0 + 12*k2 + 24, _t25_22);
      _asm256_storeu_pd(M1 + i0 + 12*k2 + 36, _t25_23);
    }
    _t26_15 = _mm256_broadcast_sd(H + 12*k2 + 8);
    _t26_14 = _mm256_broadcast_sd(H + 12*k2 + 9);
    _t26_13 = _mm256_broadcast_sd(H + 12*k2 + 10);
    _t26_12 = _mm256_broadcast_sd(H + 12*k2 + 11);
    _t26_11 = _mm256_broadcast_sd(H + 12*k2 + 20);
    _t26_10 = _mm256_broadcast_sd(H + 12*k2 + 21);
    _t26_9 = _mm256_broadcast_sd(H + 12*k2 + 22);
    _t26_8 = _mm256_broadcast_sd(H + 12*k2 + 23);
    _t26_7 = _mm256_broadcast_sd(H + 12*k2 + 32);
    _t26_6 = _mm256_broadcast_sd(H + 12*k2 + 33);
    _t26_5 = _mm256_broadcast_sd(H + 12*k2 + 34);
    _t26_4 = _mm256_broadcast_sd(H + 12*k2 + 35);
    _t26_3 = _mm256_broadcast_sd(H + 12*k2 + 44);
    _t26_2 = _mm256_broadcast_sd(H + 12*k2 + 45);
    _t26_1 = _mm256_broadcast_sd(H + 12*k2 + 46);
    _t26_0 = _mm256_broadcast_sd(H + 12*k2 + 47);
    _t26_16 = _asm256_loadu_pd(M1 + 12*k2 + 8);
    _t26_17 = _asm256_loadu_pd(M1 + 12*k2 + 20);
    _t26_18 = _asm256_loadu_pd(M1 + 12*k2 + 32);
    _t26_19 = _asm256_loadu_pd(M1 + 12*k2 + 44);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t24_0), _mm256_mul_pd(_t26_14, _t24_1)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t24_2), _mm256_mul_pd(_t26_12, _t24_3)));
    _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t24_0), _mm256_mul_pd(_t26_10, _t24_1)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t24_2), _mm256_mul_pd(_t26_8, _t24_3)));
    _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t24_0), _mm256_mul_pd(_t26_6, _t24_1)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t24_2), _mm256_mul_pd(_t26_4, _t24_3)));
    _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t24_0), _mm256_mul_pd(_t26_2, _t24_1)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t24_2), _mm256_mul_pd(_t26_0, _t24_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t26_16 = _mm256_add_pd(_t26_16, _t26_20);
    _t26_17 = _mm256_add_pd(_t26_17, _t26_21);
    _t26_18 = _mm256_add_pd(_t26_18, _t26_22);
    _t26_19 = _mm256_add_pd(_t26_19, _t26_23);

    // AVX Storer:
    _asm256_storeu_pd(M1 + 12*k2 + 8, _t26_16);
    _asm256_storeu_pd(M1 + 12*k2 + 20, _t26_17);
    _asm256_storeu_pd(M1 + 12*k2 + 32, _t26_18);
    _asm256_storeu_pd(M1 + 12*k2 + 44, _t26_19);
  }


  // Generating : M2[12,12] = ( ( ( ( ( ( Sum_{i0} ( S(h(4, 12, 0), ( G(h(4, 12, 0), Y[12,12],h(4, 12, 0)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 0)) ) ),h(4, 12, i0)) ) + Sum_{k2} ( Sum_{i0} ( S(h(4, 12, k2), ( T( G(h(4, 12, 0), Y[12,12],h(4, 12, k2)) ) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 0)) ) ),h(4, 12, i0)) ) ) ) + Sum_{i0} ( $(h(4, 12, 0), ( G(h(4, 12, 0), Y[12,12],h(4, 12, 4)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 4)) ) ),h(4, 12, i0)) ) ) + Sum_{i0} ( $(h(4, 12, 4), ( G(h(4, 12, 4), Y[12,12],h(4, 12, 4)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 4)) ) ),h(4, 12, i0)) ) ) + Sum_{i0} ( $(h(4, 12, 8), ( T( G(h(4, 12, 4), Y[12,12],h(4, 12, 8)) ) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 4)) ) ),h(4, 12, i0)) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 12, k2), ( G(h(4, 12, k2), Y[12,12],h(4, 12, 8)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 8)) ) ),h(4, 12, i0)) ) ) ) + Sum_{i0} ( $(h(4, 12, 8), ( G(h(4, 12, 8), Y[12,12],h(4, 12, 8)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 8)) ) ),h(4, 12, i0)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t27_0 = _t19_3;
  _t27_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 3), _t19_2, 12);
  _t27_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 0), _t19_1, 49);
  _t27_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 12), _mm256_shuffle_pd(_t19_1, _t19_0, 12), 49);


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t28_3 = _asm256_loadu_pd(H + 12*i0);
    _t28_2 = _asm256_loadu_pd(H + 12*i0 + 12);
    _t28_1 = _asm256_loadu_pd(H + 12*i0 + 24);
    _t28_0 = _asm256_loadu_pd(H + 12*i0 + 36);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t28_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 32);
    _t28_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t28_3, _t28_2), _mm256_unpackhi_pd(_t28_1, _t28_0), 32);
    _t28_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t28_3, _t28_2), _mm256_unpacklo_pd(_t28_1, _t28_0), 49);
    _t28_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t28_3, _t28_2), _mm256_unpackhi_pd(_t28_1, _t28_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t28_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_0, _t27_0, 32), _mm256_permute2f128_pd(_t27_0, _t27_0, 32), 0), _t28_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_0, _t27_0, 32), _mm256_permute2f128_pd(_t27_0, _t27_0, 32), 15), _t28_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_0, _t27_0, 49), _mm256_permute2f128_pd(_t27_0, _t27_0, 49), 0), _t28_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_0, _t27_0, 49), _mm256_permute2f128_pd(_t27_0, _t27_0, 49), 15), _t28_11)));
    _t28_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_1, _t27_1, 32), _mm256_permute2f128_pd(_t27_1, _t27_1, 32), 0), _t28_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_1, _t27_1, 32), _mm256_permute2f128_pd(_t27_1, _t27_1, 32), 15), _t28_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_1, _t27_1, 49), _mm256_permute2f128_pd(_t27_1, _t27_1, 49), 0), _t28_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_1, _t27_1, 49), _mm256_permute2f128_pd(_t27_1, _t27_1, 49), 15), _t28_11)));
    _t28_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_2, _t27_2, 32), _mm256_permute2f128_pd(_t27_2, _t27_2, 32), 0), _t28_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_2, _t27_2, 32), _mm256_permute2f128_pd(_t27_2, _t27_2, 32), 15), _t28_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_2, _t27_2, 49), _mm256_permute2f128_pd(_t27_2, _t27_2, 49), 0), _t28_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_2, _t27_2, 49), _mm256_permute2f128_pd(_t27_2, _t27_2, 49), 15), _t28_11)));
    _t28_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_3, _t27_3, 32), _mm256_permute2f128_pd(_t27_3, _t27_3, 32), 0), _t28_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_3, _t27_3, 32), _mm256_permute2f128_pd(_t27_3, _t27_3, 32), 15), _t28_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_3, _t27_3, 49), _mm256_permute2f128_pd(_t27_3, _t27_3, 49), 0), _t28_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t27_3, _t27_3, 49), _mm256_permute2f128_pd(_t27_3, _t27_3, 49), 15), _t28_11)));

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0, _t28_4);
    _asm256_storeu_pd(M2 + i0 + 12, _t28_5);
    _asm256_storeu_pd(M2 + i0 + 24, _t28_6);
    _asm256_storeu_pd(M2 + i0 + 36, _t28_7);
  }


  for( int k2 = 4; k2 <= 11; k2+=4 ) {
    _t29_3 = _asm256_loadu_pd(Y + k2);
    _t29_2 = _asm256_loadu_pd(Y + k2 + 12);
    _t29_1 = _asm256_loadu_pd(Y + k2 + 24);
    _t29_0 = _asm256_loadu_pd(Y + k2 + 36);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t29_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
    _t29_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
    _t29_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
    _t29_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

    for( int i0 = 0; i0 <= 11; i0+=4 ) {
      _t30_3 = _asm256_loadu_pd(H + 12*i0);
      _t30_2 = _asm256_loadu_pd(H + 12*i0 + 12);
      _t30_1 = _asm256_loadu_pd(H + 12*i0 + 24);
      _t30_0 = _asm256_loadu_pd(H + 12*i0 + 36);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
      _t29_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
      _t29_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
      _t29_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t30_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_3, _t30_2), _mm256_unpacklo_pd(_t30_1, _t30_0), 32);
      _t30_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t30_3, _t30_2), _mm256_unpackhi_pd(_t30_1, _t30_0), 32);
      _t30_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_3, _t30_2), _mm256_unpacklo_pd(_t30_1, _t30_0), 49);
      _t30_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t30_3, _t30_2), _mm256_unpackhi_pd(_t30_1, _t30_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t30_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_4, _t29_4, 32), _mm256_permute2f128_pd(_t29_4, _t29_4, 32), 0), _t30_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_4, _t29_4, 32), _mm256_permute2f128_pd(_t29_4, _t29_4, 32), 15), _t30_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_4, _t29_4, 49), _mm256_permute2f128_pd(_t29_4, _t29_4, 49), 0), _t30_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_4, _t29_4, 49), _mm256_permute2f128_pd(_t29_4, _t29_4, 49), 15), _t30_11)));
      _t30_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_5, _t29_5, 32), _mm256_permute2f128_pd(_t29_5, _t29_5, 32), 0), _t30_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_5, _t29_5, 32), _mm256_permute2f128_pd(_t29_5, _t29_5, 32), 15), _t30_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_5, _t29_5, 49), _mm256_permute2f128_pd(_t29_5, _t29_5, 49), 0), _t30_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_5, _t29_5, 49), _mm256_permute2f128_pd(_t29_5, _t29_5, 49), 15), _t30_11)));
      _t30_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_6, _t29_6, 32), _mm256_permute2f128_pd(_t29_6, _t29_6, 32), 0), _t30_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_6, _t29_6, 32), _mm256_permute2f128_pd(_t29_6, _t29_6, 32), 15), _t30_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_6, _t29_6, 49), _mm256_permute2f128_pd(_t29_6, _t29_6, 49), 0), _t30_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_6, _t29_6, 49), _mm256_permute2f128_pd(_t29_6, _t29_6, 49), 15), _t30_11)));
      _t30_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 32), _mm256_permute2f128_pd(_t29_7, _t29_7, 32), 0), _t30_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 32), _mm256_permute2f128_pd(_t29_7, _t29_7, 32), 15), _t30_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 49), _mm256_permute2f128_pd(_t29_7, _t29_7, 49), 0), _t30_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t29_7, _t29_7, 49), _mm256_permute2f128_pd(_t29_7, _t29_7, 49), 15), _t30_11)));

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 12*k2, _t30_4);
      _asm256_storeu_pd(M2 + i0 + 12*k2 + 12, _t30_5);
      _asm256_storeu_pd(M2 + i0 + 12*k2 + 24, _t30_6);
      _asm256_storeu_pd(M2 + i0 + 12*k2 + 36, _t30_7);
    }
  }


  // AVX Loader:


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t31_19 = _mm256_broadcast_sd(Y + 4);
    _t31_18 = _mm256_broadcast_sd(Y + 5);
    _t31_17 = _mm256_broadcast_sd(Y + 6);
    _t31_16 = _mm256_broadcast_sd(Y + 7);
    _t31_15 = _mm256_broadcast_sd(Y + 16);
    _t31_14 = _mm256_broadcast_sd(Y + 17);
    _t31_13 = _mm256_broadcast_sd(Y + 18);
    _t31_12 = _mm256_broadcast_sd(Y + 19);
    _t31_11 = _mm256_broadcast_sd(Y + 28);
    _t31_10 = _mm256_broadcast_sd(Y + 29);
    _t31_9 = _mm256_broadcast_sd(Y + 30);
    _t31_8 = _mm256_broadcast_sd(Y + 31);
    _t31_7 = _mm256_broadcast_sd(Y + 40);
    _t31_6 = _mm256_broadcast_sd(Y + 41);
    _t31_5 = _mm256_broadcast_sd(Y + 42);
    _t31_4 = _mm256_broadcast_sd(Y + 43);
    _t31_3 = _asm256_loadu_pd(H + 12*i0 + 4);
    _t31_2 = _asm256_loadu_pd(H + 12*i0 + 16);
    _t31_1 = _asm256_loadu_pd(H + 12*i0 + 28);
    _t31_0 = _asm256_loadu_pd(H + 12*i0 + 40);
    _t31_20 = _asm256_loadu_pd(M2 + i0);
    _t31_21 = _asm256_loadu_pd(M2 + i0 + 12);
    _t31_22 = _asm256_loadu_pd(M2 + i0 + 24);
    _t31_23 = _asm256_loadu_pd(M2 + i0 + 36);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t31_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 32);
    _t31_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 32);
    _t31_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 49);
    _t31_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t31_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t31_28), _mm256_mul_pd(_t31_18, _t31_29)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t31_30), _mm256_mul_pd(_t31_16, _t31_31)));
    _t31_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t31_28), _mm256_mul_pd(_t31_14, _t31_29)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t31_30), _mm256_mul_pd(_t31_12, _t31_31)));
    _t31_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t31_28), _mm256_mul_pd(_t31_10, _t31_29)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t31_30), _mm256_mul_pd(_t31_8, _t31_31)));
    _t31_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t31_28), _mm256_mul_pd(_t31_6, _t31_29)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t31_30), _mm256_mul_pd(_t31_4, _t31_31)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t31_20 = _mm256_add_pd(_t31_20, _t31_24);
    _t31_21 = _mm256_add_pd(_t31_21, _t31_25);
    _t31_22 = _mm256_add_pd(_t31_22, _t31_26);
    _t31_23 = _mm256_add_pd(_t31_23, _t31_27);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0, _t31_20);
    _asm256_storeu_pd(M2 + i0 + 12, _t31_21);
    _asm256_storeu_pd(M2 + i0 + 24, _t31_22);
    _asm256_storeu_pd(M2 + i0 + 36, _t31_23);
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t32_0 = _t22_3;
  _t32_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 3), _t22_2, 12);
  _t32_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 0), _t22_1, 49);
  _t32_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 12), _mm256_shuffle_pd(_t22_1, _t22_0, 12), 49);


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t33_3 = _asm256_loadu_pd(H + 12*i0 + 4);
    _t33_2 = _asm256_loadu_pd(H + 12*i0 + 16);
    _t33_1 = _asm256_loadu_pd(H + 12*i0 + 28);
    _t33_0 = _asm256_loadu_pd(H + 12*i0 + 40);
    _t33_4 = _asm256_loadu_pd(M2 + i0 + 48);
    _t33_5 = _asm256_loadu_pd(M2 + i0 + 60);
    _t33_6 = _asm256_loadu_pd(M2 + i0 + 72);
    _t33_7 = _asm256_loadu_pd(M2 + i0 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t33_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_0, _t32_0, 32), _mm256_permute2f128_pd(_t32_0, _t32_0, 32), 0), _t33_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_0, _t32_0, 32), _mm256_permute2f128_pd(_t32_0, _t32_0, 32), 15), _t33_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_0, _t32_0, 49), _mm256_permute2f128_pd(_t32_0, _t32_0, 49), 0), _t33_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_0, _t32_0, 49), _mm256_permute2f128_pd(_t32_0, _t32_0, 49), 15), _t33_15)));
    _t33_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_1, _t32_1, 32), _mm256_permute2f128_pd(_t32_1, _t32_1, 32), 0), _t33_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_1, _t32_1, 32), _mm256_permute2f128_pd(_t32_1, _t32_1, 32), 15), _t33_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_1, _t32_1, 49), _mm256_permute2f128_pd(_t32_1, _t32_1, 49), 0), _t33_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_1, _t32_1, 49), _mm256_permute2f128_pd(_t32_1, _t32_1, 49), 15), _t33_15)));
    _t33_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_2, _t32_2, 32), _mm256_permute2f128_pd(_t32_2, _t32_2, 32), 0), _t33_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_2, _t32_2, 32), _mm256_permute2f128_pd(_t32_2, _t32_2, 32), 15), _t33_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_2, _t32_2, 49), _mm256_permute2f128_pd(_t32_2, _t32_2, 49), 0), _t33_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_2, _t32_2, 49), _mm256_permute2f128_pd(_t32_2, _t32_2, 49), 15), _t33_15)));
    _t33_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_3, _t32_3, 32), _mm256_permute2f128_pd(_t32_3, _t32_3, 32), 0), _t33_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_3, _t32_3, 32), _mm256_permute2f128_pd(_t32_3, _t32_3, 32), 15), _t33_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_3, _t32_3, 49), _mm256_permute2f128_pd(_t32_3, _t32_3, 49), 0), _t33_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_3, _t32_3, 49), _mm256_permute2f128_pd(_t32_3, _t32_3, 49), 15), _t33_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t33_4 = _mm256_add_pd(_t33_4, _t33_8);
    _t33_5 = _mm256_add_pd(_t33_5, _t33_9);
    _t33_6 = _mm256_add_pd(_t33_6, _t33_10);
    _t33_7 = _mm256_add_pd(_t33_7, _t33_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 48, _t33_4);
    _asm256_storeu_pd(M2 + i0 + 60, _t33_5);
    _asm256_storeu_pd(M2 + i0 + 72, _t33_6);
    _asm256_storeu_pd(M2 + i0 + 84, _t33_7);
  }

  _t34_3 = _asm256_loadu_pd(Y + 56);
  _t34_2 = _asm256_loadu_pd(Y + 68);
  _t34_1 = _asm256_loadu_pd(Y + 80);
  _t34_0 = _asm256_loadu_pd(Y + 92);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t34_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
  _t34_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
  _t34_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
  _t34_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t35_3 = _asm256_loadu_pd(H + 12*i0 + 4);
    _t35_2 = _asm256_loadu_pd(H + 12*i0 + 16);
    _t35_1 = _asm256_loadu_pd(H + 12*i0 + 28);
    _t35_0 = _asm256_loadu_pd(H + 12*i0 + 40);
    _t35_4 = _asm256_loadu_pd(M2 + i0 + 96);
    _t35_5 = _asm256_loadu_pd(M2 + i0 + 108);
    _t35_6 = _asm256_loadu_pd(M2 + i0 + 120);
    _t35_7 = _asm256_loadu_pd(M2 + i0 + 132);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t35_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
    _t35_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
    _t35_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
    _t35_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t35_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_4, _t34_4, 32), _mm256_permute2f128_pd(_t34_4, _t34_4, 32), 0), _t35_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_4, _t34_4, 32), _mm256_permute2f128_pd(_t34_4, _t34_4, 32), 15), _t35_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_4, _t34_4, 49), _mm256_permute2f128_pd(_t34_4, _t34_4, 49), 0), _t35_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_4, _t34_4, 49), _mm256_permute2f128_pd(_t34_4, _t34_4, 49), 15), _t35_15)));
    _t35_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_5, _t34_5, 32), _mm256_permute2f128_pd(_t34_5, _t34_5, 32), 0), _t35_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_5, _t34_5, 32), _mm256_permute2f128_pd(_t34_5, _t34_5, 32), 15), _t35_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_5, _t34_5, 49), _mm256_permute2f128_pd(_t34_5, _t34_5, 49), 0), _t35_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_5, _t34_5, 49), _mm256_permute2f128_pd(_t34_5, _t34_5, 49), 15), _t35_15)));
    _t35_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_6, _t34_6, 32), _mm256_permute2f128_pd(_t34_6, _t34_6, 32), 0), _t35_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_6, _t34_6, 32), _mm256_permute2f128_pd(_t34_6, _t34_6, 32), 15), _t35_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_6, _t34_6, 49), _mm256_permute2f128_pd(_t34_6, _t34_6, 49), 0), _t35_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_6, _t34_6, 49), _mm256_permute2f128_pd(_t34_6, _t34_6, 49), 15), _t35_15)));
    _t35_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_7, _t34_7, 32), _mm256_permute2f128_pd(_t34_7, _t34_7, 32), 0), _t35_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_7, _t34_7, 32), _mm256_permute2f128_pd(_t34_7, _t34_7, 32), 15), _t35_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_7, _t34_7, 49), _mm256_permute2f128_pd(_t34_7, _t34_7, 49), 0), _t35_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_7, _t34_7, 49), _mm256_permute2f128_pd(_t34_7, _t34_7, 49), 15), _t35_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t35_4 = _mm256_add_pd(_t35_4, _t35_8);
    _t35_5 = _mm256_add_pd(_t35_5, _t35_9);
    _t35_6 = _mm256_add_pd(_t35_6, _t35_10);
    _t35_7 = _mm256_add_pd(_t35_7, _t35_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 96, _t35_4);
    _asm256_storeu_pd(M2 + i0 + 108, _t35_5);
    _asm256_storeu_pd(M2 + i0 + 120, _t35_6);
    _asm256_storeu_pd(M2 + i0 + 132, _t35_7);
  }


  for( int k2 = 0; k2 <= 7; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 11; i0+=4 ) {
      _t36_19 = _mm256_broadcast_sd(Y + 12*k2 + 8);
      _t36_18 = _mm256_broadcast_sd(Y + 12*k2 + 9);
      _t36_17 = _mm256_broadcast_sd(Y + 12*k2 + 10);
      _t36_16 = _mm256_broadcast_sd(Y + 12*k2 + 11);
      _t36_15 = _mm256_broadcast_sd(Y + 12*k2 + 20);
      _t36_14 = _mm256_broadcast_sd(Y + 12*k2 + 21);
      _t36_13 = _mm256_broadcast_sd(Y + 12*k2 + 22);
      _t36_12 = _mm256_broadcast_sd(Y + 12*k2 + 23);
      _t36_11 = _mm256_broadcast_sd(Y + 12*k2 + 32);
      _t36_10 = _mm256_broadcast_sd(Y + 12*k2 + 33);
      _t36_9 = _mm256_broadcast_sd(Y + 12*k2 + 34);
      _t36_8 = _mm256_broadcast_sd(Y + 12*k2 + 35);
      _t36_7 = _mm256_broadcast_sd(Y + 12*k2 + 44);
      _t36_6 = _mm256_broadcast_sd(Y + 12*k2 + 45);
      _t36_5 = _mm256_broadcast_sd(Y + 12*k2 + 46);
      _t36_4 = _mm256_broadcast_sd(Y + 12*k2 + 47);
      _t36_3 = _asm256_loadu_pd(H + 12*i0 + 8);
      _t36_2 = _asm256_loadu_pd(H + 12*i0 + 20);
      _t36_1 = _asm256_loadu_pd(H + 12*i0 + 32);
      _t36_0 = _asm256_loadu_pd(H + 12*i0 + 44);
      _t36_20 = _asm256_loadu_pd(M2 + i0 + 12*k2);
      _t36_21 = _asm256_loadu_pd(M2 + i0 + 12*k2 + 12);
      _t36_22 = _asm256_loadu_pd(M2 + i0 + 12*k2 + 24);
      _t36_23 = _asm256_loadu_pd(M2 + i0 + 12*k2 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t36_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_3, _t36_2), _mm256_unpacklo_pd(_t36_1, _t36_0), 32);
      _t36_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_3, _t36_2), _mm256_unpackhi_pd(_t36_1, _t36_0), 32);
      _t36_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_3, _t36_2), _mm256_unpacklo_pd(_t36_1, _t36_0), 49);
      _t36_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_3, _t36_2), _mm256_unpackhi_pd(_t36_1, _t36_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t36_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_19, _t36_28), _mm256_mul_pd(_t36_18, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_17, _t36_30), _mm256_mul_pd(_t36_16, _t36_31)));
      _t36_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_15, _t36_28), _mm256_mul_pd(_t36_14, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_13, _t36_30), _mm256_mul_pd(_t36_12, _t36_31)));
      _t36_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_11, _t36_28), _mm256_mul_pd(_t36_10, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_9, _t36_30), _mm256_mul_pd(_t36_8, _t36_31)));
      _t36_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_7, _t36_28), _mm256_mul_pd(_t36_6, _t36_29)), _mm256_add_pd(_mm256_mul_pd(_t36_5, _t36_30), _mm256_mul_pd(_t36_4, _t36_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t36_20 = _mm256_add_pd(_t36_20, _t36_24);
      _t36_21 = _mm256_add_pd(_t36_21, _t36_25);
      _t36_22 = _mm256_add_pd(_t36_22, _t36_26);
      _t36_23 = _mm256_add_pd(_t36_23, _t36_27);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 12*k2, _t36_20);
      _asm256_storeu_pd(M2 + i0 + 12*k2 + 12, _t36_21);
      _asm256_storeu_pd(M2 + i0 + 12*k2 + 24, _t36_22);
      _asm256_storeu_pd(M2 + i0 + 12*k2 + 36, _t36_23);
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t37_0 = _t13_28;
  _t37_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 3), _t13_29, 12);
  _t37_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 0), _t13_30, 49);
  _t37_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 12), _mm256_shuffle_pd(_t13_30, _t13_31, 12), 49);


  for( int i0 = 0; i0 <= 11; i0+=4 ) {
    _t38_3 = _asm256_loadu_pd(H + 12*i0 + 8);
    _t38_2 = _asm256_loadu_pd(H + 12*i0 + 20);
    _t38_1 = _asm256_loadu_pd(H + 12*i0 + 32);
    _t38_0 = _asm256_loadu_pd(H + 12*i0 + 44);
    _t38_4 = _asm256_loadu_pd(M2 + i0 + 96);
    _t38_5 = _asm256_loadu_pd(M2 + i0 + 108);
    _t38_6 = _asm256_loadu_pd(M2 + i0 + 120);
    _t38_7 = _asm256_loadu_pd(M2 + i0 + 132);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t38_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
    _t38_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
    _t38_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
    _t38_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t38_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_0, _t37_0, 32), _mm256_permute2f128_pd(_t37_0, _t37_0, 32), 0), _t38_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_0, _t37_0, 32), _mm256_permute2f128_pd(_t37_0, _t37_0, 32), 15), _t38_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_0, _t37_0, 49), _mm256_permute2f128_pd(_t37_0, _t37_0, 49), 0), _t38_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_0, _t37_0, 49), _mm256_permute2f128_pd(_t37_0, _t37_0, 49), 15), _t38_15)));
    _t38_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_1, _t37_1, 32), _mm256_permute2f128_pd(_t37_1, _t37_1, 32), 0), _t38_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_1, _t37_1, 32), _mm256_permute2f128_pd(_t37_1, _t37_1, 32), 15), _t38_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_1, _t37_1, 49), _mm256_permute2f128_pd(_t37_1, _t37_1, 49), 0), _t38_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_1, _t37_1, 49), _mm256_permute2f128_pd(_t37_1, _t37_1, 49), 15), _t38_15)));
    _t38_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_2, _t37_2, 32), _mm256_permute2f128_pd(_t37_2, _t37_2, 32), 0), _t38_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_2, _t37_2, 32), _mm256_permute2f128_pd(_t37_2, _t37_2, 32), 15), _t38_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_2, _t37_2, 49), _mm256_permute2f128_pd(_t37_2, _t37_2, 49), 0), _t38_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_2, _t37_2, 49), _mm256_permute2f128_pd(_t37_2, _t37_2, 49), 15), _t38_15)));
    _t38_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_3, _t37_3, 32), _mm256_permute2f128_pd(_t37_3, _t37_3, 32), 0), _t38_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_3, _t37_3, 32), _mm256_permute2f128_pd(_t37_3, _t37_3, 32), 15), _t38_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_3, _t37_3, 49), _mm256_permute2f128_pd(_t37_3, _t37_3, 49), 0), _t38_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_3, _t37_3, 49), _mm256_permute2f128_pd(_t37_3, _t37_3, 49), 15), _t38_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t38_4 = _mm256_add_pd(_t38_4, _t38_8);
    _t38_5 = _mm256_add_pd(_t38_5, _t38_9);
    _t38_6 = _mm256_add_pd(_t38_6, _t38_10);
    _t38_7 = _mm256_add_pd(_t38_7, _t38_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 96, _t38_4);
    _asm256_storeu_pd(M2 + i0 + 108, _t38_5);
    _asm256_storeu_pd(M2 + i0 + 120, _t38_6);
    _asm256_storeu_pd(M2 + i0 + 132, _t38_7);
  }


  // Generating : M3[12,12] = ( ( Sum_{k2} ( ( S(h(4, 12, k2), ( ( G(h(4, 12, k2), M1[12,12],h(4, 12, 0)) * T( G(h(4, 12, k2), H[12,12],h(4, 12, 0)) ) ) + G(h(4, 12, k2), R[12,12],h(4, 12, k2)) ),h(4, 12, k2)) + Sum_{i0} ( S(h(4, 12, k2), ( ( G(h(4, 12, k2), M1[12,12],h(4, 12, 0)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, 0)) ) ) + G(h(4, 12, k2), R[12,12],h(4, 12, i0)) ),h(4, 12, i0)) ) ) ) + S(h(4, 12, 8), ( ( G(h(4, 12, 8), M1[12,12],h(4, 12, 0)) * T( G(h(4, 12, 8), H[12,12],h(4, 12, 0)) ) ) + G(h(4, 12, 8), R[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 12, k2), ( G(h(4, 12, k2), M1[12,12],h(4, 12, k3)) * T( G(h(4, 12, k2), H[12,12],h(4, 12, k3)) ) ),h(4, 12, k2)) + Sum_{i0} ( $(h(4, 12, k2), ( G(h(4, 12, k2), M1[12,12],h(4, 12, k3)) * T( G(h(4, 12, i0), H[12,12],h(4, 12, k3)) ) ),h(4, 12, i0)) ) ) ) + $(h(4, 12, 8), ( G(h(4, 12, 8), M1[12,12],h(4, 12, k3)) * T( G(h(4, 12, 8), H[12,12],h(4, 12, k3)) ) ),h(4, 12, 8)) ) ) )


  for( int k2 = 0; k2 <= 7; k2+=4 ) {
    _t39_23 = _mm256_broadcast_sd(M1 + 12*k2);
    _t39_22 = _mm256_broadcast_sd(M1 + 12*k2 + 1);
    _t39_21 = _mm256_broadcast_sd(M1 + 12*k2 + 2);
    _t39_20 = _mm256_broadcast_sd(M1 + 12*k2 + 3);
    _t39_19 = _mm256_broadcast_sd(M1 + 12*k2 + 12);
    _t39_18 = _mm256_broadcast_sd(M1 + 12*k2 + 13);
    _t39_17 = _mm256_broadcast_sd(M1 + 12*k2 + 14);
    _t39_16 = _mm256_broadcast_sd(M1 + 12*k2 + 15);
    _t39_15 = _mm256_broadcast_sd(M1 + 12*k2 + 24);
    _t39_14 = _mm256_broadcast_sd(M1 + 12*k2 + 25);
    _t39_13 = _mm256_broadcast_sd(M1 + 12*k2 + 26);
    _t39_12 = _mm256_broadcast_sd(M1 + 12*k2 + 27);
    _t39_11 = _mm256_broadcast_sd(M1 + 12*k2 + 36);
    _t39_10 = _mm256_broadcast_sd(M1 + 12*k2 + 37);
    _t39_9 = _mm256_broadcast_sd(M1 + 12*k2 + 38);
    _t39_8 = _mm256_broadcast_sd(M1 + 12*k2 + 39);
    _t39_7 = _asm256_loadu_pd(H + 12*k2);
    _t39_6 = _asm256_loadu_pd(H + 12*k2 + 12);
    _t39_5 = _asm256_loadu_pd(H + 12*k2 + 24);
    _t39_4 = _asm256_loadu_pd(H + 12*k2 + 36);
    _t39_3 = _asm256_loadu_pd(R + 13*k2);
    _t39_2 = _mm256_maskload_pd(R + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t39_1 = _mm256_maskload_pd(R + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t39_0 = _mm256_maskload_pd(R + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_7, _t39_6), _mm256_unpacklo_pd(_t39_5, _t39_4), 32);
    _t39_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_7, _t39_6), _mm256_unpackhi_pd(_t39_5, _t39_4), 32);
    _t39_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_7, _t39_6), _mm256_unpacklo_pd(_t39_5, _t39_4), 49);
    _t39_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_7, _t39_6), _mm256_unpackhi_pd(_t39_5, _t39_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_23, _t39_40), _mm256_mul_pd(_t39_22, _t39_41)), _mm256_add_pd(_mm256_mul_pd(_t39_21, _t39_42), _mm256_mul_pd(_t39_20, _t39_43)));
    _t39_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_40), _mm256_mul_pd(_t39_18, _t39_41)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_42), _mm256_mul_pd(_t39_16, _t39_43)));
    _t39_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_40), _mm256_mul_pd(_t39_14, _t39_41)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_42), _mm256_mul_pd(_t39_12, _t39_43)));
    _t39_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_40), _mm256_mul_pd(_t39_10, _t39_41)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_42), _mm256_mul_pd(_t39_8, _t39_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t39_36 = _t39_3;
    _t39_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t39_3, _t39_2, 3), _t39_2, 12);
    _t39_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t39_3, _t39_2, 0), _t39_1, 49);
    _t39_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t39_3, _t39_2, 12), _mm256_shuffle_pd(_t39_1, _t39_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t39_24 = _mm256_add_pd(_t39_32, _t39_36);
    _t39_25 = _mm256_add_pd(_t39_33, _t39_37);
    _t39_26 = _mm256_add_pd(_t39_34, _t39_38);
    _t39_27 = _mm256_add_pd(_t39_35, _t39_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t39_28 = _t39_24;
    _t39_29 = _t39_25;
    _t39_30 = _t39_26;
    _t39_31 = _t39_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 11; i0+=4 ) {
      _t40_7 = _asm256_loadu_pd(H + 12*i0);
      _t40_6 = _asm256_loadu_pd(H + 12*i0 + 12);
      _t40_5 = _asm256_loadu_pd(H + 12*i0 + 24);
      _t40_4 = _asm256_loadu_pd(H + 12*i0 + 36);
      _t40_3 = _asm256_loadu_pd(R + i0 + 12*k2);
      _t40_2 = _asm256_loadu_pd(R + i0 + 12*k2 + 12);
      _t40_1 = _asm256_loadu_pd(R + i0 + 12*k2 + 24);
      _t40_0 = _asm256_loadu_pd(R + i0 + 12*k2 + 36);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t40_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_7, _t40_6), _mm256_unpacklo_pd(_t40_5, _t40_4), 32);
      _t40_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_7, _t40_6), _mm256_unpackhi_pd(_t40_5, _t40_4), 32);
      _t40_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_7, _t40_6), _mm256_unpacklo_pd(_t40_5, _t40_4), 49);
      _t40_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_7, _t40_6), _mm256_unpackhi_pd(_t40_5, _t40_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t40_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_23, _t40_16), _mm256_mul_pd(_t39_22, _t40_17)), _mm256_add_pd(_mm256_mul_pd(_t39_21, _t40_18), _mm256_mul_pd(_t39_20, _t40_19)));
      _t40_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t40_16), _mm256_mul_pd(_t39_18, _t40_17)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t40_18), _mm256_mul_pd(_t39_16, _t40_19)));
      _t40_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t40_16), _mm256_mul_pd(_t39_14, _t40_17)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t40_18), _mm256_mul_pd(_t39_12, _t40_19)));
      _t40_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t40_16), _mm256_mul_pd(_t39_10, _t40_17)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t40_18), _mm256_mul_pd(_t39_8, _t40_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t40_8 = _mm256_add_pd(_t40_12, _t40_3);
      _t40_9 = _mm256_add_pd(_t40_13, _t40_2);
      _t40_10 = _mm256_add_pd(_t40_14, _t40_1);
      _t40_11 = _mm256_add_pd(_t40_15, _t40_0);

      // AVX Storer:
      _asm256_storeu_pd(M3 + i0 + 12*k2, _t40_8);
      _asm256_storeu_pd(M3 + i0 + 12*k2 + 12, _t40_9);
      _asm256_storeu_pd(M3 + i0 + 12*k2 + 24, _t40_10);
      _asm256_storeu_pd(M3 + i0 + 12*k2 + 36, _t40_11);
    }
    _asm256_storeu_pd(M3 + 13*k2, _t39_28);
    _mm256_maskstore_pd(M3 + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t39_29);
    _mm256_maskstore_pd(M3 + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t39_30);
    _mm256_maskstore_pd(M3 + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t39_31);
  }

  _t41_23 = _mm256_broadcast_sd(M1 + 96);
  _t41_22 = _mm256_broadcast_sd(M1 + 97);
  _t41_21 = _mm256_broadcast_sd(M1 + 98);
  _t41_20 = _mm256_broadcast_sd(M1 + 99);
  _t41_19 = _mm256_broadcast_sd(M1 + 108);
  _t41_18 = _mm256_broadcast_sd(M1 + 109);
  _t41_17 = _mm256_broadcast_sd(M1 + 110);
  _t41_16 = _mm256_broadcast_sd(M1 + 111);
  _t41_15 = _mm256_broadcast_sd(M1 + 120);
  _t41_14 = _mm256_broadcast_sd(M1 + 121);
  _t41_13 = _mm256_broadcast_sd(M1 + 122);
  _t41_12 = _mm256_broadcast_sd(M1 + 123);
  _t41_11 = _mm256_broadcast_sd(M1 + 132);
  _t41_10 = _mm256_broadcast_sd(M1 + 133);
  _t41_9 = _mm256_broadcast_sd(M1 + 134);
  _t41_8 = _mm256_broadcast_sd(M1 + 135);
  _t41_7 = _asm256_loadu_pd(H + 96);
  _t41_6 = _asm256_loadu_pd(H + 108);
  _t41_5 = _asm256_loadu_pd(H + 120);
  _t41_4 = _asm256_loadu_pd(H + 132);
  _t41_3 = _asm256_loadu_pd(R + 104);
  _t41_2 = _mm256_maskload_pd(R + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t41_1 = _mm256_maskload_pd(R + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t41_0 = _mm256_maskload_pd(R + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t41_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
  _t41_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
  _t41_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
  _t41_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t41_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t41_23, _t41_40), _mm256_mul_pd(_t41_22, _t41_41)), _mm256_add_pd(_mm256_mul_pd(_t41_21, _t41_42), _mm256_mul_pd(_t41_20, _t41_43)));
  _t41_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t41_19, _t41_40), _mm256_mul_pd(_t41_18, _t41_41)), _mm256_add_pd(_mm256_mul_pd(_t41_17, _t41_42), _mm256_mul_pd(_t41_16, _t41_43)));
  _t41_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t41_15, _t41_40), _mm256_mul_pd(_t41_14, _t41_41)), _mm256_add_pd(_mm256_mul_pd(_t41_13, _t41_42), _mm256_mul_pd(_t41_12, _t41_43)));
  _t41_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t41_11, _t41_40), _mm256_mul_pd(_t41_10, _t41_41)), _mm256_add_pd(_mm256_mul_pd(_t41_9, _t41_42), _mm256_mul_pd(_t41_8, _t41_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t41_36 = _t41_3;
  _t41_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
  _t41_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
  _t41_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t41_24 = _mm256_add_pd(_t41_32, _t41_36);
  _t41_25 = _mm256_add_pd(_t41_33, _t41_37);
  _t41_26 = _mm256_add_pd(_t41_34, _t41_38);
  _t41_27 = _mm256_add_pd(_t41_35, _t41_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t41_28 = _t41_24;
  _t41_29 = _t41_25;
  _t41_30 = _t41_26;
  _t41_31 = _t41_27;


  for( int k3 = 4; k3 <= 11; k3+=4 ) {

    for( int k2 = 0; k2 <= 7; k2+=4 ) {
      _t42_19 = _mm256_broadcast_sd(M1 + 12*k2 + k3);
      _t42_18 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 1);
      _t42_17 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 2);
      _t42_16 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 3);
      _t42_15 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 12);
      _t42_14 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 13);
      _t42_13 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 14);
      _t42_12 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 15);
      _t42_11 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 24);
      _t42_10 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 25);
      _t42_9 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 26);
      _t42_8 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 27);
      _t42_7 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 36);
      _t42_6 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 37);
      _t42_5 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 38);
      _t42_4 = _mm256_broadcast_sd(M1 + 12*k2 + k3 + 39);
      _t42_3 = _asm256_loadu_pd(H + 12*k2 + k3);
      _t42_2 = _asm256_loadu_pd(H + 12*k2 + k3 + 12);
      _t42_1 = _asm256_loadu_pd(H + 12*k2 + k3 + 24);
      _t42_0 = _asm256_loadu_pd(H + 12*k2 + k3 + 36);
      _t42_20 = _asm256_loadu_pd(M3 + 13*k2);
      _t42_21 = _mm256_maskload_pd(M3 + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t42_22 = _mm256_maskload_pd(M3 + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t42_23 = _mm256_maskload_pd(M3 + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_3, _t42_2), _mm256_unpacklo_pd(_t42_1, _t42_0), 32);
      _t42_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_3, _t42_2), _mm256_unpackhi_pd(_t42_1, _t42_0), 32);
      _t42_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_3, _t42_2), _mm256_unpacklo_pd(_t42_1, _t42_0), 49);
      _t42_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_3, _t42_2), _mm256_unpackhi_pd(_t42_1, _t42_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t42_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t42_32), _mm256_mul_pd(_t42_18, _t42_33)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t42_34), _mm256_mul_pd(_t42_16, _t42_35)));
      _t42_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_32), _mm256_mul_pd(_t42_14, _t42_33)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_34), _mm256_mul_pd(_t42_12, _t42_35)));
      _t42_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_32), _mm256_mul_pd(_t42_10, _t42_33)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_34), _mm256_mul_pd(_t42_8, _t42_35)));
      _t42_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_7, _t42_32), _mm256_mul_pd(_t42_6, _t42_33)), _mm256_add_pd(_mm256_mul_pd(_t42_5, _t42_34), _mm256_mul_pd(_t42_4, _t42_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t42_28 = _t42_20;
      _t42_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t42_20, _t42_21, 3), _t42_21, 12);
      _t42_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_20, _t42_21, 0), _t42_22, 49);
      _t42_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_20, _t42_21, 12), _mm256_shuffle_pd(_t42_22, _t42_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t42_28 = _mm256_add_pd(_t42_28, _t42_24);
      _t42_29 = _mm256_add_pd(_t42_29, _t42_25);
      _t42_30 = _mm256_add_pd(_t42_30, _t42_26);
      _t42_31 = _mm256_add_pd(_t42_31, _t42_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t42_20 = _t42_28;
      _t42_21 = _t42_29;
      _t42_22 = _t42_30;
      _t42_23 = _t42_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 11; i0+=4 ) {
        _t43_3 = _asm256_loadu_pd(H + 12*i0 + k3);
        _t43_2 = _asm256_loadu_pd(H + 12*i0 + k3 + 12);
        _t43_1 = _asm256_loadu_pd(H + 12*i0 + k3 + 24);
        _t43_0 = _asm256_loadu_pd(H + 12*i0 + k3 + 36);
        _t43_4 = _asm256_loadu_pd(M3 + i0 + 12*k2);
        _t43_5 = _asm256_loadu_pd(M3 + i0 + 12*k2 + 12);
        _t43_6 = _asm256_loadu_pd(M3 + i0 + 12*k2 + 24);
        _t43_7 = _asm256_loadu_pd(M3 + i0 + 12*k2 + 36);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_3, _t43_2), _mm256_unpacklo_pd(_t43_1, _t43_0), 32);
        _t43_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_3, _t43_2), _mm256_unpackhi_pd(_t43_1, _t43_0), 32);
        _t43_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_3, _t43_2), _mm256_unpacklo_pd(_t43_1, _t43_0), 49);
        _t43_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_3, _t43_2), _mm256_unpackhi_pd(_t43_1, _t43_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t43_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t43_12), _mm256_mul_pd(_t42_18, _t43_13)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t43_14), _mm256_mul_pd(_t42_16, _t43_15)));
        _t43_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t43_12), _mm256_mul_pd(_t42_14, _t43_13)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t43_14), _mm256_mul_pd(_t42_12, _t43_15)));
        _t43_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t43_12), _mm256_mul_pd(_t42_10, _t43_13)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t43_14), _mm256_mul_pd(_t42_8, _t43_15)));
        _t43_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_7, _t43_12), _mm256_mul_pd(_t42_6, _t43_13)), _mm256_add_pd(_mm256_mul_pd(_t42_5, _t43_14), _mm256_mul_pd(_t42_4, _t43_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t43_4 = _mm256_add_pd(_t43_4, _t43_8);
        _t43_5 = _mm256_add_pd(_t43_5, _t43_9);
        _t43_6 = _mm256_add_pd(_t43_6, _t43_10);
        _t43_7 = _mm256_add_pd(_t43_7, _t43_11);

        // AVX Storer:
        _asm256_storeu_pd(M3 + i0 + 12*k2, _t43_4);
        _asm256_storeu_pd(M3 + i0 + 12*k2 + 12, _t43_5);
        _asm256_storeu_pd(M3 + i0 + 12*k2 + 24, _t43_6);
        _asm256_storeu_pd(M3 + i0 + 12*k2 + 36, _t43_7);
      }
      _asm256_storeu_pd(M3 + 13*k2, _t42_20);
      _mm256_maskstore_pd(M3 + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t42_21);
      _mm256_maskstore_pd(M3 + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t42_22);
      _mm256_maskstore_pd(M3 + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t42_23);
    }
    _t44_19 = _mm256_broadcast_sd(M1 + k3 + 96);
    _t44_18 = _mm256_broadcast_sd(M1 + k3 + 97);
    _t44_17 = _mm256_broadcast_sd(M1 + k3 + 98);
    _t44_16 = _mm256_broadcast_sd(M1 + k3 + 99);
    _t44_15 = _mm256_broadcast_sd(M1 + k3 + 108);
    _t44_14 = _mm256_broadcast_sd(M1 + k3 + 109);
    _t44_13 = _mm256_broadcast_sd(M1 + k3 + 110);
    _t44_12 = _mm256_broadcast_sd(M1 + k3 + 111);
    _t44_11 = _mm256_broadcast_sd(M1 + k3 + 120);
    _t44_10 = _mm256_broadcast_sd(M1 + k3 + 121);
    _t44_9 = _mm256_broadcast_sd(M1 + k3 + 122);
    _t44_8 = _mm256_broadcast_sd(M1 + k3 + 123);
    _t44_7 = _mm256_broadcast_sd(M1 + k3 + 132);
    _t44_6 = _mm256_broadcast_sd(M1 + k3 + 133);
    _t44_5 = _mm256_broadcast_sd(M1 + k3 + 134);
    _t44_4 = _mm256_broadcast_sd(M1 + k3 + 135);
    _t44_3 = _asm256_loadu_pd(H + k3 + 96);
    _t44_2 = _asm256_loadu_pd(H + k3 + 108);
    _t44_1 = _asm256_loadu_pd(H + k3 + 120);
    _t44_0 = _asm256_loadu_pd(H + k3 + 132);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t44_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_3, _t44_2), _mm256_unpacklo_pd(_t44_1, _t44_0), 32);
    _t44_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_3, _t44_2), _mm256_unpackhi_pd(_t44_1, _t44_0), 32);
    _t44_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_3, _t44_2), _mm256_unpacklo_pd(_t44_1, _t44_0), 49);
    _t44_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_3, _t44_2), _mm256_unpackhi_pd(_t44_1, _t44_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t44_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_19, _t44_28), _mm256_mul_pd(_t44_18, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_17, _t44_30), _mm256_mul_pd(_t44_16, _t44_31)));
    _t44_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_28), _mm256_mul_pd(_t44_14, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_30), _mm256_mul_pd(_t44_12, _t44_31)));
    _t44_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_28), _mm256_mul_pd(_t44_10, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_30), _mm256_mul_pd(_t44_8, _t44_31)));
    _t44_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_7, _t44_28), _mm256_mul_pd(_t44_6, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_5, _t44_30), _mm256_mul_pd(_t44_4, _t44_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t44_24 = _t41_28;
    _t44_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_28, _t41_29, 3), _t41_29, 12);
    _t44_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_28, _t41_29, 0), _t41_30, 49);
    _t44_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_28, _t41_29, 12), _mm256_shuffle_pd(_t41_30, _t41_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t44_24 = _mm256_add_pd(_t44_24, _t44_20);
    _t44_25 = _mm256_add_pd(_t44_25, _t44_21);
    _t44_26 = _mm256_add_pd(_t44_26, _t44_22);
    _t44_27 = _mm256_add_pd(_t44_27, _t44_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t41_28 = _t44_24;
    _t41_29 = _t44_25;
    _t41_30 = _t44_26;
    _t41_31 = _t44_27;
  }

  _t45_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t45_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t45_3 = _mm256_maskload_pd(M3 + 13, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t45_4 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t45_5 = _mm256_maskload_pd(M3 + 37, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t45_57 = _asm256_loadu_pd(M3 + 4);
  _t45_58 = _asm256_loadu_pd(M3 + 8);
  _t45_16 = _asm256_loadu_pd(M3 + 16);
  _t45_19 = _asm256_loadu_pd(M3 + 20);
  _t45_17 = _asm256_loadu_pd(M3 + 28);
  _t45_20 = _asm256_loadu_pd(M3 + 32);
  _t45_18 = _asm256_loadu_pd(M3 + 40);
  _t45_21 = _asm256_loadu_pd(M3 + 44);
  _t45_22 = _asm256_loadu_pd(M3 + 52);
  _t45_23 = _mm256_maskload_pd(M3 + 64, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t45_24 = _mm256_maskload_pd(M3 + 76, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t45_25 = _mm256_maskload_pd(M3 + 88, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t45_80 = _asm256_loadu_pd(M3 + 56);
  _t45_81 = _asm256_loadu_pd(M3 + 68);
  _t45_82 = _asm256_loadu_pd(M3 + 80);
  _t45_83 = _asm256_loadu_pd(M3 + 92);

  // Generating : U[12,12] = S(h(1, 12, 0), Sqrt( G(h(1, 12, 0), U[12,12],h(1, 12, 0)) ),h(1, 12, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_92 = _t45_0;

  // 4-BLAC: sqrt(1x4)
  _t45_93 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_92)));

  // AVX Storer:
  _t45_0 = _t45_93;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 0), U[12,12],h(1, 12, 0)) ),h(1, 12, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_94 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_95 = _t45_0;

  // 4-BLAC: 1x4 / 1x4
  _t45_96 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_94), _mm256_castpd256_pd128(_t45_95)));

  // AVX Storer:
  _t45_1 = _t45_96;

  // Generating : U[12,12] = S(h(1, 12, 0), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), U[12,12],h(3, 12, 1)) ),h(3, 12, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_97 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_98 = _t45_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_99 = _mm256_mul_pd(_t45_97, _t45_98);

  // AVX Storer:
  _t45_2 = _t45_99;

  // Generating : U[12,12] = S(h(3, 12, 1), ( G(h(3, 12, 1), U[12,12],h(3, 12, 1)) - ( T( G(h(1, 12, 0), U[12,12],h(3, 12, 1)) ) * G(h(1, 12, 0), U[12,12],h(3, 12, 1)) ) ),h(3, 12, 1))

  // AVX Loader:

  // 3x3 -> 4x4 - UpperTriang
  _t45_100 = _t45_3;
  _t45_101 = _t45_4;
  _t45_102 = _t45_5;
  _t45_103 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_104 = _t45_2;

  // 4-BLAC: (1x4)^T
  _t45_105 = _t45_104;

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_106 = _t45_2;

  // 4-BLAC: 4x1 * 1x4
  _t45_107 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_105, _t45_105, 32), _mm256_permute2f128_pd(_t45_105, _t45_105, 32), 0), _t45_106);
  _t45_108 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_105, _t45_105, 32), _mm256_permute2f128_pd(_t45_105, _t45_105, 32), 15), _t45_106);
  _t45_109 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_105, _t45_105, 49), _mm256_permute2f128_pd(_t45_105, _t45_105, 49), 0), _t45_106);
  _t45_110 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_105, _t45_105, 49), _mm256_permute2f128_pd(_t45_105, _t45_105, 49), 15), _t45_106);

  // 4-BLAC: 4x4 - 4x4
  _t45_111 = _mm256_sub_pd(_t45_100, _t45_107);
  _t45_112 = _mm256_sub_pd(_t45_101, _t45_108);
  _t45_113 = _mm256_sub_pd(_t45_102, _t45_109);
  _t45_114 = _mm256_sub_pd(_t45_103, _t45_110);

  // AVX Storer:

  // 4x4 -> 3x3 - UpTriang
  _t45_3 = _t45_111;
  _t45_4 = _t45_112;
  _t45_5 = _t45_113;

  // Generating : U[12,12] = S(h(1, 12, 1), Sqrt( G(h(1, 12, 1), U[12,12],h(1, 12, 1)) ),h(1, 12, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_115 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_3, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_116 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_115)));

  // AVX Storer:
  _t45_6 = _t45_116;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 1), U[12,12],h(1, 12, 1)) ),h(1, 12, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_117 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_118 = _t45_6;

  // 4-BLAC: 1x4 / 1x4
  _t45_119 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_117), _mm256_castpd256_pd128(_t45_118)));

  // AVX Storer:
  _t45_7 = _t45_119;

  // Generating : U[12,12] = S(h(1, 12, 1), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), U[12,12],h(2, 12, 2)) ),h(2, 12, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_120 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_121 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_3, 6), _mm256_permute2f128_pd(_t45_3, _t45_3, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_122 = _mm256_mul_pd(_t45_120, _t45_121);

  // AVX Storer:
  _t45_8 = _t45_122;

  // Generating : U[12,12] = S(h(2, 12, 2), ( G(h(2, 12, 2), U[12,12],h(2, 12, 2)) - ( T( G(h(1, 12, 1), U[12,12],h(2, 12, 2)) ) * G(h(1, 12, 1), U[12,12],h(2, 12, 2)) ) ),h(2, 12, 2))

  // AVX Loader:

  // 2x2 -> 4x4 - UpperTriang
  _t45_123 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_4, 6), _mm256_permute2f128_pd(_t45_4, _t45_4, 129), 5);
  _t45_124 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_5, 4), _mm256_permute2f128_pd(_t45_5, _t45_5, 129), 5);
  _t45_125 = _mm256_setzero_pd();
  _t45_126 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_127 = _t45_8;

  // 4-BLAC: (1x4)^T
  _t45_128 = _t45_127;

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_129 = _t45_8;

  // 4-BLAC: 4x1 * 1x4
  _t45_130 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_128, _t45_128, 32), _mm256_permute2f128_pd(_t45_128, _t45_128, 32), 0), _t45_129);
  _t45_131 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_128, _t45_128, 32), _mm256_permute2f128_pd(_t45_128, _t45_128, 32), 15), _t45_129);
  _t45_132 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_128, _t45_128, 49), _mm256_permute2f128_pd(_t45_128, _t45_128, 49), 0), _t45_129);
  _t45_133 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_128, _t45_128, 49), _mm256_permute2f128_pd(_t45_128, _t45_128, 49), 15), _t45_129);

  // 4-BLAC: 4x4 - 4x4
  _t45_134 = _mm256_sub_pd(_t45_123, _t45_130);
  _t45_135 = _mm256_sub_pd(_t45_124, _t45_131);
  _t45_136 = _mm256_sub_pd(_t45_125, _t45_132);
  _t45_137 = _mm256_sub_pd(_t45_126, _t45_133);

  // AVX Storer:

  // 4x4 -> 2x2 - UpTriang
  _t45_9 = _t45_134;
  _t45_10 = _t45_135;

  // Generating : U[12,12] = S(h(1, 12, 2), Sqrt( G(h(1, 12, 2), U[12,12],h(1, 12, 2)) ),h(1, 12, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_138 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_9, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_139 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_138)));

  // AVX Storer:
  _t45_11 = _t45_139;

  // Generating : U[12,12] = S(h(1, 12, 2), ( G(h(1, 12, 2), U[12,12],h(1, 12, 3)) Div G(h(1, 12, 2), U[12,12],h(1, 12, 2)) ),h(1, 12, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_140 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_141 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t45_142 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_140), _mm256_castpd256_pd128(_t45_141)));

  // AVX Storer:
  _t45_12 = _t45_142;

  // Generating : U[12,12] = S(h(1, 12, 3), ( G(h(1, 12, 3), U[12,12],h(1, 12, 3)) - ( T( G(h(1, 12, 2), U[12,12],h(1, 12, 3)) ) Kro G(h(1, 12, 2), U[12,12],h(1, 12, 3)) ) ),h(1, 12, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_143 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_144 = _t45_12;

  // 4-BLAC: (4x1)^T
  _t45_145 = _t45_144;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_146 = _t45_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_147 = _mm256_mul_pd(_t45_145, _t45_146);

  // 4-BLAC: 1x4 - 1x4
  _t45_148 = _mm256_sub_pd(_t45_143, _t45_147);

  // AVX Storer:
  _t45_13 = _t45_148;

  // Generating : U[12,12] = S(h(1, 12, 3), Sqrt( G(h(1, 12, 3), U[12,12],h(1, 12, 3)) ),h(1, 12, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_149 = _t45_13;

  // 4-BLAC: sqrt(1x4)
  _t45_150 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_149)));

  // AVX Storer:
  _t45_13 = _t45_150;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 2), U[12,12],h(1, 12, 2)) ),h(1, 12, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_151 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_152 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t45_153 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_151), _mm256_castpd256_pd128(_t45_152)));

  // AVX Storer:
  _t45_14 = _t45_153;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 3), U[12,12],h(1, 12, 3)) ),h(1, 12, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_154 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_155 = _t45_13;

  // 4-BLAC: 1x4 / 1x4
  _t45_156 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_154), _mm256_castpd256_pd128(_t45_155)));

  // AVX Storer:
  _t45_15 = _t45_156;

  // Generating : U[12,12] = S(h(1, 12, 0), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_157 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_57 = _mm256_mul_pd(_t45_157, _t45_57);

  // AVX Storer:

  // Generating : U[12,12] = S(h(3, 12, 1), ( G(h(3, 12, 1), U[12,12],h(4, 12, fi96 + 4)) - ( T( G(h(1, 12, 0), U[12,12],h(3, 12, 1)) ) * G(h(1, 12, 0), U[12,12],h(4, 12, fi96 + 4)) ) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t45_158 = _t45_16;
  _t45_159 = _t45_17;
  _t45_160 = _t45_18;
  _t45_161 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_162 = _t45_2;

  // 4-BLAC: (1x4)^T
  _t45_163 = _t45_162;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 32), _mm256_permute2f128_pd(_t45_163, _t45_163, 32), 0), _t45_57);
  _t45_165 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 32), _mm256_permute2f128_pd(_t45_163, _t45_163, 32), 15), _t45_57);
  _t45_166 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 49), _mm256_permute2f128_pd(_t45_163, _t45_163, 49), 0), _t45_57);
  _t45_167 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 49), _mm256_permute2f128_pd(_t45_163, _t45_163, 49), 15), _t45_57);

  // 4-BLAC: 4x4 - 4x4
  _t45_168 = _mm256_sub_pd(_t45_158, _t45_164);
  _t45_169 = _mm256_sub_pd(_t45_159, _t45_165);
  _t45_170 = _mm256_sub_pd(_t45_160, _t45_166);
  _t45_171 = _mm256_sub_pd(_t45_161, _t45_167);

  // AVX Storer:
  _t45_16 = _t45_168;
  _t45_17 = _t45_169;
  _t45_18 = _t45_170;

  // Generating : U[12,12] = S(h(1, 12, 1), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_172 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_16 = _mm256_mul_pd(_t45_172, _t45_16);

  // AVX Storer:

  // Generating : U[12,12] = S(h(2, 12, 2), ( G(h(2, 12, 2), U[12,12],h(4, 12, fi96 + 4)) - ( T( G(h(1, 12, 1), U[12,12],h(2, 12, 2)) ) * G(h(1, 12, 1), U[12,12],h(4, 12, fi96 + 4)) ) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t45_173 = _t45_17;
  _t45_174 = _t45_18;
  _t45_175 = _mm256_setzero_pd();
  _t45_176 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_177 = _t45_8;

  // 4-BLAC: (1x4)^T
  _t45_178 = _t45_177;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_179 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 32), _mm256_permute2f128_pd(_t45_178, _t45_178, 32), 0), _t45_16);
  _t45_180 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 32), _mm256_permute2f128_pd(_t45_178, _t45_178, 32), 15), _t45_16);
  _t45_181 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 49), _mm256_permute2f128_pd(_t45_178, _t45_178, 49), 0), _t45_16);
  _t45_182 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 49), _mm256_permute2f128_pd(_t45_178, _t45_178, 49), 15), _t45_16);

  // 4-BLAC: 4x4 - 4x4
  _t45_183 = _mm256_sub_pd(_t45_173, _t45_179);
  _t45_184 = _mm256_sub_pd(_t45_174, _t45_180);
  _t45_185 = _mm256_sub_pd(_t45_175, _t45_181);
  _t45_186 = _mm256_sub_pd(_t45_176, _t45_182);

  // AVX Storer:
  _t45_17 = _t45_183;
  _t45_18 = _t45_184;

  // Generating : U[12,12] = S(h(1, 12, 2), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 2)) Kro G(h(1, 12, 2), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_187 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_14, _t45_14, 32), _mm256_permute2f128_pd(_t45_14, _t45_14, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_17 = _mm256_mul_pd(_t45_187, _t45_17);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 3), ( G(h(1, 12, 3), U[12,12],h(4, 12, fi96 + 4)) - ( T( G(h(1, 12, 2), U[12,12],h(1, 12, 3)) ) Kro G(h(1, 12, 2), U[12,12],h(4, 12, fi96 + 4)) ) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_188 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_12, _t45_12, 32), _mm256_permute2f128_pd(_t45_12, _t45_12, 32), 0);

  // 4-BLAC: (4x1)^T
  _t45_189 = _t45_188;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_59 = _mm256_mul_pd(_t45_189, _t45_17);

  // 4-BLAC: 1x4 - 1x4
  _t45_18 = _mm256_sub_pd(_t45_18, _t45_59);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 3), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 3)) Kro G(h(1, 12, 3), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_190 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_15, _t45_15, 32), _mm256_permute2f128_pd(_t45_15, _t45_15, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_18 = _mm256_mul_pd(_t45_190, _t45_18);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 0), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_191 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_58 = _mm256_mul_pd(_t45_191, _t45_58);

  // AVX Storer:

  // Generating : U[12,12] = S(h(3, 12, 1), ( G(h(3, 12, 1), U[12,12],h(4, 12, fi96 + 4)) - ( T( G(h(1, 12, 0), U[12,12],h(3, 12, 1)) ) * G(h(1, 12, 0), U[12,12],h(4, 12, fi96 + 4)) ) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t45_192 = _t45_19;
  _t45_193 = _t45_20;
  _t45_194 = _t45_21;
  _t45_195 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_196 = _t45_2;

  // 4-BLAC: (1x4)^T
  _t45_163 = _t45_196;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_164 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 32), _mm256_permute2f128_pd(_t45_163, _t45_163, 32), 0), _t45_58);
  _t45_165 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 32), _mm256_permute2f128_pd(_t45_163, _t45_163, 32), 15), _t45_58);
  _t45_166 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 49), _mm256_permute2f128_pd(_t45_163, _t45_163, 49), 0), _t45_58);
  _t45_167 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_163, _t45_163, 49), _mm256_permute2f128_pd(_t45_163, _t45_163, 49), 15), _t45_58);

  // 4-BLAC: 4x4 - 4x4
  _t45_197 = _mm256_sub_pd(_t45_192, _t45_164);
  _t45_198 = _mm256_sub_pd(_t45_193, _t45_165);
  _t45_199 = _mm256_sub_pd(_t45_194, _t45_166);
  _t45_200 = _mm256_sub_pd(_t45_195, _t45_167);

  // AVX Storer:
  _t45_19 = _t45_197;
  _t45_20 = _t45_198;
  _t45_21 = _t45_199;

  // Generating : U[12,12] = S(h(1, 12, 1), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_201 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_19 = _mm256_mul_pd(_t45_201, _t45_19);

  // AVX Storer:

  // Generating : U[12,12] = S(h(2, 12, 2), ( G(h(2, 12, 2), U[12,12],h(4, 12, fi96 + 4)) - ( T( G(h(1, 12, 1), U[12,12],h(2, 12, 2)) ) * G(h(1, 12, 1), U[12,12],h(4, 12, fi96 + 4)) ) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t45_202 = _t45_20;
  _t45_203 = _t45_21;
  _t45_204 = _mm256_setzero_pd();
  _t45_205 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_206 = _t45_8;

  // 4-BLAC: (1x4)^T
  _t45_178 = _t45_206;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_179 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 32), _mm256_permute2f128_pd(_t45_178, _t45_178, 32), 0), _t45_19);
  _t45_180 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 32), _mm256_permute2f128_pd(_t45_178, _t45_178, 32), 15), _t45_19);
  _t45_181 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 49), _mm256_permute2f128_pd(_t45_178, _t45_178, 49), 0), _t45_19);
  _t45_182 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_178, _t45_178, 49), _mm256_permute2f128_pd(_t45_178, _t45_178, 49), 15), _t45_19);

  // 4-BLAC: 4x4 - 4x4
  _t45_207 = _mm256_sub_pd(_t45_202, _t45_179);
  _t45_208 = _mm256_sub_pd(_t45_203, _t45_180);
  _t45_209 = _mm256_sub_pd(_t45_204, _t45_181);
  _t45_210 = _mm256_sub_pd(_t45_205, _t45_182);

  // AVX Storer:
  _t45_20 = _t45_207;
  _t45_21 = _t45_208;

  // Generating : U[12,12] = S(h(1, 12, 2), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 2)) Kro G(h(1, 12, 2), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_211 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_14, _t45_14, 32), _mm256_permute2f128_pd(_t45_14, _t45_14, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_20 = _mm256_mul_pd(_t45_211, _t45_20);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 3), ( G(h(1, 12, 3), U[12,12],h(4, 12, fi96 + 4)) - ( T( G(h(1, 12, 2), U[12,12],h(1, 12, 3)) ) Kro G(h(1, 12, 2), U[12,12],h(4, 12, fi96 + 4)) ) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_212 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_12, _t45_12, 32), _mm256_permute2f128_pd(_t45_12, _t45_12, 32), 0);

  // 4-BLAC: (4x1)^T
  _t45_189 = _t45_212;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_59 = _mm256_mul_pd(_t45_189, _t45_20);

  // 4-BLAC: 1x4 - 1x4
  _t45_21 = _mm256_sub_pd(_t45_21, _t45_59);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 3), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 3)) Kro G(h(1, 12, 3), U[12,12],h(4, 12, fi96 + 4)) ),h(4, 12, fi96 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_213 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_15, _t45_15, 32), _mm256_permute2f128_pd(_t45_15, _t45_15, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_21 = _mm256_mul_pd(_t45_213, _t45_21);

  // AVX Storer:

  // Generating : U[12,12] = ( ( S(h(4, 12, 4), ( G(h(4, 12, 4), U[12,12],h(4, 12, 4)) - ( T( G(h(4, 12, 0), U[12,12],h(4, 12, 4)) ) * G(h(4, 12, 0), U[12,12],h(4, 12, 4)) ) ),h(4, 12, 4)) + S(h(4, 12, 4), ( G(h(4, 12, 4), U[12,12],h(4, 12, 8)) - ( T( G(h(4, 12, 0), U[12,12],h(4, 12, 4)) ) * G(h(4, 12, 0), U[12,12],h(4, 12, 8)) ) ),h(4, 12, 8)) ) + S(h(4, 12, 8), ( G(h(4, 12, 8), U[12,12],h(4, 12, 8)) - ( T( G(h(4, 12, 0), U[12,12],h(4, 12, 8)) ) * G(h(4, 12, 0), U[12,12],h(4, 12, 8)) ) ),h(4, 12, 8)) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t45_214 = _t45_22;
  _t45_215 = _t45_23;
  _t45_216 = _t45_24;
  _t45_217 = _t45_25;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t45_384 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_57, _t45_16), _mm256_unpacklo_pd(_t45_17, _t45_18), 32);
  _t45_385 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_57, _t45_16), _mm256_unpackhi_pd(_t45_17, _t45_18), 32);
  _t45_386 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_57, _t45_16), _mm256_unpacklo_pd(_t45_17, _t45_18), 49);
  _t45_387 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_57, _t45_16), _mm256_unpackhi_pd(_t45_17, _t45_18), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t45_60 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_384, _t45_384, 32), _mm256_permute2f128_pd(_t45_384, _t45_384, 32), 0), _t45_57), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_384, _t45_384, 32), _mm256_permute2f128_pd(_t45_384, _t45_384, 32), 15), _t45_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_384, _t45_384, 49), _mm256_permute2f128_pd(_t45_384, _t45_384, 49), 0), _t45_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_384, _t45_384, 49), _mm256_permute2f128_pd(_t45_384, _t45_384, 49), 15), _t45_18)));
  _t45_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_385, _t45_385, 32), _mm256_permute2f128_pd(_t45_385, _t45_385, 32), 0), _t45_57), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_385, _t45_385, 32), _mm256_permute2f128_pd(_t45_385, _t45_385, 32), 15), _t45_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_385, _t45_385, 49), _mm256_permute2f128_pd(_t45_385, _t45_385, 49), 0), _t45_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_385, _t45_385, 49), _mm256_permute2f128_pd(_t45_385, _t45_385, 49), 15), _t45_18)));
  _t45_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_386, _t45_386, 32), _mm256_permute2f128_pd(_t45_386, _t45_386, 32), 0), _t45_57), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_386, _t45_386, 32), _mm256_permute2f128_pd(_t45_386, _t45_386, 32), 15), _t45_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_386, _t45_386, 49), _mm256_permute2f128_pd(_t45_386, _t45_386, 49), 0), _t45_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_386, _t45_386, 49), _mm256_permute2f128_pd(_t45_386, _t45_386, 49), 15), _t45_18)));
  _t45_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_387, _t45_387, 32), _mm256_permute2f128_pd(_t45_387, _t45_387, 32), 0), _t45_57), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_387, _t45_387, 32), _mm256_permute2f128_pd(_t45_387, _t45_387, 32), 15), _t45_16)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_387, _t45_387, 49), _mm256_permute2f128_pd(_t45_387, _t45_387, 49), 0), _t45_17), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_387, _t45_387, 49), _mm256_permute2f128_pd(_t45_387, _t45_387, 49), 15), _t45_18)));

  // 4-BLAC: 4x4 - 4x4
  _t45_76 = _mm256_sub_pd(_t45_214, _t45_60);
  _t45_77 = _mm256_sub_pd(_t45_215, _t45_61);
  _t45_78 = _mm256_sub_pd(_t45_216, _t45_62);
  _t45_79 = _mm256_sub_pd(_t45_217, _t45_63);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t45_22 = _t45_76;
  _t45_23 = _t45_77;
  _t45_24 = _t45_78;
  _t45_25 = _t45_79;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t45_388 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_57, _t45_16), _mm256_unpacklo_pd(_t45_17, _t45_18), 32);
  _t45_389 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_57, _t45_16), _mm256_unpackhi_pd(_t45_17, _t45_18), 32);
  _t45_390 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_57, _t45_16), _mm256_unpacklo_pd(_t45_17, _t45_18), 49);
  _t45_391 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_57, _t45_16), _mm256_unpackhi_pd(_t45_17, _t45_18), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t45_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_388, _t45_388, 32), _mm256_permute2f128_pd(_t45_388, _t45_388, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_388, _t45_388, 32), _mm256_permute2f128_pd(_t45_388, _t45_388, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_388, _t45_388, 49), _mm256_permute2f128_pd(_t45_388, _t45_388, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_388, _t45_388, 49), _mm256_permute2f128_pd(_t45_388, _t45_388, 49), 15), _t45_21)));
  _t45_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_389, _t45_389, 32), _mm256_permute2f128_pd(_t45_389, _t45_389, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_389, _t45_389, 32), _mm256_permute2f128_pd(_t45_389, _t45_389, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_389, _t45_389, 49), _mm256_permute2f128_pd(_t45_389, _t45_389, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_389, _t45_389, 49), _mm256_permute2f128_pd(_t45_389, _t45_389, 49), 15), _t45_21)));
  _t45_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_390, _t45_390, 32), _mm256_permute2f128_pd(_t45_390, _t45_390, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_390, _t45_390, 32), _mm256_permute2f128_pd(_t45_390, _t45_390, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_390, _t45_390, 49), _mm256_permute2f128_pd(_t45_390, _t45_390, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_390, _t45_390, 49), _mm256_permute2f128_pd(_t45_390, _t45_390, 49), 15), _t45_21)));
  _t45_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_391, _t45_391, 32), _mm256_permute2f128_pd(_t45_391, _t45_391, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_391, _t45_391, 32), _mm256_permute2f128_pd(_t45_391, _t45_391, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_391, _t45_391, 49), _mm256_permute2f128_pd(_t45_391, _t45_391, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_391, _t45_391, 49), _mm256_permute2f128_pd(_t45_391, _t45_391, 49), 15), _t45_21)));

  // 4-BLAC: 4x4 - 4x4
  _t45_80 = _mm256_sub_pd(_t45_80, _t45_64);
  _t45_81 = _mm256_sub_pd(_t45_81, _t45_65);
  _t45_82 = _mm256_sub_pd(_t45_82, _t45_66);
  _t45_83 = _mm256_sub_pd(_t45_83, _t45_67);

  // AVX Storer:

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t45_218 = _t41_28;
  _t45_219 = _t41_29;
  _t45_220 = _t41_30;
  _t45_221 = _t41_31;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t45_392 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_58, _t45_19), _mm256_unpacklo_pd(_t45_20, _t45_21), 32);
  _t45_393 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_58, _t45_19), _mm256_unpackhi_pd(_t45_20, _t45_21), 32);
  _t45_394 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_58, _t45_19), _mm256_unpacklo_pd(_t45_20, _t45_21), 49);
  _t45_395 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_58, _t45_19), _mm256_unpackhi_pd(_t45_20, _t45_21), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t45_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_392, _t45_392, 32), _mm256_permute2f128_pd(_t45_392, _t45_392, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_392, _t45_392, 32), _mm256_permute2f128_pd(_t45_392, _t45_392, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_392, _t45_392, 49), _mm256_permute2f128_pd(_t45_392, _t45_392, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_392, _t45_392, 49), _mm256_permute2f128_pd(_t45_392, _t45_392, 49), 15), _t45_21)));
  _t45_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_393, _t45_393, 32), _mm256_permute2f128_pd(_t45_393, _t45_393, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_393, _t45_393, 32), _mm256_permute2f128_pd(_t45_393, _t45_393, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_393, _t45_393, 49), _mm256_permute2f128_pd(_t45_393, _t45_393, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_393, _t45_393, 49), _mm256_permute2f128_pd(_t45_393, _t45_393, 49), 15), _t45_21)));
  _t45_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_394, _t45_394, 32), _mm256_permute2f128_pd(_t45_394, _t45_394, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_394, _t45_394, 32), _mm256_permute2f128_pd(_t45_394, _t45_394, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_394, _t45_394, 49), _mm256_permute2f128_pd(_t45_394, _t45_394, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_394, _t45_394, 49), _mm256_permute2f128_pd(_t45_394, _t45_394, 49), 15), _t45_21)));
  _t45_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_395, _t45_395, 32), _mm256_permute2f128_pd(_t45_395, _t45_395, 32), 0), _t45_58), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_395, _t45_395, 32), _mm256_permute2f128_pd(_t45_395, _t45_395, 32), 15), _t45_19)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_395, _t45_395, 49), _mm256_permute2f128_pd(_t45_395, _t45_395, 49), 0), _t45_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_395, _t45_395, 49), _mm256_permute2f128_pd(_t45_395, _t45_395, 49), 15), _t45_21)));

  // 4-BLAC: 4x4 - 4x4
  _t45_84 = _mm256_sub_pd(_t45_218, _t45_68);
  _t45_85 = _mm256_sub_pd(_t45_219, _t45_69);
  _t45_86 = _mm256_sub_pd(_t45_220, _t45_70);
  _t45_87 = _mm256_sub_pd(_t45_221, _t45_71);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t41_28 = _t45_84;
  _t41_29 = _t45_85;
  _t41_30 = _t45_86;
  _t41_31 = _t45_87;

  // Generating : U[12,12] = S(h(1, 12, 4), Sqrt( G(h(1, 12, 4), U[12,12],h(1, 12, 4)) ),h(1, 12, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_222 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_22, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_223 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_222)));

  // AVX Storer:
  _t45_26 = _t45_223;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 4), U[12,12],h(1, 12, 4)) ),h(1, 12, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_224 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_225 = _t45_26;

  // 4-BLAC: 1x4 / 1x4
  _t45_226 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_224), _mm256_castpd256_pd128(_t45_225)));

  // AVX Storer:
  _t45_27 = _t45_226;

  // Generating : U[12,12] = S(h(1, 12, 4), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 4)) Kro G(h(1, 12, 4), U[12,12],h(3, 12, 5)) ),h(3, 12, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_227 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_27, _t45_27, 32), _mm256_permute2f128_pd(_t45_27, _t45_27, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_228 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_22, 14), _mm256_permute2f128_pd(_t45_22, _t45_22, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_229 = _mm256_mul_pd(_t45_227, _t45_228);

  // AVX Storer:
  _t45_28 = _t45_229;

  // Generating : U[12,12] = S(h(3, 12, 5), ( G(h(3, 12, 5), U[12,12],h(3, 12, 5)) - ( T( G(h(1, 12, 4), U[12,12],h(3, 12, 5)) ) * G(h(1, 12, 4), U[12,12],h(3, 12, 5)) ) ),h(3, 12, 5))

  // AVX Loader:

  // 3x3 -> 4x4 - UpperTriang
  _t45_230 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_23, 14), _mm256_permute2f128_pd(_t45_23, _t45_23, 129), 5);
  _t45_231 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_24, 12), _mm256_permute2f128_pd(_t45_24, _t45_24, 129), 5);
  _t45_232 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_25, 8), _mm256_setzero_pd());
  _t45_233 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_234 = _t45_28;

  // 4-BLAC: (1x4)^T
  _t45_235 = _t45_234;

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_236 = _t45_28;

  // 4-BLAC: 4x1 * 1x4
  _t45_237 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_235, _t45_235, 32), _mm256_permute2f128_pd(_t45_235, _t45_235, 32), 0), _t45_236);
  _t45_238 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_235, _t45_235, 32), _mm256_permute2f128_pd(_t45_235, _t45_235, 32), 15), _t45_236);
  _t45_239 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_235, _t45_235, 49), _mm256_permute2f128_pd(_t45_235, _t45_235, 49), 0), _t45_236);
  _t45_240 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_235, _t45_235, 49), _mm256_permute2f128_pd(_t45_235, _t45_235, 49), 15), _t45_236);

  // 4-BLAC: 4x4 - 4x4
  _t45_241 = _mm256_sub_pd(_t45_230, _t45_237);
  _t45_242 = _mm256_sub_pd(_t45_231, _t45_238);
  _t45_243 = _mm256_sub_pd(_t45_232, _t45_239);
  _t45_244 = _mm256_sub_pd(_t45_233, _t45_240);

  // AVX Storer:

  // 4x4 -> 3x3 - UpTriang
  _t45_29 = _t45_241;
  _t45_30 = _t45_242;
  _t45_31 = _t45_243;

  // Generating : U[12,12] = S(h(1, 12, 5), Sqrt( G(h(1, 12, 5), U[12,12],h(1, 12, 5)) ),h(1, 12, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_245 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_29, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_246 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_245)));

  // AVX Storer:
  _t45_32 = _t45_246;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 5), U[12,12],h(1, 12, 5)) ),h(1, 12, 5))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_247 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_248 = _t45_32;

  // 4-BLAC: 1x4 / 1x4
  _t45_249 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_247), _mm256_castpd256_pd128(_t45_248)));

  // AVX Storer:
  _t45_33 = _t45_249;

  // Generating : U[12,12] = S(h(1, 12, 5), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 5)) Kro G(h(1, 12, 5), U[12,12],h(2, 12, 6)) ),h(2, 12, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_250 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_33, _t45_33, 32), _mm256_permute2f128_pd(_t45_33, _t45_33, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_251 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_29, 6), _mm256_permute2f128_pd(_t45_29, _t45_29, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_252 = _mm256_mul_pd(_t45_250, _t45_251);

  // AVX Storer:
  _t45_34 = _t45_252;

  // Generating : U[12,12] = S(h(2, 12, 6), ( G(h(2, 12, 6), U[12,12],h(2, 12, 6)) - ( T( G(h(1, 12, 5), U[12,12],h(2, 12, 6)) ) * G(h(1, 12, 5), U[12,12],h(2, 12, 6)) ) ),h(2, 12, 6))

  // AVX Loader:

  // 2x2 -> 4x4 - UpperTriang
  _t45_253 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_30, 6), _mm256_permute2f128_pd(_t45_30, _t45_30, 129), 5);
  _t45_254 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_31, 4), _mm256_permute2f128_pd(_t45_31, _t45_31, 129), 5);
  _t45_255 = _mm256_setzero_pd();
  _t45_256 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_257 = _t45_34;

  // 4-BLAC: (1x4)^T
  _t45_258 = _t45_257;

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_259 = _t45_34;

  // 4-BLAC: 4x1 * 1x4
  _t45_260 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_258, _t45_258, 32), _mm256_permute2f128_pd(_t45_258, _t45_258, 32), 0), _t45_259);
  _t45_261 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_258, _t45_258, 32), _mm256_permute2f128_pd(_t45_258, _t45_258, 32), 15), _t45_259);
  _t45_262 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_258, _t45_258, 49), _mm256_permute2f128_pd(_t45_258, _t45_258, 49), 0), _t45_259);
  _t45_263 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_258, _t45_258, 49), _mm256_permute2f128_pd(_t45_258, _t45_258, 49), 15), _t45_259);

  // 4-BLAC: 4x4 - 4x4
  _t45_264 = _mm256_sub_pd(_t45_253, _t45_260);
  _t45_265 = _mm256_sub_pd(_t45_254, _t45_261);
  _t45_266 = _mm256_sub_pd(_t45_255, _t45_262);
  _t45_267 = _mm256_sub_pd(_t45_256, _t45_263);

  // AVX Storer:

  // 4x4 -> 2x2 - UpTriang
  _t45_35 = _t45_264;
  _t45_36 = _t45_265;

  // Generating : U[12,12] = S(h(1, 12, 6), Sqrt( G(h(1, 12, 6), U[12,12],h(1, 12, 6)) ),h(1, 12, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_268 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_35, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_269 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_268)));

  // AVX Storer:
  _t45_37 = _t45_269;

  // Generating : U[12,12] = S(h(1, 12, 6), ( G(h(1, 12, 6), U[12,12],h(1, 12, 7)) Div G(h(1, 12, 6), U[12,12],h(1, 12, 6)) ),h(1, 12, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_270 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_35, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_271 = _t45_37;

  // 4-BLAC: 1x4 / 1x4
  _t45_272 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_270), _mm256_castpd256_pd128(_t45_271)));

  // AVX Storer:
  _t45_38 = _t45_272;

  // Generating : U[12,12] = S(h(1, 12, 7), ( G(h(1, 12, 7), U[12,12],h(1, 12, 7)) - ( T( G(h(1, 12, 6), U[12,12],h(1, 12, 7)) ) Kro G(h(1, 12, 6), U[12,12],h(1, 12, 7)) ) ),h(1, 12, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_273 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_36, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_274 = _t45_38;

  // 4-BLAC: (4x1)^T
  _t45_275 = _t45_274;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_276 = _t45_38;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_277 = _mm256_mul_pd(_t45_275, _t45_276);

  // 4-BLAC: 1x4 - 1x4
  _t45_278 = _mm256_sub_pd(_t45_273, _t45_277);

  // AVX Storer:
  _t45_39 = _t45_278;

  // Generating : U[12,12] = S(h(1, 12, 7), Sqrt( G(h(1, 12, 7), U[12,12],h(1, 12, 7)) ),h(1, 12, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_279 = _t45_39;

  // 4-BLAC: sqrt(1x4)
  _t45_280 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_279)));

  // AVX Storer:
  _t45_39 = _t45_280;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 6), U[12,12],h(1, 12, 6)) ),h(1, 12, 6))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_281 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_282 = _t45_37;

  // 4-BLAC: 1x4 / 1x4
  _t45_283 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_281), _mm256_castpd256_pd128(_t45_282)));

  // AVX Storer:
  _t45_40 = _t45_283;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 7), U[12,12],h(1, 12, 7)) ),h(1, 12, 7))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_284 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_285 = _t45_39;

  // 4-BLAC: 1x4 / 1x4
  _t45_286 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_284), _mm256_castpd256_pd128(_t45_285)));

  // AVX Storer:
  _t45_41 = _t45_286;

  // Generating : U[12,12] = S(h(1, 12, 4), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 4)) Kro G(h(1, 12, 4), U[12,12],h(4, 12, 8)) ),h(4, 12, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_287 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_27, _t45_27, 32), _mm256_permute2f128_pd(_t45_27, _t45_27, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_80 = _mm256_mul_pd(_t45_287, _t45_80);

  // AVX Storer:

  // Generating : U[12,12] = S(h(3, 12, 5), ( G(h(3, 12, 5), U[12,12],h(4, 12, 8)) - ( T( G(h(1, 12, 4), U[12,12],h(3, 12, 5)) ) * G(h(1, 12, 4), U[12,12],h(4, 12, 8)) ) ),h(4, 12, 8))

  // AVX Loader:

  // 3x4 -> 4x4
  _t45_288 = _t45_81;
  _t45_289 = _t45_82;
  _t45_290 = _t45_83;
  _t45_291 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_292 = _t45_28;

  // 4-BLAC: (1x4)^T
  _t45_293 = _t45_292;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_294 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_293, _t45_293, 32), _mm256_permute2f128_pd(_t45_293, _t45_293, 32), 0), _t45_80);
  _t45_295 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_293, _t45_293, 32), _mm256_permute2f128_pd(_t45_293, _t45_293, 32), 15), _t45_80);
  _t45_296 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_293, _t45_293, 49), _mm256_permute2f128_pd(_t45_293, _t45_293, 49), 0), _t45_80);
  _t45_297 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_293, _t45_293, 49), _mm256_permute2f128_pd(_t45_293, _t45_293, 49), 15), _t45_80);

  // 4-BLAC: 4x4 - 4x4
  _t45_298 = _mm256_sub_pd(_t45_288, _t45_294);
  _t45_299 = _mm256_sub_pd(_t45_289, _t45_295);
  _t45_300 = _mm256_sub_pd(_t45_290, _t45_296);
  _t45_301 = _mm256_sub_pd(_t45_291, _t45_297);

  // AVX Storer:
  _t45_81 = _t45_298;
  _t45_82 = _t45_299;
  _t45_83 = _t45_300;

  // Generating : U[12,12] = S(h(1, 12, 5), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 5)) Kro G(h(1, 12, 5), U[12,12],h(4, 12, 8)) ),h(4, 12, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_302 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_33, _t45_33, 32), _mm256_permute2f128_pd(_t45_33, _t45_33, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_81 = _mm256_mul_pd(_t45_302, _t45_81);

  // AVX Storer:

  // Generating : U[12,12] = S(h(2, 12, 6), ( G(h(2, 12, 6), U[12,12],h(4, 12, 8)) - ( T( G(h(1, 12, 5), U[12,12],h(2, 12, 6)) ) * G(h(1, 12, 5), U[12,12],h(4, 12, 8)) ) ),h(4, 12, 8))

  // AVX Loader:

  // 2x4 -> 4x4
  _t45_303 = _t45_82;
  _t45_304 = _t45_83;
  _t45_305 = _mm256_setzero_pd();
  _t45_306 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_307 = _t45_34;

  // 4-BLAC: (1x4)^T
  _t45_308 = _t45_307;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_309 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_308, _t45_308, 32), _mm256_permute2f128_pd(_t45_308, _t45_308, 32), 0), _t45_81);
  _t45_310 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_308, _t45_308, 32), _mm256_permute2f128_pd(_t45_308, _t45_308, 32), 15), _t45_81);
  _t45_311 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_308, _t45_308, 49), _mm256_permute2f128_pd(_t45_308, _t45_308, 49), 0), _t45_81);
  _t45_312 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_308, _t45_308, 49), _mm256_permute2f128_pd(_t45_308, _t45_308, 49), 15), _t45_81);

  // 4-BLAC: 4x4 - 4x4
  _t45_313 = _mm256_sub_pd(_t45_303, _t45_309);
  _t45_314 = _mm256_sub_pd(_t45_304, _t45_310);
  _t45_315 = _mm256_sub_pd(_t45_305, _t45_311);
  _t45_316 = _mm256_sub_pd(_t45_306, _t45_312);

  // AVX Storer:
  _t45_82 = _t45_313;
  _t45_83 = _t45_314;

  // Generating : U[12,12] = S(h(1, 12, 6), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 6)) Kro G(h(1, 12, 6), U[12,12],h(4, 12, 8)) ),h(4, 12, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_317 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_40, _t45_40, 32), _mm256_permute2f128_pd(_t45_40, _t45_40, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_82 = _mm256_mul_pd(_t45_317, _t45_82);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 7), ( G(h(1, 12, 7), U[12,12],h(4, 12, 8)) - ( T( G(h(1, 12, 6), U[12,12],h(1, 12, 7)) ) Kro G(h(1, 12, 6), U[12,12],h(4, 12, 8)) ) ),h(4, 12, 8))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_318 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_38, _t45_38, 32), _mm256_permute2f128_pd(_t45_38, _t45_38, 32), 0);

  // 4-BLAC: (4x1)^T
  _t45_319 = _t45_318;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_56 = _mm256_mul_pd(_t45_319, _t45_82);

  // 4-BLAC: 1x4 - 1x4
  _t45_83 = _mm256_sub_pd(_t45_83, _t45_56);

  // AVX Storer:

  // Generating : U[12,12] = S(h(1, 12, 7), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 7)) Kro G(h(1, 12, 7), U[12,12],h(4, 12, 8)) ),h(4, 12, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_320 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_41, _t45_41, 32), _mm256_permute2f128_pd(_t45_41, _t45_41, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_83 = _mm256_mul_pd(_t45_320, _t45_83);

  // AVX Storer:

  // Generating : U[12,12] = S(h(4, 12, 8), ( G(h(4, 12, 8), U[12,12],h(4, 12, 8)) - ( T( G(h(4, 12, 4), U[12,12],h(4, 12, 8)) ) * G(h(4, 12, 4), U[12,12],h(4, 12, 8)) ) ),h(4, 12, 8))

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t45_321 = _t41_28;
  _t45_322 = _t41_29;
  _t45_323 = _t41_30;
  _t45_324 = _t41_31;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t45_396 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_80, _t45_81), _mm256_unpacklo_pd(_t45_82, _t45_83), 32);
  _t45_397 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_80, _t45_81), _mm256_unpackhi_pd(_t45_82, _t45_83), 32);
  _t45_398 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_80, _t45_81), _mm256_unpacklo_pd(_t45_82, _t45_83), 49);
  _t45_399 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_80, _t45_81), _mm256_unpackhi_pd(_t45_82, _t45_83), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t45_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_396, _t45_396, 32), _mm256_permute2f128_pd(_t45_396, _t45_396, 32), 0), _t45_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_396, _t45_396, 32), _mm256_permute2f128_pd(_t45_396, _t45_396, 32), 15), _t45_81)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_396, _t45_396, 49), _mm256_permute2f128_pd(_t45_396, _t45_396, 49), 0), _t45_82), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_396, _t45_396, 49), _mm256_permute2f128_pd(_t45_396, _t45_396, 49), 15), _t45_83)));
  _t45_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_397, _t45_397, 32), _mm256_permute2f128_pd(_t45_397, _t45_397, 32), 0), _t45_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_397, _t45_397, 32), _mm256_permute2f128_pd(_t45_397, _t45_397, 32), 15), _t45_81)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_397, _t45_397, 49), _mm256_permute2f128_pd(_t45_397, _t45_397, 49), 0), _t45_82), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_397, _t45_397, 49), _mm256_permute2f128_pd(_t45_397, _t45_397, 49), 15), _t45_83)));
  _t45_74 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_398, _t45_398, 32), _mm256_permute2f128_pd(_t45_398, _t45_398, 32), 0), _t45_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_398, _t45_398, 32), _mm256_permute2f128_pd(_t45_398, _t45_398, 32), 15), _t45_81)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_398, _t45_398, 49), _mm256_permute2f128_pd(_t45_398, _t45_398, 49), 0), _t45_82), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_398, _t45_398, 49), _mm256_permute2f128_pd(_t45_398, _t45_398, 49), 15), _t45_83)));
  _t45_75 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_399, _t45_399, 32), _mm256_permute2f128_pd(_t45_399, _t45_399, 32), 0), _t45_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_399, _t45_399, 32), _mm256_permute2f128_pd(_t45_399, _t45_399, 32), 15), _t45_81)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_399, _t45_399, 49), _mm256_permute2f128_pd(_t45_399, _t45_399, 49), 0), _t45_82), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_399, _t45_399, 49), _mm256_permute2f128_pd(_t45_399, _t45_399, 49), 15), _t45_83)));

  // 4-BLAC: 4x4 - 4x4
  _t45_88 = _mm256_sub_pd(_t45_321, _t45_72);
  _t45_89 = _mm256_sub_pd(_t45_322, _t45_73);
  _t45_90 = _mm256_sub_pd(_t45_323, _t45_74);
  _t45_91 = _mm256_sub_pd(_t45_324, _t45_75);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t41_28 = _t45_88;
  _t41_29 = _t45_89;
  _t41_30 = _t45_90;
  _t41_31 = _t45_91;

  // Generating : U[12,12] = S(h(1, 12, 8), Sqrt( G(h(1, 12, 8), U[12,12],h(1, 12, 8)) ),h(1, 12, 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_325 = _mm256_blend_pd(_mm256_setzero_pd(), _t41_28, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_326 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_325)));

  // AVX Storer:
  _t45_42 = _t45_326;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 8), U[12,12],h(1, 12, 8)) ),h(1, 12, 8))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_327 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_328 = _t45_42;

  // 4-BLAC: 1x4 / 1x4
  _t45_329 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_327), _mm256_castpd256_pd128(_t45_328)));

  // AVX Storer:
  _t45_43 = _t45_329;

  // Generating : U[12,12] = S(h(1, 12, 8), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 8)) Kro G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ),h(3, 12, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_330 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_43, _t45_43, 32), _mm256_permute2f128_pd(_t45_43, _t45_43, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_331 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t41_28, 14), _mm256_permute2f128_pd(_t41_28, _t41_28, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_332 = _mm256_mul_pd(_t45_330, _t45_331);

  // AVX Storer:
  _t45_44 = _t45_332;

  // Generating : U[12,12] = S(h(3, 12, 9), ( G(h(3, 12, 9), U[12,12],h(3, 12, 9)) - ( T( G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ) * G(h(1, 12, 8), U[12,12],h(3, 12, 9)) ) ),h(3, 12, 9))

  // AVX Loader:

  // 3x3 -> 4x4 - UpperTriang
  _t45_333 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t41_29, 14), _mm256_permute2f128_pd(_t41_29, _t41_29, 129), 5);
  _t45_334 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t41_30, 12), _mm256_permute2f128_pd(_t41_30, _t41_30, 129), 5);
  _t45_335 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t41_31, 8), _mm256_setzero_pd());
  _t45_336 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_337 = _t45_44;

  // 4-BLAC: (1x4)^T
  _t45_338 = _t45_337;

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_339 = _t45_44;

  // 4-BLAC: 4x1 * 1x4
  _t45_340 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_338, _t45_338, 32), _mm256_permute2f128_pd(_t45_338, _t45_338, 32), 0), _t45_339);
  _t45_341 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_338, _t45_338, 32), _mm256_permute2f128_pd(_t45_338, _t45_338, 32), 15), _t45_339);
  _t45_342 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_338, _t45_338, 49), _mm256_permute2f128_pd(_t45_338, _t45_338, 49), 0), _t45_339);
  _t45_343 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_338, _t45_338, 49), _mm256_permute2f128_pd(_t45_338, _t45_338, 49), 15), _t45_339);

  // 4-BLAC: 4x4 - 4x4
  _t45_344 = _mm256_sub_pd(_t45_333, _t45_340);
  _t45_345 = _mm256_sub_pd(_t45_334, _t45_341);
  _t45_346 = _mm256_sub_pd(_t45_335, _t45_342);
  _t45_347 = _mm256_sub_pd(_t45_336, _t45_343);

  // AVX Storer:

  // 4x4 -> 3x3 - UpTriang
  _t45_45 = _t45_344;
  _t45_46 = _t45_345;
  _t45_47 = _t45_346;

  // Generating : U[12,12] = S(h(1, 12, 9), Sqrt( G(h(1, 12, 9), U[12,12],h(1, 12, 9)) ),h(1, 12, 9))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_348 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_45, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_349 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_348)));

  // AVX Storer:
  _t45_48 = _t45_349;

  // Generating : T1976[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 9), U[12,12],h(1, 12, 9)) ),h(1, 12, 9))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_350 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_351 = _t45_48;

  // 4-BLAC: 1x4 / 1x4
  _t45_352 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_350), _mm256_castpd256_pd128(_t45_351)));

  // AVX Storer:
  _t45_49 = _t45_352;

  // Generating : U[12,12] = S(h(1, 12, 9), ( G(h(1, 1, 0), T1976[1,12],h(1, 12, 9)) Kro G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ),h(2, 12, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_353 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_49, _t45_49, 32), _mm256_permute2f128_pd(_t45_49, _t45_49, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_354 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_45, 6), _mm256_permute2f128_pd(_t45_45, _t45_45, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_355 = _mm256_mul_pd(_t45_353, _t45_354);

  // AVX Storer:
  _t45_50 = _t45_355;

  // Generating : U[12,12] = S(h(2, 12, 10), ( G(h(2, 12, 10), U[12,12],h(2, 12, 10)) - ( T( G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ) * G(h(1, 12, 9), U[12,12],h(2, 12, 10)) ) ),h(2, 12, 10))

  // AVX Loader:

  // 2x2 -> 4x4 - UpperTriang
  _t45_356 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_46, 6), _mm256_permute2f128_pd(_t45_46, _t45_46, 129), 5);
  _t45_357 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_47, 4), _mm256_permute2f128_pd(_t45_47, _t45_47, 129), 5);
  _t45_358 = _mm256_setzero_pd();
  _t45_359 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_360 = _t45_50;

  // 4-BLAC: (1x4)^T
  _t45_361 = _t45_360;

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_362 = _t45_50;

  // 4-BLAC: 4x1 * 1x4
  _t45_363 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_361, _t45_361, 32), _mm256_permute2f128_pd(_t45_361, _t45_361, 32), 0), _t45_362);
  _t45_364 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_361, _t45_361, 32), _mm256_permute2f128_pd(_t45_361, _t45_361, 32), 15), _t45_362);
  _t45_365 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_361, _t45_361, 49), _mm256_permute2f128_pd(_t45_361, _t45_361, 49), 0), _t45_362);
  _t45_366 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_361, _t45_361, 49), _mm256_permute2f128_pd(_t45_361, _t45_361, 49), 15), _t45_362);

  // 4-BLAC: 4x4 - 4x4
  _t45_367 = _mm256_sub_pd(_t45_356, _t45_363);
  _t45_368 = _mm256_sub_pd(_t45_357, _t45_364);
  _t45_369 = _mm256_sub_pd(_t45_358, _t45_365);
  _t45_370 = _mm256_sub_pd(_t45_359, _t45_366);

  // AVX Storer:

  // 4x4 -> 2x2 - UpTriang
  _t45_51 = _t45_367;
  _t45_52 = _t45_368;

  // Generating : U[12,12] = S(h(1, 12, 10), Sqrt( G(h(1, 12, 10), U[12,12],h(1, 12, 10)) ),h(1, 12, 10))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_371 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_51, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_372 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_371)));

  // AVX Storer:
  _t45_53 = _t45_372;

  // Generating : U[12,12] = S(h(1, 12, 10), ( G(h(1, 12, 10), U[12,12],h(1, 12, 11)) Div G(h(1, 12, 10), U[12,12],h(1, 12, 10)) ),h(1, 12, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_373 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_51, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_374 = _t45_53;

  // 4-BLAC: 1x4 / 1x4
  _t45_375 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t45_373), _mm256_castpd256_pd128(_t45_374)));

  // AVX Storer:
  _t45_54 = _t45_375;

  // Generating : U[12,12] = S(h(1, 12, 11), ( G(h(1, 12, 11), U[12,12],h(1, 12, 11)) - ( T( G(h(1, 12, 10), U[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), U[12,12],h(1, 12, 11)) ) ),h(1, 12, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_376 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_52, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_377 = _t45_54;

  // 4-BLAC: (4x1)^T
  _t45_378 = _t45_377;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_379 = _t45_54;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_380 = _mm256_mul_pd(_t45_378, _t45_379);

  // 4-BLAC: 1x4 - 1x4
  _t45_381 = _mm256_sub_pd(_t45_376, _t45_380);

  // AVX Storer:
  _t45_55 = _t45_381;

  // Generating : U[12,12] = S(h(1, 12, 11), Sqrt( G(h(1, 12, 11), U[12,12],h(1, 12, 11)) ),h(1, 12, 11))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_382 = _t45_55;

  // 4-BLAC: sqrt(1x4)
  _t45_383 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t45_382)));

  // AVX Storer:
  _t45_55 = _t45_383;

  _mm_store_sd(&(M3[0]), _mm256_castpd256_pd128(_t45_0));
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t45_2);
  _mm_store_sd(&(M3[13]), _mm256_castpd256_pd128(_t45_6));
  _mm256_maskstore_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t45_8);
  _mm_store_sd(&(M3[26]), _mm256_castpd256_pd128(_t45_11));
  _mm_store_sd(&(M3[27]), _mm256_castpd256_pd128(_t45_12));
  _mm_store_sd(&(M3[39]), _mm256_castpd256_pd128(_t45_13));
  _asm256_storeu_pd(M3 + 4, _t45_57);
  _asm256_storeu_pd(M3 + 16, _t45_16);
  _asm256_storeu_pd(M3 + 28, _t45_17);
  _asm256_storeu_pd(M3 + 40, _t45_18);
  _asm256_storeu_pd(M3 + 8, _t45_58);
  _asm256_storeu_pd(M3 + 20, _t45_19);
  _asm256_storeu_pd(M3 + 32, _t45_20);
  _asm256_storeu_pd(M3 + 44, _t45_21);
  _mm_store_sd(&(M3[52]), _mm256_castpd256_pd128(_t45_26));
  _mm256_maskstore_pd(M3 + 53, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t45_28);
  _mm_store_sd(&(M3[65]), _mm256_castpd256_pd128(_t45_32));
  _mm256_maskstore_pd(M3 + 66, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t45_34);
  _mm_store_sd(&(M3[78]), _mm256_castpd256_pd128(_t45_37));
  _mm_store_sd(&(M3[79]), _mm256_castpd256_pd128(_t45_38));
  _mm_store_sd(&(M3[91]), _mm256_castpd256_pd128(_t45_39));
  _asm256_storeu_pd(M3 + 56, _t45_80);
  _asm256_storeu_pd(M3 + 68, _t45_81);
  _asm256_storeu_pd(M3 + 80, _t45_82);
  _asm256_storeu_pd(M3 + 92, _t45_83);

  for( int fi35 = 0; fi35 <= 7; fi35+=4 ) {
    _t46_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35])));
    _t46_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35])));
    _t46_8 = _mm256_maskload_pd(v0 + fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t46_5 = _mm256_maskload_pd(M3 + 13*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t46_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 13])));
    _t46_3 = _mm256_maskload_pd(M3 + 13*fi35 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t46_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 26])));
    _t46_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 27])));
    _t46_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 39])));

    // Generating : v2[12,1] = S(h(1, 12, fi35), ( G(h(1, 12, fi35), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, fi35), U0[12,12],h(1, 12, fi35)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_13 = _t46_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_14 = _t46_6;

    // 4-BLAC: 1x4 / 1x4
    _t46_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t46_13), _mm256_castpd256_pd128(_t46_14)));

    // AVX Storer:
    _t46_7 = _t46_15;

    // Generating : v2[12,1] = S(h(3, 12, fi35 + 1), ( G(h(3, 12, fi35 + 1), v2[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, fi35), U0[12,12],h(3, 12, fi35 + 1)) ) Kro G(h(1, 12, fi35), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t46_16 = _t46_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t46_17 = _t46_5;

    // 4-BLAC: (1x4)^T
    _t46_18 = _t46_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t46_7, _t46_7, 32), _mm256_permute2f128_pd(_t46_7, _t46_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t46_20 = _mm256_mul_pd(_t46_18, _t46_19);

    // 4-BLAC: 4x1 - 4x1
    _t46_21 = _mm256_sub_pd(_t46_16, _t46_20);

    // AVX Storer:
    _t46_8 = _t46_21;

    // Generating : v2[12,1] = S(h(1, 12, fi35 + 1), ( G(h(1, 12, fi35 + 1), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, fi35 + 1), U0[12,12],h(1, 12, fi35 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t46_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_23 = _t46_4;

    // 4-BLAC: 1x4 / 1x4
    _t46_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t46_22), _mm256_castpd256_pd128(_t46_23)));

    // AVX Storer:
    _t46_9 = _t46_24;

    // Generating : v2[12,1] = S(h(2, 12, fi35 + 2), ( G(h(2, 12, fi35 + 2), v2[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, fi35 + 1), U0[12,12],h(2, 12, fi35 + 2)) ) Kro G(h(1, 12, fi35 + 1), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t46_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t46_8, 6), _mm256_permute2f128_pd(_t46_8, _t46_8, 129), 5);

    // AVX Loader:

    // 1x2 -> 1x4
    _t46_26 = _t46_3;

    // 4-BLAC: (1x4)^T
    _t46_27 = _t46_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t46_9, _t46_9, 32), _mm256_permute2f128_pd(_t46_9, _t46_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t46_29 = _mm256_mul_pd(_t46_27, _t46_28);

    // 4-BLAC: 4x1 - 4x1
    _t46_30 = _mm256_sub_pd(_t46_25, _t46_29);

    // AVX Storer:
    _t46_10 = _t46_30;

    // Generating : v2[12,1] = S(h(1, 12, fi35 + 2), ( G(h(1, 12, fi35 + 2), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, fi35 + 2), U0[12,12],h(1, 12, fi35 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t46_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_32 = _t46_2;

    // 4-BLAC: 1x4 / 1x4
    _t46_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t46_31), _mm256_castpd256_pd128(_t46_32)));

    // AVX Storer:
    _t46_11 = _t46_33;

    // Generating : v2[12,1] = S(h(1, 12, fi35 + 3), ( G(h(1, 12, fi35 + 3), v2[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, fi35 + 2), U0[12,12],h(1, 12, fi35 + 3)) ) Kro G(h(1, 12, fi35 + 2), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_34 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t46_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_35 = _t46_1;

    // 4-BLAC: (4x1)^T
    _t46_36 = _t46_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_37 = _t46_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t46_38 = _mm256_mul_pd(_t46_36, _t46_37);

    // 4-BLAC: 1x4 - 1x4
    _t46_39 = _mm256_sub_pd(_t46_34, _t46_38);

    // AVX Storer:
    _t46_12 = _t46_39;

    // Generating : v2[12,1] = S(h(1, 12, fi35 + 3), ( G(h(1, 12, fi35 + 3), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, fi35 + 3), U0[12,12],h(1, 12, fi35 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_40 = _t46_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_41 = _t46_0;

    // 4-BLAC: 1x4 / 1x4
    _t46_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t46_40), _mm256_castpd256_pd128(_t46_41)));

    // AVX Storer:
    _t46_12 = _t46_42;

    // Generating : v2[12,1] = Sum_{k3} ( S(h(4, 12, fi35 + k3 + 4), ( G(h(4, 12, fi35 + k3 + 4), v2[12,1],h(1, 1, 0)) - ( T( G(h(4, 12, fi35), U0[12,12],h(4, 12, fi35 + k3 + 4)) ) * G(h(4, 12, fi35), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[fi35]), _mm256_castpd256_pd128(_t46_7));
    _mm_store_sd(&(v0[fi35 + 1]), _mm256_castpd256_pd128(_t46_9));
    _mm_store_sd(&(v0[fi35 + 2]), _mm256_castpd256_pd128(_t46_11));
    _mm_store_sd(&(v0[fi35 + 3]), _mm256_castpd256_pd128(_t46_12));

    for( int k3 = 0; k3 <= -fi35 + 7; k3+=4 ) {
      _t47_9 = _asm256_loadu_pd(v0 + fi35 + k3 + 4);
      _t47_7 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 4);
      _t47_6 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 16);
      _t47_5 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 28);
      _t47_4 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 40);
      _t47_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35])));
      _t47_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35 + 1])));
      _t47_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35 + 2])));
      _t47_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi35 + 3])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t47_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_7, _t47_6), _mm256_unpacklo_pd(_t47_5, _t47_4), 32);
      _t47_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_7, _t47_6), _mm256_unpackhi_pd(_t47_5, _t47_4), 32);
      _t47_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_7, _t47_6), _mm256_unpacklo_pd(_t47_5, _t47_4), 49);
      _t47_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_7, _t47_6), _mm256_unpackhi_pd(_t47_5, _t47_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t47_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t47_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32)), _mm256_mul_pd(_t47_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t47_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32)), _mm256_mul_pd(_t47_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t47_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32)), _mm256_mul_pd(_t47_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t47_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32)), _mm256_mul_pd(_t47_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t47_9 = _mm256_sub_pd(_t47_9, _t47_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + fi35 + k3 + 4, _t47_9);
    }
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[8])));
  _t48_1 = _mm256_maskload_pd(v0 + 9, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v2[12,1] = S(h(1, 12, 8), ( G(h(1, 12, 8), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, 8), U0[12,12],h(1, 12, 8)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_6 = _t48_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_7 = _t45_42;

  // 4-BLAC: 1x4 / 1x4
  _t48_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_6), _mm256_castpd256_pd128(_t48_7)));

  // AVX Storer:
  _t48_0 = _t48_8;

  // Generating : v2[12,1] = S(h(3, 12, 9), ( G(h(3, 12, 9), v2[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, 8), U0[12,12],h(3, 12, 9)) ) Kro G(h(1, 12, 8), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_9 = _t48_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_10 = _t45_44;

  // 4-BLAC: (1x4)^T
  _t48_11 = _t48_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_0, _t48_0, 32), _mm256_permute2f128_pd(_t48_0, _t48_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t48_13 = _mm256_mul_pd(_t48_11, _t48_12);

  // 4-BLAC: 4x1 - 4x1
  _t48_14 = _mm256_sub_pd(_t48_9, _t48_13);

  // AVX Storer:
  _t48_1 = _t48_14;

  // Generating : v2[12,1] = S(h(1, 12, 9), ( G(h(1, 12, 9), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, 9), U0[12,12],h(1, 12, 9)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_15 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_16 = _t45_48;

  // 4-BLAC: 1x4 / 1x4
  _t48_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_15), _mm256_castpd256_pd128(_t48_16)));

  // AVX Storer:
  _t48_2 = _t48_17;

  // Generating : v2[12,1] = S(h(2, 12, 10), ( G(h(2, 12, 10), v2[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, 9), U0[12,12],h(2, 12, 10)) ) Kro G(h(1, 12, 9), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_18 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_1, 6), _mm256_permute2f128_pd(_t48_1, _t48_1, 129), 5);

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_19 = _t45_50;

  // 4-BLAC: (1x4)^T
  _t48_20 = _t48_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t48_22 = _mm256_mul_pd(_t48_20, _t48_21);

  // 4-BLAC: 4x1 - 4x1
  _t48_23 = _mm256_sub_pd(_t48_18, _t48_22);

  // AVX Storer:
  _t48_3 = _t48_23;

  // Generating : v2[12,1] = S(h(1, 12, 10), ( G(h(1, 12, 10), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, 10), U0[12,12],h(1, 12, 10)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_25 = _t45_53;

  // 4-BLAC: 1x4 / 1x4
  _t48_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_24), _mm256_castpd256_pd128(_t48_25)));

  // AVX Storer:
  _t48_4 = _t48_26;

  // Generating : v2[12,1] = S(h(1, 12, 11), ( G(h(1, 12, 11), v2[12,1],h(1, 1, 0)) - ( T( G(h(1, 12, 10), U0[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), v2[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_28 = _t45_54;

  // 4-BLAC: (4x1)^T
  _t48_29 = _t48_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_30 = _t48_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_31 = _mm256_mul_pd(_t48_29, _t48_30);

  // 4-BLAC: 1x4 - 1x4
  _t48_32 = _mm256_sub_pd(_t48_27, _t48_31);

  // AVX Storer:
  _t48_5 = _t48_32;

  // Generating : v2[12,1] = S(h(1, 12, 11), ( G(h(1, 12, 11), v2[12,1],h(1, 1, 0)) Div G(h(1, 12, 11), U0[12,12],h(1, 12, 11)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_33 = _t48_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_34 = _t45_55;

  // 4-BLAC: 1x4 / 1x4
  _t48_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_33), _mm256_castpd256_pd128(_t48_34)));

  // AVX Storer:
  _t48_5 = _t48_35;

  _mm_store_sd(&(M3[104]), _mm256_castpd256_pd128(_t45_42));
  _mm256_maskstore_pd(M3 + 105, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t45_44);
  _mm_store_sd(&(M3[117]), _mm256_castpd256_pd128(_t45_48));
  _mm256_maskstore_pd(M3 + 118, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t45_50);
  _mm_store_sd(&(M3[130]), _mm256_castpd256_pd128(_t45_53));
  _mm_store_sd(&(M3[131]), _mm256_castpd256_pd128(_t45_54));
  _mm_store_sd(&(M3[143]), _mm256_castpd256_pd128(_t45_55));
  _mm_store_sd(&(v0[8]), _mm256_castpd256_pd128(_t48_0));
  _mm_store_sd(&(v0[9]), _mm256_castpd256_pd128(_t48_2));
  _mm_store_sd(&(v0[10]), _mm256_castpd256_pd128(_t48_4));
  _mm_store_sd(&(v0[11]), _mm256_castpd256_pd128(_t48_5));

  for( int fi35 = 0; fi35 <= 7; fi35+=4 ) {
    _t49_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 11])));
    _t49_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 143])));
    _t49_8 = _mm256_maskload_pd(v0 + -fi35 + 8, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t49_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 107)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 119))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 131)), 32);
    _t49_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 130])));
    _t49_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 106)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 118)), 0);
    _t49_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 117])));
    _t49_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 105])));
    _t49_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 104])));

    // Generating : v4[12,1] = S(h(1, 12, -fi35 + 11), ( G(h(1, 12, -fi35 + 11), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 11), U0[12,12],h(1, 12, -fi35 + 11)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_13 = _t49_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_14 = _t49_6;

    // 4-BLAC: 1x4 / 1x4
    _t49_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_13), _mm256_castpd256_pd128(_t49_14)));

    // AVX Storer:
    _t49_7 = _t49_15;

    // Generating : v4[12,1] = S(h(3, 12, -fi35 + 8), ( G(h(3, 12, -fi35 + 8), v4[12,1],h(1, 1, 0)) - ( G(h(3, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 11)) Kro G(h(1, 12, -fi35 + 11), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t49_16 = _t49_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t49_17 = _t49_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_7, _t49_7, 32), _mm256_permute2f128_pd(_t49_7, _t49_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t49_19 = _mm256_mul_pd(_t49_17, _t49_18);

    // 4-BLAC: 4x1 - 4x1
    _t49_20 = _mm256_sub_pd(_t49_16, _t49_19);

    // AVX Storer:
    _t49_8 = _t49_20;

    // Generating : v4[12,1] = S(h(1, 12, -fi35 + 10), ( G(h(1, 12, -fi35 + 10), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 10), U0[12,12],h(1, 12, -fi35 + 10)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t49_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t49_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_22 = _t49_4;

    // 4-BLAC: 1x4 / 1x4
    _t49_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_21), _mm256_castpd256_pd128(_t49_22)));

    // AVX Storer:
    _t49_9 = _t49_23;

    // Generating : v4[12,1] = S(h(2, 12, -fi35 + 8), ( G(h(2, 12, -fi35 + 8), v4[12,1],h(1, 1, 0)) - ( G(h(2, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 10)) Kro G(h(1, 12, -fi35 + 10), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t49_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t49_8, 3);

    // AVX Loader:

    // 2x1 -> 4x1
    _t49_25 = _t49_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_9, _t49_9, 32), _mm256_permute2f128_pd(_t49_9, _t49_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t49_27 = _mm256_mul_pd(_t49_25, _t49_26);

    // 4-BLAC: 4x1 - 4x1
    _t49_28 = _mm256_sub_pd(_t49_24, _t49_27);

    // AVX Storer:
    _t49_10 = _t49_28;

    // Generating : v4[12,1] = S(h(1, 12, -fi35 + 9), ( G(h(1, 12, -fi35 + 9), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 9), U0[12,12],h(1, 12, -fi35 + 9)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t49_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_30 = _t49_2;

    // 4-BLAC: 1x4 / 1x4
    _t49_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_29), _mm256_castpd256_pd128(_t49_30)));

    // AVX Storer:
    _t49_11 = _t49_31;

    // Generating : v4[12,1] = S(h(1, 12, -fi35 + 8), ( G(h(1, 12, -fi35 + 8), v4[12,1],h(1, 1, 0)) - ( G(h(1, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 9)) Kro G(h(1, 12, -fi35 + 9), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_32 = _mm256_blend_pd(_mm256_setzero_pd(), _t49_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_33 = _t49_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_34 = _t49_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t49_35 = _mm256_mul_pd(_t49_33, _t49_34);

    // 4-BLAC: 1x4 - 1x4
    _t49_36 = _mm256_sub_pd(_t49_32, _t49_35);

    // AVX Storer:
    _t49_12 = _t49_36;

    // Generating : v4[12,1] = S(h(1, 12, -fi35 + 8), ( G(h(1, 12, -fi35 + 8), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 8)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_37 = _t49_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_38 = _t49_0;

    // 4-BLAC: 1x4 / 1x4
    _t49_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_37), _mm256_castpd256_pd128(_t49_38)));

    // AVX Storer:
    _t49_12 = _t49_39;

    // Generating : v4[12,1] = Sum_{k3} ( S(h(4, 12, k3), ( G(h(4, 12, k3), v4[12,1],h(1, 1, 0)) - ( G(h(4, 12, k3), U0[12,12],h(4, 12, -fi35 + 8)) * G(h(4, 12, -fi35 + 8), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[-fi35 + 11]), _mm256_castpd256_pd128(_t49_7));
    _mm_store_sd(&(v0[-fi35 + 10]), _mm256_castpd256_pd128(_t49_9));
    _mm_store_sd(&(v0[-fi35 + 9]), _mm256_castpd256_pd128(_t49_11));
    _mm_store_sd(&(v0[-fi35 + 8]), _mm256_castpd256_pd128(_t49_12));

    for( int k3 = 0; k3 <= -fi35 + 7; k3+=4 ) {
      _t50_9 = _asm256_loadu_pd(v0 + k3);
      _t50_7 = _asm256_loadu_pd(M3 + -fi35 + 12*k3 + 8);
      _t50_6 = _asm256_loadu_pd(M3 + -fi35 + 12*k3 + 20);
      _t50_5 = _asm256_loadu_pd(M3 + -fi35 + 12*k3 + 32);
      _t50_4 = _asm256_loadu_pd(M3 + -fi35 + 12*k3 + 44);
      _t50_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 11])));
      _t50_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 10])));
      _t50_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 9])));
      _t50_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi35 + 8])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t50_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t50_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32)), _mm256_mul_pd(_t50_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t50_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32)), _mm256_mul_pd(_t50_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t50_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32)), _mm256_mul_pd(_t50_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t50_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32)), _mm256_mul_pd(_t50_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_0, _t50_1), _mm256_unpacklo_pd(_t50_2, _t50_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t50_9 = _mm256_sub_pd(_t50_9, _t50_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k3, _t50_9);
    }
  }

  _t45_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t45_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13])));
  _t45_8 = _mm256_maskload_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t45_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t45_12 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[27])));
  _t45_11 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[26])));
  _t45_13 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[39])));
  _t51_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[3])));
  _t51_1 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v4[12,1] = S(h(1, 12, 3), ( G(h(1, 12, 3), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, 3), U0[12,12],h(1, 12, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_6 = _t51_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_7 = _t45_13;

  // 4-BLAC: 1x4 / 1x4
  _t51_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_6), _mm256_castpd256_pd128(_t51_7)));

  // AVX Storer:
  _t51_0 = _t51_8;

  // Generating : v4[12,1] = S(h(3, 12, 0), ( G(h(3, 12, 0), v4[12,1],h(1, 1, 0)) - ( G(h(3, 12, 0), U0[12,12],h(1, 12, 3)) Kro G(h(1, 12, 3), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t51_9 = _t51_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t51_10 = _mm256_blend_pd(_mm256_permute2f128_pd(_t45_2, _t45_12, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t45_8, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_0, _t51_0, 32), _mm256_permute2f128_pd(_t51_0, _t51_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t51_12 = _mm256_mul_pd(_t51_10, _t51_11);

  // 4-BLAC: 4x1 - 4x1
  _t51_13 = _mm256_sub_pd(_t51_9, _t51_12);

  // AVX Storer:
  _t51_1 = _t51_13;

  // Generating : v4[12,1] = S(h(1, 12, 2), ( G(h(1, 12, 2), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, 2), U0[12,12],h(1, 12, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_14 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t51_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t51_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_15 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t51_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_14), _mm256_castpd256_pd128(_t51_15)));

  // AVX Storer:
  _t51_2 = _t51_16;

  // Generating : v4[12,1] = S(h(2, 12, 0), ( G(h(2, 12, 0), v4[12,1],h(1, 1, 0)) - ( G(h(2, 12, 0), U0[12,12],h(1, 12, 2)) Kro G(h(1, 12, 2), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t51_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t51_1, 3);

  // AVX Loader:

  // 2x1 -> 4x1
  _t51_18 = _mm256_shuffle_pd(_mm256_blend_pd(_t45_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t45_8, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_2, _t51_2, 32), _mm256_permute2f128_pd(_t51_2, _t51_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t51_20 = _mm256_mul_pd(_t51_18, _t51_19);

  // 4-BLAC: 4x1 - 4x1
  _t51_21 = _mm256_sub_pd(_t51_17, _t51_20);

  // AVX Storer:
  _t51_3 = _t51_21;

  // Generating : v4[12,1] = S(h(1, 12, 1), ( G(h(1, 12, 1), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, 1), U0[12,12],h(1, 12, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_22 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t51_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_23 = _t45_6;

  // 4-BLAC: 1x4 / 1x4
  _t51_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_22), _mm256_castpd256_pd128(_t51_23)));

  // AVX Storer:
  _t51_4 = _t51_24;

  // Generating : v4[12,1] = S(h(1, 12, 0), ( G(h(1, 12, 0), v4[12,1],h(1, 1, 0)) - ( G(h(1, 12, 0), U0[12,12],h(1, 12, 1)) Kro G(h(1, 12, 1), v4[12,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t51_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_27 = _t51_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t51_28 = _mm256_mul_pd(_t51_26, _t51_27);

  // 4-BLAC: 1x4 - 1x4
  _t51_29 = _mm256_sub_pd(_t51_25, _t51_28);

  // AVX Storer:
  _t51_5 = _t51_29;

  // Generating : v4[12,1] = S(h(1, 12, 0), ( G(h(1, 12, 0), v4[12,1],h(1, 1, 0)) Div G(h(1, 12, 0), U0[12,12],h(1, 12, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_30 = _t51_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t51_31 = _t45_0;

  // 4-BLAC: 1x4 / 1x4
  _t51_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_30), _mm256_castpd256_pd128(_t51_31)));

  // AVX Storer:
  _t51_5 = _t51_32;


  for( int fi35 = 0; fi35 <= 7; fi35+=4 ) {
    _t52_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35])));
    _t52_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 13])));
    _t52_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 26])));
    _t52_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13*fi35 + 39])));
    _t52_14 = _asm256_loadu_pd(M1 + 12*fi35);
    _t52_11 = _asm256_loadu_pd(M1 + 12*fi35 + 12);
    _t52_12 = _asm256_loadu_pd(M1 + 12*fi35 + 24);
    _t52_13 = _asm256_loadu_pd(M1 + 12*fi35 + 36);
    _t52_2 = _mm256_maskload_pd(M3 + 13*fi35 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t52_1 = _mm256_maskload_pd(M3 + 13*fi35 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t52_0 = _mm256_broadcast_sd(&(M3[13*fi35 + 27]));

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi35), U0[12,12],h(1, 12, fi35)) ),h(1, 12, fi35))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t52_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_17 = _t52_6;

    // 4-BLAC: 1x4 / 1x4
    _t52_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_16), _mm256_castpd256_pd128(_t52_17)));

    // AVX Storer:
    _t52_7 = _t52_18;

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi35 + 1), U0[12,12],h(1, 12, fi35 + 1)) ),h(1, 12, fi35 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t52_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_20 = _t52_5;

    // 4-BLAC: 1x4 / 1x4
    _t52_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_19), _mm256_castpd256_pd128(_t52_20)));

    // AVX Storer:
    _t52_8 = _t52_21;

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi35 + 2), U0[12,12],h(1, 12, fi35 + 2)) ),h(1, 12, fi35 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t52_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_23 = _t52_4;

    // 4-BLAC: 1x4 / 1x4
    _t52_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_22), _mm256_castpd256_pd128(_t52_23)));

    // AVX Storer:
    _t52_9 = _t52_24;

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, fi35 + 3), U0[12,12],h(1, 12, fi35 + 3)) ),h(1, 12, fi35 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t52_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_26 = _t52_3;

    // 4-BLAC: 1x4 / 1x4
    _t52_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_25), _mm256_castpd256_pd128(_t52_26)));

    // AVX Storer:
    _t52_10 = _t52_27;

    // Generating : M6[12,12] = S(h(1, 12, fi35), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35)) Kro G(h(1, 12, fi35), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_7, _t52_7, 32), _mm256_permute2f128_pd(_t52_7, _t52_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_14 = _mm256_mul_pd(_t52_28, _t52_14);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(3, 12, fi35 + 1), ( G(h(3, 12, fi35 + 1), M6[12,12],h(4, 12, fi96)) - ( T( G(h(1, 12, fi35), U0[12,12],h(3, 12, fi35 + 1)) ) * G(h(1, 12, fi35), M6[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

    // AVX Loader:

    // 3x4 -> 4x4
    _t52_29 = _t52_11;
    _t52_30 = _t52_12;
    _t52_31 = _t52_13;
    _t52_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t52_33 = _t52_2;

    // 4-BLAC: (1x4)^T
    _t52_34 = _t52_33;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t52_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 0), _t52_14);
    _t52_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 15), _t52_14);
    _t52_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 0), _t52_14);
    _t52_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 15), _t52_14);

    // 4-BLAC: 4x4 - 4x4
    _t52_39 = _mm256_sub_pd(_t52_29, _t52_35);
    _t52_40 = _mm256_sub_pd(_t52_30, _t52_36);
    _t52_41 = _mm256_sub_pd(_t52_31, _t52_37);
    _t52_42 = _mm256_sub_pd(_t52_32, _t52_38);

    // AVX Storer:
    _t52_11 = _t52_39;
    _t52_12 = _t52_40;
    _t52_13 = _t52_41;

    // Generating : M6[12,12] = S(h(1, 12, fi35 + 1), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35 + 1)) Kro G(h(1, 12, fi35 + 1), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_8, _t52_8, 32), _mm256_permute2f128_pd(_t52_8, _t52_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_11 = _mm256_mul_pd(_t52_43, _t52_11);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(2, 12, fi35 + 2), ( G(h(2, 12, fi35 + 2), M6[12,12],h(4, 12, fi96)) - ( T( G(h(1, 12, fi35 + 1), U0[12,12],h(2, 12, fi35 + 2)) ) * G(h(1, 12, fi35 + 1), M6[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

    // AVX Loader:

    // 2x4 -> 4x4
    _t52_44 = _t52_12;
    _t52_45 = _t52_13;
    _t52_46 = _mm256_setzero_pd();
    _t52_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t52_48 = _t52_1;

    // 4-BLAC: (1x4)^T
    _t52_49 = _t52_48;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t52_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 32), _mm256_permute2f128_pd(_t52_49, _t52_49, 32), 0), _t52_11);
    _t52_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 32), _mm256_permute2f128_pd(_t52_49, _t52_49, 32), 15), _t52_11);
    _t52_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 49), _mm256_permute2f128_pd(_t52_49, _t52_49, 49), 0), _t52_11);
    _t52_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 49), _mm256_permute2f128_pd(_t52_49, _t52_49, 49), 15), _t52_11);

    // 4-BLAC: 4x4 - 4x4
    _t52_54 = _mm256_sub_pd(_t52_44, _t52_50);
    _t52_55 = _mm256_sub_pd(_t52_45, _t52_51);
    _t52_56 = _mm256_sub_pd(_t52_46, _t52_52);
    _t52_57 = _mm256_sub_pd(_t52_47, _t52_53);

    // AVX Storer:
    _t52_12 = _t52_54;
    _t52_13 = _t52_55;

    // Generating : M6[12,12] = S(h(1, 12, fi35 + 2), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35 + 2)) Kro G(h(1, 12, fi35 + 2), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_9, _t52_9, 32), _mm256_permute2f128_pd(_t52_9, _t52_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_12 = _mm256_mul_pd(_t52_58, _t52_12);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(1, 12, fi35 + 3), ( G(h(1, 12, fi35 + 3), M6[12,12],h(4, 12, fi96)) - ( T( G(h(1, 12, fi35 + 2), U0[12,12],h(1, 12, fi35 + 3)) ) Kro G(h(1, 12, fi35 + 2), M6[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_59 = _t52_0;

    // 4-BLAC: (4x1)^T
    _t52_60 = _t52_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_15 = _mm256_mul_pd(_t52_60, _t52_12);

    // 4-BLAC: 1x4 - 1x4
    _t52_13 = _mm256_sub_pd(_t52_13, _t52_15);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(1, 12, fi35 + 3), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35 + 3)) Kro G(h(1, 12, fi35 + 3), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t52_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_10, _t52_10, 32), _mm256_permute2f128_pd(_t52_10, _t52_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_13 = _mm256_mul_pd(_t52_61, _t52_13);

    // AVX Storer:

    for( int fi96 = 4; fi96 <= 8; fi96+=4 ) {
      _t53_3 = _asm256_loadu_pd(M1 + 12*fi35 + fi96);
      _t53_0 = _asm256_loadu_pd(M1 + 12*fi35 + fi96 + 12);
      _t53_1 = _asm256_loadu_pd(M1 + 12*fi35 + fi96 + 24);
      _t53_2 = _asm256_loadu_pd(M1 + 12*fi35 + fi96 + 36);

      // Generating : M6[12,12] = S(h(1, 12, fi35), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35)) Kro G(h(1, 12, fi35), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t53_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_7, _t52_7, 32), _mm256_permute2f128_pd(_t52_7, _t52_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t53_3 = _mm256_mul_pd(_t53_4, _t53_3);

      // AVX Storer:

      // Generating : M6[12,12] = S(h(3, 12, fi35 + 1), ( G(h(3, 12, fi35 + 1), M6[12,12],h(4, 12, fi96)) - ( T( G(h(1, 12, fi35), U0[12,12],h(3, 12, fi35 + 1)) ) * G(h(1, 12, fi35), M6[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

      // AVX Loader:

      // 3x4 -> 4x4
      _t53_5 = _t53_0;
      _t53_6 = _t53_1;
      _t53_7 = _t53_2;
      _t53_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t53_9 = _t52_2;

      // 4-BLAC: (1x4)^T
      _t52_34 = _t53_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t52_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 0), _t53_3);
      _t52_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 15), _t53_3);
      _t52_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 0), _t53_3);
      _t52_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 15), _t53_3);

      // 4-BLAC: 4x4 - 4x4
      _t53_10 = _mm256_sub_pd(_t53_5, _t52_35);
      _t53_11 = _mm256_sub_pd(_t53_6, _t52_36);
      _t53_12 = _mm256_sub_pd(_t53_7, _t52_37);
      _t53_13 = _mm256_sub_pd(_t53_8, _t52_38);

      // AVX Storer:
      _t53_0 = _t53_10;
      _t53_1 = _t53_11;
      _t53_2 = _t53_12;

      // Generating : M6[12,12] = S(h(1, 12, fi35 + 1), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35 + 1)) Kro G(h(1, 12, fi35 + 1), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t53_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_8, _t52_8, 32), _mm256_permute2f128_pd(_t52_8, _t52_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t53_0 = _mm256_mul_pd(_t53_14, _t53_0);

      // AVX Storer:

      // Generating : M6[12,12] = S(h(2, 12, fi35 + 2), ( G(h(2, 12, fi35 + 2), M6[12,12],h(4, 12, fi96)) - ( T( G(h(1, 12, fi35 + 1), U0[12,12],h(2, 12, fi35 + 2)) ) * G(h(1, 12, fi35 + 1), M6[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

      // AVX Loader:

      // 2x4 -> 4x4
      _t53_15 = _t53_1;
      _t53_16 = _t53_2;
      _t53_17 = _mm256_setzero_pd();
      _t53_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t53_19 = _t52_1;

      // 4-BLAC: (1x4)^T
      _t52_49 = _t53_19;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t52_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 32), _mm256_permute2f128_pd(_t52_49, _t52_49, 32), 0), _t53_0);
      _t52_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 32), _mm256_permute2f128_pd(_t52_49, _t52_49, 32), 15), _t53_0);
      _t52_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 49), _mm256_permute2f128_pd(_t52_49, _t52_49, 49), 0), _t53_0);
      _t52_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_49, _t52_49, 49), _mm256_permute2f128_pd(_t52_49, _t52_49, 49), 15), _t53_0);

      // 4-BLAC: 4x4 - 4x4
      _t53_20 = _mm256_sub_pd(_t53_15, _t52_50);
      _t53_21 = _mm256_sub_pd(_t53_16, _t52_51);
      _t53_22 = _mm256_sub_pd(_t53_17, _t52_52);
      _t53_23 = _mm256_sub_pd(_t53_18, _t52_53);

      // AVX Storer:
      _t53_1 = _t53_20;
      _t53_2 = _t53_21;

      // Generating : M6[12,12] = S(h(1, 12, fi35 + 2), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35 + 2)) Kro G(h(1, 12, fi35 + 2), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t53_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_9, _t52_9, 32), _mm256_permute2f128_pd(_t52_9, _t52_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t53_1 = _mm256_mul_pd(_t53_24, _t53_1);

      // AVX Storer:

      // Generating : M6[12,12] = S(h(1, 12, fi35 + 3), ( G(h(1, 12, fi35 + 3), M6[12,12],h(4, 12, fi96)) - ( T( G(h(1, 12, fi35 + 2), U0[12,12],h(1, 12, fi35 + 3)) ) Kro G(h(1, 12, fi35 + 2), M6[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t53_25 = _t52_0;

      // 4-BLAC: (4x1)^T
      _t52_60 = _t53_25;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t52_15 = _mm256_mul_pd(_t52_60, _t53_1);

      // 4-BLAC: 1x4 - 1x4
      _t53_2 = _mm256_sub_pd(_t53_2, _t52_15);

      // AVX Storer:

      // Generating : M6[12,12] = S(h(1, 12, fi35 + 3), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, fi35 + 3)) Kro G(h(1, 12, fi35 + 3), M6[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t53_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_10, _t52_10, 32), _mm256_permute2f128_pd(_t52_10, _t52_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t53_2 = _mm256_mul_pd(_t53_26, _t53_2);

      // AVX Storer:
      _asm256_storeu_pd(M1 + 12*fi35 + fi96, _t53_3);
      _asm256_storeu_pd(M1 + 12*fi35 + fi96 + 12, _t53_0);
      _asm256_storeu_pd(M1 + 12*fi35 + fi96 + 24, _t53_1);
      _asm256_storeu_pd(M1 + 12*fi35 + fi96 + 36, _t53_2);
    }

    // Generating : M6[12,12] = Sum_{k3} ( Sum_{k2} ( S(h(4, 12, fi35 + k3 + 4), ( G(h(4, 12, fi35 + k3 + 4), M6[12,12],h(4, 12, k2)) - ( T( G(h(4, 12, fi35), U0[12,12],h(4, 12, fi35 + k3 + 4)) ) * G(h(4, 12, fi35), M6[12,12],h(4, 12, k2)) ) ),h(4, 12, k2)) ) )
    _asm256_storeu_pd(M1 + 12*fi35, _t52_14);
    _asm256_storeu_pd(M1 + 12*fi35 + 12, _t52_11);
    _asm256_storeu_pd(M1 + 12*fi35 + 24, _t52_12);
    _asm256_storeu_pd(M1 + 12*fi35 + 36, _t52_13);

    for( int k3 = 0; k3 <= -fi35 + 7; k3+=4 ) {

      for( int k2 = 0; k2 <= 11; k2+=4 ) {
        _t54_12 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 12*k3 + 48);
        _t54_13 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 12*k3 + 60);
        _t54_14 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 12*k3 + 72);
        _t54_15 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 12*k3 + 84);
        _t54_7 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 4);
        _t54_6 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 16);
        _t54_5 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 28);
        _t54_4 = _asm256_loadu_pd(M3 + 13*fi35 + k3 + 40);
        _t54_3 = _asm256_loadu_pd(M1 + 12*fi35 + k2);
        _t54_2 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 12);
        _t54_1 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 24);
        _t54_0 = _asm256_loadu_pd(M1 + 12*fi35 + k2 + 36);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t54_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_7, _t54_6), _mm256_unpacklo_pd(_t54_5, _t54_4), 32);
        _t54_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_7, _t54_6), _mm256_unpackhi_pd(_t54_5, _t54_4), 32);
        _t54_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_7, _t54_6), _mm256_unpacklo_pd(_t54_5, _t54_4), 49);
        _t54_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_7, _t54_6), _mm256_unpackhi_pd(_t54_5, _t54_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t54_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_16, _t54_16, 32), _mm256_permute2f128_pd(_t54_16, _t54_16, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_16, _t54_16, 32), _mm256_permute2f128_pd(_t54_16, _t54_16, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_16, _t54_16, 49), _mm256_permute2f128_pd(_t54_16, _t54_16, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_16, _t54_16, 49), _mm256_permute2f128_pd(_t54_16, _t54_16, 49), 15), _t54_0)));
        _t54_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_17, _t54_17, 32), _mm256_permute2f128_pd(_t54_17, _t54_17, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_17, _t54_17, 32), _mm256_permute2f128_pd(_t54_17, _t54_17, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_17, _t54_17, 49), _mm256_permute2f128_pd(_t54_17, _t54_17, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_17, _t54_17, 49), _mm256_permute2f128_pd(_t54_17, _t54_17, 49), 15), _t54_0)));
        _t54_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_18, _t54_18, 32), _mm256_permute2f128_pd(_t54_18, _t54_18, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_18, _t54_18, 32), _mm256_permute2f128_pd(_t54_18, _t54_18, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_18, _t54_18, 49), _mm256_permute2f128_pd(_t54_18, _t54_18, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_18, _t54_18, 49), _mm256_permute2f128_pd(_t54_18, _t54_18, 49), 15), _t54_0)));
        _t54_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_19, _t54_19, 32), _mm256_permute2f128_pd(_t54_19, _t54_19, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_19, _t54_19, 32), _mm256_permute2f128_pd(_t54_19, _t54_19, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_19, _t54_19, 49), _mm256_permute2f128_pd(_t54_19, _t54_19, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_19, _t54_19, 49), _mm256_permute2f128_pd(_t54_19, _t54_19, 49), 15), _t54_0)));

        // 4-BLAC: 4x4 - 4x4
        _t54_12 = _mm256_sub_pd(_t54_12, _t54_8);
        _t54_13 = _mm256_sub_pd(_t54_13, _t54_9);
        _t54_14 = _mm256_sub_pd(_t54_14, _t54_10);
        _t54_15 = _mm256_sub_pd(_t54_15, _t54_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + 12*fi35 + k2 + 12*k3 + 48, _t54_12);
        _asm256_storeu_pd(M1 + 12*fi35 + k2 + 12*k3 + 60, _t54_13);
        _asm256_storeu_pd(M1 + 12*fi35 + k2 + 12*k3 + 72, _t54_14);
        _asm256_storeu_pd(M1 + 12*fi35 + k2 + 12*k3 + 84, _t54_15);
      }
    }
  }

  _t45_53 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[130])));
  _t45_50 = _mm256_maskload_pd(M3 + 118, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t45_44 = _mm256_maskload_pd(M3 + 105, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t45_48 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[117])));
  _t45_42 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[104])));
  _t45_55 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[143])));
  _t45_54 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[131])));
  _t55_7 = _asm256_loadu_pd(M1 + 96);
  _t55_4 = _asm256_loadu_pd(M1 + 108);
  _t55_5 = _asm256_loadu_pd(M1 + 120);
  _t55_6 = _asm256_loadu_pd(M1 + 132);

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 8), U0[12,12],h(1, 12, 8)) ),h(1, 12, 8))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t55_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_10 = _t45_42;

  // 4-BLAC: 1x4 / 1x4
  _t55_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t55_9), _mm256_castpd256_pd128(_t55_10)));

  // AVX Storer:
  _t55_0 = _t55_11;

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 9), U0[12,12],h(1, 12, 9)) ),h(1, 12, 9))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t55_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_13 = _t45_48;

  // 4-BLAC: 1x4 / 1x4
  _t55_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t55_12), _mm256_castpd256_pd128(_t55_13)));

  // AVX Storer:
  _t55_1 = _t55_14;

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 10), U0[12,12],h(1, 12, 10)) ),h(1, 12, 10))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t55_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_16 = _t45_53;

  // 4-BLAC: 1x4 / 1x4
  _t55_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t55_15), _mm256_castpd256_pd128(_t55_16)));

  // AVX Storer:
  _t55_2 = _t55_17;

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 11), U0[12,12],h(1, 12, 11)) ),h(1, 12, 11))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t55_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_19 = _t45_55;

  // 4-BLAC: 1x4 / 1x4
  _t55_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t55_18), _mm256_castpd256_pd128(_t55_19)));

  // AVX Storer:
  _t55_3 = _t55_20;

  // Generating : M6[12,12] = S(h(1, 12, 8), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 8)) Kro G(h(1, 12, 8), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_0, _t55_0, 32), _mm256_permute2f128_pd(_t55_0, _t55_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t55_7 = _mm256_mul_pd(_t55_21, _t55_7);

  // AVX Storer:

  // Generating : M6[12,12] = S(h(3, 12, 9), ( G(h(3, 12, 9), M6[12,12],h(4, 12, fi35)) - ( T( G(h(1, 12, 8), U0[12,12],h(3, 12, 9)) ) * G(h(1, 12, 8), M6[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

  // AVX Loader:

  // 3x4 -> 4x4
  _t55_22 = _t55_4;
  _t55_23 = _t55_5;
  _t55_24 = _t55_6;
  _t55_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t55_26 = _t45_44;

  // 4-BLAC: (1x4)^T
  _t55_27 = _t55_26;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t55_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 32), _mm256_permute2f128_pd(_t55_27, _t55_27, 32), 0), _t55_7);
  _t55_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 32), _mm256_permute2f128_pd(_t55_27, _t55_27, 32), 15), _t55_7);
  _t55_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 49), _mm256_permute2f128_pd(_t55_27, _t55_27, 49), 0), _t55_7);
  _t55_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 49), _mm256_permute2f128_pd(_t55_27, _t55_27, 49), 15), _t55_7);

  // 4-BLAC: 4x4 - 4x4
  _t55_32 = _mm256_sub_pd(_t55_22, _t55_28);
  _t55_33 = _mm256_sub_pd(_t55_23, _t55_29);
  _t55_34 = _mm256_sub_pd(_t55_24, _t55_30);
  _t55_35 = _mm256_sub_pd(_t55_25, _t55_31);

  // AVX Storer:
  _t55_4 = _t55_32;
  _t55_5 = _t55_33;
  _t55_6 = _t55_34;

  // Generating : M6[12,12] = S(h(1, 12, 9), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 9)) Kro G(h(1, 12, 9), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_1, _t55_1, 32), _mm256_permute2f128_pd(_t55_1, _t55_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t55_4 = _mm256_mul_pd(_t55_36, _t55_4);

  // AVX Storer:

  // Generating : M6[12,12] = S(h(2, 12, 10), ( G(h(2, 12, 10), M6[12,12],h(4, 12, fi35)) - ( T( G(h(1, 12, 9), U0[12,12],h(2, 12, 10)) ) * G(h(1, 12, 9), M6[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

  // AVX Loader:

  // 2x4 -> 4x4
  _t55_37 = _t55_5;
  _t55_38 = _t55_6;
  _t55_39 = _mm256_setzero_pd();
  _t55_40 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t55_41 = _t45_50;

  // 4-BLAC: (1x4)^T
  _t55_42 = _t55_41;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t55_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 32), _mm256_permute2f128_pd(_t55_42, _t55_42, 32), 0), _t55_4);
  _t55_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 32), _mm256_permute2f128_pd(_t55_42, _t55_42, 32), 15), _t55_4);
  _t55_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 49), _mm256_permute2f128_pd(_t55_42, _t55_42, 49), 0), _t55_4);
  _t55_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 49), _mm256_permute2f128_pd(_t55_42, _t55_42, 49), 15), _t55_4);

  // 4-BLAC: 4x4 - 4x4
  _t55_47 = _mm256_sub_pd(_t55_37, _t55_43);
  _t55_48 = _mm256_sub_pd(_t55_38, _t55_44);
  _t55_49 = _mm256_sub_pd(_t55_39, _t55_45);
  _t55_50 = _mm256_sub_pd(_t55_40, _t55_46);

  // AVX Storer:
  _t55_5 = _t55_47;
  _t55_6 = _t55_48;

  // Generating : M6[12,12] = S(h(1, 12, 10), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 10)) Kro G(h(1, 12, 10), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_2, _t55_2, 32), _mm256_permute2f128_pd(_t55_2, _t55_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t55_5 = _mm256_mul_pd(_t55_51, _t55_5);

  // AVX Storer:

  // Generating : M6[12,12] = S(h(1, 12, 11), ( G(h(1, 12, 11), M6[12,12],h(4, 12, fi35)) - ( T( G(h(1, 12, 10), U0[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), M6[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_54, _t45_54, 32), _mm256_permute2f128_pd(_t45_54, _t45_54, 32), 0);

  // 4-BLAC: (4x1)^T
  _t55_53 = _t55_52;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t55_8 = _mm256_mul_pd(_t55_53, _t55_5);

  // 4-BLAC: 1x4 - 1x4
  _t55_6 = _mm256_sub_pd(_t55_6, _t55_8);

  // AVX Storer:

  // Generating : M6[12,12] = S(h(1, 12, 11), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 11)) Kro G(h(1, 12, 11), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t55_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_3, _t55_3, 32), _mm256_permute2f128_pd(_t55_3, _t55_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t55_6 = _mm256_mul_pd(_t55_54, _t55_6);

  // AVX Storer:


  for( int fi35 = 4; fi35 <= 8; fi35+=4 ) {
    _t56_3 = _asm256_loadu_pd(M1 + fi35 + 96);
    _t56_0 = _asm256_loadu_pd(M1 + fi35 + 108);
    _t56_1 = _asm256_loadu_pd(M1 + fi35 + 120);
    _t56_2 = _asm256_loadu_pd(M1 + fi35 + 132);

    // Generating : M6[12,12] = S(h(1, 12, 8), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 8)) Kro G(h(1, 12, 8), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t56_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_0, _t55_0, 32), _mm256_permute2f128_pd(_t55_0, _t55_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t56_3 = _mm256_mul_pd(_t56_4, _t56_3);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(3, 12, 9), ( G(h(3, 12, 9), M6[12,12],h(4, 12, fi35)) - ( T( G(h(1, 12, 8), U0[12,12],h(3, 12, 9)) ) * G(h(1, 12, 8), M6[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

    // AVX Loader:

    // 3x4 -> 4x4
    _t56_5 = _t56_0;
    _t56_6 = _t56_1;
    _t56_7 = _t56_2;
    _t56_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t56_9 = _t45_44;

    // 4-BLAC: (1x4)^T
    _t55_27 = _t56_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t55_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 32), _mm256_permute2f128_pd(_t55_27, _t55_27, 32), 0), _t56_3);
    _t55_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 32), _mm256_permute2f128_pd(_t55_27, _t55_27, 32), 15), _t56_3);
    _t55_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 49), _mm256_permute2f128_pd(_t55_27, _t55_27, 49), 0), _t56_3);
    _t55_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_27, _t55_27, 49), _mm256_permute2f128_pd(_t55_27, _t55_27, 49), 15), _t56_3);

    // 4-BLAC: 4x4 - 4x4
    _t56_10 = _mm256_sub_pd(_t56_5, _t55_28);
    _t56_11 = _mm256_sub_pd(_t56_6, _t55_29);
    _t56_12 = _mm256_sub_pd(_t56_7, _t55_30);
    _t56_13 = _mm256_sub_pd(_t56_8, _t55_31);

    // AVX Storer:
    _t56_0 = _t56_10;
    _t56_1 = _t56_11;
    _t56_2 = _t56_12;

    // Generating : M6[12,12] = S(h(1, 12, 9), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 9)) Kro G(h(1, 12, 9), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t56_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_1, _t55_1, 32), _mm256_permute2f128_pd(_t55_1, _t55_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t56_0 = _mm256_mul_pd(_t56_14, _t56_0);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(2, 12, 10), ( G(h(2, 12, 10), M6[12,12],h(4, 12, fi35)) - ( T( G(h(1, 12, 9), U0[12,12],h(2, 12, 10)) ) * G(h(1, 12, 9), M6[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

    // AVX Loader:

    // 2x4 -> 4x4
    _t56_15 = _t56_1;
    _t56_16 = _t56_2;
    _t56_17 = _mm256_setzero_pd();
    _t56_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t56_19 = _t45_50;

    // 4-BLAC: (1x4)^T
    _t55_42 = _t56_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t55_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 32), _mm256_permute2f128_pd(_t55_42, _t55_42, 32), 0), _t56_0);
    _t55_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 32), _mm256_permute2f128_pd(_t55_42, _t55_42, 32), 15), _t56_0);
    _t55_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 49), _mm256_permute2f128_pd(_t55_42, _t55_42, 49), 0), _t56_0);
    _t55_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_42, _t55_42, 49), _mm256_permute2f128_pd(_t55_42, _t55_42, 49), 15), _t56_0);

    // 4-BLAC: 4x4 - 4x4
    _t56_20 = _mm256_sub_pd(_t56_15, _t55_43);
    _t56_21 = _mm256_sub_pd(_t56_16, _t55_44);
    _t56_22 = _mm256_sub_pd(_t56_17, _t55_45);
    _t56_23 = _mm256_sub_pd(_t56_18, _t55_46);

    // AVX Storer:
    _t56_1 = _t56_20;
    _t56_2 = _t56_21;

    // Generating : M6[12,12] = S(h(1, 12, 10), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 10)) Kro G(h(1, 12, 10), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t56_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_2, _t55_2, 32), _mm256_permute2f128_pd(_t55_2, _t55_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t56_1 = _mm256_mul_pd(_t56_24, _t56_1);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(1, 12, 11), ( G(h(1, 12, 11), M6[12,12],h(4, 12, fi35)) - ( T( G(h(1, 12, 10), U0[12,12],h(1, 12, 11)) ) Kro G(h(1, 12, 10), M6[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t56_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_54, _t45_54, 32), _mm256_permute2f128_pd(_t45_54, _t45_54, 32), 0);

    // 4-BLAC: (4x1)^T
    _t55_53 = _t56_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t55_8 = _mm256_mul_pd(_t55_53, _t56_1);

    // 4-BLAC: 1x4 - 1x4
    _t56_2 = _mm256_sub_pd(_t56_2, _t55_8);

    // AVX Storer:

    // Generating : M6[12,12] = S(h(1, 12, 11), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 11)) Kro G(h(1, 12, 11), M6[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t56_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t55_3, _t55_3, 32), _mm256_permute2f128_pd(_t55_3, _t55_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t56_2 = _mm256_mul_pd(_t56_26, _t56_2);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi35 + 96, _t56_3);
    _asm256_storeu_pd(M1 + fi35 + 108, _t56_0);
    _asm256_storeu_pd(M1 + fi35 + 120, _t56_1);
    _asm256_storeu_pd(M1 + fi35 + 132, _t56_2);
  }

  _asm256_storeu_pd(M1 + 96, _t55_7);
  _asm256_storeu_pd(M1 + 108, _t55_4);
  _asm256_storeu_pd(M1 + 120, _t55_5);
  _asm256_storeu_pd(M1 + 132, _t55_6);

  for( int fi35 = 0; fi35 <= 7; fi35+=4 ) {
    _t57_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 143])));
    _t57_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 130])));
    _t57_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 117])));
    _t57_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-13*fi35 + 104])));
    _t57_14 = _asm256_loadu_pd(M1 + -12*fi35 + 132);
    _t57_11 = _asm256_loadu_pd(M1 + -12*fi35 + 96);
    _t57_12 = _asm256_loadu_pd(M1 + -12*fi35 + 108);
    _t57_13 = _asm256_loadu_pd(M1 + -12*fi35 + 120);
    _t57_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 107)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 119))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 131)), 32);
    _t57_1 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 106)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -13*fi35 + 118)), 0);
    _t57_0 = _mm256_broadcast_sd(&(M3[-13*fi35 + 105]));

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 11), U0[12,12],h(1, 12, -fi35 + 11)) ),h(1, 12, -fi35 + 11))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t57_49 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_50 = _t57_6;

    // 4-BLAC: 1x4 / 1x4
    _t57_51 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_49), _mm256_castpd256_pd128(_t57_50)));

    // AVX Storer:
    _t57_7 = _t57_51;

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 10), U0[12,12],h(1, 12, -fi35 + 10)) ),h(1, 12, -fi35 + 10))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t57_52 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_53 = _t57_5;

    // 4-BLAC: 1x4 / 1x4
    _t57_54 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_52), _mm256_castpd256_pd128(_t57_53)));

    // AVX Storer:
    _t57_8 = _t57_54;

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 9), U0[12,12],h(1, 12, -fi35 + 9)) ),h(1, 12, -fi35 + 9))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t57_55 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_56 = _t57_4;

    // 4-BLAC: 1x4 / 1x4
    _t57_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_55), _mm256_castpd256_pd128(_t57_56)));

    // AVX Storer:
    _t57_9 = _t57_57;

    // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 8)) ),h(1, 12, -fi35 + 8))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t57_58 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_16 = _t57_3;

    // 4-BLAC: 1x4 / 1x4
    _t57_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_58), _mm256_castpd256_pd128(_t57_16)));

    // AVX Storer:
    _t57_10 = _t57_17;

    // Generating : M8[12,12] = S(h(1, 12, -fi35 + 11), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 11)) Kro G(h(1, 12, -fi35 + 11), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_7, _t57_7, 32), _mm256_permute2f128_pd(_t57_7, _t57_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_14 = _mm256_mul_pd(_t57_18, _t57_14);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(3, 12, -fi35 + 8), ( G(h(3, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) - ( G(h(3, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 11)) * G(h(1, 12, -fi35 + 11), M8[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

    // AVX Loader:

    // 3x4 -> 4x4
    _t57_19 = _t57_11;
    _t57_20 = _t57_12;
    _t57_21 = _t57_13;
    _t57_22 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t57_23 = _t57_2;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t57_24 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_23, _t57_23, 32), _mm256_permute2f128_pd(_t57_23, _t57_23, 32), 0), _t57_14);
    _t57_25 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_23, _t57_23, 32), _mm256_permute2f128_pd(_t57_23, _t57_23, 32), 15), _t57_14);
    _t57_26 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_23, _t57_23, 49), _mm256_permute2f128_pd(_t57_23, _t57_23, 49), 0), _t57_14);
    _t57_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_23, _t57_23, 49), _mm256_permute2f128_pd(_t57_23, _t57_23, 49), 15), _t57_14);

    // 4-BLAC: 4x4 - 4x4
    _t57_28 = _mm256_sub_pd(_t57_19, _t57_24);
    _t57_29 = _mm256_sub_pd(_t57_20, _t57_25);
    _t57_30 = _mm256_sub_pd(_t57_21, _t57_26);
    _t57_31 = _mm256_sub_pd(_t57_22, _t57_27);

    // AVX Storer:
    _t57_11 = _t57_28;
    _t57_12 = _t57_29;
    _t57_13 = _t57_30;

    // Generating : M8[12,12] = S(h(1, 12, -fi35 + 10), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 10)) Kro G(h(1, 12, -fi35 + 10), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_32 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_8, _t57_8, 32), _mm256_permute2f128_pd(_t57_8, _t57_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_13 = _mm256_mul_pd(_t57_32, _t57_13);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(2, 12, -fi35 + 8), ( G(h(2, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) - ( G(h(2, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 10)) * G(h(1, 12, -fi35 + 10), M8[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

    // AVX Loader:

    // 2x4 -> 4x4
    _t57_33 = _t57_11;
    _t57_34 = _t57_12;
    _t57_35 = _mm256_setzero_pd();
    _t57_36 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t57_37 = _t57_1;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t57_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_37, _t57_37, 32), _mm256_permute2f128_pd(_t57_37, _t57_37, 32), 0), _t57_13);
    _t57_39 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_37, _t57_37, 32), _mm256_permute2f128_pd(_t57_37, _t57_37, 32), 15), _t57_13);
    _t57_40 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_37, _t57_37, 49), _mm256_permute2f128_pd(_t57_37, _t57_37, 49), 0), _t57_13);
    _t57_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_37, _t57_37, 49), _mm256_permute2f128_pd(_t57_37, _t57_37, 49), 15), _t57_13);

    // 4-BLAC: 4x4 - 4x4
    _t57_42 = _mm256_sub_pd(_t57_33, _t57_38);
    _t57_43 = _mm256_sub_pd(_t57_34, _t57_39);
    _t57_44 = _mm256_sub_pd(_t57_35, _t57_40);
    _t57_45 = _mm256_sub_pd(_t57_36, _t57_41);

    // AVX Storer:
    _t57_11 = _t57_42;
    _t57_12 = _t57_43;

    // Generating : M8[12,12] = S(h(1, 12, -fi35 + 9), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 9)) Kro G(h(1, 12, -fi35 + 9), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_46 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_9, _t57_9, 32), _mm256_permute2f128_pd(_t57_9, _t57_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_12 = _mm256_mul_pd(_t57_46, _t57_12);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(1, 12, -fi35 + 8), ( G(h(1, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) - ( G(h(1, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 9)) Kro G(h(1, 12, -fi35 + 9), M8[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_47 = _t57_0;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_15 = _mm256_mul_pd(_t57_47, _t57_12);

    // 4-BLAC: 1x4 - 1x4
    _t57_11 = _mm256_sub_pd(_t57_11, _t57_15);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(1, 12, -fi35 + 8), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 8)) Kro G(h(1, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_48 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_10, _t57_10, 32), _mm256_permute2f128_pd(_t57_10, _t57_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_11 = _mm256_mul_pd(_t57_48, _t57_11);

    // AVX Storer:

    for( int fi96 = 4; fi96 <= 8; fi96+=4 ) {
      _t58_3 = _asm256_loadu_pd(M1 + -12*fi35 + fi96 + 132);
      _t58_0 = _asm256_loadu_pd(M1 + -12*fi35 + fi96 + 96);
      _t58_1 = _asm256_loadu_pd(M1 + -12*fi35 + fi96 + 108);
      _t58_2 = _asm256_loadu_pd(M1 + -12*fi35 + fi96 + 120);

      // Generating : M8[12,12] = S(h(1, 12, -fi35 + 11), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 11)) Kro G(h(1, 12, -fi35 + 11), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t58_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_7, _t57_7, 32), _mm256_permute2f128_pd(_t57_7, _t57_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t58_3 = _mm256_mul_pd(_t58_4, _t58_3);

      // AVX Storer:

      // Generating : M8[12,12] = S(h(3, 12, -fi35 + 8), ( G(h(3, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) - ( G(h(3, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 11)) * G(h(1, 12, -fi35 + 11), M8[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

      // AVX Loader:

      // 3x4 -> 4x4
      _t58_5 = _t58_0;
      _t58_6 = _t58_1;
      _t58_7 = _t58_2;
      _t58_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t58_9 = _t57_2;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t57_24 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_9, _t58_9, 32), _mm256_permute2f128_pd(_t58_9, _t58_9, 32), 0), _t58_3);
      _t57_25 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_9, _t58_9, 32), _mm256_permute2f128_pd(_t58_9, _t58_9, 32), 15), _t58_3);
      _t57_26 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_9, _t58_9, 49), _mm256_permute2f128_pd(_t58_9, _t58_9, 49), 0), _t58_3);
      _t57_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_9, _t58_9, 49), _mm256_permute2f128_pd(_t58_9, _t58_9, 49), 15), _t58_3);

      // 4-BLAC: 4x4 - 4x4
      _t58_10 = _mm256_sub_pd(_t58_5, _t57_24);
      _t58_11 = _mm256_sub_pd(_t58_6, _t57_25);
      _t58_12 = _mm256_sub_pd(_t58_7, _t57_26);
      _t58_13 = _mm256_sub_pd(_t58_8, _t57_27);

      // AVX Storer:
      _t58_0 = _t58_10;
      _t58_1 = _t58_11;
      _t58_2 = _t58_12;

      // Generating : M8[12,12] = S(h(1, 12, -fi35 + 10), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 10)) Kro G(h(1, 12, -fi35 + 10), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t58_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_8, _t57_8, 32), _mm256_permute2f128_pd(_t57_8, _t57_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t58_2 = _mm256_mul_pd(_t58_14, _t58_2);

      // AVX Storer:

      // Generating : M8[12,12] = S(h(2, 12, -fi35 + 8), ( G(h(2, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) - ( G(h(2, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 10)) * G(h(1, 12, -fi35 + 10), M8[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

      // AVX Loader:

      // 2x4 -> 4x4
      _t58_15 = _t58_0;
      _t58_16 = _t58_1;
      _t58_17 = _mm256_setzero_pd();
      _t58_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t58_19 = _t57_1;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t57_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 0), _t58_2);
      _t57_39 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 15), _t58_2);
      _t57_40 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 0), _t58_2);
      _t57_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 15), _t58_2);

      // 4-BLAC: 4x4 - 4x4
      _t58_20 = _mm256_sub_pd(_t58_15, _t57_38);
      _t58_21 = _mm256_sub_pd(_t58_16, _t57_39);
      _t58_22 = _mm256_sub_pd(_t58_17, _t57_40);
      _t58_23 = _mm256_sub_pd(_t58_18, _t57_41);

      // AVX Storer:
      _t58_0 = _t58_20;
      _t58_1 = _t58_21;

      // Generating : M8[12,12] = S(h(1, 12, -fi35 + 9), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 9)) Kro G(h(1, 12, -fi35 + 9), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t58_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_9, _t57_9, 32), _mm256_permute2f128_pd(_t57_9, _t57_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t58_1 = _mm256_mul_pd(_t58_24, _t58_1);

      // AVX Storer:

      // Generating : M8[12,12] = S(h(1, 12, -fi35 + 8), ( G(h(1, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) - ( G(h(1, 12, -fi35 + 8), U0[12,12],h(1, 12, -fi35 + 9)) Kro G(h(1, 12, -fi35 + 9), M8[12,12],h(4, 12, fi96)) ) ),h(4, 12, fi96))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t58_25 = _t57_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t57_15 = _mm256_mul_pd(_t58_25, _t58_1);

      // 4-BLAC: 1x4 - 1x4
      _t58_0 = _mm256_sub_pd(_t58_0, _t57_15);

      // AVX Storer:

      // Generating : M8[12,12] = S(h(1, 12, -fi35 + 8), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, -fi35 + 8)) Kro G(h(1, 12, -fi35 + 8), M8[12,12],h(4, 12, fi96)) ),h(4, 12, fi96))

      // AVX Loader:

      // 1x1 -> 1x4
      _t58_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_10, _t57_10, 32), _mm256_permute2f128_pd(_t57_10, _t57_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t58_0 = _mm256_mul_pd(_t58_26, _t58_0);

      // AVX Storer:
      _asm256_storeu_pd(M1 + -12*fi35 + fi96 + 132, _t58_3);
      _asm256_storeu_pd(M1 + -12*fi35 + fi96 + 120, _t58_2);
      _asm256_storeu_pd(M1 + -12*fi35 + fi96 + 108, _t58_1);
      _asm256_storeu_pd(M1 + -12*fi35 + fi96 + 96, _t58_0);
    }

    // Generating : M8[12,12] = Sum_{k3} ( Sum_{k2} ( S(h(4, 12, k3), ( G(h(4, 12, k3), M8[12,12],h(4, 12, k2)) - ( G(h(4, 12, k3), U0[12,12],h(4, 12, -fi35 + 8)) * G(h(4, 12, -fi35 + 8), M8[12,12],h(4, 12, k2)) ) ),h(4, 12, k2)) ) )
    _asm256_storeu_pd(M1 + -12*fi35 + 132, _t57_14);
    _asm256_storeu_pd(M1 + -12*fi35 + 120, _t57_13);
    _asm256_storeu_pd(M1 + -12*fi35 + 108, _t57_12);
    _asm256_storeu_pd(M1 + -12*fi35 + 96, _t57_11);

    for( int k3 = 0; k3 <= -fi35 + 7; k3+=4 ) {

      for( int k2 = 0; k2 <= 11; k2+=4 ) {
        _t59_24 = _asm256_loadu_pd(M1 + k2 + 12*k3);
        _t59_25 = _asm256_loadu_pd(M1 + k2 + 12*k3 + 12);
        _t59_26 = _asm256_loadu_pd(M1 + k2 + 12*k3 + 24);
        _t59_27 = _asm256_loadu_pd(M1 + k2 + 12*k3 + 36);
        _t59_19 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 8);
        _t59_18 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 9);
        _t59_17 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 10);
        _t59_16 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 11);
        _t59_15 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 20);
        _t59_14 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 21);
        _t59_13 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 22);
        _t59_12 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 23);
        _t59_11 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 32);
        _t59_10 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 33);
        _t59_9 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 34);
        _t59_8 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 35);
        _t59_7 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 44);
        _t59_6 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 45);
        _t59_5 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 46);
        _t59_4 = _mm256_broadcast_sd(M3 + -fi35 + 12*k3 + 47);
        _t59_3 = _asm256_loadu_pd(M1 + -12*fi35 + k2 + 96);
        _t59_2 = _asm256_loadu_pd(M1 + -12*fi35 + k2 + 108);
        _t59_1 = _asm256_loadu_pd(M1 + -12*fi35 + k2 + 120);
        _t59_0 = _asm256_loadu_pd(M1 + -12*fi35 + k2 + 132);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t59_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_19, _t59_3), _mm256_mul_pd(_t59_18, _t59_2)), _mm256_add_pd(_mm256_mul_pd(_t59_17, _t59_1), _mm256_mul_pd(_t59_16, _t59_0)));
        _t59_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_15, _t59_3), _mm256_mul_pd(_t59_14, _t59_2)), _mm256_add_pd(_mm256_mul_pd(_t59_13, _t59_1), _mm256_mul_pd(_t59_12, _t59_0)));
        _t59_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_11, _t59_3), _mm256_mul_pd(_t59_10, _t59_2)), _mm256_add_pd(_mm256_mul_pd(_t59_9, _t59_1), _mm256_mul_pd(_t59_8, _t59_0)));
        _t59_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_7, _t59_3), _mm256_mul_pd(_t59_6, _t59_2)), _mm256_add_pd(_mm256_mul_pd(_t59_5, _t59_1), _mm256_mul_pd(_t59_4, _t59_0)));

        // 4-BLAC: 4x4 - 4x4
        _t59_24 = _mm256_sub_pd(_t59_24, _t59_20);
        _t59_25 = _mm256_sub_pd(_t59_25, _t59_21);
        _t59_26 = _mm256_sub_pd(_t59_26, _t59_22);
        _t59_27 = _mm256_sub_pd(_t59_27, _t59_23);

        // AVX Storer:
        _asm256_storeu_pd(M1 + k2 + 12*k3, _t59_24);
        _asm256_storeu_pd(M1 + k2 + 12*k3 + 12, _t59_25);
        _asm256_storeu_pd(M1 + k2 + 12*k3 + 24, _t59_26);
        _asm256_storeu_pd(M1 + k2 + 12*k3 + 36, _t59_27);
      }
    }
  }

  _t45_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t45_8 = _mm256_maskload_pd(M3 + 14, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t45_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t45_13 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[39])));
  _t45_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[13])));
  _t45_12 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[27])));
  _t45_11 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[26])));
  _t60_7 = _asm256_loadu_pd(M1 + 36);
  _t60_4 = _asm256_loadu_pd(M1);
  _t60_5 = _asm256_loadu_pd(M1 + 12);
  _t60_6 = _asm256_loadu_pd(M1 + 24);

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 3), U0[12,12],h(1, 12, 3)) ),h(1, 12, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_10 = _t45_13;

  // 4-BLAC: 1x4 / 1x4
  _t60_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_9), _mm256_castpd256_pd128(_t60_10)));

  // AVX Storer:
  _t60_0 = _t60_11;

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 2), U0[12,12],h(1, 12, 2)) ),h(1, 12, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_13 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t60_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_12), _mm256_castpd256_pd128(_t60_13)));

  // AVX Storer:
  _t60_1 = _t60_14;

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 1), U0[12,12],h(1, 12, 1)) ),h(1, 12, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_16 = _t45_6;

  // 4-BLAC: 1x4 / 1x4
  _t60_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_15), _mm256_castpd256_pd128(_t60_16)));

  // AVX Storer:
  _t60_2 = _t60_17;

  // Generating : T2013[1,12] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 12, 0), U0[12,12],h(1, 12, 0)) ),h(1, 12, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_19 = _t45_0;

  // 4-BLAC: 1x4 / 1x4
  _t60_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_18), _mm256_castpd256_pd128(_t60_19)));

  // AVX Storer:
  _t60_3 = _t60_20;

  // Generating : M8[12,12] = S(h(1, 12, 3), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 3)) Kro G(h(1, 12, 3), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_0, _t60_0, 32), _mm256_permute2f128_pd(_t60_0, _t60_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_7 = _mm256_mul_pd(_t60_21, _t60_7);

  // AVX Storer:

  // Generating : M8[12,12] = S(h(3, 12, 0), ( G(h(3, 12, 0), M8[12,12],h(4, 12, fi35)) - ( G(h(3, 12, 0), U0[12,12],h(1, 12, 3)) * G(h(1, 12, 3), M8[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

  // AVX Loader:

  // 3x4 -> 4x4
  _t60_22 = _t60_4;
  _t60_23 = _t60_5;
  _t60_24 = _t60_6;
  _t60_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t60_26 = _mm256_blend_pd(_mm256_permute2f128_pd(_t45_2, _t45_12, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t45_8, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t60_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_26, _t60_26, 32), _mm256_permute2f128_pd(_t60_26, _t60_26, 32), 0), _t60_7);
  _t60_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_26, _t60_26, 32), _mm256_permute2f128_pd(_t60_26, _t60_26, 32), 15), _t60_7);
  _t60_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_26, _t60_26, 49), _mm256_permute2f128_pd(_t60_26, _t60_26, 49), 0), _t60_7);
  _t60_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_26, _t60_26, 49), _mm256_permute2f128_pd(_t60_26, _t60_26, 49), 15), _t60_7);

  // 4-BLAC: 4x4 - 4x4
  _t60_31 = _mm256_sub_pd(_t60_22, _t60_27);
  _t60_32 = _mm256_sub_pd(_t60_23, _t60_28);
  _t60_33 = _mm256_sub_pd(_t60_24, _t60_29);
  _t60_34 = _mm256_sub_pd(_t60_25, _t60_30);

  // AVX Storer:
  _t60_4 = _t60_31;
  _t60_5 = _t60_32;
  _t60_6 = _t60_33;

  // Generating : M8[12,12] = S(h(1, 12, 2), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 2)) Kro G(h(1, 12, 2), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_1, _t60_1, 32), _mm256_permute2f128_pd(_t60_1, _t60_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_6 = _mm256_mul_pd(_t60_35, _t60_6);

  // AVX Storer:

  // Generating : M8[12,12] = S(h(2, 12, 0), ( G(h(2, 12, 0), M8[12,12],h(4, 12, fi35)) - ( G(h(2, 12, 0), U0[12,12],h(1, 12, 2)) * G(h(1, 12, 2), M8[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

  // AVX Loader:

  // 2x4 -> 4x4
  _t60_36 = _t60_4;
  _t60_37 = _t60_5;
  _t60_38 = _mm256_setzero_pd();
  _t60_39 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t60_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t45_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t45_8, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t60_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_40, _t60_40, 32), _mm256_permute2f128_pd(_t60_40, _t60_40, 32), 0), _t60_6);
  _t60_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_40, _t60_40, 32), _mm256_permute2f128_pd(_t60_40, _t60_40, 32), 15), _t60_6);
  _t60_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_40, _t60_40, 49), _mm256_permute2f128_pd(_t60_40, _t60_40, 49), 0), _t60_6);
  _t60_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_40, _t60_40, 49), _mm256_permute2f128_pd(_t60_40, _t60_40, 49), 15), _t60_6);

  // 4-BLAC: 4x4 - 4x4
  _t60_45 = _mm256_sub_pd(_t60_36, _t60_41);
  _t60_46 = _mm256_sub_pd(_t60_37, _t60_42);
  _t60_47 = _mm256_sub_pd(_t60_38, _t60_43);
  _t60_48 = _mm256_sub_pd(_t60_39, _t60_44);

  // AVX Storer:
  _t60_4 = _t60_45;
  _t60_5 = _t60_46;

  // Generating : M8[12,12] = S(h(1, 12, 1), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_2, _t60_2, 32), _mm256_permute2f128_pd(_t60_2, _t60_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_5 = _mm256_mul_pd(_t60_49, _t60_5);

  // AVX Storer:

  // Generating : M8[12,12] = S(h(1, 12, 0), ( G(h(1, 12, 0), M8[12,12],h(4, 12, fi35)) - ( G(h(1, 12, 0), U0[12,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M8[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_2, _t45_2, 32), _mm256_permute2f128_pd(_t45_2, _t45_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_8 = _mm256_mul_pd(_t60_50, _t60_5);

  // 4-BLAC: 1x4 - 1x4
  _t60_4 = _mm256_sub_pd(_t60_4, _t60_8);

  // AVX Storer:

  // Generating : M8[12,12] = S(h(1, 12, 0), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_3, _t60_3, 32), _mm256_permute2f128_pd(_t60_3, _t60_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_4 = _mm256_mul_pd(_t60_51, _t60_4);

  // AVX Storer:


  for( int fi35 = 4; fi35 <= 8; fi35+=4 ) {
    _t61_3 = _asm256_loadu_pd(M1 + fi35 + 36);
    _t61_0 = _asm256_loadu_pd(M1 + fi35);
    _t61_1 = _asm256_loadu_pd(M1 + fi35 + 12);
    _t61_2 = _asm256_loadu_pd(M1 + fi35 + 24);

    // Generating : M8[12,12] = S(h(1, 12, 3), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 3)) Kro G(h(1, 12, 3), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_0, _t60_0, 32), _mm256_permute2f128_pd(_t60_0, _t60_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_3 = _mm256_mul_pd(_t61_4, _t61_3);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(3, 12, 0), ( G(h(3, 12, 0), M8[12,12],h(4, 12, fi35)) - ( G(h(3, 12, 0), U0[12,12],h(1, 12, 3)) * G(h(1, 12, 3), M8[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

    // AVX Loader:

    // 3x4 -> 4x4
    _t61_5 = _t61_0;
    _t61_6 = _t61_1;
    _t61_7 = _t61_2;
    _t61_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t61_9 = _mm256_blend_pd(_mm256_permute2f128_pd(_t45_2, _t45_12, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t45_8, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_9, _t61_9, 32), _mm256_permute2f128_pd(_t61_9, _t61_9, 32), 0), _t61_3);
    _t60_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_9, _t61_9, 32), _mm256_permute2f128_pd(_t61_9, _t61_9, 32), 15), _t61_3);
    _t60_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_9, _t61_9, 49), _mm256_permute2f128_pd(_t61_9, _t61_9, 49), 0), _t61_3);
    _t60_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_9, _t61_9, 49), _mm256_permute2f128_pd(_t61_9, _t61_9, 49), 15), _t61_3);

    // 4-BLAC: 4x4 - 4x4
    _t61_10 = _mm256_sub_pd(_t61_5, _t60_27);
    _t61_11 = _mm256_sub_pd(_t61_6, _t60_28);
    _t61_12 = _mm256_sub_pd(_t61_7, _t60_29);
    _t61_13 = _mm256_sub_pd(_t61_8, _t60_30);

    // AVX Storer:
    _t61_0 = _t61_10;
    _t61_1 = _t61_11;
    _t61_2 = _t61_12;

    // Generating : M8[12,12] = S(h(1, 12, 2), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 2)) Kro G(h(1, 12, 2), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_1, _t60_1, 32), _mm256_permute2f128_pd(_t60_1, _t60_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_2 = _mm256_mul_pd(_t61_14, _t61_2);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(2, 12, 0), ( G(h(2, 12, 0), M8[12,12],h(4, 12, fi35)) - ( G(h(2, 12, 0), U0[12,12],h(1, 12, 2)) * G(h(1, 12, 2), M8[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

    // AVX Loader:

    // 2x4 -> 4x4
    _t61_15 = _t61_0;
    _t61_16 = _t61_1;
    _t61_17 = _mm256_setzero_pd();
    _t61_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t61_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t45_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t45_8, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_19, _t61_19, 32), _mm256_permute2f128_pd(_t61_19, _t61_19, 32), 0), _t61_2);
    _t60_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_19, _t61_19, 32), _mm256_permute2f128_pd(_t61_19, _t61_19, 32), 15), _t61_2);
    _t60_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_19, _t61_19, 49), _mm256_permute2f128_pd(_t61_19, _t61_19, 49), 0), _t61_2);
    _t60_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t61_19, _t61_19, 49), _mm256_permute2f128_pd(_t61_19, _t61_19, 49), 15), _t61_2);

    // 4-BLAC: 4x4 - 4x4
    _t61_20 = _mm256_sub_pd(_t61_15, _t60_41);
    _t61_21 = _mm256_sub_pd(_t61_16, _t60_42);
    _t61_22 = _mm256_sub_pd(_t61_17, _t60_43);
    _t61_23 = _mm256_sub_pd(_t61_18, _t60_44);

    // AVX Storer:
    _t61_0 = _t61_20;
    _t61_1 = _t61_21;

    // Generating : M8[12,12] = S(h(1, 12, 1), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_2, _t60_2, 32), _mm256_permute2f128_pd(_t60_2, _t60_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_1 = _mm256_mul_pd(_t61_24, _t61_1);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(1, 12, 0), ( G(h(1, 12, 0), M8[12,12],h(4, 12, fi35)) - ( G(h(1, 12, 0), U0[12,12],h(1, 12, 1)) Kro G(h(1, 12, 1), M8[12,12],h(4, 12, fi35)) ) ),h(4, 12, fi35))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_2, _t45_2, 32), _mm256_permute2f128_pd(_t45_2, _t45_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_8 = _mm256_mul_pd(_t61_25, _t61_1);

    // 4-BLAC: 1x4 - 1x4
    _t61_0 = _mm256_sub_pd(_t61_0, _t60_8);

    // AVX Storer:

    // Generating : M8[12,12] = S(h(1, 12, 0), ( G(h(1, 1, 0), T2013[1,12],h(1, 12, 0)) Kro G(h(1, 12, 0), M8[12,12],h(4, 12, fi35)) ),h(4, 12, fi35))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_3, _t60_3, 32), _mm256_permute2f128_pd(_t60_3, _t60_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_0 = _mm256_mul_pd(_t61_26, _t61_0);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi35 + 36, _t61_3);
    _asm256_storeu_pd(M1 + fi35 + 24, _t61_2);
    _asm256_storeu_pd(M1 + fi35 + 12, _t61_1);
    _asm256_storeu_pd(M1 + fi35, _t61_0);
  }


  // Generating : x[12,1] = ( Sum_{k2} ( S(h(4, 12, k2), ( G(h(4, 12, k2), y[12,1],h(1, 1, 0)) + ( G(h(4, 12, k2), M2[12,12],h(4, 12, 0)) * G(h(4, 12, 0), v0[12,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( $(h(4, 12, k2), ( G(h(4, 12, k2), M2[12,12],h(4, 12, k3)) * G(h(4, 12, k3), v0[12,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t51_1);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t51_3);

  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t62_4 = _asm256_loadu_pd(y + k2);
    _t62_3 = _asm256_loadu_pd(M2 + 12*k2);
    _t62_2 = _asm256_loadu_pd(M2 + 12*k2 + 12);
    _t62_1 = _asm256_loadu_pd(M2 + 12*k2 + 24);
    _t62_0 = _asm256_loadu_pd(M2 + 12*k2 + 36);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t62_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t62_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32)), _mm256_mul_pd(_t62_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t62_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32)), _mm256_mul_pd(_t62_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t62_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32)), _mm256_mul_pd(_t62_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t62_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32)), _mm256_mul_pd(_t62_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_5, _t51_4), _mm256_unpacklo_pd(_t51_2, _t51_0), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t62_5 = _mm256_add_pd(_t62_4, _t62_6);

    // AVX Storer:
    _asm256_storeu_pd(x + k2, _t62_5);
  }


  for( int k3 = 4; k3 <= 11; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 11; k2+=4 ) {
      _t63_4 = _asm256_loadu_pd(M2 + 12*k2 + k3);
      _t63_3 = _asm256_loadu_pd(M2 + 12*k2 + k3 + 12);
      _t63_2 = _asm256_loadu_pd(M2 + 12*k2 + k3 + 24);
      _t63_1 = _asm256_loadu_pd(M2 + 12*k2 + k3 + 36);
      _t63_0 = _asm256_loadu_pd(v0 + k3);
      _t63_5 = _asm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t63_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t63_4, _t63_0), _mm256_mul_pd(_t63_3, _t63_0)), _mm256_hadd_pd(_mm256_mul_pd(_t63_2, _t63_0), _mm256_mul_pd(_t63_1, _t63_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t63_4, _t63_0), _mm256_mul_pd(_t63_3, _t63_0)), _mm256_hadd_pd(_mm256_mul_pd(_t63_2, _t63_0), _mm256_mul_pd(_t63_1, _t63_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t63_5 = _mm256_add_pd(_t63_5, _t63_6);

      // AVX Storer:
      _asm256_storeu_pd(x + k2, _t63_5);
    }
  }


  // Generating : P[12,12] = ( ( Sum_{k2} ( ( S(h(4, 12, k2), ( G(h(4, 12, k2), Y[12,12],h(4, 12, k2)) - ( G(h(4, 12, k2), M2[12,12],h(4, 12, 0)) * G(h(4, 12, 0), M1[12,12],h(4, 12, k2)) ) ),h(4, 12, k2)) + Sum_{i0} ( S(h(4, 12, k2), ( G(h(4, 12, k2), Y[12,12],h(4, 12, i0)) - ( G(h(4, 12, k2), M2[12,12],h(4, 12, 0)) * G(h(4, 12, 0), M1[12,12],h(4, 12, i0)) ) ),h(4, 12, i0)) ) ) ) + S(h(4, 12, 8), ( G(h(4, 12, 8), Y[12,12],h(4, 12, 8)) - ( G(h(4, 12, 8), M2[12,12],h(4, 12, 0)) * G(h(4, 12, 0), M1[12,12],h(4, 12, 8)) ) ),h(4, 12, 8)) ) + Sum_{k3} ( ( Sum_{k2} ( ( -$(h(4, 12, k2), ( G(h(4, 12, k2), M2[12,12],h(4, 12, k3)) * G(h(4, 12, k3), M1[12,12],h(4, 12, k2)) ),h(4, 12, k2)) + Sum_{i0} ( -$(h(4, 12, k2), ( G(h(4, 12, k2), M2[12,12],h(4, 12, k3)) * G(h(4, 12, k3), M1[12,12],h(4, 12, i0)) ),h(4, 12, i0)) ) ) ) + -$(h(4, 12, 8), ( G(h(4, 12, 8), M2[12,12],h(4, 12, k3)) * G(h(4, 12, k3), M1[12,12],h(4, 12, 8)) ),h(4, 12, 8)) ) ) )

  _asm256_storeu_pd(M1 + 36, _t60_7);
  _asm256_storeu_pd(M1 + 24, _t60_6);
  _asm256_storeu_pd(M1 + 12, _t60_5);
  _asm256_storeu_pd(M1, _t60_4);

  for( int k2 = 0; k2 <= 7; k2+=4 ) {
    _t64_23 = _asm256_loadu_pd(Y + 13*k2);
    _t64_22 = _mm256_maskload_pd(Y + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t64_21 = _mm256_maskload_pd(Y + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t64_20 = _mm256_maskload_pd(Y + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t64_19 = _mm256_broadcast_sd(M2 + 12*k2);
    _t64_18 = _mm256_broadcast_sd(M2 + 12*k2 + 1);
    _t64_17 = _mm256_broadcast_sd(M2 + 12*k2 + 2);
    _t64_16 = _mm256_broadcast_sd(M2 + 12*k2 + 3);
    _t64_15 = _mm256_broadcast_sd(M2 + 12*k2 + 12);
    _t64_14 = _mm256_broadcast_sd(M2 + 12*k2 + 13);
    _t64_13 = _mm256_broadcast_sd(M2 + 12*k2 + 14);
    _t64_12 = _mm256_broadcast_sd(M2 + 12*k2 + 15);
    _t64_11 = _mm256_broadcast_sd(M2 + 12*k2 + 24);
    _t64_10 = _mm256_broadcast_sd(M2 + 12*k2 + 25);
    _t64_9 = _mm256_broadcast_sd(M2 + 12*k2 + 26);
    _t64_8 = _mm256_broadcast_sd(M2 + 12*k2 + 27);
    _t64_7 = _mm256_broadcast_sd(M2 + 12*k2 + 36);
    _t64_6 = _mm256_broadcast_sd(M2 + 12*k2 + 37);
    _t64_5 = _mm256_broadcast_sd(M2 + 12*k2 + 38);
    _t64_4 = _mm256_broadcast_sd(M2 + 12*k2 + 39);
    _t64_3 = _asm256_loadu_pd(M1 + k2);
    _t64_2 = _asm256_loadu_pd(M1 + k2 + 12);
    _t64_1 = _asm256_loadu_pd(M1 + k2 + 24);
    _t64_0 = _asm256_loadu_pd(M1 + k2 + 36);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t64_36 = _t64_23;
    _t64_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t64_23, _t64_22, 3), _t64_22, 12);
    _t64_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t64_23, _t64_22, 0), _t64_21, 49);
    _t64_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t64_23, _t64_22, 12), _mm256_shuffle_pd(_t64_21, _t64_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t64_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_19, _t64_3), _mm256_mul_pd(_t64_18, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_17, _t64_1), _mm256_mul_pd(_t64_16, _t64_0)));
    _t64_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_15, _t64_3), _mm256_mul_pd(_t64_14, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_13, _t64_1), _mm256_mul_pd(_t64_12, _t64_0)));
    _t64_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_11, _t64_3), _mm256_mul_pd(_t64_10, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_9, _t64_1), _mm256_mul_pd(_t64_8, _t64_0)));
    _t64_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_7, _t64_3), _mm256_mul_pd(_t64_6, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_5, _t64_1), _mm256_mul_pd(_t64_4, _t64_0)));

    // 4-BLAC: 4x4 - 4x4
    _t64_32 = _mm256_sub_pd(_t64_36, _t64_28);
    _t64_33 = _mm256_sub_pd(_t64_37, _t64_29);
    _t64_34 = _mm256_sub_pd(_t64_38, _t64_30);
    _t64_35 = _mm256_sub_pd(_t64_39, _t64_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t64_24 = _t64_32;
    _t64_25 = _t64_33;
    _t64_26 = _t64_34;
    _t64_27 = _t64_35;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 11; i0+=4 ) {
      _t65_7 = _asm256_loadu_pd(Y + i0 + 12*k2);
      _t65_6 = _asm256_loadu_pd(Y + i0 + 12*k2 + 12);
      _t65_5 = _asm256_loadu_pd(Y + i0 + 12*k2 + 24);
      _t65_4 = _asm256_loadu_pd(Y + i0 + 12*k2 + 36);
      _t65_3 = _asm256_loadu_pd(M1 + i0);
      _t65_2 = _asm256_loadu_pd(M1 + i0 + 12);
      _t65_1 = _asm256_loadu_pd(M1 + i0 + 24);
      _t65_0 = _asm256_loadu_pd(M1 + i0 + 36);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t65_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_19, _t65_3), _mm256_mul_pd(_t64_18, _t65_2)), _mm256_add_pd(_mm256_mul_pd(_t64_17, _t65_1), _mm256_mul_pd(_t64_16, _t65_0)));
      _t65_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_15, _t65_3), _mm256_mul_pd(_t64_14, _t65_2)), _mm256_add_pd(_mm256_mul_pd(_t64_13, _t65_1), _mm256_mul_pd(_t64_12, _t65_0)));
      _t65_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_11, _t65_3), _mm256_mul_pd(_t64_10, _t65_2)), _mm256_add_pd(_mm256_mul_pd(_t64_9, _t65_1), _mm256_mul_pd(_t64_8, _t65_0)));
      _t65_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_7, _t65_3), _mm256_mul_pd(_t64_6, _t65_2)), _mm256_add_pd(_mm256_mul_pd(_t64_5, _t65_1), _mm256_mul_pd(_t64_4, _t65_0)));

      // 4-BLAC: 4x4 - 4x4
      _t65_12 = _mm256_sub_pd(_t65_7, _t65_8);
      _t65_13 = _mm256_sub_pd(_t65_6, _t65_9);
      _t65_14 = _mm256_sub_pd(_t65_5, _t65_10);
      _t65_15 = _mm256_sub_pd(_t65_4, _t65_11);

      // AVX Storer:
      _asm256_storeu_pd(P + i0 + 12*k2, _t65_12);
      _asm256_storeu_pd(P + i0 + 12*k2 + 12, _t65_13);
      _asm256_storeu_pd(P + i0 + 12*k2 + 24, _t65_14);
      _asm256_storeu_pd(P + i0 + 12*k2 + 36, _t65_15);
    }
    _asm256_storeu_pd(P + 13*k2, _t64_24);
    _mm256_maskstore_pd(P + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t64_25);
    _mm256_maskstore_pd(P + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t64_26);
    _mm256_maskstore_pd(P + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t64_27);
  }

  _t66_19 = _mm256_broadcast_sd(M2 + 96);
  _t66_18 = _mm256_broadcast_sd(M2 + 97);
  _t66_17 = _mm256_broadcast_sd(M2 + 98);
  _t66_16 = _mm256_broadcast_sd(M2 + 99);
  _t66_15 = _mm256_broadcast_sd(M2 + 108);
  _t66_14 = _mm256_broadcast_sd(M2 + 109);
  _t66_13 = _mm256_broadcast_sd(M2 + 110);
  _t66_12 = _mm256_broadcast_sd(M2 + 111);
  _t66_11 = _mm256_broadcast_sd(M2 + 120);
  _t66_10 = _mm256_broadcast_sd(M2 + 121);
  _t66_9 = _mm256_broadcast_sd(M2 + 122);
  _t66_8 = _mm256_broadcast_sd(M2 + 123);
  _t66_7 = _mm256_broadcast_sd(M2 + 132);
  _t66_6 = _mm256_broadcast_sd(M2 + 133);
  _t66_5 = _mm256_broadcast_sd(M2 + 134);
  _t66_4 = _mm256_broadcast_sd(M2 + 135);
  _t66_3 = _asm256_loadu_pd(M1 + 8);
  _t66_2 = _asm256_loadu_pd(M1 + 20);
  _t66_1 = _asm256_loadu_pd(M1 + 32);
  _t66_0 = _asm256_loadu_pd(M1 + 44);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t66_28 = _t13_28;
  _t66_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 3), _t13_29, 12);
  _t66_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 0), _t13_30, 49);
  _t66_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_28, _t13_29, 12), _mm256_shuffle_pd(_t13_30, _t13_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t66_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t66_19, _t66_3), _mm256_mul_pd(_t66_18, _t66_2)), _mm256_add_pd(_mm256_mul_pd(_t66_17, _t66_1), _mm256_mul_pd(_t66_16, _t66_0)));
  _t66_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t66_15, _t66_3), _mm256_mul_pd(_t66_14, _t66_2)), _mm256_add_pd(_mm256_mul_pd(_t66_13, _t66_1), _mm256_mul_pd(_t66_12, _t66_0)));
  _t66_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t66_11, _t66_3), _mm256_mul_pd(_t66_10, _t66_2)), _mm256_add_pd(_mm256_mul_pd(_t66_9, _t66_1), _mm256_mul_pd(_t66_8, _t66_0)));
  _t66_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t66_7, _t66_3), _mm256_mul_pd(_t66_6, _t66_2)), _mm256_add_pd(_mm256_mul_pd(_t66_5, _t66_1), _mm256_mul_pd(_t66_4, _t66_0)));

  // 4-BLAC: 4x4 - 4x4
  _t66_24 = _mm256_sub_pd(_t66_28, _t66_20);
  _t66_25 = _mm256_sub_pd(_t66_29, _t66_21);
  _t66_26 = _mm256_sub_pd(_t66_30, _t66_22);
  _t66_27 = _mm256_sub_pd(_t66_31, _t66_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t8_3 = _t66_24;
  _t8_2 = _t66_25;
  _t8_1 = _t66_26;
  _t8_0 = _t66_27;


  for( int k3 = 4; k3 <= 11; k3+=4 ) {

    for( int k2 = 0; k2 <= 7; k2+=4 ) {
      _t67_19 = _mm256_broadcast_sd(M2 + 12*k2 + k3);
      _t67_18 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 1);
      _t67_17 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 2);
      _t67_16 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 3);
      _t67_15 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 12);
      _t67_14 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 13);
      _t67_13 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 14);
      _t67_12 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 15);
      _t67_11 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 24);
      _t67_10 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 25);
      _t67_9 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 26);
      _t67_8 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 27);
      _t67_7 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 36);
      _t67_6 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 37);
      _t67_5 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 38);
      _t67_4 = _mm256_broadcast_sd(M2 + 12*k2 + k3 + 39);
      _t67_3 = _asm256_loadu_pd(M1 + k2 + 12*k3);
      _t67_2 = _asm256_loadu_pd(M1 + k2 + 12*k3 + 12);
      _t67_1 = _asm256_loadu_pd(M1 + k2 + 12*k3 + 24);
      _t67_0 = _asm256_loadu_pd(M1 + k2 + 12*k3 + 36);
      _t67_20 = _asm256_loadu_pd(P + 13*k2);
      _t67_21 = _mm256_maskload_pd(P + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t67_22 = _mm256_maskload_pd(P + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t67_23 = _mm256_maskload_pd(P + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t67_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_19, _t67_3), _mm256_mul_pd(_t67_18, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_17, _t67_1), _mm256_mul_pd(_t67_16, _t67_0)));
      _t67_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_15, _t67_3), _mm256_mul_pd(_t67_14, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_13, _t67_1), _mm256_mul_pd(_t67_12, _t67_0)));
      _t67_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_11, _t67_3), _mm256_mul_pd(_t67_10, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_9, _t67_1), _mm256_mul_pd(_t67_8, _t67_0)));
      _t67_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_7, _t67_3), _mm256_mul_pd(_t67_6, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_5, _t67_1), _mm256_mul_pd(_t67_4, _t67_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t67_28 = _t67_20;
      _t67_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t67_20, _t67_21, 3), _t67_21, 12);
      _t67_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t67_20, _t67_21, 0), _t67_22, 49);
      _t67_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t67_20, _t67_21, 12), _mm256_shuffle_pd(_t67_22, _t67_23, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t67_28 = _mm256_sub_pd(_t67_28, _t67_24);
      _t67_29 = _mm256_sub_pd(_t67_29, _t67_25);
      _t67_30 = _mm256_sub_pd(_t67_30, _t67_26);
      _t67_31 = _mm256_sub_pd(_t67_31, _t67_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t67_20 = _t67_28;
      _t67_21 = _t67_29;
      _t67_22 = _t67_30;
      _t67_23 = _t67_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 11; i0+=4 ) {
        _t68_3 = _asm256_loadu_pd(M1 + i0 + 12*k3);
        _t68_2 = _asm256_loadu_pd(M1 + i0 + 12*k3 + 12);
        _t68_1 = _asm256_loadu_pd(M1 + i0 + 12*k3 + 24);
        _t68_0 = _asm256_loadu_pd(M1 + i0 + 12*k3 + 36);
        _t68_4 = _asm256_loadu_pd(P + i0 + 12*k2);
        _t68_5 = _asm256_loadu_pd(P + i0 + 12*k2 + 12);
        _t68_6 = _asm256_loadu_pd(P + i0 + 12*k2 + 24);
        _t68_7 = _asm256_loadu_pd(P + i0 + 12*k2 + 36);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t68_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_19, _t68_3), _mm256_mul_pd(_t67_18, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t67_17, _t68_1), _mm256_mul_pd(_t67_16, _t68_0)));
        _t68_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_15, _t68_3), _mm256_mul_pd(_t67_14, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t67_13, _t68_1), _mm256_mul_pd(_t67_12, _t68_0)));
        _t68_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_11, _t68_3), _mm256_mul_pd(_t67_10, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t67_9, _t68_1), _mm256_mul_pd(_t67_8, _t68_0)));
        _t68_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_7, _t68_3), _mm256_mul_pd(_t67_6, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t67_5, _t68_1), _mm256_mul_pd(_t67_4, _t68_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t68_4 = _mm256_sub_pd(_t68_4, _t68_8);
        _t68_5 = _mm256_sub_pd(_t68_5, _t68_9);
        _t68_6 = _mm256_sub_pd(_t68_6, _t68_10);
        _t68_7 = _mm256_sub_pd(_t68_7, _t68_11);

        // AVX Storer:
        _asm256_storeu_pd(P + i0 + 12*k2, _t68_4);
        _asm256_storeu_pd(P + i0 + 12*k2 + 12, _t68_5);
        _asm256_storeu_pd(P + i0 + 12*k2 + 24, _t68_6);
        _asm256_storeu_pd(P + i0 + 12*k2 + 36, _t68_7);
      }
      _asm256_storeu_pd(P + 13*k2, _t67_20);
      _mm256_maskstore_pd(P + 13*k2 + 12, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t67_21);
      _mm256_maskstore_pd(P + 13*k2 + 24, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t67_22);
      _mm256_maskstore_pd(P + 13*k2 + 36, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t67_23);
    }
    _t69_19 = _mm256_broadcast_sd(M2 + k3 + 96);
    _t69_18 = _mm256_broadcast_sd(M2 + k3 + 97);
    _t69_17 = _mm256_broadcast_sd(M2 + k3 + 98);
    _t69_16 = _mm256_broadcast_sd(M2 + k3 + 99);
    _t69_15 = _mm256_broadcast_sd(M2 + k3 + 108);
    _t69_14 = _mm256_broadcast_sd(M2 + k3 + 109);
    _t69_13 = _mm256_broadcast_sd(M2 + k3 + 110);
    _t69_12 = _mm256_broadcast_sd(M2 + k3 + 111);
    _t69_11 = _mm256_broadcast_sd(M2 + k3 + 120);
    _t69_10 = _mm256_broadcast_sd(M2 + k3 + 121);
    _t69_9 = _mm256_broadcast_sd(M2 + k3 + 122);
    _t69_8 = _mm256_broadcast_sd(M2 + k3 + 123);
    _t69_7 = _mm256_broadcast_sd(M2 + k3 + 132);
    _t69_6 = _mm256_broadcast_sd(M2 + k3 + 133);
    _t69_5 = _mm256_broadcast_sd(M2 + k3 + 134);
    _t69_4 = _mm256_broadcast_sd(M2 + k3 + 135);
    _t69_3 = _asm256_loadu_pd(M1 + 12*k3 + 8);
    _t69_2 = _asm256_loadu_pd(M1 + 12*k3 + 20);
    _t69_1 = _asm256_loadu_pd(M1 + 12*k3 + 32);
    _t69_0 = _asm256_loadu_pd(M1 + 12*k3 + 44);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t69_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t69_19, _t69_3), _mm256_mul_pd(_t69_18, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t69_17, _t69_1), _mm256_mul_pd(_t69_16, _t69_0)));
    _t69_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t69_15, _t69_3), _mm256_mul_pd(_t69_14, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t69_13, _t69_1), _mm256_mul_pd(_t69_12, _t69_0)));
    _t69_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t69_11, _t69_3), _mm256_mul_pd(_t69_10, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t69_9, _t69_1), _mm256_mul_pd(_t69_8, _t69_0)));
    _t69_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t69_7, _t69_3), _mm256_mul_pd(_t69_6, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t69_5, _t69_1), _mm256_mul_pd(_t69_4, _t69_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t69_24 = _t8_3;
    _t69_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t8_3, _t8_2, 3), _t8_2, 12);
    _t69_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t8_3, _t8_2, 0), _t8_1, 49);
    _t69_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t8_3, _t8_2, 12), _mm256_shuffle_pd(_t8_1, _t8_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t69_24 = _mm256_sub_pd(_t69_24, _t69_20);
    _t69_25 = _mm256_sub_pd(_t69_25, _t69_21);
    _t69_26 = _mm256_sub_pd(_t69_26, _t69_22);
    _t69_27 = _mm256_sub_pd(_t69_27, _t69_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t8_3 = _t69_24;
    _t8_2 = _t69_25;
    _t8_1 = _t69_26;
    _t8_0 = _t69_27;
    _asm256_storeu_pd(P + 104, _t8_3);
    _mm256_maskstore_pd(P + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t8_2);
    _mm256_maskstore_pd(P + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t8_1);
    _mm256_maskstore_pd(P + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t8_0);
  }

  _asm256_storeu_pd(Y + 104, _t13_28);
  _mm256_maskstore_pd(Y + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t13_29);
  _mm256_maskstore_pd(Y + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t13_30);
  _mm256_maskstore_pd(Y + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t13_31);
  _mm_store_sd(&(v0[3]), _mm256_castpd256_pd128(_t51_0));
  _mm_store_sd(&(v0[2]), _mm256_castpd256_pd128(_t51_2));
  _mm_store_sd(&(v0[1]), _mm256_castpd256_pd128(_t51_4));
  _mm_store_sd(&(v0[0]), _mm256_castpd256_pd128(_t51_5));
  _asm256_storeu_pd(P + 104, _t8_3);
  _mm256_maskstore_pd(P + 116, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t8_2);
  _mm256_maskstore_pd(P + 128, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t8_1);
  _mm256_maskstore_pd(P + 140, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t8_0);

}
