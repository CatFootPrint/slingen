/*
 * kf_kernel.h
 *
Decl { {u'B': SquaredMatrix[B, (20, 20), GenMatAccess], u'F': SquaredMatrix[F, (20, 20), GenMatAccess], u'H': SquaredMatrix[H, (20, 20), GenMatAccess], u'U0': UpperTriangular[U0, (20, 20), GenMatAccess], u'M5': SquaredMatrix[M5, (20, 20), GenMatAccess], u'P': Symmetric[P, (20, 20), USMatAccess], u'M7': SquaredMatrix[M7, (20, 20), GenMatAccess], u'M6': SquaredMatrix[M6, (20, 20), GenMatAccess], u'v4': Matrix[v4, (20, 1), GenMatAccess], u'M0': SquaredMatrix[M0, (20, 20), GenMatAccess], u'M3': Symmetric[M3, (20, 20), USMatAccess], u'M2': SquaredMatrix[M2, (20, 20), GenMatAccess], u'Y': Symmetric[Y, (20, 20), USMatAccess], u'R': Symmetric[R, (20, 20), USMatAccess], 'T2152': Matrix[T2152, (1, 20), GenMatAccess], u'U': UpperTriangular[U, (20, 20), GenMatAccess], u'M8': SquaredMatrix[M8, (20, 20), GenMatAccess], 'T2203': Matrix[T2203, (1, 20), GenMatAccess], u'v0': Matrix[v0, (20, 1), GenMatAccess], u'u': Matrix[u, (20, 1), GenMatAccess], u'M4': Symmetric[M4, (20, 20), USMatAccess], u'v2': Matrix[v2, (20, 1), GenMatAccess], u'v1': Matrix[v1, (20, 1), GenMatAccess], u'v3': Matrix[v3, (20, 1), GenMatAccess], u'Q': Symmetric[Q, (20, 20), USMatAccess], u'x': Matrix[x, (20, 1), GenMatAccess], u'y': Matrix[y, (20, 1), GenMatAccess], u'M1': SquaredMatrix[M1, (20, 20), GenMatAccess], u'z': Matrix[z, (20, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt': {'m0': 'm01.ll'}, 'ldiv_utn_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 1, 'variant_tag': 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt_m01_Assign_Mul_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_ldiv_utn_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), u[20,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), P[20,20] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[20,20] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[20,20] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[20,20] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[20,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[20,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[20,20] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[20,20] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[20,20] ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(2, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(2, 20, 1)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 2)) ) = ( Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 2)) ) Div Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 1), U[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 1), U[20,20],h(1, 20, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(2, 20, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 3)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 3)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 23:
For_{fi562;4;16;4} ( Entry 0:
For_{fi641;0;fi562 - 5;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi641), U[20,20],h(1, 20, fi641)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641), U[20,20],h(3, 20, fi641 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi641 + 1), U[20,20],h(1, 20, fi641 + 1)) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 1), U[20,20],h(2, 20, fi641 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi641 + 2), U[20,20],h(1, 20, fi641 + 2)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 2), U[20,20],h(1, 20, fi641 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi641 + 3), U[20,20],h(1, 20, fi641 + 3)) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(fi562 - fi641 - 4, 20, fi641 + 4), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(fi562 - fi641 - 4, 20, fi641 + 4), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi641), U[20,20],h(fi562 - fi641 - 4, 20, fi641 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi641), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
 )Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4))) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(1, 20, Max(0, fi562 - 4))) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4))) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(3, 20, Max(0, fi562 - 4) + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(1, 20, Max(0, fi562 - 4) + 1)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(2, 20, Max(0, fi562 - 4) + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(1, 20, Max(0, fi562 - 4) + 2)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(1, 20, Max(0, fi562 - 4) + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(1, 20, Max(0, fi562 - 4) + 3)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi562), U[20,20],h(4, 20, fi562)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi562), M4[20,20],h(4, 20, fi562)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi562, 20, 0), U[20,20],h(4, 20, fi562)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi562, 20, 0), U[20,20],h(4, 20, fi562)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi562)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2152[1,20],h(1, 20, fi562)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) ) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) = ( Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) Div Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) ) )
Eq.ann: {}
 )Entry 24:
For_{fi799;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, fi799), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi799), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi799), U0[20,20],h(1, 20, fi799)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi799 + 1), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi799 + 1), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799), U0[20,20],h(3, 20, fi799 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, fi799 + 1), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi799 + 1), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi799 + 1), U0[20,20],h(1, 20, fi799 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi799 + 2), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi799 + 2), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799 + 1), U0[20,20],h(2, 20, fi799 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799 + 1), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, fi799 + 2), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi799 + 2), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi799 + 2), U0[20,20],h(1, 20, fi799 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799 + 3), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799 + 3), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799 + 2), U0[20,20],h(1, 20, fi799 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi799 + 2), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, fi799 + 3), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi799 + 3), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi799 + 3), U0[20,20],h(1, 20, fi799 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi799 + 16, 20, fi799 + 4), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi799 + 16, 20, fi799 + 4), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi799), U0[20,20],h(-fi799 + 16, 20, fi799 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi799), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 25:
Eq: Tile( (1, 1), G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 32:
For_{fi876;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, -fi876 + 19), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi876 + 19), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi876 + 19), U0[20,20],h(1, 20, -fi876 + 19)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi876 + 16), U0[20,20],h(1, 20, -fi876 + 19)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi876 + 19), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, -fi876 + 18), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi876 + 18), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi876 + 18), U0[20,20],h(1, 20, -fi876 + 18)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi876 + 16), U0[20,20],h(1, 20, -fi876 + 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi876 + 18), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, -fi876 + 17), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi876 + 17), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi876 + 17), U0[20,20],h(1, 20, -fi876 + 17)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi876 + 16), U0[20,20],h(1, 20, -fi876 + 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi876 + 17), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi876 + 16), U0[20,20],h(1, 20, -fi876 + 16)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi876 + 16, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi876 + 16, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi876 + 16, 20, 0), U0[20,20],h(4, 20, -fi876 + 16)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, -fi876 + 16), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 33:
Eq: Tile( (1, 1), G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 40:
For_{fi953;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi953), U0[20,20],h(1, 20, fi953)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi953 + 1), U0[20,20],h(1, 20, fi953 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi953 + 2), U0[20,20],h(1, 20, fi953 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi953 + 3), U0[20,20],h(1, 20, fi953 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi972;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953), M6[20,20],h(4, 20, fi972)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi953 + 1), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi953 + 1), M6[20,20],h(4, 20, fi972)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953), U0[20,20],h(3, 20, fi953 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953), M6[20,20],h(4, 20, fi972)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 1), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 1), M6[20,20],h(4, 20, fi972)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi953 + 2), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi953 + 2), M6[20,20],h(4, 20, fi972)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 1), U0[20,20],h(2, 20, fi953 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 1), M6[20,20],h(4, 20, fi972)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 2), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 2), M6[20,20],h(4, 20, fi972)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 3), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 3), M6[20,20],h(4, 20, fi972)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 2), U0[20,20],h(1, 20, fi953 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 2), M6[20,20],h(4, 20, fi972)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 3), M6[20,20],h(4, 20, fi972)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, fi953 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi953 + 3), M6[20,20],h(4, 20, fi972)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi953 + 16, 20, fi953 + 4), M6[20,20],h(20, 20, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi953 + 16, 20, fi953 + 4), M6[20,20],h(20, 20, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi953), U0[20,20],h(-fi953 + 16, 20, fi953 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi953), M6[20,20],h(20, 20, 0)) ) ) ) )
Eq.ann: {}
 )Entry 41:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 16)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 17)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 18)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 19)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 45:
For_{fi1019;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), M6[20,20],h(4, 20, fi1019)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), M6[20,20],h(4, 20, fi1019)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), M6[20,20],h(4, 20, fi1019)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), M6[20,20],h(4, 20, fi1019)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), M6[20,20],h(4, 20, fi1019)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), M6[20,20],h(4, 20, fi1019)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), M6[20,20],h(4, 20, fi1019)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1019)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), M6[20,20],h(4, 20, fi1019)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1019)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 19)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1019)) ) ) )
Eq.ann: {}
 )Entry 46:
For_{fi1066;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 19)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1066 + 19), U0[20,20],h(1, 20, -fi1066 + 19)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 18)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1066 + 18), U0[20,20],h(1, 20, -fi1066 + 18)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 17)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1066 + 17), U0[20,20],h(1, 20, -fi1066 + 17)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 16)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1066 + 16), U0[20,20],h(1, 20, -fi1066 + 16)) ) )
Eq.ann: {}
Entry 4:
For_{fi1085;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 19), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 19)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 19), M8[20,20],h(4, 20, fi1085)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1066 + 16), U0[20,20],h(1, 20, -fi1066 + 19)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 19), M8[20,20],h(4, 20, fi1085)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 18), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 18), M8[20,20],h(4, 20, fi1085)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1066 + 16), U0[20,20],h(1, 20, -fi1066 + 18)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 18), M8[20,20],h(4, 20, fi1085)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 17), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 17), M8[20,20],h(4, 20, fi1085)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 16), U0[20,20],h(1, 20, -fi1066 + 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 17), M8[20,20],h(4, 20, fi1085)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi1066 + 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1066 + 16), M8[20,20],h(4, 20, fi1085)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1066 + 16, 20, 0), M8[20,20],h(20, 20, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1066 + 16, 20, 0), M8[20,20],h(20, 20, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1066 + 16, 20, 0), U0[20,20],h(4, 20, -fi1066 + 16)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, -fi1066 + 16), M8[20,20],h(20, 20, 0)) ) ) ) )
Eq.ann: {}
 )Entry 47:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2203[1,20],h(1, 20, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 51:
For_{fi1132;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), M8[20,20],h(4, 20, fi1132)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), M8[20,20],h(4, 20, fi1132)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), M8[20,20],h(4, 20, fi1132)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), M8[20,20],h(4, 20, fi1132)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), M8[20,20],h(4, 20, fi1132)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), M8[20,20],h(4, 20, fi1132)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2203[1,20],h(1, 20, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1132)) ) ) )
Eq.ann: {}
 )Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), v0[20,1] ) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), Tile( (4, 4), P[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), M1[20,20] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 20
#define PARAM1 20
#define PARAM2 20

#define ERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27, _t13_28, _t13_29, _t13_30, _t13_31,
	_t13_32, _t13_33, _t13_34, _t13_35, _t13_36, _t13_37, _t13_38, _t13_39,
	_t13_40, _t13_41, _t13_42, _t13_43;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27, _t25_28, _t25_29, _t25_30, _t25_31;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27, _t35_28, _t35_29, _t35_30, _t35_31;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11, _t38_12, _t38_13, _t38_14, _t38_15,
	_t38_16, _t38_17, _t38_18, _t38_19, _t38_20, _t38_21, _t38_22, _t38_23;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31,
	_t42_32, _t42_33, _t42_34, _t42_35, _t42_36, _t42_37, _t42_38, _t42_39,
	_t42_40, _t42_41, _t42_42, _t42_43;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31,
	_t44_32, _t44_33, _t44_34, _t44_35, _t44_36, _t44_37, _t44_38, _t44_39,
	_t44_40, _t44_41, _t44_42, _t44_43;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27, _t45_28, _t45_29, _t45_30, _t45_31,
	_t45_32, _t45_33, _t45_34, _t45_35;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11, _t46_12, _t46_13, _t46_14, _t46_15;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27, _t47_28, _t47_29, _t47_30, _t47_31;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35, _t48_36, _t48_37, _t48_38, _t48_39,
	_t48_40, _t48_41, _t48_42, _t48_43, _t48_44, _t48_45, _t48_46, _t48_47,
	_t48_48, _t48_49, _t48_50, _t48_51, _t48_52, _t48_53, _t48_54, _t48_55,
	_t48_56, _t48_57, _t48_58, _t48_59, _t48_60, _t48_61, _t48_62, _t48_63,
	_t48_64, _t48_65, _t48_66, _t48_67, _t48_68, _t48_69, _t48_70, _t48_71,
	_t48_72, _t48_73, _t48_74, _t48_75, _t48_76, _t48_77, _t48_78, _t48_79,
	_t48_80, _t48_81, _t48_82, _t48_83, _t48_84, _t48_85, _t48_86, _t48_87,
	_t48_88, _t48_89, _t48_90, _t48_91, _t48_92, _t48_93, _t48_94, _t48_95,
	_t48_96, _t48_97, _t48_98, _t48_99, _t48_100, _t48_101, _t48_102, _t48_103,
	_t48_104, _t48_105, _t48_106, _t48_107, _t48_108, _t48_109, _t48_110, _t48_111,
	_t48_112, _t48_113, _t48_114, _t48_115, _t48_116, _t48_117, _t48_118, _t48_119,
	_t48_120, _t48_121, _t48_122, _t48_123, _t48_124, _t48_125, _t48_126, _t48_127,
	_t48_128, _t48_129, _t48_130, _t48_131, _t48_132, _t48_133, _t48_134, _t48_135,
	_t48_136, _t48_137, _t48_138, _t48_139, _t48_140, _t48_141, _t48_142, _t48_143,
	_t48_144, _t48_145, _t48_146, _t48_147, _t48_148, _t48_149, _t48_150, _t48_151,
	_t48_152, _t48_153, _t48_154, _t48_155, _t48_156, _t48_157, _t48_158, _t48_159,
	_t48_160, _t48_161, _t48_162, _t48_163, _t48_164, _t48_165, _t48_166, _t48_167,
	_t48_168, _t48_169, _t48_170, _t48_171, _t48_172, _t48_173, _t48_174, _t48_175,
	_t48_176, _t48_177, _t48_178, _t48_179, _t48_180, _t48_181, _t48_182, _t48_183,
	_t48_184, _t48_185, _t48_186, _t48_187, _t48_188, _t48_189, _t48_190, _t48_191,
	_t48_192, _t48_193, _t48_194, _t48_195, _t48_196, _t48_197, _t48_198, _t48_199,
	_t48_200, _t48_201, _t48_202, _t48_203, _t48_204, _t48_205, _t48_206, _t48_207,
	_t48_208, _t48_209, _t48_210, _t48_211, _t48_212, _t48_213, _t48_214, _t48_215,
	_t48_216, _t48_217, _t48_218, _t48_219, _t48_220, _t48_221, _t48_222, _t48_223,
	_t48_224, _t48_225, _t48_226, _t48_227, _t48_228, _t48_229, _t48_230, _t48_231,
	_t48_232, _t48_233, _t48_234, _t48_235, _t48_236, _t48_237, _t48_238, _t48_239,
	_t48_240, _t48_241, _t48_242, _t48_243, _t48_244, _t48_245, _t48_246, _t48_247,
	_t48_248, _t48_249, _t48_250, _t48_251, _t48_252, _t48_253, _t48_254, _t48_255,
	_t48_256, _t48_257, _t48_258, _t48_259, _t48_260, _t48_261, _t48_262, _t48_263,
	_t48_264, _t48_265, _t48_266, _t48_267, _t48_268, _t48_269, _t48_270, _t48_271,
	_t48_272, _t48_273, _t48_274, _t48_275, _t48_276, _t48_277, _t48_278, _t48_279,
	_t48_280, _t48_281, _t48_282, _t48_283, _t48_284, _t48_285, _t48_286, _t48_287,
	_t48_288, _t48_289, _t48_290, _t48_291, _t48_292, _t48_293, _t48_294, _t48_295,
	_t48_296, _t48_297, _t48_298, _t48_299, _t48_300, _t48_301, _t48_302, _t48_303,
	_t48_304, _t48_305, _t48_306, _t48_307, _t48_308, _t48_309, _t48_310, _t48_311,
	_t48_312, _t48_313, _t48_314, _t48_315, _t48_316, _t48_317, _t48_318, _t48_319,
	_t48_320, _t48_321, _t48_322, _t48_323, _t48_324, _t48_325, _t48_326, _t48_327,
	_t48_328, _t48_329, _t48_330, _t48_331, _t48_332, _t48_333, _t48_334, _t48_335,
	_t48_336, _t48_337, _t48_338, _t48_339, _t48_340, _t48_341, _t48_342, _t48_343,
	_t48_344, _t48_345, _t48_346, _t48_347, _t48_348, _t48_349, _t48_350, _t48_351,
	_t48_352, _t48_353, _t48_354, _t48_355, _t48_356, _t48_357, _t48_358, _t48_359,
	_t48_360, _t48_361, _t48_362, _t48_363, _t48_364, _t48_365, _t48_366, _t48_367,
	_t48_368, _t48_369, _t48_370, _t48_371, _t48_372, _t48_373, _t48_374, _t48_375,
	_t48_376, _t48_377, _t48_378, _t48_379, _t48_380, _t48_381, _t48_382, _t48_383,
	_t48_384, _t48_385, _t48_386, _t48_387, _t48_388, _t48_389, _t48_390, _t48_391,
	_t48_392, _t48_393, _t48_394, _t48_395, _t48_396, _t48_397, _t48_398, _t48_399,
	_t48_400, _t48_401, _t48_402, _t48_403, _t48_404, _t48_405, _t48_406, _t48_407,
	_t48_408, _t48_409, _t48_410, _t48_411, _t48_412, _t48_413, _t48_414, _t48_415,
	_t48_416, _t48_417, _t48_418, _t48_419, _t48_420, _t48_421, _t48_422, _t48_423,
	_t48_424, _t48_425, _t48_426, _t48_427, _t48_428, _t48_429, _t48_430, _t48_431,
	_t48_432, _t48_433, _t48_434, _t48_435, _t48_436, _t48_437;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26, _t49_27, _t49_28, _t49_29, _t49_30, _t49_31,
	_t49_32, _t49_33, _t49_34, _t49_35, _t49_36, _t49_37, _t49_38, _t49_39,
	_t49_40, _t49_41, _t49_42, _t49_43, _t49_44, _t49_45, _t49_46, _t49_47,
	_t49_48, _t49_49;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15,
	_t51_16, _t51_17, _t51_18, _t51_19, _t51_20, _t51_21, _t51_22, _t51_23,
	_t51_24, _t51_25, _t51_26, _t51_27, _t51_28, _t51_29, _t51_30, _t51_31,
	_t51_32, _t51_33, _t51_34, _t51_35, _t51_36, _t51_37, _t51_38, _t51_39,
	_t51_40, _t51_41, _t51_42, _t51_43, _t51_44, _t51_45, _t51_46, _t51_47,
	_t51_48, _t51_49, _t51_50, _t51_51, _t51_52, _t51_53, _t51_54, _t51_55,
	_t51_56, _t51_57, _t51_58, _t51_59, _t51_60, _t51_61;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26, _t53_27, _t53_28, _t53_29, _t53_30, _t53_31,
	_t53_32, _t53_33, _t53_34, _t53_35, _t53_36, _t53_37, _t53_38, _t53_39,
	_t53_40, _t53_41, _t53_42, _t53_43, _t53_44, _t53_45, _t53_46, _t53_47,
	_t53_48, _t53_49, _t53_50, _t53_51, _t53_52, _t53_53, _t53_54, _t53_55,
	_t53_56, _t53_57, _t53_58, _t53_59, _t53_60;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39,
	_t54_40, _t54_41, _t54_42;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11, _t55_12, _t55_13;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3, _t56_4, _t56_5, _t56_6, _t56_7,
	_t56_8, _t56_9, _t56_10, _t56_11, _t56_12, _t56_13, _t56_14, _t56_15,
	_t56_16, _t56_17, _t56_18, _t56_19, _t56_20, _t56_21, _t56_22, _t56_23,
	_t56_24, _t56_25, _t56_26, _t56_27, _t56_28, _t56_29, _t56_30, _t56_31,
	_t56_32, _t56_33, _t56_34, _t56_35;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11, _t57_12, _t57_13, _t57_14, _t57_15,
	_t57_16, _t57_17, _t57_18, _t57_19, _t57_20, _t57_21, _t57_22, _t57_23,
	_t57_24, _t57_25, _t57_26, _t57_27, _t57_28, _t57_29, _t57_30, _t57_31,
	_t57_32, _t57_33, _t57_34, _t57_35, _t57_36, _t57_37, _t57_38, _t57_39;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26, _t59_27, _t59_28, _t59_29, _t59_30, _t59_31,
	_t59_32;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11, _t60_12, _t60_13, _t60_14, _t60_15,
	_t60_16, _t60_17, _t60_18, _t60_19, _t60_20, _t60_21, _t60_22, _t60_23,
	_t60_24, _t60_25, _t60_26, _t60_27, _t60_28, _t60_29, _t60_30, _t60_31,
	_t60_32, _t60_33, _t60_34, _t60_35, _t60_36, _t60_37, _t60_38, _t60_39,
	_t60_40, _t60_41, _t60_42, _t60_43, _t60_44, _t60_45, _t60_46, _t60_47,
	_t60_48, _t60_49, _t60_50, _t60_51, _t60_52, _t60_53, _t60_54, _t60_55,
	_t60_56, _t60_57, _t60_58, _t60_59, _t60_60, _t60_61;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11, _t61_12, _t61_13, _t61_14, _t61_15,
	_t61_16, _t61_17, _t61_18, _t61_19, _t61_20, _t61_21, _t61_22, _t61_23,
	_t61_24, _t61_25, _t61_26;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6, _t63_7,
	_t63_8, _t63_9, _t63_10, _t63_11, _t63_12, _t63_13, _t63_14, _t63_15,
	_t63_16, _t63_17, _t63_18, _t63_19, _t63_20, _t63_21, _t63_22, _t63_23,
	_t63_24, _t63_25, _t63_26, _t63_27, _t63_28, _t63_29, _t63_30, _t63_31,
	_t63_32, _t63_33, _t63_34, _t63_35, _t63_36, _t63_37, _t63_38, _t63_39,
	_t63_40, _t63_41, _t63_42, _t63_43, _t63_44, _t63_45, _t63_46, _t63_47,
	_t63_48, _t63_49, _t63_50, _t63_51, _t63_52, _t63_53, _t63_54;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15,
	_t64_16, _t64_17, _t64_18, _t64_19, _t64_20, _t64_21, _t64_22, _t64_23,
	_t64_24, _t64_25, _t64_26;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15,
	_t65_16, _t65_17, _t65_18, _t65_19, _t65_20, _t65_21, _t65_22, _t65_23,
	_t65_24, _t65_25, _t65_26, _t65_27, _t65_28, _t65_29, _t65_30, _t65_31,
	_t65_32, _t65_33, _t65_34, _t65_35, _t65_36, _t65_37, _t65_38, _t65_39,
	_t65_40, _t65_41, _t65_42, _t65_43, _t65_44, _t65_45, _t65_46, _t65_47,
	_t65_48, _t65_49, _t65_50, _t65_51, _t65_52, _t65_53, _t65_54, _t65_55,
	_t65_56, _t65_57, _t65_58;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15,
	_t67_16, _t67_17, _t67_18, _t67_19, _t67_20, _t67_21, _t67_22, _t67_23,
	_t67_24, _t67_25, _t67_26, _t67_27;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15,
	_t68_16, _t68_17, _t68_18, _t68_19, _t68_20, _t68_21, _t68_22, _t68_23,
	_t68_24, _t68_25, _t68_26, _t68_27, _t68_28, _t68_29, _t68_30, _t68_31,
	_t68_32, _t68_33, _t68_34, _t68_35, _t68_36, _t68_37, _t68_38, _t68_39,
	_t68_40, _t68_41, _t68_42, _t68_43, _t68_44, _t68_45, _t68_46, _t68_47,
	_t68_48, _t68_49, _t68_50, _t68_51;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23,
	_t69_24, _t69_25, _t69_26;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6;
  __m256d _t71_0, _t71_1, _t71_2, _t71_3, _t71_4, _t71_5, _t71_6;
  __m256d _t72_0, _t72_1, _t72_2, _t72_3, _t72_4, _t72_5, _t72_6, _t72_7,
	_t72_8, _t72_9, _t72_10, _t72_11, _t72_12, _t72_13, _t72_14, _t72_15,
	_t72_16, _t72_17, _t72_18, _t72_19, _t72_20, _t72_21, _t72_22, _t72_23,
	_t72_24, _t72_25, _t72_26, _t72_27, _t72_28, _t72_29, _t72_30, _t72_31,
	_t72_32, _t72_33, _t72_34, _t72_35, _t72_36, _t72_37, _t72_38, _t72_39;
  __m256d _t73_0, _t73_1, _t73_2, _t73_3, _t73_4, _t73_5, _t73_6, _t73_7,
	_t73_8, _t73_9, _t73_10, _t73_11, _t73_12, _t73_13, _t73_14, _t73_15;
  __m256d _t74_0, _t74_1, _t74_2, _t74_3, _t74_4, _t74_5, _t74_6, _t74_7,
	_t74_8, _t74_9, _t74_10, _t74_11, _t74_12, _t74_13, _t74_14, _t74_15,
	_t74_16, _t74_17, _t74_18, _t74_19, _t74_20, _t74_21, _t74_22, _t74_23,
	_t74_24, _t74_25, _t74_26, _t74_27, _t74_28, _t74_29, _t74_30, _t74_31;
  __m256d _t75_0, _t75_1, _t75_2, _t75_3, _t75_4, _t75_5, _t75_6, _t75_7,
	_t75_8, _t75_9, _t75_10, _t75_11, _t75_12, _t75_13, _t75_14, _t75_15,
	_t75_16, _t75_17, _t75_18, _t75_19, _t75_20, _t75_21, _t75_22, _t75_23,
	_t75_24, _t75_25, _t75_26, _t75_27, _t75_28, _t75_29, _t75_30, _t75_31;
  __m256d _t76_0, _t76_1, _t76_2, _t76_3, _t76_4, _t76_5, _t76_6, _t76_7,
	_t76_8, _t76_9, _t76_10, _t76_11;
  __m256d _t77_0, _t77_1, _t77_2, _t77_3, _t77_4, _t77_5, _t77_6, _t77_7,
	_t77_8, _t77_9, _t77_10, _t77_11, _t77_12, _t77_13, _t77_14, _t77_15,
	_t77_16, _t77_17, _t77_18, _t77_19, _t77_20, _t77_21, _t77_22, _t77_23,
	_t77_24, _t77_25, _t77_26, _t77_27;


  // Generating : y[20,1] = ( ( Sum_{i0} ( S(h(4, 20, i0), ( ( G(h(4, 20, i0), F[20,20],h(4, 20, 0)) * G(h(4, 20, 0), x[20,1],h(1, 1, 0)) ) + ( G(h(4, 20, i0), B[20,20],h(4, 20, 0)) * G(h(4, 20, 0), u[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 20, i0), ( G(h(4, 20, i0), F[20,20],h(4, 20, k2)) * G(h(4, 20, k2), x[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) ) + Sum_{k3} ( Sum_{i0} ( $(h(4, 20, i0), ( G(h(4, 20, i0), B[20,20],h(4, 20, k3)) * G(h(4, 20, k3), u[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t0_9 = _asm256_loadu_pd(F + 20*i0);
    _t0_8 = _asm256_loadu_pd(F + 20*i0 + 20);
    _t0_7 = _asm256_loadu_pd(F + 20*i0 + 40);
    _t0_6 = _asm256_loadu_pd(F + 20*i0 + 60);
    _t0_5 = _asm256_loadu_pd(x);
    _t0_4 = _asm256_loadu_pd(B + 20*i0);
    _t0_3 = _asm256_loadu_pd(B + 20*i0 + 20);
    _t0_2 = _asm256_loadu_pd(B + 20*i0 + 40);
    _t0_1 = _asm256_loadu_pd(B + 20*i0 + 60);
    _t0_0 = _asm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:
    _asm256_storeu_pd(y + i0, _t0_10);
  }


  for( int k2 = 4; k2 <= 19; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t1_4 = _asm256_loadu_pd(F + 20*i0 + k2);
      _t1_3 = _asm256_loadu_pd(F + 20*i0 + k2 + 20);
      _t1_2 = _asm256_loadu_pd(F + 20*i0 + k2 + 40);
      _t1_1 = _asm256_loadu_pd(F + 20*i0 + k2 + 60);
      _t1_0 = _asm256_loadu_pd(x + k2);
      _t1_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t1_5 = _mm256_add_pd(_t1_5, _t1_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t1_5);
    }
  }


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t2_4 = _asm256_loadu_pd(B + 20*i0 + k3);
      _t2_3 = _asm256_loadu_pd(B + 20*i0 + k3 + 20);
      _t2_2 = _asm256_loadu_pd(B + 20*i0 + k3 + 40);
      _t2_1 = _asm256_loadu_pd(B + 20*i0 + k3 + 60);
      _t2_0 = _asm256_loadu_pd(u + k3);
      _t2_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t2_5 = _mm256_add_pd(_t2_5, _t2_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t2_5);
    }
  }

  _t3_3 = _asm256_loadu_pd(P);
  _t3_2 = _mm256_maskload_pd(P + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 0)) * G(h(4, 20, 0), P[20,20],h(4, 20, 0)) ),h(4, 20, 0)) + Sum_{i0} ( S(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 0)) * G(h(4, 20, 0), P[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), P[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) * G(h(4, 20, k3), P[20,20],h(4, 20, k3)) ),h(4, 20, k3)) ) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) * G(h(4, 20, k3), P[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), P[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 16)) * G(h(4, 20, 16), P[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_4 = _t3_3;
  _t3_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 20*k2);
    _t4_14 = _mm256_broadcast_sd(F + 20*k2 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 20*k2 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 20*k2 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 20*k2 + 20);
    _t4_10 = _mm256_broadcast_sd(F + 20*k2 + 21);
    _t4_9 = _mm256_broadcast_sd(F + 20*k2 + 22);
    _t4_8 = _mm256_broadcast_sd(F + 20*k2 + 23);
    _t4_7 = _mm256_broadcast_sd(F + 20*k2 + 40);
    _t4_6 = _mm256_broadcast_sd(F + 20*k2 + 41);
    _t4_5 = _mm256_broadcast_sd(F + 20*k2 + 42);
    _t4_4 = _mm256_broadcast_sd(F + 20*k2 + 43);
    _t4_3 = _mm256_broadcast_sd(F + 20*k2 + 60);
    _t4_2 = _mm256_broadcast_sd(F + 20*k2 + 61);
    _t4_1 = _mm256_broadcast_sd(F + 20*k2 + 62);
    _t4_0 = _mm256_broadcast_sd(F + 20*k2 + 63);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_4), _mm256_mul_pd(_t4_14, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_6), _mm256_mul_pd(_t4_12, _t3_7)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_4), _mm256_mul_pd(_t4_10, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_6), _mm256_mul_pd(_t4_8, _t3_7)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_4), _mm256_mul_pd(_t4_6, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_6), _mm256_mul_pd(_t4_4, _t3_7)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_4), _mm256_mul_pd(_t4_2, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_6), _mm256_mul_pd(_t4_0, _t3_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 19; i0+=4 ) {
      _t5_3 = _asm256_loadu_pd(P + i0);
      _t5_2 = _asm256_loadu_pd(P + i0 + 20);
      _t5_1 = _asm256_loadu_pd(P + i0 + 40);
      _t5_0 = _asm256_loadu_pd(P + i0 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t5_3), _mm256_mul_pd(_t4_14, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t5_1), _mm256_mul_pd(_t4_12, _t5_0)));
      _t5_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t5_3), _mm256_mul_pd(_t4_10, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t5_1), _mm256_mul_pd(_t4_8, _t5_0)));
      _t5_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t5_3), _mm256_mul_pd(_t4_6, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t5_1), _mm256_mul_pd(_t4_4, _t5_0)));
      _t5_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t5_3), _mm256_mul_pd(_t4_2, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t5_1), _mm256_mul_pd(_t4_0, _t5_0)));

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 20*k2, _t5_4);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t5_5);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t5_6);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t5_7);
    }
    _asm256_storeu_pd(M0 + 20*k2, _t4_16);
    _asm256_storeu_pd(M0 + 20*k2 + 20, _t4_17);
    _asm256_storeu_pd(M0 + 20*k2 + 40, _t4_18);
    _asm256_storeu_pd(M0 + 20*k2 + 60, _t4_19);
  }


  for( int k3 = 4; k3 <= 15; k3+=4 ) {
    _t6_3 = _asm256_loadu_pd(P + 21*k3);
    _t6_2 = _mm256_maskload_pd(P + 21*k3 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t6_1 = _mm256_maskload_pd(P + 21*k3 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t6_0 = _mm256_maskload_pd(P + 21*k3 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t6_4 = _t6_3;
    _t6_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
    _t6_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
    _t6_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

    for( int k2 = 0; k2 <= 19; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t7_19 = _mm256_broadcast_sd(F + 20*k2 + k3);
        _t7_18 = _mm256_broadcast_sd(F + 20*k2 + k3 + 1);
        _t7_17 = _mm256_broadcast_sd(F + 20*k2 + k3 + 2);
        _t7_16 = _mm256_broadcast_sd(F + 20*k2 + k3 + 3);
        _t7_15 = _mm256_broadcast_sd(F + 20*k2 + k3 + 20);
        _t7_14 = _mm256_broadcast_sd(F + 20*k2 + k3 + 21);
        _t7_13 = _mm256_broadcast_sd(F + 20*k2 + k3 + 22);
        _t7_12 = _mm256_broadcast_sd(F + 20*k2 + k3 + 23);
        _t7_11 = _mm256_broadcast_sd(F + 20*k2 + k3 + 40);
        _t7_10 = _mm256_broadcast_sd(F + 20*k2 + k3 + 41);
        _t7_9 = _mm256_broadcast_sd(F + 20*k2 + k3 + 42);
        _t7_8 = _mm256_broadcast_sd(F + 20*k2 + k3 + 43);
        _t7_7 = _mm256_broadcast_sd(F + 20*k2 + k3 + 60);
        _t7_6 = _mm256_broadcast_sd(F + 20*k2 + k3 + 61);
        _t7_5 = _mm256_broadcast_sd(F + 20*k2 + k3 + 62);
        _t7_4 = _mm256_broadcast_sd(F + 20*k2 + k3 + 63);
        _t7_3 = _asm256_loadu_pd(P + 20*i0 + k3);
        _t7_2 = _asm256_loadu_pd(P + 20*i0 + k3 + 20);
        _t7_1 = _asm256_loadu_pd(P + 20*i0 + k3 + 40);
        _t7_0 = _asm256_loadu_pd(P + 20*i0 + k3 + 60);
        _t7_20 = _asm256_loadu_pd(M0 + i0 + 20*k2);
        _t7_21 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 20);
        _t7_22 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 40);
        _t7_23 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t7_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
        _t7_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
        _t7_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
        _t7_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t7_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_28), _mm256_mul_pd(_t7_18, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_30), _mm256_mul_pd(_t7_16, _t7_31)));
        _t7_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_28), _mm256_mul_pd(_t7_14, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_30), _mm256_mul_pd(_t7_12, _t7_31)));
        _t7_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_28), _mm256_mul_pd(_t7_10, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_30), _mm256_mul_pd(_t7_8, _t7_31)));
        _t7_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_28), _mm256_mul_pd(_t7_6, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_30), _mm256_mul_pd(_t7_4, _t7_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t7_20 = _mm256_add_pd(_t7_20, _t7_24);
        _t7_21 = _mm256_add_pd(_t7_21, _t7_25);
        _t7_22 = _mm256_add_pd(_t7_22, _t7_26);
        _t7_23 = _mm256_add_pd(_t7_23, _t7_27);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 20*k2, _t7_20);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t7_21);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t7_22);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t7_23);
      }
      _t8_15 = _mm256_broadcast_sd(F + 20*k2 + k3);
      _t8_14 = _mm256_broadcast_sd(F + 20*k2 + k3 + 1);
      _t8_13 = _mm256_broadcast_sd(F + 20*k2 + k3 + 2);
      _t8_12 = _mm256_broadcast_sd(F + 20*k2 + k3 + 3);
      _t8_11 = _mm256_broadcast_sd(F + 20*k2 + k3 + 20);
      _t8_10 = _mm256_broadcast_sd(F + 20*k2 + k3 + 21);
      _t8_9 = _mm256_broadcast_sd(F + 20*k2 + k3 + 22);
      _t8_8 = _mm256_broadcast_sd(F + 20*k2 + k3 + 23);
      _t8_7 = _mm256_broadcast_sd(F + 20*k2 + k3 + 40);
      _t8_6 = _mm256_broadcast_sd(F + 20*k2 + k3 + 41);
      _t8_5 = _mm256_broadcast_sd(F + 20*k2 + k3 + 42);
      _t8_4 = _mm256_broadcast_sd(F + 20*k2 + k3 + 43);
      _t8_3 = _mm256_broadcast_sd(F + 20*k2 + k3 + 60);
      _t8_2 = _mm256_broadcast_sd(F + 20*k2 + k3 + 61);
      _t8_1 = _mm256_broadcast_sd(F + 20*k2 + k3 + 62);
      _t8_0 = _mm256_broadcast_sd(F + 20*k2 + k3 + 63);
      _t8_16 = _asm256_loadu_pd(M0 + 20*k2 + k3);
      _t8_17 = _asm256_loadu_pd(M0 + 20*k2 + k3 + 20);
      _t8_18 = _asm256_loadu_pd(M0 + 20*k2 + k3 + 40);
      _t8_19 = _asm256_loadu_pd(M0 + 20*k2 + k3 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t8_24 = _t6_3;
      _t8_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
      _t8_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
      _t8_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t8_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_24), _mm256_mul_pd(_t8_14, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_26), _mm256_mul_pd(_t8_12, _t8_27)));
      _t8_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_24), _mm256_mul_pd(_t8_10, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_26), _mm256_mul_pd(_t8_8, _t8_27)));
      _t8_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_24), _mm256_mul_pd(_t8_6, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_26), _mm256_mul_pd(_t8_4, _t8_27)));
      _t8_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t8_24), _mm256_mul_pd(_t8_2, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t8_26), _mm256_mul_pd(_t8_0, _t8_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_16 = _mm256_add_pd(_t8_16, _t8_20);
      _t8_17 = _mm256_add_pd(_t8_17, _t8_21);
      _t8_18 = _mm256_add_pd(_t8_18, _t8_22);
      _t8_19 = _mm256_add_pd(_t8_19, _t8_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M0 + 20*k2 + k3, _t8_16);
      _asm256_storeu_pd(M0 + 20*k2 + k3 + 20, _t8_17);
      _asm256_storeu_pd(M0 + 20*k2 + k3 + 40, _t8_18);
      _asm256_storeu_pd(M0 + 20*k2 + k3 + 60, _t8_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t9_3 = _asm256_loadu_pd(P + i0 + 20*k3);
        _t9_2 = _asm256_loadu_pd(P + i0 + 20*k3 + 20);
        _t9_1 = _asm256_loadu_pd(P + i0 + 20*k3 + 40);
        _t9_0 = _asm256_loadu_pd(P + i0 + 20*k3 + 60);
        _t9_4 = _asm256_loadu_pd(M0 + i0 + 20*k2);
        _t9_5 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 20);
        _t9_6 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 40);
        _t9_7 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t9_3), _mm256_mul_pd(_t8_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t9_1), _mm256_mul_pd(_t8_12, _t9_0)));
        _t9_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t9_3), _mm256_mul_pd(_t8_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t9_1), _mm256_mul_pd(_t8_8, _t9_0)));
        _t9_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t9_3), _mm256_mul_pd(_t8_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t9_1), _mm256_mul_pd(_t8_4, _t9_0)));
        _t9_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t9_3), _mm256_mul_pd(_t8_2, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t9_1), _mm256_mul_pd(_t8_0, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t9_4 = _mm256_add_pd(_t9_4, _t9_8);
        _t9_5 = _mm256_add_pd(_t9_5, _t9_9);
        _t9_6 = _mm256_add_pd(_t9_6, _t9_10);
        _t9_7 = _mm256_add_pd(_t9_7, _t9_11);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 20*k2, _t9_4);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t9_5);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t9_6);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t9_7);
      }
    }
  }

  _t10_3 = _asm256_loadu_pd(P + 336);
  _t10_2 = _mm256_maskload_pd(P + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t10_1 = _mm256_maskload_pd(P + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t10_0 = _mm256_maskload_pd(P + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t10_4 = _t10_3;
  _t10_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
  _t10_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
  _t10_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 15; i0+=4 ) {
      _t11_19 = _mm256_broadcast_sd(F + 20*k2 + 16);
      _t11_18 = _mm256_broadcast_sd(F + 20*k2 + 17);
      _t11_17 = _mm256_broadcast_sd(F + 20*k2 + 18);
      _t11_16 = _mm256_broadcast_sd(F + 20*k2 + 19);
      _t11_15 = _mm256_broadcast_sd(F + 20*k2 + 36);
      _t11_14 = _mm256_broadcast_sd(F + 20*k2 + 37);
      _t11_13 = _mm256_broadcast_sd(F + 20*k2 + 38);
      _t11_12 = _mm256_broadcast_sd(F + 20*k2 + 39);
      _t11_11 = _mm256_broadcast_sd(F + 20*k2 + 56);
      _t11_10 = _mm256_broadcast_sd(F + 20*k2 + 57);
      _t11_9 = _mm256_broadcast_sd(F + 20*k2 + 58);
      _t11_8 = _mm256_broadcast_sd(F + 20*k2 + 59);
      _t11_7 = _mm256_broadcast_sd(F + 20*k2 + 76);
      _t11_6 = _mm256_broadcast_sd(F + 20*k2 + 77);
      _t11_5 = _mm256_broadcast_sd(F + 20*k2 + 78);
      _t11_4 = _mm256_broadcast_sd(F + 20*k2 + 79);
      _t11_3 = _asm256_loadu_pd(P + 20*i0 + 16);
      _t11_2 = _asm256_loadu_pd(P + 20*i0 + 36);
      _t11_1 = _asm256_loadu_pd(P + 20*i0 + 56);
      _t11_0 = _asm256_loadu_pd(P + 20*i0 + 76);
      _t11_20 = _asm256_loadu_pd(M0 + i0 + 20*k2);
      _t11_21 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 20);
      _t11_22 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 40);
      _t11_23 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t11_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
      _t11_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
      _t11_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
      _t11_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t11_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_28), _mm256_mul_pd(_t11_18, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_30), _mm256_mul_pd(_t11_16, _t11_31)));
      _t11_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_28), _mm256_mul_pd(_t11_14, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_30), _mm256_mul_pd(_t11_12, _t11_31)));
      _t11_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_28), _mm256_mul_pd(_t11_10, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_30), _mm256_mul_pd(_t11_8, _t11_31)));
      _t11_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_28), _mm256_mul_pd(_t11_6, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_30), _mm256_mul_pd(_t11_4, _t11_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t11_20 = _mm256_add_pd(_t11_20, _t11_24);
      _t11_21 = _mm256_add_pd(_t11_21, _t11_25);
      _t11_22 = _mm256_add_pd(_t11_22, _t11_26);
      _t11_23 = _mm256_add_pd(_t11_23, _t11_27);

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 20*k2, _t11_20);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t11_21);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t11_22);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t11_23);
    }
    _t12_15 = _mm256_broadcast_sd(F + 20*k2 + 16);
    _t12_14 = _mm256_broadcast_sd(F + 20*k2 + 17);
    _t12_13 = _mm256_broadcast_sd(F + 20*k2 + 18);
    _t12_12 = _mm256_broadcast_sd(F + 20*k2 + 19);
    _t12_11 = _mm256_broadcast_sd(F + 20*k2 + 36);
    _t12_10 = _mm256_broadcast_sd(F + 20*k2 + 37);
    _t12_9 = _mm256_broadcast_sd(F + 20*k2 + 38);
    _t12_8 = _mm256_broadcast_sd(F + 20*k2 + 39);
    _t12_7 = _mm256_broadcast_sd(F + 20*k2 + 56);
    _t12_6 = _mm256_broadcast_sd(F + 20*k2 + 57);
    _t12_5 = _mm256_broadcast_sd(F + 20*k2 + 58);
    _t12_4 = _mm256_broadcast_sd(F + 20*k2 + 59);
    _t12_3 = _mm256_broadcast_sd(F + 20*k2 + 76);
    _t12_2 = _mm256_broadcast_sd(F + 20*k2 + 77);
    _t12_1 = _mm256_broadcast_sd(F + 20*k2 + 78);
    _t12_0 = _mm256_broadcast_sd(F + 20*k2 + 79);
    _t12_16 = _asm256_loadu_pd(M0 + 20*k2 + 16);
    _t12_17 = _asm256_loadu_pd(M0 + 20*k2 + 36);
    _t12_18 = _asm256_loadu_pd(M0 + 20*k2 + 56);
    _t12_19 = _asm256_loadu_pd(M0 + 20*k2 + 76);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t10_4), _mm256_mul_pd(_t12_14, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t10_6), _mm256_mul_pd(_t12_12, _t10_7)));
    _t12_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t10_4), _mm256_mul_pd(_t12_10, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t10_6), _mm256_mul_pd(_t12_8, _t10_7)));
    _t12_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t10_4), _mm256_mul_pd(_t12_6, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t10_6), _mm256_mul_pd(_t12_4, _t10_7)));
    _t12_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_3, _t10_4), _mm256_mul_pd(_t12_2, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_1, _t10_6), _mm256_mul_pd(_t12_0, _t10_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_16 = _mm256_add_pd(_t12_16, _t12_20);
    _t12_17 = _mm256_add_pd(_t12_17, _t12_21);
    _t12_18 = _mm256_add_pd(_t12_18, _t12_22);
    _t12_19 = _mm256_add_pd(_t12_19, _t12_23);

    // AVX Storer:
    _asm256_storeu_pd(M0 + 20*k2 + 16, _t12_16);
    _asm256_storeu_pd(M0 + 20*k2 + 36, _t12_17);
    _asm256_storeu_pd(M0 + 20*k2 + 56, _t12_18);
    _asm256_storeu_pd(M0 + 20*k2 + 76, _t12_19);
  }


  // Generating : Y[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M0[20,20],h(4, 20, 0)) * T( G(h(4, 20, k2), F[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), Q[20,20],h(4, 20, k2)) ),h(4, 20, k2)) + Sum_{i0} ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M0[20,20],h(4, 20, 0)) * T( G(h(4, 20, i0), F[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), Q[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + S(h(4, 20, 16), ( ( G(h(4, 20, 16), M0[20,20],h(4, 20, 0)) * T( G(h(4, 20, 16), F[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, 16), Q[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 20, k2), ( G(h(4, 20, k2), M0[20,20],h(4, 20, k3)) * T( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) ) ),h(4, 20, k2)) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), M0[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), F[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) ) + $(h(4, 20, 16), ( G(h(4, 20, 16), M0[20,20],h(4, 20, k3)) * T( G(h(4, 20, 16), F[20,20],h(4, 20, k3)) ) ),h(4, 20, 16)) ) ) )


  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t13_23 = _mm256_broadcast_sd(M0 + 20*k2);
    _t13_22 = _mm256_broadcast_sd(M0 + 20*k2 + 1);
    _t13_21 = _mm256_broadcast_sd(M0 + 20*k2 + 2);
    _t13_20 = _mm256_broadcast_sd(M0 + 20*k2 + 3);
    _t13_19 = _mm256_broadcast_sd(M0 + 20*k2 + 20);
    _t13_18 = _mm256_broadcast_sd(M0 + 20*k2 + 21);
    _t13_17 = _mm256_broadcast_sd(M0 + 20*k2 + 22);
    _t13_16 = _mm256_broadcast_sd(M0 + 20*k2 + 23);
    _t13_15 = _mm256_broadcast_sd(M0 + 20*k2 + 40);
    _t13_14 = _mm256_broadcast_sd(M0 + 20*k2 + 41);
    _t13_13 = _mm256_broadcast_sd(M0 + 20*k2 + 42);
    _t13_12 = _mm256_broadcast_sd(M0 + 20*k2 + 43);
    _t13_11 = _mm256_broadcast_sd(M0 + 20*k2 + 60);
    _t13_10 = _mm256_broadcast_sd(M0 + 20*k2 + 61);
    _t13_9 = _mm256_broadcast_sd(M0 + 20*k2 + 62);
    _t13_8 = _mm256_broadcast_sd(M0 + 20*k2 + 63);
    _t13_7 = _asm256_loadu_pd(F + 20*k2);
    _t13_6 = _asm256_loadu_pd(F + 20*k2 + 20);
    _t13_5 = _asm256_loadu_pd(F + 20*k2 + 40);
    _t13_4 = _asm256_loadu_pd(F + 20*k2 + 60);
    _t13_3 = _asm256_loadu_pd(Q + 21*k2);
    _t13_2 = _mm256_maskload_pd(Q + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t13_1 = _mm256_maskload_pd(Q + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t13_0 = _mm256_maskload_pd(Q + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t13_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 32);
    _t13_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 32);
    _t13_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 49);
    _t13_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t13_40), _mm256_mul_pd(_t13_22, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t13_42), _mm256_mul_pd(_t13_20, _t13_43)));
    _t13_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_40), _mm256_mul_pd(_t13_18, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_42), _mm256_mul_pd(_t13_16, _t13_43)));
    _t13_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_40), _mm256_mul_pd(_t13_14, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_42), _mm256_mul_pd(_t13_12, _t13_43)));
    _t13_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_40), _mm256_mul_pd(_t13_10, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_42), _mm256_mul_pd(_t13_8, _t13_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t13_36 = _t13_3;
    _t13_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 3), _t13_2, 12);
    _t13_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 0), _t13_1, 49);
    _t13_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 12), _mm256_shuffle_pd(_t13_1, _t13_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t13_24 = _mm256_add_pd(_t13_32, _t13_36);
    _t13_25 = _mm256_add_pd(_t13_33, _t13_37);
    _t13_26 = _mm256_add_pd(_t13_34, _t13_38);
    _t13_27 = _mm256_add_pd(_t13_35, _t13_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t13_28 = _t13_24;
    _t13_29 = _t13_25;
    _t13_30 = _t13_26;
    _t13_31 = _t13_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
      _t14_7 = _asm256_loadu_pd(F + 20*i0);
      _t14_6 = _asm256_loadu_pd(F + 20*i0 + 20);
      _t14_5 = _asm256_loadu_pd(F + 20*i0 + 40);
      _t14_4 = _asm256_loadu_pd(F + 20*i0 + 60);
      _t14_3 = _asm256_loadu_pd(Q + i0 + 20*k2);
      _t14_2 = _asm256_loadu_pd(Q + i0 + 20*k2 + 20);
      _t14_1 = _asm256_loadu_pd(Q + i0 + 20*k2 + 40);
      _t14_0 = _asm256_loadu_pd(Q + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t14_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 32);
      _t14_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 32);
      _t14_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 49);
      _t14_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t14_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t14_16), _mm256_mul_pd(_t13_22, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t14_18), _mm256_mul_pd(_t13_20, _t14_19)));
      _t14_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t14_16), _mm256_mul_pd(_t13_18, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t14_18), _mm256_mul_pd(_t13_16, _t14_19)));
      _t14_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t14_16), _mm256_mul_pd(_t13_14, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t14_18), _mm256_mul_pd(_t13_12, _t14_19)));
      _t14_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t14_16), _mm256_mul_pd(_t13_10, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t14_18), _mm256_mul_pd(_t13_8, _t14_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t14_8 = _mm256_add_pd(_t14_12, _t14_3);
      _t14_9 = _mm256_add_pd(_t14_13, _t14_2);
      _t14_10 = _mm256_add_pd(_t14_14, _t14_1);
      _t14_11 = _mm256_add_pd(_t14_15, _t14_0);

      // AVX Storer:
      _asm256_storeu_pd(Y + i0 + 20*k2, _t14_8);
      _asm256_storeu_pd(Y + i0 + 20*k2 + 20, _t14_9);
      _asm256_storeu_pd(Y + i0 + 20*k2 + 40, _t14_10);
      _asm256_storeu_pd(Y + i0 + 20*k2 + 60, _t14_11);
    }
    _asm256_storeu_pd(Y + 21*k2, _t13_28);
    _mm256_maskstore_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t13_29);
    _mm256_maskstore_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t13_30);
    _mm256_maskstore_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t13_31);
  }

  _t15_23 = _mm256_broadcast_sd(M0 + 320);
  _t15_22 = _mm256_broadcast_sd(M0 + 321);
  _t15_21 = _mm256_broadcast_sd(M0 + 322);
  _t15_20 = _mm256_broadcast_sd(M0 + 323);
  _t15_19 = _mm256_broadcast_sd(M0 + 340);
  _t15_18 = _mm256_broadcast_sd(M0 + 341);
  _t15_17 = _mm256_broadcast_sd(M0 + 342);
  _t15_16 = _mm256_broadcast_sd(M0 + 343);
  _t15_15 = _mm256_broadcast_sd(M0 + 360);
  _t15_14 = _mm256_broadcast_sd(M0 + 361);
  _t15_13 = _mm256_broadcast_sd(M0 + 362);
  _t15_12 = _mm256_broadcast_sd(M0 + 363);
  _t15_11 = _mm256_broadcast_sd(M0 + 380);
  _t15_10 = _mm256_broadcast_sd(M0 + 381);
  _t15_9 = _mm256_broadcast_sd(M0 + 382);
  _t15_8 = _mm256_broadcast_sd(M0 + 383);
  _t15_7 = _asm256_loadu_pd(F + 320);
  _t15_6 = _asm256_loadu_pd(F + 340);
  _t15_5 = _asm256_loadu_pd(F + 360);
  _t15_4 = _asm256_loadu_pd(F + 380);
  _t15_3 = _asm256_loadu_pd(Q + 336);
  _t15_2 = _mm256_maskload_pd(Q + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t15_1 = _mm256_maskload_pd(Q + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t15_0 = _mm256_maskload_pd(Q + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
  _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
  _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
  _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
  _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
  _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
  _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t15_36 = _t15_3;
  _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
  _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
  _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
  _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
  _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
  _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t15_28 = _t15_24;
  _t15_29 = _t15_25;
  _t15_30 = _t15_26;
  _t15_31 = _t15_27;


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    for( int k2 = 0; k2 <= 15; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 20*k2 + k3);
      _t16_18 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 20);
      _t16_14 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 21);
      _t16_13 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 22);
      _t16_12 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 23);
      _t16_11 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 40);
      _t16_10 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 41);
      _t16_9 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 42);
      _t16_8 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 43);
      _t16_7 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 60);
      _t16_6 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 61);
      _t16_5 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 62);
      _t16_4 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 63);
      _t16_3 = _asm256_loadu_pd(F + 20*k2 + k3);
      _t16_2 = _asm256_loadu_pd(F + 20*k2 + k3 + 20);
      _t16_1 = _asm256_loadu_pd(F + 20*k2 + k3 + 40);
      _t16_0 = _asm256_loadu_pd(F + 20*k2 + k3 + 60);
      _t16_20 = _asm256_loadu_pd(Y + 21*k2);
      _t16_21 = _mm256_maskload_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t16_22 = _mm256_maskload_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t16_23 = _mm256_maskload_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_32), _mm256_mul_pd(_t16_18, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_34), _mm256_mul_pd(_t16_16, _t16_35)));
      _t16_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_32), _mm256_mul_pd(_t16_14, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_34), _mm256_mul_pd(_t16_12, _t16_35)));
      _t16_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_32), _mm256_mul_pd(_t16_10, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_34), _mm256_mul_pd(_t16_8, _t16_35)));
      _t16_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_32), _mm256_mul_pd(_t16_6, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_34), _mm256_mul_pd(_t16_4, _t16_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_28 = _t16_20;
      _t16_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 3), _t16_21, 12);
      _t16_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 0), _t16_22, 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 12), _mm256_shuffle_pd(_t16_22, _t16_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_28 = _mm256_add_pd(_t16_28, _t16_24);
      _t16_29 = _mm256_add_pd(_t16_29, _t16_25);
      _t16_30 = _mm256_add_pd(_t16_30, _t16_26);
      _t16_31 = _mm256_add_pd(_t16_31, _t16_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t16_20 = _t16_28;
      _t16_21 = _t16_29;
      _t16_22 = _t16_30;
      _t16_23 = _t16_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t17_3 = _asm256_loadu_pd(F + 20*i0 + k3);
        _t17_2 = _asm256_loadu_pd(F + 20*i0 + k3 + 20);
        _t17_1 = _asm256_loadu_pd(F + 20*i0 + k3 + 40);
        _t17_0 = _asm256_loadu_pd(F + 20*i0 + k3 + 60);
        _t17_4 = _asm256_loadu_pd(Y + i0 + 20*k2);
        _t17_5 = _asm256_loadu_pd(Y + i0 + 20*k2 + 20);
        _t17_6 = _asm256_loadu_pd(Y + i0 + 20*k2 + 40);
        _t17_7 = _asm256_loadu_pd(Y + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t17_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
        _t17_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
        _t17_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
        _t17_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t17_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t17_12), _mm256_mul_pd(_t16_18, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t17_14), _mm256_mul_pd(_t16_16, _t17_15)));
        _t17_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t17_12), _mm256_mul_pd(_t16_14, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t17_14), _mm256_mul_pd(_t16_12, _t17_15)));
        _t17_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t17_12), _mm256_mul_pd(_t16_10, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t17_14), _mm256_mul_pd(_t16_8, _t17_15)));
        _t17_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t17_12), _mm256_mul_pd(_t16_6, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t17_14), _mm256_mul_pd(_t16_4, _t17_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_4 = _mm256_add_pd(_t17_4, _t17_8);
        _t17_5 = _mm256_add_pd(_t17_5, _t17_9);
        _t17_6 = _mm256_add_pd(_t17_6, _t17_10);
        _t17_7 = _mm256_add_pd(_t17_7, _t17_11);

        // AVX Storer:
        _asm256_storeu_pd(Y + i0 + 20*k2, _t17_4);
        _asm256_storeu_pd(Y + i0 + 20*k2 + 20, _t17_5);
        _asm256_storeu_pd(Y + i0 + 20*k2 + 40, _t17_6);
        _asm256_storeu_pd(Y + i0 + 20*k2 + 60, _t17_7);
      }
      _asm256_storeu_pd(Y + 21*k2, _t16_20);
      _mm256_maskstore_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t16_21);
      _mm256_maskstore_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t16_22);
      _mm256_maskstore_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t16_23);
    }
    _t18_19 = _mm256_broadcast_sd(M0 + k3 + 320);
    _t18_18 = _mm256_broadcast_sd(M0 + k3 + 321);
    _t18_17 = _mm256_broadcast_sd(M0 + k3 + 322);
    _t18_16 = _mm256_broadcast_sd(M0 + k3 + 323);
    _t18_15 = _mm256_broadcast_sd(M0 + k3 + 340);
    _t18_14 = _mm256_broadcast_sd(M0 + k3 + 341);
    _t18_13 = _mm256_broadcast_sd(M0 + k3 + 342);
    _t18_12 = _mm256_broadcast_sd(M0 + k3 + 343);
    _t18_11 = _mm256_broadcast_sd(M0 + k3 + 360);
    _t18_10 = _mm256_broadcast_sd(M0 + k3 + 361);
    _t18_9 = _mm256_broadcast_sd(M0 + k3 + 362);
    _t18_8 = _mm256_broadcast_sd(M0 + k3 + 363);
    _t18_7 = _mm256_broadcast_sd(M0 + k3 + 380);
    _t18_6 = _mm256_broadcast_sd(M0 + k3 + 381);
    _t18_5 = _mm256_broadcast_sd(M0 + k3 + 382);
    _t18_4 = _mm256_broadcast_sd(M0 + k3 + 383);
    _t18_3 = _asm256_loadu_pd(F + k3 + 320);
    _t18_2 = _asm256_loadu_pd(F + k3 + 340);
    _t18_1 = _asm256_loadu_pd(F + k3 + 360);
    _t18_0 = _asm256_loadu_pd(F + k3 + 380);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t18_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
    _t18_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
    _t18_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
    _t18_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_28), _mm256_mul_pd(_t18_18, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_30), _mm256_mul_pd(_t18_16, _t18_31)));
    _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_28), _mm256_mul_pd(_t18_14, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_30), _mm256_mul_pd(_t18_12, _t18_31)));
    _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_28), _mm256_mul_pd(_t18_10, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_30), _mm256_mul_pd(_t18_8, _t18_31)));
    _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_28), _mm256_mul_pd(_t18_6, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_30), _mm256_mul_pd(_t18_4, _t18_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t18_24 = _t15_28;
    _t18_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
    _t18_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
    _t18_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t18_24 = _mm256_add_pd(_t18_24, _t18_20);
    _t18_25 = _mm256_add_pd(_t18_25, _t18_21);
    _t18_26 = _mm256_add_pd(_t18_26, _t18_22);
    _t18_27 = _mm256_add_pd(_t18_27, _t18_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t18_24;
    _t15_29 = _t18_25;
    _t15_30 = _t18_26;
    _t15_31 = _t18_27;
  }


  // Generating : v0[20,1] = ( Sum_{k2} ( S(h(4, 20, k2), ( G(h(4, 20, k2), z[20,1],h(1, 1, 0)) - ( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) * G(h(4, 20, 0), y[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * G(h(4, 20, k3), y[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t19_5 = _asm256_loadu_pd(z + k2);
    _t19_4 = _asm256_loadu_pd(H + 20*k2);
    _t19_3 = _asm256_loadu_pd(H + 20*k2 + 20);
    _t19_2 = _asm256_loadu_pd(H + 20*k2 + 40);
    _t19_1 = _asm256_loadu_pd(H + 20*k2 + 60);
    _t19_0 = _asm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t19_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t19_7 = _mm256_sub_pd(_t19_5, _t19_6);

    // AVX Storer:
    _asm256_storeu_pd(v0 + k2, _t19_7);
  }


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 19; k2+=4 ) {
      _t20_4 = _asm256_loadu_pd(H + 20*k2 + k3);
      _t20_3 = _asm256_loadu_pd(H + 20*k2 + k3 + 20);
      _t20_2 = _asm256_loadu_pd(H + 20*k2 + k3 + 40);
      _t20_1 = _asm256_loadu_pd(H + 20*k2 + k3 + 60);
      _t20_0 = _asm256_loadu_pd(y + k3);
      _t20_5 = _asm256_loadu_pd(v0 + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t20_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t20_5 = _mm256_sub_pd(_t20_5, _t20_6);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k2, _t20_5);
    }
  }

  _t21_3 = _asm256_loadu_pd(Y);
  _t21_2 = _mm256_maskload_pd(Y + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t21_1 = _mm256_maskload_pd(Y + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t21_0 = _mm256_maskload_pd(Y + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) * G(h(4, 20, 0), Y[20,20],h(4, 20, 0)) ),h(4, 20, 0)) + Sum_{i0} ( S(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) * G(h(4, 20, 0), Y[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), Y[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * G(h(4, 20, k3), Y[20,20],h(4, 20, k3)) ),h(4, 20, k3)) ) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * G(h(4, 20, k3), Y[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), Y[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 16)) * G(h(4, 20, 16), Y[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t21_4 = _t21_3;
  _t21_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t21_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t21_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t22_15 = _mm256_broadcast_sd(H + 20*k2);
    _t22_14 = _mm256_broadcast_sd(H + 20*k2 + 1);
    _t22_13 = _mm256_broadcast_sd(H + 20*k2 + 2);
    _t22_12 = _mm256_broadcast_sd(H + 20*k2 + 3);
    _t22_11 = _mm256_broadcast_sd(H + 20*k2 + 20);
    _t22_10 = _mm256_broadcast_sd(H + 20*k2 + 21);
    _t22_9 = _mm256_broadcast_sd(H + 20*k2 + 22);
    _t22_8 = _mm256_broadcast_sd(H + 20*k2 + 23);
    _t22_7 = _mm256_broadcast_sd(H + 20*k2 + 40);
    _t22_6 = _mm256_broadcast_sd(H + 20*k2 + 41);
    _t22_5 = _mm256_broadcast_sd(H + 20*k2 + 42);
    _t22_4 = _mm256_broadcast_sd(H + 20*k2 + 43);
    _t22_3 = _mm256_broadcast_sd(H + 20*k2 + 60);
    _t22_2 = _mm256_broadcast_sd(H + 20*k2 + 61);
    _t22_1 = _mm256_broadcast_sd(H + 20*k2 + 62);
    _t22_0 = _mm256_broadcast_sd(H + 20*k2 + 63);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t22_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t21_4), _mm256_mul_pd(_t22_14, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t21_6), _mm256_mul_pd(_t22_12, _t21_7)));
    _t22_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t21_4), _mm256_mul_pd(_t22_10, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t21_6), _mm256_mul_pd(_t22_8, _t21_7)));
    _t22_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t21_4), _mm256_mul_pd(_t22_6, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t21_6), _mm256_mul_pd(_t22_4, _t21_7)));
    _t22_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t21_4), _mm256_mul_pd(_t22_2, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t21_6), _mm256_mul_pd(_t22_0, _t21_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 19; i0+=4 ) {
      _t23_3 = _asm256_loadu_pd(Y + i0);
      _t23_2 = _asm256_loadu_pd(Y + i0 + 20);
      _t23_1 = _asm256_loadu_pd(Y + i0 + 40);
      _t23_0 = _asm256_loadu_pd(Y + i0 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t23_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t23_3), _mm256_mul_pd(_t22_14, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t23_1), _mm256_mul_pd(_t22_12, _t23_0)));
      _t23_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t23_3), _mm256_mul_pd(_t22_10, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t23_1), _mm256_mul_pd(_t22_8, _t23_0)));
      _t23_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t23_3), _mm256_mul_pd(_t22_6, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t23_1), _mm256_mul_pd(_t22_4, _t23_0)));
      _t23_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t23_3), _mm256_mul_pd(_t22_2, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t23_1), _mm256_mul_pd(_t22_0, _t23_0)));

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 20*k2, _t23_4);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t23_5);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t23_6);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t23_7);
    }
    _asm256_storeu_pd(M1 + 20*k2, _t22_16);
    _asm256_storeu_pd(M1 + 20*k2 + 20, _t22_17);
    _asm256_storeu_pd(M1 + 20*k2 + 40, _t22_18);
    _asm256_storeu_pd(M1 + 20*k2 + 60, _t22_19);
  }


  for( int k3 = 4; k3 <= 15; k3+=4 ) {
    _t24_3 = _asm256_loadu_pd(Y + 21*k3);
    _t24_2 = _mm256_maskload_pd(Y + 21*k3 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t24_1 = _mm256_maskload_pd(Y + 21*k3 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t24_0 = _mm256_maskload_pd(Y + 21*k3 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t24_4 = _t24_3;
    _t24_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
    _t24_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
    _t24_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

    for( int k2 = 0; k2 <= 19; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t25_19 = _mm256_broadcast_sd(H + 20*k2 + k3);
        _t25_18 = _mm256_broadcast_sd(H + 20*k2 + k3 + 1);
        _t25_17 = _mm256_broadcast_sd(H + 20*k2 + k3 + 2);
        _t25_16 = _mm256_broadcast_sd(H + 20*k2 + k3 + 3);
        _t25_15 = _mm256_broadcast_sd(H + 20*k2 + k3 + 20);
        _t25_14 = _mm256_broadcast_sd(H + 20*k2 + k3 + 21);
        _t25_13 = _mm256_broadcast_sd(H + 20*k2 + k3 + 22);
        _t25_12 = _mm256_broadcast_sd(H + 20*k2 + k3 + 23);
        _t25_11 = _mm256_broadcast_sd(H + 20*k2 + k3 + 40);
        _t25_10 = _mm256_broadcast_sd(H + 20*k2 + k3 + 41);
        _t25_9 = _mm256_broadcast_sd(H + 20*k2 + k3 + 42);
        _t25_8 = _mm256_broadcast_sd(H + 20*k2 + k3 + 43);
        _t25_7 = _mm256_broadcast_sd(H + 20*k2 + k3 + 60);
        _t25_6 = _mm256_broadcast_sd(H + 20*k2 + k3 + 61);
        _t25_5 = _mm256_broadcast_sd(H + 20*k2 + k3 + 62);
        _t25_4 = _mm256_broadcast_sd(H + 20*k2 + k3 + 63);
        _t25_3 = _asm256_loadu_pd(Y + 20*i0 + k3);
        _t25_2 = _asm256_loadu_pd(Y + 20*i0 + k3 + 20);
        _t25_1 = _asm256_loadu_pd(Y + 20*i0 + k3 + 40);
        _t25_0 = _asm256_loadu_pd(Y + 20*i0 + k3 + 60);
        _t25_20 = _asm256_loadu_pd(M1 + i0 + 20*k2);
        _t25_21 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 20);
        _t25_22 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 40);
        _t25_23 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t25_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
        _t25_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
        _t25_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
        _t25_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t25_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_28), _mm256_mul_pd(_t25_18, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_30), _mm256_mul_pd(_t25_16, _t25_31)));
        _t25_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_28), _mm256_mul_pd(_t25_14, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_30), _mm256_mul_pd(_t25_12, _t25_31)));
        _t25_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_28), _mm256_mul_pd(_t25_10, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_30), _mm256_mul_pd(_t25_8, _t25_31)));
        _t25_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_28), _mm256_mul_pd(_t25_6, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_30), _mm256_mul_pd(_t25_4, _t25_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t25_20 = _mm256_add_pd(_t25_20, _t25_24);
        _t25_21 = _mm256_add_pd(_t25_21, _t25_25);
        _t25_22 = _mm256_add_pd(_t25_22, _t25_26);
        _t25_23 = _mm256_add_pd(_t25_23, _t25_27);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 20*k2, _t25_20);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t25_21);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t25_22);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t25_23);
      }
      _t26_15 = _mm256_broadcast_sd(H + 20*k2 + k3);
      _t26_14 = _mm256_broadcast_sd(H + 20*k2 + k3 + 1);
      _t26_13 = _mm256_broadcast_sd(H + 20*k2 + k3 + 2);
      _t26_12 = _mm256_broadcast_sd(H + 20*k2 + k3 + 3);
      _t26_11 = _mm256_broadcast_sd(H + 20*k2 + k3 + 20);
      _t26_10 = _mm256_broadcast_sd(H + 20*k2 + k3 + 21);
      _t26_9 = _mm256_broadcast_sd(H + 20*k2 + k3 + 22);
      _t26_8 = _mm256_broadcast_sd(H + 20*k2 + k3 + 23);
      _t26_7 = _mm256_broadcast_sd(H + 20*k2 + k3 + 40);
      _t26_6 = _mm256_broadcast_sd(H + 20*k2 + k3 + 41);
      _t26_5 = _mm256_broadcast_sd(H + 20*k2 + k3 + 42);
      _t26_4 = _mm256_broadcast_sd(H + 20*k2 + k3 + 43);
      _t26_3 = _mm256_broadcast_sd(H + 20*k2 + k3 + 60);
      _t26_2 = _mm256_broadcast_sd(H + 20*k2 + k3 + 61);
      _t26_1 = _mm256_broadcast_sd(H + 20*k2 + k3 + 62);
      _t26_0 = _mm256_broadcast_sd(H + 20*k2 + k3 + 63);
      _t26_16 = _asm256_loadu_pd(M1 + 20*k2 + k3);
      _t26_17 = _asm256_loadu_pd(M1 + 20*k2 + k3 + 20);
      _t26_18 = _asm256_loadu_pd(M1 + 20*k2 + k3 + 40);
      _t26_19 = _asm256_loadu_pd(M1 + 20*k2 + k3 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t26_24 = _t24_3;
      _t26_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
      _t26_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
      _t26_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t26_24), _mm256_mul_pd(_t26_14, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t26_26), _mm256_mul_pd(_t26_12, _t26_27)));
      _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t26_24), _mm256_mul_pd(_t26_10, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t26_26), _mm256_mul_pd(_t26_8, _t26_27)));
      _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t26_24), _mm256_mul_pd(_t26_6, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t26_26), _mm256_mul_pd(_t26_4, _t26_27)));
      _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t26_24), _mm256_mul_pd(_t26_2, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t26_26), _mm256_mul_pd(_t26_0, _t26_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_16 = _mm256_add_pd(_t26_16, _t26_20);
      _t26_17 = _mm256_add_pd(_t26_17, _t26_21);
      _t26_18 = _mm256_add_pd(_t26_18, _t26_22);
      _t26_19 = _mm256_add_pd(_t26_19, _t26_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M1 + 20*k2 + k3, _t26_16);
      _asm256_storeu_pd(M1 + 20*k2 + k3 + 20, _t26_17);
      _asm256_storeu_pd(M1 + 20*k2 + k3 + 40, _t26_18);
      _asm256_storeu_pd(M1 + 20*k2 + k3 + 60, _t26_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t27_3 = _asm256_loadu_pd(Y + i0 + 20*k3);
        _t27_2 = _asm256_loadu_pd(Y + i0 + 20*k3 + 20);
        _t27_1 = _asm256_loadu_pd(Y + i0 + 20*k3 + 40);
        _t27_0 = _asm256_loadu_pd(Y + i0 + 20*k3 + 60);
        _t27_4 = _asm256_loadu_pd(M1 + i0 + 20*k2);
        _t27_5 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 20);
        _t27_6 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 40);
        _t27_7 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t27_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t27_3), _mm256_mul_pd(_t26_14, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t27_1), _mm256_mul_pd(_t26_12, _t27_0)));
        _t27_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t27_3), _mm256_mul_pd(_t26_10, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t27_1), _mm256_mul_pd(_t26_8, _t27_0)));
        _t27_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t27_3), _mm256_mul_pd(_t26_6, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t27_1), _mm256_mul_pd(_t26_4, _t27_0)));
        _t27_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t27_3), _mm256_mul_pd(_t26_2, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t27_1), _mm256_mul_pd(_t26_0, _t27_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t27_4 = _mm256_add_pd(_t27_4, _t27_8);
        _t27_5 = _mm256_add_pd(_t27_5, _t27_9);
        _t27_6 = _mm256_add_pd(_t27_6, _t27_10);
        _t27_7 = _mm256_add_pd(_t27_7, _t27_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 20*k2, _t27_4);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t27_5);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t27_6);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t27_7);
      }
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t28_0 = _t15_28;
  _t28_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t28_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t28_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 15; i0+=4 ) {
      _t29_19 = _mm256_broadcast_sd(H + 20*k2 + 16);
      _t29_18 = _mm256_broadcast_sd(H + 20*k2 + 17);
      _t29_17 = _mm256_broadcast_sd(H + 20*k2 + 18);
      _t29_16 = _mm256_broadcast_sd(H + 20*k2 + 19);
      _t29_15 = _mm256_broadcast_sd(H + 20*k2 + 36);
      _t29_14 = _mm256_broadcast_sd(H + 20*k2 + 37);
      _t29_13 = _mm256_broadcast_sd(H + 20*k2 + 38);
      _t29_12 = _mm256_broadcast_sd(H + 20*k2 + 39);
      _t29_11 = _mm256_broadcast_sd(H + 20*k2 + 56);
      _t29_10 = _mm256_broadcast_sd(H + 20*k2 + 57);
      _t29_9 = _mm256_broadcast_sd(H + 20*k2 + 58);
      _t29_8 = _mm256_broadcast_sd(H + 20*k2 + 59);
      _t29_7 = _mm256_broadcast_sd(H + 20*k2 + 76);
      _t29_6 = _mm256_broadcast_sd(H + 20*k2 + 77);
      _t29_5 = _mm256_broadcast_sd(H + 20*k2 + 78);
      _t29_4 = _mm256_broadcast_sd(H + 20*k2 + 79);
      _t29_3 = _asm256_loadu_pd(Y + 20*i0 + 16);
      _t29_2 = _asm256_loadu_pd(Y + 20*i0 + 36);
      _t29_1 = _asm256_loadu_pd(Y + 20*i0 + 56);
      _t29_0 = _asm256_loadu_pd(Y + 20*i0 + 76);
      _t29_20 = _asm256_loadu_pd(M1 + i0 + 20*k2);
      _t29_21 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 20);
      _t29_22 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 40);
      _t29_23 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
      _t29_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
      _t29_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
      _t29_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t29_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_28), _mm256_mul_pd(_t29_18, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_30), _mm256_mul_pd(_t29_16, _t29_31)));
      _t29_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_28), _mm256_mul_pd(_t29_14, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_30), _mm256_mul_pd(_t29_12, _t29_31)));
      _t29_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_28), _mm256_mul_pd(_t29_10, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_30), _mm256_mul_pd(_t29_8, _t29_31)));
      _t29_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_28), _mm256_mul_pd(_t29_6, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_30), _mm256_mul_pd(_t29_4, _t29_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t29_20 = _mm256_add_pd(_t29_20, _t29_24);
      _t29_21 = _mm256_add_pd(_t29_21, _t29_25);
      _t29_22 = _mm256_add_pd(_t29_22, _t29_26);
      _t29_23 = _mm256_add_pd(_t29_23, _t29_27);

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 20*k2, _t29_20);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t29_21);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t29_22);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t29_23);
    }
    _t30_15 = _mm256_broadcast_sd(H + 20*k2 + 16);
    _t30_14 = _mm256_broadcast_sd(H + 20*k2 + 17);
    _t30_13 = _mm256_broadcast_sd(H + 20*k2 + 18);
    _t30_12 = _mm256_broadcast_sd(H + 20*k2 + 19);
    _t30_11 = _mm256_broadcast_sd(H + 20*k2 + 36);
    _t30_10 = _mm256_broadcast_sd(H + 20*k2 + 37);
    _t30_9 = _mm256_broadcast_sd(H + 20*k2 + 38);
    _t30_8 = _mm256_broadcast_sd(H + 20*k2 + 39);
    _t30_7 = _mm256_broadcast_sd(H + 20*k2 + 56);
    _t30_6 = _mm256_broadcast_sd(H + 20*k2 + 57);
    _t30_5 = _mm256_broadcast_sd(H + 20*k2 + 58);
    _t30_4 = _mm256_broadcast_sd(H + 20*k2 + 59);
    _t30_3 = _mm256_broadcast_sd(H + 20*k2 + 76);
    _t30_2 = _mm256_broadcast_sd(H + 20*k2 + 77);
    _t30_1 = _mm256_broadcast_sd(H + 20*k2 + 78);
    _t30_0 = _mm256_broadcast_sd(H + 20*k2 + 79);
    _t30_16 = _asm256_loadu_pd(M1 + 20*k2 + 16);
    _t30_17 = _asm256_loadu_pd(M1 + 20*k2 + 36);
    _t30_18 = _asm256_loadu_pd(M1 + 20*k2 + 56);
    _t30_19 = _asm256_loadu_pd(M1 + 20*k2 + 76);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t28_0), _mm256_mul_pd(_t30_14, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t28_2), _mm256_mul_pd(_t30_12, _t28_3)));
    _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t28_0), _mm256_mul_pd(_t30_10, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t28_2), _mm256_mul_pd(_t30_8, _t28_3)));
    _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t28_0), _mm256_mul_pd(_t30_6, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t28_2), _mm256_mul_pd(_t30_4, _t28_3)));
    _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_3, _t28_0), _mm256_mul_pd(_t30_2, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_1, _t28_2), _mm256_mul_pd(_t30_0, _t28_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t30_16 = _mm256_add_pd(_t30_16, _t30_20);
    _t30_17 = _mm256_add_pd(_t30_17, _t30_21);
    _t30_18 = _mm256_add_pd(_t30_18, _t30_22);
    _t30_19 = _mm256_add_pd(_t30_19, _t30_23);

    // AVX Storer:
    _asm256_storeu_pd(M1 + 20*k2 + 16, _t30_16);
    _asm256_storeu_pd(M1 + 20*k2 + 36, _t30_17);
    _asm256_storeu_pd(M1 + 20*k2 + 56, _t30_18);
    _asm256_storeu_pd(M1 + 20*k2 + 76, _t30_19);
  }


  // Generating : M2[20,20] = ( ( ( ( Sum_{i0} ( S(h(4, 20, 0), ( G(h(4, 20, 0), Y[20,20],h(4, 20, 0)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 0)) ) ),h(4, 20, i0)) ) + Sum_{k2} ( Sum_{i0} ( S(h(4, 20, k2), ( T( G(h(4, 20, 0), Y[20,20],h(4, 20, k2)) ) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 0)) ) ),h(4, 20, i0)) ) ) ) + Sum_{k3} ( ( ( Sum_{k2} ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) + Sum_{i0} ( $(h(4, 20, k3), ( G(h(4, 20, k3), Y[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 20, k2), ( T( G(h(4, 20, k3), Y[20,20],h(4, 20, k2)) ) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) ) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) ) ) + Sum_{i0} ( $(h(4, 20, 16), ( G(h(4, 20, 16), Y[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t31_0 = _t21_3;
  _t31_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t31_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t31_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t32_3 = _asm256_loadu_pd(H + 20*i0);
    _t32_2 = _asm256_loadu_pd(H + 20*i0 + 20);
    _t32_1 = _asm256_loadu_pd(H + 20*i0 + 40);
    _t32_0 = _asm256_loadu_pd(H + 20*i0 + 60);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t32_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32);
    _t32_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 32);
    _t32_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 49);
    _t32_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 15), _t32_11)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 15), _t32_11)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 15), _t32_11)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 15), _t32_11)));

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0, _t32_4);
    _asm256_storeu_pd(M2 + i0 + 20, _t32_5);
    _asm256_storeu_pd(M2 + i0 + 40, _t32_6);
    _asm256_storeu_pd(M2 + i0 + 60, _t32_7);
  }


  for( int k2 = 4; k2 <= 19; k2+=4 ) {
    _t33_3 = _asm256_loadu_pd(Y + k2);
    _t33_2 = _asm256_loadu_pd(Y + k2 + 20);
    _t33_1 = _asm256_loadu_pd(Y + k2 + 40);
    _t33_0 = _asm256_loadu_pd(Y + k2 + 60);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t34_3 = _asm256_loadu_pd(H + 20*i0);
      _t34_2 = _asm256_loadu_pd(H + 20*i0 + 20);
      _t34_1 = _asm256_loadu_pd(H + 20*i0 + 40);
      _t34_0 = _asm256_loadu_pd(H + 20*i0 + 60);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
      _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
      _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
      _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t34_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
      _t34_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
      _t34_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
      _t34_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t34_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 15), _t34_11)));
      _t34_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 15), _t34_11)));
      _t34_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 15), _t34_11)));
      _t34_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 15), _t34_11)));

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 20*k2, _t34_4);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t34_5);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t34_6);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t34_7);
    }
  }


  for( int k3 = 4; k3 <= 15; k3+=4 ) {

    for( int k2 = 0; k2 <= k3 - 1; k2+=4 ) {

      for( int i0 = 0; i0 <= 19; i0+=4 ) {
        _t35_19 = _mm256_broadcast_sd(Y + 20*k2 + k3);
        _t35_18 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 1);
        _t35_17 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 2);
        _t35_16 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 3);
        _t35_15 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 20);
        _t35_14 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 21);
        _t35_13 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 22);
        _t35_12 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 23);
        _t35_11 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 40);
        _t35_10 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 41);
        _t35_9 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 42);
        _t35_8 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 43);
        _t35_7 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 60);
        _t35_6 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 61);
        _t35_5 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 62);
        _t35_4 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 63);
        _t35_3 = _asm256_loadu_pd(H + 20*i0 + k3);
        _t35_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
        _t35_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
        _t35_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
        _t35_20 = _asm256_loadu_pd(M2 + i0 + 20*k2);
        _t35_21 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 20);
        _t35_22 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 40);
        _t35_23 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t35_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
        _t35_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
        _t35_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
        _t35_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t35_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_28), _mm256_mul_pd(_t35_18, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_30), _mm256_mul_pd(_t35_16, _t35_31)));
        _t35_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_28), _mm256_mul_pd(_t35_14, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_30), _mm256_mul_pd(_t35_12, _t35_31)));
        _t35_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_28), _mm256_mul_pd(_t35_10, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_30), _mm256_mul_pd(_t35_8, _t35_31)));
        _t35_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_28), _mm256_mul_pd(_t35_6, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_30), _mm256_mul_pd(_t35_4, _t35_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t35_20 = _mm256_add_pd(_t35_20, _t35_24);
        _t35_21 = _mm256_add_pd(_t35_21, _t35_25);
        _t35_22 = _mm256_add_pd(_t35_22, _t35_26);
        _t35_23 = _mm256_add_pd(_t35_23, _t35_27);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 20*k2, _t35_20);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t35_21);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t35_22);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t35_23);
      }
    }
    _t36_3 = _asm256_loadu_pd(Y + 21*k3);
    _t36_2 = _mm256_maskload_pd(Y + 21*k3 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t36_1 = _mm256_maskload_pd(Y + 21*k3 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t36_0 = _mm256_maskload_pd(Y + 21*k3 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t36_4 = _t36_3;
    _t36_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
    _t36_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
    _t36_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t37_3 = _asm256_loadu_pd(H + 20*i0 + k3);
      _t37_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
      _t37_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
      _t37_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
      _t37_4 = _asm256_loadu_pd(M2 + i0 + 20*k3);
      _t37_5 = _asm256_loadu_pd(M2 + i0 + 20*k3 + 20);
      _t37_6 = _asm256_loadu_pd(M2 + i0 + 20*k3 + 40);
      _t37_7 = _asm256_loadu_pd(M2 + i0 + 20*k3 + 60);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t37_12 = _t36_3;
      _t37_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
      _t37_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
      _t37_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 15), _t37_19)));
      _t37_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 15), _t37_19)));
      _t37_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 15), _t37_19)));
      _t37_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 15), _t37_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t37_4 = _mm256_add_pd(_t37_4, _t37_8);
      _t37_5 = _mm256_add_pd(_t37_5, _t37_9);
      _t37_6 = _mm256_add_pd(_t37_6, _t37_10);
      _t37_7 = _mm256_add_pd(_t37_7, _t37_11);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 20*k3, _t37_4);
      _asm256_storeu_pd(M2 + i0 + 20*k3 + 20, _t37_5);
      _asm256_storeu_pd(M2 + i0 + 20*k3 + 40, _t37_6);
      _asm256_storeu_pd(M2 + i0 + 20*k3 + 60, _t37_7);
    }

    for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 19; k2+=4 ) {

      for( int i0 = 0; i0 <= 19; i0+=4 ) {
        _t38_7 = _asm256_loadu_pd(Y + k2 + 20*k3);
        _t38_6 = _asm256_loadu_pd(Y + k2 + 20*k3 + 20);
        _t38_5 = _asm256_loadu_pd(Y + k2 + 20*k3 + 40);
        _t38_4 = _asm256_loadu_pd(Y + k2 + 20*k3 + 60);
        _t38_3 = _asm256_loadu_pd(H + 20*i0 + k3);
        _t38_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
        _t38_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
        _t38_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
        _t38_8 = _asm256_loadu_pd(M2 + i0 + 20*k2);
        _t38_9 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 20);
        _t38_10 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 40);
        _t38_11 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 60);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 32);
        _t38_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 32);
        _t38_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 49);
        _t38_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
        _t38_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
        _t38_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
        _t38_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t38_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 15), _t38_23)));
        _t38_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 15), _t38_23)));
        _t38_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 15), _t38_23)));
        _t38_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 15), _t38_23)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t38_8 = _mm256_add_pd(_t38_8, _t38_12);
        _t38_9 = _mm256_add_pd(_t38_9, _t38_13);
        _t38_10 = _mm256_add_pd(_t38_10, _t38_14);
        _t38_11 = _mm256_add_pd(_t38_11, _t38_15);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 20*k2, _t38_8);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t38_9);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t38_10);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t38_11);
      }
    }
  }


  for( int k2 = 0; k2 <= 15; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t39_19 = _mm256_broadcast_sd(Y + 20*k2 + 16);
      _t39_18 = _mm256_broadcast_sd(Y + 20*k2 + 17);
      _t39_17 = _mm256_broadcast_sd(Y + 20*k2 + 18);
      _t39_16 = _mm256_broadcast_sd(Y + 20*k2 + 19);
      _t39_15 = _mm256_broadcast_sd(Y + 20*k2 + 36);
      _t39_14 = _mm256_broadcast_sd(Y + 20*k2 + 37);
      _t39_13 = _mm256_broadcast_sd(Y + 20*k2 + 38);
      _t39_12 = _mm256_broadcast_sd(Y + 20*k2 + 39);
      _t39_11 = _mm256_broadcast_sd(Y + 20*k2 + 56);
      _t39_10 = _mm256_broadcast_sd(Y + 20*k2 + 57);
      _t39_9 = _mm256_broadcast_sd(Y + 20*k2 + 58);
      _t39_8 = _mm256_broadcast_sd(Y + 20*k2 + 59);
      _t39_7 = _mm256_broadcast_sd(Y + 20*k2 + 76);
      _t39_6 = _mm256_broadcast_sd(Y + 20*k2 + 77);
      _t39_5 = _mm256_broadcast_sd(Y + 20*k2 + 78);
      _t39_4 = _mm256_broadcast_sd(Y + 20*k2 + 79);
      _t39_3 = _asm256_loadu_pd(H + 20*i0 + 16);
      _t39_2 = _asm256_loadu_pd(H + 20*i0 + 36);
      _t39_1 = _asm256_loadu_pd(H + 20*i0 + 56);
      _t39_0 = _asm256_loadu_pd(H + 20*i0 + 76);
      _t39_20 = _asm256_loadu_pd(M2 + i0 + 20*k2);
      _t39_21 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 20);
      _t39_22 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 40);
      _t39_23 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t39_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 32);
      _t39_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 32);
      _t39_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 49);
      _t39_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t39_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_28), _mm256_mul_pd(_t39_18, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_30), _mm256_mul_pd(_t39_16, _t39_31)));
      _t39_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_28), _mm256_mul_pd(_t39_14, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_30), _mm256_mul_pd(_t39_12, _t39_31)));
      _t39_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_28), _mm256_mul_pd(_t39_10, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_30), _mm256_mul_pd(_t39_8, _t39_31)));
      _t39_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_28), _mm256_mul_pd(_t39_6, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_30), _mm256_mul_pd(_t39_4, _t39_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_20 = _mm256_add_pd(_t39_20, _t39_24);
      _t39_21 = _mm256_add_pd(_t39_21, _t39_25);
      _t39_22 = _mm256_add_pd(_t39_22, _t39_26);
      _t39_23 = _mm256_add_pd(_t39_23, _t39_27);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 20*k2, _t39_20);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t39_21);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t39_22);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t39_23);
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t40_0 = _t15_28;
  _t40_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t40_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t40_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t41_3 = _asm256_loadu_pd(H + 20*i0 + 16);
    _t41_2 = _asm256_loadu_pd(H + 20*i0 + 36);
    _t41_1 = _asm256_loadu_pd(H + 20*i0 + 56);
    _t41_0 = _asm256_loadu_pd(H + 20*i0 + 76);
    _t41_4 = _asm256_loadu_pd(M2 + i0 + 320);
    _t41_5 = _asm256_loadu_pd(M2 + i0 + 340);
    _t41_6 = _asm256_loadu_pd(M2 + i0 + 360);
    _t41_7 = _asm256_loadu_pd(M2 + i0 + 380);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t41_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 15), _t41_15)));
    _t41_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 15), _t41_15)));
    _t41_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 15), _t41_15)));
    _t41_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 15), _t41_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t41_4 = _mm256_add_pd(_t41_4, _t41_8);
    _t41_5 = _mm256_add_pd(_t41_5, _t41_9);
    _t41_6 = _mm256_add_pd(_t41_6, _t41_10);
    _t41_7 = _mm256_add_pd(_t41_7, _t41_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 320, _t41_4);
    _asm256_storeu_pd(M2 + i0 + 340, _t41_5);
    _asm256_storeu_pd(M2 + i0 + 360, _t41_6);
    _asm256_storeu_pd(M2 + i0 + 380, _t41_7);
  }


  // Generating : M3[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M1[20,20],h(4, 20, 0)) * T( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), R[20,20],h(4, 20, k2)) ),h(4, 20, k2)) + Sum_{i0} ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M1[20,20],h(4, 20, 0)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), R[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + S(h(4, 20, 16), ( ( G(h(4, 20, 16), M1[20,20],h(4, 20, 0)) * T( G(h(4, 20, 16), H[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, 16), R[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 20, k2), ( G(h(4, 20, k2), M1[20,20],h(4, 20, k3)) * T( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) ) ),h(4, 20, k2)) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), M1[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) ) + $(h(4, 20, 16), ( G(h(4, 20, 16), M1[20,20],h(4, 20, k3)) * T( G(h(4, 20, 16), H[20,20],h(4, 20, k3)) ) ),h(4, 20, 16)) ) ) )


  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t42_23 = _mm256_broadcast_sd(M1 + 20*k2);
    _t42_22 = _mm256_broadcast_sd(M1 + 20*k2 + 1);
    _t42_21 = _mm256_broadcast_sd(M1 + 20*k2 + 2);
    _t42_20 = _mm256_broadcast_sd(M1 + 20*k2 + 3);
    _t42_19 = _mm256_broadcast_sd(M1 + 20*k2 + 20);
    _t42_18 = _mm256_broadcast_sd(M1 + 20*k2 + 21);
    _t42_17 = _mm256_broadcast_sd(M1 + 20*k2 + 22);
    _t42_16 = _mm256_broadcast_sd(M1 + 20*k2 + 23);
    _t42_15 = _mm256_broadcast_sd(M1 + 20*k2 + 40);
    _t42_14 = _mm256_broadcast_sd(M1 + 20*k2 + 41);
    _t42_13 = _mm256_broadcast_sd(M1 + 20*k2 + 42);
    _t42_12 = _mm256_broadcast_sd(M1 + 20*k2 + 43);
    _t42_11 = _mm256_broadcast_sd(M1 + 20*k2 + 60);
    _t42_10 = _mm256_broadcast_sd(M1 + 20*k2 + 61);
    _t42_9 = _mm256_broadcast_sd(M1 + 20*k2 + 62);
    _t42_8 = _mm256_broadcast_sd(M1 + 20*k2 + 63);
    _t42_7 = _asm256_loadu_pd(H + 20*k2);
    _t42_6 = _asm256_loadu_pd(H + 20*k2 + 20);
    _t42_5 = _asm256_loadu_pd(H + 20*k2 + 40);
    _t42_4 = _asm256_loadu_pd(H + 20*k2 + 60);
    _t42_3 = _asm256_loadu_pd(R + 21*k2);
    _t42_2 = _mm256_maskload_pd(R + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t42_1 = _mm256_maskload_pd(R + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t42_0 = _mm256_maskload_pd(R + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t42_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 32);
    _t42_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 32);
    _t42_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 49);
    _t42_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t42_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t42_40), _mm256_mul_pd(_t42_22, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t42_42), _mm256_mul_pd(_t42_20, _t42_43)));
    _t42_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t42_40), _mm256_mul_pd(_t42_18, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t42_42), _mm256_mul_pd(_t42_16, _t42_43)));
    _t42_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_40), _mm256_mul_pd(_t42_14, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_42), _mm256_mul_pd(_t42_12, _t42_43)));
    _t42_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_40), _mm256_mul_pd(_t42_10, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_42), _mm256_mul_pd(_t42_8, _t42_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t42_36 = _t42_3;
    _t42_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 3), _t42_2, 12);
    _t42_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 0), _t42_1, 49);
    _t42_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 12), _mm256_shuffle_pd(_t42_1, _t42_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t42_24 = _mm256_add_pd(_t42_32, _t42_36);
    _t42_25 = _mm256_add_pd(_t42_33, _t42_37);
    _t42_26 = _mm256_add_pd(_t42_34, _t42_38);
    _t42_27 = _mm256_add_pd(_t42_35, _t42_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t42_28 = _t42_24;
    _t42_29 = _t42_25;
    _t42_30 = _t42_26;
    _t42_31 = _t42_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
      _t43_7 = _asm256_loadu_pd(H + 20*i0);
      _t43_6 = _asm256_loadu_pd(H + 20*i0 + 20);
      _t43_5 = _asm256_loadu_pd(H + 20*i0 + 40);
      _t43_4 = _asm256_loadu_pd(H + 20*i0 + 60);
      _t43_3 = _asm256_loadu_pd(R + i0 + 20*k2);
      _t43_2 = _asm256_loadu_pd(R + i0 + 20*k2 + 20);
      _t43_1 = _asm256_loadu_pd(R + i0 + 20*k2 + 40);
      _t43_0 = _asm256_loadu_pd(R + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t43_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 32);
      _t43_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 32);
      _t43_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 49);
      _t43_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t43_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t43_16), _mm256_mul_pd(_t42_22, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t43_18), _mm256_mul_pd(_t42_20, _t43_19)));
      _t43_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t43_16), _mm256_mul_pd(_t42_18, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t43_18), _mm256_mul_pd(_t42_16, _t43_19)));
      _t43_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t43_16), _mm256_mul_pd(_t42_14, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t43_18), _mm256_mul_pd(_t42_12, _t43_19)));
      _t43_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t43_16), _mm256_mul_pd(_t42_10, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t43_18), _mm256_mul_pd(_t42_8, _t43_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t43_8 = _mm256_add_pd(_t43_12, _t43_3);
      _t43_9 = _mm256_add_pd(_t43_13, _t43_2);
      _t43_10 = _mm256_add_pd(_t43_14, _t43_1);
      _t43_11 = _mm256_add_pd(_t43_15, _t43_0);

      // AVX Storer:
      _asm256_storeu_pd(M3 + i0 + 20*k2, _t43_8);
      _asm256_storeu_pd(M3 + i0 + 20*k2 + 20, _t43_9);
      _asm256_storeu_pd(M3 + i0 + 20*k2 + 40, _t43_10);
      _asm256_storeu_pd(M3 + i0 + 20*k2 + 60, _t43_11);
    }
    _asm256_storeu_pd(M3 + 21*k2, _t42_28);
    _mm256_maskstore_pd(M3 + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t42_29);
    _mm256_maskstore_pd(M3 + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t42_30);
    _mm256_maskstore_pd(M3 + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t42_31);
  }

  _t44_23 = _mm256_broadcast_sd(M1 + 320);
  _t44_22 = _mm256_broadcast_sd(M1 + 321);
  _t44_21 = _mm256_broadcast_sd(M1 + 322);
  _t44_20 = _mm256_broadcast_sd(M1 + 323);
  _t44_19 = _mm256_broadcast_sd(M1 + 340);
  _t44_18 = _mm256_broadcast_sd(M1 + 341);
  _t44_17 = _mm256_broadcast_sd(M1 + 342);
  _t44_16 = _mm256_broadcast_sd(M1 + 343);
  _t44_15 = _mm256_broadcast_sd(M1 + 360);
  _t44_14 = _mm256_broadcast_sd(M1 + 361);
  _t44_13 = _mm256_broadcast_sd(M1 + 362);
  _t44_12 = _mm256_broadcast_sd(M1 + 363);
  _t44_11 = _mm256_broadcast_sd(M1 + 380);
  _t44_10 = _mm256_broadcast_sd(M1 + 381);
  _t44_9 = _mm256_broadcast_sd(M1 + 382);
  _t44_8 = _mm256_broadcast_sd(M1 + 383);
  _t44_7 = _asm256_loadu_pd(H + 320);
  _t44_6 = _asm256_loadu_pd(H + 340);
  _t44_5 = _asm256_loadu_pd(H + 360);
  _t44_4 = _asm256_loadu_pd(H + 380);
  _t44_3 = _asm256_loadu_pd(R + 336);
  _t44_2 = _mm256_maskload_pd(R + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t44_1 = _mm256_maskload_pd(R + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t44_0 = _mm256_maskload_pd(R + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t44_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 32);
  _t44_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 32);
  _t44_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 49);
  _t44_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t44_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_23, _t44_40), _mm256_mul_pd(_t44_22, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_21, _t44_42), _mm256_mul_pd(_t44_20, _t44_43)));
  _t44_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_19, _t44_40), _mm256_mul_pd(_t44_18, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_17, _t44_42), _mm256_mul_pd(_t44_16, _t44_43)));
  _t44_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_40), _mm256_mul_pd(_t44_14, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_42), _mm256_mul_pd(_t44_12, _t44_43)));
  _t44_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_40), _mm256_mul_pd(_t44_10, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_42), _mm256_mul_pd(_t44_8, _t44_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t44_36 = _t44_3;
  _t44_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 3), _t44_2, 12);
  _t44_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 0), _t44_1, 49);
  _t44_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 12), _mm256_shuffle_pd(_t44_1, _t44_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t44_24 = _mm256_add_pd(_t44_32, _t44_36);
  _t44_25 = _mm256_add_pd(_t44_33, _t44_37);
  _t44_26 = _mm256_add_pd(_t44_34, _t44_38);
  _t44_27 = _mm256_add_pd(_t44_35, _t44_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t44_28 = _t44_24;
  _t44_29 = _t44_25;
  _t44_30 = _t44_26;
  _t44_31 = _t44_27;


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    for( int k2 = 0; k2 <= 15; k2+=4 ) {
      _t45_19 = _mm256_broadcast_sd(M1 + 20*k2 + k3);
      _t45_18 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 1);
      _t45_17 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 2);
      _t45_16 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 3);
      _t45_15 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 20);
      _t45_14 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 21);
      _t45_13 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 22);
      _t45_12 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 23);
      _t45_11 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 40);
      _t45_10 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 41);
      _t45_9 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 42);
      _t45_8 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 43);
      _t45_7 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 60);
      _t45_6 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 61);
      _t45_5 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 62);
      _t45_4 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 63);
      _t45_3 = _asm256_loadu_pd(H + 20*k2 + k3);
      _t45_2 = _asm256_loadu_pd(H + 20*k2 + k3 + 20);
      _t45_1 = _asm256_loadu_pd(H + 20*k2 + k3 + 40);
      _t45_0 = _asm256_loadu_pd(H + 20*k2 + k3 + 60);
      _t45_20 = _asm256_loadu_pd(M3 + 21*k2);
      _t45_21 = _mm256_maskload_pd(M3 + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t45_22 = _mm256_maskload_pd(M3 + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t45_23 = _mm256_maskload_pd(M3 + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t45_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
      _t45_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
      _t45_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
      _t45_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t45_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_32), _mm256_mul_pd(_t45_18, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_34), _mm256_mul_pd(_t45_16, _t45_35)));
      _t45_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_32), _mm256_mul_pd(_t45_14, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_34), _mm256_mul_pd(_t45_12, _t45_35)));
      _t45_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_32), _mm256_mul_pd(_t45_10, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_34), _mm256_mul_pd(_t45_8, _t45_35)));
      _t45_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_32), _mm256_mul_pd(_t45_6, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_34), _mm256_mul_pd(_t45_4, _t45_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t45_28 = _t45_20;
      _t45_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 3), _t45_21, 12);
      _t45_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 0), _t45_22, 49);
      _t45_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 12), _mm256_shuffle_pd(_t45_22, _t45_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t45_28 = _mm256_add_pd(_t45_28, _t45_24);
      _t45_29 = _mm256_add_pd(_t45_29, _t45_25);
      _t45_30 = _mm256_add_pd(_t45_30, _t45_26);
      _t45_31 = _mm256_add_pd(_t45_31, _t45_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t45_20 = _t45_28;
      _t45_21 = _t45_29;
      _t45_22 = _t45_30;
      _t45_23 = _t45_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t46_3 = _asm256_loadu_pd(H + 20*i0 + k3);
        _t46_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
        _t46_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
        _t46_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
        _t46_4 = _asm256_loadu_pd(M3 + i0 + 20*k2);
        _t46_5 = _asm256_loadu_pd(M3 + i0 + 20*k2 + 20);
        _t46_6 = _asm256_loadu_pd(M3 + i0 + 20*k2 + 40);
        _t46_7 = _asm256_loadu_pd(M3 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t46_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
        _t46_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
        _t46_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
        _t46_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t46_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t46_12), _mm256_mul_pd(_t45_18, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t46_14), _mm256_mul_pd(_t45_16, _t46_15)));
        _t46_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t46_12), _mm256_mul_pd(_t45_14, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t46_14), _mm256_mul_pd(_t45_12, _t46_15)));
        _t46_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t46_12), _mm256_mul_pd(_t45_10, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t46_14), _mm256_mul_pd(_t45_8, _t46_15)));
        _t46_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t46_12), _mm256_mul_pd(_t45_6, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t46_14), _mm256_mul_pd(_t45_4, _t46_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t46_4 = _mm256_add_pd(_t46_4, _t46_8);
        _t46_5 = _mm256_add_pd(_t46_5, _t46_9);
        _t46_6 = _mm256_add_pd(_t46_6, _t46_10);
        _t46_7 = _mm256_add_pd(_t46_7, _t46_11);

        // AVX Storer:
        _asm256_storeu_pd(M3 + i0 + 20*k2, _t46_4);
        _asm256_storeu_pd(M3 + i0 + 20*k2 + 20, _t46_5);
        _asm256_storeu_pd(M3 + i0 + 20*k2 + 40, _t46_6);
        _asm256_storeu_pd(M3 + i0 + 20*k2 + 60, _t46_7);
      }
      _asm256_storeu_pd(M3 + 21*k2, _t45_20);
      _mm256_maskstore_pd(M3 + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t45_21);
      _mm256_maskstore_pd(M3 + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t45_22);
      _mm256_maskstore_pd(M3 + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t45_23);
    }
    _t47_19 = _mm256_broadcast_sd(M1 + k3 + 320);
    _t47_18 = _mm256_broadcast_sd(M1 + k3 + 321);
    _t47_17 = _mm256_broadcast_sd(M1 + k3 + 322);
    _t47_16 = _mm256_broadcast_sd(M1 + k3 + 323);
    _t47_15 = _mm256_broadcast_sd(M1 + k3 + 340);
    _t47_14 = _mm256_broadcast_sd(M1 + k3 + 341);
    _t47_13 = _mm256_broadcast_sd(M1 + k3 + 342);
    _t47_12 = _mm256_broadcast_sd(M1 + k3 + 343);
    _t47_11 = _mm256_broadcast_sd(M1 + k3 + 360);
    _t47_10 = _mm256_broadcast_sd(M1 + k3 + 361);
    _t47_9 = _mm256_broadcast_sd(M1 + k3 + 362);
    _t47_8 = _mm256_broadcast_sd(M1 + k3 + 363);
    _t47_7 = _mm256_broadcast_sd(M1 + k3 + 380);
    _t47_6 = _mm256_broadcast_sd(M1 + k3 + 381);
    _t47_5 = _mm256_broadcast_sd(M1 + k3 + 382);
    _t47_4 = _mm256_broadcast_sd(M1 + k3 + 383);
    _t47_3 = _asm256_loadu_pd(H + k3 + 320);
    _t47_2 = _asm256_loadu_pd(H + k3 + 340);
    _t47_1 = _asm256_loadu_pd(H + k3 + 360);
    _t47_0 = _asm256_loadu_pd(H + k3 + 380);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32);
    _t47_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 32);
    _t47_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 49);
    _t47_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_19, _t47_28), _mm256_mul_pd(_t47_18, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_17, _t47_30), _mm256_mul_pd(_t47_16, _t47_31)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_28), _mm256_mul_pd(_t47_14, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_30), _mm256_mul_pd(_t47_12, _t47_31)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_28), _mm256_mul_pd(_t47_10, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_30), _mm256_mul_pd(_t47_8, _t47_31)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_28), _mm256_mul_pd(_t47_6, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_30), _mm256_mul_pd(_t47_4, _t47_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t47_24 = _t44_28;
    _t47_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 3), _t44_29, 12);
    _t47_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 0), _t44_30, 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 12), _mm256_shuffle_pd(_t44_30, _t44_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t47_24 = _mm256_add_pd(_t47_24, _t47_20);
    _t47_25 = _mm256_add_pd(_t47_25, _t47_21);
    _t47_26 = _mm256_add_pd(_t47_26, _t47_22);
    _t47_27 = _mm256_add_pd(_t47_27, _t47_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t44_28 = _t47_24;
    _t44_29 = _t47_25;
    _t44_30 = _t47_26;
    _t44_31 = _t47_27;
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21])));
  _t48_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[22])));
  _t48_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[42])));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[3])));
  _t48_7 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + 23)), _mm256_castpd128_pd256(_mm_load_sd(M3 + 43)), 0);
  _t48_10 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[63])));
  _t48_53 = _asm256_loadu_pd(M3 + 4);
  _t48_11 = _asm256_loadu_pd(M3 + 24);
  _t48_12 = _asm256_loadu_pd(M3 + 44);
  _t48_13 = _asm256_loadu_pd(M3 + 64);
  _t48_17 = _asm256_loadu_pd(M3 + 84);
  _t48_18 = _mm256_maskload_pd(M3 + 104, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t48_19 = _mm256_maskload_pd(M3 + 124, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t48_20 = _mm256_maskload_pd(M3 + 144, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t48_55 = _asm256_loadu_pd(M3 + 8);
  _t48_32 = _asm256_loadu_pd(M3 + 28);
  _t48_33 = _asm256_loadu_pd(M3 + 48);
  _t48_34 = _asm256_loadu_pd(M3 + 68);
  _t48_78 = _asm256_loadu_pd(M3 + 88);
  _t48_79 = _asm256_loadu_pd(M3 + 108);
  _t48_80 = _asm256_loadu_pd(M3 + 128);
  _t48_81 = _asm256_loadu_pd(M3 + 148);
  _t48_38 = _asm256_loadu_pd(M3 + 168);
  _t48_39 = _mm256_maskload_pd(M3 + 188, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t48_40 = _mm256_maskload_pd(M3 + 208, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t48_41 = _mm256_maskload_pd(M3 + 228, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : U[20,20] = S(h(1, 20, 0), Sqrt( G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_86 = _t48_0;

  // 4-BLAC: sqrt(1x4)
  _t48_87 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_86)));

  // AVX Storer:
  _t48_0 = _t48_87;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_88 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_89 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t48_90 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_88), _mm256_castpd256_pd128(_t48_89)));

  // AVX Storer:
  _t48_1 = _t48_90;

  // Generating : U[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), U[20,20],h(2, 20, 1)) ),h(2, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_91 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_92 = _t48_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_93 = _mm256_mul_pd(_t48_91, _t48_92);

  // AVX Storer:
  _t48_2 = _t48_93;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), U[20,20],h(1, 20, 1)) - ( T( G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) Kro G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_94 = _t48_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_95 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: (4x1)^T
  _t48_96 = _t48_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_97 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_98 = _mm256_mul_pd(_t48_96, _t48_97);

  // 4-BLAC: 1x4 - 1x4
  _t48_99 = _mm256_sub_pd(_t48_94, _t48_98);

  // AVX Storer:
  _t48_3 = _t48_99;

  // Generating : U[20,20] = S(h(1, 20, 1), Sqrt( G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_100 = _t48_3;

  // 4-BLAC: sqrt(1x4)
  _t48_101 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_100)));

  // AVX Storer:
  _t48_3 = _t48_101;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), U[20,20],h(1, 20, 2)) - ( T( G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) Kro G(h(1, 20, 0), U[20,20],h(1, 20, 2)) ) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_102 = _t48_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_103 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: (4x1)^T
  _t48_104 = _t48_103;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_105 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 Kro 1x4
  _t48_106 = _mm256_mul_pd(_t48_104, _t48_105);

  // 4-BLAC: 1x4 - 1x4
  _t48_107 = _mm256_sub_pd(_t48_102, _t48_106);

  // AVX Storer:
  _t48_4 = _t48_107;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), U[20,20],h(1, 20, 2)) Div G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_108 = _t48_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_109 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t48_110 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_108), _mm256_castpd256_pd128(_t48_109)));

  // AVX Storer:
  _t48_4 = _t48_110;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), U[20,20],h(1, 20, 2)) - ( T( G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) * G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_111 = _t48_5;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_112 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_113 = _t48_112;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_114 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t48_115 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_113, _t48_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_113, _t48_114), _mm256_mul_pd(_t48_113, _t48_114), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_113, _t48_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_113, _t48_114), _mm256_mul_pd(_t48_113, _t48_114), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_113, _t48_114), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_113, _t48_114), _mm256_mul_pd(_t48_113, _t48_114), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_116 = _mm256_sub_pd(_t48_111, _t48_115);

  // AVX Storer:
  _t48_5 = _t48_116;

  // Generating : U[20,20] = S(h(1, 20, 2), Sqrt( G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_117 = _t48_5;

  // 4-BLAC: sqrt(1x4)
  _t48_118 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_117)));

  // AVX Storer:
  _t48_5 = _t48_118;

  // Generating : U[20,20] = S(h(1, 20, 0), ( G(h(1, 20, 0), U[20,20],h(1, 20, 3)) Div G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_119 = _t48_6;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_120 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t48_121 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_119), _mm256_castpd256_pd128(_t48_120)));

  // AVX Storer:
  _t48_6 = _t48_121;

  // Generating : U[20,20] = S(h(2, 20, 1), ( G(h(2, 20, 1), U[20,20],h(1, 20, 3)) - ( T( G(h(1, 20, 0), U[20,20],h(2, 20, 1)) ) Kro G(h(1, 20, 0), U[20,20],h(1, 20, 3)) ) ),h(1, 20, 3))

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_122 = _t48_7;

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_123 = _t48_2;

  // 4-BLAC: (1x4)^T
  _t48_124 = _t48_123;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_125 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_6, _t48_6, 32), _mm256_permute2f128_pd(_t48_6, _t48_6, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t48_126 = _mm256_mul_pd(_t48_124, _t48_125);

  // 4-BLAC: 4x1 - 4x1
  _t48_127 = _mm256_sub_pd(_t48_122, _t48_126);

  // AVX Storer:
  _t48_7 = _t48_127;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), U[20,20],h(1, 20, 3)) Div G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_128 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_129 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t48_130 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_128), _mm256_castpd256_pd128(_t48_129)));

  // AVX Storer:
  _t48_8 = _t48_130;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) - ( T( G(h(1, 20, 1), U[20,20],h(1, 20, 2)) ) Kro G(h(1, 20, 1), U[20,20],h(1, 20, 3)) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_131 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_7, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_132 = _t48_4;

  // 4-BLAC: (4x1)^T
  _t48_133 = _t48_132;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_134 = _t48_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_135 = _mm256_mul_pd(_t48_133, _t48_134);

  // 4-BLAC: 1x4 - 1x4
  _t48_136 = _mm256_sub_pd(_t48_131, _t48_135);

  // AVX Storer:
  _t48_9 = _t48_136;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) Div G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_137 = _t48_9;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_138 = _t48_5;

  // 4-BLAC: 1x4 / 1x4
  _t48_139 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_137), _mm256_castpd256_pd128(_t48_138)));

  // AVX Storer:
  _t48_9 = _t48_139;

  // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), U[20,20],h(1, 20, 3)) - ( T( G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) * G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_140 = _t48_10;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_141 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_6, _t48_8), _mm256_unpacklo_pd(_t48_9, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t48_142 = _t48_141;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_143 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_6, _t48_8), _mm256_unpacklo_pd(_t48_9, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 * 4x1
  _t48_144 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_142, _t48_143), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_142, _t48_143), _mm256_mul_pd(_t48_142, _t48_143), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_142, _t48_143), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_142, _t48_143), _mm256_mul_pd(_t48_142, _t48_143), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_142, _t48_143), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_142, _t48_143), _mm256_mul_pd(_t48_142, _t48_143), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_145 = _mm256_sub_pd(_t48_140, _t48_144);

  // AVX Storer:
  _t48_10 = _t48_145;

  // Generating : U[20,20] = S(h(1, 20, 3), Sqrt( G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_146 = _t48_10;

  // 4-BLAC: sqrt(1x4)
  _t48_147 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_146)));

  // AVX Storer:
  _t48_10 = _t48_147;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_148 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_149 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t48_150 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_148), _mm256_castpd256_pd128(_t48_149)));

  // AVX Storer:
  _t48_1 = _t48_150;

  // Generating : U[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), U[20,20],h(4, 20, 4)) ),h(4, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_151 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_53 = _mm256_mul_pd(_t48_151, _t48_53);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, 1), ( G(h(3, 20, 1), U[20,20],h(4, 20, 4)) - ( T( G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ) * G(h(1, 20, 0), U[20,20],h(4, 20, 4)) ) ),h(4, 20, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t48_152 = _t48_11;
  _t48_153 = _t48_12;
  _t48_154 = _t48_13;
  _t48_155 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_156 = _mm256_blend_pd(_t48_2, _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_6, 1), _mm256_blend_pd(_mm256_setzero_pd(), _t48_6, 1), 8), 12);

  // 4-BLAC: (1x4)^T
  _t48_157 = _t48_156;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_158 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_157, _t48_157, 32), _mm256_permute2f128_pd(_t48_157, _t48_157, 32), 0), _t48_53);
  _t48_159 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_157, _t48_157, 32), _mm256_permute2f128_pd(_t48_157, _t48_157, 32), 15), _t48_53);
  _t48_160 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_157, _t48_157, 49), _mm256_permute2f128_pd(_t48_157, _t48_157, 49), 0), _t48_53);
  _t48_161 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_157, _t48_157, 49), _mm256_permute2f128_pd(_t48_157, _t48_157, 49), 15), _t48_53);

  // 4-BLAC: 4x4 - 4x4
  _t48_162 = _mm256_sub_pd(_t48_152, _t48_158);
  _t48_163 = _mm256_sub_pd(_t48_153, _t48_159);
  _t48_164 = _mm256_sub_pd(_t48_154, _t48_160);
  _t48_165 = _mm256_sub_pd(_t48_155, _t48_161);

  // AVX Storer:
  _t48_11 = _t48_162;
  _t48_12 = _t48_163;
  _t48_13 = _t48_164;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_166 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_167 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t48_168 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_166), _mm256_castpd256_pd128(_t48_167)));

  // AVX Storer:
  _t48_14 = _t48_168;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), U[20,20],h(4, 20, 4)) ),h(4, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_169 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_14, _t48_14, 32), _mm256_permute2f128_pd(_t48_14, _t48_14, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_11 = _mm256_mul_pd(_t48_169, _t48_11);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, 2), ( G(h(2, 20, 2), U[20,20],h(4, 20, 4)) - ( T( G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) * G(h(1, 20, 1), U[20,20],h(4, 20, 4)) ) ),h(4, 20, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t48_170 = _t48_12;
  _t48_171 = _t48_13;
  _t48_172 = _mm256_setzero_pd();
  _t48_173 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_174 = _mm256_blend_pd(_mm256_unpacklo_pd(_t48_4, _t48_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t48_175 = _t48_174;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_176 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_175, _t48_175, 32), _mm256_permute2f128_pd(_t48_175, _t48_175, 32), 0), _t48_11);
  _t48_177 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_175, _t48_175, 32), _mm256_permute2f128_pd(_t48_175, _t48_175, 32), 15), _t48_11);
  _t48_178 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_175, _t48_175, 49), _mm256_permute2f128_pd(_t48_175, _t48_175, 49), 0), _t48_11);
  _t48_179 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_175, _t48_175, 49), _mm256_permute2f128_pd(_t48_175, _t48_175, 49), 15), _t48_11);

  // 4-BLAC: 4x4 - 4x4
  _t48_180 = _mm256_sub_pd(_t48_170, _t48_176);
  _t48_181 = _mm256_sub_pd(_t48_171, _t48_177);
  _t48_182 = _mm256_sub_pd(_t48_172, _t48_178);
  _t48_183 = _mm256_sub_pd(_t48_173, _t48_179);

  // AVX Storer:
  _t48_12 = _t48_180;
  _t48_13 = _t48_181;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_184 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_185 = _t48_5;

  // 4-BLAC: 1x4 / 1x4
  _t48_186 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_184), _mm256_castpd256_pd128(_t48_185)));

  // AVX Storer:
  _t48_15 = _t48_186;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), U[20,20],h(4, 20, 4)) ),h(4, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_187 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_15, _t48_15, 32), _mm256_permute2f128_pd(_t48_15, _t48_15, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_12 = _mm256_mul_pd(_t48_187, _t48_12);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), U[20,20],h(4, 20, 4)) - ( T( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) Kro G(h(1, 20, 2), U[20,20],h(4, 20, 4)) ) ),h(4, 20, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_188 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_189 = _t48_188;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_54 = _mm256_mul_pd(_t48_189, _t48_12);

  // 4-BLAC: 1x4 - 1x4
  _t48_13 = _mm256_sub_pd(_t48_13, _t48_54);

  // AVX Storer:

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_190 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_191 = _t48_10;

  // 4-BLAC: 1x4 / 1x4
  _t48_192 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_190), _mm256_castpd256_pd128(_t48_191)));

  // AVX Storer:
  _t48_16 = _t48_192;

  // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), U[20,20],h(4, 20, 4)) ),h(4, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_193 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_16, _t48_16, 32), _mm256_permute2f128_pd(_t48_16, _t48_16, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_13 = _mm256_mul_pd(_t48_193, _t48_13);

  // AVX Storer:

  // Generating : U[20,20] = S(h(4, 20, 4), ( G(h(4, 20, 4), M4[20,20],h(4, 20, 4)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, 4)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, 4)) ) ),h(4, 20, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t48_194 = _t48_17;
  _t48_195 = _mm256_blend_pd(_mm256_shuffle_pd(_t48_17, _t48_18, 3), _t48_18, 12);
  _t48_196 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t48_17, _t48_18, 0), _t48_19, 49);
  _t48_197 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t48_17, _t48_18, 12), _mm256_shuffle_pd(_t48_19, _t48_20, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t48_422 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_53, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 32);
  _t48_423 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_53, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 32);
  _t48_424 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_53, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 49);
  _t48_425 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_53, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t48_58 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_422, _t48_422, 32), _mm256_permute2f128_pd(_t48_422, _t48_422, 32), 0), _t48_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_422, _t48_422, 32), _mm256_permute2f128_pd(_t48_422, _t48_422, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_422, _t48_422, 49), _mm256_permute2f128_pd(_t48_422, _t48_422, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_422, _t48_422, 49), _mm256_permute2f128_pd(_t48_422, _t48_422, 49), 15), _t48_13)));
  _t48_59 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_423, _t48_423, 32), _mm256_permute2f128_pd(_t48_423, _t48_423, 32), 0), _t48_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_423, _t48_423, 32), _mm256_permute2f128_pd(_t48_423, _t48_423, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_423, _t48_423, 49), _mm256_permute2f128_pd(_t48_423, _t48_423, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_423, _t48_423, 49), _mm256_permute2f128_pd(_t48_423, _t48_423, 49), 15), _t48_13)));
  _t48_60 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_424, _t48_424, 32), _mm256_permute2f128_pd(_t48_424, _t48_424, 32), 0), _t48_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_424, _t48_424, 32), _mm256_permute2f128_pd(_t48_424, _t48_424, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_424, _t48_424, 49), _mm256_permute2f128_pd(_t48_424, _t48_424, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_424, _t48_424, 49), _mm256_permute2f128_pd(_t48_424, _t48_424, 49), 15), _t48_13)));
  _t48_61 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_425, _t48_425, 32), _mm256_permute2f128_pd(_t48_425, _t48_425, 32), 0), _t48_53), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_425, _t48_425, 32), _mm256_permute2f128_pd(_t48_425, _t48_425, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_425, _t48_425, 49), _mm256_permute2f128_pd(_t48_425, _t48_425, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_425, _t48_425, 49), _mm256_permute2f128_pd(_t48_425, _t48_425, 49), 15), _t48_13)));

  // 4-BLAC: 4x4 - 4x4
  _t48_74 = _mm256_sub_pd(_t48_194, _t48_58);
  _t48_75 = _mm256_sub_pd(_t48_195, _t48_59);
  _t48_76 = _mm256_sub_pd(_t48_196, _t48_60);
  _t48_77 = _mm256_sub_pd(_t48_197, _t48_61);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t48_17 = _t48_74;
  _t48_18 = _t48_75;
  _t48_19 = _t48_76;
  _t48_20 = _t48_77;

  // Generating : U[20,20] = S(h(1, 20, 4), Sqrt( G(h(1, 20, 4), U[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_198 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_17, 1);

  // 4-BLAC: sqrt(1x4)
  _t48_199 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_198)));

  // AVX Storer:
  _t48_21 = _t48_199;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 4), U[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_200 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_201 = _t48_21;

  // 4-BLAC: 1x4 / 1x4
  _t48_202 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_200), _mm256_castpd256_pd128(_t48_201)));

  // AVX Storer:
  _t48_22 = _t48_202;

  // Generating : U[20,20] = S(h(1, 20, 4), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, 4)) Kro G(h(1, 20, 4), U[20,20],h(2, 20, 5)) ),h(2, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_203 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_22, _t48_22, 32), _mm256_permute2f128_pd(_t48_22, _t48_22, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_204 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_17, 6), _mm256_permute2f128_pd(_t48_17, _t48_17, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_205 = _mm256_mul_pd(_t48_203, _t48_204);

  // AVX Storer:
  _t48_23 = _t48_205;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), U[20,20],h(1, 20, 5)) - ( T( G(h(1, 20, 4), U[20,20],h(1, 20, 5)) ) Kro G(h(1, 20, 4), U[20,20],h(1, 20, 5)) ) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_206 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_18, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_207 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_23, 1);

  // 4-BLAC: (4x1)^T
  _t48_208 = _t48_207;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_209 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_23, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_210 = _mm256_mul_pd(_t48_208, _t48_209);

  // 4-BLAC: 1x4 - 1x4
  _t48_211 = _mm256_sub_pd(_t48_206, _t48_210);

  // AVX Storer:
  _t48_24 = _t48_211;

  // Generating : U[20,20] = S(h(1, 20, 5), Sqrt( G(h(1, 20, 5), U[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_212 = _t48_24;

  // 4-BLAC: sqrt(1x4)
  _t48_213 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_212)));

  // AVX Storer:
  _t48_24 = _t48_213;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), U[20,20],h(1, 20, 6)) - ( T( G(h(1, 20, 4), U[20,20],h(1, 20, 5)) ) Kro G(h(1, 20, 4), U[20,20],h(1, 20, 6)) ) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_214 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_18, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t48_18, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_215 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_23, 1);

  // 4-BLAC: (4x1)^T
  _t48_216 = _t48_215;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_217 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_23, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 Kro 1x4
  _t48_218 = _mm256_mul_pd(_t48_216, _t48_217);

  // 4-BLAC: 1x4 - 1x4
  _t48_219 = _mm256_sub_pd(_t48_214, _t48_218);

  // AVX Storer:
  _t48_25 = _t48_219;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), U[20,20],h(1, 20, 6)) Div G(h(1, 20, 5), U[20,20],h(1, 20, 5)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_220 = _t48_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_221 = _t48_24;

  // 4-BLAC: 1x4 / 1x4
  _t48_222 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_220), _mm256_castpd256_pd128(_t48_221)));

  // AVX Storer:
  _t48_25 = _t48_222;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), U[20,20],h(1, 20, 6)) - ( T( G(h(2, 20, 4), U[20,20],h(1, 20, 6)) ) * G(h(2, 20, 4), U[20,20],h(1, 20, 6)) ) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_223 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_19, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t48_19, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_224 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_25, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_225 = _t48_224;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_226 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_23, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_25, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t48_227 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_225, _t48_226), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_225, _t48_226), _mm256_mul_pd(_t48_225, _t48_226), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_225, _t48_226), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_225, _t48_226), _mm256_mul_pd(_t48_225, _t48_226), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_225, _t48_226), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_225, _t48_226), _mm256_mul_pd(_t48_225, _t48_226), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_228 = _mm256_sub_pd(_t48_223, _t48_227);

  // AVX Storer:
  _t48_26 = _t48_228;

  // Generating : U[20,20] = S(h(1, 20, 6), Sqrt( G(h(1, 20, 6), U[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_229 = _t48_26;

  // 4-BLAC: sqrt(1x4)
  _t48_230 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_229)));

  // AVX Storer:
  _t48_26 = _t48_230;

  // Generating : U[20,20] = S(h(1, 20, 4), ( G(h(1, 20, 4), U[20,20],h(1, 20, 7)) Div G(h(1, 20, 4), U[20,20],h(1, 20, 4)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_231 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t48_17, _t48_17, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_232 = _t48_21;

  // 4-BLAC: 1x4 / 1x4
  _t48_233 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_231), _mm256_castpd256_pd128(_t48_232)));

  // AVX Storer:
  _t48_27 = _t48_233;

  // Generating : U[20,20] = S(h(2, 20, 5), ( G(h(2, 20, 5), U[20,20],h(1, 20, 7)) - ( T( G(h(1, 20, 4), U[20,20],h(2, 20, 5)) ) Kro G(h(1, 20, 4), U[20,20],h(1, 20, 7)) ) ),h(1, 20, 7))

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_234 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t48_18, _t48_18, 129), _mm256_permute2f128_pd(_t48_19, _t48_19, 129));

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_235 = _t48_23;

  // 4-BLAC: (1x4)^T
  _t48_236 = _t48_235;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_237 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_27, _t48_27, 32), _mm256_permute2f128_pd(_t48_27, _t48_27, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t48_238 = _mm256_mul_pd(_t48_236, _t48_237);

  // 4-BLAC: 4x1 - 4x1
  _t48_239 = _mm256_sub_pd(_t48_234, _t48_238);

  // AVX Storer:
  _t48_28 = _t48_239;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), U[20,20],h(1, 20, 7)) Div G(h(1, 20, 5), U[20,20],h(1, 20, 5)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_240 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_28, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_241 = _t48_24;

  // 4-BLAC: 1x4 / 1x4
  _t48_242 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_240), _mm256_castpd256_pd128(_t48_241)));

  // AVX Storer:
  _t48_29 = _t48_242;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), U[20,20],h(1, 20, 7)) - ( T( G(h(1, 20, 5), U[20,20],h(1, 20, 6)) ) Kro G(h(1, 20, 5), U[20,20],h(1, 20, 7)) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_243 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_28, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_244 = _t48_25;

  // 4-BLAC: (4x1)^T
  _t48_245 = _t48_244;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_246 = _t48_29;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_247 = _mm256_mul_pd(_t48_245, _t48_246);

  // 4-BLAC: 1x4 - 1x4
  _t48_248 = _mm256_sub_pd(_t48_243, _t48_247);

  // AVX Storer:
  _t48_30 = _t48_248;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), U[20,20],h(1, 20, 7)) Div G(h(1, 20, 6), U[20,20],h(1, 20, 6)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_249 = _t48_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_250 = _t48_26;

  // 4-BLAC: 1x4 / 1x4
  _t48_251 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_249), _mm256_castpd256_pd128(_t48_250)));

  // AVX Storer:
  _t48_30 = _t48_251;

  // Generating : U[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), U[20,20],h(1, 20, 7)) - ( T( G(h(3, 20, 4), U[20,20],h(1, 20, 7)) ) * G(h(3, 20, 4), U[20,20],h(1, 20, 7)) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_252 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t48_20, _t48_20, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_253 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_27, _t48_29), _mm256_unpacklo_pd(_t48_30, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t48_254 = _t48_253;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_255 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_27, _t48_29), _mm256_unpacklo_pd(_t48_30, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 * 4x1
  _t48_256 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_254, _t48_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_254, _t48_255), _mm256_mul_pd(_t48_254, _t48_255), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_254, _t48_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_254, _t48_255), _mm256_mul_pd(_t48_254, _t48_255), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_254, _t48_255), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_254, _t48_255), _mm256_mul_pd(_t48_254, _t48_255), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_257 = _mm256_sub_pd(_t48_252, _t48_256);

  // AVX Storer:
  _t48_31 = _t48_257;

  // Generating : U[20,20] = S(h(1, 20, 7), Sqrt( G(h(1, 20, 7), U[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_258 = _t48_31;

  // 4-BLAC: sqrt(1x4)
  _t48_259 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_258)));

  // AVX Storer:
  _t48_31 = _t48_259;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641), U[20,20],h(1, 20, fi641)) ),h(1, 20, fi641))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_260 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_261 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t48_262 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_260), _mm256_castpd256_pd128(_t48_261)));

  // AVX Storer:
  _t48_1 = _t48_262;

  // Generating : U[20,20] = S(h(1, 20, fi641), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641)) Kro G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_263 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_55 = _mm256_mul_pd(_t48_263, _t48_55);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, fi641 + 1), ( G(h(3, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, fi641), U[20,20],h(3, 20, fi641 + 1)) ) * G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 3x4 -> 4x4
  _t48_264 = _t48_32;
  _t48_265 = _t48_33;
  _t48_266 = _t48_34;
  _t48_267 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_268 = _mm256_blend_pd(_t48_2, _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_6, 1), _mm256_blend_pd(_mm256_setzero_pd(), _t48_6, 1), 8), 12);

  // 4-BLAC: (1x4)^T
  _t48_269 = _t48_268;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_270 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 32), _mm256_permute2f128_pd(_t48_269, _t48_269, 32), 0), _t48_55);
  _t48_271 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 32), _mm256_permute2f128_pd(_t48_269, _t48_269, 32), 15), _t48_55);
  _t48_272 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 49), _mm256_permute2f128_pd(_t48_269, _t48_269, 49), 0), _t48_55);
  _t48_273 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 49), _mm256_permute2f128_pd(_t48_269, _t48_269, 49), 15), _t48_55);

  // 4-BLAC: 4x4 - 4x4
  _t48_274 = _mm256_sub_pd(_t48_264, _t48_270);
  _t48_275 = _mm256_sub_pd(_t48_265, _t48_271);
  _t48_276 = _mm256_sub_pd(_t48_266, _t48_272);
  _t48_277 = _mm256_sub_pd(_t48_267, _t48_273);

  // AVX Storer:
  _t48_32 = _t48_274;
  _t48_33 = _t48_275;
  _t48_34 = _t48_276;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641 + 1), U[20,20],h(1, 20, fi641 + 1)) ),h(1, 20, fi641 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_278 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_279 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t48_280 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_278), _mm256_castpd256_pd128(_t48_279)));

  // AVX Storer:
  _t48_14 = _t48_280;

  // Generating : U[20,20] = S(h(1, 20, fi641 + 1), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 1)) Kro G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_281 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_14, _t48_14, 32), _mm256_permute2f128_pd(_t48_14, _t48_14, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_32 = _mm256_mul_pd(_t48_281, _t48_32);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, fi641 + 2), ( G(h(2, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, fi641 + 1), U[20,20],h(2, 20, fi641 + 2)) ) * G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 2x4 -> 4x4
  _t48_282 = _t48_33;
  _t48_283 = _t48_34;
  _t48_284 = _mm256_setzero_pd();
  _t48_285 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_286 = _mm256_blend_pd(_mm256_unpacklo_pd(_t48_4, _t48_8), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t48_287 = _t48_286;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_288 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 32), _mm256_permute2f128_pd(_t48_287, _t48_287, 32), 0), _t48_32);
  _t48_289 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 32), _mm256_permute2f128_pd(_t48_287, _t48_287, 32), 15), _t48_32);
  _t48_290 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 49), _mm256_permute2f128_pd(_t48_287, _t48_287, 49), 0), _t48_32);
  _t48_291 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 49), _mm256_permute2f128_pd(_t48_287, _t48_287, 49), 15), _t48_32);

  // 4-BLAC: 4x4 - 4x4
  _t48_292 = _mm256_sub_pd(_t48_282, _t48_288);
  _t48_293 = _mm256_sub_pd(_t48_283, _t48_289);
  _t48_294 = _mm256_sub_pd(_t48_284, _t48_290);
  _t48_295 = _mm256_sub_pd(_t48_285, _t48_291);

  // AVX Storer:
  _t48_33 = _t48_292;
  _t48_34 = _t48_293;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641 + 2), U[20,20],h(1, 20, fi641 + 2)) ),h(1, 20, fi641 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_296 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_297 = _t48_5;

  // 4-BLAC: 1x4 / 1x4
  _t48_298 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_296), _mm256_castpd256_pd128(_t48_297)));

  // AVX Storer:
  _t48_15 = _t48_298;

  // Generating : U[20,20] = S(h(1, 20, fi641 + 2), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 2)) Kro G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_299 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_15, _t48_15, 32), _mm256_permute2f128_pd(_t48_15, _t48_15, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_33 = _mm256_mul_pd(_t48_299, _t48_33);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi641 + 3), ( G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, fi641 + 2), U[20,20],h(1, 20, fi641 + 3)) ) Kro G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_300 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_301 = _t48_300;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_56 = _mm256_mul_pd(_t48_301, _t48_33);

  // 4-BLAC: 1x4 - 1x4
  _t48_34 = _mm256_sub_pd(_t48_34, _t48_56);

  // AVX Storer:

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641 + 3), U[20,20],h(1, 20, fi641 + 3)) ),h(1, 20, fi641 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_302 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_303 = _t48_10;

  // 4-BLAC: 1x4 / 1x4
  _t48_304 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_302), _mm256_castpd256_pd128(_t48_303)));

  // AVX Storer:
  _t48_16 = _t48_304;

  // Generating : U[20,20] = S(h(1, 20, fi641 + 3), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 3)) Kro G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_305 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_16, _t48_16, 32), _mm256_permute2f128_pd(_t48_16, _t48_16, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_34 = _mm256_mul_pd(_t48_305, _t48_34);

  // AVX Storer:

  // Generating : U[20,20] = Sum_{k3} ( S(h(4, 20, fi641 + k3 + 4), ( G(h(4, 20, fi641 + k3 + 4), U[20,20],h(4, 20, fi562)) - ( T( G(h(4, 20, fi641), U[20,20],h(4, 20, fi641 + k3 + 4)) ) * G(h(4, 20, fi641), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562)) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t48_426 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_53, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 32);
  _t48_427 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_53, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 32);
  _t48_428 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_53, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 49);
  _t48_429 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_53, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t48_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 32), _mm256_permute2f128_pd(_t48_426, _t48_426, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 32), _mm256_permute2f128_pd(_t48_426, _t48_426, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 49), _mm256_permute2f128_pd(_t48_426, _t48_426, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 49), _mm256_permute2f128_pd(_t48_426, _t48_426, 49), 15), _t48_34)));
  _t48_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 32), _mm256_permute2f128_pd(_t48_427, _t48_427, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 32), _mm256_permute2f128_pd(_t48_427, _t48_427, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 49), _mm256_permute2f128_pd(_t48_427, _t48_427, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 49), _mm256_permute2f128_pd(_t48_427, _t48_427, 49), 15), _t48_34)));
  _t48_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 32), _mm256_permute2f128_pd(_t48_428, _t48_428, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 32), _mm256_permute2f128_pd(_t48_428, _t48_428, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 49), _mm256_permute2f128_pd(_t48_428, _t48_428, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 49), _mm256_permute2f128_pd(_t48_428, _t48_428, 49), 15), _t48_34)));
  _t48_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 32), _mm256_permute2f128_pd(_t48_429, _t48_429, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 32), _mm256_permute2f128_pd(_t48_429, _t48_429, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 49), _mm256_permute2f128_pd(_t48_429, _t48_429, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 49), _mm256_permute2f128_pd(_t48_429, _t48_429, 49), 15), _t48_34)));

  // 4-BLAC: 4x4 - 4x4
  _t48_78 = _mm256_sub_pd(_t48_78, _t48_62);
  _t48_79 = _mm256_sub_pd(_t48_79, _t48_63);
  _t48_80 = _mm256_sub_pd(_t48_80, _t48_64);
  _t48_81 = _mm256_sub_pd(_t48_81, _t48_65);

  // AVX Storer:

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(1, 20, Max(0, fi562 - 4))) ),h(1, 20, Max(0, fi562 - 4)))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_306 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_307 = _t48_21;

  // 4-BLAC: 1x4 / 1x4
  _t48_308 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_306), _mm256_castpd256_pd128(_t48_307)));

  // AVX Storer:
  _t48_22 = _t48_308;

  // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4)), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4))) Kro G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_309 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_22, _t48_22, 32), _mm256_permute2f128_pd(_t48_22, _t48_22, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_78 = _mm256_mul_pd(_t48_309, _t48_78);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, Max(0, fi562 - 4) + 1), ( G(h(3, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(3, 20, Max(0, fi562 - 4) + 1)) ) * G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 3x4 -> 4x4
  _t48_310 = _t48_79;
  _t48_311 = _t48_80;
  _t48_312 = _t48_81;
  _t48_313 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_314 = _mm256_blend_pd(_t48_23, _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_27, 1), _mm256_blend_pd(_mm256_setzero_pd(), _t48_27, 1), 8), 12);

  // 4-BLAC: (1x4)^T
  _t48_315 = _t48_314;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_316 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 32), _mm256_permute2f128_pd(_t48_315, _t48_315, 32), 0), _t48_78);
  _t48_317 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 32), _mm256_permute2f128_pd(_t48_315, _t48_315, 32), 15), _t48_78);
  _t48_318 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 49), _mm256_permute2f128_pd(_t48_315, _t48_315, 49), 0), _t48_78);
  _t48_319 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 49), _mm256_permute2f128_pd(_t48_315, _t48_315, 49), 15), _t48_78);

  // 4-BLAC: 4x4 - 4x4
  _t48_320 = _mm256_sub_pd(_t48_310, _t48_316);
  _t48_321 = _mm256_sub_pd(_t48_311, _t48_317);
  _t48_322 = _mm256_sub_pd(_t48_312, _t48_318);
  _t48_323 = _mm256_sub_pd(_t48_313, _t48_319);

  // AVX Storer:
  _t48_79 = _t48_320;
  _t48_80 = _t48_321;
  _t48_81 = _t48_322;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(1, 20, Max(0, fi562 - 4) + 1)) ),h(1, 20, Max(0, fi562 - 4) + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_324 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_325 = _t48_24;

  // 4-BLAC: 1x4 / 1x4
  _t48_326 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_324), _mm256_castpd256_pd128(_t48_325)));

  // AVX Storer:
  _t48_35 = _t48_326;

  // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 1), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 1)) Kro G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_327 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_35, _t48_35, 32), _mm256_permute2f128_pd(_t48_35, _t48_35, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_79 = _mm256_mul_pd(_t48_327, _t48_79);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, Max(0, fi562 - 4) + 2), ( G(h(2, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(2, 20, Max(0, fi562 - 4) + 2)) ) * G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 2x4 -> 4x4
  _t48_328 = _t48_80;
  _t48_329 = _t48_81;
  _t48_330 = _mm256_setzero_pd();
  _t48_331 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_332 = _mm256_blend_pd(_mm256_unpacklo_pd(_t48_25, _t48_29), _mm256_setzero_pd(), 12);

  // 4-BLAC: (1x4)^T
  _t48_333 = _t48_332;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_334 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 32), _mm256_permute2f128_pd(_t48_333, _t48_333, 32), 0), _t48_79);
  _t48_335 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 32), _mm256_permute2f128_pd(_t48_333, _t48_333, 32), 15), _t48_79);
  _t48_336 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 49), _mm256_permute2f128_pd(_t48_333, _t48_333, 49), 0), _t48_79);
  _t48_337 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 49), _mm256_permute2f128_pd(_t48_333, _t48_333, 49), 15), _t48_79);

  // 4-BLAC: 4x4 - 4x4
  _t48_338 = _mm256_sub_pd(_t48_328, _t48_334);
  _t48_339 = _mm256_sub_pd(_t48_329, _t48_335);
  _t48_340 = _mm256_sub_pd(_t48_330, _t48_336);
  _t48_341 = _mm256_sub_pd(_t48_331, _t48_337);

  // AVX Storer:
  _t48_80 = _t48_338;
  _t48_81 = _t48_339;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(1, 20, Max(0, fi562 - 4) + 2)) ),h(1, 20, Max(0, fi562 - 4) + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_342 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_343 = _t48_26;

  // 4-BLAC: 1x4 / 1x4
  _t48_344 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_342), _mm256_castpd256_pd128(_t48_343)));

  // AVX Storer:
  _t48_36 = _t48_344;

  // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 2), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 2)) Kro G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_345 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_36, _t48_36, 32), _mm256_permute2f128_pd(_t48_36, _t48_36, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_80 = _mm256_mul_pd(_t48_345, _t48_80);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 3), ( G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(1, 20, Max(0, fi562 - 4) + 3)) ) Kro G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_346 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_30, _t48_30, 32), _mm256_permute2f128_pd(_t48_30, _t48_30, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_347 = _t48_346;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_57 = _mm256_mul_pd(_t48_347, _t48_80);

  // 4-BLAC: 1x4 - 1x4
  _t48_81 = _mm256_sub_pd(_t48_81, _t48_57);

  // AVX Storer:

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(1, 20, Max(0, fi562 - 4) + 3)) ),h(1, 20, Max(0, fi562 - 4) + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_348 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_349 = _t48_31;

  // 4-BLAC: 1x4 / 1x4
  _t48_350 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_348), _mm256_castpd256_pd128(_t48_349)));

  // AVX Storer:
  _t48_37 = _t48_350;

  // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 3), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 3)) Kro G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_351 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_37, _t48_37, 32), _mm256_permute2f128_pd(_t48_37, _t48_37, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_81 = _mm256_mul_pd(_t48_351, _t48_81);

  // AVX Storer:

  // Generating : U[20,20] = ( S(h(4, 20, fi562), ( G(h(4, 20, fi562), M4[20,20],h(4, 20, fi562)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, fi562)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562)) + Sum_{k3} ( -$(h(4, 20, fi562), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, fi562)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t48_352 = _t48_38;
  _t48_353 = _mm256_blend_pd(_mm256_shuffle_pd(_t48_38, _t48_39, 3), _t48_39, 12);
  _t48_354 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t48_38, _t48_39, 0), _t48_40, 49);
  _t48_355 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t48_38, _t48_39, 12), _mm256_shuffle_pd(_t48_40, _t48_41, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t48_430 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_55, _t48_32), _mm256_unpacklo_pd(_t48_33, _t48_34), 32);
  _t48_431 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_55, _t48_32), _mm256_unpackhi_pd(_t48_33, _t48_34), 32);
  _t48_432 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_55, _t48_32), _mm256_unpacklo_pd(_t48_33, _t48_34), 49);
  _t48_433 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_55, _t48_32), _mm256_unpackhi_pd(_t48_33, _t48_34), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t48_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 32), _mm256_permute2f128_pd(_t48_430, _t48_430, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 32), _mm256_permute2f128_pd(_t48_430, _t48_430, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 49), _mm256_permute2f128_pd(_t48_430, _t48_430, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 49), _mm256_permute2f128_pd(_t48_430, _t48_430, 49), 15), _t48_34)));
  _t48_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 32), _mm256_permute2f128_pd(_t48_431, _t48_431, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 32), _mm256_permute2f128_pd(_t48_431, _t48_431, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 49), _mm256_permute2f128_pd(_t48_431, _t48_431, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 49), _mm256_permute2f128_pd(_t48_431, _t48_431, 49), 15), _t48_34)));
  _t48_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 32), _mm256_permute2f128_pd(_t48_432, _t48_432, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 32), _mm256_permute2f128_pd(_t48_432, _t48_432, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 49), _mm256_permute2f128_pd(_t48_432, _t48_432, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 49), _mm256_permute2f128_pd(_t48_432, _t48_432, 49), 15), _t48_34)));
  _t48_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 32), _mm256_permute2f128_pd(_t48_433, _t48_433, 32), 0), _t48_55), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 32), _mm256_permute2f128_pd(_t48_433, _t48_433, 32), 15), _t48_32)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 49), _mm256_permute2f128_pd(_t48_433, _t48_433, 49), 0), _t48_33), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 49), _mm256_permute2f128_pd(_t48_433, _t48_433, 49), 15), _t48_34)));

  // 4-BLAC: 4x4 - 4x4
  _t48_82 = _mm256_sub_pd(_t48_352, _t48_66);
  _t48_83 = _mm256_sub_pd(_t48_353, _t48_67);
  _t48_84 = _mm256_sub_pd(_t48_354, _t48_68);
  _t48_85 = _mm256_sub_pd(_t48_355, _t48_69);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t48_38 = _t48_82;
  _t48_39 = _t48_83;
  _t48_40 = _t48_84;
  _t48_41 = _t48_85;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t48_434 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_78, _t48_79), _mm256_unpacklo_pd(_t48_80, _t48_81), 32);
  _t48_435 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_78, _t48_79), _mm256_unpackhi_pd(_t48_80, _t48_81), 32);
  _t48_436 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_78, _t48_79), _mm256_unpacklo_pd(_t48_80, _t48_81), 49);
  _t48_437 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_78, _t48_79), _mm256_unpackhi_pd(_t48_80, _t48_81), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t48_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 32), _mm256_permute2f128_pd(_t48_434, _t48_434, 32), 0), _t48_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 32), _mm256_permute2f128_pd(_t48_434, _t48_434, 32), 15), _t48_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 49), _mm256_permute2f128_pd(_t48_434, _t48_434, 49), 0), _t48_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 49), _mm256_permute2f128_pd(_t48_434, _t48_434, 49), 15), _t48_81)));
  _t48_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 32), _mm256_permute2f128_pd(_t48_435, _t48_435, 32), 0), _t48_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 32), _mm256_permute2f128_pd(_t48_435, _t48_435, 32), 15), _t48_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 49), _mm256_permute2f128_pd(_t48_435, _t48_435, 49), 0), _t48_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 49), _mm256_permute2f128_pd(_t48_435, _t48_435, 49), 15), _t48_81)));
  _t48_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 32), _mm256_permute2f128_pd(_t48_436, _t48_436, 32), 0), _t48_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 32), _mm256_permute2f128_pd(_t48_436, _t48_436, 32), 15), _t48_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 49), _mm256_permute2f128_pd(_t48_436, _t48_436, 49), 0), _t48_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 49), _mm256_permute2f128_pd(_t48_436, _t48_436, 49), 15), _t48_81)));
  _t48_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 32), _mm256_permute2f128_pd(_t48_437, _t48_437, 32), 0), _t48_78), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 32), _mm256_permute2f128_pd(_t48_437, _t48_437, 32), 15), _t48_79)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 49), _mm256_permute2f128_pd(_t48_437, _t48_437, 49), 0), _t48_80), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 49), _mm256_permute2f128_pd(_t48_437, _t48_437, 49), 15), _t48_81)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t48_356 = _t48_38;
  _t48_357 = _t48_39;
  _t48_358 = _t48_40;
  _t48_359 = _t48_41;

  // 4-BLAC: 4x4 - 4x4
  _t48_356 = _mm256_sub_pd(_t48_356, _t48_70);
  _t48_357 = _mm256_sub_pd(_t48_357, _t48_71);
  _t48_358 = _mm256_sub_pd(_t48_358, _t48_72);
  _t48_359 = _mm256_sub_pd(_t48_359, _t48_73);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t48_38 = _t48_356;
  _t48_39 = _t48_357;
  _t48_40 = _t48_358;
  _t48_41 = _t48_359;

  // Generating : U[20,20] = S(h(1, 20, fi562), Sqrt( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ),h(1, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_360 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_38, 1);

  // 4-BLAC: sqrt(1x4)
  _t48_361 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_360)));

  // AVX Storer:
  _t48_42 = _t48_361;

  // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ),h(1, 20, fi562))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_362 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_363 = _t48_42;

  // 4-BLAC: 1x4 / 1x4
  _t48_364 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_362), _mm256_castpd256_pd128(_t48_363)));

  // AVX Storer:
  _t48_43 = _t48_364;

  // Generating : U[20,20] = S(h(1, 20, fi562), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi562)) Kro G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ),h(2, 20, fi562 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_365 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_43, _t48_43, 32), _mm256_permute2f128_pd(_t48_43, _t48_43, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_366 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_38, 6), _mm256_permute2f128_pd(_t48_38, _t48_38, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_367 = _mm256_mul_pd(_t48_365, _t48_366);

  // AVX Storer:
  _t48_44 = _t48_367;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) - ( T( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) ),h(1, 20, fi562 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_368 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_39, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_369 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_44, 1);

  // 4-BLAC: (4x1)^T
  _t48_370 = _t48_369;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_371 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_44, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_372 = _mm256_mul_pd(_t48_370, _t48_371);

  // 4-BLAC: 1x4 - 1x4
  _t48_373 = _mm256_sub_pd(_t48_368, _t48_372);

  // AVX Storer:
  _t48_45 = _t48_373;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 1), Sqrt( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_374 = _t48_45;

  // 4-BLAC: sqrt(1x4)
  _t48_375 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_374)));

  // AVX Storer:
  _t48_45 = _t48_375;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) - ( T( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ),h(1, 20, fi562 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_376 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_39, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t48_39, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_377 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_44, 1);

  // 4-BLAC: (4x1)^T
  _t48_378 = _t48_377;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_379 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_44, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 Kro 1x4
  _t48_380 = _mm256_mul_pd(_t48_378, _t48_379);

  // 4-BLAC: 1x4 - 1x4
  _t48_381 = _mm256_sub_pd(_t48_376, _t48_380);

  // AVX Storer:
  _t48_46 = _t48_381;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) Div G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_382 = _t48_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_383 = _t48_45;

  // 4-BLAC: 1x4 / 1x4
  _t48_384 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_382), _mm256_castpd256_pd128(_t48_383)));

  // AVX Storer:
  _t48_46 = _t48_384;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) - ( T( G(h(2, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) * G(h(2, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ),h(1, 20, fi562 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_385 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_40, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t48_40, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_386 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_44, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_46, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_387 = _t48_386;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_388 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_44, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_46, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t48_389 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_387, _t48_388), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_387, _t48_388), _mm256_mul_pd(_t48_387, _t48_388), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_387, _t48_388), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_387, _t48_388), _mm256_mul_pd(_t48_387, _t48_388), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_387, _t48_388), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_387, _t48_388), _mm256_mul_pd(_t48_387, _t48_388), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_390 = _mm256_sub_pd(_t48_385, _t48_389);

  // AVX Storer:
  _t48_47 = _t48_390;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 2), Sqrt( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ),h(1, 20, fi562 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_391 = _t48_47;

  // 4-BLAC: sqrt(1x4)
  _t48_392 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_391)));

  // AVX Storer:
  _t48_47 = _t48_392;

  // Generating : U[20,20] = S(h(1, 20, fi562), ( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) Div G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_393 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t48_38, _t48_38, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_394 = _t48_42;

  // 4-BLAC: 1x4 / 1x4
  _t48_395 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_393), _mm256_castpd256_pd128(_t48_394)));

  // AVX Storer:
  _t48_48 = _t48_395;

  // Generating : U[20,20] = S(h(2, 20, fi562 + 1), ( G(h(2, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) - ( T( G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_396 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t48_39, _t48_39, 129), _mm256_permute2f128_pd(_t48_40, _t48_40, 129));

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_397 = _t48_44;

  // 4-BLAC: (1x4)^T
  _t48_398 = _t48_397;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_399 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_48, _t48_48, 32), _mm256_permute2f128_pd(_t48_48, _t48_48, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t48_400 = _mm256_mul_pd(_t48_398, _t48_399);

  // 4-BLAC: 4x1 - 4x1
  _t48_401 = _mm256_sub_pd(_t48_396, _t48_400);

  // AVX Storer:
  _t48_49 = _t48_401;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) Div G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_402 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_49, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_403 = _t48_45;

  // 4-BLAC: 1x4 / 1x4
  _t48_404 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_402), _mm256_castpd256_pd128(_t48_403)));

  // AVX Storer:
  _t48_50 = _t48_404;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) - ( T( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) Kro G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_405 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_49, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_406 = _t48_46;

  // 4-BLAC: (4x1)^T
  _t48_407 = _t48_406;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_408 = _t48_50;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_409 = _mm256_mul_pd(_t48_407, _t48_408);

  // 4-BLAC: 1x4 - 1x4
  _t48_410 = _mm256_sub_pd(_t48_405, _t48_409);

  // AVX Storer:
  _t48_51 = _t48_410;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) Div G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_411 = _t48_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_412 = _t48_47;

  // 4-BLAC: 1x4 / 1x4
  _t48_413 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_411), _mm256_castpd256_pd128(_t48_412)));

  // AVX Storer:
  _t48_51 = _t48_413;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 3), ( G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) - ( T( G(h(3, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) * G(h(3, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_414 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t48_41, _t48_41, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_415 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_48, _t48_50), _mm256_unpacklo_pd(_t48_51, _mm256_setzero_pd()), 32);

  // 4-BLAC: (4x1)^T
  _t48_416 = _t48_415;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_417 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_48, _t48_50), _mm256_unpacklo_pd(_t48_51, _mm256_setzero_pd()), 32);

  // 4-BLAC: 1x4 * 4x1
  _t48_418 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_416, _t48_417), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_416, _t48_417), _mm256_mul_pd(_t48_416, _t48_417), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_416, _t48_417), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_416, _t48_417), _mm256_mul_pd(_t48_416, _t48_417), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_416, _t48_417), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_416, _t48_417), _mm256_mul_pd(_t48_416, _t48_417), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_419 = _mm256_sub_pd(_t48_414, _t48_418);

  // AVX Storer:
  _t48_52 = _t48_419;

  // Generating : U[20,20] = S(h(1, 20, fi562 + 3), Sqrt( G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) ),h(1, 20, fi562 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_420 = _t48_52;

  // 4-BLAC: sqrt(1x4)
  _t48_421 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_420)));

  // AVX Storer:
  _t48_52 = _t48_421;

  _asm256_storeu_pd(M3 + 336, _t44_28);
  _mm256_maskstore_pd(M3 + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t44_29);
  _mm256_maskstore_pd(M3 + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t44_30);
  _mm256_maskstore_pd(M3 + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t44_31);
  _mm_store_sd(&(M3[0]), _mm256_castpd256_pd128(_t48_0));
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_2);
  _mm_store_sd(&(M3[21]), _mm256_castpd256_pd128(_t48_3));
  _mm_store_sd(&(M3[22]), _mm256_castpd256_pd128(_t48_4));
  _mm_store_sd(&(M3[42]), _mm256_castpd256_pd128(_t48_5));
  _mm_store_sd(&(M3[3]), _mm256_castpd256_pd128(_t48_6));
  _mm_store_sd(&(M3[23]), _mm256_castpd256_pd128(_t48_8));
  _mm_store_sd(&(M3[43]), _mm256_castpd256_pd128(_t48_9));
  _mm_store_sd(&(M3[63]), _mm256_castpd256_pd128(_t48_10));
  _asm256_storeu_pd(M3 + 4, _t48_53);
  _asm256_storeu_pd(M3 + 24, _t48_11);
  _asm256_storeu_pd(M3 + 44, _t48_12);
  _asm256_storeu_pd(M3 + 64, _t48_13);
  _mm_store_sd(&(M3[84]), _mm256_castpd256_pd128(_t48_21));
  _mm256_maskstore_pd(M3 + 85, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_23);
  _mm_store_sd(&(M3[105]), _mm256_castpd256_pd128(_t48_24));
  _mm_store_sd(&(M3[106]), _mm256_castpd256_pd128(_t48_25));
  _mm_store_sd(&(M3[126]), _mm256_castpd256_pd128(_t48_26));
  _mm_store_sd(&(M3[87]), _mm256_castpd256_pd128(_t48_27));
  _mm_store_sd(&(M3[107]), _mm256_castpd256_pd128(_t48_29));
  _mm_store_sd(&(M3[127]), _mm256_castpd256_pd128(_t48_30));
  _mm_store_sd(&(M3[147]), _mm256_castpd256_pd128(_t48_31));
  _asm256_storeu_pd(M3 + 8, _t48_55);
  _asm256_storeu_pd(M3 + 28, _t48_32);
  _asm256_storeu_pd(M3 + 48, _t48_33);
  _asm256_storeu_pd(M3 + 68, _t48_34);
  _asm256_storeu_pd(M3 + 88, _t48_78);
  _asm256_storeu_pd(M3 + 108, _t48_79);
  _asm256_storeu_pd(M3 + 128, _t48_80);
  _asm256_storeu_pd(M3 + 148, _t48_81);
  _mm_store_sd(&(M3[168]), _mm256_castpd256_pd128(_t48_42));
  _mm256_maskstore_pd(M3 + 169, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_44);
  _mm_store_sd(&(M3[189]), _mm256_castpd256_pd128(_t48_45));
  _mm_store_sd(&(M3[190]), _mm256_castpd256_pd128(_t48_46));
  _mm_store_sd(&(M3[210]), _mm256_castpd256_pd128(_t48_47));
  _mm_store_sd(&(M3[171]), _mm256_castpd256_pd128(_t48_48));
  _mm_store_sd(&(M3[191]), _mm256_castpd256_pd128(_t48_50));
  _mm_store_sd(&(M3[211]), _mm256_castpd256_pd128(_t48_51));
  _mm_store_sd(&(M3[231]), _mm256_castpd256_pd128(_t48_52));

  for( int fi562 = 12; fi562 <= 16; fi562+=4 ) {

    for( int fi641 = 0; fi641 <= fi562 - 5; fi641+=4 ) {
      _t49_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi641])));
      _t49_14 = _asm256_loadu_pd(M3 + fi562 + 20*fi641);
      _t49_8 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 20);
      _t49_9 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 40);
      _t49_10 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 60);
      _t49_5 = _mm256_maskload_pd(M3 + 21*fi641 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
      _t49_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi641 + 21])));
      _t49_3 = _mm256_maskload_pd(M3 + 21*fi641 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
      _t49_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi641 + 42])));
      _t49_1 = _mm256_broadcast_sd(&(M3[21*fi641 + 43]));
      _t49_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi641 + 63])));

      // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641), U[20,20],h(1, 20, fi641)) ),h(1, 20, fi641))

      // AVX Loader:

      // Constant 1x1 -> 1x4
      _t49_15 = _mm256_set_pd(0, 0, 0, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_16 = _t49_6;

      // 4-BLAC: 1x4 / 1x4
      _t49_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_15), _mm256_castpd256_pd128(_t49_16)));

      // AVX Storer:
      _t49_7 = _t49_17;

      // Generating : U[20,20] = S(h(1, 20, fi641), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641)) Kro G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_7, _t49_7, 32), _mm256_permute2f128_pd(_t49_7, _t49_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_14 = _mm256_mul_pd(_t49_18, _t49_14);

      // AVX Storer:

      // Generating : U[20,20] = S(h(3, 20, fi641 + 1), ( G(h(3, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, fi641), U[20,20],h(3, 20, fi641 + 1)) ) * G(h(1, 20, fi641), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

      // AVX Loader:

      // 3x4 -> 4x4
      _t49_19 = _t49_8;
      _t49_20 = _t49_9;
      _t49_21 = _t49_10;
      _t49_22 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t49_23 = _t49_5;

      // 4-BLAC: (1x4)^T
      _t48_269 = _t49_23;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t48_270 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 32), _mm256_permute2f128_pd(_t48_269, _t48_269, 32), 0), _t49_14);
      _t48_271 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 32), _mm256_permute2f128_pd(_t48_269, _t48_269, 32), 15), _t49_14);
      _t48_272 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 49), _mm256_permute2f128_pd(_t48_269, _t48_269, 49), 0), _t49_14);
      _t48_273 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_269, _t48_269, 49), _mm256_permute2f128_pd(_t48_269, _t48_269, 49), 15), _t49_14);

      // 4-BLAC: 4x4 - 4x4
      _t49_24 = _mm256_sub_pd(_t49_19, _t48_270);
      _t49_25 = _mm256_sub_pd(_t49_20, _t48_271);
      _t49_26 = _mm256_sub_pd(_t49_21, _t48_272);
      _t49_27 = _mm256_sub_pd(_t49_22, _t48_273);

      // AVX Storer:
      _t49_8 = _t49_24;
      _t49_9 = _t49_25;
      _t49_10 = _t49_26;

      // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641 + 1), U[20,20],h(1, 20, fi641 + 1)) ),h(1, 20, fi641 + 1))

      // AVX Loader:

      // Constant 1x1 -> 1x4
      _t49_28 = _mm256_set_pd(0, 0, 0, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_29 = _t49_4;

      // 4-BLAC: 1x4 / 1x4
      _t49_30 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_28), _mm256_castpd256_pd128(_t49_29)));

      // AVX Storer:
      _t49_11 = _t49_30;

      // Generating : U[20,20] = S(h(1, 20, fi641 + 1), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 1)) Kro G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_31 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_11, _t49_11, 32), _mm256_permute2f128_pd(_t49_11, _t49_11, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_8 = _mm256_mul_pd(_t49_31, _t49_8);

      // AVX Storer:

      // Generating : U[20,20] = S(h(2, 20, fi641 + 2), ( G(h(2, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, fi641 + 1), U[20,20],h(2, 20, fi641 + 2)) ) * G(h(1, 20, fi641 + 1), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

      // AVX Loader:

      // 2x4 -> 4x4
      _t49_32 = _t49_9;
      _t49_33 = _t49_10;
      _t49_34 = _mm256_setzero_pd();
      _t49_35 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t49_36 = _t49_3;

      // 4-BLAC: (1x4)^T
      _t48_287 = _t49_36;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t48_288 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 32), _mm256_permute2f128_pd(_t48_287, _t48_287, 32), 0), _t49_8);
      _t48_289 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 32), _mm256_permute2f128_pd(_t48_287, _t48_287, 32), 15), _t49_8);
      _t48_290 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 49), _mm256_permute2f128_pd(_t48_287, _t48_287, 49), 0), _t49_8);
      _t48_291 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_287, _t48_287, 49), _mm256_permute2f128_pd(_t48_287, _t48_287, 49), 15), _t49_8);

      // 4-BLAC: 4x4 - 4x4
      _t49_37 = _mm256_sub_pd(_t49_32, _t48_288);
      _t49_38 = _mm256_sub_pd(_t49_33, _t48_289);
      _t49_39 = _mm256_sub_pd(_t49_34, _t48_290);
      _t49_40 = _mm256_sub_pd(_t49_35, _t48_291);

      // AVX Storer:
      _t49_9 = _t49_37;
      _t49_10 = _t49_38;

      // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641 + 2), U[20,20],h(1, 20, fi641 + 2)) ),h(1, 20, fi641 + 2))

      // AVX Loader:

      // Constant 1x1 -> 1x4
      _t49_41 = _mm256_set_pd(0, 0, 0, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_42 = _t49_2;

      // 4-BLAC: 1x4 / 1x4
      _t49_43 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_41), _mm256_castpd256_pd128(_t49_42)));

      // AVX Storer:
      _t49_12 = _t49_43;

      // Generating : U[20,20] = S(h(1, 20, fi641 + 2), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 2)) Kro G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_12, _t49_12, 32), _mm256_permute2f128_pd(_t49_12, _t49_12, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_9 = _mm256_mul_pd(_t49_44, _t49_9);

      // AVX Storer:

      // Generating : U[20,20] = S(h(1, 20, fi641 + 3), ( G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, fi641 + 2), U[20,20],h(1, 20, fi641 + 3)) ) Kro G(h(1, 20, fi641 + 2), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_45 = _t49_1;

      // 4-BLAC: (4x1)^T
      _t48_301 = _t49_45;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t48_56 = _mm256_mul_pd(_t48_301, _t49_9);

      // 4-BLAC: 1x4 - 1x4
      _t49_10 = _mm256_sub_pd(_t49_10, _t48_56);

      // AVX Storer:

      // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi641 + 3), U[20,20],h(1, 20, fi641 + 3)) ),h(1, 20, fi641 + 3))

      // AVX Loader:

      // Constant 1x1 -> 1x4
      _t49_46 = _mm256_set_pd(0, 0, 0, 1);

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_47 = _t49_0;

      // 4-BLAC: 1x4 / 1x4
      _t49_48 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t49_46), _mm256_castpd256_pd128(_t49_47)));

      // AVX Storer:
      _t49_13 = _t49_48;

      // Generating : U[20,20] = S(h(1, 20, fi641 + 3), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi641 + 3)) Kro G(h(1, 20, fi641 + 3), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

      // AVX Loader:

      // 1x1 -> 1x4
      _t49_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_13, _t49_13, 32), _mm256_permute2f128_pd(_t49_13, _t49_13, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t49_10 = _mm256_mul_pd(_t49_49, _t49_10);

      // AVX Storer:

      // Generating : U[20,20] = Sum_{k3} ( S(h(4, 20, fi641 + k3 + 4), ( G(h(4, 20, fi641 + k3 + 4), U[20,20],h(4, 20, fi562)) - ( T( G(h(4, 20, fi641), U[20,20],h(4, 20, fi641 + k3 + 4)) ) * G(h(4, 20, fi641), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562)) )
      _asm256_storeu_pd(M3 + fi562 + 20*fi641, _t49_14);
      _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 20, _t49_8);
      _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 40, _t49_9);
      _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 60, _t49_10);

      for( int k3 = 0; k3 <= fi562 - fi641 - 5; k3+=4 ) {
        _t50_8 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 80);
        _t50_9 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 100);
        _t50_10 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 120);
        _t50_11 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 140);
        _t50_7 = _asm256_loadu_pd(M3 + 21*fi641 + k3 + 4);
        _t50_6 = _asm256_loadu_pd(M3 + 21*fi641 + k3 + 24);
        _t50_5 = _asm256_loadu_pd(M3 + 21*fi641 + k3 + 44);
        _t50_4 = _asm256_loadu_pd(M3 + 21*fi641 + k3 + 64);
        _t50_3 = _asm256_loadu_pd(M3 + fi562 + 20*fi641);
        _t50_2 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 20);
        _t50_1 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 40);
        _t50_0 = _asm256_loadu_pd(M3 + fi562 + 20*fi641 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t48_426 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 32);
        _t48_427 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 32);
        _t48_428 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 49);
        _t48_429 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t48_62 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 32), _mm256_permute2f128_pd(_t48_426, _t48_426, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 32), _mm256_permute2f128_pd(_t48_426, _t48_426, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 49), _mm256_permute2f128_pd(_t48_426, _t48_426, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_426, _t48_426, 49), _mm256_permute2f128_pd(_t48_426, _t48_426, 49), 15), _t50_0)));
        _t48_63 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 32), _mm256_permute2f128_pd(_t48_427, _t48_427, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 32), _mm256_permute2f128_pd(_t48_427, _t48_427, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 49), _mm256_permute2f128_pd(_t48_427, _t48_427, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_427, _t48_427, 49), _mm256_permute2f128_pd(_t48_427, _t48_427, 49), 15), _t50_0)));
        _t48_64 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 32), _mm256_permute2f128_pd(_t48_428, _t48_428, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 32), _mm256_permute2f128_pd(_t48_428, _t48_428, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 49), _mm256_permute2f128_pd(_t48_428, _t48_428, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_428, _t48_428, 49), _mm256_permute2f128_pd(_t48_428, _t48_428, 49), 15), _t50_0)));
        _t48_65 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 32), _mm256_permute2f128_pd(_t48_429, _t48_429, 32), 0), _t50_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 32), _mm256_permute2f128_pd(_t48_429, _t48_429, 32), 15), _t50_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 49), _mm256_permute2f128_pd(_t48_429, _t48_429, 49), 0), _t50_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_429, _t48_429, 49), _mm256_permute2f128_pd(_t48_429, _t48_429, 49), 15), _t50_0)));

        // 4-BLAC: 4x4 - 4x4
        _t50_8 = _mm256_sub_pd(_t50_8, _t48_62);
        _t50_9 = _mm256_sub_pd(_t50_9, _t48_63);
        _t50_10 = _mm256_sub_pd(_t50_10, _t48_64);
        _t50_11 = _mm256_sub_pd(_t50_11, _t48_65);

        // AVX Storer:
        _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 80, _t50_8);
        _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 100, _t50_9);
        _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 120, _t50_10);
        _asm256_storeu_pd(M3 + fi562 + 20*fi641 + 20*k3 + 140, _t50_11);
      }
    }
    _t51_10 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*Max(0, fi562 - 4)])));
    _t51_22 = _asm256_loadu_pd(M3 + fi562 + 20*Max(0, fi562 - 4));
    _t51_12 = _asm256_loadu_pd(M3 + fi562 + 20*Max(0, fi562 - 4) + 20);
    _t51_13 = _asm256_loadu_pd(M3 + fi562 + 20*Max(0, fi562 - 4) + 40);
    _t51_14 = _asm256_loadu_pd(M3 + fi562 + 20*Max(0, fi562 - 4) + 60);
    _t51_9 = _mm256_maskload_pd(M3 + 21*Max(0, fi562 - 4) + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t51_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*Max(0, fi562 - 4) + 21])));
    _t51_7 = _mm256_maskload_pd(M3 + 21*Max(0, fi562 - 4) + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t51_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*Max(0, fi562 - 4) + 42])));
    _t51_5 = _mm256_broadcast_sd(&(M3[21*Max(0, fi562 - 4) + 43]));
    _t51_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*Max(0, fi562 - 4) + 63])));
    _t51_18 = _asm256_loadu_pd(M3 + 21*fi562);
    _t51_19 = _mm256_maskload_pd(M3 + 21*fi562 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t51_20 = _mm256_maskload_pd(M3 + 21*fi562 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t51_21 = _mm256_maskload_pd(M3 + 21*fi562 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t51_3 = _asm256_loadu_pd(M3 + fi562);
    _t51_2 = _asm256_loadu_pd(M3 + fi562 + 20);
    _t51_1 = _asm256_loadu_pd(M3 + fi562 + 40);
    _t51_0 = _asm256_loadu_pd(M3 + fi562 + 60);

    // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(1, 20, Max(0, fi562 - 4))) ),h(1, 20, Max(0, fi562 - 4)))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t51_23 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_24 = _t51_10;

    // 4-BLAC: 1x4 / 1x4
    _t51_25 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_23), _mm256_castpd256_pd128(_t51_24)));

    // AVX Storer:
    _t51_11 = _t51_25;

    // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4)), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4))) Kro G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_11, _t51_11, 32), _mm256_permute2f128_pd(_t51_11, _t51_11, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t51_22 = _mm256_mul_pd(_t51_26, _t51_22);

    // AVX Storer:

    // Generating : U[20,20] = S(h(3, 20, Max(0, fi562 - 4) + 1), ( G(h(3, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(3, 20, Max(0, fi562 - 4) + 1)) ) * G(h(1, 20, Max(0, fi562 - 4)), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // 3x4 -> 4x4
    _t51_27 = _t51_12;
    _t51_28 = _t51_13;
    _t51_29 = _t51_14;
    _t51_30 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t51_31 = _t51_9;

    // 4-BLAC: (1x4)^T
    _t48_315 = _t51_31;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t48_316 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 32), _mm256_permute2f128_pd(_t48_315, _t48_315, 32), 0), _t51_22);
    _t48_317 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 32), _mm256_permute2f128_pd(_t48_315, _t48_315, 32), 15), _t51_22);
    _t48_318 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 49), _mm256_permute2f128_pd(_t48_315, _t48_315, 49), 0), _t51_22);
    _t48_319 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_315, _t48_315, 49), _mm256_permute2f128_pd(_t48_315, _t48_315, 49), 15), _t51_22);

    // 4-BLAC: 4x4 - 4x4
    _t51_32 = _mm256_sub_pd(_t51_27, _t48_316);
    _t51_33 = _mm256_sub_pd(_t51_28, _t48_317);
    _t51_34 = _mm256_sub_pd(_t51_29, _t48_318);
    _t51_35 = _mm256_sub_pd(_t51_30, _t48_319);

    // AVX Storer:
    _t51_12 = _t51_32;
    _t51_13 = _t51_33;
    _t51_14 = _t51_34;

    // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(1, 20, Max(0, fi562 - 4) + 1)) ),h(1, 20, Max(0, fi562 - 4) + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t51_36 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_37 = _t51_8;

    // 4-BLAC: 1x4 / 1x4
    _t51_38 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_36), _mm256_castpd256_pd128(_t51_37)));

    // AVX Storer:
    _t51_15 = _t51_38;

    // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 1), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 1)) Kro G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_39 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_15, _t51_15, 32), _mm256_permute2f128_pd(_t51_15, _t51_15, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t51_12 = _mm256_mul_pd(_t51_39, _t51_12);

    // AVX Storer:

    // Generating : U[20,20] = S(h(2, 20, Max(0, fi562 - 4) + 2), ( G(h(2, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(2, 20, Max(0, fi562 - 4) + 2)) ) * G(h(1, 20, Max(0, fi562 - 4) + 1), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // 2x4 -> 4x4
    _t51_40 = _t51_13;
    _t51_41 = _t51_14;
    _t51_42 = _mm256_setzero_pd();
    _t51_43 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t51_44 = _t51_7;

    // 4-BLAC: (1x4)^T
    _t48_333 = _t51_44;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t48_334 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 32), _mm256_permute2f128_pd(_t48_333, _t48_333, 32), 0), _t51_12);
    _t48_335 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 32), _mm256_permute2f128_pd(_t48_333, _t48_333, 32), 15), _t51_12);
    _t48_336 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 49), _mm256_permute2f128_pd(_t48_333, _t48_333, 49), 0), _t51_12);
    _t48_337 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_333, _t48_333, 49), _mm256_permute2f128_pd(_t48_333, _t48_333, 49), 15), _t51_12);

    // 4-BLAC: 4x4 - 4x4
    _t51_45 = _mm256_sub_pd(_t51_40, _t48_334);
    _t51_46 = _mm256_sub_pd(_t51_41, _t48_335);
    _t51_47 = _mm256_sub_pd(_t51_42, _t48_336);
    _t51_48 = _mm256_sub_pd(_t51_43, _t48_337);

    // AVX Storer:
    _t51_13 = _t51_45;
    _t51_14 = _t51_46;

    // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(1, 20, Max(0, fi562 - 4) + 2)) ),h(1, 20, Max(0, fi562 - 4) + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t51_49 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_50 = _t51_6;

    // 4-BLAC: 1x4 / 1x4
    _t51_51 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_49), _mm256_castpd256_pd128(_t51_50)));

    // AVX Storer:
    _t51_16 = _t51_51;

    // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 2), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 2)) Kro G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_16, _t51_16, 32), _mm256_permute2f128_pd(_t51_16, _t51_16, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t51_13 = _mm256_mul_pd(_t51_52, _t51_13);

    // AVX Storer:

    // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 3), ( G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(1, 20, Max(0, fi562 - 4) + 3)) ) Kro G(h(1, 20, Max(0, fi562 - 4) + 2), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_53 = _t51_5;

    // 4-BLAC: (4x1)^T
    _t48_347 = _t51_53;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_57 = _mm256_mul_pd(_t48_347, _t51_13);

    // 4-BLAC: 1x4 - 1x4
    _t51_14 = _mm256_sub_pd(_t51_14, _t48_57);

    // AVX Storer:

    // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(1, 20, Max(0, fi562 - 4) + 3)) ),h(1, 20, Max(0, fi562 - 4) + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t51_54 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_55 = _t51_4;

    // 4-BLAC: 1x4 / 1x4
    _t51_56 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t51_54), _mm256_castpd256_pd128(_t51_55)));

    // AVX Storer:
    _t51_17 = _t51_56;

    // Generating : U[20,20] = S(h(1, 20, Max(0, fi562 - 4) + 3), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, Max(0, fi562 - 4) + 3)) Kro G(h(1, 20, Max(0, fi562 - 4) + 3), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t51_57 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t51_17, _t51_17, 32), _mm256_permute2f128_pd(_t51_17, _t51_17, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t51_14 = _mm256_mul_pd(_t51_57, _t51_14);

    // AVX Storer:

    // Generating : U[20,20] = ( S(h(4, 20, fi562), ( G(h(4, 20, fi562), M4[20,20],h(4, 20, fi562)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, fi562)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562)) + Sum_{k3} ( -$(h(4, 20, fi562), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, fi562)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, fi562)) ),h(4, 20, fi562)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t51_58 = _t51_18;
    _t51_59 = _mm256_blend_pd(_mm256_shuffle_pd(_t51_18, _t51_19, 3), _t51_19, 12);
    _t51_60 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t51_18, _t51_19, 0), _t51_20, 49);
    _t51_61 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t51_18, _t51_19, 12), _mm256_shuffle_pd(_t51_20, _t51_21, 12), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t48_430 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 32);
    _t48_431 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 32);
    _t48_432 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 49);
    _t48_433 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t48_66 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 32), _mm256_permute2f128_pd(_t48_430, _t48_430, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 32), _mm256_permute2f128_pd(_t48_430, _t48_430, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 49), _mm256_permute2f128_pd(_t48_430, _t48_430, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_430, _t48_430, 49), _mm256_permute2f128_pd(_t48_430, _t48_430, 49), 15), _t51_0)));
    _t48_67 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 32), _mm256_permute2f128_pd(_t48_431, _t48_431, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 32), _mm256_permute2f128_pd(_t48_431, _t48_431, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 49), _mm256_permute2f128_pd(_t48_431, _t48_431, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_431, _t48_431, 49), _mm256_permute2f128_pd(_t48_431, _t48_431, 49), 15), _t51_0)));
    _t48_68 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 32), _mm256_permute2f128_pd(_t48_432, _t48_432, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 32), _mm256_permute2f128_pd(_t48_432, _t48_432, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 49), _mm256_permute2f128_pd(_t48_432, _t48_432, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_432, _t48_432, 49), _mm256_permute2f128_pd(_t48_432, _t48_432, 49), 15), _t51_0)));
    _t48_69 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 32), _mm256_permute2f128_pd(_t48_433, _t48_433, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 32), _mm256_permute2f128_pd(_t48_433, _t48_433, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 49), _mm256_permute2f128_pd(_t48_433, _t48_433, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_433, _t48_433, 49), _mm256_permute2f128_pd(_t48_433, _t48_433, 49), 15), _t51_0)));

    // 4-BLAC: 4x4 - 4x4
    _t48_82 = _mm256_sub_pd(_t51_58, _t48_66);
    _t48_83 = _mm256_sub_pd(_t51_59, _t48_67);
    _t48_84 = _mm256_sub_pd(_t51_60, _t48_68);
    _t48_85 = _mm256_sub_pd(_t51_61, _t48_69);

    // AVX Storer:

    // 4x4 -> 4x4 - UpTriang
    _t51_18 = _t48_82;
    _t51_19 = _t48_83;
    _t51_20 = _t48_84;
    _t51_21 = _t48_85;
    _asm256_storeu_pd(M3 + fi562 + 20*Max(0, fi562 - 4), _t51_22);
    _asm256_storeu_pd(M3 + fi562 + 20*Max(0, fi562 - 4) + 20, _t51_12);
    _asm256_storeu_pd(M3 + fi562 + 20*Max(0, fi562 - 4) + 40, _t51_13);
    _asm256_storeu_pd(M3 + fi562 + 20*Max(0, fi562 - 4) + 60, _t51_14);

    for( int k3 = 4; k3 <= fi562 - 1; k3+=4 ) {
      _t52_3 = _asm256_loadu_pd(M3 + fi562 + 20*k3);
      _t52_2 = _asm256_loadu_pd(M3 + fi562 + 20*k3 + 20);
      _t52_1 = _asm256_loadu_pd(M3 + fi562 + 20*k3 + 40);
      _t52_0 = _asm256_loadu_pd(M3 + fi562 + 20*k3 + 60);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_434 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_3, _t52_2), _mm256_unpacklo_pd(_t52_1, _t52_0), 32);
      _t48_435 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_3, _t52_2), _mm256_unpackhi_pd(_t52_1, _t52_0), 32);
      _t48_436 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_3, _t52_2), _mm256_unpacklo_pd(_t52_1, _t52_0), 49);
      _t48_437 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_3, _t52_2), _mm256_unpackhi_pd(_t52_1, _t52_0), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t48_70 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 32), _mm256_permute2f128_pd(_t48_434, _t48_434, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 32), _mm256_permute2f128_pd(_t48_434, _t48_434, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 49), _mm256_permute2f128_pd(_t48_434, _t48_434, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_434, _t48_434, 49), _mm256_permute2f128_pd(_t48_434, _t48_434, 49), 15), _t52_0)));
      _t48_71 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 32), _mm256_permute2f128_pd(_t48_435, _t48_435, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 32), _mm256_permute2f128_pd(_t48_435, _t48_435, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 49), _mm256_permute2f128_pd(_t48_435, _t48_435, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_435, _t48_435, 49), _mm256_permute2f128_pd(_t48_435, _t48_435, 49), 15), _t52_0)));
      _t48_72 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 32), _mm256_permute2f128_pd(_t48_436, _t48_436, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 32), _mm256_permute2f128_pd(_t48_436, _t48_436, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 49), _mm256_permute2f128_pd(_t48_436, _t48_436, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_436, _t48_436, 49), _mm256_permute2f128_pd(_t48_436, _t48_436, 49), 15), _t52_0)));
      _t48_73 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 32), _mm256_permute2f128_pd(_t48_437, _t48_437, 32), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 32), _mm256_permute2f128_pd(_t48_437, _t48_437, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 49), _mm256_permute2f128_pd(_t48_437, _t48_437, 49), 0), _t52_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_437, _t48_437, 49), _mm256_permute2f128_pd(_t48_437, _t48_437, 49), 15), _t52_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpperTriang
      _t52_4 = _t51_18;
      _t52_5 = _t51_19;
      _t52_6 = _t51_20;
      _t52_7 = _t51_21;

      // 4-BLAC: 4x4 - 4x4
      _t52_4 = _mm256_sub_pd(_t52_4, _t48_70);
      _t52_5 = _mm256_sub_pd(_t52_5, _t48_71);
      _t52_6 = _mm256_sub_pd(_t52_6, _t48_72);
      _t52_7 = _mm256_sub_pd(_t52_7, _t48_73);

      // AVX Storer:

      // 4x4 -> 4x4 - UpTriang
      _t51_18 = _t52_4;
      _t51_19 = _t52_5;
      _t51_20 = _t52_6;
      _t51_21 = _t52_7;
    }

    // Generating : U[20,20] = S(h(1, 20, fi562), Sqrt( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ),h(1, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_11 = _mm256_blend_pd(_mm256_setzero_pd(), _t51_18, 1);

    // 4-BLAC: sqrt(1x4)
    _t53_12 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_11)));

    // AVX Storer:
    _t53_0 = _t53_12;

    // Generating : T2152[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ),h(1, 20, fi562))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t53_13 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_14 = _t53_0;

    // 4-BLAC: 1x4 / 1x4
    _t53_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_13), _mm256_castpd256_pd128(_t53_14)));

    // AVX Storer:
    _t53_1 = _t53_15;

    // Generating : U[20,20] = S(h(1, 20, fi562), ( G(h(1, 1, 0), T2152[1,20],h(1, 20, fi562)) Kro G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ),h(2, 20, fi562 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_16 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_1, _t53_1, 32), _mm256_permute2f128_pd(_t53_1, _t53_1, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t53_17 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t51_18, 6), _mm256_permute2f128_pd(_t51_18, _t51_18, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t53_18 = _mm256_mul_pd(_t53_16, _t53_17);

    // AVX Storer:
    _t53_2 = _t53_18;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) - ( T( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) ),h(1, 20, fi562 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_19 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t51_19, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_20 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_2, 1);

    // 4-BLAC: (4x1)^T
    _t48_370 = _t53_20;

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_2, 1);

    // 4-BLAC: 1x4 Kro 1x4
    _t48_372 = _mm256_mul_pd(_t48_370, _t53_21);

    // 4-BLAC: 1x4 - 1x4
    _t53_22 = _mm256_sub_pd(_t53_19, _t48_372);

    // AVX Storer:
    _t53_3 = _t53_22;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 1), Sqrt( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_23 = _t53_3;

    // 4-BLAC: sqrt(1x4)
    _t53_24 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_23)));

    // AVX Storer:
    _t53_3 = _t53_24;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) - ( T( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ),h(1, 20, fi562 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_25 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t51_19, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t51_19, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_2, 1);

    // 4-BLAC: (4x1)^T
    _t48_378 = _t53_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_2, 2), _mm256_setzero_pd());

    // 4-BLAC: 1x4 Kro 1x4
    _t48_380 = _mm256_mul_pd(_t48_378, _t53_27);

    // 4-BLAC: 1x4 - 1x4
    _t53_28 = _mm256_sub_pd(_t53_25, _t48_380);

    // AVX Storer:
    _t53_4 = _t53_28;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) Div G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_29 = _t53_4;

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_30 = _t53_3;

    // 4-BLAC: 1x4 / 1x4
    _t53_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_29), _mm256_castpd256_pd128(_t53_30)));

    // AVX Storer:
    _t53_4 = _t53_31;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) - ( T( G(h(2, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) * G(h(2, 20, fi562), U[20,20],h(1, 20, fi562 + 2)) ) ),h(1, 20, fi562 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_32 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t51_20, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t51_20, 4), 129);

    // AVX Loader:

    // 2x1 -> 4x1
    _t53_33 = _mm256_shuffle_pd(_mm256_blend_pd(_t53_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t53_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (4x1)^T
    _t48_387 = _t53_33;

    // AVX Loader:

    // 2x1 -> 4x1
    _t53_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t53_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t53_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: 1x4 * 4x1
    _t48_389 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_387, _t53_34), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_387, _t53_34), _mm256_mul_pd(_t48_387, _t53_34), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_387, _t53_34), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_387, _t53_34), _mm256_mul_pd(_t48_387, _t53_34), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_387, _t53_34), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_387, _t53_34), _mm256_mul_pd(_t48_387, _t53_34), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t53_35 = _mm256_sub_pd(_t53_32, _t48_389);

    // AVX Storer:
    _t53_5 = _t53_35;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 2), Sqrt( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ),h(1, 20, fi562 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_36 = _t53_5;

    // 4-BLAC: sqrt(1x4)
    _t53_37 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_36)));

    // AVX Storer:
    _t53_5 = _t53_37;

    // Generating : U[20,20] = S(h(1, 20, fi562), ( G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) Div G(h(1, 20, fi562), U[20,20],h(1, 20, fi562)) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_38 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t51_18, _t51_18, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_39 = _t53_0;

    // 4-BLAC: 1x4 / 1x4
    _t53_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_38), _mm256_castpd256_pd128(_t53_39)));

    // AVX Storer:
    _t53_6 = _t53_40;

    // Generating : U[20,20] = S(h(2, 20, fi562 + 1), ( G(h(2, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) - ( T( G(h(1, 20, fi562), U[20,20],h(2, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 2x1 -> 4x1
    _t53_41 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t51_19, _t51_19, 129), _mm256_permute2f128_pd(_t51_20, _t51_20, 129));

    // AVX Loader:

    // 1x2 -> 1x4
    _t53_42 = _t53_2;

    // 4-BLAC: (1x4)^T
    _t48_398 = _t53_42;

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t53_6, _t53_6, 32), _mm256_permute2f128_pd(_t53_6, _t53_6, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t48_400 = _mm256_mul_pd(_t48_398, _t53_43);

    // 4-BLAC: 4x1 - 4x1
    _t53_44 = _mm256_sub_pd(_t53_41, _t48_400);

    // AVX Storer:
    _t53_7 = _t53_44;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) Div G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_45 = _mm256_blend_pd(_mm256_setzero_pd(), _t53_7, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_46 = _t53_3;

    // 4-BLAC: 1x4 / 1x4
    _t53_47 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_45), _mm256_castpd256_pd128(_t53_46)));

    // AVX Storer:
    _t53_8 = _t53_47;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) - ( T( G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 2)) ) Kro G(h(1, 20, fi562 + 1), U[20,20],h(1, 20, fi562 + 3)) ) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_48 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t53_7, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_49 = _t53_4;

    // 4-BLAC: (4x1)^T
    _t48_407 = _t53_49;

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_50 = _t53_8;

    // 4-BLAC: 1x4 Kro 1x4
    _t48_409 = _mm256_mul_pd(_t48_407, _t53_50);

    // 4-BLAC: 1x4 - 1x4
    _t53_51 = _mm256_sub_pd(_t53_48, _t48_409);

    // AVX Storer:
    _t53_9 = _t53_51;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 3)) Div G(h(1, 20, fi562 + 2), U[20,20],h(1, 20, fi562 + 2)) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_52 = _t53_9;

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_53 = _t53_5;

    // 4-BLAC: 1x4 / 1x4
    _t53_54 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t53_52), _mm256_castpd256_pd128(_t53_53)));

    // AVX Storer:
    _t53_9 = _t53_54;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 3), ( G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) - ( T( G(h(3, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) * G(h(3, 20, fi562), U[20,20],h(1, 20, fi562 + 3)) ) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_55 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t51_21, _t51_21, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 3x1 -> 4x1
    _t53_56 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_6, _t53_8), _mm256_unpacklo_pd(_t53_9, _mm256_setzero_pd()), 32);

    // 4-BLAC: (4x1)^T
    _t48_416 = _t53_56;

    // AVX Loader:

    // 3x1 -> 4x1
    _t53_57 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_6, _t53_8), _mm256_unpacklo_pd(_t53_9, _mm256_setzero_pd()), 32);

    // 4-BLAC: 1x4 * 4x1
    _t48_418 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_416, _t53_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_416, _t53_57), _mm256_mul_pd(_t48_416, _t53_57), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_416, _t53_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_416, _t53_57), _mm256_mul_pd(_t48_416, _t53_57), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_416, _t53_57), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_416, _t53_57), _mm256_mul_pd(_t48_416, _t53_57), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t53_58 = _mm256_sub_pd(_t53_55, _t48_418);

    // AVX Storer:
    _t53_10 = _t53_58;

    // Generating : U[20,20] = S(h(1, 20, fi562 + 3), Sqrt( G(h(1, 20, fi562 + 3), U[20,20],h(1, 20, fi562 + 3)) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_59 = _t53_10;

    // 4-BLAC: sqrt(1x4)
    _t53_60 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t53_59)));

    // AVX Storer:
    _t53_10 = _t53_60;
    _mm_store_sd(&(M3[21*fi562]), _mm256_castpd256_pd128(_t53_0));
    _mm256_maskstore_pd(M3 + 21*fi562 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t53_2);
    _mm_store_sd(&(M3[21*fi562 + 21]), _mm256_castpd256_pd128(_t53_3));
    _mm_store_sd(&(M3[21*fi562 + 22]), _mm256_castpd256_pd128(_t53_4));
    _mm_store_sd(&(M3[21*fi562 + 42]), _mm256_castpd256_pd128(_t53_5));
    _mm_store_sd(&(M3[21*fi562 + 3]), _mm256_castpd256_pd128(_t53_6));
    _mm_store_sd(&(M3[21*fi562 + 23]), _mm256_castpd256_pd128(_t53_8));
    _mm_store_sd(&(M3[21*fi562 + 43]), _mm256_castpd256_pd128(_t53_9));
    _mm_store_sd(&(M3[21*fi562 + 63]), _mm256_castpd256_pd128(_t53_10));
  }


  for( int fi562 = 0; fi562 <= 15; fi562+=4 ) {
    _t54_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi562])));
    _t54_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562])));
    _t54_8 = _mm256_maskload_pd(v0 + fi562 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t54_5 = _mm256_maskload_pd(M3 + 21*fi562 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t54_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 21])));
    _t54_3 = _mm256_maskload_pd(M3 + 21*fi562 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t54_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 42])));
    _t54_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 43])));
    _t54_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 63])));

    // Generating : v2[20,1] = S(h(1, 20, fi562), ( G(h(1, 20, fi562), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi562), U0[20,20],h(1, 20, fi562)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_40 = _t54_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_41 = _t54_6;

    // 4-BLAC: 1x4 / 1x4
    _t54_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_40), _mm256_castpd256_pd128(_t54_41)));

    // AVX Storer:
    _t54_7 = _t54_42;

    // Generating : v2[20,1] = S(h(3, 20, fi562 + 1), ( G(h(3, 20, fi562 + 1), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, fi562), U0[20,20],h(3, 20, fi562 + 1)) ) Kro G(h(1, 20, fi562), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t54_13 = _t54_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t54_14 = _t54_5;

    // 4-BLAC: (1x4)^T
    _t54_15 = _t54_14;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_16 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_7, _t54_7, 32), _mm256_permute2f128_pd(_t54_7, _t54_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t54_17 = _mm256_mul_pd(_t54_15, _t54_16);

    // 4-BLAC: 4x1 - 4x1
    _t54_18 = _mm256_sub_pd(_t54_13, _t54_17);

    // AVX Storer:
    _t54_8 = _t54_18;

    // Generating : v2[20,1] = S(h(1, 20, fi562 + 1), ( G(h(1, 20, fi562 + 1), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi562 + 1), U0[20,20],h(1, 20, fi562 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_19 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_20 = _t54_4;

    // 4-BLAC: 1x4 / 1x4
    _t54_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_19), _mm256_castpd256_pd128(_t54_20)));

    // AVX Storer:
    _t54_9 = _t54_21;

    // Generating : v2[20,1] = S(h(2, 20, fi562 + 2), ( G(h(2, 20, fi562 + 2), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, fi562 + 1), U0[20,20],h(2, 20, fi562 + 2)) ) Kro G(h(1, 20, fi562 + 1), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t54_22 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_8, 6), _mm256_permute2f128_pd(_t54_8, _t54_8, 129), 5);

    // AVX Loader:

    // 1x2 -> 1x4
    _t54_23 = _t54_3;

    // 4-BLAC: (1x4)^T
    _t54_24 = _t54_23;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t54_26 = _mm256_mul_pd(_t54_24, _t54_25);

    // 4-BLAC: 4x1 - 4x1
    _t54_27 = _mm256_sub_pd(_t54_22, _t54_26);

    // AVX Storer:
    _t54_10 = _t54_27;

    // Generating : v2[20,1] = S(h(1, 20, fi562 + 2), ( G(h(1, 20, fi562 + 2), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi562 + 2), U0[20,20],h(1, 20, fi562 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_28 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_29 = _t54_2;

    // 4-BLAC: 1x4 / 1x4
    _t54_30 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_28), _mm256_castpd256_pd128(_t54_29)));

    // AVX Storer:
    _t54_11 = _t54_30;

    // Generating : v2[20,1] = S(h(1, 20, fi562 + 3), ( G(h(1, 20, fi562 + 3), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, fi562 + 2), U0[20,20],h(1, 20, fi562 + 3)) ) Kro G(h(1, 20, fi562 + 2), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_31 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_32 = _t54_1;

    // 4-BLAC: (4x1)^T
    _t54_33 = _t54_32;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_34 = _t54_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t54_35 = _mm256_mul_pd(_t54_33, _t54_34);

    // 4-BLAC: 1x4 - 1x4
    _t54_36 = _mm256_sub_pd(_t54_31, _t54_35);

    // AVX Storer:
    _t54_12 = _t54_36;

    // Generating : v2[20,1] = S(h(1, 20, fi562 + 3), ( G(h(1, 20, fi562 + 3), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi562 + 3), U0[20,20],h(1, 20, fi562 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_37 = _t54_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t54_38 = _t54_0;

    // 4-BLAC: 1x4 / 1x4
    _t54_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_37), _mm256_castpd256_pd128(_t54_38)));

    // AVX Storer:
    _t54_12 = _t54_39;

    // Generating : v2[20,1] = Sum_{k3} ( S(h(4, 20, fi562 + k3 + 4), ( G(h(4, 20, fi562 + k3 + 4), v2[20,1],h(1, 1, 0)) - ( T( G(h(4, 20, fi562), U0[20,20],h(4, 20, fi562 + k3 + 4)) ) * G(h(4, 20, fi562), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[fi562]), _mm256_castpd256_pd128(_t54_7));
    _mm_store_sd(&(v0[fi562 + 1]), _mm256_castpd256_pd128(_t54_9));
    _mm_store_sd(&(v0[fi562 + 2]), _mm256_castpd256_pd128(_t54_11));
    _mm_store_sd(&(v0[fi562 + 3]), _mm256_castpd256_pd128(_t54_12));

    for( int k3 = 0; k3 <= -fi562 + 15; k3+=4 ) {
      _t55_9 = _asm256_loadu_pd(v0 + fi562 + k3 + 4);
      _t55_7 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 4);
      _t55_6 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 24);
      _t55_5 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 44);
      _t55_4 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 64);
      _t55_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi562])));
      _t55_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi562 + 1])));
      _t55_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi562 + 2])));
      _t55_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi562 + 3])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t55_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_7, _t55_6), _mm256_unpacklo_pd(_t55_5, _t55_4), 32);
      _t55_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_7, _t55_6), _mm256_unpackhi_pd(_t55_5, _t55_4), 32);
      _t55_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_7, _t55_6), _mm256_unpacklo_pd(_t55_5, _t55_4), 49);
      _t55_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_7, _t55_6), _mm256_unpackhi_pd(_t55_5, _t55_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t55_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t55_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t55_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t55_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t55_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32)), _mm256_mul_pd(_t55_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t55_9 = _mm256_sub_pd(_t55_9, _t55_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + fi562 + k3 + 4, _t55_9);
    }
  }

  _t44_28 = _asm256_loadu_pd(M3 + 336);
  _t44_31 = _mm256_maskload_pd(M3 + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t44_30 = _mm256_maskload_pd(M3 + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t44_29 = _mm256_maskload_pd(M3 + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t56_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[16])));
  _t56_1 = _mm256_maskload_pd(v0 + 17, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v2[20,1] = S(h(1, 20, 16), ( G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_6 = _t56_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_7 = _mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 1);

  // 4-BLAC: 1x4 / 1x4
  _t56_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_6), _mm256_castpd256_pd128(_t56_7)));

  // AVX Storer:
  _t56_0 = _t56_8;

  // Generating : v2[20,1] = S(h(3, 20, 17), ( G(h(3, 20, 17), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) Kro G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_9 = _t56_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_10 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 14), _mm256_permute2f128_pd(_t44_28, _t44_28, 129), 5);

  // 4-BLAC: (1x4)^T
  _t56_11 = _t56_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t56_13 = _mm256_mul_pd(_t56_11, _t56_12);

  // 4-BLAC: 4x1 - 4x1
  _t56_14 = _mm256_sub_pd(_t56_9, _t56_13);

  // AVX Storer:
  _t56_1 = _t56_14;

  // Generating : v2[20,1] = S(h(1, 20, 17), ( G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_15 = _mm256_blend_pd(_mm256_setzero_pd(), _t56_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_16 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_29, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 / 1x4
  _t56_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_15), _mm256_castpd256_pd128(_t56_16)));

  // AVX Storer:
  _t56_2 = _t56_17;

  // Generating : v2[20,1] = S(h(2, 20, 18), ( G(h(2, 20, 18), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) Kro G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_18 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t56_1, 6), _mm256_permute2f128_pd(_t56_1, _t56_1, 129), 5);

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_19 = _mm256_permute2f128_pd(_t44_29, _t44_29, 129);

  // 4-BLAC: (1x4)^T
  _t56_20 = _t56_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t56_22 = _mm256_mul_pd(_t56_20, _t56_21);

  // 4-BLAC: 4x1 - 4x1
  _t56_23 = _mm256_sub_pd(_t56_18, _t56_22);

  // AVX Storer:
  _t56_3 = _t56_23;

  // Generating : v2[20,1] = S(h(1, 20, 18), ( G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t56_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_25 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), 129);

  // 4-BLAC: 1x4 / 1x4
  _t56_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_24), _mm256_castpd256_pd128(_t56_25)));

  // AVX Storer:
  _t56_4 = _t56_26;

  // Generating : v2[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) Kro G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t56_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_28 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_30, _t44_30, 129), _mm256_setzero_pd());

  // 4-BLAC: (4x1)^T
  _t56_29 = _t56_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_30 = _t56_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_31 = _mm256_mul_pd(_t56_29, _t56_30);

  // 4-BLAC: 1x4 - 1x4
  _t56_32 = _mm256_sub_pd(_t56_27, _t56_31);

  // AVX Storer:
  _t56_5 = _t56_32;

  // Generating : v2[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_33 = _t56_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_34 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_31, _t44_31, 129), _mm256_setzero_pd());

  // 4-BLAC: 1x4 / 1x4
  _t56_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t56_33), _mm256_castpd256_pd128(_t56_34)));

  // AVX Storer:
  _t56_5 = _t56_35;

  _mm_store_sd(&(v0[16]), _mm256_castpd256_pd128(_t56_0));
  _mm_store_sd(&(v0[17]), _mm256_castpd256_pd128(_t56_2));
  _mm_store_sd(&(v0[18]), _mm256_castpd256_pd128(_t56_4));
  _mm_store_sd(&(v0[19]), _mm256_castpd256_pd128(_t56_5));

  for( int fi562 = 0; fi562 <= 15; fi562+=4 ) {
    _t57_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi562 + 19])));
    _t57_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 399])));
    _t57_8 = _mm256_maskload_pd(v0 + -fi562 + 16, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t57_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 339)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 359))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 379)), 32);
    _t57_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 378])));
    _t57_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 338)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 358)), 0);
    _t57_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 357])));
    _t57_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 337])));
    _t57_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 336])));

    // Generating : v4[20,1] = S(h(1, 20, -fi562 + 19), ( G(h(1, 20, -fi562 + 19), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 19), U0[20,20],h(1, 20, -fi562 + 19)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_13 = _t57_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_14 = _t57_6;

    // 4-BLAC: 1x4 / 1x4
    _t57_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_13), _mm256_castpd256_pd128(_t57_14)));

    // AVX Storer:
    _t57_7 = _t57_15;

    // Generating : v4[20,1] = S(h(3, 20, -fi562 + 16), ( G(h(3, 20, -fi562 + 16), v4[20,1],h(1, 1, 0)) - ( G(h(3, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 19)) Kro G(h(1, 20, -fi562 + 19), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t57_16 = _t57_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t57_17 = _t57_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_7, _t57_7, 32), _mm256_permute2f128_pd(_t57_7, _t57_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t57_19 = _mm256_mul_pd(_t57_17, _t57_18);

    // 4-BLAC: 4x1 - 4x1
    _t57_20 = _mm256_sub_pd(_t57_16, _t57_19);

    // AVX Storer:
    _t57_8 = _t57_20;

    // Generating : v4[20,1] = S(h(1, 20, -fi562 + 18), ( G(h(1, 20, -fi562 + 18), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 18), U0[20,20],h(1, 20, -fi562 + 18)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t57_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t57_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_22 = _t57_4;

    // 4-BLAC: 1x4 / 1x4
    _t57_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_21), _mm256_castpd256_pd128(_t57_22)));

    // AVX Storer:
    _t57_9 = _t57_23;

    // Generating : v4[20,1] = S(h(2, 20, -fi562 + 16), ( G(h(2, 20, -fi562 + 16), v4[20,1],h(1, 1, 0)) - ( G(h(2, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 18)) Kro G(h(1, 20, -fi562 + 18), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t57_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t57_8, 3);

    // AVX Loader:

    // 2x1 -> 4x1
    _t57_25 = _t57_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t57_9, _t57_9, 32), _mm256_permute2f128_pd(_t57_9, _t57_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t57_27 = _mm256_mul_pd(_t57_25, _t57_26);

    // 4-BLAC: 4x1 - 4x1
    _t57_28 = _mm256_sub_pd(_t57_24, _t57_27);

    // AVX Storer:
    _t57_10 = _t57_28;

    // Generating : v4[20,1] = S(h(1, 20, -fi562 + 17), ( G(h(1, 20, -fi562 + 17), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 17), U0[20,20],h(1, 20, -fi562 + 17)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t57_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_30 = _t57_2;

    // 4-BLAC: 1x4 / 1x4
    _t57_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_29), _mm256_castpd256_pd128(_t57_30)));

    // AVX Storer:
    _t57_11 = _t57_31;

    // Generating : v4[20,1] = S(h(1, 20, -fi562 + 16), ( G(h(1, 20, -fi562 + 16), v4[20,1],h(1, 1, 0)) - ( G(h(1, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 17)) Kro G(h(1, 20, -fi562 + 17), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_32 = _mm256_blend_pd(_mm256_setzero_pd(), _t57_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_33 = _t57_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_34 = _t57_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t57_35 = _mm256_mul_pd(_t57_33, _t57_34);

    // 4-BLAC: 1x4 - 1x4
    _t57_36 = _mm256_sub_pd(_t57_32, _t57_35);

    // AVX Storer:
    _t57_12 = _t57_36;

    // Generating : v4[20,1] = S(h(1, 20, -fi562 + 16), ( G(h(1, 20, -fi562 + 16), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 16)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_37 = _t57_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_38 = _t57_0;

    // 4-BLAC: 1x4 / 1x4
    _t57_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t57_37), _mm256_castpd256_pd128(_t57_38)));

    // AVX Storer:
    _t57_12 = _t57_39;

    // Generating : v4[20,1] = Sum_{k3} ( S(h(4, 20, k3), ( G(h(4, 20, k3), v4[20,1],h(1, 1, 0)) - ( G(h(4, 20, k3), U0[20,20],h(4, 20, -fi562 + 16)) * G(h(4, 20, -fi562 + 16), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[-fi562 + 19]), _mm256_castpd256_pd128(_t57_7));
    _mm_store_sd(&(v0[-fi562 + 18]), _mm256_castpd256_pd128(_t57_9));
    _mm_store_sd(&(v0[-fi562 + 17]), _mm256_castpd256_pd128(_t57_11));
    _mm_store_sd(&(v0[-fi562 + 16]), _mm256_castpd256_pd128(_t57_12));

    for( int k3 = 0; k3 <= -fi562 + 15; k3+=4 ) {
      _t58_9 = _asm256_loadu_pd(v0 + k3);
      _t58_7 = _asm256_loadu_pd(M3 + -fi562 + 20*k3 + 16);
      _t58_6 = _asm256_loadu_pd(M3 + -fi562 + 20*k3 + 36);
      _t58_5 = _asm256_loadu_pd(M3 + -fi562 + 20*k3 + 56);
      _t58_4 = _asm256_loadu_pd(M3 + -fi562 + 20*k3 + 76);
      _t58_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi562 + 19])));
      _t58_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi562 + 18])));
      _t58_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi562 + 17])));
      _t58_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi562 + 16])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t58_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t58_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t58_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t58_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t58_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32)), _mm256_mul_pd(_t58_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_0, _t58_1), _mm256_unpacklo_pd(_t58_2, _t58_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t58_9 = _mm256_sub_pd(_t58_9, _t58_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k3, _t58_9);
    }
  }

  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[3])));
  _t48_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[22])));
  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_9 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[43])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21])));
  _t48_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[42])));
  _t48_10 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[63])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[23])));
  _t59_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[3])));
  _t59_1 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v4[20,1] = S(h(1, 20, 3), ( G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_6 = _t59_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_7 = _t48_10;

  // 4-BLAC: 1x4 / 1x4
  _t59_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_6), _mm256_castpd256_pd128(_t59_7)));

  // AVX Storer:
  _t59_0 = _t59_8;

  // Generating : v4[20,1] = S(h(3, 20, 0), ( G(h(3, 20, 0), v4[20,1],h(1, 1, 0)) - ( G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) Kro G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t59_9 = _t59_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t59_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_6, _t48_8), _mm256_unpacklo_pd(_t48_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t59_0, _t59_0, 32), _mm256_permute2f128_pd(_t59_0, _t59_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t59_12 = _mm256_mul_pd(_t59_10, _t59_11);

  // 4-BLAC: 4x1 - 4x1
  _t59_13 = _mm256_sub_pd(_t59_9, _t59_12);

  // AVX Storer:
  _t59_1 = _t59_13;

  // Generating : v4[20,1] = S(h(1, 20, 2), ( G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_14 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t59_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t59_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_15 = _t48_5;

  // 4-BLAC: 1x4 / 1x4
  _t59_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_14), _mm256_castpd256_pd128(_t59_15)));

  // AVX Storer:
  _t59_2 = _t59_16;

  // Generating : v4[20,1] = S(h(2, 20, 0), ( G(h(2, 20, 0), v4[20,1],h(1, 1, 0)) - ( G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) Kro G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t59_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t59_1, 3);

  // AVX Loader:

  // 2x1 -> 4x1
  _t59_18 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t59_2, _t59_2, 32), _mm256_permute2f128_pd(_t59_2, _t59_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t59_20 = _mm256_mul_pd(_t59_18, _t59_19);

  // 4-BLAC: 4x1 - 4x1
  _t59_21 = _mm256_sub_pd(_t59_17, _t59_20);

  // AVX Storer:
  _t59_3 = _t59_21;

  // Generating : v4[20,1] = S(h(1, 20, 1), ( G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_22 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t59_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_23 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t59_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_22), _mm256_castpd256_pd128(_t59_23)));

  // AVX Storer:
  _t59_4 = _t59_24;

  // Generating : v4[20,1] = S(h(1, 20, 0), ( G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) - ( G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) Kro G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t59_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_27 = _t59_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t59_28 = _mm256_mul_pd(_t59_26, _t59_27);

  // 4-BLAC: 1x4 - 1x4
  _t59_29 = _mm256_sub_pd(_t59_25, _t59_28);

  // AVX Storer:
  _t59_5 = _t59_29;

  // Generating : v4[20,1] = S(h(1, 20, 0), ( G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_30 = _t59_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t59_31 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t59_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t59_30), _mm256_castpd256_pd128(_t59_31)));

  // AVX Storer:
  _t59_5 = _t59_32;


  for( int fi562 = 0; fi562 <= 15; fi562+=4 ) {
    _t60_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562])));
    _t60_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 21])));
    _t60_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 42])));
    _t60_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi562 + 63])));
    _t60_14 = _asm256_loadu_pd(M1 + 20*fi562);
    _t60_11 = _asm256_loadu_pd(M1 + 20*fi562 + 20);
    _t60_12 = _asm256_loadu_pd(M1 + 20*fi562 + 40);
    _t60_13 = _asm256_loadu_pd(M1 + 20*fi562 + 60);
    _t60_2 = _mm256_maskload_pd(M3 + 21*fi562 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t60_1 = _mm256_maskload_pd(M3 + 21*fi562 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t60_0 = _mm256_broadcast_sd(&(M3[21*fi562 + 43]));

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi562), U0[20,20],h(1, 20, fi562)) ),h(1, 20, fi562))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_17 = _t60_6;

    // 4-BLAC: 1x4 / 1x4
    _t60_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_16), _mm256_castpd256_pd128(_t60_17)));

    // AVX Storer:
    _t60_7 = _t60_18;

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi562 + 1), U0[20,20],h(1, 20, fi562 + 1)) ),h(1, 20, fi562 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_20 = _t60_5;

    // 4-BLAC: 1x4 / 1x4
    _t60_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_19), _mm256_castpd256_pd128(_t60_20)));

    // AVX Storer:
    _t60_8 = _t60_21;

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi562 + 2), U0[20,20],h(1, 20, fi562 + 2)) ),h(1, 20, fi562 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_23 = _t60_4;

    // 4-BLAC: 1x4 / 1x4
    _t60_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_22), _mm256_castpd256_pd128(_t60_23)));

    // AVX Storer:
    _t60_9 = _t60_24;

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi562 + 3), U0[20,20],h(1, 20, fi562 + 3)) ),h(1, 20, fi562 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t60_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_26 = _t60_3;

    // 4-BLAC: 1x4 / 1x4
    _t60_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_25), _mm256_castpd256_pd128(_t60_26)));

    // AVX Storer:
    _t60_10 = _t60_27;

    // Generating : M6[20,20] = S(h(1, 20, fi562), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562)) Kro G(h(1, 20, fi562), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_7, _t60_7, 32), _mm256_permute2f128_pd(_t60_7, _t60_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_14 = _mm256_mul_pd(_t60_28, _t60_14);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(3, 20, fi562 + 1), ( G(h(3, 20, fi562 + 1), M6[20,20],h(4, 20, fi641)) - ( T( G(h(1, 20, fi562), U0[20,20],h(3, 20, fi562 + 1)) ) * G(h(1, 20, fi562), M6[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

    // AVX Loader:

    // 3x4 -> 4x4
    _t60_29 = _t60_11;
    _t60_30 = _t60_12;
    _t60_31 = _t60_13;
    _t60_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t60_33 = _t60_2;

    // 4-BLAC: (1x4)^T
    _t60_34 = _t60_33;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 0), _t60_14);
    _t60_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 15), _t60_14);
    _t60_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 0), _t60_14);
    _t60_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 15), _t60_14);

    // 4-BLAC: 4x4 - 4x4
    _t60_39 = _mm256_sub_pd(_t60_29, _t60_35);
    _t60_40 = _mm256_sub_pd(_t60_30, _t60_36);
    _t60_41 = _mm256_sub_pd(_t60_31, _t60_37);
    _t60_42 = _mm256_sub_pd(_t60_32, _t60_38);

    // AVX Storer:
    _t60_11 = _t60_39;
    _t60_12 = _t60_40;
    _t60_13 = _t60_41;

    // Generating : M6[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562 + 1)) Kro G(h(1, 20, fi562 + 1), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_8, _t60_8, 32), _mm256_permute2f128_pd(_t60_8, _t60_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_11 = _mm256_mul_pd(_t60_43, _t60_11);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(2, 20, fi562 + 2), ( G(h(2, 20, fi562 + 2), M6[20,20],h(4, 20, fi641)) - ( T( G(h(1, 20, fi562 + 1), U0[20,20],h(2, 20, fi562 + 2)) ) * G(h(1, 20, fi562 + 1), M6[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

    // AVX Loader:

    // 2x4 -> 4x4
    _t60_44 = _t60_12;
    _t60_45 = _t60_13;
    _t60_46 = _mm256_setzero_pd();
    _t60_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t60_48 = _t60_1;

    // 4-BLAC: (1x4)^T
    _t60_49 = _t60_48;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 0), _t60_11);
    _t60_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 15), _t60_11);
    _t60_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 0), _t60_11);
    _t60_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 15), _t60_11);

    // 4-BLAC: 4x4 - 4x4
    _t60_54 = _mm256_sub_pd(_t60_44, _t60_50);
    _t60_55 = _mm256_sub_pd(_t60_45, _t60_51);
    _t60_56 = _mm256_sub_pd(_t60_46, _t60_52);
    _t60_57 = _mm256_sub_pd(_t60_47, _t60_53);

    // AVX Storer:
    _t60_12 = _t60_54;
    _t60_13 = _t60_55;

    // Generating : M6[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562 + 2)) Kro G(h(1, 20, fi562 + 2), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_9, _t60_9, 32), _mm256_permute2f128_pd(_t60_9, _t60_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_12 = _mm256_mul_pd(_t60_58, _t60_12);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, fi562 + 3), ( G(h(1, 20, fi562 + 3), M6[20,20],h(4, 20, fi641)) - ( T( G(h(1, 20, fi562 + 2), U0[20,20],h(1, 20, fi562 + 3)) ) Kro G(h(1, 20, fi562 + 2), M6[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_59 = _t60_0;

    // 4-BLAC: (4x1)^T
    _t60_60 = _t60_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_15 = _mm256_mul_pd(_t60_60, _t60_12);

    // 4-BLAC: 1x4 - 1x4
    _t60_13 = _mm256_sub_pd(_t60_13, _t60_15);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, fi562 + 3), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562 + 3)) Kro G(h(1, 20, fi562 + 3), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t60_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_10, _t60_10, 32), _mm256_permute2f128_pd(_t60_10, _t60_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_13 = _mm256_mul_pd(_t60_61, _t60_13);

    // AVX Storer:

    for( int fi641 = 4; fi641 <= 16; fi641+=4 ) {
      _t61_3 = _asm256_loadu_pd(M1 + 20*fi562 + fi641);
      _t61_0 = _asm256_loadu_pd(M1 + 20*fi562 + fi641 + 20);
      _t61_1 = _asm256_loadu_pd(M1 + 20*fi562 + fi641 + 40);
      _t61_2 = _asm256_loadu_pd(M1 + 20*fi562 + fi641 + 60);

      // Generating : M6[20,20] = S(h(1, 20, fi562), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562)) Kro G(h(1, 20, fi562), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_7, _t60_7, 32), _mm256_permute2f128_pd(_t60_7, _t60_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_3 = _mm256_mul_pd(_t61_4, _t61_3);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(3, 20, fi562 + 1), ( G(h(3, 20, fi562 + 1), M6[20,20],h(4, 20, fi641)) - ( T( G(h(1, 20, fi562), U0[20,20],h(3, 20, fi562 + 1)) ) * G(h(1, 20, fi562), M6[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

      // AVX Loader:

      // 3x4 -> 4x4
      _t61_5 = _t61_0;
      _t61_6 = _t61_1;
      _t61_7 = _t61_2;
      _t61_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t61_9 = _t60_2;

      // 4-BLAC: (1x4)^T
      _t60_34 = _t61_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t60_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 0), _t61_3);
      _t60_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 32), _mm256_permute2f128_pd(_t60_34, _t60_34, 32), 15), _t61_3);
      _t60_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 0), _t61_3);
      _t60_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_34, _t60_34, 49), _mm256_permute2f128_pd(_t60_34, _t60_34, 49), 15), _t61_3);

      // 4-BLAC: 4x4 - 4x4
      _t61_10 = _mm256_sub_pd(_t61_5, _t60_35);
      _t61_11 = _mm256_sub_pd(_t61_6, _t60_36);
      _t61_12 = _mm256_sub_pd(_t61_7, _t60_37);
      _t61_13 = _mm256_sub_pd(_t61_8, _t60_38);

      // AVX Storer:
      _t61_0 = _t61_10;
      _t61_1 = _t61_11;
      _t61_2 = _t61_12;

      // Generating : M6[20,20] = S(h(1, 20, fi562 + 1), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562 + 1)) Kro G(h(1, 20, fi562 + 1), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_8, _t60_8, 32), _mm256_permute2f128_pd(_t60_8, _t60_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_0 = _mm256_mul_pd(_t61_14, _t61_0);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(2, 20, fi562 + 2), ( G(h(2, 20, fi562 + 2), M6[20,20],h(4, 20, fi641)) - ( T( G(h(1, 20, fi562 + 1), U0[20,20],h(2, 20, fi562 + 2)) ) * G(h(1, 20, fi562 + 1), M6[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

      // AVX Loader:

      // 2x4 -> 4x4
      _t61_15 = _t61_1;
      _t61_16 = _t61_2;
      _t61_17 = _mm256_setzero_pd();
      _t61_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t61_19 = _t60_1;

      // 4-BLAC: (1x4)^T
      _t60_49 = _t61_19;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t60_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 0), _t61_0);
      _t60_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 32), _mm256_permute2f128_pd(_t60_49, _t60_49, 32), 15), _t61_0);
      _t60_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 0), _t61_0);
      _t60_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_49, _t60_49, 49), _mm256_permute2f128_pd(_t60_49, _t60_49, 49), 15), _t61_0);

      // 4-BLAC: 4x4 - 4x4
      _t61_20 = _mm256_sub_pd(_t61_15, _t60_50);
      _t61_21 = _mm256_sub_pd(_t61_16, _t60_51);
      _t61_22 = _mm256_sub_pd(_t61_17, _t60_52);
      _t61_23 = _mm256_sub_pd(_t61_18, _t60_53);

      // AVX Storer:
      _t61_1 = _t61_20;
      _t61_2 = _t61_21;

      // Generating : M6[20,20] = S(h(1, 20, fi562 + 2), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562 + 2)) Kro G(h(1, 20, fi562 + 2), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_9, _t60_9, 32), _mm256_permute2f128_pd(_t60_9, _t60_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_1 = _mm256_mul_pd(_t61_24, _t61_1);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(1, 20, fi562 + 3), ( G(h(1, 20, fi562 + 3), M6[20,20],h(4, 20, fi641)) - ( T( G(h(1, 20, fi562 + 2), U0[20,20],h(1, 20, fi562 + 3)) ) Kro G(h(1, 20, fi562 + 2), M6[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_25 = _t60_0;

      // 4-BLAC: (4x1)^T
      _t60_60 = _t61_25;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t60_15 = _mm256_mul_pd(_t60_60, _t61_1);

      // 4-BLAC: 1x4 - 1x4
      _t61_2 = _mm256_sub_pd(_t61_2, _t60_15);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(1, 20, fi562 + 3), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, fi562 + 3)) Kro G(h(1, 20, fi562 + 3), M6[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t61_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_10, _t60_10, 32), _mm256_permute2f128_pd(_t60_10, _t60_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t61_2 = _mm256_mul_pd(_t61_26, _t61_2);

      // AVX Storer:
      _asm256_storeu_pd(M1 + 20*fi562 + fi641, _t61_3);
      _asm256_storeu_pd(M1 + 20*fi562 + fi641 + 20, _t61_0);
      _asm256_storeu_pd(M1 + 20*fi562 + fi641 + 40, _t61_1);
      _asm256_storeu_pd(M1 + 20*fi562 + fi641 + 60, _t61_2);
    }

    // Generating : M6[20,20] = Sum_{k3} ( Sum_{k2} ( S(h(4, 20, fi562 + k3 + 4), ( G(h(4, 20, fi562 + k3 + 4), M6[20,20],h(4, 20, k2)) - ( T( G(h(4, 20, fi562), U0[20,20],h(4, 20, fi562 + k3 + 4)) ) * G(h(4, 20, fi562), M6[20,20],h(4, 20, k2)) ) ),h(4, 20, k2)) ) )
    _asm256_storeu_pd(M1 + 20*fi562, _t60_14);
    _asm256_storeu_pd(M1 + 20*fi562 + 20, _t60_11);
    _asm256_storeu_pd(M1 + 20*fi562 + 40, _t60_12);
    _asm256_storeu_pd(M1 + 20*fi562 + 60, _t60_13);

    for( int k3 = 0; k3 <= -fi562 + 15; k3+=4 ) {

      for( int k2 = 0; k2 <= 19; k2+=4 ) {
        _t62_12 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 20*k3 + 80);
        _t62_13 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 20*k3 + 100);
        _t62_14 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 20*k3 + 120);
        _t62_15 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 20*k3 + 140);
        _t62_7 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 4);
        _t62_6 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 24);
        _t62_5 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 44);
        _t62_4 = _asm256_loadu_pd(M3 + 21*fi562 + k3 + 64);
        _t62_3 = _asm256_loadu_pd(M1 + 20*fi562 + k2);
        _t62_2 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 20);
        _t62_1 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 40);
        _t62_0 = _asm256_loadu_pd(M1 + 20*fi562 + k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t62_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t62_7, _t62_6), _mm256_unpacklo_pd(_t62_5, _t62_4), 32);
        _t62_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t62_7, _t62_6), _mm256_unpackhi_pd(_t62_5, _t62_4), 32);
        _t62_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t62_7, _t62_6), _mm256_unpacklo_pd(_t62_5, _t62_4), 49);
        _t62_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t62_7, _t62_6), _mm256_unpackhi_pd(_t62_5, _t62_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t62_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 32), _mm256_permute2f128_pd(_t62_16, _t62_16, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 32), _mm256_permute2f128_pd(_t62_16, _t62_16, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 49), _mm256_permute2f128_pd(_t62_16, _t62_16, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_16, _t62_16, 49), _mm256_permute2f128_pd(_t62_16, _t62_16, 49), 15), _t62_0)));
        _t62_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 32), _mm256_permute2f128_pd(_t62_17, _t62_17, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 32), _mm256_permute2f128_pd(_t62_17, _t62_17, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 49), _mm256_permute2f128_pd(_t62_17, _t62_17, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_17, _t62_17, 49), _mm256_permute2f128_pd(_t62_17, _t62_17, 49), 15), _t62_0)));
        _t62_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 32), _mm256_permute2f128_pd(_t62_18, _t62_18, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 32), _mm256_permute2f128_pd(_t62_18, _t62_18, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 49), _mm256_permute2f128_pd(_t62_18, _t62_18, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_18, _t62_18, 49), _mm256_permute2f128_pd(_t62_18, _t62_18, 49), 15), _t62_0)));
        _t62_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 32), _mm256_permute2f128_pd(_t62_19, _t62_19, 32), 0), _t62_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 32), _mm256_permute2f128_pd(_t62_19, _t62_19, 32), 15), _t62_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 49), _mm256_permute2f128_pd(_t62_19, _t62_19, 49), 0), _t62_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_19, _t62_19, 49), _mm256_permute2f128_pd(_t62_19, _t62_19, 49), 15), _t62_0)));

        // 4-BLAC: 4x4 - 4x4
        _t62_12 = _mm256_sub_pd(_t62_12, _t62_8);
        _t62_13 = _mm256_sub_pd(_t62_13, _t62_9);
        _t62_14 = _mm256_sub_pd(_t62_14, _t62_10);
        _t62_15 = _mm256_sub_pd(_t62_15, _t62_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + 20*fi562 + k2 + 20*k3 + 80, _t62_12);
        _asm256_storeu_pd(M1 + 20*fi562 + k2 + 20*k3 + 100, _t62_13);
        _asm256_storeu_pd(M1 + 20*fi562 + k2 + 20*k3 + 120, _t62_14);
        _asm256_storeu_pd(M1 + 20*fi562 + k2 + 20*k3 + 140, _t62_15);
      }
    }
  }

  _t44_28 = _asm256_loadu_pd(M3 + 336);
  _t44_31 = _mm256_maskload_pd(M3 + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t44_30 = _mm256_maskload_pd(M3 + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t44_29 = _mm256_maskload_pd(M3 + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t63_7 = _asm256_loadu_pd(M1 + 320);
  _t63_4 = _asm256_loadu_pd(M1 + 340);
  _t63_5 = _asm256_loadu_pd(M1 + 360);
  _t63_6 = _asm256_loadu_pd(M1 + 380);

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_10 = _mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 1);

  // 4-BLAC: 1x4 / 1x4
  _t63_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_9), _mm256_castpd256_pd128(_t63_10)));

  // AVX Storer:
  _t63_0 = _t63_11;

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_13 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_29, 2), _mm256_setzero_pd());

  // 4-BLAC: 1x4 / 1x4
  _t63_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_12), _mm256_castpd256_pd128(_t63_13)));

  // AVX Storer:
  _t63_1 = _t63_14;

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ),h(1, 20, 18))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_16 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), 129);

  // 4-BLAC: 1x4 / 1x4
  _t63_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_15), _mm256_castpd256_pd128(_t63_16)));

  // AVX Storer:
  _t63_2 = _t63_17;

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ),h(1, 20, 19))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t63_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_19 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_31, _t44_31, 129), _mm256_setzero_pd());

  // 4-BLAC: 1x4 / 1x4
  _t63_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t63_18), _mm256_castpd256_pd128(_t63_19)));

  // AVX Storer:
  _t63_3 = _t63_20;

  // Generating : M6[20,20] = S(h(1, 20, 16), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 16)) Kro G(h(1, 20, 16), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_0, _t63_0, 32), _mm256_permute2f128_pd(_t63_0, _t63_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_7 = _mm256_mul_pd(_t63_21, _t63_7);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(3, 20, 17), ( G(h(3, 20, 17), M6[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) * G(h(1, 20, 16), M6[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 3x4 -> 4x4
  _t63_22 = _t63_4;
  _t63_23 = _t63_5;
  _t63_24 = _t63_6;
  _t63_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t63_26 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 14), _mm256_permute2f128_pd(_t44_28, _t44_28, 129), 5);

  // 4-BLAC: (1x4)^T
  _t63_27 = _t63_26;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t63_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 0), _t63_7);
  _t63_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 15), _t63_7);
  _t63_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 0), _t63_7);
  _t63_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 15), _t63_7);

  // 4-BLAC: 4x4 - 4x4
  _t63_32 = _mm256_sub_pd(_t63_22, _t63_28);
  _t63_33 = _mm256_sub_pd(_t63_23, _t63_29);
  _t63_34 = _mm256_sub_pd(_t63_24, _t63_30);
  _t63_35 = _mm256_sub_pd(_t63_25, _t63_31);

  // AVX Storer:
  _t63_4 = _t63_32;
  _t63_5 = _t63_33;
  _t63_6 = _t63_34;

  // Generating : M6[20,20] = S(h(1, 20, 17), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 17)) Kro G(h(1, 20, 17), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_1, _t63_1, 32), _mm256_permute2f128_pd(_t63_1, _t63_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_4 = _mm256_mul_pd(_t63_36, _t63_4);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(2, 20, 18), ( G(h(2, 20, 18), M6[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) * G(h(1, 20, 17), M6[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 2x4 -> 4x4
  _t63_37 = _t63_5;
  _t63_38 = _t63_6;
  _t63_39 = _mm256_setzero_pd();
  _t63_40 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t63_41 = _mm256_permute2f128_pd(_t44_29, _t44_29, 129);

  // 4-BLAC: (1x4)^T
  _t63_42 = _t63_41;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t63_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 0), _t63_4);
  _t63_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 15), _t63_4);
  _t63_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 0), _t63_4);
  _t63_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 15), _t63_4);

  // 4-BLAC: 4x4 - 4x4
  _t63_47 = _mm256_sub_pd(_t63_37, _t63_43);
  _t63_48 = _mm256_sub_pd(_t63_38, _t63_44);
  _t63_49 = _mm256_sub_pd(_t63_39, _t63_45);
  _t63_50 = _mm256_sub_pd(_t63_40, _t63_46);

  // AVX Storer:
  _t63_5 = _t63_47;
  _t63_6 = _t63_48;

  // Generating : M6[20,20] = S(h(1, 20, 18), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 18)) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_2, _t63_2, 32), _mm256_permute2f128_pd(_t63_2, _t63_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_5 = _mm256_mul_pd(_t63_51, _t63_5);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), M6[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t44_30, _t44_30, 49), _mm256_permute2f128_pd(_t44_30, _t44_30, 49), 15);

  // 4-BLAC: (4x1)^T
  _t63_53 = _t63_52;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_8 = _mm256_mul_pd(_t63_53, _t63_5);

  // 4-BLAC: 1x4 - 1x4
  _t63_6 = _mm256_sub_pd(_t63_6, _t63_8);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 19)) Kro G(h(1, 20, 19), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t63_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_3, _t63_3, 32), _mm256_permute2f128_pd(_t63_3, _t63_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t63_6 = _mm256_mul_pd(_t63_54, _t63_6);

  // AVX Storer:


  for( int fi562 = 4; fi562 <= 16; fi562+=4 ) {
    _t64_3 = _asm256_loadu_pd(M1 + fi562 + 320);
    _t64_0 = _asm256_loadu_pd(M1 + fi562 + 340);
    _t64_1 = _asm256_loadu_pd(M1 + fi562 + 360);
    _t64_2 = _asm256_loadu_pd(M1 + fi562 + 380);

    // Generating : M6[20,20] = S(h(1, 20, 16), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 16)) Kro G(h(1, 20, 16), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_0, _t63_0, 32), _mm256_permute2f128_pd(_t63_0, _t63_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_3 = _mm256_mul_pd(_t64_4, _t64_3);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(3, 20, 17), ( G(h(3, 20, 17), M6[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) * G(h(1, 20, 16), M6[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // 3x4 -> 4x4
    _t64_5 = _t64_0;
    _t64_6 = _t64_1;
    _t64_7 = _t64_2;
    _t64_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t64_9 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 14), _mm256_permute2f128_pd(_t44_28, _t44_28, 129), 5);

    // 4-BLAC: (1x4)^T
    _t63_27 = _t64_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t63_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 0), _t64_3);
    _t63_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 32), _mm256_permute2f128_pd(_t63_27, _t63_27, 32), 15), _t64_3);
    _t63_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 0), _t64_3);
    _t63_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_27, _t63_27, 49), _mm256_permute2f128_pd(_t63_27, _t63_27, 49), 15), _t64_3);

    // 4-BLAC: 4x4 - 4x4
    _t64_10 = _mm256_sub_pd(_t64_5, _t63_28);
    _t64_11 = _mm256_sub_pd(_t64_6, _t63_29);
    _t64_12 = _mm256_sub_pd(_t64_7, _t63_30);
    _t64_13 = _mm256_sub_pd(_t64_8, _t63_31);

    // AVX Storer:
    _t64_0 = _t64_10;
    _t64_1 = _t64_11;
    _t64_2 = _t64_12;

    // Generating : M6[20,20] = S(h(1, 20, 17), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 17)) Kro G(h(1, 20, 17), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_1, _t63_1, 32), _mm256_permute2f128_pd(_t63_1, _t63_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_0 = _mm256_mul_pd(_t64_14, _t64_0);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(2, 20, 18), ( G(h(2, 20, 18), M6[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) * G(h(1, 20, 17), M6[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // 2x4 -> 4x4
    _t64_15 = _t64_1;
    _t64_16 = _t64_2;
    _t64_17 = _mm256_setzero_pd();
    _t64_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t64_19 = _mm256_permute2f128_pd(_t44_29, _t44_29, 129);

    // 4-BLAC: (1x4)^T
    _t63_42 = _t64_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t63_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 0), _t64_0);
    _t63_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 32), _mm256_permute2f128_pd(_t63_42, _t63_42, 32), 15), _t64_0);
    _t63_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 0), _t64_0);
    _t63_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_42, _t63_42, 49), _mm256_permute2f128_pd(_t63_42, _t63_42, 49), 15), _t64_0);

    // 4-BLAC: 4x4 - 4x4
    _t64_20 = _mm256_sub_pd(_t64_15, _t63_43);
    _t64_21 = _mm256_sub_pd(_t64_16, _t63_44);
    _t64_22 = _mm256_sub_pd(_t64_17, _t63_45);
    _t64_23 = _mm256_sub_pd(_t64_18, _t63_46);

    // AVX Storer:
    _t64_1 = _t64_20;
    _t64_2 = _t64_21;

    // Generating : M6[20,20] = S(h(1, 20, 18), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 18)) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_2, _t63_2, 32), _mm256_permute2f128_pd(_t63_2, _t63_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_1 = _mm256_mul_pd(_t64_24, _t64_1);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), M6[20,20],h(4, 20, fi562)) - ( T( G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t44_30, _t44_30, 49), _mm256_permute2f128_pd(_t44_30, _t44_30, 49), 15);

    // 4-BLAC: (4x1)^T
    _t63_53 = _t64_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t63_8 = _mm256_mul_pd(_t63_53, _t64_1);

    // 4-BLAC: 1x4 - 1x4
    _t64_2 = _mm256_sub_pd(_t64_2, _t63_8);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 19)) Kro G(h(1, 20, 19), M6[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t64_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_3, _t63_3, 32), _mm256_permute2f128_pd(_t63_3, _t63_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t64_2 = _mm256_mul_pd(_t64_26, _t64_2);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi562 + 320, _t64_3);
    _asm256_storeu_pd(M1 + fi562 + 340, _t64_0);
    _asm256_storeu_pd(M1 + fi562 + 360, _t64_1);
    _asm256_storeu_pd(M1 + fi562 + 380, _t64_2);
  }

  _asm256_storeu_pd(M1 + 320, _t63_7);
  _asm256_storeu_pd(M1 + 340, _t63_4);
  _asm256_storeu_pd(M1 + 360, _t63_5);
  _asm256_storeu_pd(M1 + 380, _t63_6);

  for( int fi562 = 0; fi562 <= 15; fi562+=4 ) {
    _t65_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 399])));
    _t65_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 378])));
    _t65_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 357])));
    _t65_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi562 + 336])));
    _t65_14 = _asm256_loadu_pd(M1 + -20*fi562 + 380);
    _t65_11 = _asm256_loadu_pd(M1 + -20*fi562 + 320);
    _t65_12 = _asm256_loadu_pd(M1 + -20*fi562 + 340);
    _t65_13 = _asm256_loadu_pd(M1 + -20*fi562 + 360);
    _t65_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 339)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 359))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 379)), 32);
    _t65_1 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 338)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi562 + 358)), 0);
    _t65_0 = _mm256_broadcast_sd(&(M3[-21*fi562 + 337]));

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 19), U0[20,20],h(1, 20, -fi562 + 19)) ),h(1, 20, -fi562 + 19))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_17 = _t65_6;

    // 4-BLAC: 1x4 / 1x4
    _t65_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_16), _mm256_castpd256_pd128(_t65_17)));

    // AVX Storer:
    _t65_7 = _t65_18;

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 18), U0[20,20],h(1, 20, -fi562 + 18)) ),h(1, 20, -fi562 + 18))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_20 = _t65_5;

    // 4-BLAC: 1x4 / 1x4
    _t65_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_19), _mm256_castpd256_pd128(_t65_20)));

    // AVX Storer:
    _t65_8 = _t65_21;

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 17), U0[20,20],h(1, 20, -fi562 + 17)) ),h(1, 20, -fi562 + 17))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_23 = _t65_4;

    // 4-BLAC: 1x4 / 1x4
    _t65_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_22), _mm256_castpd256_pd128(_t65_23)));

    // AVX Storer:
    _t65_9 = _t65_24;

    // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 16)) ),h(1, 20, -fi562 + 16))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_26 = _t65_3;

    // 4-BLAC: 1x4 / 1x4
    _t65_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_25), _mm256_castpd256_pd128(_t65_26)));

    // AVX Storer:
    _t65_10 = _t65_27;

    // Generating : M8[20,20] = S(h(1, 20, -fi562 + 19), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 19)) Kro G(h(1, 20, -fi562 + 19), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_7, _t65_7, 32), _mm256_permute2f128_pd(_t65_7, _t65_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_14 = _mm256_mul_pd(_t65_28, _t65_14);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(3, 20, -fi562 + 16), ( G(h(3, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) - ( G(h(3, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 19)) * G(h(1, 20, -fi562 + 19), M8[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

    // AVX Loader:

    // 3x4 -> 4x4
    _t65_29 = _t65_11;
    _t65_30 = _t65_12;
    _t65_31 = _t65_13;
    _t65_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t65_33 = _t65_2;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 32), _mm256_permute2f128_pd(_t65_33, _t65_33, 32), 0), _t65_14);
    _t65_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 32), _mm256_permute2f128_pd(_t65_33, _t65_33, 32), 15), _t65_14);
    _t65_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 49), _mm256_permute2f128_pd(_t65_33, _t65_33, 49), 0), _t65_14);
    _t65_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_33, _t65_33, 49), _mm256_permute2f128_pd(_t65_33, _t65_33, 49), 15), _t65_14);

    // 4-BLAC: 4x4 - 4x4
    _t65_38 = _mm256_sub_pd(_t65_29, _t65_34);
    _t65_39 = _mm256_sub_pd(_t65_30, _t65_35);
    _t65_40 = _mm256_sub_pd(_t65_31, _t65_36);
    _t65_41 = _mm256_sub_pd(_t65_32, _t65_37);

    // AVX Storer:
    _t65_11 = _t65_38;
    _t65_12 = _t65_39;
    _t65_13 = _t65_40;

    // Generating : M8[20,20] = S(h(1, 20, -fi562 + 18), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 18)) Kro G(h(1, 20, -fi562 + 18), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_8, _t65_8, 32), _mm256_permute2f128_pd(_t65_8, _t65_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_13 = _mm256_mul_pd(_t65_42, _t65_13);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(2, 20, -fi562 + 16), ( G(h(2, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) - ( G(h(2, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 18)) * G(h(1, 20, -fi562 + 18), M8[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

    // AVX Loader:

    // 2x4 -> 4x4
    _t65_43 = _t65_11;
    _t65_44 = _t65_12;
    _t65_45 = _mm256_setzero_pd();
    _t65_46 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t65_47 = _t65_1;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 32), _mm256_permute2f128_pd(_t65_47, _t65_47, 32), 0), _t65_13);
    _t65_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 32), _mm256_permute2f128_pd(_t65_47, _t65_47, 32), 15), _t65_13);
    _t65_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 49), _mm256_permute2f128_pd(_t65_47, _t65_47, 49), 0), _t65_13);
    _t65_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_47, _t65_47, 49), _mm256_permute2f128_pd(_t65_47, _t65_47, 49), 15), _t65_13);

    // 4-BLAC: 4x4 - 4x4
    _t65_52 = _mm256_sub_pd(_t65_43, _t65_48);
    _t65_53 = _mm256_sub_pd(_t65_44, _t65_49);
    _t65_54 = _mm256_sub_pd(_t65_45, _t65_50);
    _t65_55 = _mm256_sub_pd(_t65_46, _t65_51);

    // AVX Storer:
    _t65_11 = _t65_52;
    _t65_12 = _t65_53;

    // Generating : M8[20,20] = S(h(1, 20, -fi562 + 17), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 17)) Kro G(h(1, 20, -fi562 + 17), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_56 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_9, _t65_9, 32), _mm256_permute2f128_pd(_t65_9, _t65_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_12 = _mm256_mul_pd(_t65_56, _t65_12);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, -fi562 + 16), ( G(h(1, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) - ( G(h(1, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 17)) Kro G(h(1, 20, -fi562 + 17), M8[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_57 = _t65_0;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_15 = _mm256_mul_pd(_t65_57, _t65_12);

    // 4-BLAC: 1x4 - 1x4
    _t65_11 = _mm256_sub_pd(_t65_11, _t65_15);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, -fi562 + 16), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 16)) Kro G(h(1, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_10, _t65_10, 32), _mm256_permute2f128_pd(_t65_10, _t65_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_11 = _mm256_mul_pd(_t65_58, _t65_11);

    // AVX Storer:

    for( int fi641 = 4; fi641 <= 16; fi641+=4 ) {
      _t66_3 = _asm256_loadu_pd(M1 + -20*fi562 + fi641 + 380);
      _t66_0 = _asm256_loadu_pd(M1 + -20*fi562 + fi641 + 320);
      _t66_1 = _asm256_loadu_pd(M1 + -20*fi562 + fi641 + 340);
      _t66_2 = _asm256_loadu_pd(M1 + -20*fi562 + fi641 + 360);

      // Generating : M8[20,20] = S(h(1, 20, -fi562 + 19), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 19)) Kro G(h(1, 20, -fi562 + 19), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_7, _t65_7, 32), _mm256_permute2f128_pd(_t65_7, _t65_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_3 = _mm256_mul_pd(_t66_4, _t66_3);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(3, 20, -fi562 + 16), ( G(h(3, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) - ( G(h(3, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 19)) * G(h(1, 20, -fi562 + 19), M8[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

      // AVX Loader:

      // 3x4 -> 4x4
      _t66_5 = _t66_0;
      _t66_6 = _t66_1;
      _t66_7 = _t66_2;
      _t66_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t66_9 = _t65_2;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t65_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 32), _mm256_permute2f128_pd(_t66_9, _t66_9, 32), 0), _t66_3);
      _t65_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 32), _mm256_permute2f128_pd(_t66_9, _t66_9, 32), 15), _t66_3);
      _t65_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 49), _mm256_permute2f128_pd(_t66_9, _t66_9, 49), 0), _t66_3);
      _t65_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_9, _t66_9, 49), _mm256_permute2f128_pd(_t66_9, _t66_9, 49), 15), _t66_3);

      // 4-BLAC: 4x4 - 4x4
      _t66_10 = _mm256_sub_pd(_t66_5, _t65_34);
      _t66_11 = _mm256_sub_pd(_t66_6, _t65_35);
      _t66_12 = _mm256_sub_pd(_t66_7, _t65_36);
      _t66_13 = _mm256_sub_pd(_t66_8, _t65_37);

      // AVX Storer:
      _t66_0 = _t66_10;
      _t66_1 = _t66_11;
      _t66_2 = _t66_12;

      // Generating : M8[20,20] = S(h(1, 20, -fi562 + 18), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 18)) Kro G(h(1, 20, -fi562 + 18), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_8, _t65_8, 32), _mm256_permute2f128_pd(_t65_8, _t65_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_2 = _mm256_mul_pd(_t66_14, _t66_2);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(2, 20, -fi562 + 16), ( G(h(2, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) - ( G(h(2, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 18)) * G(h(1, 20, -fi562 + 18), M8[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

      // AVX Loader:

      // 2x4 -> 4x4
      _t66_15 = _t66_0;
      _t66_16 = _t66_1;
      _t66_17 = _mm256_setzero_pd();
      _t66_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t66_19 = _t65_1;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t65_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 32), _mm256_permute2f128_pd(_t66_19, _t66_19, 32), 0), _t66_2);
      _t65_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 32), _mm256_permute2f128_pd(_t66_19, _t66_19, 32), 15), _t66_2);
      _t65_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 49), _mm256_permute2f128_pd(_t66_19, _t66_19, 49), 0), _t66_2);
      _t65_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t66_19, _t66_19, 49), _mm256_permute2f128_pd(_t66_19, _t66_19, 49), 15), _t66_2);

      // 4-BLAC: 4x4 - 4x4
      _t66_20 = _mm256_sub_pd(_t66_15, _t65_48);
      _t66_21 = _mm256_sub_pd(_t66_16, _t65_49);
      _t66_22 = _mm256_sub_pd(_t66_17, _t65_50);
      _t66_23 = _mm256_sub_pd(_t66_18, _t65_51);

      // AVX Storer:
      _t66_0 = _t66_20;
      _t66_1 = _t66_21;

      // Generating : M8[20,20] = S(h(1, 20, -fi562 + 17), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 17)) Kro G(h(1, 20, -fi562 + 17), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_9, _t65_9, 32), _mm256_permute2f128_pd(_t65_9, _t65_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_1 = _mm256_mul_pd(_t66_24, _t66_1);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(1, 20, -fi562 + 16), ( G(h(1, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) - ( G(h(1, 20, -fi562 + 16), U0[20,20],h(1, 20, -fi562 + 17)) Kro G(h(1, 20, -fi562 + 17), M8[20,20],h(4, 20, fi641)) ) ),h(4, 20, fi641))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_25 = _t65_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t65_15 = _mm256_mul_pd(_t66_25, _t66_1);

      // 4-BLAC: 1x4 - 1x4
      _t66_0 = _mm256_sub_pd(_t66_0, _t65_15);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(1, 20, -fi562 + 16), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, -fi562 + 16)) Kro G(h(1, 20, -fi562 + 16), M8[20,20],h(4, 20, fi641)) ),h(4, 20, fi641))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_10, _t65_10, 32), _mm256_permute2f128_pd(_t65_10, _t65_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_0 = _mm256_mul_pd(_t66_26, _t66_0);

      // AVX Storer:
      _asm256_storeu_pd(M1 + -20*fi562 + fi641 + 380, _t66_3);
      _asm256_storeu_pd(M1 + -20*fi562 + fi641 + 360, _t66_2);
      _asm256_storeu_pd(M1 + -20*fi562 + fi641 + 340, _t66_1);
      _asm256_storeu_pd(M1 + -20*fi562 + fi641 + 320, _t66_0);
    }

    // Generating : M8[20,20] = Sum_{k3} ( Sum_{k2} ( S(h(4, 20, k3), ( G(h(4, 20, k3), M8[20,20],h(4, 20, k2)) - ( G(h(4, 20, k3), U0[20,20],h(4, 20, -fi562 + 16)) * G(h(4, 20, -fi562 + 16), M8[20,20],h(4, 20, k2)) ) ),h(4, 20, k2)) ) )
    _asm256_storeu_pd(M1 + -20*fi562 + 380, _t65_14);
    _asm256_storeu_pd(M1 + -20*fi562 + 360, _t65_13);
    _asm256_storeu_pd(M1 + -20*fi562 + 340, _t65_12);
    _asm256_storeu_pd(M1 + -20*fi562 + 320, _t65_11);

    for( int k3 = 0; k3 <= -fi562 + 15; k3+=4 ) {

      for( int k2 = 0; k2 <= 19; k2+=4 ) {
        _t67_24 = _asm256_loadu_pd(M1 + k2 + 20*k3);
        _t67_25 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 20);
        _t67_26 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 40);
        _t67_27 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 60);
        _t67_19 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 16);
        _t67_18 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 17);
        _t67_17 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 18);
        _t67_16 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 19);
        _t67_15 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 36);
        _t67_14 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 37);
        _t67_13 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 38);
        _t67_12 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 39);
        _t67_11 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 56);
        _t67_10 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 57);
        _t67_9 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 58);
        _t67_8 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 59);
        _t67_7 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 76);
        _t67_6 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 77);
        _t67_5 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 78);
        _t67_4 = _mm256_broadcast_sd(M3 + -fi562 + 20*k3 + 79);
        _t67_3 = _asm256_loadu_pd(M1 + -20*fi562 + k2 + 320);
        _t67_2 = _asm256_loadu_pd(M1 + -20*fi562 + k2 + 340);
        _t67_1 = _asm256_loadu_pd(M1 + -20*fi562 + k2 + 360);
        _t67_0 = _asm256_loadu_pd(M1 + -20*fi562 + k2 + 380);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t67_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_19, _t67_3), _mm256_mul_pd(_t67_18, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_17, _t67_1), _mm256_mul_pd(_t67_16, _t67_0)));
        _t67_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_15, _t67_3), _mm256_mul_pd(_t67_14, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_13, _t67_1), _mm256_mul_pd(_t67_12, _t67_0)));
        _t67_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_11, _t67_3), _mm256_mul_pd(_t67_10, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_9, _t67_1), _mm256_mul_pd(_t67_8, _t67_0)));
        _t67_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t67_7, _t67_3), _mm256_mul_pd(_t67_6, _t67_2)), _mm256_add_pd(_mm256_mul_pd(_t67_5, _t67_1), _mm256_mul_pd(_t67_4, _t67_0)));

        // 4-BLAC: 4x4 - 4x4
        _t67_24 = _mm256_sub_pd(_t67_24, _t67_20);
        _t67_25 = _mm256_sub_pd(_t67_25, _t67_21);
        _t67_26 = _mm256_sub_pd(_t67_26, _t67_22);
        _t67_27 = _mm256_sub_pd(_t67_27, _t67_23);

        // AVX Storer:
        _asm256_storeu_pd(M1 + k2 + 20*k3, _t67_24);
        _asm256_storeu_pd(M1 + k2 + 20*k3 + 20, _t67_25);
        _asm256_storeu_pd(M1 + k2 + 20*k3 + 40, _t67_26);
        _asm256_storeu_pd(M1 + k2 + 20*k3 + 60, _t67_27);
      }
    }
  }

  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[3])));
  _t48_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[22])));
  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_9 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[43])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21])));
  _t48_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[42])));
  _t48_10 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[63])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[23])));
  _t68_7 = _asm256_loadu_pd(M1 + 60);
  _t68_4 = _asm256_loadu_pd(M1);
  _t68_5 = _asm256_loadu_pd(M1 + 20);
  _t68_6 = _asm256_loadu_pd(M1 + 40);

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_10 = _t48_10;

  // 4-BLAC: 1x4 / 1x4
  _t68_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_9), _mm256_castpd256_pd128(_t68_10)));

  // AVX Storer:
  _t68_0 = _t68_11;

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_13 = _t48_5;

  // 4-BLAC: 1x4 / 1x4
  _t68_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_12), _mm256_castpd256_pd128(_t68_13)));

  // AVX Storer:
  _t68_1 = _t68_14;

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_16 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t68_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_15), _mm256_castpd256_pd128(_t68_16)));

  // AVX Storer:
  _t68_2 = _t68_17;

  // Generating : T2203[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t68_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_19 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t68_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_18), _mm256_castpd256_pd128(_t68_19)));

  // AVX Storer:
  _t68_3 = _t68_20;

  // Generating : M8[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_0, _t68_0, 32), _mm256_permute2f128_pd(_t68_0, _t68_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_7 = _mm256_mul_pd(_t68_21, _t68_7);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(3, 20, 0), ( G(h(3, 20, 0), M8[20,20],h(4, 20, fi562)) - ( G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) * G(h(1, 20, 3), M8[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 3x4 -> 4x4
  _t68_22 = _t68_4;
  _t68_23 = _t68_5;
  _t68_24 = _t68_6;
  _t68_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t68_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_6, _t48_8), _mm256_unpacklo_pd(_t48_9, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t68_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_26, _t68_26, 32), _mm256_permute2f128_pd(_t68_26, _t68_26, 32), 0), _t68_7);
  _t68_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_26, _t68_26, 32), _mm256_permute2f128_pd(_t68_26, _t68_26, 32), 15), _t68_7);
  _t68_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_26, _t68_26, 49), _mm256_permute2f128_pd(_t68_26, _t68_26, 49), 0), _t68_7);
  _t68_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_26, _t68_26, 49), _mm256_permute2f128_pd(_t68_26, _t68_26, 49), 15), _t68_7);

  // 4-BLAC: 4x4 - 4x4
  _t68_31 = _mm256_sub_pd(_t68_22, _t68_27);
  _t68_32 = _mm256_sub_pd(_t68_23, _t68_28);
  _t68_33 = _mm256_sub_pd(_t68_24, _t68_29);
  _t68_34 = _mm256_sub_pd(_t68_25, _t68_30);

  // AVX Storer:
  _t68_4 = _t68_31;
  _t68_5 = _t68_32;
  _t68_6 = _t68_33;

  // Generating : M8[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_1, _t68_1, 32), _mm256_permute2f128_pd(_t68_1, _t68_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_6 = _mm256_mul_pd(_t68_35, _t68_6);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(2, 20, 0), ( G(h(2, 20, 0), M8[20,20],h(4, 20, fi562)) - ( G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) * G(h(1, 20, 2), M8[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // 2x4 -> 4x4
  _t68_36 = _t68_4;
  _t68_37 = _t68_5;
  _t68_38 = _mm256_setzero_pd();
  _t68_39 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t68_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t68_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_40, _t68_40, 32), _mm256_permute2f128_pd(_t68_40, _t68_40, 32), 0), _t68_6);
  _t68_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_40, _t68_40, 32), _mm256_permute2f128_pd(_t68_40, _t68_40, 32), 15), _t68_6);
  _t68_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_40, _t68_40, 49), _mm256_permute2f128_pd(_t68_40, _t68_40, 49), 0), _t68_6);
  _t68_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_40, _t68_40, 49), _mm256_permute2f128_pd(_t68_40, _t68_40, 49), 15), _t68_6);

  // 4-BLAC: 4x4 - 4x4
  _t68_45 = _mm256_sub_pd(_t68_36, _t68_41);
  _t68_46 = _mm256_sub_pd(_t68_37, _t68_42);
  _t68_47 = _mm256_sub_pd(_t68_38, _t68_43);
  _t68_48 = _mm256_sub_pd(_t68_39, _t68_44);

  // AVX Storer:
  _t68_4 = _t68_45;
  _t68_5 = _t68_46;

  // Generating : M8[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_2, _t68_2, 32), _mm256_permute2f128_pd(_t68_2, _t68_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_5 = _mm256_mul_pd(_t68_49, _t68_5);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 20, 0), M8[20,20],h(4, 20, fi562)) - ( G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_8 = _mm256_mul_pd(_t68_50, _t68_5);

  // 4-BLAC: 1x4 - 1x4
  _t68_4 = _mm256_sub_pd(_t68_4, _t68_8);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

  // AVX Loader:

  // 1x1 -> 1x4
  _t68_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_3, _t68_3, 32), _mm256_permute2f128_pd(_t68_3, _t68_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t68_4 = _mm256_mul_pd(_t68_51, _t68_4);

  // AVX Storer:


  for( int fi562 = 4; fi562 <= 16; fi562+=4 ) {
    _t69_3 = _asm256_loadu_pd(M1 + fi562 + 60);
    _t69_0 = _asm256_loadu_pd(M1 + fi562);
    _t69_1 = _asm256_loadu_pd(M1 + fi562 + 20);
    _t69_2 = _asm256_loadu_pd(M1 + fi562 + 40);

    // Generating : M8[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_0, _t68_0, 32), _mm256_permute2f128_pd(_t68_0, _t68_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_3 = _mm256_mul_pd(_t69_4, _t69_3);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(3, 20, 0), ( G(h(3, 20, 0), M8[20,20],h(4, 20, fi562)) - ( G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) * G(h(1, 20, 3), M8[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // 3x4 -> 4x4
    _t69_5 = _t69_0;
    _t69_6 = _t69_1;
    _t69_7 = _t69_2;
    _t69_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t69_9 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_6, _t48_8), _mm256_unpacklo_pd(_t48_9, _mm256_setzero_pd()), 32);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t68_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 32), _mm256_permute2f128_pd(_t69_9, _t69_9, 32), 0), _t69_3);
    _t68_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 32), _mm256_permute2f128_pd(_t69_9, _t69_9, 32), 15), _t69_3);
    _t68_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 49), _mm256_permute2f128_pd(_t69_9, _t69_9, 49), 0), _t69_3);
    _t68_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_9, _t69_9, 49), _mm256_permute2f128_pd(_t69_9, _t69_9, 49), 15), _t69_3);

    // 4-BLAC: 4x4 - 4x4
    _t69_10 = _mm256_sub_pd(_t69_5, _t68_27);
    _t69_11 = _mm256_sub_pd(_t69_6, _t68_28);
    _t69_12 = _mm256_sub_pd(_t69_7, _t68_29);
    _t69_13 = _mm256_sub_pd(_t69_8, _t68_30);

    // AVX Storer:
    _t69_0 = _t69_10;
    _t69_1 = _t69_11;
    _t69_2 = _t69_12;

    // Generating : M8[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_1, _t68_1, 32), _mm256_permute2f128_pd(_t68_1, _t68_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_2 = _mm256_mul_pd(_t69_14, _t69_2);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(2, 20, 0), ( G(h(2, 20, 0), M8[20,20],h(4, 20, fi562)) - ( G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) * G(h(1, 20, 2), M8[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // 2x4 -> 4x4
    _t69_15 = _t69_0;
    _t69_16 = _t69_1;
    _t69_17 = _mm256_setzero_pd();
    _t69_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t69_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t68_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 32), _mm256_permute2f128_pd(_t69_19, _t69_19, 32), 0), _t69_2);
    _t68_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 32), _mm256_permute2f128_pd(_t69_19, _t69_19, 32), 15), _t69_2);
    _t68_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 49), _mm256_permute2f128_pd(_t69_19, _t69_19, 49), 0), _t69_2);
    _t68_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_19, _t69_19, 49), _mm256_permute2f128_pd(_t69_19, _t69_19, 49), 15), _t69_2);

    // 4-BLAC: 4x4 - 4x4
    _t69_20 = _mm256_sub_pd(_t69_15, _t68_41);
    _t69_21 = _mm256_sub_pd(_t69_16, _t68_42);
    _t69_22 = _mm256_sub_pd(_t69_17, _t68_43);
    _t69_23 = _mm256_sub_pd(_t69_18, _t68_44);

    // AVX Storer:
    _t69_0 = _t69_20;
    _t69_1 = _t69_21;

    // Generating : M8[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_2, _t68_2, 32), _mm256_permute2f128_pd(_t68_2, _t68_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_1 = _mm256_mul_pd(_t69_24, _t69_1);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 20, 0), M8[20,20],h(4, 20, fi562)) - ( G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi562)) ) ),h(4, 20, fi562))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t68_8 = _mm256_mul_pd(_t69_25, _t69_1);

    // 4-BLAC: 1x4 - 1x4
    _t69_0 = _mm256_sub_pd(_t69_0, _t68_8);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2203[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), M8[20,20],h(4, 20, fi562)) ),h(4, 20, fi562))

    // AVX Loader:

    // 1x1 -> 1x4
    _t69_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_3, _t68_3, 32), _mm256_permute2f128_pd(_t68_3, _t68_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t69_0 = _mm256_mul_pd(_t69_26, _t69_0);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi562 + 60, _t69_3);
    _asm256_storeu_pd(M1 + fi562 + 40, _t69_2);
    _asm256_storeu_pd(M1 + fi562 + 20, _t69_1);
    _asm256_storeu_pd(M1 + fi562, _t69_0);
  }


  // Generating : x[20,1] = ( Sum_{k2} ( S(h(4, 20, k2), ( G(h(4, 20, k2), y[20,1],h(1, 1, 0)) + ( G(h(4, 20, k2), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), v0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( $(h(4, 20, k2), ( G(h(4, 20, k2), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), v0[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t59_1);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t59_3);

  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t70_4 = _asm256_loadu_pd(y + k2);
    _t70_3 = _asm256_loadu_pd(M2 + 20*k2);
    _t70_2 = _asm256_loadu_pd(M2 + 20*k2 + 20);
    _t70_1 = _asm256_loadu_pd(M2 + 20*k2 + 40);
    _t70_0 = _asm256_loadu_pd(M2 + 20*k2 + 60);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t70_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t70_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32)), _mm256_mul_pd(_t70_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t70_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32)), _mm256_mul_pd(_t70_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t70_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32)), _mm256_mul_pd(_t70_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t70_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32)), _mm256_mul_pd(_t70_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_5, _t59_4), _mm256_unpacklo_pd(_t59_2, _t59_0), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t70_5 = _mm256_add_pd(_t70_4, _t70_6);

    // AVX Storer:
    _asm256_storeu_pd(x + k2, _t70_5);
  }


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 19; k2+=4 ) {
      _t71_4 = _asm256_loadu_pd(M2 + 20*k2 + k3);
      _t71_3 = _asm256_loadu_pd(M2 + 20*k2 + k3 + 20);
      _t71_2 = _asm256_loadu_pd(M2 + 20*k2 + k3 + 40);
      _t71_1 = _asm256_loadu_pd(M2 + 20*k2 + k3 + 60);
      _t71_0 = _asm256_loadu_pd(v0 + k3);
      _t71_5 = _asm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t71_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t71_4, _t71_0), _mm256_mul_pd(_t71_3, _t71_0)), _mm256_hadd_pd(_mm256_mul_pd(_t71_2, _t71_0), _mm256_mul_pd(_t71_1, _t71_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t71_4, _t71_0), _mm256_mul_pd(_t71_3, _t71_0)), _mm256_hadd_pd(_mm256_mul_pd(_t71_2, _t71_0), _mm256_mul_pd(_t71_1, _t71_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t71_5 = _mm256_add_pd(_t71_5, _t71_6);

      // AVX Storer:
      _asm256_storeu_pd(x + k2, _t71_5);
    }
  }


  // Generating : P[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, k2)) - ( G(h(4, 20, k2), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), M1[20,20],h(4, 20, k2)) ) ),h(4, 20, k2)) + Sum_{i0} ( S(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, i0)) - ( G(h(4, 20, k2), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), M1[20,20],h(4, 20, i0)) ) ),h(4, 20, i0)) ) ) ) + S(h(4, 20, 16), ( G(h(4, 20, 16), Y[20,20],h(4, 20, 16)) - ( G(h(4, 20, 16), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), M1[20,20],h(4, 20, 16)) ) ),h(4, 20, 16)) ) + Sum_{k3} ( ( Sum_{k2} ( ( -$(h(4, 20, k2), ( G(h(4, 20, k2), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), M1[20,20],h(4, 20, k2)) ),h(4, 20, k2)) + Sum_{i0} ( -$(h(4, 20, k2), ( G(h(4, 20, k2), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), M1[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + -$(h(4, 20, 16), ( G(h(4, 20, 16), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), M1[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) ) )

  _asm256_storeu_pd(M1 + 60, _t68_7);
  _asm256_storeu_pd(M1 + 40, _t68_6);
  _asm256_storeu_pd(M1 + 20, _t68_5);
  _asm256_storeu_pd(M1, _t68_4);

  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t72_23 = _asm256_loadu_pd(Y + 21*k2);
    _t72_22 = _mm256_maskload_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t72_21 = _mm256_maskload_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t72_20 = _mm256_maskload_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t72_19 = _mm256_broadcast_sd(M2 + 20*k2);
    _t72_18 = _mm256_broadcast_sd(M2 + 20*k2 + 1);
    _t72_17 = _mm256_broadcast_sd(M2 + 20*k2 + 2);
    _t72_16 = _mm256_broadcast_sd(M2 + 20*k2 + 3);
    _t72_15 = _mm256_broadcast_sd(M2 + 20*k2 + 20);
    _t72_14 = _mm256_broadcast_sd(M2 + 20*k2 + 21);
    _t72_13 = _mm256_broadcast_sd(M2 + 20*k2 + 22);
    _t72_12 = _mm256_broadcast_sd(M2 + 20*k2 + 23);
    _t72_11 = _mm256_broadcast_sd(M2 + 20*k2 + 40);
    _t72_10 = _mm256_broadcast_sd(M2 + 20*k2 + 41);
    _t72_9 = _mm256_broadcast_sd(M2 + 20*k2 + 42);
    _t72_8 = _mm256_broadcast_sd(M2 + 20*k2 + 43);
    _t72_7 = _mm256_broadcast_sd(M2 + 20*k2 + 60);
    _t72_6 = _mm256_broadcast_sd(M2 + 20*k2 + 61);
    _t72_5 = _mm256_broadcast_sd(M2 + 20*k2 + 62);
    _t72_4 = _mm256_broadcast_sd(M2 + 20*k2 + 63);
    _t72_3 = _asm256_loadu_pd(M1 + k2);
    _t72_2 = _asm256_loadu_pd(M1 + k2 + 20);
    _t72_1 = _asm256_loadu_pd(M1 + k2 + 40);
    _t72_0 = _asm256_loadu_pd(M1 + k2 + 60);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t72_36 = _t72_23;
    _t72_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t72_23, _t72_22, 3), _t72_22, 12);
    _t72_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t72_23, _t72_22, 0), _t72_21, 49);
    _t72_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t72_23, _t72_22, 12), _mm256_shuffle_pd(_t72_21, _t72_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t72_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_19, _t72_3), _mm256_mul_pd(_t72_18, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_17, _t72_1), _mm256_mul_pd(_t72_16, _t72_0)));
    _t72_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_15, _t72_3), _mm256_mul_pd(_t72_14, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_13, _t72_1), _mm256_mul_pd(_t72_12, _t72_0)));
    _t72_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_11, _t72_3), _mm256_mul_pd(_t72_10, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_9, _t72_1), _mm256_mul_pd(_t72_8, _t72_0)));
    _t72_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_7, _t72_3), _mm256_mul_pd(_t72_6, _t72_2)), _mm256_add_pd(_mm256_mul_pd(_t72_5, _t72_1), _mm256_mul_pd(_t72_4, _t72_0)));

    // 4-BLAC: 4x4 - 4x4
    _t72_32 = _mm256_sub_pd(_t72_36, _t72_28);
    _t72_33 = _mm256_sub_pd(_t72_37, _t72_29);
    _t72_34 = _mm256_sub_pd(_t72_38, _t72_30);
    _t72_35 = _mm256_sub_pd(_t72_39, _t72_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t72_24 = _t72_32;
    _t72_25 = _t72_33;
    _t72_26 = _t72_34;
    _t72_27 = _t72_35;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
      _t73_7 = _asm256_loadu_pd(Y + i0 + 20*k2);
      _t73_6 = _asm256_loadu_pd(Y + i0 + 20*k2 + 20);
      _t73_5 = _asm256_loadu_pd(Y + i0 + 20*k2 + 40);
      _t73_4 = _asm256_loadu_pd(Y + i0 + 20*k2 + 60);
      _t73_3 = _asm256_loadu_pd(M1 + i0);
      _t73_2 = _asm256_loadu_pd(M1 + i0 + 20);
      _t73_1 = _asm256_loadu_pd(M1 + i0 + 40);
      _t73_0 = _asm256_loadu_pd(M1 + i0 + 60);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t73_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_19, _t73_3), _mm256_mul_pd(_t72_18, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_17, _t73_1), _mm256_mul_pd(_t72_16, _t73_0)));
      _t73_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_15, _t73_3), _mm256_mul_pd(_t72_14, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_13, _t73_1), _mm256_mul_pd(_t72_12, _t73_0)));
      _t73_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_11, _t73_3), _mm256_mul_pd(_t72_10, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_9, _t73_1), _mm256_mul_pd(_t72_8, _t73_0)));
      _t73_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t72_7, _t73_3), _mm256_mul_pd(_t72_6, _t73_2)), _mm256_add_pd(_mm256_mul_pd(_t72_5, _t73_1), _mm256_mul_pd(_t72_4, _t73_0)));

      // 4-BLAC: 4x4 - 4x4
      _t73_12 = _mm256_sub_pd(_t73_7, _t73_8);
      _t73_13 = _mm256_sub_pd(_t73_6, _t73_9);
      _t73_14 = _mm256_sub_pd(_t73_5, _t73_10);
      _t73_15 = _mm256_sub_pd(_t73_4, _t73_11);

      // AVX Storer:
      _asm256_storeu_pd(P + i0 + 20*k2, _t73_12);
      _asm256_storeu_pd(P + i0 + 20*k2 + 20, _t73_13);
      _asm256_storeu_pd(P + i0 + 20*k2 + 40, _t73_14);
      _asm256_storeu_pd(P + i0 + 20*k2 + 60, _t73_15);
    }
    _asm256_storeu_pd(P + 21*k2, _t72_24);
    _mm256_maskstore_pd(P + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t72_25);
    _mm256_maskstore_pd(P + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t72_26);
    _mm256_maskstore_pd(P + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t72_27);
  }

  _t74_19 = _mm256_broadcast_sd(M2 + 320);
  _t74_18 = _mm256_broadcast_sd(M2 + 321);
  _t74_17 = _mm256_broadcast_sd(M2 + 322);
  _t74_16 = _mm256_broadcast_sd(M2 + 323);
  _t74_15 = _mm256_broadcast_sd(M2 + 340);
  _t74_14 = _mm256_broadcast_sd(M2 + 341);
  _t74_13 = _mm256_broadcast_sd(M2 + 342);
  _t74_12 = _mm256_broadcast_sd(M2 + 343);
  _t74_11 = _mm256_broadcast_sd(M2 + 360);
  _t74_10 = _mm256_broadcast_sd(M2 + 361);
  _t74_9 = _mm256_broadcast_sd(M2 + 362);
  _t74_8 = _mm256_broadcast_sd(M2 + 363);
  _t74_7 = _mm256_broadcast_sd(M2 + 380);
  _t74_6 = _mm256_broadcast_sd(M2 + 381);
  _t74_5 = _mm256_broadcast_sd(M2 + 382);
  _t74_4 = _mm256_broadcast_sd(M2 + 383);
  _t74_3 = _asm256_loadu_pd(M1 + 16);
  _t74_2 = _asm256_loadu_pd(M1 + 36);
  _t74_1 = _asm256_loadu_pd(M1 + 56);
  _t74_0 = _asm256_loadu_pd(M1 + 76);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t74_28 = _t15_28;
  _t74_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t74_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t74_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t74_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_19, _t74_3), _mm256_mul_pd(_t74_18, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_17, _t74_1), _mm256_mul_pd(_t74_16, _t74_0)));
  _t74_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_15, _t74_3), _mm256_mul_pd(_t74_14, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_13, _t74_1), _mm256_mul_pd(_t74_12, _t74_0)));
  _t74_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_11, _t74_3), _mm256_mul_pd(_t74_10, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_9, _t74_1), _mm256_mul_pd(_t74_8, _t74_0)));
  _t74_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t74_7, _t74_3), _mm256_mul_pd(_t74_6, _t74_2)), _mm256_add_pd(_mm256_mul_pd(_t74_5, _t74_1), _mm256_mul_pd(_t74_4, _t74_0)));

  // 4-BLAC: 4x4 - 4x4
  _t74_24 = _mm256_sub_pd(_t74_28, _t74_20);
  _t74_25 = _mm256_sub_pd(_t74_29, _t74_21);
  _t74_26 = _mm256_sub_pd(_t74_30, _t74_22);
  _t74_27 = _mm256_sub_pd(_t74_31, _t74_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t10_3 = _t74_24;
  _t10_2 = _t74_25;
  _t10_1 = _t74_26;
  _t10_0 = _t74_27;


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    for( int k2 = 0; k2 <= 15; k2+=4 ) {
      _t75_19 = _mm256_broadcast_sd(M2 + 20*k2 + k3);
      _t75_18 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 1);
      _t75_17 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 2);
      _t75_16 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 3);
      _t75_15 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 20);
      _t75_14 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 21);
      _t75_13 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 22);
      _t75_12 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 23);
      _t75_11 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 40);
      _t75_10 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 41);
      _t75_9 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 42);
      _t75_8 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 43);
      _t75_7 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 60);
      _t75_6 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 61);
      _t75_5 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 62);
      _t75_4 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 63);
      _t75_3 = _asm256_loadu_pd(M1 + k2 + 20*k3);
      _t75_2 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 20);
      _t75_1 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 40);
      _t75_0 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 60);
      _t75_20 = _asm256_loadu_pd(P + 21*k2);
      _t75_21 = _mm256_maskload_pd(P + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t75_22 = _mm256_maskload_pd(P + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t75_23 = _mm256_maskload_pd(P + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t75_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_19, _t75_3), _mm256_mul_pd(_t75_18, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_17, _t75_1), _mm256_mul_pd(_t75_16, _t75_0)));
      _t75_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_15, _t75_3), _mm256_mul_pd(_t75_14, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_13, _t75_1), _mm256_mul_pd(_t75_12, _t75_0)));
      _t75_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_11, _t75_3), _mm256_mul_pd(_t75_10, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_9, _t75_1), _mm256_mul_pd(_t75_8, _t75_0)));
      _t75_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_7, _t75_3), _mm256_mul_pd(_t75_6, _t75_2)), _mm256_add_pd(_mm256_mul_pd(_t75_5, _t75_1), _mm256_mul_pd(_t75_4, _t75_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t75_28 = _t75_20;
      _t75_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t75_20, _t75_21, 3), _t75_21, 12);
      _t75_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t75_20, _t75_21, 0), _t75_22, 49);
      _t75_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t75_20, _t75_21, 12), _mm256_shuffle_pd(_t75_22, _t75_23, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t75_28 = _mm256_sub_pd(_t75_28, _t75_24);
      _t75_29 = _mm256_sub_pd(_t75_29, _t75_25);
      _t75_30 = _mm256_sub_pd(_t75_30, _t75_26);
      _t75_31 = _mm256_sub_pd(_t75_31, _t75_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t75_20 = _t75_28;
      _t75_21 = _t75_29;
      _t75_22 = _t75_30;
      _t75_23 = _t75_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t76_3 = _asm256_loadu_pd(M1 + i0 + 20*k3);
        _t76_2 = _asm256_loadu_pd(M1 + i0 + 20*k3 + 20);
        _t76_1 = _asm256_loadu_pd(M1 + i0 + 20*k3 + 40);
        _t76_0 = _asm256_loadu_pd(M1 + i0 + 20*k3 + 60);
        _t76_4 = _asm256_loadu_pd(P + i0 + 20*k2);
        _t76_5 = _asm256_loadu_pd(P + i0 + 20*k2 + 20);
        _t76_6 = _asm256_loadu_pd(P + i0 + 20*k2 + 40);
        _t76_7 = _asm256_loadu_pd(P + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t76_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_19, _t76_3), _mm256_mul_pd(_t75_18, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_17, _t76_1), _mm256_mul_pd(_t75_16, _t76_0)));
        _t76_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_15, _t76_3), _mm256_mul_pd(_t75_14, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_13, _t76_1), _mm256_mul_pd(_t75_12, _t76_0)));
        _t76_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_11, _t76_3), _mm256_mul_pd(_t75_10, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_9, _t76_1), _mm256_mul_pd(_t75_8, _t76_0)));
        _t76_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t75_7, _t76_3), _mm256_mul_pd(_t75_6, _t76_2)), _mm256_add_pd(_mm256_mul_pd(_t75_5, _t76_1), _mm256_mul_pd(_t75_4, _t76_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t76_4 = _mm256_sub_pd(_t76_4, _t76_8);
        _t76_5 = _mm256_sub_pd(_t76_5, _t76_9);
        _t76_6 = _mm256_sub_pd(_t76_6, _t76_10);
        _t76_7 = _mm256_sub_pd(_t76_7, _t76_11);

        // AVX Storer:
        _asm256_storeu_pd(P + i0 + 20*k2, _t76_4);
        _asm256_storeu_pd(P + i0 + 20*k2 + 20, _t76_5);
        _asm256_storeu_pd(P + i0 + 20*k2 + 40, _t76_6);
        _asm256_storeu_pd(P + i0 + 20*k2 + 60, _t76_7);
      }
      _asm256_storeu_pd(P + 21*k2, _t75_20);
      _mm256_maskstore_pd(P + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t75_21);
      _mm256_maskstore_pd(P + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t75_22);
      _mm256_maskstore_pd(P + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t75_23);
    }
    _t77_19 = _mm256_broadcast_sd(M2 + k3 + 320);
    _t77_18 = _mm256_broadcast_sd(M2 + k3 + 321);
    _t77_17 = _mm256_broadcast_sd(M2 + k3 + 322);
    _t77_16 = _mm256_broadcast_sd(M2 + k3 + 323);
    _t77_15 = _mm256_broadcast_sd(M2 + k3 + 340);
    _t77_14 = _mm256_broadcast_sd(M2 + k3 + 341);
    _t77_13 = _mm256_broadcast_sd(M2 + k3 + 342);
    _t77_12 = _mm256_broadcast_sd(M2 + k3 + 343);
    _t77_11 = _mm256_broadcast_sd(M2 + k3 + 360);
    _t77_10 = _mm256_broadcast_sd(M2 + k3 + 361);
    _t77_9 = _mm256_broadcast_sd(M2 + k3 + 362);
    _t77_8 = _mm256_broadcast_sd(M2 + k3 + 363);
    _t77_7 = _mm256_broadcast_sd(M2 + k3 + 380);
    _t77_6 = _mm256_broadcast_sd(M2 + k3 + 381);
    _t77_5 = _mm256_broadcast_sd(M2 + k3 + 382);
    _t77_4 = _mm256_broadcast_sd(M2 + k3 + 383);
    _t77_3 = _asm256_loadu_pd(M1 + 20*k3 + 16);
    _t77_2 = _asm256_loadu_pd(M1 + 20*k3 + 36);
    _t77_1 = _asm256_loadu_pd(M1 + 20*k3 + 56);
    _t77_0 = _asm256_loadu_pd(M1 + 20*k3 + 76);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t77_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_19, _t77_3), _mm256_mul_pd(_t77_18, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_17, _t77_1), _mm256_mul_pd(_t77_16, _t77_0)));
    _t77_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_15, _t77_3), _mm256_mul_pd(_t77_14, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_13, _t77_1), _mm256_mul_pd(_t77_12, _t77_0)));
    _t77_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_11, _t77_3), _mm256_mul_pd(_t77_10, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_9, _t77_1), _mm256_mul_pd(_t77_8, _t77_0)));
    _t77_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t77_7, _t77_3), _mm256_mul_pd(_t77_6, _t77_2)), _mm256_add_pd(_mm256_mul_pd(_t77_5, _t77_1), _mm256_mul_pd(_t77_4, _t77_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t77_24 = _t10_3;
    _t77_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
    _t77_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
    _t77_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t77_24 = _mm256_sub_pd(_t77_24, _t77_20);
    _t77_25 = _mm256_sub_pd(_t77_25, _t77_21);
    _t77_26 = _mm256_sub_pd(_t77_26, _t77_22);
    _t77_27 = _mm256_sub_pd(_t77_27, _t77_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t10_3 = _t77_24;
    _t10_2 = _t77_25;
    _t10_1 = _t77_26;
    _t10_0 = _t77_27;
    _asm256_storeu_pd(P + 336, _t10_3);
    _mm256_maskstore_pd(P + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
    _mm256_maskstore_pd(P + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
    _mm256_maskstore_pd(P + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);
  }

  _asm256_storeu_pd(Y + 336, _t15_28);
  _mm256_maskstore_pd(Y + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
  _mm256_maskstore_pd(Y + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
  _mm256_maskstore_pd(Y + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  _mm_store_sd(&(v0[3]), _mm256_castpd256_pd128(_t59_0));
  _mm_store_sd(&(v0[2]), _mm256_castpd256_pd128(_t59_2));
  _mm_store_sd(&(v0[1]), _mm256_castpd256_pd128(_t59_4));
  _mm_store_sd(&(v0[0]), _mm256_castpd256_pd128(_t59_5));
  _asm256_storeu_pd(P + 336, _t10_3);
  _mm256_maskstore_pd(P + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
  _mm256_maskstore_pd(P + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
  _mm256_maskstore_pd(P + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);

}
