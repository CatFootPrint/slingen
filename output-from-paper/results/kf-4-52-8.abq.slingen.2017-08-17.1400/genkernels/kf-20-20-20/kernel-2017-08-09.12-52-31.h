/*
 * kf_kernel.h
 *
Decl { {u'B': SquaredMatrix[B, (20, 20), GenMatAccess], u'F': SquaredMatrix[F, (20, 20), GenMatAccess], 'T2407': Matrix[T2407, (1, 20), GenMatAccess], 'T2344': Matrix[T2344, (1, 20), GenMatAccess], u'U0': UpperTriangular[U0, (20, 20), GenMatAccess], u'M5': SquaredMatrix[M5, (20, 20), GenMatAccess], u'P': Symmetric[P, (20, 20), USMatAccess], u'M7': SquaredMatrix[M7, (20, 20), GenMatAccess], u'M6': SquaredMatrix[M6, (20, 20), GenMatAccess], u'v4': Matrix[v4, (20, 1), GenMatAccess], u'M0': SquaredMatrix[M0, (20, 20), GenMatAccess], u'M3': Symmetric[M3, (20, 20), USMatAccess], u'M2': SquaredMatrix[M2, (20, 20), GenMatAccess], u'Y': Symmetric[Y, (20, 20), USMatAccess], u'R': Symmetric[R, (20, 20), USMatAccess], u'U': UpperTriangular[U, (20, 20), GenMatAccess], u'M8': SquaredMatrix[M8, (20, 20), GenMatAccess], u'v0': Matrix[v0, (20, 1), GenMatAccess], u'u': Matrix[u, (20, 1), GenMatAccess], u'M4': Symmetric[M4, (20, 20), USMatAccess], u'v2': Matrix[v2, (20, 1), GenMatAccess], u'v1': Matrix[v1, (20, 1), GenMatAccess], u'v3': Matrix[v3, (20, 1), GenMatAccess], u'Q': Symmetric[Q, (20, 20), USMatAccess], u'x': Matrix[x, (20, 1), GenMatAccess], u'H': SquaredMatrix[H, (20, 20), GenMatAccess], u'y': Matrix[y, (20, 1), GenMatAccess], u'M1': SquaredMatrix[M1, (20, 20), GenMatAccess], u'z': Matrix[z, (20, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt': {'m0': 'm02.ll'}, 'ldiv_utn_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt_m02_Assign_Mul_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_ldiv_utn_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), u[20,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), P[20,20] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[20,20] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[20,20] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[20,20] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[20,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[20,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[20,20] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[20,20] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[20,20] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[20,20] ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(2, 20, 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U[20,20],h(1, 20, 3)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) = ( Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) Div Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 23:
For_{fi1304;0;12;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) ) ) )
Eq.ann: {}
 )Entry 24:
For_{fi1179;4;15;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1179), U[20,20],h(4, 20, fi1179)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1179), M4[20,20],h(4, 20, fi1179)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 20, 0), U[20,20],h(4, 20, fi1179)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 20, 0), U[20,20],h(4, 20, fi1179)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 20, fi1179), U[20,20],h(1, 20, fi1179)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi1179), U[20,20],h(1, 20, fi1179)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1179), U[20,20],h(1, 20, fi1179)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(3, 20, fi1179 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(3, 20, fi1179 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(1, 20, fi1179 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(1, 20, fi1179 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(1, 20, fi1179 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(1, 20, fi1179 + 1)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 20, fi1179 + 1), U[20,20],h(1, 20, fi1179 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi1179 + 1), U[20,20],h(1, 20, fi1179 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(2, 20, fi1179 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(2, 20, fi1179 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(1, 20, fi1179 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(2, 20, fi1179 + 2)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1179 + 1), U[20,20],h(1, 20, fi1179 + 1)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(2, 20, fi1179 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(2, 20, fi1179 + 2)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1179), U[20,20],h(1, 20, fi1179 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1179), U[20,20],h(1, 20, fi1179 + 2)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 2)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1179), U[20,20],h(1, 20, fi1179 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1179), U[20,20],h(1, 20, fi1179 + 3)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 3)) ) = ( Tile( (1, 1), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 3)) ) Div Tile( (1, 1), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 3), U[20,20],h(1, 20, fi1179 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 3), U[20,20],h(1, 20, fi1179 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1179), U[20,20],h(1, 20, fi1179 + 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1179), U[20,20],h(1, 20, fi1179 + 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 20, fi1179 + 3), U[20,20],h(1, 20, fi1179 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, fi1179 + 3), U[20,20],h(1, 20, fi1179 + 3)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1179), U[20,20],h(-fi1179 + 16, 20, fi1179 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1179), M4[20,20],h(-fi1179 + 16, 20, fi1179 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 20, 0), U[20,20],h(4, 20, fi1179)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 20, 0), U[20,20],h(-fi1179 + 16, 20, fi1179 + 4)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1179 + 3), U[20,20],h(1, 20, fi1179 + 3)) ) )
Eq.ann: {}
Entry 18:
For_{fi1429;0;-fi1179 + 12;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1179 + 1), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1179 + 1), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(3, 20, fi1179 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1179 + 2), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1179 + 2), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(2, 20, fi1179 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 1), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 3), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 3), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(1, 20, fi1179 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 2), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 3), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1179 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1179 + 3), U[20,20],h(4, 20, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
 ) )Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 20, 16), U[20,20],h(4, 20, 16)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, 16), M4[20,20],h(4, 20, 16)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(16, 20, 0), U[20,20],h(4, 20, 16)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(16, 20, 0), U[20,20],h(4, 20, 16)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 20, 16), U[20,20],h(1, 20, 16)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 16), U[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, 16)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 16), U[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U[20,20],h(3, 20, 17)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U[20,20],h(3, 20, 17)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U[20,20],h(1, 20, 17)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U[20,20],h(1, 20, 17)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U[20,20],h(2, 20, 18)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U[20,20],h(2, 20, 18)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U[20,20],h(1, 20, 17)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U[20,20],h(2, 20, 18)) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,20],h(1, 20, 17)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U[20,20],h(2, 20, 18)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,20],h(1, 20, 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U[20,20],h(2, 20, 18)) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 16), U[20,20],h(1, 20, 18)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 16), U[20,20],h(1, 20, 18)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U[20,20],h(1, 20, 19)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U[20,20],h(1, 20, 19)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 16), U[20,20],h(1, 20, 18)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 16), U[20,20],h(1, 20, 19)) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 20, 18), U[20,20],h(1, 20, 19)) ) = ( Tile( (1, 1), G(h(1, 20, 18), U[20,20],h(1, 20, 19)) ) Div Tile( (1, 1), G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), U[20,20],h(1, 20, 19)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), U[20,20],h(1, 20, 19)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 16), U[20,20],h(1, 20, 19)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 16), U[20,20],h(1, 20, 19)) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 20, 19), U[20,20],h(1, 20, 19)) ) = Sqrt( Tile( (1, 1), G(h(1, 20, 19), U[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 40:
For_{fi1554;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, fi1554), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1554), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1554), U0[20,20],h(1, 20, fi1554)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1554 + 1), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1554 + 1), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554), U0[20,20],h(3, 20, fi1554 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, fi1554 + 1), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1554 + 1), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1554 + 1), U0[20,20],h(1, 20, fi1554 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1554 + 2), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1554 + 2), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554 + 1), U0[20,20],h(2, 20, fi1554 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554 + 1), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, fi1554 + 2), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1554 + 2), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1554 + 2), U0[20,20],h(1, 20, fi1554 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554 + 3), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554 + 3), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554 + 2), U0[20,20],h(1, 20, fi1554 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1554 + 2), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, fi1554 + 3), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, fi1554 + 3), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, fi1554 + 3), U0[20,20],h(1, 20, fi1554 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1554 + 16, 20, fi1554 + 4), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1554 + 16, 20, fi1554 + 4), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1554), U0[20,20],h(-fi1554 + 16, 20, fi1554 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1554), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 41:
Eq: Tile( (1, 1), G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 48:
For_{fi1631;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 20, -fi1631 + 19), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1631 + 19), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1631 + 19), U0[20,20],h(1, 20, -fi1631 + 19)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1631 + 16), U0[20,20],h(1, 20, -fi1631 + 19)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1631 + 19), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 20, -fi1631 + 18), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1631 + 18), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1631 + 18), U0[20,20],h(1, 20, -fi1631 + 18)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1631 + 16), U0[20,20],h(1, 20, -fi1631 + 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1631 + 18), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 20, -fi1631 + 17), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1631 + 17), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1631 + 17), U0[20,20],h(1, 20, -fi1631 + 17)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1631 + 16), U0[20,20],h(1, 20, -fi1631 + 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1631 + 17), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, -fi1631 + 16), U0[20,20],h(1, 20, -fi1631 + 16)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1631 + 16, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1631 + 16, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1631 + 16, 20, 0), U0[20,20],h(4, 20, -fi1631 + 16)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, -fi1631 + 16), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 49:
Eq: Tile( (1, 1), G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 56:
For_{fi1708;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1708), U0[20,20],h(1, 20, fi1708)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1708 + 1), U0[20,20],h(1, 20, fi1708 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1708 + 2), U0[20,20],h(1, 20, fi1708 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, fi1708 + 3), U0[20,20],h(1, 20, fi1708 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi1727;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708), M6[20,20],h(4, 20, fi1727)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1708 + 1), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, fi1708 + 1), M6[20,20],h(4, 20, fi1727)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708), U0[20,20],h(3, 20, fi1708 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708), M6[20,20],h(4, 20, fi1727)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 1), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 1), M6[20,20],h(4, 20, fi1727)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1708 + 2), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, fi1708 + 2), M6[20,20],h(4, 20, fi1727)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 1), U0[20,20],h(2, 20, fi1708 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 1), M6[20,20],h(4, 20, fi1727)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 2), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 2), M6[20,20],h(4, 20, fi1727)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 3), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 3), M6[20,20],h(4, 20, fi1727)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 2), U0[20,20],h(1, 20, fi1708 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 2), M6[20,20],h(4, 20, fi1727)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 3), M6[20,20],h(4, 20, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1708 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, fi1708 + 3), M6[20,20],h(4, 20, fi1727)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1708 + 16, 20, fi1708 + 4), M6[20,20],h(20, 20, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1708 + 16, 20, fi1708 + 4), M6[20,20],h(20, 20, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1708), U0[20,20],h(-fi1708 + 16, 20, fi1708 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, fi1708), M6[20,20],h(20, 20, 0)) ) ) ) )
Eq.ann: {}
 )Entry 57:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 16)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 17)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ) )
Eq.ann: {}
Entry 59:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 18)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ) )
Eq.ann: {}
Entry 60:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 19)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ) )
Eq.ann: {}
Entry 61:
For_{fi1774;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), M6[20,20],h(4, 20, fi1774)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 17), M6[20,20],h(4, 20, fi1774)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 16), M6[20,20],h(4, 20, fi1774)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), M6[20,20],h(4, 20, fi1774)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 18), M6[20,20],h(4, 20, fi1774)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 17), M6[20,20],h(4, 20, fi1774)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), M6[20,20],h(4, 20, fi1774)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1774)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 18), M6[20,20],h(4, 20, fi1774)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 19)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 19), M6[20,20],h(4, 20, fi1774)) ) ) )
Eq.ann: {}
 )Entry 62:
For_{fi1821;0;15;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 19)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1821 + 19), U0[20,20],h(1, 20, -fi1821 + 19)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 18)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1821 + 18), U0[20,20],h(1, 20, -fi1821 + 18)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 17)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1821 + 17), U0[20,20],h(1, 20, -fi1821 + 17)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 16)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, -fi1821 + 16), U0[20,20],h(1, 20, -fi1821 + 16)) ) )
Eq.ann: {}
Entry 4:
For_{fi1840;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 19), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 19)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 19), M8[20,20],h(4, 20, fi1840)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, -fi1821 + 16), U0[20,20],h(1, 20, -fi1821 + 19)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 19), M8[20,20],h(4, 20, fi1840)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 18), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 18)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 18), M8[20,20],h(4, 20, fi1840)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, -fi1821 + 16), U0[20,20],h(1, 20, -fi1821 + 18)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 18), M8[20,20],h(4, 20, fi1840)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 17), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 17), M8[20,20],h(4, 20, fi1840)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 16), U0[20,20],h(1, 20, -fi1821 + 17)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 17), M8[20,20],h(4, 20, fi1840)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1821 + 16)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, -fi1821 + 16), M8[20,20],h(4, 20, fi1840)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1821 + 16, 20, 0), M8[20,20],h(20, 20, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1821 + 16, 20, 0), M8[20,20],h(20, 20, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1821 + 16, 20, 0), U0[20,20],h(4, 20, -fi1821 + 16)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 20, -fi1821 + 16), M8[20,20],h(20, 20, 0)) ) ) ) )
Eq.ann: {}
 )Entry 63:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ) )
Eq.ann: {}
Entry 64:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ) )
Eq.ann: {}
Entry 65:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ) )
Eq.ann: {}
Entry 66:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,20],h(1, 20, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ) )
Eq.ann: {}
Entry 67:
For_{fi1887;0;16;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), M8[20,20],h(4, 20, fi1887)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 3), M8[20,20],h(4, 20, fi1887)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), M8[20,20],h(4, 20, fi1887)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 2), M8[20,20],h(4, 20, fi1887)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), M8[20,20],h(4, 20, fi1887)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 1), M8[20,20],h(4, 20, fi1887)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,20],h(1, 20, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 20, 0), M8[20,20],h(4, 20, fi1887)) ) ) )
Eq.ann: {}
 )Entry 68:
Eq: Tile( (1, 1), Tile( (4, 4), x[20,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[20,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), v0[20,1] ) ) ) )
Eq.ann: {}
Entry 69:
Eq: Tile( (1, 1), Tile( (4, 4), P[20,20] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[20,20] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[20,20] ) ) * Tile( (1, 1), Tile( (4, 4), M1[20,20] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 20
#define PARAM1 20
#define PARAM2 20

#define ERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27, _t13_28, _t13_29, _t13_30, _t13_31,
	_t13_32, _t13_33, _t13_34, _t13_35, _t13_36, _t13_37, _t13_38, _t13_39,
	_t13_40, _t13_41, _t13_42, _t13_43;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27, _t25_28, _t25_29, _t25_30, _t25_31;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27, _t35_28, _t35_29, _t35_30, _t35_31;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11, _t38_12, _t38_13, _t38_14, _t38_15,
	_t38_16, _t38_17, _t38_18, _t38_19, _t38_20, _t38_21, _t38_22, _t38_23;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31,
	_t42_32, _t42_33, _t42_34, _t42_35, _t42_36, _t42_37, _t42_38, _t42_39,
	_t42_40, _t42_41, _t42_42, _t42_43;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31,
	_t44_32, _t44_33, _t44_34, _t44_35, _t44_36, _t44_37, _t44_38, _t44_39,
	_t44_40, _t44_41, _t44_42, _t44_43;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27, _t45_28, _t45_29, _t45_30, _t45_31,
	_t45_32, _t45_33, _t45_34, _t45_35;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11, _t46_12, _t46_13, _t46_14, _t46_15;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27, _t47_28, _t47_29, _t47_30, _t47_31;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35, _t48_36, _t48_37, _t48_38, _t48_39,
	_t48_40, _t48_41, _t48_42, _t48_43, _t48_44, _t48_45, _t48_46, _t48_47,
	_t48_48, _t48_49, _t48_50, _t48_51, _t48_52, _t48_53, _t48_54, _t48_55,
	_t48_56, _t48_57, _t48_58, _t48_59, _t48_60, _t48_61, _t48_62, _t48_63,
	_t48_64, _t48_65, _t48_66, _t48_67, _t48_68, _t48_69, _t48_70, _t48_71,
	_t48_72, _t48_73, _t48_74, _t48_75, _t48_76, _t48_77, _t48_78, _t48_79,
	_t48_80, _t48_81, _t48_82, _t48_83, _t48_84, _t48_85, _t48_86, _t48_87,
	_t48_88, _t48_89, _t48_90, _t48_91, _t48_92, _t48_93, _t48_94, _t48_95,
	_t48_96, _t48_97, _t48_98, _t48_99, _t48_100, _t48_101, _t48_102, _t48_103,
	_t48_104, _t48_105, _t48_106, _t48_107, _t48_108;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23,
	_t50_24, _t50_25, _t50_26, _t50_27, _t50_28, _t50_29, _t50_30, _t50_31,
	_t50_32, _t50_33, _t50_34, _t50_35, _t50_36, _t50_37, _t50_38, _t50_39,
	_t50_40, _t50_41, _t50_42, _t50_43, _t50_44, _t50_45, _t50_46, _t50_47,
	_t50_48, _t50_49, _t50_50, _t50_51, _t50_52, _t50_53, _t50_54, _t50_55,
	_t50_56, _t50_57, _t50_58, _t50_59, _t50_60, _t50_61, _t50_62, _t50_63,
	_t50_64, _t50_65, _t50_66, _t50_67, _t50_68, _t50_69, _t50_70, _t50_71,
	_t50_72, _t50_73, _t50_74, _t50_75, _t50_76, _t50_77, _t50_78, _t50_79,
	_t50_80, _t50_81, _t50_82, _t50_83, _t50_84, _t50_85;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19, _t52_20, _t52_21, _t52_22, _t52_23,
	_t52_24, _t52_25, _t52_26, _t52_27, _t52_28, _t52_29, _t52_30, _t52_31,
	_t52_32, _t52_33, _t52_34, _t52_35, _t52_36, _t52_37, _t52_38, _t52_39,
	_t52_40, _t52_41, _t52_42, _t52_43, _t52_44, _t52_45, _t52_46;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39,
	_t54_40, _t54_41, _t54_42, _t54_43, _t54_44, _t54_45, _t54_46, _t54_47,
	_t54_48, _t54_49, _t54_50, _t54_51, _t54_52, _t54_53, _t54_54, _t54_55,
	_t54_56, _t54_57, _t54_58, _t54_59, _t54_60, _t54_61, _t54_62, _t54_63,
	_t54_64, _t54_65, _t54_66, _t54_67, _t54_68, _t54_69, _t54_70, _t54_71,
	_t54_72, _t54_73, _t54_74, _t54_75, _t54_76, _t54_77, _t54_78, _t54_79,
	_t54_80, _t54_81, _t54_82, _t54_83, _t54_84, _t54_85, _t54_86, _t54_87,
	_t54_88, _t54_89, _t54_90, _t54_91, _t54_92, _t54_93, _t54_94, _t54_95,
	_t54_96, _t54_97, _t54_98, _t54_99, _t54_100, _t54_101;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9, _t58_10, _t58_11, _t58_12, _t58_13, _t58_14, _t58_15,
	_t58_16, _t58_17, _t58_18, _t58_19, _t58_20, _t58_21, _t58_22, _t58_23,
	_t58_24, _t58_25, _t58_26, _t58_27, _t58_28, _t58_29, _t58_30, _t58_31,
	_t58_32, _t58_33, _t58_34, _t58_35, _t58_36, _t58_37, _t58_38, _t58_39,
	_t58_40, _t58_41, _t58_42, _t58_43, _t58_44, _t58_45, _t58_46, _t58_47,
	_t58_48, _t58_49, _t58_50, _t58_51, _t58_52, _t58_53, _t58_54, _t58_55,
	_t58_56, _t58_57, _t58_58, _t58_59, _t58_60, _t58_61, _t58_62, _t58_63,
	_t58_64, _t58_65, _t58_66, _t58_67, _t58_68, _t58_69, _t58_70, _t58_71,
	_t58_72, _t58_73, _t58_74, _t58_75, _t58_76, _t58_77, _t58_78, _t58_79,
	_t58_80, _t58_81, _t58_82, _t58_83, _t58_84, _t58_85;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11, _t60_12, _t60_13, _t60_14, _t60_15,
	_t60_16, _t60_17, _t60_18, _t60_19, _t60_20, _t60_21, _t60_22, _t60_23,
	_t60_24, _t60_25, _t60_26, _t60_27, _t60_28, _t60_29, _t60_30, _t60_31,
	_t60_32, _t60_33, _t60_34, _t60_35, _t60_36, _t60_37, _t60_38, _t60_39,
	_t60_40, _t60_41, _t60_42, _t60_43, _t60_44, _t60_45, _t60_46, _t60_47,
	_t60_48, _t60_49, _t60_50, _t60_51, _t60_52, _t60_53, _t60_54, _t60_55,
	_t60_56, _t60_57, _t60_58, _t60_59, _t60_60, _t60_61, _t60_62, _t60_63;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19, _t62_20, _t62_21, _t62_22, _t62_23,
	_t62_24, _t62_25, _t62_26, _t62_27, _t62_28, _t62_29, _t62_30, _t62_31,
	_t62_32, _t62_33, _t62_34, _t62_35, _t62_36, _t62_37, _t62_38, _t62_39,
	_t62_40, _t62_41, _t62_42, _t62_43, _t62_44, _t62_45, _t62_46;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6, _t63_7,
	_t63_8, _t63_9, _t63_10, _t63_11, _t63_12, _t63_13, _t63_14, _t63_15;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15,
	_t64_16, _t64_17, _t64_18, _t64_19, _t64_20, _t64_21, _t64_22, _t64_23,
	_t64_24, _t64_25, _t64_26, _t64_27, _t64_28, _t64_29, _t64_30, _t64_31,
	_t64_32, _t64_33, _t64_34, _t64_35, _t64_36, _t64_37, _t64_38, _t64_39,
	_t64_40, _t64_41, _t64_42, _t64_43, _t64_44, _t64_45, _t64_46, _t64_47,
	_t64_48, _t64_49, _t64_50, _t64_51, _t64_52, _t64_53, _t64_54, _t64_55,
	_t64_56, _t64_57, _t64_58, _t64_59, _t64_60, _t64_61;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15,
	_t65_16, _t65_17, _t65_18, _t65_19, _t65_20, _t65_21, _t65_22, _t65_23,
	_t65_24, _t65_25, _t65_26, _t65_27, _t65_28, _t65_29, _t65_30, _t65_31,
	_t65_32, _t65_33, _t65_34, _t65_35, _t65_36, _t65_37, _t65_38, _t65_39,
	_t65_40, _t65_41, _t65_42;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15,
	_t67_16, _t67_17, _t67_18, _t67_19, _t67_20, _t67_21, _t67_22, _t67_23,
	_t67_24, _t67_25, _t67_26, _t67_27, _t67_28, _t67_29, _t67_30, _t67_31,
	_t67_32, _t67_33, _t67_34, _t67_35;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15,
	_t68_16, _t68_17, _t68_18, _t68_19, _t68_20, _t68_21, _t68_22, _t68_23,
	_t68_24, _t68_25, _t68_26, _t68_27, _t68_28, _t68_29, _t68_30, _t68_31,
	_t68_32, _t68_33, _t68_34, _t68_35, _t68_36, _t68_37, _t68_38, _t68_39;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6, _t70_7,
	_t70_8, _t70_9, _t70_10, _t70_11, _t70_12, _t70_13, _t70_14, _t70_15,
	_t70_16, _t70_17, _t70_18, _t70_19, _t70_20, _t70_21, _t70_22, _t70_23,
	_t70_24, _t70_25, _t70_26, _t70_27, _t70_28, _t70_29, _t70_30, _t70_31,
	_t70_32;
  __m256d _t71_0, _t71_1, _t71_2, _t71_3, _t71_4, _t71_5, _t71_6, _t71_7,
	_t71_8, _t71_9, _t71_10, _t71_11, _t71_12, _t71_13, _t71_14, _t71_15,
	_t71_16, _t71_17, _t71_18, _t71_19, _t71_20, _t71_21, _t71_22, _t71_23,
	_t71_24, _t71_25, _t71_26, _t71_27, _t71_28, _t71_29, _t71_30, _t71_31,
	_t71_32, _t71_33, _t71_34, _t71_35, _t71_36, _t71_37, _t71_38, _t71_39,
	_t71_40, _t71_41, _t71_42, _t71_43, _t71_44, _t71_45, _t71_46, _t71_47,
	_t71_48, _t71_49, _t71_50, _t71_51, _t71_52, _t71_53, _t71_54, _t71_55,
	_t71_56, _t71_57, _t71_58, _t71_59, _t71_60, _t71_61;
  __m256d _t72_0, _t72_1, _t72_2, _t72_3, _t72_4, _t72_5, _t72_6, _t72_7,
	_t72_8, _t72_9, _t72_10, _t72_11, _t72_12, _t72_13, _t72_14, _t72_15,
	_t72_16, _t72_17, _t72_18, _t72_19, _t72_20, _t72_21, _t72_22, _t72_23,
	_t72_24, _t72_25, _t72_26;
  __m256d _t73_0, _t73_1, _t73_2, _t73_3, _t73_4, _t73_5, _t73_6, _t73_7,
	_t73_8, _t73_9, _t73_10, _t73_11, _t73_12, _t73_13, _t73_14, _t73_15,
	_t73_16, _t73_17, _t73_18, _t73_19;
  __m256d _t74_0, _t74_1, _t74_2, _t74_3, _t74_4, _t74_5, _t74_6, _t74_7,
	_t74_8, _t74_9, _t74_10, _t74_11, _t74_12, _t74_13, _t74_14, _t74_15,
	_t74_16, _t74_17, _t74_18, _t74_19, _t74_20, _t74_21, _t74_22, _t74_23,
	_t74_24, _t74_25, _t74_26, _t74_27, _t74_28, _t74_29, _t74_30, _t74_31,
	_t74_32, _t74_33, _t74_34, _t74_35, _t74_36, _t74_37, _t74_38, _t74_39,
	_t74_40, _t74_41, _t74_42, _t74_43, _t74_44, _t74_45, _t74_46, _t74_47,
	_t74_48, _t74_49, _t74_50, _t74_51, _t74_52, _t74_53, _t74_54;
  __m256d _t75_0, _t75_1, _t75_2, _t75_3, _t75_4, _t75_5, _t75_6, _t75_7,
	_t75_8, _t75_9, _t75_10, _t75_11, _t75_12, _t75_13, _t75_14, _t75_15,
	_t75_16, _t75_17, _t75_18, _t75_19, _t75_20, _t75_21, _t75_22, _t75_23,
	_t75_24, _t75_25, _t75_26;
  __m256d _t76_0, _t76_1, _t76_2, _t76_3, _t76_4, _t76_5, _t76_6, _t76_7,
	_t76_8, _t76_9, _t76_10, _t76_11, _t76_12, _t76_13, _t76_14, _t76_15,
	_t76_16, _t76_17, _t76_18, _t76_19, _t76_20, _t76_21, _t76_22, _t76_23,
	_t76_24, _t76_25, _t76_26, _t76_27, _t76_28, _t76_29, _t76_30, _t76_31,
	_t76_32, _t76_33, _t76_34, _t76_35, _t76_36, _t76_37, _t76_38, _t76_39,
	_t76_40, _t76_41, _t76_42, _t76_43, _t76_44, _t76_45, _t76_46, _t76_47,
	_t76_48, _t76_49, _t76_50, _t76_51, _t76_52, _t76_53, _t76_54, _t76_55,
	_t76_56, _t76_57, _t76_58;
  __m256d _t77_0, _t77_1, _t77_2, _t77_3, _t77_4, _t77_5, _t77_6, _t77_7,
	_t77_8, _t77_9, _t77_10, _t77_11, _t77_12, _t77_13, _t77_14, _t77_15,
	_t77_16, _t77_17, _t77_18, _t77_19, _t77_20, _t77_21, _t77_22, _t77_23,
	_t77_24, _t77_25, _t77_26;
  __m256d _t78_0, _t78_1, _t78_2, _t78_3, _t78_4, _t78_5, _t78_6, _t78_7,
	_t78_8, _t78_9, _t78_10, _t78_11, _t78_12, _t78_13, _t78_14, _t78_15,
	_t78_16, _t78_17, _t78_18, _t78_19, _t78_20, _t78_21, _t78_22, _t78_23,
	_t78_24, _t78_25, _t78_26, _t78_27;
  __m256d _t79_0, _t79_1, _t79_2, _t79_3, _t79_4, _t79_5, _t79_6, _t79_7,
	_t79_8, _t79_9, _t79_10, _t79_11, _t79_12, _t79_13, _t79_14, _t79_15,
	_t79_16, _t79_17, _t79_18, _t79_19, _t79_20, _t79_21, _t79_22, _t79_23,
	_t79_24, _t79_25, _t79_26, _t79_27, _t79_28, _t79_29, _t79_30, _t79_31,
	_t79_32, _t79_33, _t79_34, _t79_35, _t79_36, _t79_37, _t79_38, _t79_39,
	_t79_40, _t79_41, _t79_42, _t79_43, _t79_44, _t79_45, _t79_46, _t79_47,
	_t79_48, _t79_49, _t79_50, _t79_51;
  __m256d _t80_0, _t80_1, _t80_2, _t80_3, _t80_4, _t80_5, _t80_6, _t80_7,
	_t80_8, _t80_9, _t80_10, _t80_11, _t80_12, _t80_13, _t80_14, _t80_15,
	_t80_16, _t80_17, _t80_18, _t80_19, _t80_20, _t80_21, _t80_22, _t80_23,
	_t80_24, _t80_25, _t80_26;
  __m256d _t81_0, _t81_1, _t81_2, _t81_3, _t81_4, _t81_5, _t81_6;
  __m256d _t82_0, _t82_1, _t82_2, _t82_3, _t82_4, _t82_5, _t82_6;
  __m256d _t83_0, _t83_1, _t83_2, _t83_3, _t83_4, _t83_5, _t83_6, _t83_7,
	_t83_8, _t83_9, _t83_10, _t83_11, _t83_12, _t83_13, _t83_14, _t83_15,
	_t83_16, _t83_17, _t83_18, _t83_19, _t83_20, _t83_21, _t83_22, _t83_23,
	_t83_24, _t83_25, _t83_26, _t83_27, _t83_28, _t83_29, _t83_30, _t83_31,
	_t83_32, _t83_33, _t83_34, _t83_35, _t83_36, _t83_37, _t83_38, _t83_39;
  __m256d _t84_0, _t84_1, _t84_2, _t84_3, _t84_4, _t84_5, _t84_6, _t84_7,
	_t84_8, _t84_9, _t84_10, _t84_11, _t84_12, _t84_13, _t84_14, _t84_15;
  __m256d _t85_0, _t85_1, _t85_2, _t85_3, _t85_4, _t85_5, _t85_6, _t85_7,
	_t85_8, _t85_9, _t85_10, _t85_11, _t85_12, _t85_13, _t85_14, _t85_15,
	_t85_16, _t85_17, _t85_18, _t85_19, _t85_20, _t85_21, _t85_22, _t85_23,
	_t85_24, _t85_25, _t85_26, _t85_27, _t85_28, _t85_29, _t85_30, _t85_31;
  __m256d _t86_0, _t86_1, _t86_2, _t86_3, _t86_4, _t86_5, _t86_6, _t86_7,
	_t86_8, _t86_9, _t86_10, _t86_11, _t86_12, _t86_13, _t86_14, _t86_15,
	_t86_16, _t86_17, _t86_18, _t86_19, _t86_20, _t86_21, _t86_22, _t86_23,
	_t86_24, _t86_25, _t86_26, _t86_27, _t86_28, _t86_29, _t86_30, _t86_31;
  __m256d _t87_0, _t87_1, _t87_2, _t87_3, _t87_4, _t87_5, _t87_6, _t87_7,
	_t87_8, _t87_9, _t87_10, _t87_11;
  __m256d _t88_0, _t88_1, _t88_2, _t88_3, _t88_4, _t88_5, _t88_6, _t88_7,
	_t88_8, _t88_9, _t88_10, _t88_11, _t88_12, _t88_13, _t88_14, _t88_15,
	_t88_16, _t88_17, _t88_18, _t88_19, _t88_20, _t88_21, _t88_22, _t88_23,
	_t88_24, _t88_25, _t88_26, _t88_27;


  // Generating : y[20,1] = ( ( Sum_{i0} ( S(h(4, 20, i0), ( ( G(h(4, 20, i0), F[20,20],h(4, 20, 0)) * G(h(4, 20, 0), x[20,1],h(1, 1, 0)) ) + ( G(h(4, 20, i0), B[20,20],h(4, 20, 0)) * G(h(4, 20, 0), u[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 20, i0), ( G(h(4, 20, i0), F[20,20],h(4, 20, k2)) * G(h(4, 20, k2), x[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) ) + Sum_{k3} ( Sum_{i0} ( $(h(4, 20, i0), ( G(h(4, 20, i0), B[20,20],h(4, 20, k3)) * G(h(4, 20, k3), u[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t0_9 = _asm256_loadu_pd(F + 20*i0);
    _t0_8 = _asm256_loadu_pd(F + 20*i0 + 20);
    _t0_7 = _asm256_loadu_pd(F + 20*i0 + 40);
    _t0_6 = _asm256_loadu_pd(F + 20*i0 + 60);
    _t0_5 = _asm256_loadu_pd(x);
    _t0_4 = _asm256_loadu_pd(B + 20*i0);
    _t0_3 = _asm256_loadu_pd(B + 20*i0 + 20);
    _t0_2 = _asm256_loadu_pd(B + 20*i0 + 40);
    _t0_1 = _asm256_loadu_pd(B + 20*i0 + 60);
    _t0_0 = _asm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:
    _asm256_storeu_pd(y + i0, _t0_10);
  }


  for( int k2 = 4; k2 <= 19; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t1_4 = _asm256_loadu_pd(F + 20*i0 + k2);
      _t1_3 = _asm256_loadu_pd(F + 20*i0 + k2 + 20);
      _t1_2 = _asm256_loadu_pd(F + 20*i0 + k2 + 40);
      _t1_1 = _asm256_loadu_pd(F + 20*i0 + k2 + 60);
      _t1_0 = _asm256_loadu_pd(x + k2);
      _t1_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t1_5 = _mm256_add_pd(_t1_5, _t1_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t1_5);
    }
  }


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t2_4 = _asm256_loadu_pd(B + 20*i0 + k3);
      _t2_3 = _asm256_loadu_pd(B + 20*i0 + k3 + 20);
      _t2_2 = _asm256_loadu_pd(B + 20*i0 + k3 + 40);
      _t2_1 = _asm256_loadu_pd(B + 20*i0 + k3 + 60);
      _t2_0 = _asm256_loadu_pd(u + k3);
      _t2_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t2_5 = _mm256_add_pd(_t2_5, _t2_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t2_5);
    }
  }

  _t3_3 = _asm256_loadu_pd(P);
  _t3_2 = _mm256_maskload_pd(P + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 0)) * G(h(4, 20, 0), P[20,20],h(4, 20, 0)) ),h(4, 20, 0)) + Sum_{i0} ( S(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 0)) * G(h(4, 20, 0), P[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), P[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) * G(h(4, 20, k3), P[20,20],h(4, 20, k3)) ),h(4, 20, k3)) ) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) * G(h(4, 20, k3), P[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), P[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), F[20,20],h(4, 20, 16)) * G(h(4, 20, 16), P[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_4 = _t3_3;
  _t3_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 20*k2);
    _t4_14 = _mm256_broadcast_sd(F + 20*k2 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 20*k2 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 20*k2 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 20*k2 + 20);
    _t4_10 = _mm256_broadcast_sd(F + 20*k2 + 21);
    _t4_9 = _mm256_broadcast_sd(F + 20*k2 + 22);
    _t4_8 = _mm256_broadcast_sd(F + 20*k2 + 23);
    _t4_7 = _mm256_broadcast_sd(F + 20*k2 + 40);
    _t4_6 = _mm256_broadcast_sd(F + 20*k2 + 41);
    _t4_5 = _mm256_broadcast_sd(F + 20*k2 + 42);
    _t4_4 = _mm256_broadcast_sd(F + 20*k2 + 43);
    _t4_3 = _mm256_broadcast_sd(F + 20*k2 + 60);
    _t4_2 = _mm256_broadcast_sd(F + 20*k2 + 61);
    _t4_1 = _mm256_broadcast_sd(F + 20*k2 + 62);
    _t4_0 = _mm256_broadcast_sd(F + 20*k2 + 63);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_4), _mm256_mul_pd(_t4_14, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_6), _mm256_mul_pd(_t4_12, _t3_7)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_4), _mm256_mul_pd(_t4_10, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_6), _mm256_mul_pd(_t4_8, _t3_7)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_4), _mm256_mul_pd(_t4_6, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_6), _mm256_mul_pd(_t4_4, _t3_7)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_4), _mm256_mul_pd(_t4_2, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_6), _mm256_mul_pd(_t4_0, _t3_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 19; i0+=4 ) {
      _t5_3 = _asm256_loadu_pd(P + i0);
      _t5_2 = _asm256_loadu_pd(P + i0 + 20);
      _t5_1 = _asm256_loadu_pd(P + i0 + 40);
      _t5_0 = _asm256_loadu_pd(P + i0 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t5_3), _mm256_mul_pd(_t4_14, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t5_1), _mm256_mul_pd(_t4_12, _t5_0)));
      _t5_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t5_3), _mm256_mul_pd(_t4_10, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t5_1), _mm256_mul_pd(_t4_8, _t5_0)));
      _t5_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t5_3), _mm256_mul_pd(_t4_6, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t5_1), _mm256_mul_pd(_t4_4, _t5_0)));
      _t5_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t5_3), _mm256_mul_pd(_t4_2, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t5_1), _mm256_mul_pd(_t4_0, _t5_0)));

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 20*k2, _t5_4);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t5_5);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t5_6);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t5_7);
    }
    _asm256_storeu_pd(M0 + 20*k2, _t4_16);
    _asm256_storeu_pd(M0 + 20*k2 + 20, _t4_17);
    _asm256_storeu_pd(M0 + 20*k2 + 40, _t4_18);
    _asm256_storeu_pd(M0 + 20*k2 + 60, _t4_19);
  }


  for( int k3 = 4; k3 <= 15; k3+=4 ) {
    _t6_3 = _asm256_loadu_pd(P + 21*k3);
    _t6_2 = _mm256_maskload_pd(P + 21*k3 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t6_1 = _mm256_maskload_pd(P + 21*k3 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t6_0 = _mm256_maskload_pd(P + 21*k3 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t6_4 = _t6_3;
    _t6_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
    _t6_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
    _t6_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

    for( int k2 = 0; k2 <= 19; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t7_19 = _mm256_broadcast_sd(F + 20*k2 + k3);
        _t7_18 = _mm256_broadcast_sd(F + 20*k2 + k3 + 1);
        _t7_17 = _mm256_broadcast_sd(F + 20*k2 + k3 + 2);
        _t7_16 = _mm256_broadcast_sd(F + 20*k2 + k3 + 3);
        _t7_15 = _mm256_broadcast_sd(F + 20*k2 + k3 + 20);
        _t7_14 = _mm256_broadcast_sd(F + 20*k2 + k3 + 21);
        _t7_13 = _mm256_broadcast_sd(F + 20*k2 + k3 + 22);
        _t7_12 = _mm256_broadcast_sd(F + 20*k2 + k3 + 23);
        _t7_11 = _mm256_broadcast_sd(F + 20*k2 + k3 + 40);
        _t7_10 = _mm256_broadcast_sd(F + 20*k2 + k3 + 41);
        _t7_9 = _mm256_broadcast_sd(F + 20*k2 + k3 + 42);
        _t7_8 = _mm256_broadcast_sd(F + 20*k2 + k3 + 43);
        _t7_7 = _mm256_broadcast_sd(F + 20*k2 + k3 + 60);
        _t7_6 = _mm256_broadcast_sd(F + 20*k2 + k3 + 61);
        _t7_5 = _mm256_broadcast_sd(F + 20*k2 + k3 + 62);
        _t7_4 = _mm256_broadcast_sd(F + 20*k2 + k3 + 63);
        _t7_3 = _asm256_loadu_pd(P + 20*i0 + k3);
        _t7_2 = _asm256_loadu_pd(P + 20*i0 + k3 + 20);
        _t7_1 = _asm256_loadu_pd(P + 20*i0 + k3 + 40);
        _t7_0 = _asm256_loadu_pd(P + 20*i0 + k3 + 60);
        _t7_20 = _asm256_loadu_pd(M0 + i0 + 20*k2);
        _t7_21 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 20);
        _t7_22 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 40);
        _t7_23 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t7_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
        _t7_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
        _t7_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
        _t7_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t7_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_28), _mm256_mul_pd(_t7_18, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_30), _mm256_mul_pd(_t7_16, _t7_31)));
        _t7_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_28), _mm256_mul_pd(_t7_14, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_30), _mm256_mul_pd(_t7_12, _t7_31)));
        _t7_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_28), _mm256_mul_pd(_t7_10, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_30), _mm256_mul_pd(_t7_8, _t7_31)));
        _t7_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_28), _mm256_mul_pd(_t7_6, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_30), _mm256_mul_pd(_t7_4, _t7_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t7_20 = _mm256_add_pd(_t7_20, _t7_24);
        _t7_21 = _mm256_add_pd(_t7_21, _t7_25);
        _t7_22 = _mm256_add_pd(_t7_22, _t7_26);
        _t7_23 = _mm256_add_pd(_t7_23, _t7_27);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 20*k2, _t7_20);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t7_21);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t7_22);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t7_23);
      }
      _t8_15 = _mm256_broadcast_sd(F + 20*k2 + k3);
      _t8_14 = _mm256_broadcast_sd(F + 20*k2 + k3 + 1);
      _t8_13 = _mm256_broadcast_sd(F + 20*k2 + k3 + 2);
      _t8_12 = _mm256_broadcast_sd(F + 20*k2 + k3 + 3);
      _t8_11 = _mm256_broadcast_sd(F + 20*k2 + k3 + 20);
      _t8_10 = _mm256_broadcast_sd(F + 20*k2 + k3 + 21);
      _t8_9 = _mm256_broadcast_sd(F + 20*k2 + k3 + 22);
      _t8_8 = _mm256_broadcast_sd(F + 20*k2 + k3 + 23);
      _t8_7 = _mm256_broadcast_sd(F + 20*k2 + k3 + 40);
      _t8_6 = _mm256_broadcast_sd(F + 20*k2 + k3 + 41);
      _t8_5 = _mm256_broadcast_sd(F + 20*k2 + k3 + 42);
      _t8_4 = _mm256_broadcast_sd(F + 20*k2 + k3 + 43);
      _t8_3 = _mm256_broadcast_sd(F + 20*k2 + k3 + 60);
      _t8_2 = _mm256_broadcast_sd(F + 20*k2 + k3 + 61);
      _t8_1 = _mm256_broadcast_sd(F + 20*k2 + k3 + 62);
      _t8_0 = _mm256_broadcast_sd(F + 20*k2 + k3 + 63);
      _t8_16 = _asm256_loadu_pd(M0 + 20*k2 + k3);
      _t8_17 = _asm256_loadu_pd(M0 + 20*k2 + k3 + 20);
      _t8_18 = _asm256_loadu_pd(M0 + 20*k2 + k3 + 40);
      _t8_19 = _asm256_loadu_pd(M0 + 20*k2 + k3 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t8_24 = _t6_3;
      _t8_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
      _t8_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
      _t8_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t8_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_24), _mm256_mul_pd(_t8_14, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_26), _mm256_mul_pd(_t8_12, _t8_27)));
      _t8_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_24), _mm256_mul_pd(_t8_10, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_26), _mm256_mul_pd(_t8_8, _t8_27)));
      _t8_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_24), _mm256_mul_pd(_t8_6, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_26), _mm256_mul_pd(_t8_4, _t8_27)));
      _t8_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t8_24), _mm256_mul_pd(_t8_2, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t8_26), _mm256_mul_pd(_t8_0, _t8_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_16 = _mm256_add_pd(_t8_16, _t8_20);
      _t8_17 = _mm256_add_pd(_t8_17, _t8_21);
      _t8_18 = _mm256_add_pd(_t8_18, _t8_22);
      _t8_19 = _mm256_add_pd(_t8_19, _t8_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M0 + 20*k2 + k3, _t8_16);
      _asm256_storeu_pd(M0 + 20*k2 + k3 + 20, _t8_17);
      _asm256_storeu_pd(M0 + 20*k2 + k3 + 40, _t8_18);
      _asm256_storeu_pd(M0 + 20*k2 + k3 + 60, _t8_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t9_3 = _asm256_loadu_pd(P + i0 + 20*k3);
        _t9_2 = _asm256_loadu_pd(P + i0 + 20*k3 + 20);
        _t9_1 = _asm256_loadu_pd(P + i0 + 20*k3 + 40);
        _t9_0 = _asm256_loadu_pd(P + i0 + 20*k3 + 60);
        _t9_4 = _asm256_loadu_pd(M0 + i0 + 20*k2);
        _t9_5 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 20);
        _t9_6 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 40);
        _t9_7 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t9_3), _mm256_mul_pd(_t8_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t9_1), _mm256_mul_pd(_t8_12, _t9_0)));
        _t9_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t9_3), _mm256_mul_pd(_t8_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t9_1), _mm256_mul_pd(_t8_8, _t9_0)));
        _t9_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t9_3), _mm256_mul_pd(_t8_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t9_1), _mm256_mul_pd(_t8_4, _t9_0)));
        _t9_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t9_3), _mm256_mul_pd(_t8_2, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t9_1), _mm256_mul_pd(_t8_0, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t9_4 = _mm256_add_pd(_t9_4, _t9_8);
        _t9_5 = _mm256_add_pd(_t9_5, _t9_9);
        _t9_6 = _mm256_add_pd(_t9_6, _t9_10);
        _t9_7 = _mm256_add_pd(_t9_7, _t9_11);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 20*k2, _t9_4);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t9_5);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t9_6);
        _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t9_7);
      }
    }
  }

  _t10_3 = _asm256_loadu_pd(P + 336);
  _t10_2 = _mm256_maskload_pd(P + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t10_1 = _mm256_maskload_pd(P + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t10_0 = _mm256_maskload_pd(P + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t10_4 = _t10_3;
  _t10_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
  _t10_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
  _t10_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 15; i0+=4 ) {
      _t11_19 = _mm256_broadcast_sd(F + 20*k2 + 16);
      _t11_18 = _mm256_broadcast_sd(F + 20*k2 + 17);
      _t11_17 = _mm256_broadcast_sd(F + 20*k2 + 18);
      _t11_16 = _mm256_broadcast_sd(F + 20*k2 + 19);
      _t11_15 = _mm256_broadcast_sd(F + 20*k2 + 36);
      _t11_14 = _mm256_broadcast_sd(F + 20*k2 + 37);
      _t11_13 = _mm256_broadcast_sd(F + 20*k2 + 38);
      _t11_12 = _mm256_broadcast_sd(F + 20*k2 + 39);
      _t11_11 = _mm256_broadcast_sd(F + 20*k2 + 56);
      _t11_10 = _mm256_broadcast_sd(F + 20*k2 + 57);
      _t11_9 = _mm256_broadcast_sd(F + 20*k2 + 58);
      _t11_8 = _mm256_broadcast_sd(F + 20*k2 + 59);
      _t11_7 = _mm256_broadcast_sd(F + 20*k2 + 76);
      _t11_6 = _mm256_broadcast_sd(F + 20*k2 + 77);
      _t11_5 = _mm256_broadcast_sd(F + 20*k2 + 78);
      _t11_4 = _mm256_broadcast_sd(F + 20*k2 + 79);
      _t11_3 = _asm256_loadu_pd(P + 20*i0 + 16);
      _t11_2 = _asm256_loadu_pd(P + 20*i0 + 36);
      _t11_1 = _asm256_loadu_pd(P + 20*i0 + 56);
      _t11_0 = _asm256_loadu_pd(P + 20*i0 + 76);
      _t11_20 = _asm256_loadu_pd(M0 + i0 + 20*k2);
      _t11_21 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 20);
      _t11_22 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 40);
      _t11_23 = _asm256_loadu_pd(M0 + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t11_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
      _t11_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
      _t11_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
      _t11_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t11_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_28), _mm256_mul_pd(_t11_18, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_30), _mm256_mul_pd(_t11_16, _t11_31)));
      _t11_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_28), _mm256_mul_pd(_t11_14, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_30), _mm256_mul_pd(_t11_12, _t11_31)));
      _t11_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_28), _mm256_mul_pd(_t11_10, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_30), _mm256_mul_pd(_t11_8, _t11_31)));
      _t11_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_28), _mm256_mul_pd(_t11_6, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_30), _mm256_mul_pd(_t11_4, _t11_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t11_20 = _mm256_add_pd(_t11_20, _t11_24);
      _t11_21 = _mm256_add_pd(_t11_21, _t11_25);
      _t11_22 = _mm256_add_pd(_t11_22, _t11_26);
      _t11_23 = _mm256_add_pd(_t11_23, _t11_27);

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 20*k2, _t11_20);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 20, _t11_21);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 40, _t11_22);
      _asm256_storeu_pd(M0 + i0 + 20*k2 + 60, _t11_23);
    }
    _t12_15 = _mm256_broadcast_sd(F + 20*k2 + 16);
    _t12_14 = _mm256_broadcast_sd(F + 20*k2 + 17);
    _t12_13 = _mm256_broadcast_sd(F + 20*k2 + 18);
    _t12_12 = _mm256_broadcast_sd(F + 20*k2 + 19);
    _t12_11 = _mm256_broadcast_sd(F + 20*k2 + 36);
    _t12_10 = _mm256_broadcast_sd(F + 20*k2 + 37);
    _t12_9 = _mm256_broadcast_sd(F + 20*k2 + 38);
    _t12_8 = _mm256_broadcast_sd(F + 20*k2 + 39);
    _t12_7 = _mm256_broadcast_sd(F + 20*k2 + 56);
    _t12_6 = _mm256_broadcast_sd(F + 20*k2 + 57);
    _t12_5 = _mm256_broadcast_sd(F + 20*k2 + 58);
    _t12_4 = _mm256_broadcast_sd(F + 20*k2 + 59);
    _t12_3 = _mm256_broadcast_sd(F + 20*k2 + 76);
    _t12_2 = _mm256_broadcast_sd(F + 20*k2 + 77);
    _t12_1 = _mm256_broadcast_sd(F + 20*k2 + 78);
    _t12_0 = _mm256_broadcast_sd(F + 20*k2 + 79);
    _t12_16 = _asm256_loadu_pd(M0 + 20*k2 + 16);
    _t12_17 = _asm256_loadu_pd(M0 + 20*k2 + 36);
    _t12_18 = _asm256_loadu_pd(M0 + 20*k2 + 56);
    _t12_19 = _asm256_loadu_pd(M0 + 20*k2 + 76);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t10_4), _mm256_mul_pd(_t12_14, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t10_6), _mm256_mul_pd(_t12_12, _t10_7)));
    _t12_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t10_4), _mm256_mul_pd(_t12_10, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t10_6), _mm256_mul_pd(_t12_8, _t10_7)));
    _t12_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t10_4), _mm256_mul_pd(_t12_6, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t10_6), _mm256_mul_pd(_t12_4, _t10_7)));
    _t12_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_3, _t10_4), _mm256_mul_pd(_t12_2, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_1, _t10_6), _mm256_mul_pd(_t12_0, _t10_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_16 = _mm256_add_pd(_t12_16, _t12_20);
    _t12_17 = _mm256_add_pd(_t12_17, _t12_21);
    _t12_18 = _mm256_add_pd(_t12_18, _t12_22);
    _t12_19 = _mm256_add_pd(_t12_19, _t12_23);

    // AVX Storer:
    _asm256_storeu_pd(M0 + 20*k2 + 16, _t12_16);
    _asm256_storeu_pd(M0 + 20*k2 + 36, _t12_17);
    _asm256_storeu_pd(M0 + 20*k2 + 56, _t12_18);
    _asm256_storeu_pd(M0 + 20*k2 + 76, _t12_19);
  }


  // Generating : Y[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M0[20,20],h(4, 20, 0)) * T( G(h(4, 20, k2), F[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), Q[20,20],h(4, 20, k2)) ),h(4, 20, k2)) + Sum_{i0} ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M0[20,20],h(4, 20, 0)) * T( G(h(4, 20, i0), F[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), Q[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + S(h(4, 20, 16), ( ( G(h(4, 20, 16), M0[20,20],h(4, 20, 0)) * T( G(h(4, 20, 16), F[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, 16), Q[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 20, k2), ( G(h(4, 20, k2), M0[20,20],h(4, 20, k3)) * T( G(h(4, 20, k2), F[20,20],h(4, 20, k3)) ) ),h(4, 20, k2)) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), M0[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), F[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) ) + $(h(4, 20, 16), ( G(h(4, 20, 16), M0[20,20],h(4, 20, k3)) * T( G(h(4, 20, 16), F[20,20],h(4, 20, k3)) ) ),h(4, 20, 16)) ) ) )


  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t13_23 = _mm256_broadcast_sd(M0 + 20*k2);
    _t13_22 = _mm256_broadcast_sd(M0 + 20*k2 + 1);
    _t13_21 = _mm256_broadcast_sd(M0 + 20*k2 + 2);
    _t13_20 = _mm256_broadcast_sd(M0 + 20*k2 + 3);
    _t13_19 = _mm256_broadcast_sd(M0 + 20*k2 + 20);
    _t13_18 = _mm256_broadcast_sd(M0 + 20*k2 + 21);
    _t13_17 = _mm256_broadcast_sd(M0 + 20*k2 + 22);
    _t13_16 = _mm256_broadcast_sd(M0 + 20*k2 + 23);
    _t13_15 = _mm256_broadcast_sd(M0 + 20*k2 + 40);
    _t13_14 = _mm256_broadcast_sd(M0 + 20*k2 + 41);
    _t13_13 = _mm256_broadcast_sd(M0 + 20*k2 + 42);
    _t13_12 = _mm256_broadcast_sd(M0 + 20*k2 + 43);
    _t13_11 = _mm256_broadcast_sd(M0 + 20*k2 + 60);
    _t13_10 = _mm256_broadcast_sd(M0 + 20*k2 + 61);
    _t13_9 = _mm256_broadcast_sd(M0 + 20*k2 + 62);
    _t13_8 = _mm256_broadcast_sd(M0 + 20*k2 + 63);
    _t13_7 = _asm256_loadu_pd(F + 20*k2);
    _t13_6 = _asm256_loadu_pd(F + 20*k2 + 20);
    _t13_5 = _asm256_loadu_pd(F + 20*k2 + 40);
    _t13_4 = _asm256_loadu_pd(F + 20*k2 + 60);
    _t13_3 = _asm256_loadu_pd(Q + 21*k2);
    _t13_2 = _mm256_maskload_pd(Q + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t13_1 = _mm256_maskload_pd(Q + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t13_0 = _mm256_maskload_pd(Q + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t13_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 32);
    _t13_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 32);
    _t13_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 49);
    _t13_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t13_40), _mm256_mul_pd(_t13_22, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t13_42), _mm256_mul_pd(_t13_20, _t13_43)));
    _t13_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_40), _mm256_mul_pd(_t13_18, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_42), _mm256_mul_pd(_t13_16, _t13_43)));
    _t13_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_40), _mm256_mul_pd(_t13_14, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_42), _mm256_mul_pd(_t13_12, _t13_43)));
    _t13_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_40), _mm256_mul_pd(_t13_10, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_42), _mm256_mul_pd(_t13_8, _t13_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t13_36 = _t13_3;
    _t13_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 3), _t13_2, 12);
    _t13_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 0), _t13_1, 49);
    _t13_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 12), _mm256_shuffle_pd(_t13_1, _t13_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t13_24 = _mm256_add_pd(_t13_32, _t13_36);
    _t13_25 = _mm256_add_pd(_t13_33, _t13_37);
    _t13_26 = _mm256_add_pd(_t13_34, _t13_38);
    _t13_27 = _mm256_add_pd(_t13_35, _t13_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t13_28 = _t13_24;
    _t13_29 = _t13_25;
    _t13_30 = _t13_26;
    _t13_31 = _t13_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
      _t14_7 = _asm256_loadu_pd(F + 20*i0);
      _t14_6 = _asm256_loadu_pd(F + 20*i0 + 20);
      _t14_5 = _asm256_loadu_pd(F + 20*i0 + 40);
      _t14_4 = _asm256_loadu_pd(F + 20*i0 + 60);
      _t14_3 = _asm256_loadu_pd(Q + i0 + 20*k2);
      _t14_2 = _asm256_loadu_pd(Q + i0 + 20*k2 + 20);
      _t14_1 = _asm256_loadu_pd(Q + i0 + 20*k2 + 40);
      _t14_0 = _asm256_loadu_pd(Q + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t14_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 32);
      _t14_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 32);
      _t14_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 49);
      _t14_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t14_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t14_16), _mm256_mul_pd(_t13_22, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t14_18), _mm256_mul_pd(_t13_20, _t14_19)));
      _t14_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t14_16), _mm256_mul_pd(_t13_18, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t14_18), _mm256_mul_pd(_t13_16, _t14_19)));
      _t14_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t14_16), _mm256_mul_pd(_t13_14, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t14_18), _mm256_mul_pd(_t13_12, _t14_19)));
      _t14_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t14_16), _mm256_mul_pd(_t13_10, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t14_18), _mm256_mul_pd(_t13_8, _t14_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t14_8 = _mm256_add_pd(_t14_12, _t14_3);
      _t14_9 = _mm256_add_pd(_t14_13, _t14_2);
      _t14_10 = _mm256_add_pd(_t14_14, _t14_1);
      _t14_11 = _mm256_add_pd(_t14_15, _t14_0);

      // AVX Storer:
      _asm256_storeu_pd(Y + i0 + 20*k2, _t14_8);
      _asm256_storeu_pd(Y + i0 + 20*k2 + 20, _t14_9);
      _asm256_storeu_pd(Y + i0 + 20*k2 + 40, _t14_10);
      _asm256_storeu_pd(Y + i0 + 20*k2 + 60, _t14_11);
    }
    _asm256_storeu_pd(Y + 21*k2, _t13_28);
    _mm256_maskstore_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t13_29);
    _mm256_maskstore_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t13_30);
    _mm256_maskstore_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t13_31);
  }

  _t15_23 = _mm256_broadcast_sd(M0 + 320);
  _t15_22 = _mm256_broadcast_sd(M0 + 321);
  _t15_21 = _mm256_broadcast_sd(M0 + 322);
  _t15_20 = _mm256_broadcast_sd(M0 + 323);
  _t15_19 = _mm256_broadcast_sd(M0 + 340);
  _t15_18 = _mm256_broadcast_sd(M0 + 341);
  _t15_17 = _mm256_broadcast_sd(M0 + 342);
  _t15_16 = _mm256_broadcast_sd(M0 + 343);
  _t15_15 = _mm256_broadcast_sd(M0 + 360);
  _t15_14 = _mm256_broadcast_sd(M0 + 361);
  _t15_13 = _mm256_broadcast_sd(M0 + 362);
  _t15_12 = _mm256_broadcast_sd(M0 + 363);
  _t15_11 = _mm256_broadcast_sd(M0 + 380);
  _t15_10 = _mm256_broadcast_sd(M0 + 381);
  _t15_9 = _mm256_broadcast_sd(M0 + 382);
  _t15_8 = _mm256_broadcast_sd(M0 + 383);
  _t15_7 = _asm256_loadu_pd(F + 320);
  _t15_6 = _asm256_loadu_pd(F + 340);
  _t15_5 = _asm256_loadu_pd(F + 360);
  _t15_4 = _asm256_loadu_pd(F + 380);
  _t15_3 = _asm256_loadu_pd(Q + 336);
  _t15_2 = _mm256_maskload_pd(Q + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t15_1 = _mm256_maskload_pd(Q + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t15_0 = _mm256_maskload_pd(Q + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
  _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
  _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
  _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
  _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
  _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
  _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t15_36 = _t15_3;
  _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
  _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
  _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
  _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
  _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
  _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t15_28 = _t15_24;
  _t15_29 = _t15_25;
  _t15_30 = _t15_26;
  _t15_31 = _t15_27;


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    for( int k2 = 0; k2 <= 15; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 20*k2 + k3);
      _t16_18 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 20);
      _t16_14 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 21);
      _t16_13 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 22);
      _t16_12 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 23);
      _t16_11 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 40);
      _t16_10 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 41);
      _t16_9 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 42);
      _t16_8 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 43);
      _t16_7 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 60);
      _t16_6 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 61);
      _t16_5 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 62);
      _t16_4 = _mm256_broadcast_sd(M0 + 20*k2 + k3 + 63);
      _t16_3 = _asm256_loadu_pd(F + 20*k2 + k3);
      _t16_2 = _asm256_loadu_pd(F + 20*k2 + k3 + 20);
      _t16_1 = _asm256_loadu_pd(F + 20*k2 + k3 + 40);
      _t16_0 = _asm256_loadu_pd(F + 20*k2 + k3 + 60);
      _t16_20 = _asm256_loadu_pd(Y + 21*k2);
      _t16_21 = _mm256_maskload_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t16_22 = _mm256_maskload_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t16_23 = _mm256_maskload_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_32), _mm256_mul_pd(_t16_18, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_34), _mm256_mul_pd(_t16_16, _t16_35)));
      _t16_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_32), _mm256_mul_pd(_t16_14, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_34), _mm256_mul_pd(_t16_12, _t16_35)));
      _t16_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_32), _mm256_mul_pd(_t16_10, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_34), _mm256_mul_pd(_t16_8, _t16_35)));
      _t16_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_32), _mm256_mul_pd(_t16_6, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_34), _mm256_mul_pd(_t16_4, _t16_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_28 = _t16_20;
      _t16_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 3), _t16_21, 12);
      _t16_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 0), _t16_22, 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 12), _mm256_shuffle_pd(_t16_22, _t16_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_28 = _mm256_add_pd(_t16_28, _t16_24);
      _t16_29 = _mm256_add_pd(_t16_29, _t16_25);
      _t16_30 = _mm256_add_pd(_t16_30, _t16_26);
      _t16_31 = _mm256_add_pd(_t16_31, _t16_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t16_20 = _t16_28;
      _t16_21 = _t16_29;
      _t16_22 = _t16_30;
      _t16_23 = _t16_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t17_3 = _asm256_loadu_pd(F + 20*i0 + k3);
        _t17_2 = _asm256_loadu_pd(F + 20*i0 + k3 + 20);
        _t17_1 = _asm256_loadu_pd(F + 20*i0 + k3 + 40);
        _t17_0 = _asm256_loadu_pd(F + 20*i0 + k3 + 60);
        _t17_4 = _asm256_loadu_pd(Y + i0 + 20*k2);
        _t17_5 = _asm256_loadu_pd(Y + i0 + 20*k2 + 20);
        _t17_6 = _asm256_loadu_pd(Y + i0 + 20*k2 + 40);
        _t17_7 = _asm256_loadu_pd(Y + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t17_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
        _t17_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
        _t17_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
        _t17_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t17_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t17_12), _mm256_mul_pd(_t16_18, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t17_14), _mm256_mul_pd(_t16_16, _t17_15)));
        _t17_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t17_12), _mm256_mul_pd(_t16_14, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t17_14), _mm256_mul_pd(_t16_12, _t17_15)));
        _t17_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t17_12), _mm256_mul_pd(_t16_10, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t17_14), _mm256_mul_pd(_t16_8, _t17_15)));
        _t17_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t17_12), _mm256_mul_pd(_t16_6, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t17_14), _mm256_mul_pd(_t16_4, _t17_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_4 = _mm256_add_pd(_t17_4, _t17_8);
        _t17_5 = _mm256_add_pd(_t17_5, _t17_9);
        _t17_6 = _mm256_add_pd(_t17_6, _t17_10);
        _t17_7 = _mm256_add_pd(_t17_7, _t17_11);

        // AVX Storer:
        _asm256_storeu_pd(Y + i0 + 20*k2, _t17_4);
        _asm256_storeu_pd(Y + i0 + 20*k2 + 20, _t17_5);
        _asm256_storeu_pd(Y + i0 + 20*k2 + 40, _t17_6);
        _asm256_storeu_pd(Y + i0 + 20*k2 + 60, _t17_7);
      }
      _asm256_storeu_pd(Y + 21*k2, _t16_20);
      _mm256_maskstore_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t16_21);
      _mm256_maskstore_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t16_22);
      _mm256_maskstore_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t16_23);
    }
    _t18_19 = _mm256_broadcast_sd(M0 + k3 + 320);
    _t18_18 = _mm256_broadcast_sd(M0 + k3 + 321);
    _t18_17 = _mm256_broadcast_sd(M0 + k3 + 322);
    _t18_16 = _mm256_broadcast_sd(M0 + k3 + 323);
    _t18_15 = _mm256_broadcast_sd(M0 + k3 + 340);
    _t18_14 = _mm256_broadcast_sd(M0 + k3 + 341);
    _t18_13 = _mm256_broadcast_sd(M0 + k3 + 342);
    _t18_12 = _mm256_broadcast_sd(M0 + k3 + 343);
    _t18_11 = _mm256_broadcast_sd(M0 + k3 + 360);
    _t18_10 = _mm256_broadcast_sd(M0 + k3 + 361);
    _t18_9 = _mm256_broadcast_sd(M0 + k3 + 362);
    _t18_8 = _mm256_broadcast_sd(M0 + k3 + 363);
    _t18_7 = _mm256_broadcast_sd(M0 + k3 + 380);
    _t18_6 = _mm256_broadcast_sd(M0 + k3 + 381);
    _t18_5 = _mm256_broadcast_sd(M0 + k3 + 382);
    _t18_4 = _mm256_broadcast_sd(M0 + k3 + 383);
    _t18_3 = _asm256_loadu_pd(F + k3 + 320);
    _t18_2 = _asm256_loadu_pd(F + k3 + 340);
    _t18_1 = _asm256_loadu_pd(F + k3 + 360);
    _t18_0 = _asm256_loadu_pd(F + k3 + 380);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t18_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
    _t18_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
    _t18_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
    _t18_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_28), _mm256_mul_pd(_t18_18, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_30), _mm256_mul_pd(_t18_16, _t18_31)));
    _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_28), _mm256_mul_pd(_t18_14, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_30), _mm256_mul_pd(_t18_12, _t18_31)));
    _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_28), _mm256_mul_pd(_t18_10, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_30), _mm256_mul_pd(_t18_8, _t18_31)));
    _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_28), _mm256_mul_pd(_t18_6, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_30), _mm256_mul_pd(_t18_4, _t18_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t18_24 = _t15_28;
    _t18_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
    _t18_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
    _t18_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t18_24 = _mm256_add_pd(_t18_24, _t18_20);
    _t18_25 = _mm256_add_pd(_t18_25, _t18_21);
    _t18_26 = _mm256_add_pd(_t18_26, _t18_22);
    _t18_27 = _mm256_add_pd(_t18_27, _t18_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t18_24;
    _t15_29 = _t18_25;
    _t15_30 = _t18_26;
    _t15_31 = _t18_27;
  }


  // Generating : v0[20,1] = ( Sum_{k2} ( S(h(4, 20, k2), ( G(h(4, 20, k2), z[20,1],h(1, 1, 0)) - ( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) * G(h(4, 20, 0), y[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * G(h(4, 20, k3), y[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t19_5 = _asm256_loadu_pd(z + k2);
    _t19_4 = _asm256_loadu_pd(H + 20*k2);
    _t19_3 = _asm256_loadu_pd(H + 20*k2 + 20);
    _t19_2 = _asm256_loadu_pd(H + 20*k2 + 40);
    _t19_1 = _asm256_loadu_pd(H + 20*k2 + 60);
    _t19_0 = _asm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t19_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t19_7 = _mm256_sub_pd(_t19_5, _t19_6);

    // AVX Storer:
    _asm256_storeu_pd(v0 + k2, _t19_7);
  }


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 19; k2+=4 ) {
      _t20_4 = _asm256_loadu_pd(H + 20*k2 + k3);
      _t20_3 = _asm256_loadu_pd(H + 20*k2 + k3 + 20);
      _t20_2 = _asm256_loadu_pd(H + 20*k2 + k3 + 40);
      _t20_1 = _asm256_loadu_pd(H + 20*k2 + k3 + 60);
      _t20_0 = _asm256_loadu_pd(y + k3);
      _t20_5 = _asm256_loadu_pd(v0 + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t20_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t20_5 = _mm256_sub_pd(_t20_5, _t20_6);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k2, _t20_5);
    }
  }

  _t21_3 = _asm256_loadu_pd(Y);
  _t21_2 = _mm256_maskload_pd(Y + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t21_1 = _mm256_maskload_pd(Y + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t21_0 = _mm256_maskload_pd(Y + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) * G(h(4, 20, 0), Y[20,20],h(4, 20, 0)) ),h(4, 20, 0)) + Sum_{i0} ( S(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) * G(h(4, 20, 0), Y[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), Y[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * G(h(4, 20, k3), Y[20,20],h(4, 20, k3)) ),h(4, 20, k3)) ) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) * G(h(4, 20, k3), Y[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), Y[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) + $(h(4, 20, k2), ( G(h(4, 20, k2), H[20,20],h(4, 20, 16)) * G(h(4, 20, 16), Y[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t21_4 = _t21_3;
  _t21_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t21_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t21_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t22_15 = _mm256_broadcast_sd(H + 20*k2);
    _t22_14 = _mm256_broadcast_sd(H + 20*k2 + 1);
    _t22_13 = _mm256_broadcast_sd(H + 20*k2 + 2);
    _t22_12 = _mm256_broadcast_sd(H + 20*k2 + 3);
    _t22_11 = _mm256_broadcast_sd(H + 20*k2 + 20);
    _t22_10 = _mm256_broadcast_sd(H + 20*k2 + 21);
    _t22_9 = _mm256_broadcast_sd(H + 20*k2 + 22);
    _t22_8 = _mm256_broadcast_sd(H + 20*k2 + 23);
    _t22_7 = _mm256_broadcast_sd(H + 20*k2 + 40);
    _t22_6 = _mm256_broadcast_sd(H + 20*k2 + 41);
    _t22_5 = _mm256_broadcast_sd(H + 20*k2 + 42);
    _t22_4 = _mm256_broadcast_sd(H + 20*k2 + 43);
    _t22_3 = _mm256_broadcast_sd(H + 20*k2 + 60);
    _t22_2 = _mm256_broadcast_sd(H + 20*k2 + 61);
    _t22_1 = _mm256_broadcast_sd(H + 20*k2 + 62);
    _t22_0 = _mm256_broadcast_sd(H + 20*k2 + 63);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t22_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t21_4), _mm256_mul_pd(_t22_14, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t21_6), _mm256_mul_pd(_t22_12, _t21_7)));
    _t22_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t21_4), _mm256_mul_pd(_t22_10, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t21_6), _mm256_mul_pd(_t22_8, _t21_7)));
    _t22_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t21_4), _mm256_mul_pd(_t22_6, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t21_6), _mm256_mul_pd(_t22_4, _t21_7)));
    _t22_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t21_4), _mm256_mul_pd(_t22_2, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t21_6), _mm256_mul_pd(_t22_0, _t21_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 19; i0+=4 ) {
      _t23_3 = _asm256_loadu_pd(Y + i0);
      _t23_2 = _asm256_loadu_pd(Y + i0 + 20);
      _t23_1 = _asm256_loadu_pd(Y + i0 + 40);
      _t23_0 = _asm256_loadu_pd(Y + i0 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t23_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t23_3), _mm256_mul_pd(_t22_14, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t23_1), _mm256_mul_pd(_t22_12, _t23_0)));
      _t23_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t23_3), _mm256_mul_pd(_t22_10, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t23_1), _mm256_mul_pd(_t22_8, _t23_0)));
      _t23_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t23_3), _mm256_mul_pd(_t22_6, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t23_1), _mm256_mul_pd(_t22_4, _t23_0)));
      _t23_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t23_3), _mm256_mul_pd(_t22_2, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t23_1), _mm256_mul_pd(_t22_0, _t23_0)));

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 20*k2, _t23_4);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t23_5);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t23_6);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t23_7);
    }
    _asm256_storeu_pd(M1 + 20*k2, _t22_16);
    _asm256_storeu_pd(M1 + 20*k2 + 20, _t22_17);
    _asm256_storeu_pd(M1 + 20*k2 + 40, _t22_18);
    _asm256_storeu_pd(M1 + 20*k2 + 60, _t22_19);
  }


  for( int k3 = 4; k3 <= 15; k3+=4 ) {
    _t24_3 = _asm256_loadu_pd(Y + 21*k3);
    _t24_2 = _mm256_maskload_pd(Y + 21*k3 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t24_1 = _mm256_maskload_pd(Y + 21*k3 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t24_0 = _mm256_maskload_pd(Y + 21*k3 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t24_4 = _t24_3;
    _t24_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
    _t24_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
    _t24_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

    for( int k2 = 0; k2 <= 19; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t25_19 = _mm256_broadcast_sd(H + 20*k2 + k3);
        _t25_18 = _mm256_broadcast_sd(H + 20*k2 + k3 + 1);
        _t25_17 = _mm256_broadcast_sd(H + 20*k2 + k3 + 2);
        _t25_16 = _mm256_broadcast_sd(H + 20*k2 + k3 + 3);
        _t25_15 = _mm256_broadcast_sd(H + 20*k2 + k3 + 20);
        _t25_14 = _mm256_broadcast_sd(H + 20*k2 + k3 + 21);
        _t25_13 = _mm256_broadcast_sd(H + 20*k2 + k3 + 22);
        _t25_12 = _mm256_broadcast_sd(H + 20*k2 + k3 + 23);
        _t25_11 = _mm256_broadcast_sd(H + 20*k2 + k3 + 40);
        _t25_10 = _mm256_broadcast_sd(H + 20*k2 + k3 + 41);
        _t25_9 = _mm256_broadcast_sd(H + 20*k2 + k3 + 42);
        _t25_8 = _mm256_broadcast_sd(H + 20*k2 + k3 + 43);
        _t25_7 = _mm256_broadcast_sd(H + 20*k2 + k3 + 60);
        _t25_6 = _mm256_broadcast_sd(H + 20*k2 + k3 + 61);
        _t25_5 = _mm256_broadcast_sd(H + 20*k2 + k3 + 62);
        _t25_4 = _mm256_broadcast_sd(H + 20*k2 + k3 + 63);
        _t25_3 = _asm256_loadu_pd(Y + 20*i0 + k3);
        _t25_2 = _asm256_loadu_pd(Y + 20*i0 + k3 + 20);
        _t25_1 = _asm256_loadu_pd(Y + 20*i0 + k3 + 40);
        _t25_0 = _asm256_loadu_pd(Y + 20*i0 + k3 + 60);
        _t25_20 = _asm256_loadu_pd(M1 + i0 + 20*k2);
        _t25_21 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 20);
        _t25_22 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 40);
        _t25_23 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t25_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
        _t25_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
        _t25_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
        _t25_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t25_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_28), _mm256_mul_pd(_t25_18, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_30), _mm256_mul_pd(_t25_16, _t25_31)));
        _t25_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_28), _mm256_mul_pd(_t25_14, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_30), _mm256_mul_pd(_t25_12, _t25_31)));
        _t25_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_28), _mm256_mul_pd(_t25_10, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_30), _mm256_mul_pd(_t25_8, _t25_31)));
        _t25_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_28), _mm256_mul_pd(_t25_6, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_30), _mm256_mul_pd(_t25_4, _t25_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t25_20 = _mm256_add_pd(_t25_20, _t25_24);
        _t25_21 = _mm256_add_pd(_t25_21, _t25_25);
        _t25_22 = _mm256_add_pd(_t25_22, _t25_26);
        _t25_23 = _mm256_add_pd(_t25_23, _t25_27);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 20*k2, _t25_20);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t25_21);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t25_22);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t25_23);
      }
      _t26_15 = _mm256_broadcast_sd(H + 20*k2 + k3);
      _t26_14 = _mm256_broadcast_sd(H + 20*k2 + k3 + 1);
      _t26_13 = _mm256_broadcast_sd(H + 20*k2 + k3 + 2);
      _t26_12 = _mm256_broadcast_sd(H + 20*k2 + k3 + 3);
      _t26_11 = _mm256_broadcast_sd(H + 20*k2 + k3 + 20);
      _t26_10 = _mm256_broadcast_sd(H + 20*k2 + k3 + 21);
      _t26_9 = _mm256_broadcast_sd(H + 20*k2 + k3 + 22);
      _t26_8 = _mm256_broadcast_sd(H + 20*k2 + k3 + 23);
      _t26_7 = _mm256_broadcast_sd(H + 20*k2 + k3 + 40);
      _t26_6 = _mm256_broadcast_sd(H + 20*k2 + k3 + 41);
      _t26_5 = _mm256_broadcast_sd(H + 20*k2 + k3 + 42);
      _t26_4 = _mm256_broadcast_sd(H + 20*k2 + k3 + 43);
      _t26_3 = _mm256_broadcast_sd(H + 20*k2 + k3 + 60);
      _t26_2 = _mm256_broadcast_sd(H + 20*k2 + k3 + 61);
      _t26_1 = _mm256_broadcast_sd(H + 20*k2 + k3 + 62);
      _t26_0 = _mm256_broadcast_sd(H + 20*k2 + k3 + 63);
      _t26_16 = _asm256_loadu_pd(M1 + 20*k2 + k3);
      _t26_17 = _asm256_loadu_pd(M1 + 20*k2 + k3 + 20);
      _t26_18 = _asm256_loadu_pd(M1 + 20*k2 + k3 + 40);
      _t26_19 = _asm256_loadu_pd(M1 + 20*k2 + k3 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t26_24 = _t24_3;
      _t26_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
      _t26_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
      _t26_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t26_24), _mm256_mul_pd(_t26_14, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t26_26), _mm256_mul_pd(_t26_12, _t26_27)));
      _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t26_24), _mm256_mul_pd(_t26_10, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t26_26), _mm256_mul_pd(_t26_8, _t26_27)));
      _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t26_24), _mm256_mul_pd(_t26_6, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t26_26), _mm256_mul_pd(_t26_4, _t26_27)));
      _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t26_24), _mm256_mul_pd(_t26_2, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t26_26), _mm256_mul_pd(_t26_0, _t26_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_16 = _mm256_add_pd(_t26_16, _t26_20);
      _t26_17 = _mm256_add_pd(_t26_17, _t26_21);
      _t26_18 = _mm256_add_pd(_t26_18, _t26_22);
      _t26_19 = _mm256_add_pd(_t26_19, _t26_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M1 + 20*k2 + k3, _t26_16);
      _asm256_storeu_pd(M1 + 20*k2 + k3 + 20, _t26_17);
      _asm256_storeu_pd(M1 + 20*k2 + k3 + 40, _t26_18);
      _asm256_storeu_pd(M1 + 20*k2 + k3 + 60, _t26_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t27_3 = _asm256_loadu_pd(Y + i0 + 20*k3);
        _t27_2 = _asm256_loadu_pd(Y + i0 + 20*k3 + 20);
        _t27_1 = _asm256_loadu_pd(Y + i0 + 20*k3 + 40);
        _t27_0 = _asm256_loadu_pd(Y + i0 + 20*k3 + 60);
        _t27_4 = _asm256_loadu_pd(M1 + i0 + 20*k2);
        _t27_5 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 20);
        _t27_6 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 40);
        _t27_7 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t27_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t27_3), _mm256_mul_pd(_t26_14, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t27_1), _mm256_mul_pd(_t26_12, _t27_0)));
        _t27_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t27_3), _mm256_mul_pd(_t26_10, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t27_1), _mm256_mul_pd(_t26_8, _t27_0)));
        _t27_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t27_3), _mm256_mul_pd(_t26_6, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t27_1), _mm256_mul_pd(_t26_4, _t27_0)));
        _t27_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t27_3), _mm256_mul_pd(_t26_2, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t27_1), _mm256_mul_pd(_t26_0, _t27_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t27_4 = _mm256_add_pd(_t27_4, _t27_8);
        _t27_5 = _mm256_add_pd(_t27_5, _t27_9);
        _t27_6 = _mm256_add_pd(_t27_6, _t27_10);
        _t27_7 = _mm256_add_pd(_t27_7, _t27_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 20*k2, _t27_4);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t27_5);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t27_6);
        _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t27_7);
      }
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t28_0 = _t15_28;
  _t28_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t28_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t28_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 15; i0+=4 ) {
      _t29_19 = _mm256_broadcast_sd(H + 20*k2 + 16);
      _t29_18 = _mm256_broadcast_sd(H + 20*k2 + 17);
      _t29_17 = _mm256_broadcast_sd(H + 20*k2 + 18);
      _t29_16 = _mm256_broadcast_sd(H + 20*k2 + 19);
      _t29_15 = _mm256_broadcast_sd(H + 20*k2 + 36);
      _t29_14 = _mm256_broadcast_sd(H + 20*k2 + 37);
      _t29_13 = _mm256_broadcast_sd(H + 20*k2 + 38);
      _t29_12 = _mm256_broadcast_sd(H + 20*k2 + 39);
      _t29_11 = _mm256_broadcast_sd(H + 20*k2 + 56);
      _t29_10 = _mm256_broadcast_sd(H + 20*k2 + 57);
      _t29_9 = _mm256_broadcast_sd(H + 20*k2 + 58);
      _t29_8 = _mm256_broadcast_sd(H + 20*k2 + 59);
      _t29_7 = _mm256_broadcast_sd(H + 20*k2 + 76);
      _t29_6 = _mm256_broadcast_sd(H + 20*k2 + 77);
      _t29_5 = _mm256_broadcast_sd(H + 20*k2 + 78);
      _t29_4 = _mm256_broadcast_sd(H + 20*k2 + 79);
      _t29_3 = _asm256_loadu_pd(Y + 20*i0 + 16);
      _t29_2 = _asm256_loadu_pd(Y + 20*i0 + 36);
      _t29_1 = _asm256_loadu_pd(Y + 20*i0 + 56);
      _t29_0 = _asm256_loadu_pd(Y + 20*i0 + 76);
      _t29_20 = _asm256_loadu_pd(M1 + i0 + 20*k2);
      _t29_21 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 20);
      _t29_22 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 40);
      _t29_23 = _asm256_loadu_pd(M1 + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
      _t29_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
      _t29_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
      _t29_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t29_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_28), _mm256_mul_pd(_t29_18, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_30), _mm256_mul_pd(_t29_16, _t29_31)));
      _t29_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_28), _mm256_mul_pd(_t29_14, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_30), _mm256_mul_pd(_t29_12, _t29_31)));
      _t29_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_28), _mm256_mul_pd(_t29_10, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_30), _mm256_mul_pd(_t29_8, _t29_31)));
      _t29_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_28), _mm256_mul_pd(_t29_6, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_30), _mm256_mul_pd(_t29_4, _t29_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t29_20 = _mm256_add_pd(_t29_20, _t29_24);
      _t29_21 = _mm256_add_pd(_t29_21, _t29_25);
      _t29_22 = _mm256_add_pd(_t29_22, _t29_26);
      _t29_23 = _mm256_add_pd(_t29_23, _t29_27);

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 20*k2, _t29_20);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 20, _t29_21);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 40, _t29_22);
      _asm256_storeu_pd(M1 + i0 + 20*k2 + 60, _t29_23);
    }
    _t30_15 = _mm256_broadcast_sd(H + 20*k2 + 16);
    _t30_14 = _mm256_broadcast_sd(H + 20*k2 + 17);
    _t30_13 = _mm256_broadcast_sd(H + 20*k2 + 18);
    _t30_12 = _mm256_broadcast_sd(H + 20*k2 + 19);
    _t30_11 = _mm256_broadcast_sd(H + 20*k2 + 36);
    _t30_10 = _mm256_broadcast_sd(H + 20*k2 + 37);
    _t30_9 = _mm256_broadcast_sd(H + 20*k2 + 38);
    _t30_8 = _mm256_broadcast_sd(H + 20*k2 + 39);
    _t30_7 = _mm256_broadcast_sd(H + 20*k2 + 56);
    _t30_6 = _mm256_broadcast_sd(H + 20*k2 + 57);
    _t30_5 = _mm256_broadcast_sd(H + 20*k2 + 58);
    _t30_4 = _mm256_broadcast_sd(H + 20*k2 + 59);
    _t30_3 = _mm256_broadcast_sd(H + 20*k2 + 76);
    _t30_2 = _mm256_broadcast_sd(H + 20*k2 + 77);
    _t30_1 = _mm256_broadcast_sd(H + 20*k2 + 78);
    _t30_0 = _mm256_broadcast_sd(H + 20*k2 + 79);
    _t30_16 = _asm256_loadu_pd(M1 + 20*k2 + 16);
    _t30_17 = _asm256_loadu_pd(M1 + 20*k2 + 36);
    _t30_18 = _asm256_loadu_pd(M1 + 20*k2 + 56);
    _t30_19 = _asm256_loadu_pd(M1 + 20*k2 + 76);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t28_0), _mm256_mul_pd(_t30_14, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t28_2), _mm256_mul_pd(_t30_12, _t28_3)));
    _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t28_0), _mm256_mul_pd(_t30_10, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t28_2), _mm256_mul_pd(_t30_8, _t28_3)));
    _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t28_0), _mm256_mul_pd(_t30_6, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t28_2), _mm256_mul_pd(_t30_4, _t28_3)));
    _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_3, _t28_0), _mm256_mul_pd(_t30_2, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_1, _t28_2), _mm256_mul_pd(_t30_0, _t28_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t30_16 = _mm256_add_pd(_t30_16, _t30_20);
    _t30_17 = _mm256_add_pd(_t30_17, _t30_21);
    _t30_18 = _mm256_add_pd(_t30_18, _t30_22);
    _t30_19 = _mm256_add_pd(_t30_19, _t30_23);

    // AVX Storer:
    _asm256_storeu_pd(M1 + 20*k2 + 16, _t30_16);
    _asm256_storeu_pd(M1 + 20*k2 + 36, _t30_17);
    _asm256_storeu_pd(M1 + 20*k2 + 56, _t30_18);
    _asm256_storeu_pd(M1 + 20*k2 + 76, _t30_19);
  }


  // Generating : M2[20,20] = ( ( ( ( Sum_{i0} ( S(h(4, 20, 0), ( G(h(4, 20, 0), Y[20,20],h(4, 20, 0)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 0)) ) ),h(4, 20, i0)) ) + Sum_{k2} ( Sum_{i0} ( S(h(4, 20, k2), ( T( G(h(4, 20, 0), Y[20,20],h(4, 20, k2)) ) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 0)) ) ),h(4, 20, i0)) ) ) ) + Sum_{k3} ( ( ( Sum_{k2} ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) + Sum_{i0} ( $(h(4, 20, k3), ( G(h(4, 20, k3), Y[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 20, k2), ( T( G(h(4, 20, k3), Y[20,20],h(4, 20, k2)) ) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) ) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) ) ) + Sum_{i0} ( $(h(4, 20, 16), ( G(h(4, 20, 16), Y[20,20],h(4, 20, 16)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 16)) ) ),h(4, 20, i0)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t31_0 = _t21_3;
  _t31_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t31_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t31_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t32_3 = _asm256_loadu_pd(H + 20*i0);
    _t32_2 = _asm256_loadu_pd(H + 20*i0 + 20);
    _t32_1 = _asm256_loadu_pd(H + 20*i0 + 40);
    _t32_0 = _asm256_loadu_pd(H + 20*i0 + 60);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t32_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32);
    _t32_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 32);
    _t32_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 49);
    _t32_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 15), _t32_11)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 15), _t32_11)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 15), _t32_11)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 15), _t32_11)));

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0, _t32_4);
    _asm256_storeu_pd(M2 + i0 + 20, _t32_5);
    _asm256_storeu_pd(M2 + i0 + 40, _t32_6);
    _asm256_storeu_pd(M2 + i0 + 60, _t32_7);
  }


  for( int k2 = 4; k2 <= 19; k2+=4 ) {
    _t33_3 = _asm256_loadu_pd(Y + k2);
    _t33_2 = _asm256_loadu_pd(Y + k2 + 20);
    _t33_1 = _asm256_loadu_pd(Y + k2 + 40);
    _t33_0 = _asm256_loadu_pd(Y + k2 + 60);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t34_3 = _asm256_loadu_pd(H + 20*i0);
      _t34_2 = _asm256_loadu_pd(H + 20*i0 + 20);
      _t34_1 = _asm256_loadu_pd(H + 20*i0 + 40);
      _t34_0 = _asm256_loadu_pd(H + 20*i0 + 60);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
      _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
      _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
      _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t34_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
      _t34_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
      _t34_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
      _t34_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t34_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 15), _t34_11)));
      _t34_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 15), _t34_11)));
      _t34_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 15), _t34_11)));
      _t34_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 15), _t34_11)));

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 20*k2, _t34_4);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t34_5);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t34_6);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t34_7);
    }
  }


  for( int k3 = 4; k3 <= 15; k3+=4 ) {

    for( int k2 = 0; k2 <= k3 - 1; k2+=4 ) {

      for( int i0 = 0; i0 <= 19; i0+=4 ) {
        _t35_19 = _mm256_broadcast_sd(Y + 20*k2 + k3);
        _t35_18 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 1);
        _t35_17 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 2);
        _t35_16 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 3);
        _t35_15 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 20);
        _t35_14 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 21);
        _t35_13 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 22);
        _t35_12 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 23);
        _t35_11 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 40);
        _t35_10 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 41);
        _t35_9 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 42);
        _t35_8 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 43);
        _t35_7 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 60);
        _t35_6 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 61);
        _t35_5 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 62);
        _t35_4 = _mm256_broadcast_sd(Y + 20*k2 + k3 + 63);
        _t35_3 = _asm256_loadu_pd(H + 20*i0 + k3);
        _t35_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
        _t35_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
        _t35_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
        _t35_20 = _asm256_loadu_pd(M2 + i0 + 20*k2);
        _t35_21 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 20);
        _t35_22 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 40);
        _t35_23 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t35_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
        _t35_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
        _t35_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
        _t35_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t35_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_28), _mm256_mul_pd(_t35_18, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_30), _mm256_mul_pd(_t35_16, _t35_31)));
        _t35_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_28), _mm256_mul_pd(_t35_14, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_30), _mm256_mul_pd(_t35_12, _t35_31)));
        _t35_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_28), _mm256_mul_pd(_t35_10, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_30), _mm256_mul_pd(_t35_8, _t35_31)));
        _t35_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_28), _mm256_mul_pd(_t35_6, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_30), _mm256_mul_pd(_t35_4, _t35_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t35_20 = _mm256_add_pd(_t35_20, _t35_24);
        _t35_21 = _mm256_add_pd(_t35_21, _t35_25);
        _t35_22 = _mm256_add_pd(_t35_22, _t35_26);
        _t35_23 = _mm256_add_pd(_t35_23, _t35_27);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 20*k2, _t35_20);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t35_21);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t35_22);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t35_23);
      }
    }
    _t36_3 = _asm256_loadu_pd(Y + 21*k3);
    _t36_2 = _mm256_maskload_pd(Y + 21*k3 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t36_1 = _mm256_maskload_pd(Y + 21*k3 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t36_0 = _mm256_maskload_pd(Y + 21*k3 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t36_4 = _t36_3;
    _t36_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
    _t36_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
    _t36_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t37_3 = _asm256_loadu_pd(H + 20*i0 + k3);
      _t37_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
      _t37_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
      _t37_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
      _t37_4 = _asm256_loadu_pd(M2 + i0 + 20*k3);
      _t37_5 = _asm256_loadu_pd(M2 + i0 + 20*k3 + 20);
      _t37_6 = _asm256_loadu_pd(M2 + i0 + 20*k3 + 40);
      _t37_7 = _asm256_loadu_pd(M2 + i0 + 20*k3 + 60);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t37_12 = _t36_3;
      _t37_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
      _t37_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
      _t37_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 15), _t37_19)));
      _t37_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 15), _t37_19)));
      _t37_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 15), _t37_19)));
      _t37_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 15), _t37_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t37_4 = _mm256_add_pd(_t37_4, _t37_8);
      _t37_5 = _mm256_add_pd(_t37_5, _t37_9);
      _t37_6 = _mm256_add_pd(_t37_6, _t37_10);
      _t37_7 = _mm256_add_pd(_t37_7, _t37_11);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 20*k3, _t37_4);
      _asm256_storeu_pd(M2 + i0 + 20*k3 + 20, _t37_5);
      _asm256_storeu_pd(M2 + i0 + 20*k3 + 40, _t37_6);
      _asm256_storeu_pd(M2 + i0 + 20*k3 + 60, _t37_7);
    }

    for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 19; k2+=4 ) {

      for( int i0 = 0; i0 <= 19; i0+=4 ) {
        _t38_7 = _asm256_loadu_pd(Y + k2 + 20*k3);
        _t38_6 = _asm256_loadu_pd(Y + k2 + 20*k3 + 20);
        _t38_5 = _asm256_loadu_pd(Y + k2 + 20*k3 + 40);
        _t38_4 = _asm256_loadu_pd(Y + k2 + 20*k3 + 60);
        _t38_3 = _asm256_loadu_pd(H + 20*i0 + k3);
        _t38_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
        _t38_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
        _t38_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
        _t38_8 = _asm256_loadu_pd(M2 + i0 + 20*k2);
        _t38_9 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 20);
        _t38_10 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 40);
        _t38_11 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 60);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 32);
        _t38_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 32);
        _t38_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 49);
        _t38_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
        _t38_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
        _t38_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
        _t38_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t38_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 15), _t38_23)));
        _t38_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 15), _t38_23)));
        _t38_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 15), _t38_23)));
        _t38_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 15), _t38_23)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t38_8 = _mm256_add_pd(_t38_8, _t38_12);
        _t38_9 = _mm256_add_pd(_t38_9, _t38_13);
        _t38_10 = _mm256_add_pd(_t38_10, _t38_14);
        _t38_11 = _mm256_add_pd(_t38_11, _t38_15);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 20*k2, _t38_8);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t38_9);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t38_10);
        _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t38_11);
      }
    }
  }


  for( int k2 = 0; k2 <= 15; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 19; i0+=4 ) {
      _t39_19 = _mm256_broadcast_sd(Y + 20*k2 + 16);
      _t39_18 = _mm256_broadcast_sd(Y + 20*k2 + 17);
      _t39_17 = _mm256_broadcast_sd(Y + 20*k2 + 18);
      _t39_16 = _mm256_broadcast_sd(Y + 20*k2 + 19);
      _t39_15 = _mm256_broadcast_sd(Y + 20*k2 + 36);
      _t39_14 = _mm256_broadcast_sd(Y + 20*k2 + 37);
      _t39_13 = _mm256_broadcast_sd(Y + 20*k2 + 38);
      _t39_12 = _mm256_broadcast_sd(Y + 20*k2 + 39);
      _t39_11 = _mm256_broadcast_sd(Y + 20*k2 + 56);
      _t39_10 = _mm256_broadcast_sd(Y + 20*k2 + 57);
      _t39_9 = _mm256_broadcast_sd(Y + 20*k2 + 58);
      _t39_8 = _mm256_broadcast_sd(Y + 20*k2 + 59);
      _t39_7 = _mm256_broadcast_sd(Y + 20*k2 + 76);
      _t39_6 = _mm256_broadcast_sd(Y + 20*k2 + 77);
      _t39_5 = _mm256_broadcast_sd(Y + 20*k2 + 78);
      _t39_4 = _mm256_broadcast_sd(Y + 20*k2 + 79);
      _t39_3 = _asm256_loadu_pd(H + 20*i0 + 16);
      _t39_2 = _asm256_loadu_pd(H + 20*i0 + 36);
      _t39_1 = _asm256_loadu_pd(H + 20*i0 + 56);
      _t39_0 = _asm256_loadu_pd(H + 20*i0 + 76);
      _t39_20 = _asm256_loadu_pd(M2 + i0 + 20*k2);
      _t39_21 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 20);
      _t39_22 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 40);
      _t39_23 = _asm256_loadu_pd(M2 + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t39_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 32);
      _t39_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 32);
      _t39_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 49);
      _t39_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t39_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_28), _mm256_mul_pd(_t39_18, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_30), _mm256_mul_pd(_t39_16, _t39_31)));
      _t39_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_28), _mm256_mul_pd(_t39_14, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_30), _mm256_mul_pd(_t39_12, _t39_31)));
      _t39_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_28), _mm256_mul_pd(_t39_10, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_30), _mm256_mul_pd(_t39_8, _t39_31)));
      _t39_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_28), _mm256_mul_pd(_t39_6, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_30), _mm256_mul_pd(_t39_4, _t39_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_20 = _mm256_add_pd(_t39_20, _t39_24);
      _t39_21 = _mm256_add_pd(_t39_21, _t39_25);
      _t39_22 = _mm256_add_pd(_t39_22, _t39_26);
      _t39_23 = _mm256_add_pd(_t39_23, _t39_27);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 20*k2, _t39_20);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 20, _t39_21);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 40, _t39_22);
      _asm256_storeu_pd(M2 + i0 + 20*k2 + 60, _t39_23);
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t40_0 = _t15_28;
  _t40_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t40_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t40_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int i0 = 0; i0 <= 19; i0+=4 ) {
    _t41_3 = _asm256_loadu_pd(H + 20*i0 + 16);
    _t41_2 = _asm256_loadu_pd(H + 20*i0 + 36);
    _t41_1 = _asm256_loadu_pd(H + 20*i0 + 56);
    _t41_0 = _asm256_loadu_pd(H + 20*i0 + 76);
    _t41_4 = _asm256_loadu_pd(M2 + i0 + 320);
    _t41_5 = _asm256_loadu_pd(M2 + i0 + 340);
    _t41_6 = _asm256_loadu_pd(M2 + i0 + 360);
    _t41_7 = _asm256_loadu_pd(M2 + i0 + 380);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t41_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 15), _t41_15)));
    _t41_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 15), _t41_15)));
    _t41_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 15), _t41_15)));
    _t41_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 15), _t41_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t41_4 = _mm256_add_pd(_t41_4, _t41_8);
    _t41_5 = _mm256_add_pd(_t41_5, _t41_9);
    _t41_6 = _mm256_add_pd(_t41_6, _t41_10);
    _t41_7 = _mm256_add_pd(_t41_7, _t41_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 320, _t41_4);
    _asm256_storeu_pd(M2 + i0 + 340, _t41_5);
    _asm256_storeu_pd(M2 + i0 + 360, _t41_6);
    _asm256_storeu_pd(M2 + i0 + 380, _t41_7);
  }


  // Generating : M3[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M1[20,20],h(4, 20, 0)) * T( G(h(4, 20, k2), H[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), R[20,20],h(4, 20, k2)) ),h(4, 20, k2)) + Sum_{i0} ( S(h(4, 20, k2), ( ( G(h(4, 20, k2), M1[20,20],h(4, 20, 0)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, k2), R[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + S(h(4, 20, 16), ( ( G(h(4, 20, 16), M1[20,20],h(4, 20, 0)) * T( G(h(4, 20, 16), H[20,20],h(4, 20, 0)) ) ) + G(h(4, 20, 16), R[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 20, k2), ( G(h(4, 20, k2), M1[20,20],h(4, 20, k3)) * T( G(h(4, 20, k2), H[20,20],h(4, 20, k3)) ) ),h(4, 20, k2)) + Sum_{i0} ( $(h(4, 20, k2), ( G(h(4, 20, k2), M1[20,20],h(4, 20, k3)) * T( G(h(4, 20, i0), H[20,20],h(4, 20, k3)) ) ),h(4, 20, i0)) ) ) ) + $(h(4, 20, 16), ( G(h(4, 20, 16), M1[20,20],h(4, 20, k3)) * T( G(h(4, 20, 16), H[20,20],h(4, 20, k3)) ) ),h(4, 20, 16)) ) ) )


  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t42_23 = _mm256_broadcast_sd(M1 + 20*k2);
    _t42_22 = _mm256_broadcast_sd(M1 + 20*k2 + 1);
    _t42_21 = _mm256_broadcast_sd(M1 + 20*k2 + 2);
    _t42_20 = _mm256_broadcast_sd(M1 + 20*k2 + 3);
    _t42_19 = _mm256_broadcast_sd(M1 + 20*k2 + 20);
    _t42_18 = _mm256_broadcast_sd(M1 + 20*k2 + 21);
    _t42_17 = _mm256_broadcast_sd(M1 + 20*k2 + 22);
    _t42_16 = _mm256_broadcast_sd(M1 + 20*k2 + 23);
    _t42_15 = _mm256_broadcast_sd(M1 + 20*k2 + 40);
    _t42_14 = _mm256_broadcast_sd(M1 + 20*k2 + 41);
    _t42_13 = _mm256_broadcast_sd(M1 + 20*k2 + 42);
    _t42_12 = _mm256_broadcast_sd(M1 + 20*k2 + 43);
    _t42_11 = _mm256_broadcast_sd(M1 + 20*k2 + 60);
    _t42_10 = _mm256_broadcast_sd(M1 + 20*k2 + 61);
    _t42_9 = _mm256_broadcast_sd(M1 + 20*k2 + 62);
    _t42_8 = _mm256_broadcast_sd(M1 + 20*k2 + 63);
    _t42_7 = _asm256_loadu_pd(H + 20*k2);
    _t42_6 = _asm256_loadu_pd(H + 20*k2 + 20);
    _t42_5 = _asm256_loadu_pd(H + 20*k2 + 40);
    _t42_4 = _asm256_loadu_pd(H + 20*k2 + 60);
    _t42_3 = _asm256_loadu_pd(R + 21*k2);
    _t42_2 = _mm256_maskload_pd(R + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t42_1 = _mm256_maskload_pd(R + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t42_0 = _mm256_maskload_pd(R + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t42_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 32);
    _t42_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 32);
    _t42_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 49);
    _t42_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t42_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t42_40), _mm256_mul_pd(_t42_22, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t42_42), _mm256_mul_pd(_t42_20, _t42_43)));
    _t42_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t42_40), _mm256_mul_pd(_t42_18, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t42_42), _mm256_mul_pd(_t42_16, _t42_43)));
    _t42_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_40), _mm256_mul_pd(_t42_14, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_42), _mm256_mul_pd(_t42_12, _t42_43)));
    _t42_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_40), _mm256_mul_pd(_t42_10, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_42), _mm256_mul_pd(_t42_8, _t42_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t42_36 = _t42_3;
    _t42_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 3), _t42_2, 12);
    _t42_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 0), _t42_1, 49);
    _t42_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 12), _mm256_shuffle_pd(_t42_1, _t42_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t42_24 = _mm256_add_pd(_t42_32, _t42_36);
    _t42_25 = _mm256_add_pd(_t42_33, _t42_37);
    _t42_26 = _mm256_add_pd(_t42_34, _t42_38);
    _t42_27 = _mm256_add_pd(_t42_35, _t42_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t42_28 = _t42_24;
    _t42_29 = _t42_25;
    _t42_30 = _t42_26;
    _t42_31 = _t42_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
      _t43_7 = _asm256_loadu_pd(H + 20*i0);
      _t43_6 = _asm256_loadu_pd(H + 20*i0 + 20);
      _t43_5 = _asm256_loadu_pd(H + 20*i0 + 40);
      _t43_4 = _asm256_loadu_pd(H + 20*i0 + 60);
      _t43_3 = _asm256_loadu_pd(R + i0 + 20*k2);
      _t43_2 = _asm256_loadu_pd(R + i0 + 20*k2 + 20);
      _t43_1 = _asm256_loadu_pd(R + i0 + 20*k2 + 40);
      _t43_0 = _asm256_loadu_pd(R + i0 + 20*k2 + 60);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t43_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 32);
      _t43_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 32);
      _t43_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 49);
      _t43_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t43_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t43_16), _mm256_mul_pd(_t42_22, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t43_18), _mm256_mul_pd(_t42_20, _t43_19)));
      _t43_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t43_16), _mm256_mul_pd(_t42_18, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t43_18), _mm256_mul_pd(_t42_16, _t43_19)));
      _t43_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t43_16), _mm256_mul_pd(_t42_14, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t43_18), _mm256_mul_pd(_t42_12, _t43_19)));
      _t43_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t43_16), _mm256_mul_pd(_t42_10, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t43_18), _mm256_mul_pd(_t42_8, _t43_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t43_8 = _mm256_add_pd(_t43_12, _t43_3);
      _t43_9 = _mm256_add_pd(_t43_13, _t43_2);
      _t43_10 = _mm256_add_pd(_t43_14, _t43_1);
      _t43_11 = _mm256_add_pd(_t43_15, _t43_0);

      // AVX Storer:
      _asm256_storeu_pd(M3 + i0 + 20*k2, _t43_8);
      _asm256_storeu_pd(M3 + i0 + 20*k2 + 20, _t43_9);
      _asm256_storeu_pd(M3 + i0 + 20*k2 + 40, _t43_10);
      _asm256_storeu_pd(M3 + i0 + 20*k2 + 60, _t43_11);
    }
    _asm256_storeu_pd(M3 + 21*k2, _t42_28);
    _mm256_maskstore_pd(M3 + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t42_29);
    _mm256_maskstore_pd(M3 + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t42_30);
    _mm256_maskstore_pd(M3 + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t42_31);
  }

  _t44_23 = _mm256_broadcast_sd(M1 + 320);
  _t44_22 = _mm256_broadcast_sd(M1 + 321);
  _t44_21 = _mm256_broadcast_sd(M1 + 322);
  _t44_20 = _mm256_broadcast_sd(M1 + 323);
  _t44_19 = _mm256_broadcast_sd(M1 + 340);
  _t44_18 = _mm256_broadcast_sd(M1 + 341);
  _t44_17 = _mm256_broadcast_sd(M1 + 342);
  _t44_16 = _mm256_broadcast_sd(M1 + 343);
  _t44_15 = _mm256_broadcast_sd(M1 + 360);
  _t44_14 = _mm256_broadcast_sd(M1 + 361);
  _t44_13 = _mm256_broadcast_sd(M1 + 362);
  _t44_12 = _mm256_broadcast_sd(M1 + 363);
  _t44_11 = _mm256_broadcast_sd(M1 + 380);
  _t44_10 = _mm256_broadcast_sd(M1 + 381);
  _t44_9 = _mm256_broadcast_sd(M1 + 382);
  _t44_8 = _mm256_broadcast_sd(M1 + 383);
  _t44_7 = _asm256_loadu_pd(H + 320);
  _t44_6 = _asm256_loadu_pd(H + 340);
  _t44_5 = _asm256_loadu_pd(H + 360);
  _t44_4 = _asm256_loadu_pd(H + 380);
  _t44_3 = _asm256_loadu_pd(R + 336);
  _t44_2 = _mm256_maskload_pd(R + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t44_1 = _mm256_maskload_pd(R + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t44_0 = _mm256_maskload_pd(R + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t44_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 32);
  _t44_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 32);
  _t44_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 49);
  _t44_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t44_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_23, _t44_40), _mm256_mul_pd(_t44_22, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_21, _t44_42), _mm256_mul_pd(_t44_20, _t44_43)));
  _t44_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_19, _t44_40), _mm256_mul_pd(_t44_18, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_17, _t44_42), _mm256_mul_pd(_t44_16, _t44_43)));
  _t44_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_40), _mm256_mul_pd(_t44_14, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_42), _mm256_mul_pd(_t44_12, _t44_43)));
  _t44_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_40), _mm256_mul_pd(_t44_10, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_42), _mm256_mul_pd(_t44_8, _t44_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t44_36 = _t44_3;
  _t44_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 3), _t44_2, 12);
  _t44_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 0), _t44_1, 49);
  _t44_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 12), _mm256_shuffle_pd(_t44_1, _t44_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t44_24 = _mm256_add_pd(_t44_32, _t44_36);
  _t44_25 = _mm256_add_pd(_t44_33, _t44_37);
  _t44_26 = _mm256_add_pd(_t44_34, _t44_38);
  _t44_27 = _mm256_add_pd(_t44_35, _t44_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t44_28 = _t44_24;
  _t44_29 = _t44_25;
  _t44_30 = _t44_26;
  _t44_31 = _t44_27;


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    for( int k2 = 0; k2 <= 15; k2+=4 ) {
      _t45_19 = _mm256_broadcast_sd(M1 + 20*k2 + k3);
      _t45_18 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 1);
      _t45_17 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 2);
      _t45_16 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 3);
      _t45_15 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 20);
      _t45_14 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 21);
      _t45_13 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 22);
      _t45_12 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 23);
      _t45_11 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 40);
      _t45_10 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 41);
      _t45_9 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 42);
      _t45_8 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 43);
      _t45_7 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 60);
      _t45_6 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 61);
      _t45_5 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 62);
      _t45_4 = _mm256_broadcast_sd(M1 + 20*k2 + k3 + 63);
      _t45_3 = _asm256_loadu_pd(H + 20*k2 + k3);
      _t45_2 = _asm256_loadu_pd(H + 20*k2 + k3 + 20);
      _t45_1 = _asm256_loadu_pd(H + 20*k2 + k3 + 40);
      _t45_0 = _asm256_loadu_pd(H + 20*k2 + k3 + 60);
      _t45_20 = _asm256_loadu_pd(M3 + 21*k2);
      _t45_21 = _mm256_maskload_pd(M3 + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t45_22 = _mm256_maskload_pd(M3 + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t45_23 = _mm256_maskload_pd(M3 + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t45_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
      _t45_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
      _t45_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
      _t45_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t45_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_32), _mm256_mul_pd(_t45_18, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_34), _mm256_mul_pd(_t45_16, _t45_35)));
      _t45_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_32), _mm256_mul_pd(_t45_14, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_34), _mm256_mul_pd(_t45_12, _t45_35)));
      _t45_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_32), _mm256_mul_pd(_t45_10, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_34), _mm256_mul_pd(_t45_8, _t45_35)));
      _t45_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_32), _mm256_mul_pd(_t45_6, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_34), _mm256_mul_pd(_t45_4, _t45_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t45_28 = _t45_20;
      _t45_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 3), _t45_21, 12);
      _t45_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 0), _t45_22, 49);
      _t45_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 12), _mm256_shuffle_pd(_t45_22, _t45_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t45_28 = _mm256_add_pd(_t45_28, _t45_24);
      _t45_29 = _mm256_add_pd(_t45_29, _t45_25);
      _t45_30 = _mm256_add_pd(_t45_30, _t45_26);
      _t45_31 = _mm256_add_pd(_t45_31, _t45_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t45_20 = _t45_28;
      _t45_21 = _t45_29;
      _t45_22 = _t45_30;
      _t45_23 = _t45_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t46_3 = _asm256_loadu_pd(H + 20*i0 + k3);
        _t46_2 = _asm256_loadu_pd(H + 20*i0 + k3 + 20);
        _t46_1 = _asm256_loadu_pd(H + 20*i0 + k3 + 40);
        _t46_0 = _asm256_loadu_pd(H + 20*i0 + k3 + 60);
        _t46_4 = _asm256_loadu_pd(M3 + i0 + 20*k2);
        _t46_5 = _asm256_loadu_pd(M3 + i0 + 20*k2 + 20);
        _t46_6 = _asm256_loadu_pd(M3 + i0 + 20*k2 + 40);
        _t46_7 = _asm256_loadu_pd(M3 + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t46_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
        _t46_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
        _t46_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
        _t46_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t46_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t46_12), _mm256_mul_pd(_t45_18, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t46_14), _mm256_mul_pd(_t45_16, _t46_15)));
        _t46_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t46_12), _mm256_mul_pd(_t45_14, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t46_14), _mm256_mul_pd(_t45_12, _t46_15)));
        _t46_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t46_12), _mm256_mul_pd(_t45_10, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t46_14), _mm256_mul_pd(_t45_8, _t46_15)));
        _t46_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t46_12), _mm256_mul_pd(_t45_6, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t46_14), _mm256_mul_pd(_t45_4, _t46_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t46_4 = _mm256_add_pd(_t46_4, _t46_8);
        _t46_5 = _mm256_add_pd(_t46_5, _t46_9);
        _t46_6 = _mm256_add_pd(_t46_6, _t46_10);
        _t46_7 = _mm256_add_pd(_t46_7, _t46_11);

        // AVX Storer:
        _asm256_storeu_pd(M3 + i0 + 20*k2, _t46_4);
        _asm256_storeu_pd(M3 + i0 + 20*k2 + 20, _t46_5);
        _asm256_storeu_pd(M3 + i0 + 20*k2 + 40, _t46_6);
        _asm256_storeu_pd(M3 + i0 + 20*k2 + 60, _t46_7);
      }
      _asm256_storeu_pd(M3 + 21*k2, _t45_20);
      _mm256_maskstore_pd(M3 + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t45_21);
      _mm256_maskstore_pd(M3 + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t45_22);
      _mm256_maskstore_pd(M3 + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t45_23);
    }
    _t47_19 = _mm256_broadcast_sd(M1 + k3 + 320);
    _t47_18 = _mm256_broadcast_sd(M1 + k3 + 321);
    _t47_17 = _mm256_broadcast_sd(M1 + k3 + 322);
    _t47_16 = _mm256_broadcast_sd(M1 + k3 + 323);
    _t47_15 = _mm256_broadcast_sd(M1 + k3 + 340);
    _t47_14 = _mm256_broadcast_sd(M1 + k3 + 341);
    _t47_13 = _mm256_broadcast_sd(M1 + k3 + 342);
    _t47_12 = _mm256_broadcast_sd(M1 + k3 + 343);
    _t47_11 = _mm256_broadcast_sd(M1 + k3 + 360);
    _t47_10 = _mm256_broadcast_sd(M1 + k3 + 361);
    _t47_9 = _mm256_broadcast_sd(M1 + k3 + 362);
    _t47_8 = _mm256_broadcast_sd(M1 + k3 + 363);
    _t47_7 = _mm256_broadcast_sd(M1 + k3 + 380);
    _t47_6 = _mm256_broadcast_sd(M1 + k3 + 381);
    _t47_5 = _mm256_broadcast_sd(M1 + k3 + 382);
    _t47_4 = _mm256_broadcast_sd(M1 + k3 + 383);
    _t47_3 = _asm256_loadu_pd(H + k3 + 320);
    _t47_2 = _asm256_loadu_pd(H + k3 + 340);
    _t47_1 = _asm256_loadu_pd(H + k3 + 360);
    _t47_0 = _asm256_loadu_pd(H + k3 + 380);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32);
    _t47_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 32);
    _t47_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 49);
    _t47_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_19, _t47_28), _mm256_mul_pd(_t47_18, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_17, _t47_30), _mm256_mul_pd(_t47_16, _t47_31)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_28), _mm256_mul_pd(_t47_14, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_30), _mm256_mul_pd(_t47_12, _t47_31)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_28), _mm256_mul_pd(_t47_10, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_30), _mm256_mul_pd(_t47_8, _t47_31)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_28), _mm256_mul_pd(_t47_6, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_30), _mm256_mul_pd(_t47_4, _t47_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t47_24 = _t44_28;
    _t47_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 3), _t44_29, 12);
    _t47_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 0), _t44_30, 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 12), _mm256_shuffle_pd(_t44_30, _t44_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t47_24 = _mm256_add_pd(_t47_24, _t47_20);
    _t47_25 = _mm256_add_pd(_t47_25, _t47_21);
    _t47_26 = _mm256_add_pd(_t47_26, _t47_22);
    _t47_27 = _mm256_add_pd(_t47_27, _t47_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t44_28 = _t47_24;
    _t44_29 = _t47_25;
    _t44_30 = _t47_26;
    _t44_31 = _t47_27;
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21])));
  _t48_4 = _mm256_maskload_pd(M3 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[42])));
  _t48_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[43])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[63])));
  _t48_14 = _asm256_loadu_pd(M3 + 4);
  _t48_11 = _asm256_loadu_pd(M3 + 24);
  _t48_12 = _asm256_loadu_pd(M3 + 44);
  _t48_13 = _asm256_loadu_pd(M3 + 64);

  // Generating : U[20,20] = S(h(1, 20, 0), Sqrt( G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_16 = _t48_0;

  // 4-BLAC: sqrt(1x4)
  _t48_17 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_16)));

  // AVX Storer:
  _t48_0 = _t48_17;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 0), U[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_19 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t48_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_18), _mm256_castpd256_pd128(_t48_19)));

  // AVX Storer:
  _t48_1 = _t48_20;

  // Generating : U[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ),h(3, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_22 = _t48_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_23 = _mm256_mul_pd(_t48_21, _t48_22);

  // AVX Storer:
  _t48_2 = _t48_23;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), U[20,20],h(1, 20, 1)) - ( T( G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) Kro G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_24 = _t48_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: (4x1)^T
  _t48_26 = _t48_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_27 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_28 = _mm256_mul_pd(_t48_26, _t48_27);

  // 4-BLAC: 1x4 - 1x4
  _t48_29 = _mm256_sub_pd(_t48_24, _t48_28);

  // AVX Storer:
  _t48_3 = _t48_29;

  // Generating : U[20,20] = S(h(1, 20, 1), Sqrt( G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_30 = _t48_3;

  // 4-BLAC: sqrt(1x4)
  _t48_31 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_30)));

  // AVX Storer:
  _t48_3 = _t48_31;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 20, 1), U[20,20],h(2, 20, 2)) - ( T( G(h(1, 20, 0), U[20,20],h(1, 20, 1)) ) Kro G(h(1, 20, 0), U[20,20],h(2, 20, 2)) ) ),h(2, 20, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_32 = _t48_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_34 = _t48_33;

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_35 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 6), _mm256_permute2f128_pd(_t48_2, _t48_2, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_36 = _mm256_mul_pd(_t48_34, _t48_35);

  // 4-BLAC: 1x4 - 1x4
  _t48_37 = _mm256_sub_pd(_t48_32, _t48_36);

  // AVX Storer:
  _t48_4 = _t48_37;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 1), U[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_38 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_39 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t48_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_38), _mm256_castpd256_pd128(_t48_39)));

  // AVX Storer:
  _t48_5 = _t48_40;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ),h(2, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_41 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_5, _t48_5, 32), _mm256_permute2f128_pd(_t48_5, _t48_5, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_42 = _t48_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_43 = _mm256_mul_pd(_t48_41, _t48_42);

  // AVX Storer:
  _t48_4 = _t48_43;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), U[20,20],h(1, 20, 2)) - ( T( G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) * G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_44 = _t48_6;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_45 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_46 = _t48_45;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_47 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t48_48 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_mul_pd(_t48_46, _t48_47), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_mul_pd(_t48_46, _t48_47), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_mul_pd(_t48_46, _t48_47), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_49 = _mm256_sub_pd(_t48_44, _t48_48);

  // AVX Storer:
  _t48_6 = _t48_49;

  // Generating : U[20,20] = S(h(1, 20, 2), Sqrt( G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_50 = _t48_6;

  // 4-BLAC: sqrt(1x4)
  _t48_51 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_50)));

  // AVX Storer:
  _t48_6 = _t48_51;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) - ( T( G(h(2, 20, 0), U[20,20],h(1, 20, 2)) ) * G(h(2, 20, 0), U[20,20],h(1, 20, 3)) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_52 = _t48_7;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_53 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_54 = _t48_53;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_55 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 129), _t48_4, 2);

  // 4-BLAC: 1x4 * 4x1
  _t48_56 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_mul_pd(_t48_54, _t48_55), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_mul_pd(_t48_54, _t48_55), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_mul_pd(_t48_54, _t48_55), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_57 = _mm256_sub_pd(_t48_52, _t48_56);

  // AVX Storer:
  _t48_7 = _t48_57;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) Div G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_58 = _t48_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_59 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t48_60 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_58), _mm256_castpd256_pd128(_t48_59)));

  // AVX Storer:
  _t48_7 = _t48_60;

  // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), U[20,20],h(1, 20, 3)) - ( T( G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) * G(h(3, 20, 0), U[20,20],h(1, 20, 3)) ) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_61 = _t48_8;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_62 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // 4-BLAC: (4x1)^T
  _t48_63 = _t48_62;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_64 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t48_65 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_mul_pd(_t48_63, _t48_64), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_mul_pd(_t48_63, _t48_64), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_mul_pd(_t48_63, _t48_64), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_66 = _mm256_sub_pd(_t48_61, _t48_65);

  // AVX Storer:
  _t48_8 = _t48_66;

  // Generating : U[20,20] = S(h(1, 20, 3), Sqrt( G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_67 = _t48_8;

  // 4-BLAC: sqrt(1x4)
  _t48_68 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_67)));

  // AVX Storer:
  _t48_8 = _t48_68;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 2), U[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_69 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_70 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t48_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_69), _mm256_castpd256_pd128(_t48_70)));

  // AVX Storer:
  _t48_9 = _t48_71;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 3), U[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_72 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_73 = _t48_8;

  // 4-BLAC: 1x4 / 1x4
  _t48_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_72), _mm256_castpd256_pd128(_t48_73)));

  // AVX Storer:
  _t48_10 = _t48_74;

  // Generating : U[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_75 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_14 = _mm256_mul_pd(_t48_75, _t48_14);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, 1), ( G(h(3, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) - ( T( G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ) * G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t48_76 = _t48_11;
  _t48_77 = _t48_12;
  _t48_78 = _t48_13;
  _t48_79 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_80 = _t48_2;

  // 4-BLAC: (1x4)^T
  _t48_81 = _t48_80;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_82 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 0), _t48_14);
  _t48_83 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 15), _t48_14);
  _t48_84 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 0), _t48_14);
  _t48_85 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 15), _t48_14);

  // 4-BLAC: 4x4 - 4x4
  _t48_86 = _mm256_sub_pd(_t48_76, _t48_82);
  _t48_87 = _mm256_sub_pd(_t48_77, _t48_83);
  _t48_88 = _mm256_sub_pd(_t48_78, _t48_84);
  _t48_89 = _mm256_sub_pd(_t48_79, _t48_85);

  // AVX Storer:
  _t48_11 = _t48_86;
  _t48_12 = _t48_87;
  _t48_13 = _t48_88;

  // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_90 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_5, _t48_5, 32), _mm256_permute2f128_pd(_t48_5, _t48_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_11 = _mm256_mul_pd(_t48_90, _t48_11);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, 2), ( G(h(2, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) - ( T( G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) * G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t48_91 = _t48_12;
  _t48_92 = _t48_13;
  _t48_93 = _mm256_setzero_pd();
  _t48_94 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_95 = _t48_4;

  // 4-BLAC: (1x4)^T
  _t48_96 = _t48_95;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 0), _t48_11);
  _t48_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 15), _t48_11);
  _t48_99 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 0), _t48_11);
  _t48_100 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 15), _t48_11);

  // 4-BLAC: 4x4 - 4x4
  _t48_101 = _mm256_sub_pd(_t48_91, _t48_97);
  _t48_102 = _mm256_sub_pd(_t48_92, _t48_98);
  _t48_103 = _mm256_sub_pd(_t48_93, _t48_99);
  _t48_104 = _mm256_sub_pd(_t48_94, _t48_100);

  // AVX Storer:
  _t48_12 = _t48_101;
  _t48_13 = _t48_102;

  // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_105 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_12 = _mm256_mul_pd(_t48_105, _t48_12);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) - ( T( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) Kro G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_106 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_7, _t48_7, 32), _mm256_permute2f128_pd(_t48_7, _t48_7, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_107 = _t48_106;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_15 = _mm256_mul_pd(_t48_107, _t48_12);

  // 4-BLAC: 1x4 - 1x4
  _t48_13 = _mm256_sub_pd(_t48_13, _t48_15);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_108 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_10, _t48_10, 32), _mm256_permute2f128_pd(_t48_10, _t48_10, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_13 = _mm256_mul_pd(_t48_108, _t48_13);

  // AVX Storer:


  for( int fi1304 = 4; fi1304 <= 12; fi1304+=4 ) {
    _t49_3 = _asm256_loadu_pd(M3 + fi1304 + 4);
    _t49_0 = _asm256_loadu_pd(M3 + fi1304 + 24);
    _t49_1 = _asm256_loadu_pd(M3 + fi1304 + 44);
    _t49_2 = _asm256_loadu_pd(M3 + fi1304 + 64);

    // Generating : U[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_3 = _mm256_mul_pd(_t49_4, _t49_3);

    // AVX Storer:

    // Generating : U[20,20] = S(h(3, 20, 1), ( G(h(3, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) - ( T( G(h(1, 20, 0), U[20,20],h(3, 20, 1)) ) * G(h(1, 20, 0), U[20,20],h(4, 20, fi1304 + 4)) ) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t49_5 = _t49_0;
    _t49_6 = _t49_1;
    _t49_7 = _t49_2;
    _t49_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t49_9 = _t48_2;

    // 4-BLAC: (1x4)^T
    _t48_81 = _t49_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t48_82 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 0), _t49_3);
    _t48_83 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 15), _t49_3);
    _t48_84 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 0), _t49_3);
    _t48_85 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 15), _t49_3);

    // 4-BLAC: 4x4 - 4x4
    _t49_10 = _mm256_sub_pd(_t49_5, _t48_82);
    _t49_11 = _mm256_sub_pd(_t49_6, _t48_83);
    _t49_12 = _mm256_sub_pd(_t49_7, _t48_84);
    _t49_13 = _mm256_sub_pd(_t49_8, _t48_85);

    // AVX Storer:
    _t49_0 = _t49_10;
    _t49_1 = _t49_11;
    _t49_2 = _t49_12;

    // Generating : U[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_5, _t48_5, 32), _mm256_permute2f128_pd(_t48_5, _t48_5, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_0 = _mm256_mul_pd(_t49_14, _t49_0);

    // AVX Storer:

    // Generating : U[20,20] = S(h(2, 20, 2), ( G(h(2, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) - ( T( G(h(1, 20, 1), U[20,20],h(2, 20, 2)) ) * G(h(1, 20, 1), U[20,20],h(4, 20, fi1304 + 4)) ) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t49_15 = _t49_1;
    _t49_16 = _t49_2;
    _t49_17 = _mm256_setzero_pd();
    _t49_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t49_19 = _t48_4;

    // 4-BLAC: (1x4)^T
    _t48_96 = _t49_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t48_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 0), _t49_0);
    _t48_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 15), _t49_0);
    _t48_99 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 0), _t49_0);
    _t48_100 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 15), _t49_0);

    // 4-BLAC: 4x4 - 4x4
    _t49_20 = _mm256_sub_pd(_t49_15, _t48_97);
    _t49_21 = _mm256_sub_pd(_t49_16, _t48_98);
    _t49_22 = _mm256_sub_pd(_t49_17, _t48_99);
    _t49_23 = _mm256_sub_pd(_t49_18, _t48_100);

    // AVX Storer:
    _t49_1 = _t49_20;
    _t49_2 = _t49_21;

    // Generating : U[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_1 = _mm256_mul_pd(_t49_24, _t49_1);

    // AVX Storer:

    // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) - ( T( G(h(1, 20, 2), U[20,20],h(1, 20, 3)) ) Kro G(h(1, 20, 2), U[20,20],h(4, 20, fi1304 + 4)) ) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_7, _t48_7, 32), _mm256_permute2f128_pd(_t48_7, _t48_7, 32), 0);

    // 4-BLAC: (4x1)^T
    _t48_107 = _t49_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_15 = _mm256_mul_pd(_t48_107, _t49_1);

    // 4-BLAC: 1x4 - 1x4
    _t49_2 = _mm256_sub_pd(_t49_2, _t48_15);

    // AVX Storer:

    // Generating : U[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), U[20,20],h(4, 20, fi1304 + 4)) ),h(4, 20, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_10, _t48_10, 32), _mm256_permute2f128_pd(_t48_10, _t48_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_2 = _mm256_mul_pd(_t49_26, _t49_2);

    // AVX Storer:
    _asm256_storeu_pd(M3 + fi1304 + 4, _t49_3);
    _asm256_storeu_pd(M3 + fi1304 + 24, _t49_0);
    _asm256_storeu_pd(M3 + fi1304 + 44, _t49_1);
    _asm256_storeu_pd(M3 + fi1304 + 64, _t49_2);
  }

  _t50_0 = _asm256_loadu_pd(M3 + 84);
  _t50_1 = _mm256_maskload_pd(M3 + 104, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t50_2 = _mm256_maskload_pd(M3 + 124, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t50_3 = _mm256_maskload_pd(M3 + 144, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : U[20,20] = S(h(4, 20, 4), ( G(h(4, 20, 4), M4[20,20],h(4, 20, 4)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, 4)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, 4)) ) ),h(4, 20, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t50_21 = _t50_0;
  _t50_22 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_0, _t50_1, 3), _t50_1, 12);
  _t50_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_0, _t50_1, 0), _t50_2, 49);
  _t50_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_0, _t50_1, 12), _mm256_shuffle_pd(_t50_2, _t50_3, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t50_78 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 32);
  _t50_79 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 32);
  _t50_80 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 49);
  _t50_81 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t50_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 32), _mm256_permute2f128_pd(_t50_78, _t50_78, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 32), _mm256_permute2f128_pd(_t50_78, _t50_78, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 49), _mm256_permute2f128_pd(_t50_78, _t50_78, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 49), _mm256_permute2f128_pd(_t50_78, _t50_78, 49), 15), _t48_13)));
  _t50_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 32), _mm256_permute2f128_pd(_t50_79, _t50_79, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 32), _mm256_permute2f128_pd(_t50_79, _t50_79, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 49), _mm256_permute2f128_pd(_t50_79, _t50_79, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 49), _mm256_permute2f128_pd(_t50_79, _t50_79, 49), 15), _t48_13)));
  _t50_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 32), _mm256_permute2f128_pd(_t50_80, _t50_80, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 32), _mm256_permute2f128_pd(_t50_80, _t50_80, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 49), _mm256_permute2f128_pd(_t50_80, _t50_80, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 49), _mm256_permute2f128_pd(_t50_80, _t50_80, 49), 15), _t48_13)));
  _t50_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 32), _mm256_permute2f128_pd(_t50_81, _t50_81, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 32), _mm256_permute2f128_pd(_t50_81, _t50_81, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 49), _mm256_permute2f128_pd(_t50_81, _t50_81, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 49), _mm256_permute2f128_pd(_t50_81, _t50_81, 49), 15), _t48_13)));

  // 4-BLAC: 4x4 - 4x4
  _t50_17 = _mm256_sub_pd(_t50_21, _t50_13);
  _t50_18 = _mm256_sub_pd(_t50_22, _t50_14);
  _t50_19 = _mm256_sub_pd(_t50_23, _t50_15);
  _t50_20 = _mm256_sub_pd(_t50_24, _t50_16);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t50_0 = _t50_17;
  _t50_1 = _t50_18;
  _t50_2 = _t50_19;
  _t50_3 = _t50_20;

  // Generating : U[20,20] = S(h(1, 20, 4), Sqrt( G(h(1, 20, 4), U[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t50_0, 1);

  // 4-BLAC: sqrt(1x4)
  _t50_26 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_25)));

  // AVX Storer:
  _t50_4 = _t50_26;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 4), U[20,20],h(1, 20, 4)) ),h(1, 20, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t50_27 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_28 = _t50_4;

  // 4-BLAC: 1x4 / 1x4
  _t50_29 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t50_27), _mm256_castpd256_pd128(_t50_28)));

  // AVX Storer:
  _t50_5 = _t50_29;

  // Generating : U[20,20] = S(h(1, 20, 4), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 4)) Kro G(h(1, 20, 4), U[20,20],h(3, 20, 5)) ),h(3, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_30 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_5, _t50_5, 32), _mm256_permute2f128_pd(_t50_5, _t50_5, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t50_31 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_0, 14), _mm256_permute2f128_pd(_t50_0, _t50_0, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t50_32 = _mm256_mul_pd(_t50_30, _t50_31);

  // AVX Storer:
  _t50_6 = _t50_32;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), U[20,20],h(1, 20, 5)) - ( T( G(h(1, 20, 4), U[20,20],h(1, 20, 5)) ) Kro G(h(1, 20, 4), U[20,20],h(1, 20, 5)) ) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_33 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_1, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t50_6, 1);

  // 4-BLAC: (4x1)^T
  _t50_35 = _t50_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_36 = _mm256_blend_pd(_mm256_setzero_pd(), _t50_6, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t50_37 = _mm256_mul_pd(_t50_35, _t50_36);

  // 4-BLAC: 1x4 - 1x4
  _t50_38 = _mm256_sub_pd(_t50_33, _t50_37);

  // AVX Storer:
  _t50_7 = _t50_38;

  // Generating : U[20,20] = S(h(1, 20, 5), Sqrt( G(h(1, 20, 5), U[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_39 = _t50_7;

  // 4-BLAC: sqrt(1x4)
  _t50_40 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_39)));

  // AVX Storer:
  _t50_7 = _t50_40;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 20, 5), U[20,20],h(2, 20, 6)) - ( T( G(h(1, 20, 4), U[20,20],h(1, 20, 5)) ) Kro G(h(1, 20, 4), U[20,20],h(2, 20, 6)) ) ),h(2, 20, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t50_41 = _mm256_permute2f128_pd(_t50_1, _t50_1, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_6, _t50_6, 32), _mm256_permute2f128_pd(_t50_6, _t50_6, 32), 0);

  // 4-BLAC: (4x1)^T
  _t50_43 = _t50_42;

  // AVX Loader:

  // 1x2 -> 1x4
  _t50_44 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_6, 6), _mm256_permute2f128_pd(_t50_6, _t50_6, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t50_45 = _mm256_mul_pd(_t50_43, _t50_44);

  // 4-BLAC: 1x4 - 1x4
  _t50_46 = _mm256_sub_pd(_t50_41, _t50_45);

  // AVX Storer:
  _t50_8 = _t50_46;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 5), U[20,20],h(1, 20, 5)) ),h(1, 20, 5))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t50_47 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_48 = _t50_7;

  // 4-BLAC: 1x4 / 1x4
  _t50_49 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t50_47), _mm256_castpd256_pd128(_t50_48)));

  // AVX Storer:
  _t50_9 = _t50_49;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 5)) Kro G(h(1, 20, 5), U[20,20],h(2, 20, 6)) ),h(2, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_9, _t50_9, 32), _mm256_permute2f128_pd(_t50_9, _t50_9, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t50_51 = _t50_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t50_52 = _mm256_mul_pd(_t50_50, _t50_51);

  // AVX Storer:
  _t50_8 = _t50_52;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), U[20,20],h(1, 20, 6)) - ( T( G(h(2, 20, 4), U[20,20],h(1, 20, 6)) ) * G(h(2, 20, 4), U[20,20],h(1, 20, 6)) ) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_53 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_2, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t50_2, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_54 = _mm256_shuffle_pd(_mm256_blend_pd(_t50_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t50_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t50_55 = _t50_54;

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_56 = _mm256_shuffle_pd(_mm256_blend_pd(_t50_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t50_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t50_57 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_mul_pd(_t50_55, _t50_56), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_mul_pd(_t50_55, _t50_56), 129)), _mm256_add_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_mul_pd(_t50_55, _t50_56), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t50_58 = _mm256_sub_pd(_t50_53, _t50_57);

  // AVX Storer:
  _t50_10 = _t50_58;

  // Generating : U[20,20] = S(h(1, 20, 6), Sqrt( G(h(1, 20, 6), U[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_59 = _t50_10;

  // 4-BLAC: sqrt(1x4)
  _t50_60 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_59)));

  // AVX Storer:
  _t50_10 = _t50_60;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), U[20,20],h(1, 20, 7)) - ( T( G(h(2, 20, 4), U[20,20],h(1, 20, 6)) ) * G(h(2, 20, 4), U[20,20],h(1, 20, 7)) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_61 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t50_2, _t50_2, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_62 = _mm256_shuffle_pd(_mm256_blend_pd(_t50_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t50_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t50_63 = _t50_62;

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_64 = _mm256_blend_pd(_mm256_permute2f128_pd(_t50_6, _t50_6, 129), _t50_8, 2);

  // 4-BLAC: 1x4 * 4x1
  _t50_65 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_mul_pd(_t50_63, _t50_64), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_mul_pd(_t50_63, _t50_64), 129)), _mm256_add_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_mul_pd(_t50_63, _t50_64), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t50_66 = _mm256_sub_pd(_t50_61, _t50_65);

  // AVX Storer:
  _t50_11 = _t50_66;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 20, 6), U[20,20],h(1, 20, 7)) Div G(h(1, 20, 6), U[20,20],h(1, 20, 6)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_67 = _t50_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_68 = _t50_10;

  // 4-BLAC: 1x4 / 1x4
  _t50_69 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t50_67), _mm256_castpd256_pd128(_t50_68)));

  // AVX Storer:
  _t50_11 = _t50_69;

  // Generating : U[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), U[20,20],h(1, 20, 7)) - ( T( G(h(3, 20, 4), U[20,20],h(1, 20, 7)) ) * G(h(3, 20, 4), U[20,20],h(1, 20, 7)) ) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_70 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t50_3, _t50_3, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t50_71 = _mm256_blend_pd(_mm256_permute2f128_pd(_t50_6, _t50_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t50_8, 2), 10);

  // 4-BLAC: (4x1)^T
  _t50_72 = _t50_71;

  // AVX Loader:

  // 3x1 -> 4x1
  _t50_73 = _mm256_blend_pd(_mm256_permute2f128_pd(_t50_6, _t50_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t50_8, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t50_74 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_mul_pd(_t50_72, _t50_73), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_mul_pd(_t50_72, _t50_73), 129)), _mm256_add_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_mul_pd(_t50_72, _t50_73), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t50_75 = _mm256_sub_pd(_t50_70, _t50_74);

  // AVX Storer:
  _t50_12 = _t50_75;

  // Generating : U[20,20] = S(h(1, 20, 7), Sqrt( G(h(1, 20, 7), U[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_76 = _t50_12;

  // 4-BLAC: sqrt(1x4)
  _t50_77 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_76)));

  // AVX Storer:
  _t50_12 = _t50_77;

  // Generating : U[20,20] = Sum_{k2} ( S(h(4, 20, 4), ( G(h(4, 20, 4), M4[20,20],h(4, 20, k2 + 8)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, 4)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, k2 + 8)) ) ),h(4, 20, k2 + 8)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t50_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 32);
  _t50_83 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 32);
  _t50_84 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 49);
  _t50_85 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 49);


  for( int k2 = 0; k2 <= 11; k2+=4 ) {
    _t51_8 = _asm256_loadu_pd(M3 + k2 + 88);
    _t51_9 = _asm256_loadu_pd(M3 + k2 + 108);
    _t51_10 = _asm256_loadu_pd(M3 + k2 + 128);
    _t51_11 = _asm256_loadu_pd(M3 + k2 + 148);
    _t51_3 = _asm256_loadu_pd(M3 + k2 + 8);
    _t51_2 = _asm256_loadu_pd(M3 + k2 + 28);
    _t51_1 = _asm256_loadu_pd(M3 + k2 + 48);
    _t51_0 = _asm256_loadu_pd(M3 + k2 + 68);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t51_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 32), _mm256_permute2f128_pd(_t50_82, _t50_82, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 32), _mm256_permute2f128_pd(_t50_82, _t50_82, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 49), _mm256_permute2f128_pd(_t50_82, _t50_82, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 49), _mm256_permute2f128_pd(_t50_82, _t50_82, 49), 15), _t51_0)));
    _t51_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 32), _mm256_permute2f128_pd(_t50_83, _t50_83, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 32), _mm256_permute2f128_pd(_t50_83, _t50_83, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 49), _mm256_permute2f128_pd(_t50_83, _t50_83, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 49), _mm256_permute2f128_pd(_t50_83, _t50_83, 49), 15), _t51_0)));
    _t51_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 32), _mm256_permute2f128_pd(_t50_84, _t50_84, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 32), _mm256_permute2f128_pd(_t50_84, _t50_84, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 49), _mm256_permute2f128_pd(_t50_84, _t50_84, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 49), _mm256_permute2f128_pd(_t50_84, _t50_84, 49), 15), _t51_0)));
    _t51_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 32), _mm256_permute2f128_pd(_t50_85, _t50_85, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 32), _mm256_permute2f128_pd(_t50_85, _t50_85, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 49), _mm256_permute2f128_pd(_t50_85, _t50_85, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 49), _mm256_permute2f128_pd(_t50_85, _t50_85, 49), 15), _t51_0)));

    // 4-BLAC: 4x4 - 4x4
    _t51_8 = _mm256_sub_pd(_t51_8, _t51_4);
    _t51_9 = _mm256_sub_pd(_t51_9, _t51_5);
    _t51_10 = _mm256_sub_pd(_t51_10, _t51_6);
    _t51_11 = _mm256_sub_pd(_t51_11, _t51_7);

    // AVX Storer:
    _asm256_storeu_pd(M3 + k2 + 88, _t51_8);
    _asm256_storeu_pd(M3 + k2 + 108, _t51_9);
    _asm256_storeu_pd(M3 + k2 + 128, _t51_10);
    _asm256_storeu_pd(M3 + k2 + 148, _t51_11);
  }

  _t52_5 = _asm256_loadu_pd(M3 + 88);
  _t52_2 = _asm256_loadu_pd(M3 + 108);
  _t52_3 = _asm256_loadu_pd(M3 + 128);
  _t52_4 = _asm256_loadu_pd(M3 + 148);

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 6), U[20,20],h(1, 20, 6)) ),h(1, 20, 6))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t52_7 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_8 = _t50_10;

  // 4-BLAC: 1x4 / 1x4
  _t52_9 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_7), _mm256_castpd256_pd128(_t52_8)));

  // AVX Storer:
  _t52_0 = _t52_9;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 7), U[20,20],h(1, 20, 7)) ),h(1, 20, 7))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t52_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_11 = _t50_12;

  // 4-BLAC: 1x4 / 1x4
  _t52_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_10), _mm256_castpd256_pd128(_t52_11)));

  // AVX Storer:
  _t52_1 = _t52_12;

  // Generating : U[20,20] = S(h(1, 20, 4), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 4)) Kro G(h(1, 20, 4), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_5, _t50_5, 32), _mm256_permute2f128_pd(_t50_5, _t50_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_5 = _mm256_mul_pd(_t52_13, _t52_5);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, 5), ( G(h(3, 20, 5), U[20,20],h(4, 20, fi1429 + 8)) - ( T( G(h(1, 20, 4), U[20,20],h(3, 20, 5)) ) * G(h(1, 20, 4), U[20,20],h(4, 20, fi1429 + 8)) ) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // 3x4 -> 4x4
  _t52_14 = _t52_2;
  _t52_15 = _t52_3;
  _t52_16 = _t52_4;
  _t52_17 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t52_18 = _t50_6;

  // 4-BLAC: (1x4)^T
  _t52_19 = _t52_18;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t52_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 0), _t52_5);
  _t52_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 15), _t52_5);
  _t52_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 0), _t52_5);
  _t52_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 15), _t52_5);

  // 4-BLAC: 4x4 - 4x4
  _t52_24 = _mm256_sub_pd(_t52_14, _t52_20);
  _t52_25 = _mm256_sub_pd(_t52_15, _t52_21);
  _t52_26 = _mm256_sub_pd(_t52_16, _t52_22);
  _t52_27 = _mm256_sub_pd(_t52_17, _t52_23);

  // AVX Storer:
  _t52_2 = _t52_24;
  _t52_3 = _t52_25;
  _t52_4 = _t52_26;

  // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 5)) Kro G(h(1, 20, 5), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_9, _t50_9, 32), _mm256_permute2f128_pd(_t50_9, _t50_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_2 = _mm256_mul_pd(_t52_28, _t52_2);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, 6), ( G(h(2, 20, 6), U[20,20],h(4, 20, fi1429 + 8)) - ( T( G(h(1, 20, 5), U[20,20],h(2, 20, 6)) ) * G(h(1, 20, 5), U[20,20],h(4, 20, fi1429 + 8)) ) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // 2x4 -> 4x4
  _t52_29 = _t52_3;
  _t52_30 = _t52_4;
  _t52_31 = _mm256_setzero_pd();
  _t52_32 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t52_33 = _t50_8;

  // 4-BLAC: (1x4)^T
  _t52_34 = _t52_33;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t52_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 0), _t52_2);
  _t52_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 15), _t52_2);
  _t52_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 0), _t52_2);
  _t52_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 15), _t52_2);

  // 4-BLAC: 4x4 - 4x4
  _t52_39 = _mm256_sub_pd(_t52_29, _t52_35);
  _t52_40 = _mm256_sub_pd(_t52_30, _t52_36);
  _t52_41 = _mm256_sub_pd(_t52_31, _t52_37);
  _t52_42 = _mm256_sub_pd(_t52_32, _t52_38);

  // AVX Storer:
  _t52_3 = _t52_39;
  _t52_4 = _t52_40;

  // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 6)) Kro G(h(1, 20, 6), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_0, _t52_0, 32), _mm256_permute2f128_pd(_t52_0, _t52_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_3 = _mm256_mul_pd(_t52_43, _t52_3);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), U[20,20],h(4, 20, fi1429 + 8)) - ( T( G(h(1, 20, 6), U[20,20],h(1, 20, 7)) ) Kro G(h(1, 20, 6), U[20,20],h(4, 20, fi1429 + 8)) ) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_11, _t50_11, 32), _mm256_permute2f128_pd(_t50_11, _t50_11, 32), 0);

  // 4-BLAC: (4x1)^T
  _t52_45 = _t52_44;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_6 = _mm256_mul_pd(_t52_45, _t52_3);

  // 4-BLAC: 1x4 - 1x4
  _t52_4 = _mm256_sub_pd(_t52_4, _t52_6);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, 7), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 7)) Kro G(h(1, 20, 7), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_46 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_1, _t52_1, 32), _mm256_permute2f128_pd(_t52_1, _t52_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_4 = _mm256_mul_pd(_t52_46, _t52_4);

  // AVX Storer:

  _asm256_storeu_pd(M3 + 84, _t50_0);
  _mm256_maskstore_pd(M3 + 104, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t50_1);
  _mm256_maskstore_pd(M3 + 124, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t50_2);

  for( int fi1429 = 4; fi1429 <= 8; fi1429+=4 ) {
    _t53_3 = _asm256_loadu_pd(M3 + fi1429 + 88);
    _t53_0 = _asm256_loadu_pd(M3 + fi1429 + 108);
    _t53_1 = _asm256_loadu_pd(M3 + fi1429 + 128);
    _t53_2 = _asm256_loadu_pd(M3 + fi1429 + 148);

    // Generating : U[20,20] = S(h(1, 20, 4), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 4)) Kro G(h(1, 20, 4), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_5, _t50_5, 32), _mm256_permute2f128_pd(_t50_5, _t50_5, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_3 = _mm256_mul_pd(_t53_4, _t53_3);

    // AVX Storer:

    // Generating : U[20,20] = S(h(3, 20, 5), ( G(h(3, 20, 5), U[20,20],h(4, 20, fi1429 + 8)) - ( T( G(h(1, 20, 4), U[20,20],h(3, 20, 5)) ) * G(h(1, 20, 4), U[20,20],h(4, 20, fi1429 + 8)) ) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // 3x4 -> 4x4
    _t53_5 = _t53_0;
    _t53_6 = _t53_1;
    _t53_7 = _t53_2;
    _t53_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t53_9 = _t50_6;

    // 4-BLAC: (1x4)^T
    _t52_19 = _t53_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t52_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 0), _t53_3);
    _t52_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 15), _t53_3);
    _t52_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 0), _t53_3);
    _t52_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 15), _t53_3);

    // 4-BLAC: 4x4 - 4x4
    _t53_10 = _mm256_sub_pd(_t53_5, _t52_20);
    _t53_11 = _mm256_sub_pd(_t53_6, _t52_21);
    _t53_12 = _mm256_sub_pd(_t53_7, _t52_22);
    _t53_13 = _mm256_sub_pd(_t53_8, _t52_23);

    // AVX Storer:
    _t53_0 = _t53_10;
    _t53_1 = _t53_11;
    _t53_2 = _t53_12;

    // Generating : U[20,20] = S(h(1, 20, 5), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 5)) Kro G(h(1, 20, 5), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_9, _t50_9, 32), _mm256_permute2f128_pd(_t50_9, _t50_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_0 = _mm256_mul_pd(_t53_14, _t53_0);

    // AVX Storer:

    // Generating : U[20,20] = S(h(2, 20, 6), ( G(h(2, 20, 6), U[20,20],h(4, 20, fi1429 + 8)) - ( T( G(h(1, 20, 5), U[20,20],h(2, 20, 6)) ) * G(h(1, 20, 5), U[20,20],h(4, 20, fi1429 + 8)) ) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // 2x4 -> 4x4
    _t53_15 = _t53_1;
    _t53_16 = _t53_2;
    _t53_17 = _mm256_setzero_pd();
    _t53_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t53_19 = _t50_8;

    // 4-BLAC: (1x4)^T
    _t52_34 = _t53_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t52_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 0), _t53_0);
    _t52_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 15), _t53_0);
    _t52_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 0), _t53_0);
    _t52_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 15), _t53_0);

    // 4-BLAC: 4x4 - 4x4
    _t53_20 = _mm256_sub_pd(_t53_15, _t52_35);
    _t53_21 = _mm256_sub_pd(_t53_16, _t52_36);
    _t53_22 = _mm256_sub_pd(_t53_17, _t52_37);
    _t53_23 = _mm256_sub_pd(_t53_18, _t52_38);

    // AVX Storer:
    _t53_1 = _t53_20;
    _t53_2 = _t53_21;

    // Generating : U[20,20] = S(h(1, 20, 6), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 6)) Kro G(h(1, 20, 6), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_0, _t52_0, 32), _mm256_permute2f128_pd(_t52_0, _t52_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_1 = _mm256_mul_pd(_t53_24, _t53_1);

    // AVX Storer:

    // Generating : U[20,20] = S(h(1, 20, 7), ( G(h(1, 20, 7), U[20,20],h(4, 20, fi1429 + 8)) - ( T( G(h(1, 20, 6), U[20,20],h(1, 20, 7)) ) Kro G(h(1, 20, 6), U[20,20],h(4, 20, fi1429 + 8)) ) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_11, _t50_11, 32), _mm256_permute2f128_pd(_t50_11, _t50_11, 32), 0);

    // 4-BLAC: (4x1)^T
    _t52_45 = _t53_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_6 = _mm256_mul_pd(_t52_45, _t53_1);

    // 4-BLAC: 1x4 - 1x4
    _t53_2 = _mm256_sub_pd(_t53_2, _t52_6);

    // AVX Storer:

    // Generating : U[20,20] = S(h(1, 20, 7), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 7)) Kro G(h(1, 20, 7), U[20,20],h(4, 20, fi1429 + 8)) ),h(4, 20, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_1, _t52_1, 32), _mm256_permute2f128_pd(_t52_1, _t52_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_2 = _mm256_mul_pd(_t53_26, _t53_2);

    // AVX Storer:
    _asm256_storeu_pd(M3 + fi1429 + 88, _t53_3);
    _asm256_storeu_pd(M3 + fi1429 + 108, _t53_0);
    _asm256_storeu_pd(M3 + fi1429 + 128, _t53_1);
    _asm256_storeu_pd(M3 + fi1429 + 148, _t53_2);
  }

  _t54_4 = _asm256_loadu_pd(M3 + 168);
  _t54_5 = _mm256_maskload_pd(M3 + 188, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t54_6 = _mm256_maskload_pd(M3 + 208, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t54_7 = _mm256_maskload_pd(M3 + 228, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t54_3 = _asm256_loadu_pd(M3 + 8);
  _t54_2 = _asm256_loadu_pd(M3 + 28);
  _t54_1 = _asm256_loadu_pd(M3 + 48);
  _t54_0 = _asm256_loadu_pd(M3 + 68);

  // Generating : U[20,20] = ( S(h(4, 20, fi1304), ( G(h(4, 20, fi1304), M4[20,20],h(4, 20, fi1304)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304)) + Sum_{k3} ( -$(h(4, 20, fi1304), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t54_29 = _t54_4;
  _t54_30 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_4, _t54_5, 3), _t54_5, 12);
  _t54_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_4, _t54_5, 0), _t54_6, 49);
  _t54_32 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_4, _t54_5, 12), _mm256_shuffle_pd(_t54_6, _t54_7, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_90 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 32);
  _t54_91 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 32);
  _t54_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 49);
  _t54_93 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t54_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 15), _t54_0)));
  _t54_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 15), _t54_0)));
  _t54_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 15), _t54_0)));
  _t54_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 15), _t54_0)));

  // 4-BLAC: 4x4 - 4x4
  _t54_25 = _mm256_sub_pd(_t54_29, _t54_17);
  _t54_26 = _mm256_sub_pd(_t54_30, _t54_18);
  _t54_27 = _mm256_sub_pd(_t54_31, _t54_19);
  _t54_28 = _mm256_sub_pd(_t54_32, _t54_20);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t54_4 = _t54_25;
  _t54_5 = _t54_26;
  _t54_6 = _t54_27;
  _t54_7 = _t54_28;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 32);
  _t54_95 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 32);
  _t54_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 49);
  _t54_97 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t54_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 15), _t52_4)));
  _t54_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 15), _t52_4)));
  _t54_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 15), _t52_4)));
  _t54_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 15), _t52_4)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t54_33 = _t54_4;
  _t54_34 = _t54_5;
  _t54_35 = _t54_6;
  _t54_36 = _t54_7;

  // 4-BLAC: 4x4 - 4x4
  _t54_33 = _mm256_sub_pd(_t54_33, _t54_21);
  _t54_34 = _mm256_sub_pd(_t54_34, _t54_22);
  _t54_35 = _mm256_sub_pd(_t54_35, _t54_23);
  _t54_36 = _mm256_sub_pd(_t54_36, _t54_24);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t54_4 = _t54_33;
  _t54_5 = _t54_34;
  _t54_6 = _t54_35;
  _t54_7 = _t54_36;

  // Generating : U[20,20] = S(h(1, 20, fi1304), Sqrt( G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304)) ),h(1, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_37 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_4, 1);

  // 4-BLAC: sqrt(1x4)
  _t54_38 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_37)));

  // AVX Storer:
  _t54_8 = _t54_38;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304)) ),h(1, 20, fi1304))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t54_39 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_40 = _t54_8;

  // 4-BLAC: 1x4 / 1x4
  _t54_41 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_39), _mm256_castpd256_pd128(_t54_40)));

  // AVX Storer:
  _t54_9 = _t54_41;

  // Generating : U[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), U[20,20],h(3, 20, fi1304 + 1)) ),h(3, 20, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t54_43 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_4, 14), _mm256_permute2f128_pd(_t54_4, _t54_4, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_44 = _mm256_mul_pd(_t54_42, _t54_43);

  // AVX Storer:
  _t54_10 = _t54_44;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 20, fi1304 + 1), U[20,20],h(1, 20, fi1304 + 1)) - ( T( G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304 + 1)) ) Kro G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304 + 1)) ) ),h(1, 20, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_45 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_46 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 1);

  // 4-BLAC: (4x1)^T
  _t54_47 = _t54_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_48 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_49 = _mm256_mul_pd(_t54_47, _t54_48);

  // 4-BLAC: 1x4 - 1x4
  _t54_50 = _mm256_sub_pd(_t54_45, _t54_49);

  // AVX Storer:
  _t54_11 = _t54_50;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), Sqrt( G(h(1, 20, fi1304 + 1), U[20,20],h(1, 20, fi1304 + 1)) ),h(1, 20, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_51 = _t54_11;

  // 4-BLAC: sqrt(1x4)
  _t54_52 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_51)));

  // AVX Storer:
  _t54_11 = _t54_52;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) - ( T( G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304 + 1)) ) Kro G(h(1, 20, fi1304), U[20,20],h(2, 20, fi1304 + 2)) ) ),h(2, 20, fi1304 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t54_53 = _mm256_permute2f128_pd(_t54_5, _t54_5, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_10, _t54_10, 32), _mm256_permute2f128_pd(_t54_10, _t54_10, 32), 0);

  // 4-BLAC: (4x1)^T
  _t54_55 = _t54_54;

  // AVX Loader:

  // 1x2 -> 1x4
  _t54_56 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 6), _mm256_permute2f128_pd(_t54_10, _t54_10, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_57 = _mm256_mul_pd(_t54_55, _t54_56);

  // 4-BLAC: 1x4 - 1x4
  _t54_58 = _mm256_sub_pd(_t54_53, _t54_57);

  // AVX Storer:
  _t54_12 = _t54_58;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 1), U[20,20],h(1, 20, fi1304 + 1)) ),h(1, 20, fi1304 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t54_59 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_60 = _t54_11;

  // 4-BLAC: 1x4 / 1x4
  _t54_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_59), _mm256_castpd256_pd128(_t54_60)));

  // AVX Storer:
  _t54_13 = _t54_61;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) ),h(2, 20, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_62 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_13, _t54_13, 32), _mm256_permute2f128_pd(_t54_13, _t54_13, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t54_63 = _t54_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t54_64 = _mm256_mul_pd(_t54_62, _t54_63);

  // AVX Storer:
  _t54_12 = _t54_64;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) - ( T( G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 2)) ) * G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 2)) ) ),h(1, 20, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_65 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t54_6, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_66 = _mm256_shuffle_pd(_mm256_blend_pd(_t54_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t54_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t54_67 = _t54_66;

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_68 = _mm256_shuffle_pd(_mm256_blend_pd(_t54_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t54_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t54_69 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_mul_pd(_t54_67, _t54_68), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_mul_pd(_t54_67, _t54_68), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_mul_pd(_t54_67, _t54_68), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t54_70 = _mm256_sub_pd(_t54_65, _t54_69);

  // AVX Storer:
  _t54_14 = _t54_70;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), Sqrt( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_71 = _t54_14;

  // 4-BLAC: sqrt(1x4)
  _t54_72 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_71)));

  // AVX Storer:
  _t54_14 = _t54_72;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) - ( T( G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 2)) ) * G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 3)) ) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_73 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t54_6, _t54_6, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_74 = _mm256_shuffle_pd(_mm256_blend_pd(_t54_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t54_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t54_75 = _t54_74;

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_76 = _mm256_blend_pd(_mm256_permute2f128_pd(_t54_10, _t54_10, 129), _t54_12, 2);

  // 4-BLAC: 1x4 * 4x1
  _t54_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_mul_pd(_t54_75, _t54_76), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_mul_pd(_t54_75, _t54_76), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_mul_pd(_t54_75, _t54_76), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t54_78 = _mm256_sub_pd(_t54_73, _t54_77);

  // AVX Storer:
  _t54_15 = _t54_78;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) Div G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_79 = _t54_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_80 = _t54_14;

  // 4-BLAC: 1x4 / 1x4
  _t54_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_79), _mm256_castpd256_pd128(_t54_80)));

  // AVX Storer:
  _t54_15 = _t54_81;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), U[20,20],h(1, 20, fi1304 + 3)) - ( T( G(h(3, 20, fi1304), U[20,20],h(1, 20, fi1304 + 3)) ) * G(h(3, 20, fi1304), U[20,20],h(1, 20, fi1304 + 3)) ) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_82 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t54_7, _t54_7, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t54_83 = _mm256_blend_pd(_mm256_permute2f128_pd(_t54_10, _t54_15, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t54_12, 2), 10);

  // 4-BLAC: (4x1)^T
  _t54_84 = _t54_83;

  // AVX Loader:

  // 3x1 -> 4x1
  _t54_85 = _mm256_blend_pd(_mm256_permute2f128_pd(_t54_10, _t54_15, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t54_12, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t54_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_mul_pd(_t54_84, _t54_85), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_mul_pd(_t54_84, _t54_85), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_mul_pd(_t54_84, _t54_85), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t54_87 = _mm256_sub_pd(_t54_82, _t54_86);

  // AVX Storer:
  _t54_16 = _t54_87;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), Sqrt( G(h(1, 20, fi1304 + 3), U[20,20],h(1, 20, fi1304 + 3)) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_88 = _t54_16;

  // 4-BLAC: sqrt(1x4)
  _t54_89 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_88)));

  // AVX Storer:
  _t54_16 = _t54_89;

  // Generating : U[20,20] = ( Sum_{k2} ( S(h(4, 20, fi1304), ( G(h(4, 20, fi1304), M4[20,20],h(4, 20, fi1304 + k2 + 4)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, fi1304 + k2 + 4)) ) ),h(4, 20, fi1304 + k2 + 4)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 20, fi1304), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, fi1304 + k2 + 4)) ),h(4, 20, fi1304 + k2 + 4)) ) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 32);
  _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 32);
  _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 49);
  _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 49);


  for( int k2 = 0; k2 <= 7; k2+=4 ) {
    _t55_8 = _asm256_loadu_pd(M3 + k2 + 172);
    _t55_9 = _asm256_loadu_pd(M3 + k2 + 192);
    _t55_10 = _asm256_loadu_pd(M3 + k2 + 212);
    _t55_11 = _asm256_loadu_pd(M3 + k2 + 232);
    _t55_3 = _asm256_loadu_pd(M3 + k2 + 12);
    _t55_2 = _asm256_loadu_pd(M3 + k2 + 32);
    _t55_1 = _asm256_loadu_pd(M3 + k2 + 52);
    _t55_0 = _asm256_loadu_pd(M3 + k2 + 72);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 32);
    _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 32);
    _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 49);
    _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t55_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 15), _t55_0)));
    _t55_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 15), _t55_0)));
    _t55_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 15), _t55_0)));
    _t55_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 15), _t55_0)));

    // 4-BLAC: 4x4 - 4x4
    _t55_8 = _mm256_sub_pd(_t55_8, _t55_4);
    _t55_9 = _mm256_sub_pd(_t55_9, _t55_5);
    _t55_10 = _mm256_sub_pd(_t55_10, _t55_6);
    _t55_11 = _mm256_sub_pd(_t55_11, _t55_7);

    // AVX Storer:
    _asm256_storeu_pd(M3 + k2 + 172, _t55_8);
    _asm256_storeu_pd(M3 + k2 + 192, _t55_9);
    _asm256_storeu_pd(M3 + k2 + 212, _t55_10);
    _asm256_storeu_pd(M3 + k2 + 232, _t55_11);
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 32);
  _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 32);
  _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 49);
  _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 49);


  for( int k2 = 0; k2 <= 7; k2+=4 ) {
    _t57_3 = _asm256_loadu_pd(M3 + k2 + 92);
    _t57_2 = _asm256_loadu_pd(M3 + k2 + 112);
    _t57_1 = _asm256_loadu_pd(M3 + k2 + 132);
    _t57_0 = _asm256_loadu_pd(M3 + k2 + 152);
    _t57_4 = _asm256_loadu_pd(M3 + k2 + 172);
    _t57_5 = _asm256_loadu_pd(M3 + k2 + 192);
    _t57_6 = _asm256_loadu_pd(M3 + k2 + 212);
    _t57_7 = _asm256_loadu_pd(M3 + k2 + 232);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 32);
    _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 32);
    _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 49);
    _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t57_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 15), _t57_0)));
    _t57_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 15), _t57_0)));
    _t57_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 15), _t57_0)));
    _t57_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 15), _t57_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t57_4 = _mm256_sub_pd(_t57_4, _t57_8);
    _t57_5 = _mm256_sub_pd(_t57_5, _t57_9);
    _t57_6 = _mm256_sub_pd(_t57_6, _t57_10);
    _t57_7 = _mm256_sub_pd(_t57_7, _t57_11);

    // AVX Storer:
    _asm256_storeu_pd(M3 + k2 + 172, _t57_4);
    _asm256_storeu_pd(M3 + k2 + 192, _t57_5);
    _asm256_storeu_pd(M3 + k2 + 212, _t57_6);
    _asm256_storeu_pd(M3 + k2 + 232, _t57_7);
  }

  _t58_12 = _asm256_loadu_pd(M3 + 252);
  _t58_13 = _mm256_maskload_pd(M3 + 272, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t58_14 = _mm256_maskload_pd(M3 + 292, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t58_15 = _mm256_maskload_pd(M3 + 312, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t58_3 = _asm256_loadu_pd(M3 + 12);
  _t58_2 = _asm256_loadu_pd(M3 + 32);
  _t58_1 = _asm256_loadu_pd(M3 + 52);
  _t58_0 = _asm256_loadu_pd(M3 + 72);
  _t58_16 = _asm256_loadu_pd(M3 + 172);
  _t58_17 = _asm256_loadu_pd(M3 + 176);
  _t58_6 = _asm256_loadu_pd(M3 + 192);
  _t58_9 = _asm256_loadu_pd(M3 + 196);
  _t58_7 = _asm256_loadu_pd(M3 + 212);
  _t58_10 = _asm256_loadu_pd(M3 + 216);
  _t58_8 = _asm256_loadu_pd(M3 + 232);
  _t58_11 = _asm256_loadu_pd(M3 + 236);

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_19 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_20 = _t54_14;

  // 4-BLAC: 1x4 / 1x4
  _t58_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t58_19), _mm256_castpd256_pd128(_t58_20)));

  // AVX Storer:
  _t58_4 = _t58_21;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 3), U[20,20],h(1, 20, fi1304 + 3)) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_22 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_23 = _t54_16;

  // 4-BLAC: 1x4 / 1x4
  _t58_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t58_22), _mm256_castpd256_pd128(_t58_23)));

  // AVX Storer:
  _t58_5 = _t58_24;

  // Generating : U[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_16 = _mm256_mul_pd(_t58_25, _t58_16);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, fi1304 + 1), ( G(h(3, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304), U[20,20],h(3, 20, fi1304 + 1)) ) * G(h(1, 20, fi1304), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t58_26 = _t58_6;
  _t58_27 = _t58_7;
  _t58_28 = _t58_8;
  _t58_29 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_30 = _t54_10;

  // 4-BLAC: (1x4)^T
  _t58_31 = _t58_30;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_32 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 32), _mm256_permute2f128_pd(_t58_31, _t58_31, 32), 0), _t58_16);
  _t58_33 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 32), _mm256_permute2f128_pd(_t58_31, _t58_31, 32), 15), _t58_16);
  _t58_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 49), _mm256_permute2f128_pd(_t58_31, _t58_31, 49), 0), _t58_16);
  _t58_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 49), _mm256_permute2f128_pd(_t58_31, _t58_31, 49), 15), _t58_16);

  // 4-BLAC: 4x4 - 4x4
  _t58_36 = _mm256_sub_pd(_t58_26, _t58_32);
  _t58_37 = _mm256_sub_pd(_t58_27, _t58_33);
  _t58_38 = _mm256_sub_pd(_t58_28, _t58_34);
  _t58_39 = _mm256_sub_pd(_t58_29, _t58_35);

  // AVX Storer:
  _t58_6 = _t58_36;
  _t58_7 = _t58_37;
  _t58_8 = _t58_38;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_40 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_13, _t54_13, 32), _mm256_permute2f128_pd(_t54_13, _t54_13, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_6 = _mm256_mul_pd(_t58_40, _t58_6);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, fi1304 + 2), ( G(h(2, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) ) * G(h(1, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t58_41 = _t58_7;
  _t58_42 = _t58_8;
  _t58_43 = _mm256_setzero_pd();
  _t58_44 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_45 = _t54_12;

  // 4-BLAC: (1x4)^T
  _t58_46 = _t58_45;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_47 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 32), _mm256_permute2f128_pd(_t58_46, _t58_46, 32), 0), _t58_6);
  _t58_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 32), _mm256_permute2f128_pd(_t58_46, _t58_46, 32), 15), _t58_6);
  _t58_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 49), _mm256_permute2f128_pd(_t58_46, _t58_46, 49), 0), _t58_6);
  _t58_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 49), _mm256_permute2f128_pd(_t58_46, _t58_46, 49), 15), _t58_6);

  // 4-BLAC: 4x4 - 4x4
  _t58_51 = _mm256_sub_pd(_t58_41, _t58_47);
  _t58_52 = _mm256_sub_pd(_t58_42, _t58_48);
  _t58_53 = _mm256_sub_pd(_t58_43, _t58_49);
  _t58_54 = _mm256_sub_pd(_t58_44, _t58_50);

  // AVX Storer:
  _t58_7 = _t58_51;
  _t58_8 = _t58_52;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 2)) Kro G(h(1, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_55 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_4, _t58_4, 32), _mm256_permute2f128_pd(_t58_4, _t58_4, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_7 = _mm256_mul_pd(_t58_55, _t58_7);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) ) Kro G(h(1, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_56 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_15, _t54_15, 32), _mm256_permute2f128_pd(_t54_15, _t54_15, 32), 0);

  // 4-BLAC: (4x1)^T
  _t58_57 = _t58_56;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_18 = _mm256_mul_pd(_t58_57, _t58_7);

  // 4-BLAC: 1x4 - 1x4
  _t58_8 = _mm256_sub_pd(_t58_8, _t58_18);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 3)) Kro G(h(1, 20, fi1304 + 3), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_5, _t58_5, 32), _mm256_permute2f128_pd(_t58_5, _t58_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_8 = _mm256_mul_pd(_t58_58, _t58_8);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_59 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_17 = _mm256_mul_pd(_t58_59, _t58_17);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, fi1304 + 1), ( G(h(3, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304), U[20,20],h(3, 20, fi1304 + 1)) ) * G(h(1, 20, fi1304), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t58_60 = _t58_9;
  _t58_61 = _t58_10;
  _t58_62 = _t58_11;
  _t58_63 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_64 = _t54_10;

  // 4-BLAC: (1x4)^T
  _t58_31 = _t58_64;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_32 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 32), _mm256_permute2f128_pd(_t58_31, _t58_31, 32), 0), _t58_17);
  _t58_33 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 32), _mm256_permute2f128_pd(_t58_31, _t58_31, 32), 15), _t58_17);
  _t58_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 49), _mm256_permute2f128_pd(_t58_31, _t58_31, 49), 0), _t58_17);
  _t58_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 49), _mm256_permute2f128_pd(_t58_31, _t58_31, 49), 15), _t58_17);

  // 4-BLAC: 4x4 - 4x4
  _t58_65 = _mm256_sub_pd(_t58_60, _t58_32);
  _t58_66 = _mm256_sub_pd(_t58_61, _t58_33);
  _t58_67 = _mm256_sub_pd(_t58_62, _t58_34);
  _t58_68 = _mm256_sub_pd(_t58_63, _t58_35);

  // AVX Storer:
  _t58_9 = _t58_65;
  _t58_10 = _t58_66;
  _t58_11 = _t58_67;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_69 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_13, _t54_13, 32), _mm256_permute2f128_pd(_t54_13, _t54_13, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_9 = _mm256_mul_pd(_t58_69, _t58_9);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, fi1304 + 2), ( G(h(2, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) ) * G(h(1, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t58_70 = _t58_10;
  _t58_71 = _t58_11;
  _t58_72 = _mm256_setzero_pd();
  _t58_73 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_74 = _t54_12;

  // 4-BLAC: (1x4)^T
  _t58_46 = _t58_74;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_47 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 32), _mm256_permute2f128_pd(_t58_46, _t58_46, 32), 0), _t58_9);
  _t58_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 32), _mm256_permute2f128_pd(_t58_46, _t58_46, 32), 15), _t58_9);
  _t58_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 49), _mm256_permute2f128_pd(_t58_46, _t58_46, 49), 0), _t58_9);
  _t58_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 49), _mm256_permute2f128_pd(_t58_46, _t58_46, 49), 15), _t58_9);

  // 4-BLAC: 4x4 - 4x4
  _t58_75 = _mm256_sub_pd(_t58_70, _t58_47);
  _t58_76 = _mm256_sub_pd(_t58_71, _t58_48);
  _t58_77 = _mm256_sub_pd(_t58_72, _t58_49);
  _t58_78 = _mm256_sub_pd(_t58_73, _t58_50);

  // AVX Storer:
  _t58_10 = _t58_75;
  _t58_11 = _t58_76;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 2)) Kro G(h(1, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_79 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_4, _t58_4, 32), _mm256_permute2f128_pd(_t58_4, _t58_4, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_10 = _mm256_mul_pd(_t58_79, _t58_10);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) ) Kro G(h(1, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_80 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_15, _t54_15, 32), _mm256_permute2f128_pd(_t54_15, _t54_15, 32), 0);

  // 4-BLAC: (4x1)^T
  _t58_57 = _t58_80;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_18 = _mm256_mul_pd(_t58_57, _t58_10);

  // 4-BLAC: 1x4 - 1x4
  _t58_11 = _mm256_sub_pd(_t58_11, _t58_18);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 3)) Kro G(h(1, 20, fi1304 + 3), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_81 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_5, _t58_5, 32), _mm256_permute2f128_pd(_t58_5, _t58_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_11 = _mm256_mul_pd(_t58_81, _t58_11);

  // AVX Storer:

  // Generating : U[20,20] = ( S(h(4, 20, fi1304), ( G(h(4, 20, fi1304), M4[20,20],h(4, 20, fi1304)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304)) + Sum_{k3} ( -$(h(4, 20, fi1304), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t58_82 = _t58_12;
  _t58_83 = _mm256_blend_pd(_mm256_shuffle_pd(_t58_12, _t58_13, 3), _t58_13, 12);
  _t58_84 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t58_12, _t58_13, 0), _t58_14, 49);
  _t58_85 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t58_12, _t58_13, 12), _mm256_shuffle_pd(_t58_14, _t58_15, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_90 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_3, _t58_2), _mm256_unpacklo_pd(_t58_1, _t58_0), 32);
  _t54_91 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_3, _t58_2), _mm256_unpackhi_pd(_t58_1, _t58_0), 32);
  _t54_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_3, _t58_2), _mm256_unpacklo_pd(_t58_1, _t58_0), 49);
  _t54_93 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_3, _t58_2), _mm256_unpackhi_pd(_t58_1, _t58_0), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t54_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 0), _t58_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 15), _t58_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 0), _t58_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 15), _t58_0)));
  _t54_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 0), _t58_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 15), _t58_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 0), _t58_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 15), _t58_0)));
  _t54_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 0), _t58_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 15), _t58_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 0), _t58_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 15), _t58_0)));
  _t54_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 0), _t58_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 15), _t58_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 0), _t58_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 15), _t58_0)));

  // 4-BLAC: 4x4 - 4x4
  _t54_25 = _mm256_sub_pd(_t58_82, _t54_17);
  _t54_26 = _mm256_sub_pd(_t58_83, _t54_18);
  _t54_27 = _mm256_sub_pd(_t58_84, _t54_19);
  _t54_28 = _mm256_sub_pd(_t58_85, _t54_20);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t58_12 = _t54_25;
  _t58_13 = _t54_26;
  _t58_14 = _t54_27;
  _t58_15 = _t54_28;

  _asm256_storeu_pd(M3 + 172, _t58_16);
  _asm256_storeu_pd(M3 + 192, _t58_6);
  _asm256_storeu_pd(M3 + 212, _t58_7);
  _asm256_storeu_pd(M3 + 232, _t58_8);

  for( int k3 = 4; k3 <= 11; k3+=4 ) {
    _t59_3 = _asm256_loadu_pd(M3 + 20*k3 + 12);
    _t59_2 = _asm256_loadu_pd(M3 + 20*k3 + 32);
    _t59_1 = _asm256_loadu_pd(M3 + 20*k3 + 52);
    _t59_0 = _asm256_loadu_pd(M3 + 20*k3 + 72);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t54_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_3, _t59_2), _mm256_unpacklo_pd(_t59_1, _t59_0), 32);
    _t54_95 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t59_3, _t59_2), _mm256_unpackhi_pd(_t59_1, _t59_0), 32);
    _t54_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t59_3, _t59_2), _mm256_unpacklo_pd(_t59_1, _t59_0), 49);
    _t54_97 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t59_3, _t59_2), _mm256_unpackhi_pd(_t59_1, _t59_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t54_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 0), _t59_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 15), _t59_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 0), _t59_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 15), _t59_0)));
    _t54_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 0), _t59_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 15), _t59_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 0), _t59_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 15), _t59_0)));
    _t54_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 0), _t59_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 15), _t59_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 0), _t59_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 15), _t59_0)));
    _t54_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 0), _t59_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 15), _t59_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 0), _t59_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 15), _t59_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpperTriang
    _t59_4 = _t58_12;
    _t59_5 = _t58_13;
    _t59_6 = _t58_14;
    _t59_7 = _t58_15;

    // 4-BLAC: 4x4 - 4x4
    _t59_4 = _mm256_sub_pd(_t59_4, _t54_21);
    _t59_5 = _mm256_sub_pd(_t59_5, _t54_22);
    _t59_6 = _mm256_sub_pd(_t59_6, _t54_23);
    _t59_7 = _mm256_sub_pd(_t59_7, _t54_24);

    // AVX Storer:

    // 4x4 -> 4x4 - UpTriang
    _t58_12 = _t59_4;
    _t58_13 = _t59_5;
    _t58_14 = _t59_6;
    _t58_15 = _t59_7;
  }

  _t60_17 = _asm256_loadu_pd(M3 + 256);
  _t60_18 = _asm256_loadu_pd(M3 + 276);
  _t60_19 = _asm256_loadu_pd(M3 + 296);
  _t60_20 = _asm256_loadu_pd(M3 + 316);
  _t60_3 = _asm256_loadu_pd(M3 + 16);
  _t60_2 = _asm256_loadu_pd(M3 + 36);
  _t60_1 = _asm256_loadu_pd(M3 + 56);
  _t60_0 = _asm256_loadu_pd(M3 + 76);

  // Generating : U[20,20] = S(h(1, 20, fi1304), Sqrt( G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304)) ),h(1, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_21 = _mm256_blend_pd(_mm256_setzero_pd(), _t58_12, 1);

  // 4-BLAC: sqrt(1x4)
  _t60_22 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t60_21)));

  // AVX Storer:
  _t60_4 = _t60_22;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304)) ),h(1, 20, fi1304))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_23 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_24 = _t60_4;

  // 4-BLAC: 1x4 / 1x4
  _t60_25 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_23), _mm256_castpd256_pd128(_t60_24)));

  // AVX Storer:
  _t60_5 = _t60_25;

  // Generating : U[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), U[20,20],h(3, 20, fi1304 + 1)) ),h(3, 20, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_5, _t60_5, 32), _mm256_permute2f128_pd(_t60_5, _t60_5, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t60_27 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t58_12, 14), _mm256_permute2f128_pd(_t58_12, _t58_12, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t60_28 = _mm256_mul_pd(_t60_26, _t60_27);

  // AVX Storer:
  _t60_6 = _t60_28;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 20, fi1304 + 1), U[20,20],h(1, 20, fi1304 + 1)) - ( T( G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304 + 1)) ) Kro G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304 + 1)) ) ),h(1, 20, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t58_13, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_30 = _mm256_blend_pd(_mm256_setzero_pd(), _t60_6, 1);

  // 4-BLAC: (4x1)^T
  _t54_47 = _t60_30;

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t60_6, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_49 = _mm256_mul_pd(_t54_47, _t60_31);

  // 4-BLAC: 1x4 - 1x4
  _t60_32 = _mm256_sub_pd(_t60_29, _t54_49);

  // AVX Storer:
  _t60_7 = _t60_32;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), Sqrt( G(h(1, 20, fi1304 + 1), U[20,20],h(1, 20, fi1304 + 1)) ),h(1, 20, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_33 = _t60_7;

  // 4-BLAC: sqrt(1x4)
  _t60_34 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t60_33)));

  // AVX Storer:
  _t60_7 = _t60_34;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) - ( T( G(h(1, 20, fi1304), U[20,20],h(1, 20, fi1304 + 1)) ) Kro G(h(1, 20, fi1304), U[20,20],h(2, 20, fi1304 + 2)) ) ),h(2, 20, fi1304 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t60_35 = _mm256_permute2f128_pd(_t58_13, _t58_13, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_6, _t60_6, 32), _mm256_permute2f128_pd(_t60_6, _t60_6, 32), 0);

  // 4-BLAC: (4x1)^T
  _t54_55 = _t60_36;

  // AVX Loader:

  // 1x2 -> 1x4
  _t60_37 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t60_6, 6), _mm256_permute2f128_pd(_t60_6, _t60_6, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_57 = _mm256_mul_pd(_t54_55, _t60_37);

  // 4-BLAC: 1x4 - 1x4
  _t60_38 = _mm256_sub_pd(_t60_35, _t54_57);

  // AVX Storer:
  _t60_8 = _t60_38;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 1), U[20,20],h(1, 20, fi1304 + 1)) ),h(1, 20, fi1304 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_39 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_40 = _t60_7;

  // 4-BLAC: 1x4 / 1x4
  _t60_41 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_39), _mm256_castpd256_pd128(_t60_40)));

  // AVX Storer:
  _t60_9 = _t60_41;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) ),h(2, 20, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_9, _t60_9, 32), _mm256_permute2f128_pd(_t60_9, _t60_9, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t60_43 = _t60_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t60_44 = _mm256_mul_pd(_t60_42, _t60_43);

  // AVX Storer:
  _t60_8 = _t60_44;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) - ( T( G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 2)) ) * G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 2)) ) ),h(1, 20, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_45 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t58_14, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t58_14, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t60_46 = _mm256_shuffle_pd(_mm256_blend_pd(_t60_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t60_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t54_67 = _t60_46;

  // AVX Loader:

  // 2x1 -> 4x1
  _t60_47 = _mm256_shuffle_pd(_mm256_blend_pd(_t60_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t60_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t54_69 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t60_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t60_47), _mm256_mul_pd(_t54_67, _t60_47), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t60_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t60_47), _mm256_mul_pd(_t54_67, _t60_47), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_67, _t60_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t60_47), _mm256_mul_pd(_t54_67, _t60_47), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t60_48 = _mm256_sub_pd(_t60_45, _t54_69);

  // AVX Storer:
  _t60_10 = _t60_48;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), Sqrt( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_49 = _t60_10;

  // 4-BLAC: sqrt(1x4)
  _t60_50 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t60_49)));

  // AVX Storer:
  _t60_10 = _t60_50;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) - ( T( G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 2)) ) * G(h(2, 20, fi1304), U[20,20],h(1, 20, fi1304 + 3)) ) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_51 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t58_14, _t58_14, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t60_52 = _mm256_shuffle_pd(_mm256_blend_pd(_t60_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t60_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t54_75 = _t60_52;

  // AVX Loader:

  // 2x1 -> 4x1
  _t60_53 = _mm256_blend_pd(_mm256_permute2f128_pd(_t60_6, _t60_6, 129), _t60_8, 2);

  // 4-BLAC: 1x4 * 4x1
  _t54_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t60_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t60_53), _mm256_mul_pd(_t54_75, _t60_53), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t60_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t60_53), _mm256_mul_pd(_t54_75, _t60_53), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_75, _t60_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t60_53), _mm256_mul_pd(_t54_75, _t60_53), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t60_54 = _mm256_sub_pd(_t60_51, _t54_77);

  // AVX Storer:
  _t60_11 = _t60_54;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) Div G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_55 = _t60_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_56 = _t60_10;

  // 4-BLAC: 1x4 / 1x4
  _t60_57 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t60_55), _mm256_castpd256_pd128(_t60_56)));

  // AVX Storer:
  _t60_11 = _t60_57;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), U[20,20],h(1, 20, fi1304 + 3)) - ( T( G(h(3, 20, fi1304), U[20,20],h(1, 20, fi1304 + 3)) ) * G(h(3, 20, fi1304), U[20,20],h(1, 20, fi1304 + 3)) ) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_58 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t58_15, _t58_15, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t60_59 = _mm256_blend_pd(_mm256_permute2f128_pd(_t60_6, _t60_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t60_8, 2), 10);

  // 4-BLAC: (4x1)^T
  _t54_84 = _t60_59;

  // AVX Loader:

  // 3x1 -> 4x1
  _t60_60 = _mm256_blend_pd(_mm256_permute2f128_pd(_t60_6, _t60_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t60_8, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t54_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t60_60), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t60_60), _mm256_mul_pd(_t54_84, _t60_60), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t60_60), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t60_60), _mm256_mul_pd(_t54_84, _t60_60), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_84, _t60_60), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t60_60), _mm256_mul_pd(_t54_84, _t60_60), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t60_61 = _mm256_sub_pd(_t60_58, _t54_86);

  // AVX Storer:
  _t60_12 = _t60_61;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), Sqrt( G(h(1, 20, fi1304 + 3), U[20,20],h(1, 20, fi1304 + 3)) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_62 = _t60_12;

  // 4-BLAC: sqrt(1x4)
  _t60_63 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t60_62)));

  // AVX Storer:
  _t60_12 = _t60_63;

  // Generating : U[20,20] = ( Sum_{k2} ( S(h(4, 20, fi1304), ( G(h(4, 20, fi1304), M4[20,20],h(4, 20, fi1304 + k2 + 4)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, fi1304 + k2 + 4)) ) ),h(4, 20, fi1304 + k2 + 4)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 20, fi1304), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, fi1304)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, fi1304 + k2 + 4)) ),h(4, 20, fi1304 + k2 + 4)) ) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_3, _t58_2), _mm256_unpacklo_pd(_t58_1, _t58_0), 32);
  _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_3, _t58_2), _mm256_unpackhi_pd(_t58_1, _t58_0), 32);
  _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_3, _t58_2), _mm256_unpacklo_pd(_t58_1, _t58_0), 49);
  _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_3, _t58_2), _mm256_unpackhi_pd(_t58_1, _t58_0), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_3, _t58_2), _mm256_unpacklo_pd(_t58_1, _t58_0), 32);
  _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_3, _t58_2), _mm256_unpackhi_pd(_t58_1, _t58_0), 32);
  _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t58_3, _t58_2), _mm256_unpacklo_pd(_t58_1, _t58_0), 49);
  _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t58_3, _t58_2), _mm256_unpackhi_pd(_t58_1, _t58_0), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t60_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 15), _t60_0)));
  _t60_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 15), _t60_0)));
  _t60_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 15), _t60_0)));
  _t60_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 15), _t60_0)));

  // 4-BLAC: 4x4 - 4x4
  _t60_17 = _mm256_sub_pd(_t60_17, _t60_13);
  _t60_18 = _mm256_sub_pd(_t60_18, _t60_14);
  _t60_19 = _mm256_sub_pd(_t60_19, _t60_15);
  _t60_20 = _mm256_sub_pd(_t60_20, _t60_16);

  // AVX Storer:

  _asm256_storeu_pd(M3 + 176, _t58_17);
  _asm256_storeu_pd(M3 + 196, _t58_9);
  _asm256_storeu_pd(M3 + 216, _t58_10);
  _asm256_storeu_pd(M3 + 236, _t58_11);

  for( int k3 = 4; k3 <= 11; k3+=4 ) {
    _t61_7 = _asm256_loadu_pd(M3 + 20*k3 + 12);
    _t61_6 = _asm256_loadu_pd(M3 + 20*k3 + 32);
    _t61_5 = _asm256_loadu_pd(M3 + 20*k3 + 52);
    _t61_4 = _asm256_loadu_pd(M3 + 20*k3 + 72);
    _t61_3 = _asm256_loadu_pd(M3 + 20*k3 + 16);
    _t61_2 = _asm256_loadu_pd(M3 + 20*k3 + 36);
    _t61_1 = _asm256_loadu_pd(M3 + 20*k3 + 56);
    _t61_0 = _asm256_loadu_pd(M3 + 20*k3 + 76);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_7, _t61_6), _mm256_unpacklo_pd(_t61_5, _t61_4), 32);
    _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_7, _t61_6), _mm256_unpackhi_pd(_t61_5, _t61_4), 32);
    _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_7, _t61_6), _mm256_unpacklo_pd(_t61_5, _t61_4), 49);
    _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_7, _t61_6), _mm256_unpackhi_pd(_t61_5, _t61_4), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_7, _t61_6), _mm256_unpacklo_pd(_t61_5, _t61_4), 32);
    _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_7, _t61_6), _mm256_unpackhi_pd(_t61_5, _t61_4), 32);
    _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_7, _t61_6), _mm256_unpacklo_pd(_t61_5, _t61_4), 49);
    _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_7, _t61_6), _mm256_unpackhi_pd(_t61_5, _t61_4), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t61_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 15), _t61_0)));
    _t61_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 15), _t61_0)));
    _t61_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 15), _t61_0)));
    _t61_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 15), _t61_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t60_17 = _mm256_sub_pd(_t60_17, _t61_8);
    _t60_18 = _mm256_sub_pd(_t60_18, _t61_9);
    _t60_19 = _mm256_sub_pd(_t60_19, _t61_10);
    _t60_20 = _mm256_sub_pd(_t60_20, _t61_11);

    // AVX Storer:
  }


  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t62_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_11 = _t60_10;

  // 4-BLAC: 1x4 / 1x4
  _t62_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t62_10), _mm256_castpd256_pd128(_t62_11)));

  // AVX Storer:
  _t62_0 = _t62_12;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 3), U[20,20],h(1, 20, fi1304 + 3)) ),h(1, 20, fi1304 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t62_13 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_14 = _t60_12;

  // 4-BLAC: 1x4 / 1x4
  _t62_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t62_13), _mm256_castpd256_pd128(_t62_14)));

  // AVX Storer:
  _t62_1 = _t62_15;

  // Generating : U[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_16 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_5, _t60_5, 32), _mm256_permute2f128_pd(_t60_5, _t60_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_17 = _mm256_mul_pd(_t62_16, _t60_17);

  // AVX Storer:

  // Generating : U[20,20] = S(h(3, 20, fi1304 + 1), ( G(h(3, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304), U[20,20],h(3, 20, fi1304 + 1)) ) * G(h(1, 20, fi1304), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t62_17 = _t60_18;
  _t62_18 = _t60_19;
  _t62_19 = _t60_20;
  _t62_20 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t62_21 = _t60_6;

  // 4-BLAC: (1x4)^T
  _t58_31 = _t62_21;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_32 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 32), _mm256_permute2f128_pd(_t58_31, _t58_31, 32), 0), _t60_17);
  _t58_33 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 32), _mm256_permute2f128_pd(_t58_31, _t58_31, 32), 15), _t60_17);
  _t58_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 49), _mm256_permute2f128_pd(_t58_31, _t58_31, 49), 0), _t60_17);
  _t58_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_31, _t58_31, 49), _mm256_permute2f128_pd(_t58_31, _t58_31, 49), 15), _t60_17);

  // 4-BLAC: 4x4 - 4x4
  _t62_22 = _mm256_sub_pd(_t62_17, _t58_32);
  _t62_23 = _mm256_sub_pd(_t62_18, _t58_33);
  _t62_24 = _mm256_sub_pd(_t62_19, _t58_34);
  _t62_25 = _mm256_sub_pd(_t62_20, _t58_35);

  // AVX Storer:
  _t60_18 = _t62_22;
  _t60_19 = _t62_23;
  _t60_20 = _t62_24;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_9, _t60_9, 32), _mm256_permute2f128_pd(_t60_9, _t60_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_18 = _mm256_mul_pd(_t62_26, _t60_18);

  // AVX Storer:

  // Generating : U[20,20] = S(h(2, 20, fi1304 + 2), ( G(h(2, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304 + 1), U[20,20],h(2, 20, fi1304 + 2)) ) * G(h(1, 20, fi1304 + 1), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t62_27 = _t60_19;
  _t62_28 = _t60_20;
  _t62_29 = _mm256_setzero_pd();
  _t62_30 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t62_31 = _t60_8;

  // 4-BLAC: (1x4)^T
  _t58_46 = _t62_31;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_47 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 32), _mm256_permute2f128_pd(_t58_46, _t58_46, 32), 0), _t60_18);
  _t58_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 32), _mm256_permute2f128_pd(_t58_46, _t58_46, 32), 15), _t60_18);
  _t58_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 49), _mm256_permute2f128_pd(_t58_46, _t58_46, 49), 0), _t60_18);
  _t58_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_46, _t58_46, 49), _mm256_permute2f128_pd(_t58_46, _t58_46, 49), 15), _t60_18);

  // 4-BLAC: 4x4 - 4x4
  _t62_32 = _mm256_sub_pd(_t62_27, _t58_47);
  _t62_33 = _mm256_sub_pd(_t62_28, _t58_48);
  _t62_34 = _mm256_sub_pd(_t62_29, _t58_49);
  _t62_35 = _mm256_sub_pd(_t62_30, _t58_50);

  // AVX Storer:
  _t60_19 = _t62_32;
  _t60_20 = _t62_33;

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 2)) Kro G(h(1, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_0, _t62_0, 32), _mm256_permute2f128_pd(_t62_0, _t62_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_19 = _mm256_mul_pd(_t62_36, _t60_19);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) - ( T( G(h(1, 20, fi1304 + 2), U[20,20],h(1, 20, fi1304 + 3)) ) Kro G(h(1, 20, fi1304 + 2), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_37 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t60_11, _t60_11, 32), _mm256_permute2f128_pd(_t60_11, _t60_11, 32), 0);

  // 4-BLAC: (4x1)^T
  _t58_57 = _t62_37;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_18 = _mm256_mul_pd(_t58_57, _t60_19);

  // 4-BLAC: 1x4 - 1x4
  _t60_20 = _mm256_sub_pd(_t60_20, _t58_18);

  // AVX Storer:

  // Generating : U[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, fi1304 + 3)) Kro G(h(1, 20, fi1304 + 3), U[20,20],h(4, 20, fi1304 + fi1429 + 4)) ),h(4, 20, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_38 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_1, _t62_1, 32), _mm256_permute2f128_pd(_t62_1, _t62_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_20 = _mm256_mul_pd(_t62_38, _t60_20);

  // AVX Storer:

  // Generating : U[20,20] = ( S(h(4, 20, 16), ( G(h(4, 20, 16), M4[20,20],h(4, 20, 16)) - ( T( G(h(4, 20, 0), U[20,20],h(4, 20, 16)) ) * G(h(4, 20, 0), U[20,20],h(4, 20, 16)) ) ),h(4, 20, 16)) + Sum_{k3} ( -$(h(4, 20, 16), ( T( G(h(4, 20, k3), U[20,20],h(4, 20, 16)) ) * G(h(4, 20, k3), U[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t62_39 = _t44_28;
  _t62_40 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 3), _t44_29, 12);
  _t62_41 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 0), _t44_30, 49);
  _t62_42 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 12), _mm256_shuffle_pd(_t44_30, _t44_31, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t62_43 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 32);
  _t62_44 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 32);
  _t62_45 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 49);
  _t62_46 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t62_2 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_43, _t62_43, 32), _mm256_permute2f128_pd(_t62_43, _t62_43, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_43, _t62_43, 32), _mm256_permute2f128_pd(_t62_43, _t62_43, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_43, _t62_43, 49), _mm256_permute2f128_pd(_t62_43, _t62_43, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_43, _t62_43, 49), _mm256_permute2f128_pd(_t62_43, _t62_43, 49), 15), _t60_0)));
  _t62_3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_44, _t62_44, 32), _mm256_permute2f128_pd(_t62_44, _t62_44, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_44, _t62_44, 32), _mm256_permute2f128_pd(_t62_44, _t62_44, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_44, _t62_44, 49), _mm256_permute2f128_pd(_t62_44, _t62_44, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_44, _t62_44, 49), _mm256_permute2f128_pd(_t62_44, _t62_44, 49), 15), _t60_0)));
  _t62_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_45, _t62_45, 32), _mm256_permute2f128_pd(_t62_45, _t62_45, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_45, _t62_45, 32), _mm256_permute2f128_pd(_t62_45, _t62_45, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_45, _t62_45, 49), _mm256_permute2f128_pd(_t62_45, _t62_45, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_45, _t62_45, 49), _mm256_permute2f128_pd(_t62_45, _t62_45, 49), 15), _t60_0)));
  _t62_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_46, _t62_46, 32), _mm256_permute2f128_pd(_t62_46, _t62_46, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_46, _t62_46, 32), _mm256_permute2f128_pd(_t62_46, _t62_46, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_46, _t62_46, 49), _mm256_permute2f128_pd(_t62_46, _t62_46, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_46, _t62_46, 49), _mm256_permute2f128_pd(_t62_46, _t62_46, 49), 15), _t60_0)));

  // 4-BLAC: 4x4 - 4x4
  _t62_6 = _mm256_sub_pd(_t62_39, _t62_2);
  _t62_7 = _mm256_sub_pd(_t62_40, _t62_3);
  _t62_8 = _mm256_sub_pd(_t62_41, _t62_4);
  _t62_9 = _mm256_sub_pd(_t62_42, _t62_5);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t44_28 = _t62_6;
  _t44_29 = _t62_7;
  _t44_30 = _t62_8;
  _t44_31 = _t62_9;

  _asm256_storeu_pd(M3 + 256, _t60_17);
  _asm256_storeu_pd(M3 + 276, _t60_18);
  _asm256_storeu_pd(M3 + 296, _t60_19);
  _asm256_storeu_pd(M3 + 316, _t60_20);

  for( int k3 = 4; k3 <= 15; k3+=4 ) {
    _t63_3 = _asm256_loadu_pd(M3 + 20*k3 + 16);
    _t63_2 = _asm256_loadu_pd(M3 + 20*k3 + 36);
    _t63_1 = _asm256_loadu_pd(M3 + 20*k3 + 56);
    _t63_0 = _asm256_loadu_pd(M3 + 20*k3 + 76);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t63_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_3, _t63_2), _mm256_unpacklo_pd(_t63_1, _t63_0), 32);
    _t63_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t63_3, _t63_2), _mm256_unpackhi_pd(_t63_1, _t63_0), 32);
    _t63_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t63_3, _t63_2), _mm256_unpacklo_pd(_t63_1, _t63_0), 49);
    _t63_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t63_3, _t63_2), _mm256_unpackhi_pd(_t63_1, _t63_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t63_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_12, _t63_12, 32), _mm256_permute2f128_pd(_t63_12, _t63_12, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_12, _t63_12, 32), _mm256_permute2f128_pd(_t63_12, _t63_12, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_12, _t63_12, 49), _mm256_permute2f128_pd(_t63_12, _t63_12, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_12, _t63_12, 49), _mm256_permute2f128_pd(_t63_12, _t63_12, 49), 15), _t63_0)));
    _t63_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_13, _t63_13, 32), _mm256_permute2f128_pd(_t63_13, _t63_13, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_13, _t63_13, 32), _mm256_permute2f128_pd(_t63_13, _t63_13, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_13, _t63_13, 49), _mm256_permute2f128_pd(_t63_13, _t63_13, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_13, _t63_13, 49), _mm256_permute2f128_pd(_t63_13, _t63_13, 49), 15), _t63_0)));
    _t63_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_14, _t63_14, 32), _mm256_permute2f128_pd(_t63_14, _t63_14, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_14, _t63_14, 32), _mm256_permute2f128_pd(_t63_14, _t63_14, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_14, _t63_14, 49), _mm256_permute2f128_pd(_t63_14, _t63_14, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_14, _t63_14, 49), _mm256_permute2f128_pd(_t63_14, _t63_14, 49), 15), _t63_0)));
    _t63_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_15, _t63_15, 32), _mm256_permute2f128_pd(_t63_15, _t63_15, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_15, _t63_15, 32), _mm256_permute2f128_pd(_t63_15, _t63_15, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_15, _t63_15, 49), _mm256_permute2f128_pd(_t63_15, _t63_15, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t63_15, _t63_15, 49), _mm256_permute2f128_pd(_t63_15, _t63_15, 49), 15), _t63_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpperTriang
    _t63_8 = _t44_28;
    _t63_9 = _t44_29;
    _t63_10 = _t44_30;
    _t63_11 = _t44_31;

    // 4-BLAC: 4x4 - 4x4
    _t63_8 = _mm256_sub_pd(_t63_8, _t63_4);
    _t63_9 = _mm256_sub_pd(_t63_9, _t63_5);
    _t63_10 = _mm256_sub_pd(_t63_10, _t63_6);
    _t63_11 = _mm256_sub_pd(_t63_11, _t63_7);

    // AVX Storer:

    // 4x4 -> 4x4 - UpTriang
    _t44_28 = _t63_8;
    _t44_29 = _t63_9;
    _t44_30 = _t63_10;
    _t44_31 = _t63_11;
  }


  // Generating : U[20,20] = S(h(1, 20, 16), Sqrt( G(h(1, 20, 16), U[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 1);

  // 4-BLAC: sqrt(1x4)
  _t64_23 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t64_22)));

  // AVX Storer:
  _t64_0 = _t64_23;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 16), U[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t64_24 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_25 = _t64_0;

  // 4-BLAC: 1x4 / 1x4
  _t64_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t64_24), _mm256_castpd256_pd128(_t64_25)));

  // AVX Storer:
  _t64_1 = _t64_26;

  // Generating : U[20,20] = S(h(1, 20, 16), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 16)) Kro G(h(1, 20, 16), U[20,20],h(3, 20, 17)) ),h(3, 20, 17))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_27 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t64_1, _t64_1, 32), _mm256_permute2f128_pd(_t64_1, _t64_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t64_28 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 14), _mm256_permute2f128_pd(_t44_28, _t44_28, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t64_29 = _mm256_mul_pd(_t64_27, _t64_28);

  // AVX Storer:
  _t64_2 = _t64_29;

  // Generating : U[20,20] = S(h(1, 20, 17), ( G(h(1, 20, 17), U[20,20],h(1, 20, 17)) - ( T( G(h(1, 20, 16), U[20,20],h(1, 20, 17)) ) Kro G(h(1, 20, 16), U[20,20],h(1, 20, 17)) ) ),h(1, 20, 17))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_30 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t64_2, 1);

  // 4-BLAC: (4x1)^T
  _t64_32 = _t64_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_33 = _mm256_blend_pd(_mm256_setzero_pd(), _t64_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t64_34 = _mm256_mul_pd(_t64_32, _t64_33);

  // 4-BLAC: 1x4 - 1x4
  _t64_35 = _mm256_sub_pd(_t64_30, _t64_34);

  // AVX Storer:
  _t64_3 = _t64_35;

  // Generating : U[20,20] = S(h(1, 20, 17), Sqrt( G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_36 = _t64_3;

  // 4-BLAC: sqrt(1x4)
  _t64_37 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t64_36)));

  // AVX Storer:
  _t64_3 = _t64_37;

  // Generating : U[20,20] = S(h(1, 20, 17), ( G(h(1, 20, 17), U[20,20],h(2, 20, 18)) - ( T( G(h(1, 20, 16), U[20,20],h(1, 20, 17)) ) Kro G(h(1, 20, 16), U[20,20],h(2, 20, 18)) ) ),h(2, 20, 18))

  // AVX Loader:

  // 1x2 -> 1x4
  _t64_38 = _mm256_permute2f128_pd(_t44_29, _t44_29, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_39 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t64_2, _t64_2, 32), _mm256_permute2f128_pd(_t64_2, _t64_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t64_40 = _t64_39;

  // AVX Loader:

  // 1x2 -> 1x4
  _t64_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t64_2, 6), _mm256_permute2f128_pd(_t64_2, _t64_2, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t64_42 = _mm256_mul_pd(_t64_40, _t64_41);

  // 4-BLAC: 1x4 - 1x4
  _t64_43 = _mm256_sub_pd(_t64_38, _t64_42);

  // AVX Storer:
  _t64_4 = _t64_43;

  // Generating : T2344[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 17), U[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t64_44 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_45 = _t64_3;

  // 4-BLAC: 1x4 / 1x4
  _t64_46 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t64_44), _mm256_castpd256_pd128(_t64_45)));

  // AVX Storer:
  _t64_5 = _t64_46;

  // Generating : U[20,20] = S(h(1, 20, 17), ( G(h(1, 1, 0), T2344[1,20],h(1, 20, 17)) Kro G(h(1, 20, 17), U[20,20],h(2, 20, 18)) ),h(2, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_47 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t64_5, _t64_5, 32), _mm256_permute2f128_pd(_t64_5, _t64_5, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t64_48 = _t64_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t64_49 = _mm256_mul_pd(_t64_47, _t64_48);

  // AVX Storer:
  _t64_4 = _t64_49;

  // Generating : U[20,20] = S(h(1, 20, 18), ( G(h(1, 20, 18), U[20,20],h(1, 20, 18)) - ( T( G(h(2, 20, 16), U[20,20],h(1, 20, 18)) ) * G(h(2, 20, 16), U[20,20],h(1, 20, 18)) ) ),h(1, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_50 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t64_51 = _mm256_shuffle_pd(_mm256_blend_pd(_t64_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t64_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t64_52 = _t64_51;

  // AVX Loader:

  // 2x1 -> 4x1
  _t64_53 = _mm256_shuffle_pd(_mm256_blend_pd(_t64_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t64_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t64_54 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t64_52, _t64_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_52, _t64_53), _mm256_mul_pd(_t64_52, _t64_53), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t64_52, _t64_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_52, _t64_53), _mm256_mul_pd(_t64_52, _t64_53), 129)), _mm256_add_pd(_mm256_mul_pd(_t64_52, _t64_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_52, _t64_53), _mm256_mul_pd(_t64_52, _t64_53), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t64_55 = _mm256_sub_pd(_t64_50, _t64_54);

  // AVX Storer:
  _t64_6 = _t64_55;

  // Generating : U[20,20] = S(h(1, 20, 18), Sqrt( G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ),h(1, 20, 18))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_56 = _t64_6;

  // 4-BLAC: sqrt(1x4)
  _t64_57 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t64_56)));

  // AVX Storer:
  _t64_6 = _t64_57;

  // Generating : U[20,20] = S(h(1, 20, 18), ( G(h(1, 20, 18), U[20,20],h(1, 20, 19)) - ( T( G(h(2, 20, 16), U[20,20],h(1, 20, 18)) ) * G(h(2, 20, 16), U[20,20],h(1, 20, 19)) ) ),h(1, 20, 19))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_58 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_30, _t44_30, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t64_59 = _mm256_shuffle_pd(_mm256_blend_pd(_t64_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t64_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t64_60 = _t64_59;

  // AVX Loader:

  // 2x1 -> 4x1
  _t64_61 = _mm256_blend_pd(_mm256_permute2f128_pd(_t64_2, _t64_2, 129), _t64_4, 2);

  // 4-BLAC: 1x4 * 4x1
  _t64_9 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t64_60, _t64_61), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_60, _t64_61), _mm256_mul_pd(_t64_60, _t64_61), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t64_60, _t64_61), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_60, _t64_61), _mm256_mul_pd(_t64_60, _t64_61), 129)), _mm256_add_pd(_mm256_mul_pd(_t64_60, _t64_61), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_60, _t64_61), _mm256_mul_pd(_t64_60, _t64_61), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t64_10 = _mm256_sub_pd(_t64_58, _t64_9);

  // AVX Storer:
  _t64_7 = _t64_10;

  // Generating : U[20,20] = S(h(1, 20, 18), ( G(h(1, 20, 18), U[20,20],h(1, 20, 19)) Div G(h(1, 20, 18), U[20,20],h(1, 20, 18)) ),h(1, 20, 19))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_11 = _t64_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_12 = _t64_6;

  // 4-BLAC: 1x4 / 1x4
  _t64_13 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t64_11), _mm256_castpd256_pd128(_t64_12)));

  // AVX Storer:
  _t64_7 = _t64_13;

  // Generating : U[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), U[20,20],h(1, 20, 19)) - ( T( G(h(3, 20, 16), U[20,20],h(1, 20, 19)) ) * G(h(3, 20, 16), U[20,20],h(1, 20, 19)) ) ),h(1, 20, 19))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_14 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_31, _t44_31, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t64_15 = _mm256_blend_pd(_mm256_permute2f128_pd(_t64_2, _t64_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t64_4, 2), 10);

  // 4-BLAC: (4x1)^T
  _t64_16 = _t64_15;

  // AVX Loader:

  // 3x1 -> 4x1
  _t64_17 = _mm256_blend_pd(_mm256_permute2f128_pd(_t64_2, _t64_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t64_4, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t64_18 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t64_16, _t64_17), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_16, _t64_17), _mm256_mul_pd(_t64_16, _t64_17), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t64_16, _t64_17), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_16, _t64_17), _mm256_mul_pd(_t64_16, _t64_17), 129)), _mm256_add_pd(_mm256_mul_pd(_t64_16, _t64_17), _mm256_permute2f128_pd(_mm256_mul_pd(_t64_16, _t64_17), _mm256_mul_pd(_t64_16, _t64_17), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t64_19 = _mm256_sub_pd(_t64_14, _t64_18);

  // AVX Storer:
  _t64_8 = _t64_19;

  // Generating : U[20,20] = S(h(1, 20, 19), Sqrt( G(h(1, 20, 19), U[20,20],h(1, 20, 19)) ),h(1, 20, 19))

  // AVX Loader:

  // 1x1 -> 1x4
  _t64_20 = _t64_8;

  // 4-BLAC: sqrt(1x4)
  _t64_21 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t64_20)));

  // AVX Storer:
  _t64_8 = _t64_21;

  _mm_store_sd(&(M3[0]), _mm256_castpd256_pd128(_t48_0));
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t48_2);
  _mm_store_sd(&(M3[21]), _mm256_castpd256_pd128(_t48_3));
  _mm256_maskstore_pd(M3 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_4);
  _mm_store_sd(&(M3[42]), _mm256_castpd256_pd128(_t48_6));
  _mm_store_sd(&(M3[43]), _mm256_castpd256_pd128(_t48_7));
  _mm_store_sd(&(M3[63]), _mm256_castpd256_pd128(_t48_8));
  _asm256_storeu_pd(M3 + 4, _t48_14);
  _asm256_storeu_pd(M3 + 24, _t48_11);
  _asm256_storeu_pd(M3 + 44, _t48_12);
  _asm256_storeu_pd(M3 + 64, _t48_13);
  _mm_store_sd(&(M3[84]), _mm256_castpd256_pd128(_t50_4));
  _mm256_maskstore_pd(M3 + 85, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t50_6);
  _mm_store_sd(&(M3[105]), _mm256_castpd256_pd128(_t50_7));
  _mm256_maskstore_pd(M3 + 106, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t50_8);
  _mm_store_sd(&(M3[126]), _mm256_castpd256_pd128(_t50_10));
  _mm_store_sd(&(M3[127]), _mm256_castpd256_pd128(_t50_11));
  _mm_store_sd(&(M3[147]), _mm256_castpd256_pd128(_t50_12));
  _asm256_storeu_pd(M3 + 88, _t52_5);
  _asm256_storeu_pd(M3 + 108, _t52_2);
  _asm256_storeu_pd(M3 + 128, _t52_3);
  _asm256_storeu_pd(M3 + 148, _t52_4);
  _mm_store_sd(&(M3[168]), _mm256_castpd256_pd128(_t54_8));
  _mm256_maskstore_pd(M3 + 169, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t54_10);
  _mm_store_sd(&(M3[189]), _mm256_castpd256_pd128(_t54_11));
  _mm256_maskstore_pd(M3 + 190, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t54_12);
  _mm_store_sd(&(M3[210]), _mm256_castpd256_pd128(_t54_14));
  _mm_store_sd(&(M3[211]), _mm256_castpd256_pd128(_t54_15));
  _mm_store_sd(&(M3[231]), _mm256_castpd256_pd128(_t54_16));
  _mm_store_sd(&(M3[252]), _mm256_castpd256_pd128(_t60_4));
  _mm256_maskstore_pd(M3 + 253, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t60_6);
  _mm_store_sd(&(M3[273]), _mm256_castpd256_pd128(_t60_7));
  _mm256_maskstore_pd(M3 + 274, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t60_8);
  _mm_store_sd(&(M3[294]), _mm256_castpd256_pd128(_t60_10));
  _mm_store_sd(&(M3[295]), _mm256_castpd256_pd128(_t60_11));
  _mm_store_sd(&(M3[315]), _mm256_castpd256_pd128(_t60_12));

  for( int fi1304 = 0; fi1304 <= 15; fi1304+=4 ) {
    _t65_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304])));
    _t65_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304])));
    _t65_8 = _mm256_maskload_pd(v0 + fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t65_5 = _mm256_maskload_pd(M3 + 21*fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t65_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 21])));
    _t65_3 = _mm256_maskload_pd(M3 + 21*fi1304 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t65_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 42])));
    _t65_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 43])));
    _t65_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 63])));

    // Generating : v2[20,1] = S(h(1, 20, fi1304), ( G(h(1, 20, fi1304), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi1304), U0[20,20],h(1, 20, fi1304)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_13 = _t65_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_14 = _t65_6;

    // 4-BLAC: 1x4 / 1x4
    _t65_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_13), _mm256_castpd256_pd128(_t65_14)));

    // AVX Storer:
    _t65_7 = _t65_15;

    // Generating : v2[20,1] = S(h(3, 20, fi1304 + 1), ( G(h(3, 20, fi1304 + 1), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, fi1304), U0[20,20],h(3, 20, fi1304 + 1)) ) Kro G(h(1, 20, fi1304), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t65_16 = _t65_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t65_17 = _t65_5;

    // 4-BLAC: (1x4)^T
    _t65_18 = _t65_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_7, _t65_7, 32), _mm256_permute2f128_pd(_t65_7, _t65_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t65_20 = _mm256_mul_pd(_t65_18, _t65_19);

    // 4-BLAC: 4x1 - 4x1
    _t65_21 = _mm256_sub_pd(_t65_16, _t65_20);

    // AVX Storer:
    _t65_8 = _t65_21;

    // Generating : v2[20,1] = S(h(1, 20, fi1304 + 1), ( G(h(1, 20, fi1304 + 1), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 1), U0[20,20],h(1, 20, fi1304 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t65_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_23 = _t65_4;

    // 4-BLAC: 1x4 / 1x4
    _t65_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_22), _mm256_castpd256_pd128(_t65_23)));

    // AVX Storer:
    _t65_9 = _t65_24;

    // Generating : v2[20,1] = S(h(2, 20, fi1304 + 2), ( G(h(2, 20, fi1304 + 2), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, fi1304 + 1), U0[20,20],h(2, 20, fi1304 + 2)) ) Kro G(h(1, 20, fi1304 + 1), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t65_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t65_8, 6), _mm256_permute2f128_pd(_t65_8, _t65_8, 129), 5);

    // AVX Loader:

    // 1x2 -> 1x4
    _t65_26 = _t65_3;

    // 4-BLAC: (1x4)^T
    _t65_27 = _t65_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_9, _t65_9, 32), _mm256_permute2f128_pd(_t65_9, _t65_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t65_29 = _mm256_mul_pd(_t65_27, _t65_28);

    // 4-BLAC: 4x1 - 4x1
    _t65_30 = _mm256_sub_pd(_t65_25, _t65_29);

    // AVX Storer:
    _t65_10 = _t65_30;

    // Generating : v2[20,1] = S(h(1, 20, fi1304 + 2), ( G(h(1, 20, fi1304 + 2), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 2), U0[20,20],h(1, 20, fi1304 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t65_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_32 = _t65_2;

    // 4-BLAC: 1x4 / 1x4
    _t65_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_31), _mm256_castpd256_pd128(_t65_32)));

    // AVX Storer:
    _t65_11 = _t65_33;

    // Generating : v2[20,1] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, fi1304 + 2), U0[20,20],h(1, 20, fi1304 + 3)) ) Kro G(h(1, 20, fi1304 + 2), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_34 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t65_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_35 = _t65_1;

    // 4-BLAC: (4x1)^T
    _t65_36 = _t65_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_37 = _t65_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t65_38 = _mm256_mul_pd(_t65_36, _t65_37);

    // 4-BLAC: 1x4 - 1x4
    _t65_39 = _mm256_sub_pd(_t65_34, _t65_38);

    // AVX Storer:
    _t65_12 = _t65_39;

    // Generating : v2[20,1] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 3), U0[20,20],h(1, 20, fi1304 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_40 = _t65_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_41 = _t65_0;

    // 4-BLAC: 1x4 / 1x4
    _t65_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_40), _mm256_castpd256_pd128(_t65_41)));

    // AVX Storer:
    _t65_12 = _t65_42;

    // Generating : v2[20,1] = Sum_{k3} ( S(h(4, 20, fi1304 + k3 + 4), ( G(h(4, 20, fi1304 + k3 + 4), v2[20,1],h(1, 1, 0)) - ( T( G(h(4, 20, fi1304), U0[20,20],h(4, 20, fi1304 + k3 + 4)) ) * G(h(4, 20, fi1304), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[fi1304]), _mm256_castpd256_pd128(_t65_7));
    _mm_store_sd(&(v0[fi1304 + 1]), _mm256_castpd256_pd128(_t65_9));
    _mm_store_sd(&(v0[fi1304 + 2]), _mm256_castpd256_pd128(_t65_11));
    _mm_store_sd(&(v0[fi1304 + 3]), _mm256_castpd256_pd128(_t65_12));

    for( int k3 = 0; k3 <= -fi1304 + 15; k3+=4 ) {
      _t66_9 = _asm256_loadu_pd(v0 + fi1304 + k3 + 4);
      _t66_7 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 4);
      _t66_6 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 24);
      _t66_5 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 44);
      _t66_4 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 64);
      _t66_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304])));
      _t66_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304 + 1])));
      _t66_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304 + 2])));
      _t66_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304 + 3])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t66_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_7, _t66_6), _mm256_unpacklo_pd(_t66_5, _t66_4), 32);
      _t66_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t66_7, _t66_6), _mm256_unpackhi_pd(_t66_5, _t66_4), 32);
      _t66_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_7, _t66_6), _mm256_unpacklo_pd(_t66_5, _t66_4), 49);
      _t66_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t66_7, _t66_6), _mm256_unpackhi_pd(_t66_5, _t66_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t66_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t66_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32)), _mm256_mul_pd(_t66_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t66_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32)), _mm256_mul_pd(_t66_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t66_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32)), _mm256_mul_pd(_t66_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t66_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32)), _mm256_mul_pd(_t66_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t66_3, _t66_2), _mm256_unpacklo_pd(_t66_1, _t66_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t66_9 = _mm256_sub_pd(_t66_9, _t66_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + fi1304 + k3 + 4, _t66_9);
    }
  }

  _t67_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[16])));
  _t67_1 = _mm256_maskload_pd(v0 + 17, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v2[20,1] = S(h(1, 20, 16), ( G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_6 = _t67_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_7 = _t64_0;

  // 4-BLAC: 1x4 / 1x4
  _t67_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t67_6), _mm256_castpd256_pd128(_t67_7)));

  // AVX Storer:
  _t67_0 = _t67_8;

  // Generating : v2[20,1] = S(h(3, 20, 17), ( G(h(3, 20, 17), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) Kro G(h(1, 20, 16), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t67_9 = _t67_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t67_10 = _t64_2;

  // 4-BLAC: (1x4)^T
  _t67_11 = _t67_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_0, _t67_0, 32), _mm256_permute2f128_pd(_t67_0, _t67_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t67_13 = _mm256_mul_pd(_t67_11, _t67_12);

  // 4-BLAC: 4x1 - 4x1
  _t67_14 = _mm256_sub_pd(_t67_9, _t67_13);

  // AVX Storer:
  _t67_1 = _t67_14;

  // Generating : v2[20,1] = S(h(1, 20, 17), ( G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_15 = _mm256_blend_pd(_mm256_setzero_pd(), _t67_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_16 = _t64_3;

  // 4-BLAC: 1x4 / 1x4
  _t67_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t67_15), _mm256_castpd256_pd128(_t67_16)));

  // AVX Storer:
  _t67_2 = _t67_17;

  // Generating : v2[20,1] = S(h(2, 20, 18), ( G(h(2, 20, 18), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) Kro G(h(1, 20, 17), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t67_18 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t67_1, 6), _mm256_permute2f128_pd(_t67_1, _t67_1, 129), 5);

  // AVX Loader:

  // 1x2 -> 1x4
  _t67_19 = _t64_4;

  // 4-BLAC: (1x4)^T
  _t67_20 = _t67_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t67_2, 32), _mm256_permute2f128_pd(_t67_2, _t67_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t67_22 = _mm256_mul_pd(_t67_20, _t67_21);

  // 4-BLAC: 4x1 - 4x1
  _t67_23 = _mm256_sub_pd(_t67_18, _t67_22);

  // AVX Storer:
  _t67_3 = _t67_23;

  // Generating : v2[20,1] = S(h(1, 20, 18), ( G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t67_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_25 = _t64_6;

  // 4-BLAC: 1x4 / 1x4
  _t67_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t67_24), _mm256_castpd256_pd128(_t67_25)));

  // AVX Storer:
  _t67_4 = _t67_26;

  // Generating : v2[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) - ( T( G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) Kro G(h(1, 20, 18), v2[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t67_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_28 = _t64_7;

  // 4-BLAC: (4x1)^T
  _t67_29 = _t67_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_30 = _t67_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t67_31 = _mm256_mul_pd(_t67_29, _t67_30);

  // 4-BLAC: 1x4 - 1x4
  _t67_32 = _mm256_sub_pd(_t67_27, _t67_31);

  // AVX Storer:
  _t67_5 = _t67_32;

  // Generating : v2[20,1] = S(h(1, 20, 19), ( G(h(1, 20, 19), v2[20,1],h(1, 1, 0)) Div G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_33 = _t67_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t67_34 = _t64_8;

  // 4-BLAC: 1x4 / 1x4
  _t67_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t67_33), _mm256_castpd256_pd128(_t67_34)));

  // AVX Storer:
  _t67_5 = _t67_35;

  _mm_store_sd(&(M3[336]), _mm256_castpd256_pd128(_t64_0));
  _mm256_maskstore_pd(M3 + 337, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t64_2);
  _mm_store_sd(&(M3[357]), _mm256_castpd256_pd128(_t64_3));
  _mm256_maskstore_pd(M3 + 358, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t64_4);
  _mm_store_sd(&(M3[378]), _mm256_castpd256_pd128(_t64_6));
  _mm_store_sd(&(M3[379]), _mm256_castpd256_pd128(_t64_7));
  _mm_store_sd(&(M3[399]), _mm256_castpd256_pd128(_t64_8));
  _mm_store_sd(&(v0[16]), _mm256_castpd256_pd128(_t67_0));
  _mm_store_sd(&(v0[17]), _mm256_castpd256_pd128(_t67_2));
  _mm_store_sd(&(v0[18]), _mm256_castpd256_pd128(_t67_4));
  _mm_store_sd(&(v0[19]), _mm256_castpd256_pd128(_t67_5));

  for( int fi1304 = 0; fi1304 <= 15; fi1304+=4 ) {
    _t68_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 19])));
    _t68_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 399])));
    _t68_8 = _mm256_maskload_pd(v0 + -fi1304 + 16, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t68_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 339)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 359))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 379)), 32);
    _t68_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 378])));
    _t68_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 338)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 358)), 0);
    _t68_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 357])));
    _t68_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 337])));
    _t68_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 336])));

    // Generating : v4[20,1] = S(h(1, 20, -fi1304 + 19), ( G(h(1, 20, -fi1304 + 19), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 19), U0[20,20],h(1, 20, -fi1304 + 19)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_13 = _t68_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_14 = _t68_6;

    // 4-BLAC: 1x4 / 1x4
    _t68_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_13), _mm256_castpd256_pd128(_t68_14)));

    // AVX Storer:
    _t68_7 = _t68_15;

    // Generating : v4[20,1] = S(h(3, 20, -fi1304 + 16), ( G(h(3, 20, -fi1304 + 16), v4[20,1],h(1, 1, 0)) - ( G(h(3, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 19)) Kro G(h(1, 20, -fi1304 + 19), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t68_16 = _t68_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t68_17 = _t68_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_7, _t68_7, 32), _mm256_permute2f128_pd(_t68_7, _t68_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t68_19 = _mm256_mul_pd(_t68_17, _t68_18);

    // 4-BLAC: 4x1 - 4x1
    _t68_20 = _mm256_sub_pd(_t68_16, _t68_19);

    // AVX Storer:
    _t68_8 = _t68_20;

    // Generating : v4[20,1] = S(h(1, 20, -fi1304 + 18), ( G(h(1, 20, -fi1304 + 18), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 18), U0[20,20],h(1, 20, -fi1304 + 18)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t68_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t68_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_22 = _t68_4;

    // 4-BLAC: 1x4 / 1x4
    _t68_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_21), _mm256_castpd256_pd128(_t68_22)));

    // AVX Storer:
    _t68_9 = _t68_23;

    // Generating : v4[20,1] = S(h(2, 20, -fi1304 + 16), ( G(h(2, 20, -fi1304 + 16), v4[20,1],h(1, 1, 0)) - ( G(h(2, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 18)) Kro G(h(1, 20, -fi1304 + 18), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t68_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t68_8, 3);

    // AVX Loader:

    // 2x1 -> 4x1
    _t68_25 = _t68_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_9, _t68_9, 32), _mm256_permute2f128_pd(_t68_9, _t68_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t68_27 = _mm256_mul_pd(_t68_25, _t68_26);

    // 4-BLAC: 4x1 - 4x1
    _t68_28 = _mm256_sub_pd(_t68_24, _t68_27);

    // AVX Storer:
    _t68_10 = _t68_28;

    // Generating : v4[20,1] = S(h(1, 20, -fi1304 + 17), ( G(h(1, 20, -fi1304 + 17), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 17), U0[20,20],h(1, 20, -fi1304 + 17)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t68_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_30 = _t68_2;

    // 4-BLAC: 1x4 / 1x4
    _t68_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_29), _mm256_castpd256_pd128(_t68_30)));

    // AVX Storer:
    _t68_11 = _t68_31;

    // Generating : v4[20,1] = S(h(1, 20, -fi1304 + 16), ( G(h(1, 20, -fi1304 + 16), v4[20,1],h(1, 1, 0)) - ( G(h(1, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 17)) Kro G(h(1, 20, -fi1304 + 17), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_32 = _mm256_blend_pd(_mm256_setzero_pd(), _t68_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_33 = _t68_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_34 = _t68_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t68_35 = _mm256_mul_pd(_t68_33, _t68_34);

    // 4-BLAC: 1x4 - 1x4
    _t68_36 = _mm256_sub_pd(_t68_32, _t68_35);

    // AVX Storer:
    _t68_12 = _t68_36;

    // Generating : v4[20,1] = S(h(1, 20, -fi1304 + 16), ( G(h(1, 20, -fi1304 + 16), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 16)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_37 = _t68_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t68_38 = _t68_0;

    // 4-BLAC: 1x4 / 1x4
    _t68_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t68_37), _mm256_castpd256_pd128(_t68_38)));

    // AVX Storer:
    _t68_12 = _t68_39;

    // Generating : v4[20,1] = Sum_{k3} ( S(h(4, 20, k3), ( G(h(4, 20, k3), v4[20,1],h(1, 1, 0)) - ( G(h(4, 20, k3), U0[20,20],h(4, 20, -fi1304 + 16)) * G(h(4, 20, -fi1304 + 16), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[-fi1304 + 19]), _mm256_castpd256_pd128(_t68_7));
    _mm_store_sd(&(v0[-fi1304 + 18]), _mm256_castpd256_pd128(_t68_9));
    _mm_store_sd(&(v0[-fi1304 + 17]), _mm256_castpd256_pd128(_t68_11));
    _mm_store_sd(&(v0[-fi1304 + 16]), _mm256_castpd256_pd128(_t68_12));

    for( int k3 = 0; k3 <= -fi1304 + 15; k3+=4 ) {
      _t69_9 = _asm256_loadu_pd(v0 + k3);
      _t69_7 = _asm256_loadu_pd(M3 + -fi1304 + 20*k3 + 16);
      _t69_6 = _asm256_loadu_pd(M3 + -fi1304 + 20*k3 + 36);
      _t69_5 = _asm256_loadu_pd(M3 + -fi1304 + 20*k3 + 56);
      _t69_4 = _asm256_loadu_pd(M3 + -fi1304 + 20*k3 + 76);
      _t69_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 19])));
      _t69_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 18])));
      _t69_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 17])));
      _t69_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 16])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t69_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t69_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32)), _mm256_mul_pd(_t69_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t69_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32)), _mm256_mul_pd(_t69_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t69_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32)), _mm256_mul_pd(_t69_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t69_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32)), _mm256_mul_pd(_t69_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t69_0, _t69_1), _mm256_unpacklo_pd(_t69_2, _t69_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t69_9 = _mm256_sub_pd(_t69_9, _t69_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k3, _t69_9);
    }
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[63])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t48_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[43])));
  _t48_4 = _mm256_maskload_pd(M3 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[42])));
  _t70_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[3])));
  _t70_1 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v4[20,1] = S(h(1, 20, 3), ( G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_6 = _t70_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_7 = _t48_8;

  // 4-BLAC: 1x4 / 1x4
  _t70_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_6), _mm256_castpd256_pd128(_t70_7)));

  // AVX Storer:
  _t70_0 = _t70_8;

  // Generating : v4[20,1] = S(h(3, 20, 0), ( G(h(3, 20, 0), v4[20,1],h(1, 1, 0)) - ( G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) Kro G(h(1, 20, 3), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t70_9 = _t70_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t70_10 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t70_0, _t70_0, 32), _mm256_permute2f128_pd(_t70_0, _t70_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t70_12 = _mm256_mul_pd(_t70_10, _t70_11);

  // 4-BLAC: 4x1 - 4x1
  _t70_13 = _mm256_sub_pd(_t70_9, _t70_12);

  // AVX Storer:
  _t70_1 = _t70_13;

  // Generating : v4[20,1] = S(h(1, 20, 2), ( G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_14 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t70_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t70_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_15 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t70_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_14), _mm256_castpd256_pd128(_t70_15)));

  // AVX Storer:
  _t70_2 = _t70_16;

  // Generating : v4[20,1] = S(h(2, 20, 0), ( G(h(2, 20, 0), v4[20,1],h(1, 1, 0)) - ( G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) Kro G(h(1, 20, 2), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t70_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t70_1, 3);

  // AVX Loader:

  // 2x1 -> 4x1
  _t70_18 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t70_2, _t70_2, 32), _mm256_permute2f128_pd(_t70_2, _t70_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t70_20 = _mm256_mul_pd(_t70_18, _t70_19);

  // 4-BLAC: 4x1 - 4x1
  _t70_21 = _mm256_sub_pd(_t70_17, _t70_20);

  // AVX Storer:
  _t70_3 = _t70_21;

  // Generating : v4[20,1] = S(h(1, 20, 1), ( G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_22 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t70_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_23 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t70_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_22), _mm256_castpd256_pd128(_t70_23)));

  // AVX Storer:
  _t70_4 = _t70_24;

  // Generating : v4[20,1] = S(h(1, 20, 0), ( G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) - ( G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) Kro G(h(1, 20, 1), v4[20,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t70_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_27 = _t70_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t70_28 = _mm256_mul_pd(_t70_26, _t70_27);

  // 4-BLAC: 1x4 - 1x4
  _t70_29 = _mm256_sub_pd(_t70_25, _t70_28);

  // AVX Storer:
  _t70_5 = _t70_29;

  // Generating : v4[20,1] = S(h(1, 20, 0), ( G(h(1, 20, 0), v4[20,1],h(1, 1, 0)) Div G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_30 = _t70_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t70_31 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t70_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_30), _mm256_castpd256_pd128(_t70_31)));

  // AVX Storer:
  _t70_5 = _t70_32;


  for( int fi1304 = 0; fi1304 <= 15; fi1304+=4 ) {
    _t71_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304])));
    _t71_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 21])));
    _t71_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 42])));
    _t71_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21*fi1304 + 63])));
    _t71_14 = _asm256_loadu_pd(M1 + 20*fi1304);
    _t71_11 = _asm256_loadu_pd(M1 + 20*fi1304 + 20);
    _t71_12 = _asm256_loadu_pd(M1 + 20*fi1304 + 40);
    _t71_13 = _asm256_loadu_pd(M1 + 20*fi1304 + 60);
    _t71_2 = _mm256_maskload_pd(M3 + 21*fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t71_1 = _mm256_maskload_pd(M3 + 21*fi1304 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t71_0 = _mm256_broadcast_sd(&(M3[21*fi1304 + 43]));

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304), U0[20,20],h(1, 20, fi1304)) ),h(1, 20, fi1304))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t71_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_17 = _t71_6;

    // 4-BLAC: 1x4 / 1x4
    _t71_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t71_16), _mm256_castpd256_pd128(_t71_17)));

    // AVX Storer:
    _t71_7 = _t71_18;

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 1), U0[20,20],h(1, 20, fi1304 + 1)) ),h(1, 20, fi1304 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t71_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_20 = _t71_5;

    // 4-BLAC: 1x4 / 1x4
    _t71_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t71_19), _mm256_castpd256_pd128(_t71_20)));

    // AVX Storer:
    _t71_8 = _t71_21;

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 2), U0[20,20],h(1, 20, fi1304 + 2)) ),h(1, 20, fi1304 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t71_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_23 = _t71_4;

    // 4-BLAC: 1x4 / 1x4
    _t71_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t71_22), _mm256_castpd256_pd128(_t71_23)));

    // AVX Storer:
    _t71_9 = _t71_24;

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, fi1304 + 3), U0[20,20],h(1, 20, fi1304 + 3)) ),h(1, 20, fi1304 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t71_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_26 = _t71_3;

    // 4-BLAC: 1x4 / 1x4
    _t71_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t71_25), _mm256_castpd256_pd128(_t71_26)));

    // AVX Storer:
    _t71_10 = _t71_27;

    // Generating : M6[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_7, _t71_7, 32), _mm256_permute2f128_pd(_t71_7, _t71_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t71_14 = _mm256_mul_pd(_t71_28, _t71_14);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(3, 20, fi1304 + 1), ( G(h(3, 20, fi1304 + 1), M6[20,20],h(4, 20, fi1429)) - ( T( G(h(1, 20, fi1304), U0[20,20],h(3, 20, fi1304 + 1)) ) * G(h(1, 20, fi1304), M6[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

    // AVX Loader:

    // 3x4 -> 4x4
    _t71_29 = _t71_11;
    _t71_30 = _t71_12;
    _t71_31 = _t71_13;
    _t71_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t71_33 = _t71_2;

    // 4-BLAC: (1x4)^T
    _t71_34 = _t71_33;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t71_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 32), _mm256_permute2f128_pd(_t71_34, _t71_34, 32), 0), _t71_14);
    _t71_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 32), _mm256_permute2f128_pd(_t71_34, _t71_34, 32), 15), _t71_14);
    _t71_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 49), _mm256_permute2f128_pd(_t71_34, _t71_34, 49), 0), _t71_14);
    _t71_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 49), _mm256_permute2f128_pd(_t71_34, _t71_34, 49), 15), _t71_14);

    // 4-BLAC: 4x4 - 4x4
    _t71_39 = _mm256_sub_pd(_t71_29, _t71_35);
    _t71_40 = _mm256_sub_pd(_t71_30, _t71_36);
    _t71_41 = _mm256_sub_pd(_t71_31, _t71_37);
    _t71_42 = _mm256_sub_pd(_t71_32, _t71_38);

    // AVX Storer:
    _t71_11 = _t71_39;
    _t71_12 = _t71_40;
    _t71_13 = _t71_41;

    // Generating : M6[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_8, _t71_8, 32), _mm256_permute2f128_pd(_t71_8, _t71_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t71_11 = _mm256_mul_pd(_t71_43, _t71_11);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(2, 20, fi1304 + 2), ( G(h(2, 20, fi1304 + 2), M6[20,20],h(4, 20, fi1429)) - ( T( G(h(1, 20, fi1304 + 1), U0[20,20],h(2, 20, fi1304 + 2)) ) * G(h(1, 20, fi1304 + 1), M6[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

    // AVX Loader:

    // 2x4 -> 4x4
    _t71_44 = _t71_12;
    _t71_45 = _t71_13;
    _t71_46 = _mm256_setzero_pd();
    _t71_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t71_48 = _t71_1;

    // 4-BLAC: (1x4)^T
    _t71_49 = _t71_48;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t71_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 32), _mm256_permute2f128_pd(_t71_49, _t71_49, 32), 0), _t71_11);
    _t71_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 32), _mm256_permute2f128_pd(_t71_49, _t71_49, 32), 15), _t71_11);
    _t71_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 49), _mm256_permute2f128_pd(_t71_49, _t71_49, 49), 0), _t71_11);
    _t71_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 49), _mm256_permute2f128_pd(_t71_49, _t71_49, 49), 15), _t71_11);

    // 4-BLAC: 4x4 - 4x4
    _t71_54 = _mm256_sub_pd(_t71_44, _t71_50);
    _t71_55 = _mm256_sub_pd(_t71_45, _t71_51);
    _t71_56 = _mm256_sub_pd(_t71_46, _t71_52);
    _t71_57 = _mm256_sub_pd(_t71_47, _t71_53);

    // AVX Storer:
    _t71_12 = _t71_54;
    _t71_13 = _t71_55;

    // Generating : M6[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304 + 2)) Kro G(h(1, 20, fi1304 + 2), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_9, _t71_9, 32), _mm256_permute2f128_pd(_t71_9, _t71_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t71_12 = _mm256_mul_pd(_t71_58, _t71_12);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), M6[20,20],h(4, 20, fi1429)) - ( T( G(h(1, 20, fi1304 + 2), U0[20,20],h(1, 20, fi1304 + 3)) ) Kro G(h(1, 20, fi1304 + 2), M6[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_59 = _t71_0;

    // 4-BLAC: (4x1)^T
    _t71_60 = _t71_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t71_15 = _mm256_mul_pd(_t71_60, _t71_12);

    // 4-BLAC: 1x4 - 1x4
    _t71_13 = _mm256_sub_pd(_t71_13, _t71_15);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304 + 3)) Kro G(h(1, 20, fi1304 + 3), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t71_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_10, _t71_10, 32), _mm256_permute2f128_pd(_t71_10, _t71_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t71_13 = _mm256_mul_pd(_t71_61, _t71_13);

    // AVX Storer:

    for( int fi1429 = 4; fi1429 <= 16; fi1429+=4 ) {
      _t72_3 = _asm256_loadu_pd(M1 + 20*fi1304 + fi1429);
      _t72_0 = _asm256_loadu_pd(M1 + 20*fi1304 + fi1429 + 20);
      _t72_1 = _asm256_loadu_pd(M1 + 20*fi1304 + fi1429 + 40);
      _t72_2 = _asm256_loadu_pd(M1 + 20*fi1304 + fi1429 + 60);

      // Generating : M6[20,20] = S(h(1, 20, fi1304), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304)) Kro G(h(1, 20, fi1304), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t72_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_7, _t71_7, 32), _mm256_permute2f128_pd(_t71_7, _t71_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t72_3 = _mm256_mul_pd(_t72_4, _t72_3);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(3, 20, fi1304 + 1), ( G(h(3, 20, fi1304 + 1), M6[20,20],h(4, 20, fi1429)) - ( T( G(h(1, 20, fi1304), U0[20,20],h(3, 20, fi1304 + 1)) ) * G(h(1, 20, fi1304), M6[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

      // AVX Loader:

      // 3x4 -> 4x4
      _t72_5 = _t72_0;
      _t72_6 = _t72_1;
      _t72_7 = _t72_2;
      _t72_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t72_9 = _t71_2;

      // 4-BLAC: (1x4)^T
      _t71_34 = _t72_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t71_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 32), _mm256_permute2f128_pd(_t71_34, _t71_34, 32), 0), _t72_3);
      _t71_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 32), _mm256_permute2f128_pd(_t71_34, _t71_34, 32), 15), _t72_3);
      _t71_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 49), _mm256_permute2f128_pd(_t71_34, _t71_34, 49), 0), _t72_3);
      _t71_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_34, _t71_34, 49), _mm256_permute2f128_pd(_t71_34, _t71_34, 49), 15), _t72_3);

      // 4-BLAC: 4x4 - 4x4
      _t72_10 = _mm256_sub_pd(_t72_5, _t71_35);
      _t72_11 = _mm256_sub_pd(_t72_6, _t71_36);
      _t72_12 = _mm256_sub_pd(_t72_7, _t71_37);
      _t72_13 = _mm256_sub_pd(_t72_8, _t71_38);

      // AVX Storer:
      _t72_0 = _t72_10;
      _t72_1 = _t72_11;
      _t72_2 = _t72_12;

      // Generating : M6[20,20] = S(h(1, 20, fi1304 + 1), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304 + 1)) Kro G(h(1, 20, fi1304 + 1), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t72_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_8, _t71_8, 32), _mm256_permute2f128_pd(_t71_8, _t71_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t72_0 = _mm256_mul_pd(_t72_14, _t72_0);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(2, 20, fi1304 + 2), ( G(h(2, 20, fi1304 + 2), M6[20,20],h(4, 20, fi1429)) - ( T( G(h(1, 20, fi1304 + 1), U0[20,20],h(2, 20, fi1304 + 2)) ) * G(h(1, 20, fi1304 + 1), M6[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

      // AVX Loader:

      // 2x4 -> 4x4
      _t72_15 = _t72_1;
      _t72_16 = _t72_2;
      _t72_17 = _mm256_setzero_pd();
      _t72_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t72_19 = _t71_1;

      // 4-BLAC: (1x4)^T
      _t71_49 = _t72_19;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t71_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 32), _mm256_permute2f128_pd(_t71_49, _t71_49, 32), 0), _t72_0);
      _t71_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 32), _mm256_permute2f128_pd(_t71_49, _t71_49, 32), 15), _t72_0);
      _t71_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 49), _mm256_permute2f128_pd(_t71_49, _t71_49, 49), 0), _t72_0);
      _t71_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_49, _t71_49, 49), _mm256_permute2f128_pd(_t71_49, _t71_49, 49), 15), _t72_0);

      // 4-BLAC: 4x4 - 4x4
      _t72_20 = _mm256_sub_pd(_t72_15, _t71_50);
      _t72_21 = _mm256_sub_pd(_t72_16, _t71_51);
      _t72_22 = _mm256_sub_pd(_t72_17, _t71_52);
      _t72_23 = _mm256_sub_pd(_t72_18, _t71_53);

      // AVX Storer:
      _t72_1 = _t72_20;
      _t72_2 = _t72_21;

      // Generating : M6[20,20] = S(h(1, 20, fi1304 + 2), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304 + 2)) Kro G(h(1, 20, fi1304 + 2), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t72_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_9, _t71_9, 32), _mm256_permute2f128_pd(_t71_9, _t71_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t72_1 = _mm256_mul_pd(_t72_24, _t72_1);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 20, fi1304 + 3), M6[20,20],h(4, 20, fi1429)) - ( T( G(h(1, 20, fi1304 + 2), U0[20,20],h(1, 20, fi1304 + 3)) ) Kro G(h(1, 20, fi1304 + 2), M6[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t72_25 = _t71_0;

      // 4-BLAC: (4x1)^T
      _t71_60 = _t72_25;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t71_15 = _mm256_mul_pd(_t71_60, _t72_1);

      // 4-BLAC: 1x4 - 1x4
      _t72_2 = _mm256_sub_pd(_t72_2, _t71_15);

      // AVX Storer:

      // Generating : M6[20,20] = S(h(1, 20, fi1304 + 3), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, fi1304 + 3)) Kro G(h(1, 20, fi1304 + 3), M6[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t72_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t71_10, _t71_10, 32), _mm256_permute2f128_pd(_t71_10, _t71_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t72_2 = _mm256_mul_pd(_t72_26, _t72_2);

      // AVX Storer:
      _asm256_storeu_pd(M1 + 20*fi1304 + fi1429, _t72_3);
      _asm256_storeu_pd(M1 + 20*fi1304 + fi1429 + 20, _t72_0);
      _asm256_storeu_pd(M1 + 20*fi1304 + fi1429 + 40, _t72_1);
      _asm256_storeu_pd(M1 + 20*fi1304 + fi1429 + 60, _t72_2);
    }

    // Generating : M6[20,20] = Sum_{k3} ( Sum_{k2} ( S(h(4, 20, fi1304 + k3 + 4), ( G(h(4, 20, fi1304 + k3 + 4), M6[20,20],h(4, 20, k2)) - ( T( G(h(4, 20, fi1304), U0[20,20],h(4, 20, fi1304 + k3 + 4)) ) * G(h(4, 20, fi1304), M6[20,20],h(4, 20, k2)) ) ),h(4, 20, k2)) ) )
    _asm256_storeu_pd(M1 + 20*fi1304, _t71_14);
    _asm256_storeu_pd(M1 + 20*fi1304 + 20, _t71_11);
    _asm256_storeu_pd(M1 + 20*fi1304 + 40, _t71_12);
    _asm256_storeu_pd(M1 + 20*fi1304 + 60, _t71_13);

    for( int k3 = 0; k3 <= -fi1304 + 15; k3+=4 ) {

      for( int k2 = 0; k2 <= 19; k2+=4 ) {
        _t73_12 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 80);
        _t73_13 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 100);
        _t73_14 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 120);
        _t73_15 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 140);
        _t73_7 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 4);
        _t73_6 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 24);
        _t73_5 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 44);
        _t73_4 = _asm256_loadu_pd(M3 + 21*fi1304 + k3 + 64);
        _t73_3 = _asm256_loadu_pd(M1 + 20*fi1304 + k2);
        _t73_2 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 20);
        _t73_1 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 40);
        _t73_0 = _asm256_loadu_pd(M1 + 20*fi1304 + k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t73_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t73_7, _t73_6), _mm256_unpacklo_pd(_t73_5, _t73_4), 32);
        _t73_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t73_7, _t73_6), _mm256_unpackhi_pd(_t73_5, _t73_4), 32);
        _t73_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t73_7, _t73_6), _mm256_unpacklo_pd(_t73_5, _t73_4), 49);
        _t73_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t73_7, _t73_6), _mm256_unpackhi_pd(_t73_5, _t73_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t73_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_16, _t73_16, 32), _mm256_permute2f128_pd(_t73_16, _t73_16, 32), 0), _t73_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_16, _t73_16, 32), _mm256_permute2f128_pd(_t73_16, _t73_16, 32), 15), _t73_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_16, _t73_16, 49), _mm256_permute2f128_pd(_t73_16, _t73_16, 49), 0), _t73_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_16, _t73_16, 49), _mm256_permute2f128_pd(_t73_16, _t73_16, 49), 15), _t73_0)));
        _t73_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_17, _t73_17, 32), _mm256_permute2f128_pd(_t73_17, _t73_17, 32), 0), _t73_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_17, _t73_17, 32), _mm256_permute2f128_pd(_t73_17, _t73_17, 32), 15), _t73_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_17, _t73_17, 49), _mm256_permute2f128_pd(_t73_17, _t73_17, 49), 0), _t73_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_17, _t73_17, 49), _mm256_permute2f128_pd(_t73_17, _t73_17, 49), 15), _t73_0)));
        _t73_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_18, _t73_18, 32), _mm256_permute2f128_pd(_t73_18, _t73_18, 32), 0), _t73_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_18, _t73_18, 32), _mm256_permute2f128_pd(_t73_18, _t73_18, 32), 15), _t73_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_18, _t73_18, 49), _mm256_permute2f128_pd(_t73_18, _t73_18, 49), 0), _t73_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_18, _t73_18, 49), _mm256_permute2f128_pd(_t73_18, _t73_18, 49), 15), _t73_0)));
        _t73_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_19, _t73_19, 32), _mm256_permute2f128_pd(_t73_19, _t73_19, 32), 0), _t73_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_19, _t73_19, 32), _mm256_permute2f128_pd(_t73_19, _t73_19, 32), 15), _t73_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_19, _t73_19, 49), _mm256_permute2f128_pd(_t73_19, _t73_19, 49), 0), _t73_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_19, _t73_19, 49), _mm256_permute2f128_pd(_t73_19, _t73_19, 49), 15), _t73_0)));

        // 4-BLAC: 4x4 - 4x4
        _t73_12 = _mm256_sub_pd(_t73_12, _t73_8);
        _t73_13 = _mm256_sub_pd(_t73_13, _t73_9);
        _t73_14 = _mm256_sub_pd(_t73_14, _t73_10);
        _t73_15 = _mm256_sub_pd(_t73_15, _t73_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 80, _t73_12);
        _asm256_storeu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 100, _t73_13);
        _asm256_storeu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 120, _t73_14);
        _asm256_storeu_pd(M1 + 20*fi1304 + k2 + 20*k3 + 140, _t73_15);
      }
    }
  }

  _t64_2 = _mm256_maskload_pd(M3 + 337, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t64_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[336])));
  _t64_4 = _mm256_maskload_pd(M3 + 358, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t64_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[378])));
  _t64_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[379])));
  _t64_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[399])));
  _t64_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[357])));
  _t74_7 = _asm256_loadu_pd(M1 + 320);
  _t74_4 = _asm256_loadu_pd(M1 + 340);
  _t74_5 = _asm256_loadu_pd(M1 + 360);
  _t74_6 = _asm256_loadu_pd(M1 + 380);

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 16), U0[20,20],h(1, 20, 16)) ),h(1, 20, 16))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t74_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_10 = _t64_0;

  // 4-BLAC: 1x4 / 1x4
  _t74_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t74_9), _mm256_castpd256_pd128(_t74_10)));

  // AVX Storer:
  _t74_0 = _t74_11;

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 17), U0[20,20],h(1, 20, 17)) ),h(1, 20, 17))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t74_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_13 = _t64_3;

  // 4-BLAC: 1x4 / 1x4
  _t74_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t74_12), _mm256_castpd256_pd128(_t74_13)));

  // AVX Storer:
  _t74_1 = _t74_14;

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 18), U0[20,20],h(1, 20, 18)) ),h(1, 20, 18))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t74_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_16 = _t64_6;

  // 4-BLAC: 1x4 / 1x4
  _t74_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t74_15), _mm256_castpd256_pd128(_t74_16)));

  // AVX Storer:
  _t74_2 = _t74_17;

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 19), U0[20,20],h(1, 20, 19)) ),h(1, 20, 19))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t74_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_19 = _t64_8;

  // 4-BLAC: 1x4 / 1x4
  _t74_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t74_18), _mm256_castpd256_pd128(_t74_19)));

  // AVX Storer:
  _t74_3 = _t74_20;

  // Generating : M6[20,20] = S(h(1, 20, 16), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 16)) Kro G(h(1, 20, 16), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_0, _t74_0, 32), _mm256_permute2f128_pd(_t74_0, _t74_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t74_7 = _mm256_mul_pd(_t74_21, _t74_7);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(3, 20, 17), ( G(h(3, 20, 17), M6[20,20],h(4, 20, fi1304)) - ( T( G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) * G(h(1, 20, 16), M6[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

  // AVX Loader:

  // 3x4 -> 4x4
  _t74_22 = _t74_4;
  _t74_23 = _t74_5;
  _t74_24 = _t74_6;
  _t74_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t74_26 = _t64_2;

  // 4-BLAC: (1x4)^T
  _t74_27 = _t74_26;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t74_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 32), _mm256_permute2f128_pd(_t74_27, _t74_27, 32), 0), _t74_7);
  _t74_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 32), _mm256_permute2f128_pd(_t74_27, _t74_27, 32), 15), _t74_7);
  _t74_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 49), _mm256_permute2f128_pd(_t74_27, _t74_27, 49), 0), _t74_7);
  _t74_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 49), _mm256_permute2f128_pd(_t74_27, _t74_27, 49), 15), _t74_7);

  // 4-BLAC: 4x4 - 4x4
  _t74_32 = _mm256_sub_pd(_t74_22, _t74_28);
  _t74_33 = _mm256_sub_pd(_t74_23, _t74_29);
  _t74_34 = _mm256_sub_pd(_t74_24, _t74_30);
  _t74_35 = _mm256_sub_pd(_t74_25, _t74_31);

  // AVX Storer:
  _t74_4 = _t74_32;
  _t74_5 = _t74_33;
  _t74_6 = _t74_34;

  // Generating : M6[20,20] = S(h(1, 20, 17), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 17)) Kro G(h(1, 20, 17), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_1, _t74_1, 32), _mm256_permute2f128_pd(_t74_1, _t74_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t74_4 = _mm256_mul_pd(_t74_36, _t74_4);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(2, 20, 18), ( G(h(2, 20, 18), M6[20,20],h(4, 20, fi1304)) - ( T( G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) * G(h(1, 20, 17), M6[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

  // AVX Loader:

  // 2x4 -> 4x4
  _t74_37 = _t74_5;
  _t74_38 = _t74_6;
  _t74_39 = _mm256_setzero_pd();
  _t74_40 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t74_41 = _t64_4;

  // 4-BLAC: (1x4)^T
  _t74_42 = _t74_41;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t74_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 32), _mm256_permute2f128_pd(_t74_42, _t74_42, 32), 0), _t74_4);
  _t74_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 32), _mm256_permute2f128_pd(_t74_42, _t74_42, 32), 15), _t74_4);
  _t74_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 49), _mm256_permute2f128_pd(_t74_42, _t74_42, 49), 0), _t74_4);
  _t74_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 49), _mm256_permute2f128_pd(_t74_42, _t74_42, 49), 15), _t74_4);

  // 4-BLAC: 4x4 - 4x4
  _t74_47 = _mm256_sub_pd(_t74_37, _t74_43);
  _t74_48 = _mm256_sub_pd(_t74_38, _t74_44);
  _t74_49 = _mm256_sub_pd(_t74_39, _t74_45);
  _t74_50 = _mm256_sub_pd(_t74_40, _t74_46);

  // AVX Storer:
  _t74_5 = _t74_47;
  _t74_6 = _t74_48;

  // Generating : M6[20,20] = S(h(1, 20, 18), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 18)) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_2, _t74_2, 32), _mm256_permute2f128_pd(_t74_2, _t74_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t74_5 = _mm256_mul_pd(_t74_51, _t74_5);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), M6[20,20],h(4, 20, fi1304)) - ( T( G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t64_7, _t64_7, 32), _mm256_permute2f128_pd(_t64_7, _t64_7, 32), 0);

  // 4-BLAC: (4x1)^T
  _t74_53 = _t74_52;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t74_8 = _mm256_mul_pd(_t74_53, _t74_5);

  // 4-BLAC: 1x4 - 1x4
  _t74_6 = _mm256_sub_pd(_t74_6, _t74_8);

  // AVX Storer:

  // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 19)) Kro G(h(1, 20, 19), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t74_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_3, _t74_3, 32), _mm256_permute2f128_pd(_t74_3, _t74_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t74_6 = _mm256_mul_pd(_t74_54, _t74_6);

  // AVX Storer:


  for( int fi1304 = 4; fi1304 <= 16; fi1304+=4 ) {
    _t75_3 = _asm256_loadu_pd(M1 + fi1304 + 320);
    _t75_0 = _asm256_loadu_pd(M1 + fi1304 + 340);
    _t75_1 = _asm256_loadu_pd(M1 + fi1304 + 360);
    _t75_2 = _asm256_loadu_pd(M1 + fi1304 + 380);

    // Generating : M6[20,20] = S(h(1, 20, 16), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 16)) Kro G(h(1, 20, 16), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t75_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_0, _t74_0, 32), _mm256_permute2f128_pd(_t74_0, _t74_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t75_3 = _mm256_mul_pd(_t75_4, _t75_3);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(3, 20, 17), ( G(h(3, 20, 17), M6[20,20],h(4, 20, fi1304)) - ( T( G(h(1, 20, 16), U0[20,20],h(3, 20, 17)) ) * G(h(1, 20, 16), M6[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

    // AVX Loader:

    // 3x4 -> 4x4
    _t75_5 = _t75_0;
    _t75_6 = _t75_1;
    _t75_7 = _t75_2;
    _t75_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t75_9 = _t64_2;

    // 4-BLAC: (1x4)^T
    _t74_27 = _t75_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t74_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 32), _mm256_permute2f128_pd(_t74_27, _t74_27, 32), 0), _t75_3);
    _t74_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 32), _mm256_permute2f128_pd(_t74_27, _t74_27, 32), 15), _t75_3);
    _t74_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 49), _mm256_permute2f128_pd(_t74_27, _t74_27, 49), 0), _t75_3);
    _t74_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_27, _t74_27, 49), _mm256_permute2f128_pd(_t74_27, _t74_27, 49), 15), _t75_3);

    // 4-BLAC: 4x4 - 4x4
    _t75_10 = _mm256_sub_pd(_t75_5, _t74_28);
    _t75_11 = _mm256_sub_pd(_t75_6, _t74_29);
    _t75_12 = _mm256_sub_pd(_t75_7, _t74_30);
    _t75_13 = _mm256_sub_pd(_t75_8, _t74_31);

    // AVX Storer:
    _t75_0 = _t75_10;
    _t75_1 = _t75_11;
    _t75_2 = _t75_12;

    // Generating : M6[20,20] = S(h(1, 20, 17), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 17)) Kro G(h(1, 20, 17), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t75_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_1, _t74_1, 32), _mm256_permute2f128_pd(_t74_1, _t74_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t75_0 = _mm256_mul_pd(_t75_14, _t75_0);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(2, 20, 18), ( G(h(2, 20, 18), M6[20,20],h(4, 20, fi1304)) - ( T( G(h(1, 20, 17), U0[20,20],h(2, 20, 18)) ) * G(h(1, 20, 17), M6[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

    // AVX Loader:

    // 2x4 -> 4x4
    _t75_15 = _t75_1;
    _t75_16 = _t75_2;
    _t75_17 = _mm256_setzero_pd();
    _t75_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t75_19 = _t64_4;

    // 4-BLAC: (1x4)^T
    _t74_42 = _t75_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t74_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 32), _mm256_permute2f128_pd(_t74_42, _t74_42, 32), 0), _t75_0);
    _t74_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 32), _mm256_permute2f128_pd(_t74_42, _t74_42, 32), 15), _t75_0);
    _t74_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 49), _mm256_permute2f128_pd(_t74_42, _t74_42, 49), 0), _t75_0);
    _t74_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_42, _t74_42, 49), _mm256_permute2f128_pd(_t74_42, _t74_42, 49), 15), _t75_0);

    // 4-BLAC: 4x4 - 4x4
    _t75_20 = _mm256_sub_pd(_t75_15, _t74_43);
    _t75_21 = _mm256_sub_pd(_t75_16, _t74_44);
    _t75_22 = _mm256_sub_pd(_t75_17, _t74_45);
    _t75_23 = _mm256_sub_pd(_t75_18, _t74_46);

    // AVX Storer:
    _t75_1 = _t75_20;
    _t75_2 = _t75_21;

    // Generating : M6[20,20] = S(h(1, 20, 18), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 18)) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t75_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_2, _t74_2, 32), _mm256_permute2f128_pd(_t74_2, _t74_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t75_1 = _mm256_mul_pd(_t75_24, _t75_1);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 20, 19), M6[20,20],h(4, 20, fi1304)) - ( T( G(h(1, 20, 18), U0[20,20],h(1, 20, 19)) ) Kro G(h(1, 20, 18), M6[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t75_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t64_7, _t64_7, 32), _mm256_permute2f128_pd(_t64_7, _t64_7, 32), 0);

    // 4-BLAC: (4x1)^T
    _t74_53 = _t75_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t74_8 = _mm256_mul_pd(_t74_53, _t75_1);

    // 4-BLAC: 1x4 - 1x4
    _t75_2 = _mm256_sub_pd(_t75_2, _t74_8);

    // AVX Storer:

    // Generating : M6[20,20] = S(h(1, 20, 19), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 19)) Kro G(h(1, 20, 19), M6[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t75_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t74_3, _t74_3, 32), _mm256_permute2f128_pd(_t74_3, _t74_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t75_2 = _mm256_mul_pd(_t75_26, _t75_2);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi1304 + 320, _t75_3);
    _asm256_storeu_pd(M1 + fi1304 + 340, _t75_0);
    _asm256_storeu_pd(M1 + fi1304 + 360, _t75_1);
    _asm256_storeu_pd(M1 + fi1304 + 380, _t75_2);
  }

  _asm256_storeu_pd(M1 + 320, _t74_7);
  _asm256_storeu_pd(M1 + 340, _t74_4);
  _asm256_storeu_pd(M1 + 360, _t74_5);
  _asm256_storeu_pd(M1 + 380, _t74_6);

  for( int fi1304 = 0; fi1304 <= 15; fi1304+=4 ) {
    _t76_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 399])));
    _t76_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 378])));
    _t76_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 357])));
    _t76_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-21*fi1304 + 336])));
    _t76_14 = _asm256_loadu_pd(M1 + -20*fi1304 + 380);
    _t76_11 = _asm256_loadu_pd(M1 + -20*fi1304 + 320);
    _t76_12 = _asm256_loadu_pd(M1 + -20*fi1304 + 340);
    _t76_13 = _asm256_loadu_pd(M1 + -20*fi1304 + 360);
    _t76_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 339)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 359))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 379)), 32);
    _t76_1 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 338)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -21*fi1304 + 358)), 0);
    _t76_0 = _mm256_broadcast_sd(&(M3[-21*fi1304 + 337]));

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 19), U0[20,20],h(1, 20, -fi1304 + 19)) ),h(1, 20, -fi1304 + 19))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_17 = _t76_6;

    // 4-BLAC: 1x4 / 1x4
    _t76_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_16), _mm256_castpd256_pd128(_t76_17)));

    // AVX Storer:
    _t76_7 = _t76_18;

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 18), U0[20,20],h(1, 20, -fi1304 + 18)) ),h(1, 20, -fi1304 + 18))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_20 = _t76_5;

    // 4-BLAC: 1x4 / 1x4
    _t76_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_19), _mm256_castpd256_pd128(_t76_20)));

    // AVX Storer:
    _t76_8 = _t76_21;

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 17), U0[20,20],h(1, 20, -fi1304 + 17)) ),h(1, 20, -fi1304 + 17))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_23 = _t76_4;

    // 4-BLAC: 1x4 / 1x4
    _t76_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_22), _mm256_castpd256_pd128(_t76_23)));

    // AVX Storer:
    _t76_9 = _t76_24;

    // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 16)) ),h(1, 20, -fi1304 + 16))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_26 = _t76_3;

    // 4-BLAC: 1x4 / 1x4
    _t76_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_25), _mm256_castpd256_pd128(_t76_26)));

    // AVX Storer:
    _t76_10 = _t76_27;

    // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 19), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 19)) Kro G(h(1, 20, -fi1304 + 19), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_7, _t76_7, 32), _mm256_permute2f128_pd(_t76_7, _t76_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_14 = _mm256_mul_pd(_t76_28, _t76_14);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(3, 20, -fi1304 + 16), ( G(h(3, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) - ( G(h(3, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 19)) * G(h(1, 20, -fi1304 + 19), M8[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

    // AVX Loader:

    // 3x4 -> 4x4
    _t76_29 = _t76_11;
    _t76_30 = _t76_12;
    _t76_31 = _t76_13;
    _t76_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t76_33 = _t76_2;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t76_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_33, _t76_33, 32), _mm256_permute2f128_pd(_t76_33, _t76_33, 32), 0), _t76_14);
    _t76_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_33, _t76_33, 32), _mm256_permute2f128_pd(_t76_33, _t76_33, 32), 15), _t76_14);
    _t76_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_33, _t76_33, 49), _mm256_permute2f128_pd(_t76_33, _t76_33, 49), 0), _t76_14);
    _t76_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_33, _t76_33, 49), _mm256_permute2f128_pd(_t76_33, _t76_33, 49), 15), _t76_14);

    // 4-BLAC: 4x4 - 4x4
    _t76_38 = _mm256_sub_pd(_t76_29, _t76_34);
    _t76_39 = _mm256_sub_pd(_t76_30, _t76_35);
    _t76_40 = _mm256_sub_pd(_t76_31, _t76_36);
    _t76_41 = _mm256_sub_pd(_t76_32, _t76_37);

    // AVX Storer:
    _t76_11 = _t76_38;
    _t76_12 = _t76_39;
    _t76_13 = _t76_40;

    // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 18), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 18)) Kro G(h(1, 20, -fi1304 + 18), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_8, _t76_8, 32), _mm256_permute2f128_pd(_t76_8, _t76_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_13 = _mm256_mul_pd(_t76_42, _t76_13);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(2, 20, -fi1304 + 16), ( G(h(2, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) - ( G(h(2, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 18)) * G(h(1, 20, -fi1304 + 18), M8[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

    // AVX Loader:

    // 2x4 -> 4x4
    _t76_43 = _t76_11;
    _t76_44 = _t76_12;
    _t76_45 = _mm256_setzero_pd();
    _t76_46 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t76_47 = _t76_1;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t76_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_47, _t76_47, 32), _mm256_permute2f128_pd(_t76_47, _t76_47, 32), 0), _t76_13);
    _t76_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_47, _t76_47, 32), _mm256_permute2f128_pd(_t76_47, _t76_47, 32), 15), _t76_13);
    _t76_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_47, _t76_47, 49), _mm256_permute2f128_pd(_t76_47, _t76_47, 49), 0), _t76_13);
    _t76_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_47, _t76_47, 49), _mm256_permute2f128_pd(_t76_47, _t76_47, 49), 15), _t76_13);

    // 4-BLAC: 4x4 - 4x4
    _t76_52 = _mm256_sub_pd(_t76_43, _t76_48);
    _t76_53 = _mm256_sub_pd(_t76_44, _t76_49);
    _t76_54 = _mm256_sub_pd(_t76_45, _t76_50);
    _t76_55 = _mm256_sub_pd(_t76_46, _t76_51);

    // AVX Storer:
    _t76_11 = _t76_52;
    _t76_12 = _t76_53;

    // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 17), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 17)) Kro G(h(1, 20, -fi1304 + 17), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_56 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_9, _t76_9, 32), _mm256_permute2f128_pd(_t76_9, _t76_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_12 = _mm256_mul_pd(_t76_56, _t76_12);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 16), ( G(h(1, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) - ( G(h(1, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 17)) Kro G(h(1, 20, -fi1304 + 17), M8[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_57 = _t76_0;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_15 = _mm256_mul_pd(_t76_57, _t76_12);

    // 4-BLAC: 1x4 - 1x4
    _t76_11 = _mm256_sub_pd(_t76_11, _t76_15);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 16), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 16)) Kro G(h(1, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_10, _t76_10, 32), _mm256_permute2f128_pd(_t76_10, _t76_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_11 = _mm256_mul_pd(_t76_58, _t76_11);

    // AVX Storer:

    for( int fi1429 = 4; fi1429 <= 16; fi1429+=4 ) {
      _t77_3 = _asm256_loadu_pd(M1 + -20*fi1304 + fi1429 + 380);
      _t77_0 = _asm256_loadu_pd(M1 + -20*fi1304 + fi1429 + 320);
      _t77_1 = _asm256_loadu_pd(M1 + -20*fi1304 + fi1429 + 340);
      _t77_2 = _asm256_loadu_pd(M1 + -20*fi1304 + fi1429 + 360);

      // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 19), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 19)) Kro G(h(1, 20, -fi1304 + 19), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_7, _t76_7, 32), _mm256_permute2f128_pd(_t76_7, _t76_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_3 = _mm256_mul_pd(_t77_4, _t77_3);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(3, 20, -fi1304 + 16), ( G(h(3, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) - ( G(h(3, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 19)) * G(h(1, 20, -fi1304 + 19), M8[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

      // AVX Loader:

      // 3x4 -> 4x4
      _t77_5 = _t77_0;
      _t77_6 = _t77_1;
      _t77_7 = _t77_2;
      _t77_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t77_9 = _t76_2;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t76_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_9, _t77_9, 32), _mm256_permute2f128_pd(_t77_9, _t77_9, 32), 0), _t77_3);
      _t76_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_9, _t77_9, 32), _mm256_permute2f128_pd(_t77_9, _t77_9, 32), 15), _t77_3);
      _t76_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_9, _t77_9, 49), _mm256_permute2f128_pd(_t77_9, _t77_9, 49), 0), _t77_3);
      _t76_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_9, _t77_9, 49), _mm256_permute2f128_pd(_t77_9, _t77_9, 49), 15), _t77_3);

      // 4-BLAC: 4x4 - 4x4
      _t77_10 = _mm256_sub_pd(_t77_5, _t76_34);
      _t77_11 = _mm256_sub_pd(_t77_6, _t76_35);
      _t77_12 = _mm256_sub_pd(_t77_7, _t76_36);
      _t77_13 = _mm256_sub_pd(_t77_8, _t76_37);

      // AVX Storer:
      _t77_0 = _t77_10;
      _t77_1 = _t77_11;
      _t77_2 = _t77_12;

      // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 18), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 18)) Kro G(h(1, 20, -fi1304 + 18), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_8, _t76_8, 32), _mm256_permute2f128_pd(_t76_8, _t76_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_2 = _mm256_mul_pd(_t77_14, _t77_2);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(2, 20, -fi1304 + 16), ( G(h(2, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) - ( G(h(2, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 18)) * G(h(1, 20, -fi1304 + 18), M8[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

      // AVX Loader:

      // 2x4 -> 4x4
      _t77_15 = _t77_0;
      _t77_16 = _t77_1;
      _t77_17 = _mm256_setzero_pd();
      _t77_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t77_19 = _t76_1;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t76_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_19, _t77_19, 32), _mm256_permute2f128_pd(_t77_19, _t77_19, 32), 0), _t77_2);
      _t76_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_19, _t77_19, 32), _mm256_permute2f128_pd(_t77_19, _t77_19, 32), 15), _t77_2);
      _t76_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_19, _t77_19, 49), _mm256_permute2f128_pd(_t77_19, _t77_19, 49), 0), _t77_2);
      _t76_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t77_19, _t77_19, 49), _mm256_permute2f128_pd(_t77_19, _t77_19, 49), 15), _t77_2);

      // 4-BLAC: 4x4 - 4x4
      _t77_20 = _mm256_sub_pd(_t77_15, _t76_48);
      _t77_21 = _mm256_sub_pd(_t77_16, _t76_49);
      _t77_22 = _mm256_sub_pd(_t77_17, _t76_50);
      _t77_23 = _mm256_sub_pd(_t77_18, _t76_51);

      // AVX Storer:
      _t77_0 = _t77_20;
      _t77_1 = _t77_21;

      // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 17), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 17)) Kro G(h(1, 20, -fi1304 + 17), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_9, _t76_9, 32), _mm256_permute2f128_pd(_t76_9, _t76_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_1 = _mm256_mul_pd(_t77_24, _t77_1);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 16), ( G(h(1, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) - ( G(h(1, 20, -fi1304 + 16), U0[20,20],h(1, 20, -fi1304 + 17)) Kro G(h(1, 20, -fi1304 + 17), M8[20,20],h(4, 20, fi1429)) ) ),h(4, 20, fi1429))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_25 = _t76_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t76_15 = _mm256_mul_pd(_t77_25, _t77_1);

      // 4-BLAC: 1x4 - 1x4
      _t77_0 = _mm256_sub_pd(_t77_0, _t76_15);

      // AVX Storer:

      // Generating : M8[20,20] = S(h(1, 20, -fi1304 + 16), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, -fi1304 + 16)) Kro G(h(1, 20, -fi1304 + 16), M8[20,20],h(4, 20, fi1429)) ),h(4, 20, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_10, _t76_10, 32), _mm256_permute2f128_pd(_t76_10, _t76_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_0 = _mm256_mul_pd(_t77_26, _t77_0);

      // AVX Storer:
      _asm256_storeu_pd(M1 + -20*fi1304 + fi1429 + 380, _t77_3);
      _asm256_storeu_pd(M1 + -20*fi1304 + fi1429 + 360, _t77_2);
      _asm256_storeu_pd(M1 + -20*fi1304 + fi1429 + 340, _t77_1);
      _asm256_storeu_pd(M1 + -20*fi1304 + fi1429 + 320, _t77_0);
    }

    // Generating : M8[20,20] = Sum_{k3} ( Sum_{k2} ( S(h(4, 20, k3), ( G(h(4, 20, k3), M8[20,20],h(4, 20, k2)) - ( G(h(4, 20, k3), U0[20,20],h(4, 20, -fi1304 + 16)) * G(h(4, 20, -fi1304 + 16), M8[20,20],h(4, 20, k2)) ) ),h(4, 20, k2)) ) )
    _asm256_storeu_pd(M1 + -20*fi1304 + 380, _t76_14);
    _asm256_storeu_pd(M1 + -20*fi1304 + 360, _t76_13);
    _asm256_storeu_pd(M1 + -20*fi1304 + 340, _t76_12);
    _asm256_storeu_pd(M1 + -20*fi1304 + 320, _t76_11);

    for( int k3 = 0; k3 <= -fi1304 + 15; k3+=4 ) {

      for( int k2 = 0; k2 <= 19; k2+=4 ) {
        _t78_24 = _asm256_loadu_pd(M1 + k2 + 20*k3);
        _t78_25 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 20);
        _t78_26 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 40);
        _t78_27 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 60);
        _t78_19 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 16);
        _t78_18 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 17);
        _t78_17 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 18);
        _t78_16 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 19);
        _t78_15 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 36);
        _t78_14 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 37);
        _t78_13 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 38);
        _t78_12 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 39);
        _t78_11 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 56);
        _t78_10 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 57);
        _t78_9 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 58);
        _t78_8 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 59);
        _t78_7 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 76);
        _t78_6 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 77);
        _t78_5 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 78);
        _t78_4 = _mm256_broadcast_sd(M3 + -fi1304 + 20*k3 + 79);
        _t78_3 = _asm256_loadu_pd(M1 + -20*fi1304 + k2 + 320);
        _t78_2 = _asm256_loadu_pd(M1 + -20*fi1304 + k2 + 340);
        _t78_1 = _asm256_loadu_pd(M1 + -20*fi1304 + k2 + 360);
        _t78_0 = _asm256_loadu_pd(M1 + -20*fi1304 + k2 + 380);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t78_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_19, _t78_3), _mm256_mul_pd(_t78_18, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_17, _t78_1), _mm256_mul_pd(_t78_16, _t78_0)));
        _t78_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_15, _t78_3), _mm256_mul_pd(_t78_14, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_13, _t78_1), _mm256_mul_pd(_t78_12, _t78_0)));
        _t78_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_11, _t78_3), _mm256_mul_pd(_t78_10, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_9, _t78_1), _mm256_mul_pd(_t78_8, _t78_0)));
        _t78_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t78_7, _t78_3), _mm256_mul_pd(_t78_6, _t78_2)), _mm256_add_pd(_mm256_mul_pd(_t78_5, _t78_1), _mm256_mul_pd(_t78_4, _t78_0)));

        // 4-BLAC: 4x4 - 4x4
        _t78_24 = _mm256_sub_pd(_t78_24, _t78_20);
        _t78_25 = _mm256_sub_pd(_t78_25, _t78_21);
        _t78_26 = _mm256_sub_pd(_t78_26, _t78_22);
        _t78_27 = _mm256_sub_pd(_t78_27, _t78_23);

        // AVX Storer:
        _asm256_storeu_pd(M1 + k2 + 20*k3, _t78_24);
        _asm256_storeu_pd(M1 + k2 + 20*k3 + 20, _t78_25);
        _asm256_storeu_pd(M1 + k2 + 20*k3 + 40, _t78_26);
        _asm256_storeu_pd(M1 + k2 + 20*k3 + 60, _t78_27);
      }
    }
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[21])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[63])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t48_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[43])));
  _t48_4 = _mm256_maskload_pd(M3 + 22, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[42])));
  _t79_7 = _asm256_loadu_pd(M1 + 60);
  _t79_4 = _asm256_loadu_pd(M1);
  _t79_5 = _asm256_loadu_pd(M1 + 20);
  _t79_6 = _asm256_loadu_pd(M1 + 40);

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 3), U0[20,20],h(1, 20, 3)) ),h(1, 20, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_10 = _t48_8;

  // 4-BLAC: 1x4 / 1x4
  _t79_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_9), _mm256_castpd256_pd128(_t79_10)));

  // AVX Storer:
  _t79_0 = _t79_11;

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 2), U0[20,20],h(1, 20, 2)) ),h(1, 20, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_13 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t79_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_12), _mm256_castpd256_pd128(_t79_13)));

  // AVX Storer:
  _t79_1 = _t79_14;

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 1), U0[20,20],h(1, 20, 1)) ),h(1, 20, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_16 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t79_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_15), _mm256_castpd256_pd128(_t79_16)));

  // AVX Storer:
  _t79_2 = _t79_17;

  // Generating : T2407[1,20] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 20, 0), U0[20,20],h(1, 20, 0)) ),h(1, 20, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_19 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t79_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_18), _mm256_castpd256_pd128(_t79_19)));

  // AVX Storer:
  _t79_3 = _t79_20;

  // Generating : M8[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_0, _t79_0, 32), _mm256_permute2f128_pd(_t79_0, _t79_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_7 = _mm256_mul_pd(_t79_21, _t79_7);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(3, 20, 0), ( G(h(3, 20, 0), M8[20,20],h(4, 20, fi1304)) - ( G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) * G(h(1, 20, 3), M8[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

  // AVX Loader:

  // 3x4 -> 4x4
  _t79_22 = _t79_4;
  _t79_23 = _t79_5;
  _t79_24 = _t79_6;
  _t79_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t79_26 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t79_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_26, _t79_26, 32), _mm256_permute2f128_pd(_t79_26, _t79_26, 32), 0), _t79_7);
  _t79_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_26, _t79_26, 32), _mm256_permute2f128_pd(_t79_26, _t79_26, 32), 15), _t79_7);
  _t79_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_26, _t79_26, 49), _mm256_permute2f128_pd(_t79_26, _t79_26, 49), 0), _t79_7);
  _t79_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_26, _t79_26, 49), _mm256_permute2f128_pd(_t79_26, _t79_26, 49), 15), _t79_7);

  // 4-BLAC: 4x4 - 4x4
  _t79_31 = _mm256_sub_pd(_t79_22, _t79_27);
  _t79_32 = _mm256_sub_pd(_t79_23, _t79_28);
  _t79_33 = _mm256_sub_pd(_t79_24, _t79_29);
  _t79_34 = _mm256_sub_pd(_t79_25, _t79_30);

  // AVX Storer:
  _t79_4 = _t79_31;
  _t79_5 = _t79_32;
  _t79_6 = _t79_33;

  // Generating : M8[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_1, _t79_1, 32), _mm256_permute2f128_pd(_t79_1, _t79_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_6 = _mm256_mul_pd(_t79_35, _t79_6);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(2, 20, 0), ( G(h(2, 20, 0), M8[20,20],h(4, 20, fi1304)) - ( G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) * G(h(1, 20, 2), M8[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

  // AVX Loader:

  // 2x4 -> 4x4
  _t79_36 = _t79_4;
  _t79_37 = _t79_5;
  _t79_38 = _mm256_setzero_pd();
  _t79_39 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t79_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t79_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_40, _t79_40, 32), _mm256_permute2f128_pd(_t79_40, _t79_40, 32), 0), _t79_6);
  _t79_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_40, _t79_40, 32), _mm256_permute2f128_pd(_t79_40, _t79_40, 32), 15), _t79_6);
  _t79_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_40, _t79_40, 49), _mm256_permute2f128_pd(_t79_40, _t79_40, 49), 0), _t79_6);
  _t79_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_40, _t79_40, 49), _mm256_permute2f128_pd(_t79_40, _t79_40, 49), 15), _t79_6);

  // 4-BLAC: 4x4 - 4x4
  _t79_45 = _mm256_sub_pd(_t79_36, _t79_41);
  _t79_46 = _mm256_sub_pd(_t79_37, _t79_42);
  _t79_47 = _mm256_sub_pd(_t79_38, _t79_43);
  _t79_48 = _mm256_sub_pd(_t79_39, _t79_44);

  // AVX Storer:
  _t79_4 = _t79_45;
  _t79_5 = _t79_46;

  // Generating : M8[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_2, _t79_2, 32), _mm256_permute2f128_pd(_t79_2, _t79_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_5 = _mm256_mul_pd(_t79_49, _t79_5);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 20, 0), M8[20,20],h(4, 20, fi1304)) - ( G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_8 = _mm256_mul_pd(_t79_50, _t79_5);

  // 4-BLAC: 1x4 - 1x4
  _t79_4 = _mm256_sub_pd(_t79_4, _t79_8);

  // AVX Storer:

  // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_3, _t79_3, 32), _mm256_permute2f128_pd(_t79_3, _t79_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_4 = _mm256_mul_pd(_t79_51, _t79_4);

  // AVX Storer:


  for( int fi1304 = 4; fi1304 <= 16; fi1304+=4 ) {
    _t80_3 = _asm256_loadu_pd(M1 + fi1304 + 60);
    _t80_0 = _asm256_loadu_pd(M1 + fi1304);
    _t80_1 = _asm256_loadu_pd(M1 + fi1304 + 20);
    _t80_2 = _asm256_loadu_pd(M1 + fi1304 + 40);

    // Generating : M8[20,20] = S(h(1, 20, 3), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 3)) Kro G(h(1, 20, 3), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_0, _t79_0, 32), _mm256_permute2f128_pd(_t79_0, _t79_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_3 = _mm256_mul_pd(_t80_4, _t80_3);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(3, 20, 0), ( G(h(3, 20, 0), M8[20,20],h(4, 20, fi1304)) - ( G(h(3, 20, 0), U0[20,20],h(1, 20, 3)) * G(h(1, 20, 3), M8[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

    // AVX Loader:

    // 3x4 -> 4x4
    _t80_5 = _t80_0;
    _t80_6 = _t80_1;
    _t80_7 = _t80_2;
    _t80_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t80_9 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t79_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_9, _t80_9, 32), _mm256_permute2f128_pd(_t80_9, _t80_9, 32), 0), _t80_3);
    _t79_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_9, _t80_9, 32), _mm256_permute2f128_pd(_t80_9, _t80_9, 32), 15), _t80_3);
    _t79_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_9, _t80_9, 49), _mm256_permute2f128_pd(_t80_9, _t80_9, 49), 0), _t80_3);
    _t79_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_9, _t80_9, 49), _mm256_permute2f128_pd(_t80_9, _t80_9, 49), 15), _t80_3);

    // 4-BLAC: 4x4 - 4x4
    _t80_10 = _mm256_sub_pd(_t80_5, _t79_27);
    _t80_11 = _mm256_sub_pd(_t80_6, _t79_28);
    _t80_12 = _mm256_sub_pd(_t80_7, _t79_29);
    _t80_13 = _mm256_sub_pd(_t80_8, _t79_30);

    // AVX Storer:
    _t80_0 = _t80_10;
    _t80_1 = _t80_11;
    _t80_2 = _t80_12;

    // Generating : M8[20,20] = S(h(1, 20, 2), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 2)) Kro G(h(1, 20, 2), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_1, _t79_1, 32), _mm256_permute2f128_pd(_t79_1, _t79_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_2 = _mm256_mul_pd(_t80_14, _t80_2);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(2, 20, 0), ( G(h(2, 20, 0), M8[20,20],h(4, 20, fi1304)) - ( G(h(2, 20, 0), U0[20,20],h(1, 20, 2)) * G(h(1, 20, 2), M8[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

    // AVX Loader:

    // 2x4 -> 4x4
    _t80_15 = _t80_0;
    _t80_16 = _t80_1;
    _t80_17 = _mm256_setzero_pd();
    _t80_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t80_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t79_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_19, _t80_19, 32), _mm256_permute2f128_pd(_t80_19, _t80_19, 32), 0), _t80_2);
    _t79_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_19, _t80_19, 32), _mm256_permute2f128_pd(_t80_19, _t80_19, 32), 15), _t80_2);
    _t79_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_19, _t80_19, 49), _mm256_permute2f128_pd(_t80_19, _t80_19, 49), 0), _t80_2);
    _t79_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t80_19, _t80_19, 49), _mm256_permute2f128_pd(_t80_19, _t80_19, 49), 15), _t80_2);

    // 4-BLAC: 4x4 - 4x4
    _t80_20 = _mm256_sub_pd(_t80_15, _t79_41);
    _t80_21 = _mm256_sub_pd(_t80_16, _t79_42);
    _t80_22 = _mm256_sub_pd(_t80_17, _t79_43);
    _t80_23 = _mm256_sub_pd(_t80_18, _t79_44);

    // AVX Storer:
    _t80_0 = _t80_20;
    _t80_1 = _t80_21;

    // Generating : M8[20,20] = S(h(1, 20, 1), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_2, _t79_2, 32), _mm256_permute2f128_pd(_t79_2, _t79_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_1 = _mm256_mul_pd(_t80_24, _t80_1);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 20, 0), M8[20,20],h(4, 20, fi1304)) - ( G(h(1, 20, 0), U0[20,20],h(1, 20, 1)) Kro G(h(1, 20, 1), M8[20,20],h(4, 20, fi1304)) ) ),h(4, 20, fi1304))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t79_8 = _mm256_mul_pd(_t80_25, _t80_1);

    // 4-BLAC: 1x4 - 1x4
    _t80_0 = _mm256_sub_pd(_t80_0, _t79_8);

    // AVX Storer:

    // Generating : M8[20,20] = S(h(1, 20, 0), ( G(h(1, 1, 0), T2407[1,20],h(1, 20, 0)) Kro G(h(1, 20, 0), M8[20,20],h(4, 20, fi1304)) ),h(4, 20, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_3, _t79_3, 32), _mm256_permute2f128_pd(_t79_3, _t79_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_0 = _mm256_mul_pd(_t80_26, _t80_0);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi1304 + 60, _t80_3);
    _asm256_storeu_pd(M1 + fi1304 + 40, _t80_2);
    _asm256_storeu_pd(M1 + fi1304 + 20, _t80_1);
    _asm256_storeu_pd(M1 + fi1304, _t80_0);
  }


  // Generating : x[20,1] = ( Sum_{k2} ( S(h(4, 20, k2), ( G(h(4, 20, k2), y[20,1],h(1, 1, 0)) + ( G(h(4, 20, k2), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), v0[20,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( $(h(4, 20, k2), ( G(h(4, 20, k2), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), v0[20,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t70_1);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t70_3);

  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t81_4 = _asm256_loadu_pd(y + k2);
    _t81_3 = _asm256_loadu_pd(M2 + 20*k2);
    _t81_2 = _asm256_loadu_pd(M2 + 20*k2 + 20);
    _t81_1 = _asm256_loadu_pd(M2 + 20*k2 + 40);
    _t81_0 = _asm256_loadu_pd(M2 + 20*k2 + 60);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t81_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t81_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32)), _mm256_mul_pd(_t81_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t81_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32)), _mm256_mul_pd(_t81_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t81_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32)), _mm256_mul_pd(_t81_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t81_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32)), _mm256_mul_pd(_t81_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t70_5, _t70_4), _mm256_unpacklo_pd(_t70_2, _t70_0), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t81_5 = _mm256_add_pd(_t81_4, _t81_6);

    // AVX Storer:
    _asm256_storeu_pd(x + k2, _t81_5);
  }


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 19; k2+=4 ) {
      _t82_4 = _asm256_loadu_pd(M2 + 20*k2 + k3);
      _t82_3 = _asm256_loadu_pd(M2 + 20*k2 + k3 + 20);
      _t82_2 = _asm256_loadu_pd(M2 + 20*k2 + k3 + 40);
      _t82_1 = _asm256_loadu_pd(M2 + 20*k2 + k3 + 60);
      _t82_0 = _asm256_loadu_pd(v0 + k3);
      _t82_5 = _asm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t82_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t82_4, _t82_0), _mm256_mul_pd(_t82_3, _t82_0)), _mm256_hadd_pd(_mm256_mul_pd(_t82_2, _t82_0), _mm256_mul_pd(_t82_1, _t82_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t82_4, _t82_0), _mm256_mul_pd(_t82_3, _t82_0)), _mm256_hadd_pd(_mm256_mul_pd(_t82_2, _t82_0), _mm256_mul_pd(_t82_1, _t82_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t82_5 = _mm256_add_pd(_t82_5, _t82_6);

      // AVX Storer:
      _asm256_storeu_pd(x + k2, _t82_5);
    }
  }


  // Generating : P[20,20] = ( ( Sum_{k2} ( ( S(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, k2)) - ( G(h(4, 20, k2), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), M1[20,20],h(4, 20, k2)) ) ),h(4, 20, k2)) + Sum_{i0} ( S(h(4, 20, k2), ( G(h(4, 20, k2), Y[20,20],h(4, 20, i0)) - ( G(h(4, 20, k2), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), M1[20,20],h(4, 20, i0)) ) ),h(4, 20, i0)) ) ) ) + S(h(4, 20, 16), ( G(h(4, 20, 16), Y[20,20],h(4, 20, 16)) - ( G(h(4, 20, 16), M2[20,20],h(4, 20, 0)) * G(h(4, 20, 0), M1[20,20],h(4, 20, 16)) ) ),h(4, 20, 16)) ) + Sum_{k3} ( ( Sum_{k2} ( ( -$(h(4, 20, k2), ( G(h(4, 20, k2), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), M1[20,20],h(4, 20, k2)) ),h(4, 20, k2)) + Sum_{i0} ( -$(h(4, 20, k2), ( G(h(4, 20, k2), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), M1[20,20],h(4, 20, i0)) ),h(4, 20, i0)) ) ) ) + -$(h(4, 20, 16), ( G(h(4, 20, 16), M2[20,20],h(4, 20, k3)) * G(h(4, 20, k3), M1[20,20],h(4, 20, 16)) ),h(4, 20, 16)) ) ) )

  _asm256_storeu_pd(M1 + 60, _t79_7);
  _asm256_storeu_pd(M1 + 40, _t79_6);
  _asm256_storeu_pd(M1 + 20, _t79_5);
  _asm256_storeu_pd(M1, _t79_4);

  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t83_23 = _asm256_loadu_pd(Y + 21*k2);
    _t83_22 = _mm256_maskload_pd(Y + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t83_21 = _mm256_maskload_pd(Y + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t83_20 = _mm256_maskload_pd(Y + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t83_19 = _mm256_broadcast_sd(M2 + 20*k2);
    _t83_18 = _mm256_broadcast_sd(M2 + 20*k2 + 1);
    _t83_17 = _mm256_broadcast_sd(M2 + 20*k2 + 2);
    _t83_16 = _mm256_broadcast_sd(M2 + 20*k2 + 3);
    _t83_15 = _mm256_broadcast_sd(M2 + 20*k2 + 20);
    _t83_14 = _mm256_broadcast_sd(M2 + 20*k2 + 21);
    _t83_13 = _mm256_broadcast_sd(M2 + 20*k2 + 22);
    _t83_12 = _mm256_broadcast_sd(M2 + 20*k2 + 23);
    _t83_11 = _mm256_broadcast_sd(M2 + 20*k2 + 40);
    _t83_10 = _mm256_broadcast_sd(M2 + 20*k2 + 41);
    _t83_9 = _mm256_broadcast_sd(M2 + 20*k2 + 42);
    _t83_8 = _mm256_broadcast_sd(M2 + 20*k2 + 43);
    _t83_7 = _mm256_broadcast_sd(M2 + 20*k2 + 60);
    _t83_6 = _mm256_broadcast_sd(M2 + 20*k2 + 61);
    _t83_5 = _mm256_broadcast_sd(M2 + 20*k2 + 62);
    _t83_4 = _mm256_broadcast_sd(M2 + 20*k2 + 63);
    _t83_3 = _asm256_loadu_pd(M1 + k2);
    _t83_2 = _asm256_loadu_pd(M1 + k2 + 20);
    _t83_1 = _asm256_loadu_pd(M1 + k2 + 40);
    _t83_0 = _asm256_loadu_pd(M1 + k2 + 60);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t83_36 = _t83_23;
    _t83_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t83_23, _t83_22, 3), _t83_22, 12);
    _t83_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t83_23, _t83_22, 0), _t83_21, 49);
    _t83_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t83_23, _t83_22, 12), _mm256_shuffle_pd(_t83_21, _t83_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t83_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_19, _t83_3), _mm256_mul_pd(_t83_18, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_17, _t83_1), _mm256_mul_pd(_t83_16, _t83_0)));
    _t83_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_15, _t83_3), _mm256_mul_pd(_t83_14, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_13, _t83_1), _mm256_mul_pd(_t83_12, _t83_0)));
    _t83_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_11, _t83_3), _mm256_mul_pd(_t83_10, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_9, _t83_1), _mm256_mul_pd(_t83_8, _t83_0)));
    _t83_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_7, _t83_3), _mm256_mul_pd(_t83_6, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_5, _t83_1), _mm256_mul_pd(_t83_4, _t83_0)));

    // 4-BLAC: 4x4 - 4x4
    _t83_32 = _mm256_sub_pd(_t83_36, _t83_28);
    _t83_33 = _mm256_sub_pd(_t83_37, _t83_29);
    _t83_34 = _mm256_sub_pd(_t83_38, _t83_30);
    _t83_35 = _mm256_sub_pd(_t83_39, _t83_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t83_24 = _t83_32;
    _t83_25 = _t83_33;
    _t83_26 = _t83_34;
    _t83_27 = _t83_35;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
      _t84_7 = _asm256_loadu_pd(Y + i0 + 20*k2);
      _t84_6 = _asm256_loadu_pd(Y + i0 + 20*k2 + 20);
      _t84_5 = _asm256_loadu_pd(Y + i0 + 20*k2 + 40);
      _t84_4 = _asm256_loadu_pd(Y + i0 + 20*k2 + 60);
      _t84_3 = _asm256_loadu_pd(M1 + i0);
      _t84_2 = _asm256_loadu_pd(M1 + i0 + 20);
      _t84_1 = _asm256_loadu_pd(M1 + i0 + 40);
      _t84_0 = _asm256_loadu_pd(M1 + i0 + 60);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t84_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_19, _t84_3), _mm256_mul_pd(_t83_18, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t83_17, _t84_1), _mm256_mul_pd(_t83_16, _t84_0)));
      _t84_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_15, _t84_3), _mm256_mul_pd(_t83_14, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t83_13, _t84_1), _mm256_mul_pd(_t83_12, _t84_0)));
      _t84_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_11, _t84_3), _mm256_mul_pd(_t83_10, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t83_9, _t84_1), _mm256_mul_pd(_t83_8, _t84_0)));
      _t84_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_7, _t84_3), _mm256_mul_pd(_t83_6, _t84_2)), _mm256_add_pd(_mm256_mul_pd(_t83_5, _t84_1), _mm256_mul_pd(_t83_4, _t84_0)));

      // 4-BLAC: 4x4 - 4x4
      _t84_12 = _mm256_sub_pd(_t84_7, _t84_8);
      _t84_13 = _mm256_sub_pd(_t84_6, _t84_9);
      _t84_14 = _mm256_sub_pd(_t84_5, _t84_10);
      _t84_15 = _mm256_sub_pd(_t84_4, _t84_11);

      // AVX Storer:
      _asm256_storeu_pd(P + i0 + 20*k2, _t84_12);
      _asm256_storeu_pd(P + i0 + 20*k2 + 20, _t84_13);
      _asm256_storeu_pd(P + i0 + 20*k2 + 40, _t84_14);
      _asm256_storeu_pd(P + i0 + 20*k2 + 60, _t84_15);
    }
    _asm256_storeu_pd(P + 21*k2, _t83_24);
    _mm256_maskstore_pd(P + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t83_25);
    _mm256_maskstore_pd(P + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t83_26);
    _mm256_maskstore_pd(P + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t83_27);
  }

  _t85_19 = _mm256_broadcast_sd(M2 + 320);
  _t85_18 = _mm256_broadcast_sd(M2 + 321);
  _t85_17 = _mm256_broadcast_sd(M2 + 322);
  _t85_16 = _mm256_broadcast_sd(M2 + 323);
  _t85_15 = _mm256_broadcast_sd(M2 + 340);
  _t85_14 = _mm256_broadcast_sd(M2 + 341);
  _t85_13 = _mm256_broadcast_sd(M2 + 342);
  _t85_12 = _mm256_broadcast_sd(M2 + 343);
  _t85_11 = _mm256_broadcast_sd(M2 + 360);
  _t85_10 = _mm256_broadcast_sd(M2 + 361);
  _t85_9 = _mm256_broadcast_sd(M2 + 362);
  _t85_8 = _mm256_broadcast_sd(M2 + 363);
  _t85_7 = _mm256_broadcast_sd(M2 + 380);
  _t85_6 = _mm256_broadcast_sd(M2 + 381);
  _t85_5 = _mm256_broadcast_sd(M2 + 382);
  _t85_4 = _mm256_broadcast_sd(M2 + 383);
  _t85_3 = _asm256_loadu_pd(M1 + 16);
  _t85_2 = _asm256_loadu_pd(M1 + 36);
  _t85_1 = _asm256_loadu_pd(M1 + 56);
  _t85_0 = _asm256_loadu_pd(M1 + 76);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t85_28 = _t15_28;
  _t85_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t85_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t85_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t85_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_19, _t85_3), _mm256_mul_pd(_t85_18, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_17, _t85_1), _mm256_mul_pd(_t85_16, _t85_0)));
  _t85_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_15, _t85_3), _mm256_mul_pd(_t85_14, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_13, _t85_1), _mm256_mul_pd(_t85_12, _t85_0)));
  _t85_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_11, _t85_3), _mm256_mul_pd(_t85_10, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_9, _t85_1), _mm256_mul_pd(_t85_8, _t85_0)));
  _t85_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t85_7, _t85_3), _mm256_mul_pd(_t85_6, _t85_2)), _mm256_add_pd(_mm256_mul_pd(_t85_5, _t85_1), _mm256_mul_pd(_t85_4, _t85_0)));

  // 4-BLAC: 4x4 - 4x4
  _t85_24 = _mm256_sub_pd(_t85_28, _t85_20);
  _t85_25 = _mm256_sub_pd(_t85_29, _t85_21);
  _t85_26 = _mm256_sub_pd(_t85_30, _t85_22);
  _t85_27 = _mm256_sub_pd(_t85_31, _t85_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t10_3 = _t85_24;
  _t10_2 = _t85_25;
  _t10_1 = _t85_26;
  _t10_0 = _t85_27;


  for( int k3 = 4; k3 <= 19; k3+=4 ) {

    for( int k2 = 0; k2 <= 15; k2+=4 ) {
      _t86_19 = _mm256_broadcast_sd(M2 + 20*k2 + k3);
      _t86_18 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 1);
      _t86_17 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 2);
      _t86_16 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 3);
      _t86_15 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 20);
      _t86_14 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 21);
      _t86_13 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 22);
      _t86_12 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 23);
      _t86_11 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 40);
      _t86_10 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 41);
      _t86_9 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 42);
      _t86_8 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 43);
      _t86_7 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 60);
      _t86_6 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 61);
      _t86_5 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 62);
      _t86_4 = _mm256_broadcast_sd(M2 + 20*k2 + k3 + 63);
      _t86_3 = _asm256_loadu_pd(M1 + k2 + 20*k3);
      _t86_2 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 20);
      _t86_1 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 40);
      _t86_0 = _asm256_loadu_pd(M1 + k2 + 20*k3 + 60);
      _t86_20 = _asm256_loadu_pd(P + 21*k2);
      _t86_21 = _mm256_maskload_pd(P + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t86_22 = _mm256_maskload_pd(P + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t86_23 = _mm256_maskload_pd(P + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t86_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_19, _t86_3), _mm256_mul_pd(_t86_18, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_17, _t86_1), _mm256_mul_pd(_t86_16, _t86_0)));
      _t86_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_15, _t86_3), _mm256_mul_pd(_t86_14, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_13, _t86_1), _mm256_mul_pd(_t86_12, _t86_0)));
      _t86_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_11, _t86_3), _mm256_mul_pd(_t86_10, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_9, _t86_1), _mm256_mul_pd(_t86_8, _t86_0)));
      _t86_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_7, _t86_3), _mm256_mul_pd(_t86_6, _t86_2)), _mm256_add_pd(_mm256_mul_pd(_t86_5, _t86_1), _mm256_mul_pd(_t86_4, _t86_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t86_28 = _t86_20;
      _t86_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t86_20, _t86_21, 3), _t86_21, 12);
      _t86_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t86_20, _t86_21, 0), _t86_22, 49);
      _t86_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t86_20, _t86_21, 12), _mm256_shuffle_pd(_t86_22, _t86_23, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t86_28 = _mm256_sub_pd(_t86_28, _t86_24);
      _t86_29 = _mm256_sub_pd(_t86_29, _t86_25);
      _t86_30 = _mm256_sub_pd(_t86_30, _t86_26);
      _t86_31 = _mm256_sub_pd(_t86_31, _t86_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t86_20 = _t86_28;
      _t86_21 = _t86_29;
      _t86_22 = _t86_30;
      _t86_23 = _t86_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 19; i0+=4 ) {
        _t87_3 = _asm256_loadu_pd(M1 + i0 + 20*k3);
        _t87_2 = _asm256_loadu_pd(M1 + i0 + 20*k3 + 20);
        _t87_1 = _asm256_loadu_pd(M1 + i0 + 20*k3 + 40);
        _t87_0 = _asm256_loadu_pd(M1 + i0 + 20*k3 + 60);
        _t87_4 = _asm256_loadu_pd(P + i0 + 20*k2);
        _t87_5 = _asm256_loadu_pd(P + i0 + 20*k2 + 20);
        _t87_6 = _asm256_loadu_pd(P + i0 + 20*k2 + 40);
        _t87_7 = _asm256_loadu_pd(P + i0 + 20*k2 + 60);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t87_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_19, _t87_3), _mm256_mul_pd(_t86_18, _t87_2)), _mm256_add_pd(_mm256_mul_pd(_t86_17, _t87_1), _mm256_mul_pd(_t86_16, _t87_0)));
        _t87_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_15, _t87_3), _mm256_mul_pd(_t86_14, _t87_2)), _mm256_add_pd(_mm256_mul_pd(_t86_13, _t87_1), _mm256_mul_pd(_t86_12, _t87_0)));
        _t87_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_11, _t87_3), _mm256_mul_pd(_t86_10, _t87_2)), _mm256_add_pd(_mm256_mul_pd(_t86_9, _t87_1), _mm256_mul_pd(_t86_8, _t87_0)));
        _t87_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t86_7, _t87_3), _mm256_mul_pd(_t86_6, _t87_2)), _mm256_add_pd(_mm256_mul_pd(_t86_5, _t87_1), _mm256_mul_pd(_t86_4, _t87_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t87_4 = _mm256_sub_pd(_t87_4, _t87_8);
        _t87_5 = _mm256_sub_pd(_t87_5, _t87_9);
        _t87_6 = _mm256_sub_pd(_t87_6, _t87_10);
        _t87_7 = _mm256_sub_pd(_t87_7, _t87_11);

        // AVX Storer:
        _asm256_storeu_pd(P + i0 + 20*k2, _t87_4);
        _asm256_storeu_pd(P + i0 + 20*k2 + 20, _t87_5);
        _asm256_storeu_pd(P + i0 + 20*k2 + 40, _t87_6);
        _asm256_storeu_pd(P + i0 + 20*k2 + 60, _t87_7);
      }
      _asm256_storeu_pd(P + 21*k2, _t86_20);
      _mm256_maskstore_pd(P + 21*k2 + 20, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t86_21);
      _mm256_maskstore_pd(P + 21*k2 + 40, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t86_22);
      _mm256_maskstore_pd(P + 21*k2 + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t86_23);
    }
    _t88_19 = _mm256_broadcast_sd(M2 + k3 + 320);
    _t88_18 = _mm256_broadcast_sd(M2 + k3 + 321);
    _t88_17 = _mm256_broadcast_sd(M2 + k3 + 322);
    _t88_16 = _mm256_broadcast_sd(M2 + k3 + 323);
    _t88_15 = _mm256_broadcast_sd(M2 + k3 + 340);
    _t88_14 = _mm256_broadcast_sd(M2 + k3 + 341);
    _t88_13 = _mm256_broadcast_sd(M2 + k3 + 342);
    _t88_12 = _mm256_broadcast_sd(M2 + k3 + 343);
    _t88_11 = _mm256_broadcast_sd(M2 + k3 + 360);
    _t88_10 = _mm256_broadcast_sd(M2 + k3 + 361);
    _t88_9 = _mm256_broadcast_sd(M2 + k3 + 362);
    _t88_8 = _mm256_broadcast_sd(M2 + k3 + 363);
    _t88_7 = _mm256_broadcast_sd(M2 + k3 + 380);
    _t88_6 = _mm256_broadcast_sd(M2 + k3 + 381);
    _t88_5 = _mm256_broadcast_sd(M2 + k3 + 382);
    _t88_4 = _mm256_broadcast_sd(M2 + k3 + 383);
    _t88_3 = _asm256_loadu_pd(M1 + 20*k3 + 16);
    _t88_2 = _asm256_loadu_pd(M1 + 20*k3 + 36);
    _t88_1 = _asm256_loadu_pd(M1 + 20*k3 + 56);
    _t88_0 = _asm256_loadu_pd(M1 + 20*k3 + 76);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t88_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_19, _t88_3), _mm256_mul_pd(_t88_18, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_17, _t88_1), _mm256_mul_pd(_t88_16, _t88_0)));
    _t88_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_15, _t88_3), _mm256_mul_pd(_t88_14, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_13, _t88_1), _mm256_mul_pd(_t88_12, _t88_0)));
    _t88_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_11, _t88_3), _mm256_mul_pd(_t88_10, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_9, _t88_1), _mm256_mul_pd(_t88_8, _t88_0)));
    _t88_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_7, _t88_3), _mm256_mul_pd(_t88_6, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_5, _t88_1), _mm256_mul_pd(_t88_4, _t88_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t88_24 = _t10_3;
    _t88_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
    _t88_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
    _t88_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t88_24 = _mm256_sub_pd(_t88_24, _t88_20);
    _t88_25 = _mm256_sub_pd(_t88_25, _t88_21);
    _t88_26 = _mm256_sub_pd(_t88_26, _t88_22);
    _t88_27 = _mm256_sub_pd(_t88_27, _t88_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t10_3 = _t88_24;
    _t10_2 = _t88_25;
    _t10_1 = _t88_26;
    _t10_0 = _t88_27;
    _asm256_storeu_pd(P + 336, _t10_3);
    _mm256_maskstore_pd(P + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
    _mm256_maskstore_pd(P + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
    _mm256_maskstore_pd(P + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);
  }

  _asm256_storeu_pd(Y + 336, _t15_28);
  _mm256_maskstore_pd(Y + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
  _mm256_maskstore_pd(Y + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
  _mm256_maskstore_pd(Y + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  _mm_store_sd(&(v0[3]), _mm256_castpd256_pd128(_t70_0));
  _mm_store_sd(&(v0[2]), _mm256_castpd256_pd128(_t70_2));
  _mm_store_sd(&(v0[1]), _mm256_castpd256_pd128(_t70_4));
  _mm_store_sd(&(v0[0]), _mm256_castpd256_pd128(_t70_5));
  _asm256_storeu_pd(P + 336, _t10_3);
  _mm256_maskstore_pd(P + 356, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
  _mm256_maskstore_pd(P + 376, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
  _mm256_maskstore_pd(P + 396, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);

}
