/*
 * kf_kernel.h
 *
Decl { {u'B': SquaredMatrix[B, (28, 28), GenMatAccess], u'F': SquaredMatrix[F, (28, 28), GenMatAccess], 'T2407': Matrix[T2407, (1, 28), GenMatAccess], 'T2344': Matrix[T2344, (1, 28), GenMatAccess], u'U0': UpperTriangular[U0, (28, 28), GenMatAccess], u'M5': SquaredMatrix[M5, (28, 28), GenMatAccess], u'P': Symmetric[P, (28, 28), USMatAccess], u'M7': SquaredMatrix[M7, (28, 28), GenMatAccess], u'M6': SquaredMatrix[M6, (28, 28), GenMatAccess], u'v4': Matrix[v4, (28, 1), GenMatAccess], u'M0': SquaredMatrix[M0, (28, 28), GenMatAccess], u'M3': Symmetric[M3, (28, 28), USMatAccess], u'M2': SquaredMatrix[M2, (28, 28), GenMatAccess], u'Y': Symmetric[Y, (28, 28), USMatAccess], u'R': Symmetric[R, (28, 28), USMatAccess], u'U': UpperTriangular[U, (28, 28), GenMatAccess], u'M8': SquaredMatrix[M8, (28, 28), GenMatAccess], u'v0': Matrix[v0, (28, 1), GenMatAccess], u'u': Matrix[u, (28, 1), GenMatAccess], u'M4': Symmetric[M4, (28, 28), USMatAccess], u'v2': Matrix[v2, (28, 1), GenMatAccess], u'v1': Matrix[v1, (28, 1), GenMatAccess], u'v3': Matrix[v3, (28, 1), GenMatAccess], u'Q': Symmetric[Q, (28, 28), USMatAccess], u'x': Matrix[x, (28, 1), GenMatAccess], u'H': SquaredMatrix[H, (28, 28), GenMatAccess], u'y': Matrix[y, (28, 1), GenMatAccess], u'M1': SquaredMatrix[M1, (28, 28), GenMatAccess], u'z': Matrix[z, (28, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'Assign_Mul_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt': {'m0': 'm02.ll'}, 'ldiv_utn_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}, 'Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt': {'m0': 'm04.ll', 'm2': 'm21.ll'}}, 'cl1ck_v': 2, 'variant_tag': 'Assign_Mul_T_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_Assign_Mul_T_UpperTriangular_UpperTriangular_Symmetric_opt_m02_Assign_Mul_UpperTriangular_Matrix_Matrix_opt_m04_m21_Assign_Mul_UpperTriangular_SquaredMatrix_SquaredMatrix_opt_m04_m21_ldiv_utn_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), u[28,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[28,28] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[28,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[28,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[28,28] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[28,28] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[28,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[28,28] ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 28, 0), U[28,28],h(1, 28, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 0), U[28,28],h(1, 28, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 0), U[28,28],h(1, 28, 0)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(3, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(3, 28, 1)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(1, 28, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(1, 28, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(1, 28, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(2, 28, 2)) ) ) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), U[28,28],h(1, 28, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), U[28,28],h(1, 28, 2)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), U[28,28],h(1, 28, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), U[28,28],h(1, 28, 3)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) = ( Tile( (1, 1), G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) Div Tile( (1, 1), G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), U[28,28],h(1, 28, 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), U[28,28],h(1, 28, 3)) ) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ) )
Eq.ann: {}
Entry 23:
For_{fi1304;0;20;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(3, 28, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) ) ) )
Eq.ann: {}
 )Entry 24:
For_{fi1179;4;23;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1179), U[28,28],h(4, 28, fi1179)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1179), M4[28,28],h(4, 28, fi1179)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 28, 0), U[28,28],h(4, 28, fi1179)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 28, 0), U[28,28],h(4, 28, fi1179)) ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 28, fi1179), U[28,28],h(1, 28, fi1179)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, fi1179), U[28,28],h(1, 28, fi1179)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1179), U[28,28],h(1, 28, fi1179)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(3, 28, fi1179 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(3, 28, fi1179 + 1)) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(1, 28, fi1179 + 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(1, 28, fi1179 + 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(1, 28, fi1179 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(1, 28, fi1179 + 1)) ) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), G(h(1, 28, fi1179 + 1), U[28,28],h(1, 28, fi1179 + 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, fi1179 + 1), U[28,28],h(1, 28, fi1179 + 1)) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(2, 28, fi1179 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(2, 28, fi1179 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(1, 28, fi1179 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(2, 28, fi1179 + 2)) ) ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1179 + 1), U[28,28],h(1, 28, fi1179 + 1)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(2, 28, fi1179 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(2, 28, fi1179 + 2)) ) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1179), U[28,28],h(1, 28, fi1179 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1179), U[28,28],h(1, 28, fi1179 + 2)) ) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 2)) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1179), U[28,28],h(1, 28, fi1179 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1179), U[28,28],h(1, 28, fi1179 + 3)) ) ) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 3)) ) = ( Tile( (1, 1), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 3)) ) Div Tile( (1, 1), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 2)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 3), U[28,28],h(1, 28, fi1179 + 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 3), U[28,28],h(1, 28, fi1179 + 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1179), U[28,28],h(1, 28, fi1179 + 3)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1179), U[28,28],h(1, 28, fi1179 + 3)) ) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), G(h(1, 28, fi1179 + 3), U[28,28],h(1, 28, fi1179 + 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, fi1179 + 3), U[28,28],h(1, 28, fi1179 + 3)) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1179), U[28,28],h(-fi1179 + 24, 28, fi1179 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1179), M4[28,28],h(-fi1179 + 24, 28, fi1179 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 28, 0), U[28,28],h(4, 28, fi1179)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(fi1179, 28, 0), U[28,28],h(-fi1179 + 24, 28, fi1179 + 4)) ) ) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1179 + 3), U[28,28],h(1, 28, fi1179 + 3)) ) )
Eq.ann: {}
Entry 18:
For_{fi1429;0;-fi1179 + 20;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1179 + 1), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1179 + 1), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(3, 28, fi1179 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1179 + 2), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1179 + 2), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(2, 28, fi1179 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 1), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 3), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 3), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(1, 28, fi1179 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 2), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 3), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1179 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1179 + 3), U[28,28],h(4, 28, fi1179 + fi1429 + 4)) ) ) )
Eq.ann: {}
 ) )Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), U[28,28],h(4, 28, 24)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, 24), M4[28,28],h(4, 28, 24)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 0), U[28,28],h(4, 28, 24)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(24, 28, 0), U[28,28],h(4, 28, 24)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 28, 24), U[28,28],h(1, 28, 24)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 24), U[28,28],h(1, 28, 24)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, 24)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 24), U[28,28],h(1, 28, 24)) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U[28,28],h(3, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 24)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U[28,28],h(3, 28, 25)) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U[28,28],h(1, 28, 25)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U[28,28],h(1, 28, 25)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U[28,28],h(2, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U[28,28],h(2, 28, 26)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U[28,28],h(1, 28, 25)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U[28,28],h(2, 28, 26)) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2344[1,28],h(1, 28, 25)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U[28,28],h(2, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2344[1,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U[28,28],h(2, 28, 26)) ) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 24), U[28,28],h(1, 28, 26)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 24), U[28,28],h(1, 28, 26)) ) ) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), U[28,28],h(1, 28, 27)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), U[28,28],h(1, 28, 27)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 24), U[28,28],h(1, 28, 26)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 24), U[28,28],h(1, 28, 27)) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 28, 26), U[28,28],h(1, 28, 27)) ) = ( Tile( (1, 1), G(h(1, 28, 26), U[28,28],h(1, 28, 27)) ) Div Tile( (1, 1), G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), U[28,28],h(1, 28, 27)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), U[28,28],h(1, 28, 27)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 24), U[28,28],h(1, 28, 27)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 24), U[28,28],h(1, 28, 27)) ) ) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), G(h(1, 28, 27), U[28,28],h(1, 28, 27)) ) = Sqrt( Tile( (1, 1), G(h(1, 28, 27), U[28,28],h(1, 28, 27)) ) )
Eq.ann: {}
Entry 40:
For_{fi1554;0;23;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, fi1554), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1554), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, fi1554), U0[28,28],h(1, 28, fi1554)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1554 + 1), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1554 + 1), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554), U0[28,28],h(3, 28, fi1554 + 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, fi1554 + 1), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1554 + 1), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, fi1554 + 1), U0[28,28],h(1, 28, fi1554 + 1)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1554 + 2), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1554 + 2), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554 + 1), U0[28,28],h(2, 28, fi1554 + 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554 + 1), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, fi1554 + 2), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1554 + 2), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, fi1554 + 2), U0[28,28],h(1, 28, fi1554 + 2)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554 + 3), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554 + 3), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554 + 2), U0[28,28],h(1, 28, fi1554 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1554 + 2), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, fi1554 + 3), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, fi1554 + 3), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, fi1554 + 3), U0[28,28],h(1, 28, fi1554 + 3)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1554 + 24, 28, fi1554 + 4), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1554 + 24, 28, fi1554 + 4), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1554), U0[28,28],h(-fi1554 + 24, 28, fi1554 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1554), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 41:
Eq: Tile( (1, 1), G(h(1, 28, 24), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 24), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 24), U0[28,28],h(1, 28, 24)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U0[28,28],h(3, 28, 25)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 28, 25), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 25), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 25), U0[28,28],h(1, 28, 25)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U0[28,28],h(2, 28, 26)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 28, 26), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 26), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 26), U0[28,28],h(1, 28, 26)) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), v2[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), v2[28,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), U0[28,28],h(1, 28, 27)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), v2[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 28, 27), v2[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 27), v2[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 27), U0[28,28],h(1, 28, 27)) ) )
Eq.ann: {}
Entry 48:
For_{fi1631;0;23;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 28, -fi1631 + 27), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, -fi1631 + 27), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, -fi1631 + 27), U0[28,28],h(1, 28, -fi1631 + 27)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, -fi1631 + 24), U0[28,28],h(1, 28, -fi1631 + 27)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1631 + 27), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 28, -fi1631 + 26), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, -fi1631 + 26), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, -fi1631 + 26), U0[28,28],h(1, 28, -fi1631 + 26)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, -fi1631 + 24), U0[28,28],h(1, 28, -fi1631 + 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1631 + 26), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), G(h(1, 28, -fi1631 + 25), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, -fi1631 + 25), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, -fi1631 + 25), U0[28,28],h(1, 28, -fi1631 + 25)) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1631 + 24), U0[28,28],h(1, 28, -fi1631 + 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1631 + 25), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), G(h(1, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, -fi1631 + 24), U0[28,28],h(1, 28, -fi1631 + 24)) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1631 + 24, 28, 0), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1631 + 24, 28, 0), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1631 + 24, 28, 0), U0[28,28],h(4, 28, -fi1631 + 24)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, -fi1631 + 24), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
 )Entry 49:
Eq: Tile( (1, 1), G(h(1, 28, 3), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 3), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 3), U0[28,28],h(1, 28, 3)) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), U0[28,28],h(1, 28, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), G(h(1, 28, 2), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 2), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 2), U0[28,28],h(1, 28, 2)) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), U0[28,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), G(h(1, 28, 1), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 1), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 1), U0[28,28],h(1, 28, 1)) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), v4[28,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), v4[28,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U0[28,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), v4[28,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), G(h(1, 28, 0), v4[28,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 28, 0), v4[28,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 28, 0), U0[28,28],h(1, 28, 0)) ) )
Eq.ann: {}
Entry 56:
For_{fi1708;0;23;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1708), U0[28,28],h(1, 28, fi1708)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708 + 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1708 + 1), U0[28,28],h(1, 28, fi1708 + 1)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708 + 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1708 + 2), U0[28,28],h(1, 28, fi1708 + 2)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708 + 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, fi1708 + 3), U0[28,28],h(1, 28, fi1708 + 3)) ) )
Eq.ann: {}
Entry 4:
For_{fi1727;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708), M6[28,28],h(4, 28, fi1727)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1708 + 1), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, fi1708 + 1), M6[28,28],h(4, 28, fi1727)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708), U0[28,28],h(3, 28, fi1708 + 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708), M6[28,28],h(4, 28, fi1727)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 1), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708 + 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 1), M6[28,28],h(4, 28, fi1727)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1708 + 2), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, fi1708 + 2), M6[28,28],h(4, 28, fi1727)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 1), U0[28,28],h(2, 28, fi1708 + 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 1), M6[28,28],h(4, 28, fi1727)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 2), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708 + 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 2), M6[28,28],h(4, 28, fi1727)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 3), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 3), M6[28,28],h(4, 28, fi1727)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 2), U0[28,28],h(1, 28, fi1708 + 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 2), M6[28,28],h(4, 28, fi1727)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 3), M6[28,28],h(4, 28, fi1727)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1708 + 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, fi1708 + 3), M6[28,28],h(4, 28, fi1727)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1708 + 24, 28, fi1708 + 4), M6[28,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1708 + 24, 28, fi1708 + 4), M6[28,28],h(28, 28, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1708), U0[28,28],h(-fi1708 + 24, 28, fi1708 + 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, fi1708), M6[28,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 57:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 24)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 24), U0[28,28],h(1, 28, 24)) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 25)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 25), U0[28,28],h(1, 28, 25)) ) )
Eq.ann: {}
Entry 59:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 26)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 26), U0[28,28],h(1, 28, 26)) ) )
Eq.ann: {}
Entry 60:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 27)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 27), U0[28,28],h(1, 28, 27)) ) )
Eq.ann: {}
Entry 61:
For_{fi1774;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 24)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), M6[28,28],h(4, 28, fi1774)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 25), M6[28,28],h(4, 28, fi1774)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), U0[28,28],h(3, 28, 25)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 24), M6[28,28],h(4, 28, fi1774)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), M6[28,28],h(4, 28, fi1774)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 26), M6[28,28],h(4, 28, fi1774)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), U0[28,28],h(2, 28, 26)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 25), M6[28,28],h(4, 28, fi1774)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), M6[28,28],h(4, 28, fi1774)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), M6[28,28],h(4, 28, fi1774)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), U0[28,28],h(1, 28, 27)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 26), M6[28,28],h(4, 28, fi1774)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), M6[28,28],h(4, 28, fi1774)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 27)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 27), M6[28,28],h(4, 28, fi1774)) ) ) )
Eq.ann: {}
 )Entry 62:
For_{fi1821;0;23;4} ( Entry 0:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 27)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, -fi1821 + 27), U0[28,28],h(1, 28, -fi1821 + 27)) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 26)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, -fi1821 + 26), U0[28,28],h(1, 28, -fi1821 + 26)) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 25)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, -fi1821 + 25), U0[28,28],h(1, 28, -fi1821 + 25)) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 24)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, -fi1821 + 24), U0[28,28],h(1, 28, -fi1821 + 24)) ) )
Eq.ann: {}
Entry 4:
For_{fi1840;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 27), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 27)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 27), M8[28,28],h(4, 28, fi1840)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, -fi1821 + 24), U0[28,28],h(1, 28, -fi1821 + 27)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 27), M8[28,28],h(4, 28, fi1840)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 26), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 26)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 26), M8[28,28],h(4, 28, fi1840)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, -fi1821 + 24), U0[28,28],h(1, 28, -fi1821 + 26)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 26), M8[28,28],h(4, 28, fi1840)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 25), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 25), M8[28,28],h(4, 28, fi1840)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 24), U0[28,28],h(1, 28, -fi1821 + 25)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 25), M8[28,28],h(4, 28, fi1840)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1821 + 24)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, -fi1821 + 24), M8[28,28],h(4, 28, fi1840)) ) ) )
Eq.ann: {}
 )Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(-fi1821 + 24, 28, 0), M8[28,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1821 + 24, 28, 0), M8[28,28],h(28, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(-fi1821 + 24, 28, 0), U0[28,28],h(4, 28, -fi1821 + 24)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 28, -fi1821 + 24), M8[28,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
 )Entry 63:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 3), U0[28,28],h(1, 28, 3)) ) )
Eq.ann: {}
Entry 64:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 2), U0[28,28],h(1, 28, 2)) ) )
Eq.ann: {}
Entry 65:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 1), U0[28,28],h(1, 28, 1)) ) )
Eq.ann: {}
Entry 66:
Eq: Tile( (1, 1), G(h(1, 1, 0), T2407[1,28],h(1, 28, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 28, 0), U0[28,28],h(1, 28, 0)) ) )
Eq.ann: {}
Entry 67:
For_{fi1887;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), M8[28,28],h(4, 28, fi1887)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 28, 0), U0[28,28],h(1, 28, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 3), M8[28,28],h(4, 28, fi1887)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), M8[28,28],h(4, 28, fi1887)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 28, 0), U0[28,28],h(1, 28, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 2), M8[28,28],h(4, 28, fi1887)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), M8[28,28],h(4, 28, fi1887)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), U0[28,28],h(1, 28, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 1), M8[28,28],h(4, 28, fi1887)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T2407[1,28],h(1, 28, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 28, 0), M8[28,28],h(4, 28, fi1887)) ) ) )
Eq.ann: {}
 )Entry 68:
Eq: Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), v0[28,1] ) ) ) )
Eq.ann: {}
Entry 69:
Eq: Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), M1[28,28] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-08-09
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


static __inline__ __m256d _asm256_loadu_pd(const double* p) {
  __m256d v;
  __asm__("vmovupd %1, %0" : "=x" (v) : "m" (*p));
  return v;
}

static __inline__ void _asm256_storeu_pd(double* p, const __m256d& v) {
  __asm__("vmovupd %1, %0" : "=rm" (*p) : "x" (v));
}

#define PARAM0 28
#define PARAM1 28
#define PARAM2 28

#define ERRTHRESH 1e-5

#define SOFTERRTHRESH 1e-7

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))
#define Max(x,y)    ((x) > (y) ? (x) : (y))
#define Min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5, _t1_6;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27, _t7_28, _t7_29, _t7_30, _t7_31;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23,
	_t8_24, _t8_25, _t8_26, _t8_27;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27, _t11_28, _t11_29, _t11_30, _t11_31;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23,
	_t13_24, _t13_25, _t13_26, _t13_27, _t13_28, _t13_29, _t13_30, _t13_31,
	_t13_32, _t13_33, _t13_34, _t13_35, _t13_36, _t13_37, _t13_38, _t13_39,
	_t13_40, _t13_41, _t13_42, _t13_43;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31,
	_t16_32, _t16_33, _t16_34, _t16_35;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27, _t25_28, _t25_29, _t25_30, _t25_31;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27, _t29_28, _t29_29, _t29_30, _t29_31;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27, _t35_28, _t35_29, _t35_30, _t35_31;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11, _t38_12, _t38_13, _t38_14, _t38_15,
	_t38_16, _t38_17, _t38_18, _t38_19, _t38_20, _t38_21, _t38_22, _t38_23;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31,
	_t42_32, _t42_33, _t42_34, _t42_35, _t42_36, _t42_37, _t42_38, _t42_39,
	_t42_40, _t42_41, _t42_42, _t42_43;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31,
	_t44_32, _t44_33, _t44_34, _t44_35, _t44_36, _t44_37, _t44_38, _t44_39,
	_t44_40, _t44_41, _t44_42, _t44_43;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27, _t45_28, _t45_29, _t45_30, _t45_31,
	_t45_32, _t45_33, _t45_34, _t45_35;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11, _t46_12, _t46_13, _t46_14, _t46_15;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27, _t47_28, _t47_29, _t47_30, _t47_31;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35, _t48_36, _t48_37, _t48_38, _t48_39,
	_t48_40, _t48_41, _t48_42, _t48_43, _t48_44, _t48_45, _t48_46, _t48_47,
	_t48_48, _t48_49, _t48_50, _t48_51, _t48_52, _t48_53, _t48_54, _t48_55,
	_t48_56, _t48_57, _t48_58, _t48_59, _t48_60, _t48_61, _t48_62, _t48_63,
	_t48_64, _t48_65, _t48_66, _t48_67, _t48_68, _t48_69, _t48_70, _t48_71,
	_t48_72, _t48_73, _t48_74, _t48_75, _t48_76, _t48_77, _t48_78, _t48_79,
	_t48_80, _t48_81, _t48_82, _t48_83, _t48_84, _t48_85, _t48_86, _t48_87,
	_t48_88, _t48_89, _t48_90, _t48_91, _t48_92, _t48_93, _t48_94, _t48_95,
	_t48_96, _t48_97, _t48_98, _t48_99, _t48_100, _t48_101, _t48_102, _t48_103,
	_t48_104, _t48_105, _t48_106, _t48_107, _t48_108;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23,
	_t50_24, _t50_25, _t50_26, _t50_27, _t50_28, _t50_29, _t50_30, _t50_31,
	_t50_32, _t50_33, _t50_34, _t50_35, _t50_36, _t50_37, _t50_38, _t50_39,
	_t50_40, _t50_41, _t50_42, _t50_43, _t50_44, _t50_45, _t50_46, _t50_47,
	_t50_48, _t50_49, _t50_50, _t50_51, _t50_52, _t50_53, _t50_54, _t50_55,
	_t50_56, _t50_57, _t50_58, _t50_59, _t50_60, _t50_61, _t50_62, _t50_63,
	_t50_64, _t50_65, _t50_66, _t50_67, _t50_68, _t50_69, _t50_70, _t50_71,
	_t50_72, _t50_73, _t50_74, _t50_75, _t50_76, _t50_77, _t50_78, _t50_79,
	_t50_80, _t50_81, _t50_82, _t50_83, _t50_84, _t50_85;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19, _t52_20, _t52_21, _t52_22, _t52_23,
	_t52_24, _t52_25, _t52_26, _t52_27, _t52_28, _t52_29, _t52_30, _t52_31,
	_t52_32, _t52_33, _t52_34, _t52_35, _t52_36, _t52_37, _t52_38, _t52_39,
	_t52_40, _t52_41, _t52_42, _t52_43, _t52_44, _t52_45, _t52_46;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39,
	_t54_40, _t54_41, _t54_42, _t54_43, _t54_44, _t54_45, _t54_46, _t54_47,
	_t54_48, _t54_49, _t54_50, _t54_51, _t54_52, _t54_53, _t54_54, _t54_55,
	_t54_56, _t54_57, _t54_58, _t54_59, _t54_60, _t54_61, _t54_62, _t54_63,
	_t54_64, _t54_65, _t54_66, _t54_67, _t54_68, _t54_69, _t54_70, _t54_71,
	_t54_72, _t54_73, _t54_74, _t54_75, _t54_76, _t54_77, _t54_78, _t54_79,
	_t54_80, _t54_81, _t54_82, _t54_83, _t54_84, _t54_85, _t54_86, _t54_87,
	_t54_88, _t54_89, _t54_90, _t54_91, _t54_92, _t54_93, _t54_94, _t54_95,
	_t54_96, _t54_97, _t54_98, _t54_99, _t54_100, _t54_101;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3, _t58_4, _t58_5, _t58_6, _t58_7,
	_t58_8, _t58_9, _t58_10, _t58_11, _t58_12, _t58_13, _t58_14, _t58_15,
	_t58_16, _t58_17, _t58_18, _t58_19, _t58_20, _t58_21, _t58_22, _t58_23,
	_t58_24, _t58_25, _t58_26, _t58_27, _t58_28, _t58_29, _t58_30, _t58_31,
	_t58_32, _t58_33, _t58_34, _t58_35, _t58_36, _t58_37, _t58_38, _t58_39,
	_t58_40, _t58_41, _t58_42, _t58_43, _t58_44, _t58_45, _t58_46;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19, _t62_20, _t62_21, _t62_22, _t62_23,
	_t62_24, _t62_25, _t62_26, _t62_27, _t62_28, _t62_29, _t62_30, _t62_31,
	_t62_32, _t62_33, _t62_34, _t62_35, _t62_36, _t62_37, _t62_38, _t62_39,
	_t62_40, _t62_41, _t62_42, _t62_43, _t62_44, _t62_45, _t62_46, _t62_47,
	_t62_48, _t62_49, _t62_50, _t62_51;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6, _t63_7,
	_t63_8, _t63_9, _t63_10, _t63_11;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15,
	_t67_16, _t67_17, _t67_18, _t67_19;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23,
	_t69_24, _t69_25, _t69_26, _t69_27, _t69_28, _t69_29, _t69_30, _t69_31,
	_t69_32, _t69_33, _t69_34, _t69_35, _t69_36, _t69_37, _t69_38, _t69_39,
	_t69_40, _t69_41, _t69_42, _t69_43, _t69_44, _t69_45, _t69_46, _t69_47,
	_t69_48, _t69_49, _t69_50, _t69_51, _t69_52, _t69_53, _t69_54, _t69_55,
	_t69_56, _t69_57, _t69_58, _t69_59, _t69_60, _t69_61;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6, _t70_7,
	_t70_8, _t70_9, _t70_10, _t70_11, _t70_12, _t70_13, _t70_14, _t70_15,
	_t70_16, _t70_17, _t70_18, _t70_19, _t70_20, _t70_21, _t70_22, _t70_23,
	_t70_24, _t70_25, _t70_26, _t70_27, _t70_28, _t70_29, _t70_30, _t70_31,
	_t70_32, _t70_33, _t70_34, _t70_35, _t70_36, _t70_37, _t70_38, _t70_39,
	_t70_40, _t70_41, _t70_42;
  __m256d _t71_0, _t71_1, _t71_2, _t71_3, _t71_4, _t71_5, _t71_6, _t71_7,
	_t71_8, _t71_9, _t71_10, _t71_11, _t71_12, _t71_13;
  __m256d _t72_0, _t72_1, _t72_2, _t72_3, _t72_4, _t72_5, _t72_6, _t72_7,
	_t72_8, _t72_9, _t72_10, _t72_11, _t72_12, _t72_13, _t72_14, _t72_15,
	_t72_16, _t72_17, _t72_18, _t72_19, _t72_20, _t72_21, _t72_22, _t72_23,
	_t72_24, _t72_25, _t72_26, _t72_27, _t72_28, _t72_29, _t72_30, _t72_31,
	_t72_32, _t72_33, _t72_34, _t72_35;
  __m256d _t73_0, _t73_1, _t73_2, _t73_3, _t73_4, _t73_5, _t73_6, _t73_7,
	_t73_8, _t73_9, _t73_10, _t73_11, _t73_12, _t73_13, _t73_14, _t73_15,
	_t73_16, _t73_17, _t73_18, _t73_19, _t73_20, _t73_21, _t73_22, _t73_23,
	_t73_24, _t73_25, _t73_26, _t73_27, _t73_28, _t73_29, _t73_30, _t73_31,
	_t73_32, _t73_33, _t73_34, _t73_35, _t73_36, _t73_37, _t73_38, _t73_39;
  __m256d _t74_0, _t74_1, _t74_2, _t74_3, _t74_4, _t74_5, _t74_6, _t74_7,
	_t74_8, _t74_9;
  __m256d _t75_0, _t75_1, _t75_2, _t75_3, _t75_4, _t75_5, _t75_6, _t75_7,
	_t75_8, _t75_9, _t75_10, _t75_11, _t75_12, _t75_13, _t75_14, _t75_15,
	_t75_16, _t75_17, _t75_18, _t75_19, _t75_20, _t75_21, _t75_22, _t75_23,
	_t75_24, _t75_25, _t75_26, _t75_27, _t75_28, _t75_29, _t75_30, _t75_31,
	_t75_32;
  __m256d _t76_0, _t76_1, _t76_2, _t76_3, _t76_4, _t76_5, _t76_6, _t76_7,
	_t76_8, _t76_9, _t76_10, _t76_11, _t76_12, _t76_13, _t76_14, _t76_15,
	_t76_16, _t76_17, _t76_18, _t76_19, _t76_20, _t76_21, _t76_22, _t76_23,
	_t76_24, _t76_25, _t76_26, _t76_27, _t76_28, _t76_29, _t76_30, _t76_31,
	_t76_32, _t76_33, _t76_34, _t76_35, _t76_36, _t76_37, _t76_38, _t76_39,
	_t76_40, _t76_41, _t76_42, _t76_43, _t76_44, _t76_45, _t76_46, _t76_47,
	_t76_48, _t76_49, _t76_50, _t76_51, _t76_52, _t76_53, _t76_54, _t76_55,
	_t76_56, _t76_57, _t76_58, _t76_59, _t76_60, _t76_61;
  __m256d _t77_0, _t77_1, _t77_2, _t77_3, _t77_4, _t77_5, _t77_6, _t77_7,
	_t77_8, _t77_9, _t77_10, _t77_11, _t77_12, _t77_13, _t77_14, _t77_15,
	_t77_16, _t77_17, _t77_18, _t77_19, _t77_20, _t77_21, _t77_22, _t77_23,
	_t77_24, _t77_25, _t77_26;
  __m256d _t78_0, _t78_1, _t78_2, _t78_3, _t78_4, _t78_5, _t78_6, _t78_7,
	_t78_8, _t78_9, _t78_10, _t78_11, _t78_12, _t78_13, _t78_14, _t78_15,
	_t78_16, _t78_17, _t78_18, _t78_19;
  __m256d _t79_0, _t79_1, _t79_2, _t79_3, _t79_4, _t79_5, _t79_6, _t79_7,
	_t79_8, _t79_9, _t79_10, _t79_11, _t79_12, _t79_13, _t79_14, _t79_15,
	_t79_16, _t79_17, _t79_18, _t79_19, _t79_20, _t79_21, _t79_22, _t79_23,
	_t79_24, _t79_25, _t79_26, _t79_27, _t79_28, _t79_29, _t79_30, _t79_31,
	_t79_32, _t79_33, _t79_34, _t79_35, _t79_36, _t79_37, _t79_38, _t79_39,
	_t79_40, _t79_41, _t79_42, _t79_43, _t79_44, _t79_45, _t79_46, _t79_47,
	_t79_48, _t79_49, _t79_50, _t79_51, _t79_52, _t79_53, _t79_54;
  __m256d _t80_0, _t80_1, _t80_2, _t80_3, _t80_4, _t80_5, _t80_6, _t80_7,
	_t80_8, _t80_9, _t80_10, _t80_11, _t80_12, _t80_13, _t80_14, _t80_15,
	_t80_16, _t80_17, _t80_18, _t80_19, _t80_20, _t80_21, _t80_22, _t80_23,
	_t80_24, _t80_25, _t80_26;
  __m256d _t81_0, _t81_1, _t81_2, _t81_3, _t81_4, _t81_5, _t81_6, _t81_7,
	_t81_8, _t81_9, _t81_10, _t81_11, _t81_12, _t81_13, _t81_14, _t81_15,
	_t81_16, _t81_17, _t81_18, _t81_19, _t81_20, _t81_21, _t81_22, _t81_23,
	_t81_24, _t81_25, _t81_26, _t81_27, _t81_28, _t81_29, _t81_30, _t81_31,
	_t81_32, _t81_33, _t81_34, _t81_35, _t81_36, _t81_37, _t81_38, _t81_39,
	_t81_40, _t81_41, _t81_42, _t81_43, _t81_44, _t81_45, _t81_46, _t81_47,
	_t81_48, _t81_49, _t81_50, _t81_51, _t81_52, _t81_53, _t81_54, _t81_55,
	_t81_56, _t81_57, _t81_58;
  __m256d _t82_0, _t82_1, _t82_2, _t82_3, _t82_4, _t82_5, _t82_6, _t82_7,
	_t82_8, _t82_9, _t82_10, _t82_11, _t82_12, _t82_13, _t82_14, _t82_15,
	_t82_16, _t82_17, _t82_18, _t82_19, _t82_20, _t82_21, _t82_22, _t82_23,
	_t82_24, _t82_25, _t82_26;
  __m256d _t83_0, _t83_1, _t83_2, _t83_3, _t83_4, _t83_5, _t83_6, _t83_7,
	_t83_8, _t83_9, _t83_10, _t83_11, _t83_12, _t83_13, _t83_14, _t83_15,
	_t83_16, _t83_17, _t83_18, _t83_19, _t83_20, _t83_21, _t83_22, _t83_23,
	_t83_24, _t83_25, _t83_26, _t83_27;
  __m256d _t84_0, _t84_1, _t84_2, _t84_3, _t84_4, _t84_5, _t84_6, _t84_7,
	_t84_8, _t84_9, _t84_10, _t84_11, _t84_12, _t84_13, _t84_14, _t84_15,
	_t84_16, _t84_17, _t84_18, _t84_19, _t84_20, _t84_21, _t84_22, _t84_23,
	_t84_24, _t84_25, _t84_26, _t84_27, _t84_28, _t84_29, _t84_30, _t84_31,
	_t84_32, _t84_33, _t84_34, _t84_35, _t84_36, _t84_37, _t84_38, _t84_39,
	_t84_40, _t84_41, _t84_42, _t84_43, _t84_44, _t84_45, _t84_46, _t84_47,
	_t84_48, _t84_49, _t84_50, _t84_51;
  __m256d _t85_0, _t85_1, _t85_2, _t85_3, _t85_4, _t85_5, _t85_6, _t85_7,
	_t85_8, _t85_9, _t85_10, _t85_11, _t85_12, _t85_13, _t85_14, _t85_15,
	_t85_16, _t85_17, _t85_18, _t85_19, _t85_20, _t85_21, _t85_22, _t85_23,
	_t85_24, _t85_25, _t85_26;
  __m256d _t86_0, _t86_1, _t86_2, _t86_3, _t86_4, _t86_5, _t86_6;
  __m256d _t87_0, _t87_1, _t87_2, _t87_3, _t87_4, _t87_5, _t87_6;
  __m256d _t88_0, _t88_1, _t88_2, _t88_3, _t88_4, _t88_5, _t88_6, _t88_7,
	_t88_8, _t88_9, _t88_10, _t88_11, _t88_12, _t88_13, _t88_14, _t88_15,
	_t88_16, _t88_17, _t88_18, _t88_19, _t88_20, _t88_21, _t88_22, _t88_23,
	_t88_24, _t88_25, _t88_26, _t88_27, _t88_28, _t88_29, _t88_30, _t88_31,
	_t88_32, _t88_33, _t88_34, _t88_35, _t88_36, _t88_37, _t88_38, _t88_39;
  __m256d _t89_0, _t89_1, _t89_2, _t89_3, _t89_4, _t89_5, _t89_6, _t89_7,
	_t89_8, _t89_9, _t89_10, _t89_11, _t89_12, _t89_13, _t89_14, _t89_15;
  __m256d _t90_0, _t90_1, _t90_2, _t90_3, _t90_4, _t90_5, _t90_6, _t90_7,
	_t90_8, _t90_9, _t90_10, _t90_11, _t90_12, _t90_13, _t90_14, _t90_15,
	_t90_16, _t90_17, _t90_18, _t90_19, _t90_20, _t90_21, _t90_22, _t90_23,
	_t90_24, _t90_25, _t90_26, _t90_27, _t90_28, _t90_29, _t90_30, _t90_31;
  __m256d _t91_0, _t91_1, _t91_2, _t91_3, _t91_4, _t91_5, _t91_6, _t91_7,
	_t91_8, _t91_9, _t91_10, _t91_11, _t91_12, _t91_13, _t91_14, _t91_15,
	_t91_16, _t91_17, _t91_18, _t91_19, _t91_20, _t91_21, _t91_22, _t91_23,
	_t91_24, _t91_25, _t91_26, _t91_27, _t91_28, _t91_29, _t91_30, _t91_31;
  __m256d _t92_0, _t92_1, _t92_2, _t92_3, _t92_4, _t92_5, _t92_6, _t92_7,
	_t92_8, _t92_9, _t92_10, _t92_11;
  __m256d _t93_0, _t93_1, _t93_2, _t93_3, _t93_4, _t93_5, _t93_6, _t93_7,
	_t93_8, _t93_9, _t93_10, _t93_11, _t93_12, _t93_13, _t93_14, _t93_15,
	_t93_16, _t93_17, _t93_18, _t93_19, _t93_20, _t93_21, _t93_22, _t93_23,
	_t93_24, _t93_25, _t93_26, _t93_27;


  // Generating : y[28,1] = ( ( Sum_{i0} ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), x[28,1],h(1, 1, 0)) ) + ( G(h(4, 28, i0), B[28,28],h(4, 28, 0)) * G(h(4, 28, 0), u[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), x[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) ) + Sum_{k3} ( Sum_{i0} ( $(h(4, 28, i0), ( G(h(4, 28, i0), B[28,28],h(4, 28, k3)) * G(h(4, 28, k3), u[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t0_9 = _asm256_loadu_pd(F + 28*i0);
    _t0_8 = _asm256_loadu_pd(F + 28*i0 + 28);
    _t0_7 = _asm256_loadu_pd(F + 28*i0 + 56);
    _t0_6 = _asm256_loadu_pd(F + 28*i0 + 84);
    _t0_5 = _asm256_loadu_pd(x);
    _t0_4 = _asm256_loadu_pd(B + 28*i0);
    _t0_3 = _asm256_loadu_pd(B + 28*i0 + 28);
    _t0_2 = _asm256_loadu_pd(B + 28*i0 + 56);
    _t0_1 = _asm256_loadu_pd(B + 28*i0 + 84);
    _t0_0 = _asm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:
    _asm256_storeu_pd(y + i0, _t0_10);
  }


  for( int k2 = 4; k2 <= 27; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 27; i0+=4 ) {
      _t1_4 = _asm256_loadu_pd(F + 28*i0 + k2);
      _t1_3 = _asm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t1_2 = _asm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t1_1 = _asm256_loadu_pd(F + 28*i0 + k2 + 84);
      _t1_0 = _asm256_loadu_pd(x + k2);
      _t1_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t1_5 = _mm256_add_pd(_t1_5, _t1_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t1_5);
    }
  }


  for( int k3 = 4; k3 <= 27; k3+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 27; i0+=4 ) {
      _t2_4 = _asm256_loadu_pd(B + 28*i0 + k3);
      _t2_3 = _asm256_loadu_pd(B + 28*i0 + k3 + 28);
      _t2_2 = _asm256_loadu_pd(B + 28*i0 + k3 + 56);
      _t2_1 = _asm256_loadu_pd(B + 28*i0 + k3 + 84);
      _t2_0 = _asm256_loadu_pd(u + k3);
      _t2_5 = _asm256_loadu_pd(y + i0);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t2_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t2_5 = _mm256_add_pd(_t2_5, _t2_6);

      // AVX Storer:
      _asm256_storeu_pd(y + i0, _t2_5);
    }
  }

  _t3_3 = _asm256_loadu_pd(P);
  _t3_2 = _mm256_maskload_pd(P + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[28,28] = ( ( Sum_{k2} ( ( S(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{i0} ( S(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, k3)) * T( G(h(4, 28, i0), P[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) + $(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, k3)) * G(h(4, 28, k3), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, k3)) * G(h(4, 28, k3), P[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, 24)) * T( G(h(4, 28, i0), P[28,28],h(4, 28, 24)) ) ),h(4, 28, i0)) ) + $(h(4, 28, k2), ( G(h(4, 28, k2), F[28,28],h(4, 28, 24)) * G(h(4, 28, 24), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_4 = _t3_3;
  _t3_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int k2 = 0; k2 <= 27; k2+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 28*k2);
    _t4_14 = _mm256_broadcast_sd(F + 28*k2 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 28*k2 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 28*k2 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 28*k2 + 28);
    _t4_10 = _mm256_broadcast_sd(F + 28*k2 + 29);
    _t4_9 = _mm256_broadcast_sd(F + 28*k2 + 30);
    _t4_8 = _mm256_broadcast_sd(F + 28*k2 + 31);
    _t4_7 = _mm256_broadcast_sd(F + 28*k2 + 56);
    _t4_6 = _mm256_broadcast_sd(F + 28*k2 + 57);
    _t4_5 = _mm256_broadcast_sd(F + 28*k2 + 58);
    _t4_4 = _mm256_broadcast_sd(F + 28*k2 + 59);
    _t4_3 = _mm256_broadcast_sd(F + 28*k2 + 84);
    _t4_2 = _mm256_broadcast_sd(F + 28*k2 + 85);
    _t4_1 = _mm256_broadcast_sd(F + 28*k2 + 86);
    _t4_0 = _mm256_broadcast_sd(F + 28*k2 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_4), _mm256_mul_pd(_t4_14, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_6), _mm256_mul_pd(_t4_12, _t3_7)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_4), _mm256_mul_pd(_t4_10, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_6), _mm256_mul_pd(_t4_8, _t3_7)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_4), _mm256_mul_pd(_t4_6, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_6), _mm256_mul_pd(_t4_4, _t3_7)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_4), _mm256_mul_pd(_t4_2, _t3_5)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_6), _mm256_mul_pd(_t4_0, _t3_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 27; i0+=4 ) {
      _t5_3 = _asm256_loadu_pd(P + i0);
      _t5_2 = _asm256_loadu_pd(P + i0 + 28);
      _t5_1 = _asm256_loadu_pd(P + i0 + 56);
      _t5_0 = _asm256_loadu_pd(P + i0 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t5_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t5_3), _mm256_mul_pd(_t4_14, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t5_1), _mm256_mul_pd(_t4_12, _t5_0)));
      _t5_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t5_3), _mm256_mul_pd(_t4_10, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t5_1), _mm256_mul_pd(_t4_8, _t5_0)));
      _t5_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t5_3), _mm256_mul_pd(_t4_6, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t5_1), _mm256_mul_pd(_t4_4, _t5_0)));
      _t5_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t5_3), _mm256_mul_pd(_t4_2, _t5_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t5_1), _mm256_mul_pd(_t4_0, _t5_0)));

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 28*k2, _t5_4);
      _asm256_storeu_pd(M0 + i0 + 28*k2 + 28, _t5_5);
      _asm256_storeu_pd(M0 + i0 + 28*k2 + 56, _t5_6);
      _asm256_storeu_pd(M0 + i0 + 28*k2 + 84, _t5_7);
    }
    _asm256_storeu_pd(M0 + 28*k2, _t4_16);
    _asm256_storeu_pd(M0 + 28*k2 + 28, _t4_17);
    _asm256_storeu_pd(M0 + 28*k2 + 56, _t4_18);
    _asm256_storeu_pd(M0 + 28*k2 + 84, _t4_19);
  }


  for( int k3 = 4; k3 <= 23; k3+=4 ) {
    _t6_3 = _asm256_loadu_pd(P + 29*k3);
    _t6_2 = _mm256_maskload_pd(P + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t6_1 = _mm256_maskload_pd(P + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t6_0 = _mm256_maskload_pd(P + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t6_4 = _t6_3;
    _t6_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
    _t6_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
    _t6_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

    for( int k2 = 0; k2 <= 27; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t7_19 = _mm256_broadcast_sd(F + 28*k2 + k3);
        _t7_18 = _mm256_broadcast_sd(F + 28*k2 + k3 + 1);
        _t7_17 = _mm256_broadcast_sd(F + 28*k2 + k3 + 2);
        _t7_16 = _mm256_broadcast_sd(F + 28*k2 + k3 + 3);
        _t7_15 = _mm256_broadcast_sd(F + 28*k2 + k3 + 28);
        _t7_14 = _mm256_broadcast_sd(F + 28*k2 + k3 + 29);
        _t7_13 = _mm256_broadcast_sd(F + 28*k2 + k3 + 30);
        _t7_12 = _mm256_broadcast_sd(F + 28*k2 + k3 + 31);
        _t7_11 = _mm256_broadcast_sd(F + 28*k2 + k3 + 56);
        _t7_10 = _mm256_broadcast_sd(F + 28*k2 + k3 + 57);
        _t7_9 = _mm256_broadcast_sd(F + 28*k2 + k3 + 58);
        _t7_8 = _mm256_broadcast_sd(F + 28*k2 + k3 + 59);
        _t7_7 = _mm256_broadcast_sd(F + 28*k2 + k3 + 84);
        _t7_6 = _mm256_broadcast_sd(F + 28*k2 + k3 + 85);
        _t7_5 = _mm256_broadcast_sd(F + 28*k2 + k3 + 86);
        _t7_4 = _mm256_broadcast_sd(F + 28*k2 + k3 + 87);
        _t7_3 = _asm256_loadu_pd(P + 28*i0 + k3);
        _t7_2 = _asm256_loadu_pd(P + 28*i0 + k3 + 28);
        _t7_1 = _asm256_loadu_pd(P + 28*i0 + k3 + 56);
        _t7_0 = _asm256_loadu_pd(P + 28*i0 + k3 + 84);
        _t7_20 = _asm256_loadu_pd(M0 + i0 + 28*k2);
        _t7_21 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 28);
        _t7_22 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 56);
        _t7_23 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t7_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
        _t7_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
        _t7_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
        _t7_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t7_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_28), _mm256_mul_pd(_t7_18, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_30), _mm256_mul_pd(_t7_16, _t7_31)));
        _t7_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_28), _mm256_mul_pd(_t7_14, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_30), _mm256_mul_pd(_t7_12, _t7_31)));
        _t7_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_28), _mm256_mul_pd(_t7_10, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_30), _mm256_mul_pd(_t7_8, _t7_31)));
        _t7_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_28), _mm256_mul_pd(_t7_6, _t7_29)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_30), _mm256_mul_pd(_t7_4, _t7_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t7_20 = _mm256_add_pd(_t7_20, _t7_24);
        _t7_21 = _mm256_add_pd(_t7_21, _t7_25);
        _t7_22 = _mm256_add_pd(_t7_22, _t7_26);
        _t7_23 = _mm256_add_pd(_t7_23, _t7_27);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 28*k2, _t7_20);
        _asm256_storeu_pd(M0 + i0 + 28*k2 + 28, _t7_21);
        _asm256_storeu_pd(M0 + i0 + 28*k2 + 56, _t7_22);
        _asm256_storeu_pd(M0 + i0 + 28*k2 + 84, _t7_23);
      }
      _t8_15 = _mm256_broadcast_sd(F + 28*k2 + k3);
      _t8_14 = _mm256_broadcast_sd(F + 28*k2 + k3 + 1);
      _t8_13 = _mm256_broadcast_sd(F + 28*k2 + k3 + 2);
      _t8_12 = _mm256_broadcast_sd(F + 28*k2 + k3 + 3);
      _t8_11 = _mm256_broadcast_sd(F + 28*k2 + k3 + 28);
      _t8_10 = _mm256_broadcast_sd(F + 28*k2 + k3 + 29);
      _t8_9 = _mm256_broadcast_sd(F + 28*k2 + k3 + 30);
      _t8_8 = _mm256_broadcast_sd(F + 28*k2 + k3 + 31);
      _t8_7 = _mm256_broadcast_sd(F + 28*k2 + k3 + 56);
      _t8_6 = _mm256_broadcast_sd(F + 28*k2 + k3 + 57);
      _t8_5 = _mm256_broadcast_sd(F + 28*k2 + k3 + 58);
      _t8_4 = _mm256_broadcast_sd(F + 28*k2 + k3 + 59);
      _t8_3 = _mm256_broadcast_sd(F + 28*k2 + k3 + 84);
      _t8_2 = _mm256_broadcast_sd(F + 28*k2 + k3 + 85);
      _t8_1 = _mm256_broadcast_sd(F + 28*k2 + k3 + 86);
      _t8_0 = _mm256_broadcast_sd(F + 28*k2 + k3 + 87);
      _t8_16 = _asm256_loadu_pd(M0 + 28*k2 + k3);
      _t8_17 = _asm256_loadu_pd(M0 + 28*k2 + k3 + 28);
      _t8_18 = _asm256_loadu_pd(M0 + 28*k2 + k3 + 56);
      _t8_19 = _asm256_loadu_pd(M0 + 28*k2 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t8_24 = _t6_3;
      _t8_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 3), _t6_2, 12);
      _t8_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 0), _t6_1, 49);
      _t8_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t6_3, _t6_2, 12), _mm256_shuffle_pd(_t6_1, _t6_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t8_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_24), _mm256_mul_pd(_t8_14, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_26), _mm256_mul_pd(_t8_12, _t8_27)));
      _t8_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_24), _mm256_mul_pd(_t8_10, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_26), _mm256_mul_pd(_t8_8, _t8_27)));
      _t8_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_24), _mm256_mul_pd(_t8_6, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_26), _mm256_mul_pd(_t8_4, _t8_27)));
      _t8_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t8_24), _mm256_mul_pd(_t8_2, _t8_25)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t8_26), _mm256_mul_pd(_t8_0, _t8_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_16 = _mm256_add_pd(_t8_16, _t8_20);
      _t8_17 = _mm256_add_pd(_t8_17, _t8_21);
      _t8_18 = _mm256_add_pd(_t8_18, _t8_22);
      _t8_19 = _mm256_add_pd(_t8_19, _t8_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M0 + 28*k2 + k3, _t8_16);
      _asm256_storeu_pd(M0 + 28*k2 + k3 + 28, _t8_17);
      _asm256_storeu_pd(M0 + 28*k2 + k3 + 56, _t8_18);
      _asm256_storeu_pd(M0 + 28*k2 + k3 + 84, _t8_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
        _t9_3 = _asm256_loadu_pd(P + i0 + 28*k3);
        _t9_2 = _asm256_loadu_pd(P + i0 + 28*k3 + 28);
        _t9_1 = _asm256_loadu_pd(P + i0 + 28*k3 + 56);
        _t9_0 = _asm256_loadu_pd(P + i0 + 28*k3 + 84);
        _t9_4 = _asm256_loadu_pd(M0 + i0 + 28*k2);
        _t9_5 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 28);
        _t9_6 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 56);
        _t9_7 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t9_3), _mm256_mul_pd(_t8_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t9_1), _mm256_mul_pd(_t8_12, _t9_0)));
        _t9_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t9_3), _mm256_mul_pd(_t8_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t9_1), _mm256_mul_pd(_t8_8, _t9_0)));
        _t9_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t9_3), _mm256_mul_pd(_t8_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t9_1), _mm256_mul_pd(_t8_4, _t9_0)));
        _t9_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_3, _t9_3), _mm256_mul_pd(_t8_2, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t8_1, _t9_1), _mm256_mul_pd(_t8_0, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t9_4 = _mm256_add_pd(_t9_4, _t9_8);
        _t9_5 = _mm256_add_pd(_t9_5, _t9_9);
        _t9_6 = _mm256_add_pd(_t9_6, _t9_10);
        _t9_7 = _mm256_add_pd(_t9_7, _t9_11);

        // AVX Storer:
        _asm256_storeu_pd(M0 + i0 + 28*k2, _t9_4);
        _asm256_storeu_pd(M0 + i0 + 28*k2 + 28, _t9_5);
        _asm256_storeu_pd(M0 + i0 + 28*k2 + 56, _t9_6);
        _asm256_storeu_pd(M0 + i0 + 28*k2 + 84, _t9_7);
      }
    }
  }

  _t10_3 = _asm256_loadu_pd(P + 696);
  _t10_2 = _mm256_maskload_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t10_1 = _mm256_maskload_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t10_0 = _mm256_maskload_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t10_4 = _t10_3;
  _t10_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
  _t10_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
  _t10_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);


  for( int k2 = 0; k2 <= 27; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 23; i0+=4 ) {
      _t11_19 = _mm256_broadcast_sd(F + 28*k2 + 24);
      _t11_18 = _mm256_broadcast_sd(F + 28*k2 + 25);
      _t11_17 = _mm256_broadcast_sd(F + 28*k2 + 26);
      _t11_16 = _mm256_broadcast_sd(F + 28*k2 + 27);
      _t11_15 = _mm256_broadcast_sd(F + 28*k2 + 52);
      _t11_14 = _mm256_broadcast_sd(F + 28*k2 + 53);
      _t11_13 = _mm256_broadcast_sd(F + 28*k2 + 54);
      _t11_12 = _mm256_broadcast_sd(F + 28*k2 + 55);
      _t11_11 = _mm256_broadcast_sd(F + 28*k2 + 80);
      _t11_10 = _mm256_broadcast_sd(F + 28*k2 + 81);
      _t11_9 = _mm256_broadcast_sd(F + 28*k2 + 82);
      _t11_8 = _mm256_broadcast_sd(F + 28*k2 + 83);
      _t11_7 = _mm256_broadcast_sd(F + 28*k2 + 108);
      _t11_6 = _mm256_broadcast_sd(F + 28*k2 + 109);
      _t11_5 = _mm256_broadcast_sd(F + 28*k2 + 110);
      _t11_4 = _mm256_broadcast_sd(F + 28*k2 + 111);
      _t11_3 = _asm256_loadu_pd(P + 28*i0 + 24);
      _t11_2 = _asm256_loadu_pd(P + 28*i0 + 52);
      _t11_1 = _asm256_loadu_pd(P + 28*i0 + 80);
      _t11_0 = _asm256_loadu_pd(P + 28*i0 + 108);
      _t11_20 = _asm256_loadu_pd(M0 + i0 + 28*k2);
      _t11_21 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 28);
      _t11_22 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 56);
      _t11_23 = _asm256_loadu_pd(M0 + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t11_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
      _t11_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
      _t11_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
      _t11_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t11_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_28), _mm256_mul_pd(_t11_18, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_30), _mm256_mul_pd(_t11_16, _t11_31)));
      _t11_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_28), _mm256_mul_pd(_t11_14, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_30), _mm256_mul_pd(_t11_12, _t11_31)));
      _t11_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_28), _mm256_mul_pd(_t11_10, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_30), _mm256_mul_pd(_t11_8, _t11_31)));
      _t11_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_28), _mm256_mul_pd(_t11_6, _t11_29)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_30), _mm256_mul_pd(_t11_4, _t11_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t11_20 = _mm256_add_pd(_t11_20, _t11_24);
      _t11_21 = _mm256_add_pd(_t11_21, _t11_25);
      _t11_22 = _mm256_add_pd(_t11_22, _t11_26);
      _t11_23 = _mm256_add_pd(_t11_23, _t11_27);

      // AVX Storer:
      _asm256_storeu_pd(M0 + i0 + 28*k2, _t11_20);
      _asm256_storeu_pd(M0 + i0 + 28*k2 + 28, _t11_21);
      _asm256_storeu_pd(M0 + i0 + 28*k2 + 56, _t11_22);
      _asm256_storeu_pd(M0 + i0 + 28*k2 + 84, _t11_23);
    }
    _t12_15 = _mm256_broadcast_sd(F + 28*k2 + 24);
    _t12_14 = _mm256_broadcast_sd(F + 28*k2 + 25);
    _t12_13 = _mm256_broadcast_sd(F + 28*k2 + 26);
    _t12_12 = _mm256_broadcast_sd(F + 28*k2 + 27);
    _t12_11 = _mm256_broadcast_sd(F + 28*k2 + 52);
    _t12_10 = _mm256_broadcast_sd(F + 28*k2 + 53);
    _t12_9 = _mm256_broadcast_sd(F + 28*k2 + 54);
    _t12_8 = _mm256_broadcast_sd(F + 28*k2 + 55);
    _t12_7 = _mm256_broadcast_sd(F + 28*k2 + 80);
    _t12_6 = _mm256_broadcast_sd(F + 28*k2 + 81);
    _t12_5 = _mm256_broadcast_sd(F + 28*k2 + 82);
    _t12_4 = _mm256_broadcast_sd(F + 28*k2 + 83);
    _t12_3 = _mm256_broadcast_sd(F + 28*k2 + 108);
    _t12_2 = _mm256_broadcast_sd(F + 28*k2 + 109);
    _t12_1 = _mm256_broadcast_sd(F + 28*k2 + 110);
    _t12_0 = _mm256_broadcast_sd(F + 28*k2 + 111);
    _t12_16 = _asm256_loadu_pd(M0 + 28*k2 + 24);
    _t12_17 = _asm256_loadu_pd(M0 + 28*k2 + 52);
    _t12_18 = _asm256_loadu_pd(M0 + 28*k2 + 80);
    _t12_19 = _asm256_loadu_pd(M0 + 28*k2 + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t10_4), _mm256_mul_pd(_t12_14, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t10_6), _mm256_mul_pd(_t12_12, _t10_7)));
    _t12_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t10_4), _mm256_mul_pd(_t12_10, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t10_6), _mm256_mul_pd(_t12_8, _t10_7)));
    _t12_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t10_4), _mm256_mul_pd(_t12_6, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t10_6), _mm256_mul_pd(_t12_4, _t10_7)));
    _t12_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_3, _t10_4), _mm256_mul_pd(_t12_2, _t10_5)), _mm256_add_pd(_mm256_mul_pd(_t12_1, _t10_6), _mm256_mul_pd(_t12_0, _t10_7)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_16 = _mm256_add_pd(_t12_16, _t12_20);
    _t12_17 = _mm256_add_pd(_t12_17, _t12_21);
    _t12_18 = _mm256_add_pd(_t12_18, _t12_22);
    _t12_19 = _mm256_add_pd(_t12_19, _t12_23);

    // AVX Storer:
    _asm256_storeu_pd(M0 + 28*k2 + 24, _t12_16);
    _asm256_storeu_pd(M0 + 28*k2 + 52, _t12_17);
    _asm256_storeu_pd(M0 + 28*k2 + 80, _t12_18);
    _asm256_storeu_pd(M0 + 28*k2 + 108, _t12_19);
  }


  // Generating : Y[28,28] = ( ( Sum_{k2} ( ( S(h(4, 28, k2), ( ( G(h(4, 28, k2), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, k2), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, k2), Q[28,28],h(4, 28, k2)) ),h(4, 28, k2)) + Sum_{i0} ( S(h(4, 28, k2), ( ( G(h(4, 28, k2), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, k2), Q[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) + S(h(4, 28, 24), ( ( G(h(4, 28, 24), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, 24), Q[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 28, k2), ( G(h(4, 28, k2), M0[28,28],h(4, 28, k3)) * T( G(h(4, 28, k2), F[28,28],h(4, 28, k3)) ) ),h(4, 28, k2)) + Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), M0[28,28],h(4, 28, k3)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) ) ) + $(h(4, 28, 24), ( G(h(4, 28, 24), M0[28,28],h(4, 28, k3)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, k3)) ) ),h(4, 28, 24)) ) ) )


  for( int k2 = 0; k2 <= 23; k2+=4 ) {
    _t13_23 = _mm256_broadcast_sd(M0 + 28*k2);
    _t13_22 = _mm256_broadcast_sd(M0 + 28*k2 + 1);
    _t13_21 = _mm256_broadcast_sd(M0 + 28*k2 + 2);
    _t13_20 = _mm256_broadcast_sd(M0 + 28*k2 + 3);
    _t13_19 = _mm256_broadcast_sd(M0 + 28*k2 + 28);
    _t13_18 = _mm256_broadcast_sd(M0 + 28*k2 + 29);
    _t13_17 = _mm256_broadcast_sd(M0 + 28*k2 + 30);
    _t13_16 = _mm256_broadcast_sd(M0 + 28*k2 + 31);
    _t13_15 = _mm256_broadcast_sd(M0 + 28*k2 + 56);
    _t13_14 = _mm256_broadcast_sd(M0 + 28*k2 + 57);
    _t13_13 = _mm256_broadcast_sd(M0 + 28*k2 + 58);
    _t13_12 = _mm256_broadcast_sd(M0 + 28*k2 + 59);
    _t13_11 = _mm256_broadcast_sd(M0 + 28*k2 + 84);
    _t13_10 = _mm256_broadcast_sd(M0 + 28*k2 + 85);
    _t13_9 = _mm256_broadcast_sd(M0 + 28*k2 + 86);
    _t13_8 = _mm256_broadcast_sd(M0 + 28*k2 + 87);
    _t13_7 = _asm256_loadu_pd(F + 28*k2);
    _t13_6 = _asm256_loadu_pd(F + 28*k2 + 28);
    _t13_5 = _asm256_loadu_pd(F + 28*k2 + 56);
    _t13_4 = _asm256_loadu_pd(F + 28*k2 + 84);
    _t13_3 = _asm256_loadu_pd(Q + 29*k2);
    _t13_2 = _mm256_maskload_pd(Q + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t13_1 = _mm256_maskload_pd(Q + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t13_0 = _mm256_maskload_pd(Q + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t13_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 32);
    _t13_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 32);
    _t13_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t13_7, _t13_6), _mm256_unpacklo_pd(_t13_5, _t13_4), 49);
    _t13_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t13_7, _t13_6), _mm256_unpackhi_pd(_t13_5, _t13_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t13_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t13_40), _mm256_mul_pd(_t13_22, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t13_42), _mm256_mul_pd(_t13_20, _t13_43)));
    _t13_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_40), _mm256_mul_pd(_t13_18, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_42), _mm256_mul_pd(_t13_16, _t13_43)));
    _t13_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_40), _mm256_mul_pd(_t13_14, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_42), _mm256_mul_pd(_t13_12, _t13_43)));
    _t13_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_40), _mm256_mul_pd(_t13_10, _t13_41)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_42), _mm256_mul_pd(_t13_8, _t13_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t13_36 = _t13_3;
    _t13_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 3), _t13_2, 12);
    _t13_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 0), _t13_1, 49);
    _t13_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t13_3, _t13_2, 12), _mm256_shuffle_pd(_t13_1, _t13_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t13_24 = _mm256_add_pd(_t13_32, _t13_36);
    _t13_25 = _mm256_add_pd(_t13_33, _t13_37);
    _t13_26 = _mm256_add_pd(_t13_34, _t13_38);
    _t13_27 = _mm256_add_pd(_t13_35, _t13_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t13_28 = _t13_24;
    _t13_29 = _t13_25;
    _t13_30 = _t13_26;
    _t13_31 = _t13_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
      _t14_7 = _asm256_loadu_pd(F + 28*i0);
      _t14_6 = _asm256_loadu_pd(F + 28*i0 + 28);
      _t14_5 = _asm256_loadu_pd(F + 28*i0 + 56);
      _t14_4 = _asm256_loadu_pd(F + 28*i0 + 84);
      _t14_3 = _asm256_loadu_pd(Q + i0 + 28*k2);
      _t14_2 = _asm256_loadu_pd(Q + i0 + 28*k2 + 28);
      _t14_1 = _asm256_loadu_pd(Q + i0 + 28*k2 + 56);
      _t14_0 = _asm256_loadu_pd(Q + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t14_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 32);
      _t14_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 32);
      _t14_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 49);
      _t14_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t14_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_23, _t14_16), _mm256_mul_pd(_t13_22, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_21, _t14_18), _mm256_mul_pd(_t13_20, _t14_19)));
      _t14_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t14_16), _mm256_mul_pd(_t13_18, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t14_18), _mm256_mul_pd(_t13_16, _t14_19)));
      _t14_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t14_16), _mm256_mul_pd(_t13_14, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t14_18), _mm256_mul_pd(_t13_12, _t14_19)));
      _t14_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t14_16), _mm256_mul_pd(_t13_10, _t14_17)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t14_18), _mm256_mul_pd(_t13_8, _t14_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t14_8 = _mm256_add_pd(_t14_12, _t14_3);
      _t14_9 = _mm256_add_pd(_t14_13, _t14_2);
      _t14_10 = _mm256_add_pd(_t14_14, _t14_1);
      _t14_11 = _mm256_add_pd(_t14_15, _t14_0);

      // AVX Storer:
      _asm256_storeu_pd(Y + i0 + 28*k2, _t14_8);
      _asm256_storeu_pd(Y + i0 + 28*k2 + 28, _t14_9);
      _asm256_storeu_pd(Y + i0 + 28*k2 + 56, _t14_10);
      _asm256_storeu_pd(Y + i0 + 28*k2 + 84, _t14_11);
    }
    _asm256_storeu_pd(Y + 29*k2, _t13_28);
    _mm256_maskstore_pd(Y + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t13_29);
    _mm256_maskstore_pd(Y + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t13_30);
    _mm256_maskstore_pd(Y + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t13_31);
  }

  _t15_23 = _mm256_broadcast_sd(M0 + 672);
  _t15_22 = _mm256_broadcast_sd(M0 + 673);
  _t15_21 = _mm256_broadcast_sd(M0 + 674);
  _t15_20 = _mm256_broadcast_sd(M0 + 675);
  _t15_19 = _mm256_broadcast_sd(M0 + 700);
  _t15_18 = _mm256_broadcast_sd(M0 + 701);
  _t15_17 = _mm256_broadcast_sd(M0 + 702);
  _t15_16 = _mm256_broadcast_sd(M0 + 703);
  _t15_15 = _mm256_broadcast_sd(M0 + 728);
  _t15_14 = _mm256_broadcast_sd(M0 + 729);
  _t15_13 = _mm256_broadcast_sd(M0 + 730);
  _t15_12 = _mm256_broadcast_sd(M0 + 731);
  _t15_11 = _mm256_broadcast_sd(M0 + 756);
  _t15_10 = _mm256_broadcast_sd(M0 + 757);
  _t15_9 = _mm256_broadcast_sd(M0 + 758);
  _t15_8 = _mm256_broadcast_sd(M0 + 759);
  _t15_7 = _asm256_loadu_pd(F + 672);
  _t15_6 = _asm256_loadu_pd(F + 700);
  _t15_5 = _asm256_loadu_pd(F + 728);
  _t15_4 = _asm256_loadu_pd(F + 756);
  _t15_3 = _asm256_loadu_pd(Q + 696);
  _t15_2 = _mm256_maskload_pd(Q + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t15_1 = _mm256_maskload_pd(Q + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t15_0 = _mm256_maskload_pd(Q + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
  _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
  _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
  _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
  _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
  _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
  _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t15_36 = _t15_3;
  _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
  _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
  _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
  _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
  _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
  _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t15_28 = _t15_24;
  _t15_29 = _t15_25;
  _t15_30 = _t15_26;
  _t15_31 = _t15_27;


  for( int k3 = 4; k3 <= 27; k3+=4 ) {

    for( int k2 = 0; k2 <= 23; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 28*k2 + k3);
      _t16_18 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 28);
      _t16_14 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 29);
      _t16_13 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 30);
      _t16_12 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 31);
      _t16_11 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 56);
      _t16_10 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 57);
      _t16_9 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 58);
      _t16_8 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 59);
      _t16_7 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 84);
      _t16_6 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 85);
      _t16_5 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 86);
      _t16_4 = _mm256_broadcast_sd(M0 + 28*k2 + k3 + 87);
      _t16_3 = _asm256_loadu_pd(F + 28*k2 + k3);
      _t16_2 = _asm256_loadu_pd(F + 28*k2 + k3 + 28);
      _t16_1 = _asm256_loadu_pd(F + 28*k2 + k3 + 56);
      _t16_0 = _asm256_loadu_pd(F + 28*k2 + k3 + 84);
      _t16_20 = _asm256_loadu_pd(Y + 29*k2);
      _t16_21 = _mm256_maskload_pd(Y + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t16_22 = _mm256_maskload_pd(Y + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t16_23 = _mm256_maskload_pd(Y + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_32), _mm256_mul_pd(_t16_18, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_34), _mm256_mul_pd(_t16_16, _t16_35)));
      _t16_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_32), _mm256_mul_pd(_t16_14, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_34), _mm256_mul_pd(_t16_12, _t16_35)));
      _t16_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_32), _mm256_mul_pd(_t16_10, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_34), _mm256_mul_pd(_t16_8, _t16_35)));
      _t16_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_32), _mm256_mul_pd(_t16_6, _t16_33)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_34), _mm256_mul_pd(_t16_4, _t16_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_28 = _t16_20;
      _t16_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 3), _t16_21, 12);
      _t16_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 0), _t16_22, 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t16_20, _t16_21, 12), _mm256_shuffle_pd(_t16_22, _t16_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_28 = _mm256_add_pd(_t16_28, _t16_24);
      _t16_29 = _mm256_add_pd(_t16_29, _t16_25);
      _t16_30 = _mm256_add_pd(_t16_30, _t16_26);
      _t16_31 = _mm256_add_pd(_t16_31, _t16_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t16_20 = _t16_28;
      _t16_21 = _t16_29;
      _t16_22 = _t16_30;
      _t16_23 = _t16_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
        _t17_3 = _asm256_loadu_pd(F + 28*i0 + k3);
        _t17_2 = _asm256_loadu_pd(F + 28*i0 + k3 + 28);
        _t17_1 = _asm256_loadu_pd(F + 28*i0 + k3 + 56);
        _t17_0 = _asm256_loadu_pd(F + 28*i0 + k3 + 84);
        _t17_4 = _asm256_loadu_pd(Y + i0 + 28*k2);
        _t17_5 = _asm256_loadu_pd(Y + i0 + 28*k2 + 28);
        _t17_6 = _asm256_loadu_pd(Y + i0 + 28*k2 + 56);
        _t17_7 = _asm256_loadu_pd(Y + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t17_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
        _t17_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
        _t17_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
        _t17_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t17_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t17_12), _mm256_mul_pd(_t16_18, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t17_14), _mm256_mul_pd(_t16_16, _t17_15)));
        _t17_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t17_12), _mm256_mul_pd(_t16_14, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t17_14), _mm256_mul_pd(_t16_12, _t17_15)));
        _t17_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t17_12), _mm256_mul_pd(_t16_10, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t17_14), _mm256_mul_pd(_t16_8, _t17_15)));
        _t17_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t17_12), _mm256_mul_pd(_t16_6, _t17_13)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t17_14), _mm256_mul_pd(_t16_4, _t17_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_4 = _mm256_add_pd(_t17_4, _t17_8);
        _t17_5 = _mm256_add_pd(_t17_5, _t17_9);
        _t17_6 = _mm256_add_pd(_t17_6, _t17_10);
        _t17_7 = _mm256_add_pd(_t17_7, _t17_11);

        // AVX Storer:
        _asm256_storeu_pd(Y + i0 + 28*k2, _t17_4);
        _asm256_storeu_pd(Y + i0 + 28*k2 + 28, _t17_5);
        _asm256_storeu_pd(Y + i0 + 28*k2 + 56, _t17_6);
        _asm256_storeu_pd(Y + i0 + 28*k2 + 84, _t17_7);
      }
      _asm256_storeu_pd(Y + 29*k2, _t16_20);
      _mm256_maskstore_pd(Y + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t16_21);
      _mm256_maskstore_pd(Y + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t16_22);
      _mm256_maskstore_pd(Y + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t16_23);
    }
    _t18_19 = _mm256_broadcast_sd(M0 + k3 + 672);
    _t18_18 = _mm256_broadcast_sd(M0 + k3 + 673);
    _t18_17 = _mm256_broadcast_sd(M0 + k3 + 674);
    _t18_16 = _mm256_broadcast_sd(M0 + k3 + 675);
    _t18_15 = _mm256_broadcast_sd(M0 + k3 + 700);
    _t18_14 = _mm256_broadcast_sd(M0 + k3 + 701);
    _t18_13 = _mm256_broadcast_sd(M0 + k3 + 702);
    _t18_12 = _mm256_broadcast_sd(M0 + k3 + 703);
    _t18_11 = _mm256_broadcast_sd(M0 + k3 + 728);
    _t18_10 = _mm256_broadcast_sd(M0 + k3 + 729);
    _t18_9 = _mm256_broadcast_sd(M0 + k3 + 730);
    _t18_8 = _mm256_broadcast_sd(M0 + k3 + 731);
    _t18_7 = _mm256_broadcast_sd(M0 + k3 + 756);
    _t18_6 = _mm256_broadcast_sd(M0 + k3 + 757);
    _t18_5 = _mm256_broadcast_sd(M0 + k3 + 758);
    _t18_4 = _mm256_broadcast_sd(M0 + k3 + 759);
    _t18_3 = _asm256_loadu_pd(F + k3 + 672);
    _t18_2 = _asm256_loadu_pd(F + k3 + 700);
    _t18_1 = _asm256_loadu_pd(F + k3 + 728);
    _t18_0 = _asm256_loadu_pd(F + k3 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t18_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
    _t18_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
    _t18_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
    _t18_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_28), _mm256_mul_pd(_t18_18, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_30), _mm256_mul_pd(_t18_16, _t18_31)));
    _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_28), _mm256_mul_pd(_t18_14, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_30), _mm256_mul_pd(_t18_12, _t18_31)));
    _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_28), _mm256_mul_pd(_t18_10, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_30), _mm256_mul_pd(_t18_8, _t18_31)));
    _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_28), _mm256_mul_pd(_t18_6, _t18_29)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_30), _mm256_mul_pd(_t18_4, _t18_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t18_24 = _t15_28;
    _t18_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
    _t18_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
    _t18_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t18_24 = _mm256_add_pd(_t18_24, _t18_20);
    _t18_25 = _mm256_add_pd(_t18_25, _t18_21);
    _t18_26 = _mm256_add_pd(_t18_26, _t18_22);
    _t18_27 = _mm256_add_pd(_t18_27, _t18_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t18_24;
    _t15_29 = _t18_25;
    _t15_30 = _t18_26;
    _t15_31 = _t18_27;
  }


  // Generating : v0[28,1] = ( Sum_{k2} ( S(h(4, 28, k2), ( G(h(4, 28, k2), z[28,1],h(1, 1, 0)) - ( G(h(4, 28, k2), H[28,28],h(4, 28, 0)) * G(h(4, 28, 0), y[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, k3)) * G(h(4, 28, k3), y[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int k2 = 0; k2 <= 27; k2+=4 ) {
    _t19_5 = _asm256_loadu_pd(z + k2);
    _t19_4 = _asm256_loadu_pd(H + 28*k2);
    _t19_3 = _asm256_loadu_pd(H + 28*k2 + 28);
    _t19_2 = _asm256_loadu_pd(H + 28*k2 + 56);
    _t19_1 = _asm256_loadu_pd(H + 28*k2 + 84);
    _t19_0 = _asm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t19_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t19_4, _t19_0), _mm256_mul_pd(_t19_3, _t19_0)), _mm256_hadd_pd(_mm256_mul_pd(_t19_2, _t19_0), _mm256_mul_pd(_t19_1, _t19_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t19_7 = _mm256_sub_pd(_t19_5, _t19_6);

    // AVX Storer:
    _asm256_storeu_pd(v0 + k2, _t19_7);
  }


  for( int k3 = 4; k3 <= 27; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 27; k2+=4 ) {
      _t20_4 = _asm256_loadu_pd(H + 28*k2 + k3);
      _t20_3 = _asm256_loadu_pd(H + 28*k2 + k3 + 28);
      _t20_2 = _asm256_loadu_pd(H + 28*k2 + k3 + 56);
      _t20_1 = _asm256_loadu_pd(H + 28*k2 + k3 + 84);
      _t20_0 = _asm256_loadu_pd(y + k3);
      _t20_5 = _asm256_loadu_pd(v0 + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t20_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t20_5 = _mm256_sub_pd(_t20_5, _t20_6);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k2, _t20_5);
    }
  }

  _t21_3 = _asm256_loadu_pd(Y);
  _t21_2 = _mm256_maskload_pd(Y + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t21_1 = _mm256_maskload_pd(Y + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t21_0 = _mm256_maskload_pd(Y + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[28,28] = ( ( Sum_{k2} ( ( S(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{i0} ( S(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) + Sum_{k3} ( Sum_{k2} ( ( ( Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, k3)) * T( G(h(4, 28, i0), Y[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) + $(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, k3)) * G(h(4, 28, k3), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, k3)) * G(h(4, 28, k3), Y[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) ) ) + Sum_{k2} ( ( Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, 24)) * T( G(h(4, 28, i0), Y[28,28],h(4, 28, 24)) ) ),h(4, 28, i0)) ) + $(h(4, 28, k2), ( G(h(4, 28, k2), H[28,28],h(4, 28, 24)) * G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t21_4 = _t21_3;
  _t21_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t21_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t21_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int k2 = 0; k2 <= 27; k2+=4 ) {
    _t22_15 = _mm256_broadcast_sd(H + 28*k2);
    _t22_14 = _mm256_broadcast_sd(H + 28*k2 + 1);
    _t22_13 = _mm256_broadcast_sd(H + 28*k2 + 2);
    _t22_12 = _mm256_broadcast_sd(H + 28*k2 + 3);
    _t22_11 = _mm256_broadcast_sd(H + 28*k2 + 28);
    _t22_10 = _mm256_broadcast_sd(H + 28*k2 + 29);
    _t22_9 = _mm256_broadcast_sd(H + 28*k2 + 30);
    _t22_8 = _mm256_broadcast_sd(H + 28*k2 + 31);
    _t22_7 = _mm256_broadcast_sd(H + 28*k2 + 56);
    _t22_6 = _mm256_broadcast_sd(H + 28*k2 + 57);
    _t22_5 = _mm256_broadcast_sd(H + 28*k2 + 58);
    _t22_4 = _mm256_broadcast_sd(H + 28*k2 + 59);
    _t22_3 = _mm256_broadcast_sd(H + 28*k2 + 84);
    _t22_2 = _mm256_broadcast_sd(H + 28*k2 + 85);
    _t22_1 = _mm256_broadcast_sd(H + 28*k2 + 86);
    _t22_0 = _mm256_broadcast_sd(H + 28*k2 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t22_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t21_4), _mm256_mul_pd(_t22_14, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t21_6), _mm256_mul_pd(_t22_12, _t21_7)));
    _t22_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t21_4), _mm256_mul_pd(_t22_10, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t21_6), _mm256_mul_pd(_t22_8, _t21_7)));
    _t22_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t21_4), _mm256_mul_pd(_t22_6, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t21_6), _mm256_mul_pd(_t22_4, _t21_7)));
    _t22_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t21_4), _mm256_mul_pd(_t22_2, _t21_5)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t21_6), _mm256_mul_pd(_t22_0, _t21_7)));

    // AVX Storer:

    // AVX Loader:

    for( int i0 = 4; i0 <= 27; i0+=4 ) {
      _t23_3 = _asm256_loadu_pd(Y + i0);
      _t23_2 = _asm256_loadu_pd(Y + i0 + 28);
      _t23_1 = _asm256_loadu_pd(Y + i0 + 56);
      _t23_0 = _asm256_loadu_pd(Y + i0 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t23_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t23_3), _mm256_mul_pd(_t22_14, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t23_1), _mm256_mul_pd(_t22_12, _t23_0)));
      _t23_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t23_3), _mm256_mul_pd(_t22_10, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t23_1), _mm256_mul_pd(_t22_8, _t23_0)));
      _t23_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t23_3), _mm256_mul_pd(_t22_6, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t23_1), _mm256_mul_pd(_t22_4, _t23_0)));
      _t23_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_3, _t23_3), _mm256_mul_pd(_t22_2, _t23_2)), _mm256_add_pd(_mm256_mul_pd(_t22_1, _t23_1), _mm256_mul_pd(_t22_0, _t23_0)));

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 28*k2, _t23_4);
      _asm256_storeu_pd(M1 + i0 + 28*k2 + 28, _t23_5);
      _asm256_storeu_pd(M1 + i0 + 28*k2 + 56, _t23_6);
      _asm256_storeu_pd(M1 + i0 + 28*k2 + 84, _t23_7);
    }
    _asm256_storeu_pd(M1 + 28*k2, _t22_16);
    _asm256_storeu_pd(M1 + 28*k2 + 28, _t22_17);
    _asm256_storeu_pd(M1 + 28*k2 + 56, _t22_18);
    _asm256_storeu_pd(M1 + 28*k2 + 84, _t22_19);
  }


  for( int k3 = 4; k3 <= 23; k3+=4 ) {
    _t24_3 = _asm256_loadu_pd(Y + 29*k3);
    _t24_2 = _mm256_maskload_pd(Y + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t24_1 = _mm256_maskload_pd(Y + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t24_0 = _mm256_maskload_pd(Y + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t24_4 = _t24_3;
    _t24_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
    _t24_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
    _t24_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

    for( int k2 = 0; k2 <= 27; k2+=4 ) {

      // AVX Loader:

      for( int i0 = 0; i0 <= k3 - 1; i0+=4 ) {
        _t25_19 = _mm256_broadcast_sd(H + 28*k2 + k3);
        _t25_18 = _mm256_broadcast_sd(H + 28*k2 + k3 + 1);
        _t25_17 = _mm256_broadcast_sd(H + 28*k2 + k3 + 2);
        _t25_16 = _mm256_broadcast_sd(H + 28*k2 + k3 + 3);
        _t25_15 = _mm256_broadcast_sd(H + 28*k2 + k3 + 28);
        _t25_14 = _mm256_broadcast_sd(H + 28*k2 + k3 + 29);
        _t25_13 = _mm256_broadcast_sd(H + 28*k2 + k3 + 30);
        _t25_12 = _mm256_broadcast_sd(H + 28*k2 + k3 + 31);
        _t25_11 = _mm256_broadcast_sd(H + 28*k2 + k3 + 56);
        _t25_10 = _mm256_broadcast_sd(H + 28*k2 + k3 + 57);
        _t25_9 = _mm256_broadcast_sd(H + 28*k2 + k3 + 58);
        _t25_8 = _mm256_broadcast_sd(H + 28*k2 + k3 + 59);
        _t25_7 = _mm256_broadcast_sd(H + 28*k2 + k3 + 84);
        _t25_6 = _mm256_broadcast_sd(H + 28*k2 + k3 + 85);
        _t25_5 = _mm256_broadcast_sd(H + 28*k2 + k3 + 86);
        _t25_4 = _mm256_broadcast_sd(H + 28*k2 + k3 + 87);
        _t25_3 = _asm256_loadu_pd(Y + 28*i0 + k3);
        _t25_2 = _asm256_loadu_pd(Y + 28*i0 + k3 + 28);
        _t25_1 = _asm256_loadu_pd(Y + 28*i0 + k3 + 56);
        _t25_0 = _asm256_loadu_pd(Y + 28*i0 + k3 + 84);
        _t25_20 = _asm256_loadu_pd(M1 + i0 + 28*k2);
        _t25_21 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 28);
        _t25_22 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 56);
        _t25_23 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t25_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
        _t25_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
        _t25_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
        _t25_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t25_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_28), _mm256_mul_pd(_t25_18, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_30), _mm256_mul_pd(_t25_16, _t25_31)));
        _t25_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_28), _mm256_mul_pd(_t25_14, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_30), _mm256_mul_pd(_t25_12, _t25_31)));
        _t25_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_28), _mm256_mul_pd(_t25_10, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_30), _mm256_mul_pd(_t25_8, _t25_31)));
        _t25_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_28), _mm256_mul_pd(_t25_6, _t25_29)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_30), _mm256_mul_pd(_t25_4, _t25_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t25_20 = _mm256_add_pd(_t25_20, _t25_24);
        _t25_21 = _mm256_add_pd(_t25_21, _t25_25);
        _t25_22 = _mm256_add_pd(_t25_22, _t25_26);
        _t25_23 = _mm256_add_pd(_t25_23, _t25_27);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 28*k2, _t25_20);
        _asm256_storeu_pd(M1 + i0 + 28*k2 + 28, _t25_21);
        _asm256_storeu_pd(M1 + i0 + 28*k2 + 56, _t25_22);
        _asm256_storeu_pd(M1 + i0 + 28*k2 + 84, _t25_23);
      }
      _t26_15 = _mm256_broadcast_sd(H + 28*k2 + k3);
      _t26_14 = _mm256_broadcast_sd(H + 28*k2 + k3 + 1);
      _t26_13 = _mm256_broadcast_sd(H + 28*k2 + k3 + 2);
      _t26_12 = _mm256_broadcast_sd(H + 28*k2 + k3 + 3);
      _t26_11 = _mm256_broadcast_sd(H + 28*k2 + k3 + 28);
      _t26_10 = _mm256_broadcast_sd(H + 28*k2 + k3 + 29);
      _t26_9 = _mm256_broadcast_sd(H + 28*k2 + k3 + 30);
      _t26_8 = _mm256_broadcast_sd(H + 28*k2 + k3 + 31);
      _t26_7 = _mm256_broadcast_sd(H + 28*k2 + k3 + 56);
      _t26_6 = _mm256_broadcast_sd(H + 28*k2 + k3 + 57);
      _t26_5 = _mm256_broadcast_sd(H + 28*k2 + k3 + 58);
      _t26_4 = _mm256_broadcast_sd(H + 28*k2 + k3 + 59);
      _t26_3 = _mm256_broadcast_sd(H + 28*k2 + k3 + 84);
      _t26_2 = _mm256_broadcast_sd(H + 28*k2 + k3 + 85);
      _t26_1 = _mm256_broadcast_sd(H + 28*k2 + k3 + 86);
      _t26_0 = _mm256_broadcast_sd(H + 28*k2 + k3 + 87);
      _t26_16 = _asm256_loadu_pd(M1 + 28*k2 + k3);
      _t26_17 = _asm256_loadu_pd(M1 + 28*k2 + k3 + 28);
      _t26_18 = _asm256_loadu_pd(M1 + 28*k2 + k3 + 56);
      _t26_19 = _asm256_loadu_pd(M1 + 28*k2 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t26_24 = _t24_3;
      _t26_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
      _t26_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
      _t26_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t26_24), _mm256_mul_pd(_t26_14, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t26_26), _mm256_mul_pd(_t26_12, _t26_27)));
      _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t26_24), _mm256_mul_pd(_t26_10, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t26_26), _mm256_mul_pd(_t26_8, _t26_27)));
      _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t26_24), _mm256_mul_pd(_t26_6, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t26_26), _mm256_mul_pd(_t26_4, _t26_27)));
      _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t26_24), _mm256_mul_pd(_t26_2, _t26_25)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t26_26), _mm256_mul_pd(_t26_0, _t26_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_16 = _mm256_add_pd(_t26_16, _t26_20);
      _t26_17 = _mm256_add_pd(_t26_17, _t26_21);
      _t26_18 = _mm256_add_pd(_t26_18, _t26_22);
      _t26_19 = _mm256_add_pd(_t26_19, _t26_23);

      // AVX Storer:

      // AVX Loader:
      _asm256_storeu_pd(M1 + 28*k2 + k3, _t26_16);
      _asm256_storeu_pd(M1 + 28*k2 + k3 + 28, _t26_17);
      _asm256_storeu_pd(M1 + 28*k2 + k3 + 56, _t26_18);
      _asm256_storeu_pd(M1 + 28*k2 + k3 + 84, _t26_19);

      for( int i0 = 4*floord(k3 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
        _t27_3 = _asm256_loadu_pd(Y + i0 + 28*k3);
        _t27_2 = _asm256_loadu_pd(Y + i0 + 28*k3 + 28);
        _t27_1 = _asm256_loadu_pd(Y + i0 + 28*k3 + 56);
        _t27_0 = _asm256_loadu_pd(Y + i0 + 28*k3 + 84);
        _t27_4 = _asm256_loadu_pd(M1 + i0 + 28*k2);
        _t27_5 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 28);
        _t27_6 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 56);
        _t27_7 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t27_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t27_3), _mm256_mul_pd(_t26_14, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t27_1), _mm256_mul_pd(_t26_12, _t27_0)));
        _t27_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t27_3), _mm256_mul_pd(_t26_10, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t27_1), _mm256_mul_pd(_t26_8, _t27_0)));
        _t27_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t27_3), _mm256_mul_pd(_t26_6, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t27_1), _mm256_mul_pd(_t26_4, _t27_0)));
        _t27_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t27_3), _mm256_mul_pd(_t26_2, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t27_1), _mm256_mul_pd(_t26_0, _t27_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t27_4 = _mm256_add_pd(_t27_4, _t27_8);
        _t27_5 = _mm256_add_pd(_t27_5, _t27_9);
        _t27_6 = _mm256_add_pd(_t27_6, _t27_10);
        _t27_7 = _mm256_add_pd(_t27_7, _t27_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + i0 + 28*k2, _t27_4);
        _asm256_storeu_pd(M1 + i0 + 28*k2 + 28, _t27_5);
        _asm256_storeu_pd(M1 + i0 + 28*k2 + 56, _t27_6);
        _asm256_storeu_pd(M1 + i0 + 28*k2 + 84, _t27_7);
      }
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t28_0 = _t15_28;
  _t28_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t28_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t28_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int k2 = 0; k2 <= 27; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 23; i0+=4 ) {
      _t29_19 = _mm256_broadcast_sd(H + 28*k2 + 24);
      _t29_18 = _mm256_broadcast_sd(H + 28*k2 + 25);
      _t29_17 = _mm256_broadcast_sd(H + 28*k2 + 26);
      _t29_16 = _mm256_broadcast_sd(H + 28*k2 + 27);
      _t29_15 = _mm256_broadcast_sd(H + 28*k2 + 52);
      _t29_14 = _mm256_broadcast_sd(H + 28*k2 + 53);
      _t29_13 = _mm256_broadcast_sd(H + 28*k2 + 54);
      _t29_12 = _mm256_broadcast_sd(H + 28*k2 + 55);
      _t29_11 = _mm256_broadcast_sd(H + 28*k2 + 80);
      _t29_10 = _mm256_broadcast_sd(H + 28*k2 + 81);
      _t29_9 = _mm256_broadcast_sd(H + 28*k2 + 82);
      _t29_8 = _mm256_broadcast_sd(H + 28*k2 + 83);
      _t29_7 = _mm256_broadcast_sd(H + 28*k2 + 108);
      _t29_6 = _mm256_broadcast_sd(H + 28*k2 + 109);
      _t29_5 = _mm256_broadcast_sd(H + 28*k2 + 110);
      _t29_4 = _mm256_broadcast_sd(H + 28*k2 + 111);
      _t29_3 = _asm256_loadu_pd(Y + 28*i0 + 24);
      _t29_2 = _asm256_loadu_pd(Y + 28*i0 + 52);
      _t29_1 = _asm256_loadu_pd(Y + 28*i0 + 80);
      _t29_0 = _asm256_loadu_pd(Y + 28*i0 + 108);
      _t29_20 = _asm256_loadu_pd(M1 + i0 + 28*k2);
      _t29_21 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 28);
      _t29_22 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 56);
      _t29_23 = _asm256_loadu_pd(M1 + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
      _t29_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
      _t29_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
      _t29_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t29_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_28), _mm256_mul_pd(_t29_18, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_30), _mm256_mul_pd(_t29_16, _t29_31)));
      _t29_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_28), _mm256_mul_pd(_t29_14, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_30), _mm256_mul_pd(_t29_12, _t29_31)));
      _t29_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_28), _mm256_mul_pd(_t29_10, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_30), _mm256_mul_pd(_t29_8, _t29_31)));
      _t29_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_28), _mm256_mul_pd(_t29_6, _t29_29)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_30), _mm256_mul_pd(_t29_4, _t29_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t29_20 = _mm256_add_pd(_t29_20, _t29_24);
      _t29_21 = _mm256_add_pd(_t29_21, _t29_25);
      _t29_22 = _mm256_add_pd(_t29_22, _t29_26);
      _t29_23 = _mm256_add_pd(_t29_23, _t29_27);

      // AVX Storer:
      _asm256_storeu_pd(M1 + i0 + 28*k2, _t29_20);
      _asm256_storeu_pd(M1 + i0 + 28*k2 + 28, _t29_21);
      _asm256_storeu_pd(M1 + i0 + 28*k2 + 56, _t29_22);
      _asm256_storeu_pd(M1 + i0 + 28*k2 + 84, _t29_23);
    }
    _t30_15 = _mm256_broadcast_sd(H + 28*k2 + 24);
    _t30_14 = _mm256_broadcast_sd(H + 28*k2 + 25);
    _t30_13 = _mm256_broadcast_sd(H + 28*k2 + 26);
    _t30_12 = _mm256_broadcast_sd(H + 28*k2 + 27);
    _t30_11 = _mm256_broadcast_sd(H + 28*k2 + 52);
    _t30_10 = _mm256_broadcast_sd(H + 28*k2 + 53);
    _t30_9 = _mm256_broadcast_sd(H + 28*k2 + 54);
    _t30_8 = _mm256_broadcast_sd(H + 28*k2 + 55);
    _t30_7 = _mm256_broadcast_sd(H + 28*k2 + 80);
    _t30_6 = _mm256_broadcast_sd(H + 28*k2 + 81);
    _t30_5 = _mm256_broadcast_sd(H + 28*k2 + 82);
    _t30_4 = _mm256_broadcast_sd(H + 28*k2 + 83);
    _t30_3 = _mm256_broadcast_sd(H + 28*k2 + 108);
    _t30_2 = _mm256_broadcast_sd(H + 28*k2 + 109);
    _t30_1 = _mm256_broadcast_sd(H + 28*k2 + 110);
    _t30_0 = _mm256_broadcast_sd(H + 28*k2 + 111);
    _t30_16 = _asm256_loadu_pd(M1 + 28*k2 + 24);
    _t30_17 = _asm256_loadu_pd(M1 + 28*k2 + 52);
    _t30_18 = _asm256_loadu_pd(M1 + 28*k2 + 80);
    _t30_19 = _asm256_loadu_pd(M1 + 28*k2 + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t28_0), _mm256_mul_pd(_t30_14, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t28_2), _mm256_mul_pd(_t30_12, _t28_3)));
    _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t28_0), _mm256_mul_pd(_t30_10, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t28_2), _mm256_mul_pd(_t30_8, _t28_3)));
    _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t28_0), _mm256_mul_pd(_t30_6, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t28_2), _mm256_mul_pd(_t30_4, _t28_3)));
    _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_3, _t28_0), _mm256_mul_pd(_t30_2, _t28_1)), _mm256_add_pd(_mm256_mul_pd(_t30_1, _t28_2), _mm256_mul_pd(_t30_0, _t28_3)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t30_16 = _mm256_add_pd(_t30_16, _t30_20);
    _t30_17 = _mm256_add_pd(_t30_17, _t30_21);
    _t30_18 = _mm256_add_pd(_t30_18, _t30_22);
    _t30_19 = _mm256_add_pd(_t30_19, _t30_23);

    // AVX Storer:
    _asm256_storeu_pd(M1 + 28*k2 + 24, _t30_16);
    _asm256_storeu_pd(M1 + 28*k2 + 52, _t30_17);
    _asm256_storeu_pd(M1 + 28*k2 + 80, _t30_18);
    _asm256_storeu_pd(M1 + 28*k2 + 108, _t30_19);
  }


  // Generating : M2[28,28] = ( ( ( ( Sum_{i0} ( S(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, 0)) ) ),h(4, 28, i0)) ) + Sum_{k2} ( Sum_{i0} ( S(h(4, 28, k2), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) ) * T( G(h(4, 28, i0), H[28,28],h(4, 28, 0)) ) ),h(4, 28, i0)) ) ) ) + Sum_{k3} ( ( ( Sum_{k2} ( Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), Y[28,28],h(4, 28, k3)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) ) + Sum_{i0} ( $(h(4, 28, k3), ( G(h(4, 28, k3), Y[28,28],h(4, 28, k3)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 28, k2), ( T( G(h(4, 28, k3), Y[28,28],h(4, 28, k2)) ) * T( G(h(4, 28, i0), H[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) ) ) ) ) + Sum_{k2} ( Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, 24)) ) ),h(4, 28, i0)) ) ) ) + Sum_{i0} ( $(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, 24)) ) ),h(4, 28, i0)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t31_0 = _t21_3;
  _t31_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 3), _t21_2, 12);
  _t31_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 0), _t21_1, 49);
  _t31_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t21_3, _t21_2, 12), _mm256_shuffle_pd(_t21_1, _t21_0, 12), 49);


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t32_3 = _asm256_loadu_pd(H + 28*i0);
    _t32_2 = _asm256_loadu_pd(H + 28*i0 + 28);
    _t32_1 = _asm256_loadu_pd(H + 28*i0 + 56);
    _t32_0 = _asm256_loadu_pd(H + 28*i0 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t32_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 32);
    _t32_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 32);
    _t32_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t32_3, _t32_2), _mm256_unpacklo_pd(_t32_1, _t32_0), 49);
    _t32_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t32_3, _t32_2), _mm256_unpackhi_pd(_t32_1, _t32_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 32), _mm256_permute2f128_pd(_t31_0, _t31_0, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_0, _t31_0, 49), _mm256_permute2f128_pd(_t31_0, _t31_0, 49), 15), _t32_11)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 32), _mm256_permute2f128_pd(_t31_1, _t31_1, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_1, _t31_1, 49), _mm256_permute2f128_pd(_t31_1, _t31_1, 49), 15), _t32_11)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 32), _mm256_permute2f128_pd(_t31_2, _t31_2, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_2, _t31_2, 49), _mm256_permute2f128_pd(_t31_2, _t31_2, 49), 15), _t32_11)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 0), _t32_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 32), _mm256_permute2f128_pd(_t31_3, _t31_3, 32), 15), _t32_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 0), _t32_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t31_3, _t31_3, 49), _mm256_permute2f128_pd(_t31_3, _t31_3, 49), 15), _t32_11)));

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0, _t32_4);
    _asm256_storeu_pd(M2 + i0 + 28, _t32_5);
    _asm256_storeu_pd(M2 + i0 + 56, _t32_6);
    _asm256_storeu_pd(M2 + i0 + 84, _t32_7);
  }


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t33_3 = _asm256_loadu_pd(Y + k2);
    _t33_2 = _asm256_loadu_pd(Y + k2 + 28);
    _t33_1 = _asm256_loadu_pd(Y + k2 + 56);
    _t33_0 = _asm256_loadu_pd(Y + k2 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    for( int i0 = 0; i0 <= 27; i0+=4 ) {
      _t34_3 = _asm256_loadu_pd(H + 28*i0);
      _t34_2 = _asm256_loadu_pd(H + 28*i0 + 28);
      _t34_1 = _asm256_loadu_pd(H + 28*i0 + 56);
      _t34_0 = _asm256_loadu_pd(H + 28*i0 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t33_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
      _t33_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
      _t33_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
      _t33_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t34_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
      _t34_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
      _t34_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
      _t34_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t34_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 32), _mm256_permute2f128_pd(_t33_4, _t33_4, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_4, _t33_4, 49), _mm256_permute2f128_pd(_t33_4, _t33_4, 49), 15), _t34_11)));
      _t34_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 32), _mm256_permute2f128_pd(_t33_5, _t33_5, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_5, _t33_5, 49), _mm256_permute2f128_pd(_t33_5, _t33_5, 49), 15), _t34_11)));
      _t34_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 32), _mm256_permute2f128_pd(_t33_6, _t33_6, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_6, _t33_6, 49), _mm256_permute2f128_pd(_t33_6, _t33_6, 49), 15), _t34_11)));
      _t34_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 0), _t34_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 32), _mm256_permute2f128_pd(_t33_7, _t33_7, 32), 15), _t34_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 0), _t34_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t33_7, _t33_7, 49), _mm256_permute2f128_pd(_t33_7, _t33_7, 49), 15), _t34_11)));

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 28*k2, _t34_4);
      _asm256_storeu_pd(M2 + i0 + 28*k2 + 28, _t34_5);
      _asm256_storeu_pd(M2 + i0 + 28*k2 + 56, _t34_6);
      _asm256_storeu_pd(M2 + i0 + 28*k2 + 84, _t34_7);
    }
  }


  for( int k3 = 4; k3 <= 23; k3+=4 ) {

    for( int k2 = 0; k2 <= k3 - 1; k2+=4 ) {

      for( int i0 = 0; i0 <= 27; i0+=4 ) {
        _t35_19 = _mm256_broadcast_sd(Y + 28*k2 + k3);
        _t35_18 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 1);
        _t35_17 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 2);
        _t35_16 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 3);
        _t35_15 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 28);
        _t35_14 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 29);
        _t35_13 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 30);
        _t35_12 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 31);
        _t35_11 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 56);
        _t35_10 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 57);
        _t35_9 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 58);
        _t35_8 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 59);
        _t35_7 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 84);
        _t35_6 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 85);
        _t35_5 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 86);
        _t35_4 = _mm256_broadcast_sd(Y + 28*k2 + k3 + 87);
        _t35_3 = _asm256_loadu_pd(H + 28*i0 + k3);
        _t35_2 = _asm256_loadu_pd(H + 28*i0 + k3 + 28);
        _t35_1 = _asm256_loadu_pd(H + 28*i0 + k3 + 56);
        _t35_0 = _asm256_loadu_pd(H + 28*i0 + k3 + 84);
        _t35_20 = _asm256_loadu_pd(M2 + i0 + 28*k2);
        _t35_21 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 28);
        _t35_22 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 56);
        _t35_23 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t35_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
        _t35_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
        _t35_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
        _t35_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t35_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_28), _mm256_mul_pd(_t35_18, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_30), _mm256_mul_pd(_t35_16, _t35_31)));
        _t35_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_28), _mm256_mul_pd(_t35_14, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_30), _mm256_mul_pd(_t35_12, _t35_31)));
        _t35_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_28), _mm256_mul_pd(_t35_10, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_30), _mm256_mul_pd(_t35_8, _t35_31)));
        _t35_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_28), _mm256_mul_pd(_t35_6, _t35_29)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_30), _mm256_mul_pd(_t35_4, _t35_31)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t35_20 = _mm256_add_pd(_t35_20, _t35_24);
        _t35_21 = _mm256_add_pd(_t35_21, _t35_25);
        _t35_22 = _mm256_add_pd(_t35_22, _t35_26);
        _t35_23 = _mm256_add_pd(_t35_23, _t35_27);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 28*k2, _t35_20);
        _asm256_storeu_pd(M2 + i0 + 28*k2 + 28, _t35_21);
        _asm256_storeu_pd(M2 + i0 + 28*k2 + 56, _t35_22);
        _asm256_storeu_pd(M2 + i0 + 28*k2 + 84, _t35_23);
      }
    }
    _t36_3 = _asm256_loadu_pd(Y + 29*k3);
    _t36_2 = _mm256_maskload_pd(Y + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t36_1 = _mm256_maskload_pd(Y + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t36_0 = _mm256_maskload_pd(Y + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t36_4 = _t36_3;
    _t36_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
    _t36_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
    _t36_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

    for( int i0 = 0; i0 <= 27; i0+=4 ) {
      _t37_3 = _asm256_loadu_pd(H + 28*i0 + k3);
      _t37_2 = _asm256_loadu_pd(H + 28*i0 + k3 + 28);
      _t37_1 = _asm256_loadu_pd(H + 28*i0 + k3 + 56);
      _t37_0 = _asm256_loadu_pd(H + 28*i0 + k3 + 84);
      _t37_4 = _asm256_loadu_pd(M2 + i0 + 28*k3);
      _t37_5 = _asm256_loadu_pd(M2 + i0 + 28*k3 + 28);
      _t37_6 = _asm256_loadu_pd(M2 + i0 + 28*k3 + 56);
      _t37_7 = _asm256_loadu_pd(M2 + i0 + 28*k3 + 84);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t37_12 = _t36_3;
      _t37_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 3), _t36_2, 12);
      _t37_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 0), _t36_1, 49);
      _t37_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t36_3, _t36_2, 12), _mm256_shuffle_pd(_t36_1, _t36_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 32), _mm256_permute2f128_pd(_t37_12, _t37_12, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_12, _t37_12, 49), _mm256_permute2f128_pd(_t37_12, _t37_12, 49), 15), _t37_19)));
      _t37_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 32), _mm256_permute2f128_pd(_t37_13, _t37_13, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_13, _t37_13, 49), _mm256_permute2f128_pd(_t37_13, _t37_13, 49), 15), _t37_19)));
      _t37_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 32), _mm256_permute2f128_pd(_t37_14, _t37_14, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_14, _t37_14, 49), _mm256_permute2f128_pd(_t37_14, _t37_14, 49), 15), _t37_19)));
      _t37_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 0), _t37_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 32), _mm256_permute2f128_pd(_t37_15, _t37_15, 32), 15), _t37_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 0), _t37_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_15, _t37_15, 49), _mm256_permute2f128_pd(_t37_15, _t37_15, 49), 15), _t37_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t37_4 = _mm256_add_pd(_t37_4, _t37_8);
      _t37_5 = _mm256_add_pd(_t37_5, _t37_9);
      _t37_6 = _mm256_add_pd(_t37_6, _t37_10);
      _t37_7 = _mm256_add_pd(_t37_7, _t37_11);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 28*k3, _t37_4);
      _asm256_storeu_pd(M2 + i0 + 28*k3 + 28, _t37_5);
      _asm256_storeu_pd(M2 + i0 + 28*k3 + 56, _t37_6);
      _asm256_storeu_pd(M2 + i0 + 28*k3 + 84, _t37_7);
    }

    for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {

      for( int i0 = 0; i0 <= 27; i0+=4 ) {
        _t38_7 = _asm256_loadu_pd(Y + k2 + 28*k3);
        _t38_6 = _asm256_loadu_pd(Y + k2 + 28*k3 + 28);
        _t38_5 = _asm256_loadu_pd(Y + k2 + 28*k3 + 56);
        _t38_4 = _asm256_loadu_pd(Y + k2 + 28*k3 + 84);
        _t38_3 = _asm256_loadu_pd(H + 28*i0 + k3);
        _t38_2 = _asm256_loadu_pd(H + 28*i0 + k3 + 28);
        _t38_1 = _asm256_loadu_pd(H + 28*i0 + k3 + 56);
        _t38_0 = _asm256_loadu_pd(H + 28*i0 + k3 + 84);
        _t38_8 = _asm256_loadu_pd(M2 + i0 + 28*k2);
        _t38_9 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 28);
        _t38_10 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 56);
        _t38_11 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 84);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 32);
        _t38_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 32);
        _t38_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 49);
        _t38_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t38_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
        _t38_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
        _t38_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
        _t38_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t38_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 32), _mm256_permute2f128_pd(_t38_16, _t38_16, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_16, _t38_16, 49), _mm256_permute2f128_pd(_t38_16, _t38_16, 49), 15), _t38_23)));
        _t38_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 32), _mm256_permute2f128_pd(_t38_17, _t38_17, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_17, _t38_17, 49), _mm256_permute2f128_pd(_t38_17, _t38_17, 49), 15), _t38_23)));
        _t38_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 32), _mm256_permute2f128_pd(_t38_18, _t38_18, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_18, _t38_18, 49), _mm256_permute2f128_pd(_t38_18, _t38_18, 49), 15), _t38_23)));
        _t38_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 0), _t38_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 32), _mm256_permute2f128_pd(_t38_19, _t38_19, 32), 15), _t38_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 0), _t38_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_19, _t38_19, 49), _mm256_permute2f128_pd(_t38_19, _t38_19, 49), 15), _t38_23)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t38_8 = _mm256_add_pd(_t38_8, _t38_12);
        _t38_9 = _mm256_add_pd(_t38_9, _t38_13);
        _t38_10 = _mm256_add_pd(_t38_10, _t38_14);
        _t38_11 = _mm256_add_pd(_t38_11, _t38_15);

        // AVX Storer:
        _asm256_storeu_pd(M2 + i0 + 28*k2, _t38_8);
        _asm256_storeu_pd(M2 + i0 + 28*k2 + 28, _t38_9);
        _asm256_storeu_pd(M2 + i0 + 28*k2 + 56, _t38_10);
        _asm256_storeu_pd(M2 + i0 + 28*k2 + 84, _t38_11);
      }
    }
  }


  for( int k2 = 0; k2 <= 23; k2+=4 ) {

    // AVX Loader:

    for( int i0 = 0; i0 <= 27; i0+=4 ) {
      _t39_19 = _mm256_broadcast_sd(Y + 28*k2 + 24);
      _t39_18 = _mm256_broadcast_sd(Y + 28*k2 + 25);
      _t39_17 = _mm256_broadcast_sd(Y + 28*k2 + 26);
      _t39_16 = _mm256_broadcast_sd(Y + 28*k2 + 27);
      _t39_15 = _mm256_broadcast_sd(Y + 28*k2 + 52);
      _t39_14 = _mm256_broadcast_sd(Y + 28*k2 + 53);
      _t39_13 = _mm256_broadcast_sd(Y + 28*k2 + 54);
      _t39_12 = _mm256_broadcast_sd(Y + 28*k2 + 55);
      _t39_11 = _mm256_broadcast_sd(Y + 28*k2 + 80);
      _t39_10 = _mm256_broadcast_sd(Y + 28*k2 + 81);
      _t39_9 = _mm256_broadcast_sd(Y + 28*k2 + 82);
      _t39_8 = _mm256_broadcast_sd(Y + 28*k2 + 83);
      _t39_7 = _mm256_broadcast_sd(Y + 28*k2 + 108);
      _t39_6 = _mm256_broadcast_sd(Y + 28*k2 + 109);
      _t39_5 = _mm256_broadcast_sd(Y + 28*k2 + 110);
      _t39_4 = _mm256_broadcast_sd(Y + 28*k2 + 111);
      _t39_3 = _asm256_loadu_pd(H + 28*i0 + 24);
      _t39_2 = _asm256_loadu_pd(H + 28*i0 + 52);
      _t39_1 = _asm256_loadu_pd(H + 28*i0 + 80);
      _t39_0 = _asm256_loadu_pd(H + 28*i0 + 108);
      _t39_20 = _asm256_loadu_pd(M2 + i0 + 28*k2);
      _t39_21 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 28);
      _t39_22 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 56);
      _t39_23 = _asm256_loadu_pd(M2 + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t39_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 32);
      _t39_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 32);
      _t39_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 49);
      _t39_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t39_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_28), _mm256_mul_pd(_t39_18, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_30), _mm256_mul_pd(_t39_16, _t39_31)));
      _t39_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_28), _mm256_mul_pd(_t39_14, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_30), _mm256_mul_pd(_t39_12, _t39_31)));
      _t39_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_28), _mm256_mul_pd(_t39_10, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_30), _mm256_mul_pd(_t39_8, _t39_31)));
      _t39_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_28), _mm256_mul_pd(_t39_6, _t39_29)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_30), _mm256_mul_pd(_t39_4, _t39_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_20 = _mm256_add_pd(_t39_20, _t39_24);
      _t39_21 = _mm256_add_pd(_t39_21, _t39_25);
      _t39_22 = _mm256_add_pd(_t39_22, _t39_26);
      _t39_23 = _mm256_add_pd(_t39_23, _t39_27);

      // AVX Storer:
      _asm256_storeu_pd(M2 + i0 + 28*k2, _t39_20);
      _asm256_storeu_pd(M2 + i0 + 28*k2 + 28, _t39_21);
      _asm256_storeu_pd(M2 + i0 + 28*k2 + 56, _t39_22);
      _asm256_storeu_pd(M2 + i0 + 28*k2 + 84, _t39_23);
    }
  }


  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t40_0 = _t15_28;
  _t40_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t40_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t40_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t41_3 = _asm256_loadu_pd(H + 28*i0 + 24);
    _t41_2 = _asm256_loadu_pd(H + 28*i0 + 52);
    _t41_1 = _asm256_loadu_pd(H + 28*i0 + 80);
    _t41_0 = _asm256_loadu_pd(H + 28*i0 + 108);
    _t41_4 = _asm256_loadu_pd(M2 + i0 + 672);
    _t41_5 = _asm256_loadu_pd(M2 + i0 + 700);
    _t41_6 = _asm256_loadu_pd(M2 + i0 + 728);
    _t41_7 = _asm256_loadu_pd(M2 + i0 + 756);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_3, _t41_2), _mm256_unpacklo_pd(_t41_1, _t41_0), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_3, _t41_2), _mm256_unpackhi_pd(_t41_1, _t41_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t41_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 32), _mm256_permute2f128_pd(_t40_0, _t40_0, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_0, _t40_0, 49), _mm256_permute2f128_pd(_t40_0, _t40_0, 49), 15), _t41_15)));
    _t41_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 32), _mm256_permute2f128_pd(_t40_1, _t40_1, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_1, _t40_1, 49), _mm256_permute2f128_pd(_t40_1, _t40_1, 49), 15), _t41_15)));
    _t41_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 32), _mm256_permute2f128_pd(_t40_2, _t40_2, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_2, _t40_2, 49), _mm256_permute2f128_pd(_t40_2, _t40_2, 49), 15), _t41_15)));
    _t41_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 0), _t41_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 32), _mm256_permute2f128_pd(_t40_3, _t40_3, 32), 15), _t41_13)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 0), _t41_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t40_3, _t40_3, 49), _mm256_permute2f128_pd(_t40_3, _t40_3, 49), 15), _t41_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t41_4 = _mm256_add_pd(_t41_4, _t41_8);
    _t41_5 = _mm256_add_pd(_t41_5, _t41_9);
    _t41_6 = _mm256_add_pd(_t41_6, _t41_10);
    _t41_7 = _mm256_add_pd(_t41_7, _t41_11);

    // AVX Storer:
    _asm256_storeu_pd(M2 + i0 + 672, _t41_4);
    _asm256_storeu_pd(M2 + i0 + 700, _t41_5);
    _asm256_storeu_pd(M2 + i0 + 728, _t41_6);
    _asm256_storeu_pd(M2 + i0 + 756, _t41_7);
  }


  // Generating : M3[28,28] = ( ( Sum_{k2} ( ( S(h(4, 28, k2), ( ( G(h(4, 28, k2), M1[28,28],h(4, 28, 0)) * T( G(h(4, 28, k2), H[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, k2), R[28,28],h(4, 28, k2)) ),h(4, 28, k2)) + Sum_{i0} ( S(h(4, 28, k2), ( ( G(h(4, 28, k2), M1[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, k2), R[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) + S(h(4, 28, 24), ( ( G(h(4, 28, 24), M1[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), H[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, 24), R[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k3} ( ( Sum_{k2} ( ( $(h(4, 28, k2), ( G(h(4, 28, k2), M1[28,28],h(4, 28, k3)) * T( G(h(4, 28, k2), H[28,28],h(4, 28, k3)) ) ),h(4, 28, k2)) + Sum_{i0} ( $(h(4, 28, k2), ( G(h(4, 28, k2), M1[28,28],h(4, 28, k3)) * T( G(h(4, 28, i0), H[28,28],h(4, 28, k3)) ) ),h(4, 28, i0)) ) ) ) + $(h(4, 28, 24), ( G(h(4, 28, 24), M1[28,28],h(4, 28, k3)) * T( G(h(4, 28, 24), H[28,28],h(4, 28, k3)) ) ),h(4, 28, 24)) ) ) )


  for( int k2 = 0; k2 <= 23; k2+=4 ) {
    _t42_23 = _mm256_broadcast_sd(M1 + 28*k2);
    _t42_22 = _mm256_broadcast_sd(M1 + 28*k2 + 1);
    _t42_21 = _mm256_broadcast_sd(M1 + 28*k2 + 2);
    _t42_20 = _mm256_broadcast_sd(M1 + 28*k2 + 3);
    _t42_19 = _mm256_broadcast_sd(M1 + 28*k2 + 28);
    _t42_18 = _mm256_broadcast_sd(M1 + 28*k2 + 29);
    _t42_17 = _mm256_broadcast_sd(M1 + 28*k2 + 30);
    _t42_16 = _mm256_broadcast_sd(M1 + 28*k2 + 31);
    _t42_15 = _mm256_broadcast_sd(M1 + 28*k2 + 56);
    _t42_14 = _mm256_broadcast_sd(M1 + 28*k2 + 57);
    _t42_13 = _mm256_broadcast_sd(M1 + 28*k2 + 58);
    _t42_12 = _mm256_broadcast_sd(M1 + 28*k2 + 59);
    _t42_11 = _mm256_broadcast_sd(M1 + 28*k2 + 84);
    _t42_10 = _mm256_broadcast_sd(M1 + 28*k2 + 85);
    _t42_9 = _mm256_broadcast_sd(M1 + 28*k2 + 86);
    _t42_8 = _mm256_broadcast_sd(M1 + 28*k2 + 87);
    _t42_7 = _asm256_loadu_pd(H + 28*k2);
    _t42_6 = _asm256_loadu_pd(H + 28*k2 + 28);
    _t42_5 = _asm256_loadu_pd(H + 28*k2 + 56);
    _t42_4 = _asm256_loadu_pd(H + 28*k2 + 84);
    _t42_3 = _asm256_loadu_pd(R + 29*k2);
    _t42_2 = _mm256_maskload_pd(R + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t42_1 = _mm256_maskload_pd(R + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t42_0 = _mm256_maskload_pd(R + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t42_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 32);
    _t42_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 32);
    _t42_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 49);
    _t42_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t42_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t42_40), _mm256_mul_pd(_t42_22, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t42_42), _mm256_mul_pd(_t42_20, _t42_43)));
    _t42_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t42_40), _mm256_mul_pd(_t42_18, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t42_42), _mm256_mul_pd(_t42_16, _t42_43)));
    _t42_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_40), _mm256_mul_pd(_t42_14, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_42), _mm256_mul_pd(_t42_12, _t42_43)));
    _t42_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_40), _mm256_mul_pd(_t42_10, _t42_41)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_42), _mm256_mul_pd(_t42_8, _t42_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t42_36 = _t42_3;
    _t42_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 3), _t42_2, 12);
    _t42_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 0), _t42_1, 49);
    _t42_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t42_3, _t42_2, 12), _mm256_shuffle_pd(_t42_1, _t42_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t42_24 = _mm256_add_pd(_t42_32, _t42_36);
    _t42_25 = _mm256_add_pd(_t42_33, _t42_37);
    _t42_26 = _mm256_add_pd(_t42_34, _t42_38);
    _t42_27 = _mm256_add_pd(_t42_35, _t42_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t42_28 = _t42_24;
    _t42_29 = _t42_25;
    _t42_30 = _t42_26;
    _t42_31 = _t42_27;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
      _t43_7 = _asm256_loadu_pd(H + 28*i0);
      _t43_6 = _asm256_loadu_pd(H + 28*i0 + 28);
      _t43_5 = _asm256_loadu_pd(H + 28*i0 + 56);
      _t43_4 = _asm256_loadu_pd(H + 28*i0 + 84);
      _t43_3 = _asm256_loadu_pd(R + i0 + 28*k2);
      _t43_2 = _asm256_loadu_pd(R + i0 + 28*k2 + 28);
      _t43_1 = _asm256_loadu_pd(R + i0 + 28*k2 + 56);
      _t43_0 = _asm256_loadu_pd(R + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t43_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 32);
      _t43_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 32);
      _t43_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 49);
      _t43_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t43_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_23, _t43_16), _mm256_mul_pd(_t42_22, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_21, _t43_18), _mm256_mul_pd(_t42_20, _t43_19)));
      _t43_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_19, _t43_16), _mm256_mul_pd(_t42_18, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_17, _t43_18), _mm256_mul_pd(_t42_16, _t43_19)));
      _t43_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t43_16), _mm256_mul_pd(_t42_14, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t43_18), _mm256_mul_pd(_t42_12, _t43_19)));
      _t43_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t43_16), _mm256_mul_pd(_t42_10, _t43_17)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t43_18), _mm256_mul_pd(_t42_8, _t43_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t43_8 = _mm256_add_pd(_t43_12, _t43_3);
      _t43_9 = _mm256_add_pd(_t43_13, _t43_2);
      _t43_10 = _mm256_add_pd(_t43_14, _t43_1);
      _t43_11 = _mm256_add_pd(_t43_15, _t43_0);

      // AVX Storer:
      _asm256_storeu_pd(M3 + i0 + 28*k2, _t43_8);
      _asm256_storeu_pd(M3 + i0 + 28*k2 + 28, _t43_9);
      _asm256_storeu_pd(M3 + i0 + 28*k2 + 56, _t43_10);
      _asm256_storeu_pd(M3 + i0 + 28*k2 + 84, _t43_11);
    }
    _asm256_storeu_pd(M3 + 29*k2, _t42_28);
    _mm256_maskstore_pd(M3 + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t42_29);
    _mm256_maskstore_pd(M3 + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t42_30);
    _mm256_maskstore_pd(M3 + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t42_31);
  }

  _t44_23 = _mm256_broadcast_sd(M1 + 672);
  _t44_22 = _mm256_broadcast_sd(M1 + 673);
  _t44_21 = _mm256_broadcast_sd(M1 + 674);
  _t44_20 = _mm256_broadcast_sd(M1 + 675);
  _t44_19 = _mm256_broadcast_sd(M1 + 700);
  _t44_18 = _mm256_broadcast_sd(M1 + 701);
  _t44_17 = _mm256_broadcast_sd(M1 + 702);
  _t44_16 = _mm256_broadcast_sd(M1 + 703);
  _t44_15 = _mm256_broadcast_sd(M1 + 728);
  _t44_14 = _mm256_broadcast_sd(M1 + 729);
  _t44_13 = _mm256_broadcast_sd(M1 + 730);
  _t44_12 = _mm256_broadcast_sd(M1 + 731);
  _t44_11 = _mm256_broadcast_sd(M1 + 756);
  _t44_10 = _mm256_broadcast_sd(M1 + 757);
  _t44_9 = _mm256_broadcast_sd(M1 + 758);
  _t44_8 = _mm256_broadcast_sd(M1 + 759);
  _t44_7 = _asm256_loadu_pd(H + 672);
  _t44_6 = _asm256_loadu_pd(H + 700);
  _t44_5 = _asm256_loadu_pd(H + 728);
  _t44_4 = _asm256_loadu_pd(H + 756);
  _t44_3 = _asm256_loadu_pd(R + 696);
  _t44_2 = _mm256_maskload_pd(R + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t44_1 = _mm256_maskload_pd(R + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t44_0 = _mm256_maskload_pd(R + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t44_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 32);
  _t44_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 32);
  _t44_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_7, _t44_6), _mm256_unpacklo_pd(_t44_5, _t44_4), 49);
  _t44_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_7, _t44_6), _mm256_unpackhi_pd(_t44_5, _t44_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t44_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_23, _t44_40), _mm256_mul_pd(_t44_22, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_21, _t44_42), _mm256_mul_pd(_t44_20, _t44_43)));
  _t44_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_19, _t44_40), _mm256_mul_pd(_t44_18, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_17, _t44_42), _mm256_mul_pd(_t44_16, _t44_43)));
  _t44_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_40), _mm256_mul_pd(_t44_14, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_42), _mm256_mul_pd(_t44_12, _t44_43)));
  _t44_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_40), _mm256_mul_pd(_t44_10, _t44_41)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_42), _mm256_mul_pd(_t44_8, _t44_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t44_36 = _t44_3;
  _t44_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 3), _t44_2, 12);
  _t44_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 0), _t44_1, 49);
  _t44_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_3, _t44_2, 12), _mm256_shuffle_pd(_t44_1, _t44_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t44_24 = _mm256_add_pd(_t44_32, _t44_36);
  _t44_25 = _mm256_add_pd(_t44_33, _t44_37);
  _t44_26 = _mm256_add_pd(_t44_34, _t44_38);
  _t44_27 = _mm256_add_pd(_t44_35, _t44_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t44_28 = _t44_24;
  _t44_29 = _t44_25;
  _t44_30 = _t44_26;
  _t44_31 = _t44_27;


  for( int k3 = 4; k3 <= 27; k3+=4 ) {

    for( int k2 = 0; k2 <= 23; k2+=4 ) {
      _t45_19 = _mm256_broadcast_sd(M1 + 28*k2 + k3);
      _t45_18 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 1);
      _t45_17 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 2);
      _t45_16 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 3);
      _t45_15 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 28);
      _t45_14 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 29);
      _t45_13 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 30);
      _t45_12 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 31);
      _t45_11 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 56);
      _t45_10 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 57);
      _t45_9 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 58);
      _t45_8 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 59);
      _t45_7 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 84);
      _t45_6 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 85);
      _t45_5 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 86);
      _t45_4 = _mm256_broadcast_sd(M1 + 28*k2 + k3 + 87);
      _t45_3 = _asm256_loadu_pd(H + 28*k2 + k3);
      _t45_2 = _asm256_loadu_pd(H + 28*k2 + k3 + 28);
      _t45_1 = _asm256_loadu_pd(H + 28*k2 + k3 + 56);
      _t45_0 = _asm256_loadu_pd(H + 28*k2 + k3 + 84);
      _t45_20 = _asm256_loadu_pd(M3 + 29*k2);
      _t45_21 = _mm256_maskload_pd(M3 + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t45_22 = _mm256_maskload_pd(M3 + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t45_23 = _mm256_maskload_pd(M3 + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t45_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
      _t45_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
      _t45_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
      _t45_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t45_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_32), _mm256_mul_pd(_t45_18, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_34), _mm256_mul_pd(_t45_16, _t45_35)));
      _t45_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_32), _mm256_mul_pd(_t45_14, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_34), _mm256_mul_pd(_t45_12, _t45_35)));
      _t45_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_32), _mm256_mul_pd(_t45_10, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_34), _mm256_mul_pd(_t45_8, _t45_35)));
      _t45_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_32), _mm256_mul_pd(_t45_6, _t45_33)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_34), _mm256_mul_pd(_t45_4, _t45_35)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t45_28 = _t45_20;
      _t45_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 3), _t45_21, 12);
      _t45_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 0), _t45_22, 49);
      _t45_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t45_20, _t45_21, 12), _mm256_shuffle_pd(_t45_22, _t45_23, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t45_28 = _mm256_add_pd(_t45_28, _t45_24);
      _t45_29 = _mm256_add_pd(_t45_29, _t45_25);
      _t45_30 = _mm256_add_pd(_t45_30, _t45_26);
      _t45_31 = _mm256_add_pd(_t45_31, _t45_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t45_20 = _t45_28;
      _t45_21 = _t45_29;
      _t45_22 = _t45_30;
      _t45_23 = _t45_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
        _t46_3 = _asm256_loadu_pd(H + 28*i0 + k3);
        _t46_2 = _asm256_loadu_pd(H + 28*i0 + k3 + 28);
        _t46_1 = _asm256_loadu_pd(H + 28*i0 + k3 + 56);
        _t46_0 = _asm256_loadu_pd(H + 28*i0 + k3 + 84);
        _t46_4 = _asm256_loadu_pd(M3 + i0 + 28*k2);
        _t46_5 = _asm256_loadu_pd(M3 + i0 + 28*k2 + 28);
        _t46_6 = _asm256_loadu_pd(M3 + i0 + 28*k2 + 56);
        _t46_7 = _asm256_loadu_pd(M3 + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t46_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
        _t46_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
        _t46_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
        _t46_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t46_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t46_12), _mm256_mul_pd(_t45_18, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t46_14), _mm256_mul_pd(_t45_16, _t46_15)));
        _t46_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t46_12), _mm256_mul_pd(_t45_14, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t46_14), _mm256_mul_pd(_t45_12, _t46_15)));
        _t46_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t46_12), _mm256_mul_pd(_t45_10, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t46_14), _mm256_mul_pd(_t45_8, _t46_15)));
        _t46_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t46_12), _mm256_mul_pd(_t45_6, _t46_13)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t46_14), _mm256_mul_pd(_t45_4, _t46_15)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t46_4 = _mm256_add_pd(_t46_4, _t46_8);
        _t46_5 = _mm256_add_pd(_t46_5, _t46_9);
        _t46_6 = _mm256_add_pd(_t46_6, _t46_10);
        _t46_7 = _mm256_add_pd(_t46_7, _t46_11);

        // AVX Storer:
        _asm256_storeu_pd(M3 + i0 + 28*k2, _t46_4);
        _asm256_storeu_pd(M3 + i0 + 28*k2 + 28, _t46_5);
        _asm256_storeu_pd(M3 + i0 + 28*k2 + 56, _t46_6);
        _asm256_storeu_pd(M3 + i0 + 28*k2 + 84, _t46_7);
      }
      _asm256_storeu_pd(M3 + 29*k2, _t45_20);
      _mm256_maskstore_pd(M3 + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t45_21);
      _mm256_maskstore_pd(M3 + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t45_22);
      _mm256_maskstore_pd(M3 + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t45_23);
    }
    _t47_19 = _mm256_broadcast_sd(M1 + k3 + 672);
    _t47_18 = _mm256_broadcast_sd(M1 + k3 + 673);
    _t47_17 = _mm256_broadcast_sd(M1 + k3 + 674);
    _t47_16 = _mm256_broadcast_sd(M1 + k3 + 675);
    _t47_15 = _mm256_broadcast_sd(M1 + k3 + 700);
    _t47_14 = _mm256_broadcast_sd(M1 + k3 + 701);
    _t47_13 = _mm256_broadcast_sd(M1 + k3 + 702);
    _t47_12 = _mm256_broadcast_sd(M1 + k3 + 703);
    _t47_11 = _mm256_broadcast_sd(M1 + k3 + 728);
    _t47_10 = _mm256_broadcast_sd(M1 + k3 + 729);
    _t47_9 = _mm256_broadcast_sd(M1 + k3 + 730);
    _t47_8 = _mm256_broadcast_sd(M1 + k3 + 731);
    _t47_7 = _mm256_broadcast_sd(M1 + k3 + 756);
    _t47_6 = _mm256_broadcast_sd(M1 + k3 + 757);
    _t47_5 = _mm256_broadcast_sd(M1 + k3 + 758);
    _t47_4 = _mm256_broadcast_sd(M1 + k3 + 759);
    _t47_3 = _asm256_loadu_pd(H + k3 + 672);
    _t47_2 = _asm256_loadu_pd(H + k3 + 700);
    _t47_1 = _asm256_loadu_pd(H + k3 + 728);
    _t47_0 = _asm256_loadu_pd(H + k3 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 32);
    _t47_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 32);
    _t47_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_3, _t47_2), _mm256_unpacklo_pd(_t47_1, _t47_0), 49);
    _t47_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_3, _t47_2), _mm256_unpackhi_pd(_t47_1, _t47_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_19, _t47_28), _mm256_mul_pd(_t47_18, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_17, _t47_30), _mm256_mul_pd(_t47_16, _t47_31)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_28), _mm256_mul_pd(_t47_14, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_30), _mm256_mul_pd(_t47_12, _t47_31)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_28), _mm256_mul_pd(_t47_10, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_30), _mm256_mul_pd(_t47_8, _t47_31)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_28), _mm256_mul_pd(_t47_6, _t47_29)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_30), _mm256_mul_pd(_t47_4, _t47_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t47_24 = _t44_28;
    _t47_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 3), _t44_29, 12);
    _t47_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 0), _t44_30, 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 12), _mm256_shuffle_pd(_t44_30, _t44_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t47_24 = _mm256_add_pd(_t47_24, _t47_20);
    _t47_25 = _mm256_add_pd(_t47_25, _t47_21);
    _t47_26 = _mm256_add_pd(_t47_26, _t47_22);
    _t47_27 = _mm256_add_pd(_t47_27, _t47_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t44_28 = _t47_24;
    _t44_29 = _t47_25;
    _t44_30 = _t47_26;
    _t44_31 = _t47_27;
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29])));
  _t48_4 = _mm256_maskload_pd(M3 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[58])));
  _t48_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[59])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[87])));
  _t48_14 = _asm256_loadu_pd(M3 + 4);
  _t48_11 = _asm256_loadu_pd(M3 + 32);
  _t48_12 = _asm256_loadu_pd(M3 + 60);
  _t48_13 = _asm256_loadu_pd(M3 + 88);

  // Generating : U[28,28] = S(h(1, 28, 0), Sqrt( G(h(1, 28, 0), U[28,28],h(1, 28, 0)) ),h(1, 28, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_16 = _t48_0;

  // 4-BLAC: sqrt(1x4)
  _t48_17 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_16)));

  // AVX Storer:
  _t48_0 = _t48_17;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 0), U[28,28],h(1, 28, 0)) ),h(1, 28, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_19 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t48_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_18), _mm256_castpd256_pd128(_t48_19)));

  // AVX Storer:
  _t48_1 = _t48_20;

  // Generating : U[28,28] = S(h(1, 28, 0), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 0)) Kro G(h(1, 28, 0), U[28,28],h(3, 28, 1)) ),h(3, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_22 = _t48_2;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_23 = _mm256_mul_pd(_t48_21, _t48_22);

  // AVX Storer:
  _t48_2 = _t48_23;

  // Generating : U[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), U[28,28],h(1, 28, 1)) - ( T( G(h(1, 28, 0), U[28,28],h(1, 28, 1)) ) Kro G(h(1, 28, 0), U[28,28],h(1, 28, 1)) ) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_24 = _t48_3;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: (4x1)^T
  _t48_26 = _t48_25;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_27 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_28 = _mm256_mul_pd(_t48_26, _t48_27);

  // 4-BLAC: 1x4 - 1x4
  _t48_29 = _mm256_sub_pd(_t48_24, _t48_28);

  // AVX Storer:
  _t48_3 = _t48_29;

  // Generating : U[28,28] = S(h(1, 28, 1), Sqrt( G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ),h(1, 28, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_30 = _t48_3;

  // 4-BLAC: sqrt(1x4)
  _t48_31 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_30)));

  // AVX Storer:
  _t48_3 = _t48_31;

  // Generating : U[28,28] = S(h(1, 28, 1), ( G(h(1, 28, 1), U[28,28],h(2, 28, 2)) - ( T( G(h(1, 28, 0), U[28,28],h(1, 28, 1)) ) Kro G(h(1, 28, 0), U[28,28],h(2, 28, 2)) ) ),h(2, 28, 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_32 = _t48_4;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_33 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_34 = _t48_33;

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_35 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 6), _mm256_permute2f128_pd(_t48_2, _t48_2, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t48_36 = _mm256_mul_pd(_t48_34, _t48_35);

  // 4-BLAC: 1x4 - 1x4
  _t48_37 = _mm256_sub_pd(_t48_32, _t48_36);

  // AVX Storer:
  _t48_4 = _t48_37;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 1), U[28,28],h(1, 28, 1)) ),h(1, 28, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_38 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_39 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t48_40 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_38), _mm256_castpd256_pd128(_t48_39)));

  // AVX Storer:
  _t48_5 = _t48_40;

  // Generating : U[28,28] = S(h(1, 28, 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 1)) Kro G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ),h(2, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_41 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_5, _t48_5, 32), _mm256_permute2f128_pd(_t48_5, _t48_5, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_42 = _t48_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t48_43 = _mm256_mul_pd(_t48_41, _t48_42);

  // AVX Storer:
  _t48_4 = _t48_43;

  // Generating : U[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), U[28,28],h(1, 28, 2)) - ( T( G(h(2, 28, 0), U[28,28],h(1, 28, 2)) ) * G(h(2, 28, 0), U[28,28],h(1, 28, 2)) ) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_44 = _t48_6;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_45 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_46 = _t48_45;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_47 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t48_48 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_mul_pd(_t48_46, _t48_47), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_mul_pd(_t48_46, _t48_47), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_46, _t48_47), _mm256_mul_pd(_t48_46, _t48_47), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_49 = _mm256_sub_pd(_t48_44, _t48_48);

  // AVX Storer:
  _t48_6 = _t48_49;

  // Generating : U[28,28] = S(h(1, 28, 2), Sqrt( G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ),h(1, 28, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_50 = _t48_6;

  // 4-BLAC: sqrt(1x4)
  _t48_51 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_50)));

  // AVX Storer:
  _t48_6 = _t48_51;

  // Generating : U[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), U[28,28],h(1, 28, 3)) - ( T( G(h(2, 28, 0), U[28,28],h(1, 28, 2)) ) * G(h(2, 28, 0), U[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_52 = _t48_7;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_53 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t48_54 = _t48_53;

  // AVX Loader:

  // 2x1 -> 4x1
  _t48_55 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 129), _t48_4, 2);

  // 4-BLAC: 1x4 * 4x1
  _t48_56 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_mul_pd(_t48_54, _t48_55), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_mul_pd(_t48_54, _t48_55), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_54, _t48_55), _mm256_mul_pd(_t48_54, _t48_55), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_57 = _mm256_sub_pd(_t48_52, _t48_56);

  // AVX Storer:
  _t48_7 = _t48_57;

  // Generating : U[28,28] = S(h(1, 28, 2), ( G(h(1, 28, 2), U[28,28],h(1, 28, 3)) Div G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_58 = _t48_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_59 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t48_60 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_58), _mm256_castpd256_pd128(_t48_59)));

  // AVX Storer:
  _t48_7 = _t48_60;

  // Generating : U[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), U[28,28],h(1, 28, 3)) - ( T( G(h(3, 28, 0), U[28,28],h(1, 28, 3)) ) * G(h(3, 28, 0), U[28,28],h(1, 28, 3)) ) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_61 = _t48_8;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_62 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // 4-BLAC: (4x1)^T
  _t48_63 = _t48_62;

  // AVX Loader:

  // 3x1 -> 4x1
  _t48_64 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t48_65 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_mul_pd(_t48_63, _t48_64), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_mul_pd(_t48_63, _t48_64), 129)), _mm256_add_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t48_63, _t48_64), _mm256_mul_pd(_t48_63, _t48_64), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t48_66 = _mm256_sub_pd(_t48_61, _t48_65);

  // AVX Storer:
  _t48_8 = _t48_66;

  // Generating : U[28,28] = S(h(1, 28, 3), Sqrt( G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ),h(1, 28, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_67 = _t48_8;

  // 4-BLAC: sqrt(1x4)
  _t48_68 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t48_67)));

  // AVX Storer:
  _t48_8 = _t48_68;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 2), U[28,28],h(1, 28, 2)) ),h(1, 28, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_69 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_70 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t48_71 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_69), _mm256_castpd256_pd128(_t48_70)));

  // AVX Storer:
  _t48_9 = _t48_71;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 3), U[28,28],h(1, 28, 3)) ),h(1, 28, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t48_72 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_73 = _t48_8;

  // 4-BLAC: 1x4 / 1x4
  _t48_74 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t48_72), _mm256_castpd256_pd128(_t48_73)));

  // AVX Storer:
  _t48_10 = _t48_74;

  // Generating : U[28,28] = S(h(1, 28, 0), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 0)) Kro G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_75 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_14 = _mm256_mul_pd(_t48_75, _t48_14);

  // AVX Storer:

  // Generating : U[28,28] = S(h(3, 28, 1), ( G(h(3, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) - ( T( G(h(1, 28, 0), U[28,28],h(3, 28, 1)) ) * G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t48_76 = _t48_11;
  _t48_77 = _t48_12;
  _t48_78 = _t48_13;
  _t48_79 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t48_80 = _t48_2;

  // 4-BLAC: (1x4)^T
  _t48_81 = _t48_80;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_82 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 0), _t48_14);
  _t48_83 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 15), _t48_14);
  _t48_84 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 0), _t48_14);
  _t48_85 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 15), _t48_14);

  // 4-BLAC: 4x4 - 4x4
  _t48_86 = _mm256_sub_pd(_t48_76, _t48_82);
  _t48_87 = _mm256_sub_pd(_t48_77, _t48_83);
  _t48_88 = _mm256_sub_pd(_t48_78, _t48_84);
  _t48_89 = _mm256_sub_pd(_t48_79, _t48_85);

  // AVX Storer:
  _t48_11 = _t48_86;
  _t48_12 = _t48_87;
  _t48_13 = _t48_88;

  // Generating : U[28,28] = S(h(1, 28, 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 1)) Kro G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_90 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_5, _t48_5, 32), _mm256_permute2f128_pd(_t48_5, _t48_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_11 = _mm256_mul_pd(_t48_90, _t48_11);

  // AVX Storer:

  // Generating : U[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) - ( T( G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) * G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t48_91 = _t48_12;
  _t48_92 = _t48_13;
  _t48_93 = _mm256_setzero_pd();
  _t48_94 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t48_95 = _t48_4;

  // 4-BLAC: (1x4)^T
  _t48_96 = _t48_95;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t48_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 0), _t48_11);
  _t48_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 15), _t48_11);
  _t48_99 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 0), _t48_11);
  _t48_100 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 15), _t48_11);

  // 4-BLAC: 4x4 - 4x4
  _t48_101 = _mm256_sub_pd(_t48_91, _t48_97);
  _t48_102 = _mm256_sub_pd(_t48_92, _t48_98);
  _t48_103 = _mm256_sub_pd(_t48_93, _t48_99);
  _t48_104 = _mm256_sub_pd(_t48_94, _t48_100);

  // AVX Storer:
  _t48_12 = _t48_101;
  _t48_13 = _t48_102;

  // Generating : U[28,28] = S(h(1, 28, 2), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 2)) Kro G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_105 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_12 = _mm256_mul_pd(_t48_105, _t48_12);

  // AVX Storer:

  // Generating : U[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) - ( T( G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) Kro G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_106 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_7, _t48_7, 32), _mm256_permute2f128_pd(_t48_7, _t48_7, 32), 0);

  // 4-BLAC: (4x1)^T
  _t48_107 = _t48_106;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_15 = _mm256_mul_pd(_t48_107, _t48_12);

  // 4-BLAC: 1x4 - 1x4
  _t48_13 = _mm256_sub_pd(_t48_13, _t48_15);

  // AVX Storer:

  // Generating : U[28,28] = S(h(1, 28, 3), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 3)) Kro G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t48_108 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_10, _t48_10, 32), _mm256_permute2f128_pd(_t48_10, _t48_10, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t48_13 = _mm256_mul_pd(_t48_108, _t48_13);

  // AVX Storer:


  for( int fi1304 = 4; fi1304 <= 20; fi1304+=4 ) {
    _t49_3 = _asm256_loadu_pd(M3 + fi1304 + 4);
    _t49_0 = _asm256_loadu_pd(M3 + fi1304 + 32);
    _t49_1 = _asm256_loadu_pd(M3 + fi1304 + 60);
    _t49_2 = _asm256_loadu_pd(M3 + fi1304 + 88);

    // Generating : U[28,28] = S(h(1, 28, 0), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 0)) Kro G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_1, _t48_1, 32), _mm256_permute2f128_pd(_t48_1, _t48_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_3 = _mm256_mul_pd(_t49_4, _t49_3);

    // AVX Storer:

    // Generating : U[28,28] = S(h(3, 28, 1), ( G(h(3, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) - ( T( G(h(1, 28, 0), U[28,28],h(3, 28, 1)) ) * G(h(1, 28, 0), U[28,28],h(4, 28, fi1304 + 4)) ) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t49_5 = _t49_0;
    _t49_6 = _t49_1;
    _t49_7 = _t49_2;
    _t49_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t49_9 = _t48_2;

    // 4-BLAC: (1x4)^T
    _t48_81 = _t49_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t48_82 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 0), _t49_3);
    _t48_83 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 32), _mm256_permute2f128_pd(_t48_81, _t48_81, 32), 15), _t49_3);
    _t48_84 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 0), _t49_3);
    _t48_85 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_81, _t48_81, 49), _mm256_permute2f128_pd(_t48_81, _t48_81, 49), 15), _t49_3);

    // 4-BLAC: 4x4 - 4x4
    _t49_10 = _mm256_sub_pd(_t49_5, _t48_82);
    _t49_11 = _mm256_sub_pd(_t49_6, _t48_83);
    _t49_12 = _mm256_sub_pd(_t49_7, _t48_84);
    _t49_13 = _mm256_sub_pd(_t49_8, _t48_85);

    // AVX Storer:
    _t49_0 = _t49_10;
    _t49_1 = _t49_11;
    _t49_2 = _t49_12;

    // Generating : U[28,28] = S(h(1, 28, 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 1)) Kro G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_5, _t48_5, 32), _mm256_permute2f128_pd(_t48_5, _t48_5, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_0 = _mm256_mul_pd(_t49_14, _t49_0);

    // AVX Storer:

    // Generating : U[28,28] = S(h(2, 28, 2), ( G(h(2, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) - ( T( G(h(1, 28, 1), U[28,28],h(2, 28, 2)) ) * G(h(1, 28, 1), U[28,28],h(4, 28, fi1304 + 4)) ) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t49_15 = _t49_1;
    _t49_16 = _t49_2;
    _t49_17 = _mm256_setzero_pd();
    _t49_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t49_19 = _t48_4;

    // 4-BLAC: (1x4)^T
    _t48_96 = _t49_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t48_97 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 0), _t49_0);
    _t48_98 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 32), _mm256_permute2f128_pd(_t48_96, _t48_96, 32), 15), _t49_0);
    _t48_99 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 0), _t49_0);
    _t48_100 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_96, _t48_96, 49), _mm256_permute2f128_pd(_t48_96, _t48_96, 49), 15), _t49_0);

    // 4-BLAC: 4x4 - 4x4
    _t49_20 = _mm256_sub_pd(_t49_15, _t48_97);
    _t49_21 = _mm256_sub_pd(_t49_16, _t48_98);
    _t49_22 = _mm256_sub_pd(_t49_17, _t48_99);
    _t49_23 = _mm256_sub_pd(_t49_18, _t48_100);

    // AVX Storer:
    _t49_1 = _t49_20;
    _t49_2 = _t49_21;

    // Generating : U[28,28] = S(h(1, 28, 2), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 2)) Kro G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_1 = _mm256_mul_pd(_t49_24, _t49_1);

    // AVX Storer:

    // Generating : U[28,28] = S(h(1, 28, 3), ( G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) - ( T( G(h(1, 28, 2), U[28,28],h(1, 28, 3)) ) Kro G(h(1, 28, 2), U[28,28],h(4, 28, fi1304 + 4)) ) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_7, _t48_7, 32), _mm256_permute2f128_pd(_t48_7, _t48_7, 32), 0);

    // 4-BLAC: (4x1)^T
    _t48_107 = _t49_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_15 = _mm256_mul_pd(_t48_107, _t49_1);

    // 4-BLAC: 1x4 - 1x4
    _t49_2 = _mm256_sub_pd(_t49_2, _t48_15);

    // AVX Storer:

    // Generating : U[28,28] = S(h(1, 28, 3), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 3)) Kro G(h(1, 28, 3), U[28,28],h(4, 28, fi1304 + 4)) ),h(4, 28, fi1304 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t49_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_10, _t48_10, 32), _mm256_permute2f128_pd(_t48_10, _t48_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t49_2 = _mm256_mul_pd(_t49_26, _t49_2);

    // AVX Storer:
    _asm256_storeu_pd(M3 + fi1304 + 4, _t49_3);
    _asm256_storeu_pd(M3 + fi1304 + 32, _t49_0);
    _asm256_storeu_pd(M3 + fi1304 + 60, _t49_1);
    _asm256_storeu_pd(M3 + fi1304 + 88, _t49_2);
  }

  _t50_0 = _asm256_loadu_pd(M3 + 116);
  _t50_1 = _mm256_maskload_pd(M3 + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t50_2 = _mm256_maskload_pd(M3 + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t50_3 = _mm256_maskload_pd(M3 + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : U[28,28] = S(h(4, 28, 4), ( G(h(4, 28, 4), M4[28,28],h(4, 28, 4)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, 4)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, 4)) ) ),h(4, 28, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t50_21 = _t50_0;
  _t50_22 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_0, _t50_1, 3), _t50_1, 12);
  _t50_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_0, _t50_1, 0), _t50_2, 49);
  _t50_24 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_0, _t50_1, 12), _mm256_shuffle_pd(_t50_2, _t50_3, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t50_78 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 32);
  _t50_79 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 32);
  _t50_80 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 49);
  _t50_81 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t50_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 32), _mm256_permute2f128_pd(_t50_78, _t50_78, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 32), _mm256_permute2f128_pd(_t50_78, _t50_78, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 49), _mm256_permute2f128_pd(_t50_78, _t50_78, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_78, _t50_78, 49), _mm256_permute2f128_pd(_t50_78, _t50_78, 49), 15), _t48_13)));
  _t50_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 32), _mm256_permute2f128_pd(_t50_79, _t50_79, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 32), _mm256_permute2f128_pd(_t50_79, _t50_79, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 49), _mm256_permute2f128_pd(_t50_79, _t50_79, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_79, _t50_79, 49), _mm256_permute2f128_pd(_t50_79, _t50_79, 49), 15), _t48_13)));
  _t50_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 32), _mm256_permute2f128_pd(_t50_80, _t50_80, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 32), _mm256_permute2f128_pd(_t50_80, _t50_80, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 49), _mm256_permute2f128_pd(_t50_80, _t50_80, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_80, _t50_80, 49), _mm256_permute2f128_pd(_t50_80, _t50_80, 49), 15), _t48_13)));
  _t50_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 32), _mm256_permute2f128_pd(_t50_81, _t50_81, 32), 0), _t48_14), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 32), _mm256_permute2f128_pd(_t50_81, _t50_81, 32), 15), _t48_11)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 49), _mm256_permute2f128_pd(_t50_81, _t50_81, 49), 0), _t48_12), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_81, _t50_81, 49), _mm256_permute2f128_pd(_t50_81, _t50_81, 49), 15), _t48_13)));

  // 4-BLAC: 4x4 - 4x4
  _t50_17 = _mm256_sub_pd(_t50_21, _t50_13);
  _t50_18 = _mm256_sub_pd(_t50_22, _t50_14);
  _t50_19 = _mm256_sub_pd(_t50_23, _t50_15);
  _t50_20 = _mm256_sub_pd(_t50_24, _t50_16);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t50_0 = _t50_17;
  _t50_1 = _t50_18;
  _t50_2 = _t50_19;
  _t50_3 = _t50_20;

  // Generating : U[28,28] = S(h(1, 28, 4), Sqrt( G(h(1, 28, 4), U[28,28],h(1, 28, 4)) ),h(1, 28, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t50_0, 1);

  // 4-BLAC: sqrt(1x4)
  _t50_26 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_25)));

  // AVX Storer:
  _t50_4 = _t50_26;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 4), U[28,28],h(1, 28, 4)) ),h(1, 28, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t50_27 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_28 = _t50_4;

  // 4-BLAC: 1x4 / 1x4
  _t50_29 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t50_27), _mm256_castpd256_pd128(_t50_28)));

  // AVX Storer:
  _t50_5 = _t50_29;

  // Generating : U[28,28] = S(h(1, 28, 4), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 4)) Kro G(h(1, 28, 4), U[28,28],h(3, 28, 5)) ),h(3, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_30 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_5, _t50_5, 32), _mm256_permute2f128_pd(_t50_5, _t50_5, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t50_31 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_0, 14), _mm256_permute2f128_pd(_t50_0, _t50_0, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t50_32 = _mm256_mul_pd(_t50_30, _t50_31);

  // AVX Storer:
  _t50_6 = _t50_32;

  // Generating : U[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), U[28,28],h(1, 28, 5)) - ( T( G(h(1, 28, 4), U[28,28],h(1, 28, 5)) ) Kro G(h(1, 28, 4), U[28,28],h(1, 28, 5)) ) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_33 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_1, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_34 = _mm256_blend_pd(_mm256_setzero_pd(), _t50_6, 1);

  // 4-BLAC: (4x1)^T
  _t50_35 = _t50_34;

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_36 = _mm256_blend_pd(_mm256_setzero_pd(), _t50_6, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t50_37 = _mm256_mul_pd(_t50_35, _t50_36);

  // 4-BLAC: 1x4 - 1x4
  _t50_38 = _mm256_sub_pd(_t50_33, _t50_37);

  // AVX Storer:
  _t50_7 = _t50_38;

  // Generating : U[28,28] = S(h(1, 28, 5), Sqrt( G(h(1, 28, 5), U[28,28],h(1, 28, 5)) ),h(1, 28, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_39 = _t50_7;

  // 4-BLAC: sqrt(1x4)
  _t50_40 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_39)));

  // AVX Storer:
  _t50_7 = _t50_40;

  // Generating : U[28,28] = S(h(1, 28, 5), ( G(h(1, 28, 5), U[28,28],h(2, 28, 6)) - ( T( G(h(1, 28, 4), U[28,28],h(1, 28, 5)) ) Kro G(h(1, 28, 4), U[28,28],h(2, 28, 6)) ) ),h(2, 28, 6))

  // AVX Loader:

  // 1x2 -> 1x4
  _t50_41 = _mm256_permute2f128_pd(_t50_1, _t50_1, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_6, _t50_6, 32), _mm256_permute2f128_pd(_t50_6, _t50_6, 32), 0);

  // 4-BLAC: (4x1)^T
  _t50_43 = _t50_42;

  // AVX Loader:

  // 1x2 -> 1x4
  _t50_44 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_6, 6), _mm256_permute2f128_pd(_t50_6, _t50_6, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t50_45 = _mm256_mul_pd(_t50_43, _t50_44);

  // 4-BLAC: 1x4 - 1x4
  _t50_46 = _mm256_sub_pd(_t50_41, _t50_45);

  // AVX Storer:
  _t50_8 = _t50_46;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 5), U[28,28],h(1, 28, 5)) ),h(1, 28, 5))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t50_47 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_48 = _t50_7;

  // 4-BLAC: 1x4 / 1x4
  _t50_49 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t50_47), _mm256_castpd256_pd128(_t50_48)));

  // AVX Storer:
  _t50_9 = _t50_49;

  // Generating : U[28,28] = S(h(1, 28, 5), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 5)) Kro G(h(1, 28, 5), U[28,28],h(2, 28, 6)) ),h(2, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_9, _t50_9, 32), _mm256_permute2f128_pd(_t50_9, _t50_9, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t50_51 = _t50_8;

  // 4-BLAC: 1x4 Kro 1x4
  _t50_52 = _mm256_mul_pd(_t50_50, _t50_51);

  // AVX Storer:
  _t50_8 = _t50_52;

  // Generating : U[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), U[28,28],h(1, 28, 6)) - ( T( G(h(2, 28, 4), U[28,28],h(1, 28, 6)) ) * G(h(2, 28, 4), U[28,28],h(1, 28, 6)) ) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_53 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t50_2, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t50_2, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_54 = _mm256_shuffle_pd(_mm256_blend_pd(_t50_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t50_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t50_55 = _t50_54;

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_56 = _mm256_shuffle_pd(_mm256_blend_pd(_t50_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t50_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t50_57 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_mul_pd(_t50_55, _t50_56), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_mul_pd(_t50_55, _t50_56), 129)), _mm256_add_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_55, _t50_56), _mm256_mul_pd(_t50_55, _t50_56), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t50_58 = _mm256_sub_pd(_t50_53, _t50_57);

  // AVX Storer:
  _t50_10 = _t50_58;

  // Generating : U[28,28] = S(h(1, 28, 6), Sqrt( G(h(1, 28, 6), U[28,28],h(1, 28, 6)) ),h(1, 28, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_59 = _t50_10;

  // 4-BLAC: sqrt(1x4)
  _t50_60 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_59)));

  // AVX Storer:
  _t50_10 = _t50_60;

  // Generating : U[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), U[28,28],h(1, 28, 7)) - ( T( G(h(2, 28, 4), U[28,28],h(1, 28, 6)) ) * G(h(2, 28, 4), U[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_61 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t50_2, _t50_2, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_62 = _mm256_shuffle_pd(_mm256_blend_pd(_t50_6, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t50_8, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t50_63 = _t50_62;

  // AVX Loader:

  // 2x1 -> 4x1
  _t50_64 = _mm256_blend_pd(_mm256_permute2f128_pd(_t50_6, _t50_6, 129), _t50_8, 2);

  // 4-BLAC: 1x4 * 4x1
  _t50_65 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_mul_pd(_t50_63, _t50_64), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_mul_pd(_t50_63, _t50_64), 129)), _mm256_add_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_63, _t50_64), _mm256_mul_pd(_t50_63, _t50_64), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t50_66 = _mm256_sub_pd(_t50_61, _t50_65);

  // AVX Storer:
  _t50_11 = _t50_66;

  // Generating : U[28,28] = S(h(1, 28, 6), ( G(h(1, 28, 6), U[28,28],h(1, 28, 7)) Div G(h(1, 28, 6), U[28,28],h(1, 28, 6)) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_67 = _t50_11;

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_68 = _t50_10;

  // 4-BLAC: 1x4 / 1x4
  _t50_69 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t50_67), _mm256_castpd256_pd128(_t50_68)));

  // AVX Storer:
  _t50_11 = _t50_69;

  // Generating : U[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), U[28,28],h(1, 28, 7)) - ( T( G(h(3, 28, 4), U[28,28],h(1, 28, 7)) ) * G(h(3, 28, 4), U[28,28],h(1, 28, 7)) ) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_70 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t50_3, _t50_3, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t50_71 = _mm256_blend_pd(_mm256_permute2f128_pd(_t50_6, _t50_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t50_8, 2), 10);

  // 4-BLAC: (4x1)^T
  _t50_72 = _t50_71;

  // AVX Loader:

  // 3x1 -> 4x1
  _t50_73 = _mm256_blend_pd(_mm256_permute2f128_pd(_t50_6, _t50_11, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t50_8, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t50_74 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_mul_pd(_t50_72, _t50_73), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_mul_pd(_t50_72, _t50_73), 129)), _mm256_add_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_permute2f128_pd(_mm256_mul_pd(_t50_72, _t50_73), _mm256_mul_pd(_t50_72, _t50_73), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t50_75 = _mm256_sub_pd(_t50_70, _t50_74);

  // AVX Storer:
  _t50_12 = _t50_75;

  // Generating : U[28,28] = S(h(1, 28, 7), Sqrt( G(h(1, 28, 7), U[28,28],h(1, 28, 7)) ),h(1, 28, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t50_76 = _t50_12;

  // 4-BLAC: sqrt(1x4)
  _t50_77 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t50_76)));

  // AVX Storer:
  _t50_12 = _t50_77;

  // Generating : U[28,28] = Sum_{k2} ( S(h(4, 28, 4), ( G(h(4, 28, 4), M4[28,28],h(4, 28, k2 + 8)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, 4)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, k2 + 8)) ) ),h(4, 28, k2 + 8)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t50_82 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 32);
  _t50_83 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 32);
  _t50_84 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_14, _t48_11), _mm256_unpacklo_pd(_t48_12, _t48_13), 49);
  _t50_85 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_14, _t48_11), _mm256_unpackhi_pd(_t48_12, _t48_13), 49);


  for( int k2 = 0; k2 <= 19; k2+=4 ) {
    _t51_8 = _asm256_loadu_pd(M3 + k2 + 120);
    _t51_9 = _asm256_loadu_pd(M3 + k2 + 148);
    _t51_10 = _asm256_loadu_pd(M3 + k2 + 176);
    _t51_11 = _asm256_loadu_pd(M3 + k2 + 204);
    _t51_3 = _asm256_loadu_pd(M3 + k2 + 8);
    _t51_2 = _asm256_loadu_pd(M3 + k2 + 36);
    _t51_1 = _asm256_loadu_pd(M3 + k2 + 64);
    _t51_0 = _asm256_loadu_pd(M3 + k2 + 92);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t51_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 32), _mm256_permute2f128_pd(_t50_82, _t50_82, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 32), _mm256_permute2f128_pd(_t50_82, _t50_82, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 49), _mm256_permute2f128_pd(_t50_82, _t50_82, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_82, _t50_82, 49), _mm256_permute2f128_pd(_t50_82, _t50_82, 49), 15), _t51_0)));
    _t51_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 32), _mm256_permute2f128_pd(_t50_83, _t50_83, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 32), _mm256_permute2f128_pd(_t50_83, _t50_83, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 49), _mm256_permute2f128_pd(_t50_83, _t50_83, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_83, _t50_83, 49), _mm256_permute2f128_pd(_t50_83, _t50_83, 49), 15), _t51_0)));
    _t51_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 32), _mm256_permute2f128_pd(_t50_84, _t50_84, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 32), _mm256_permute2f128_pd(_t50_84, _t50_84, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 49), _mm256_permute2f128_pd(_t50_84, _t50_84, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_84, _t50_84, 49), _mm256_permute2f128_pd(_t50_84, _t50_84, 49), 15), _t51_0)));
    _t51_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 32), _mm256_permute2f128_pd(_t50_85, _t50_85, 32), 0), _t51_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 32), _mm256_permute2f128_pd(_t50_85, _t50_85, 32), 15), _t51_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 49), _mm256_permute2f128_pd(_t50_85, _t50_85, 49), 0), _t51_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_85, _t50_85, 49), _mm256_permute2f128_pd(_t50_85, _t50_85, 49), 15), _t51_0)));

    // 4-BLAC: 4x4 - 4x4
    _t51_8 = _mm256_sub_pd(_t51_8, _t51_4);
    _t51_9 = _mm256_sub_pd(_t51_9, _t51_5);
    _t51_10 = _mm256_sub_pd(_t51_10, _t51_6);
    _t51_11 = _mm256_sub_pd(_t51_11, _t51_7);

    // AVX Storer:
    _asm256_storeu_pd(M3 + k2 + 120, _t51_8);
    _asm256_storeu_pd(M3 + k2 + 148, _t51_9);
    _asm256_storeu_pd(M3 + k2 + 176, _t51_10);
    _asm256_storeu_pd(M3 + k2 + 204, _t51_11);
  }

  _t52_5 = _asm256_loadu_pd(M3 + 120);
  _t52_2 = _asm256_loadu_pd(M3 + 148);
  _t52_3 = _asm256_loadu_pd(M3 + 176);
  _t52_4 = _asm256_loadu_pd(M3 + 204);

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 6), U[28,28],h(1, 28, 6)) ),h(1, 28, 6))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t52_7 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_8 = _t50_10;

  // 4-BLAC: 1x4 / 1x4
  _t52_9 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_7), _mm256_castpd256_pd128(_t52_8)));

  // AVX Storer:
  _t52_0 = _t52_9;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 7), U[28,28],h(1, 28, 7)) ),h(1, 28, 7))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t52_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_11 = _t50_12;

  // 4-BLAC: 1x4 / 1x4
  _t52_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t52_10), _mm256_castpd256_pd128(_t52_11)));

  // AVX Storer:
  _t52_1 = _t52_12;

  // Generating : U[28,28] = S(h(1, 28, 4), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 4)) Kro G(h(1, 28, 4), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_5, _t50_5, 32), _mm256_permute2f128_pd(_t50_5, _t50_5, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_5 = _mm256_mul_pd(_t52_13, _t52_5);

  // AVX Storer:

  // Generating : U[28,28] = S(h(3, 28, 5), ( G(h(3, 28, 5), U[28,28],h(4, 28, fi1429 + 8)) - ( T( G(h(1, 28, 4), U[28,28],h(3, 28, 5)) ) * G(h(1, 28, 4), U[28,28],h(4, 28, fi1429 + 8)) ) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // 3x4 -> 4x4
  _t52_14 = _t52_2;
  _t52_15 = _t52_3;
  _t52_16 = _t52_4;
  _t52_17 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t52_18 = _t50_6;

  // 4-BLAC: (1x4)^T
  _t52_19 = _t52_18;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t52_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 0), _t52_5);
  _t52_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 15), _t52_5);
  _t52_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 0), _t52_5);
  _t52_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 15), _t52_5);

  // 4-BLAC: 4x4 - 4x4
  _t52_24 = _mm256_sub_pd(_t52_14, _t52_20);
  _t52_25 = _mm256_sub_pd(_t52_15, _t52_21);
  _t52_26 = _mm256_sub_pd(_t52_16, _t52_22);
  _t52_27 = _mm256_sub_pd(_t52_17, _t52_23);

  // AVX Storer:
  _t52_2 = _t52_24;
  _t52_3 = _t52_25;
  _t52_4 = _t52_26;

  // Generating : U[28,28] = S(h(1, 28, 5), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 5)) Kro G(h(1, 28, 5), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_9, _t50_9, 32), _mm256_permute2f128_pd(_t50_9, _t50_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_2 = _mm256_mul_pd(_t52_28, _t52_2);

  // AVX Storer:

  // Generating : U[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), U[28,28],h(4, 28, fi1429 + 8)) - ( T( G(h(1, 28, 5), U[28,28],h(2, 28, 6)) ) * G(h(1, 28, 5), U[28,28],h(4, 28, fi1429 + 8)) ) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // 2x4 -> 4x4
  _t52_29 = _t52_3;
  _t52_30 = _t52_4;
  _t52_31 = _mm256_setzero_pd();
  _t52_32 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t52_33 = _t50_8;

  // 4-BLAC: (1x4)^T
  _t52_34 = _t52_33;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t52_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 0), _t52_2);
  _t52_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 15), _t52_2);
  _t52_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 0), _t52_2);
  _t52_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 15), _t52_2);

  // 4-BLAC: 4x4 - 4x4
  _t52_39 = _mm256_sub_pd(_t52_29, _t52_35);
  _t52_40 = _mm256_sub_pd(_t52_30, _t52_36);
  _t52_41 = _mm256_sub_pd(_t52_31, _t52_37);
  _t52_42 = _mm256_sub_pd(_t52_32, _t52_38);

  // AVX Storer:
  _t52_3 = _t52_39;
  _t52_4 = _t52_40;

  // Generating : U[28,28] = S(h(1, 28, 6), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 6)) Kro G(h(1, 28, 6), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_0, _t52_0, 32), _mm256_permute2f128_pd(_t52_0, _t52_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_3 = _mm256_mul_pd(_t52_43, _t52_3);

  // AVX Storer:

  // Generating : U[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), U[28,28],h(4, 28, fi1429 + 8)) - ( T( G(h(1, 28, 6), U[28,28],h(1, 28, 7)) ) Kro G(h(1, 28, 6), U[28,28],h(4, 28, fi1429 + 8)) ) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_11, _t50_11, 32), _mm256_permute2f128_pd(_t50_11, _t50_11, 32), 0);

  // 4-BLAC: (4x1)^T
  _t52_45 = _t52_44;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_6 = _mm256_mul_pd(_t52_45, _t52_3);

  // 4-BLAC: 1x4 - 1x4
  _t52_4 = _mm256_sub_pd(_t52_4, _t52_6);

  // AVX Storer:

  // Generating : U[28,28] = S(h(1, 28, 7), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 7)) Kro G(h(1, 28, 7), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

  // AVX Loader:

  // 1x1 -> 1x4
  _t52_46 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_1, _t52_1, 32), _mm256_permute2f128_pd(_t52_1, _t52_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_4 = _mm256_mul_pd(_t52_46, _t52_4);

  // AVX Storer:

  _asm256_storeu_pd(M3 + 116, _t50_0);
  _mm256_maskstore_pd(M3 + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t50_1);
  _mm256_maskstore_pd(M3 + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t50_2);

  for( int fi1429 = 4; fi1429 <= 16; fi1429+=4 ) {
    _t53_3 = _asm256_loadu_pd(M3 + fi1429 + 120);
    _t53_0 = _asm256_loadu_pd(M3 + fi1429 + 148);
    _t53_1 = _asm256_loadu_pd(M3 + fi1429 + 176);
    _t53_2 = _asm256_loadu_pd(M3 + fi1429 + 204);

    // Generating : U[28,28] = S(h(1, 28, 4), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 4)) Kro G(h(1, 28, 4), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_5, _t50_5, 32), _mm256_permute2f128_pd(_t50_5, _t50_5, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_3 = _mm256_mul_pd(_t53_4, _t53_3);

    // AVX Storer:

    // Generating : U[28,28] = S(h(3, 28, 5), ( G(h(3, 28, 5), U[28,28],h(4, 28, fi1429 + 8)) - ( T( G(h(1, 28, 4), U[28,28],h(3, 28, 5)) ) * G(h(1, 28, 4), U[28,28],h(4, 28, fi1429 + 8)) ) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // 3x4 -> 4x4
    _t53_5 = _t53_0;
    _t53_6 = _t53_1;
    _t53_7 = _t53_2;
    _t53_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t53_9 = _t50_6;

    // 4-BLAC: (1x4)^T
    _t52_19 = _t53_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t52_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 0), _t53_3);
    _t52_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 32), _mm256_permute2f128_pd(_t52_19, _t52_19, 32), 15), _t53_3);
    _t52_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 0), _t53_3);
    _t52_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_19, _t52_19, 49), _mm256_permute2f128_pd(_t52_19, _t52_19, 49), 15), _t53_3);

    // 4-BLAC: 4x4 - 4x4
    _t53_10 = _mm256_sub_pd(_t53_5, _t52_20);
    _t53_11 = _mm256_sub_pd(_t53_6, _t52_21);
    _t53_12 = _mm256_sub_pd(_t53_7, _t52_22);
    _t53_13 = _mm256_sub_pd(_t53_8, _t52_23);

    // AVX Storer:
    _t53_0 = _t53_10;
    _t53_1 = _t53_11;
    _t53_2 = _t53_12;

    // Generating : U[28,28] = S(h(1, 28, 5), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 5)) Kro G(h(1, 28, 5), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_9, _t50_9, 32), _mm256_permute2f128_pd(_t50_9, _t50_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_0 = _mm256_mul_pd(_t53_14, _t53_0);

    // AVX Storer:

    // Generating : U[28,28] = S(h(2, 28, 6), ( G(h(2, 28, 6), U[28,28],h(4, 28, fi1429 + 8)) - ( T( G(h(1, 28, 5), U[28,28],h(2, 28, 6)) ) * G(h(1, 28, 5), U[28,28],h(4, 28, fi1429 + 8)) ) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // 2x4 -> 4x4
    _t53_15 = _t53_1;
    _t53_16 = _t53_2;
    _t53_17 = _mm256_setzero_pd();
    _t53_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t53_19 = _t50_8;

    // 4-BLAC: (1x4)^T
    _t52_34 = _t53_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t52_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 0), _t53_0);
    _t52_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 32), _mm256_permute2f128_pd(_t52_34, _t52_34, 32), 15), _t53_0);
    _t52_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 0), _t53_0);
    _t52_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_34, _t52_34, 49), _mm256_permute2f128_pd(_t52_34, _t52_34, 49), 15), _t53_0);

    // 4-BLAC: 4x4 - 4x4
    _t53_20 = _mm256_sub_pd(_t53_15, _t52_35);
    _t53_21 = _mm256_sub_pd(_t53_16, _t52_36);
    _t53_22 = _mm256_sub_pd(_t53_17, _t52_37);
    _t53_23 = _mm256_sub_pd(_t53_18, _t52_38);

    // AVX Storer:
    _t53_1 = _t53_20;
    _t53_2 = _t53_21;

    // Generating : U[28,28] = S(h(1, 28, 6), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 6)) Kro G(h(1, 28, 6), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_0, _t52_0, 32), _mm256_permute2f128_pd(_t52_0, _t52_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_1 = _mm256_mul_pd(_t53_24, _t53_1);

    // AVX Storer:

    // Generating : U[28,28] = S(h(1, 28, 7), ( G(h(1, 28, 7), U[28,28],h(4, 28, fi1429 + 8)) - ( T( G(h(1, 28, 6), U[28,28],h(1, 28, 7)) ) Kro G(h(1, 28, 6), U[28,28],h(4, 28, fi1429 + 8)) ) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t50_11, _t50_11, 32), _mm256_permute2f128_pd(_t50_11, _t50_11, 32), 0);

    // 4-BLAC: (4x1)^T
    _t52_45 = _t53_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t52_6 = _mm256_mul_pd(_t52_45, _t53_1);

    // 4-BLAC: 1x4 - 1x4
    _t53_2 = _mm256_sub_pd(_t53_2, _t52_6);

    // AVX Storer:

    // Generating : U[28,28] = S(h(1, 28, 7), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 7)) Kro G(h(1, 28, 7), U[28,28],h(4, 28, fi1429 + 8)) ),h(4, 28, fi1429 + 8))

    // AVX Loader:

    // 1x1 -> 1x4
    _t53_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t52_1, _t52_1, 32), _mm256_permute2f128_pd(_t52_1, _t52_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t53_2 = _mm256_mul_pd(_t53_26, _t53_2);

    // AVX Storer:
    _asm256_storeu_pd(M3 + fi1429 + 120, _t53_3);
    _asm256_storeu_pd(M3 + fi1429 + 148, _t53_0);
    _asm256_storeu_pd(M3 + fi1429 + 176, _t53_1);
    _asm256_storeu_pd(M3 + fi1429 + 204, _t53_2);
  }

  _t54_4 = _asm256_loadu_pd(M3 + 232);
  _t54_5 = _mm256_maskload_pd(M3 + 260, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t54_6 = _mm256_maskload_pd(M3 + 288, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t54_7 = _mm256_maskload_pd(M3 + 316, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t54_3 = _asm256_loadu_pd(M3 + 8);
  _t54_2 = _asm256_loadu_pd(M3 + 36);
  _t54_1 = _asm256_loadu_pd(M3 + 64);
  _t54_0 = _asm256_loadu_pd(M3 + 92);

  // Generating : U[28,28] = ( S(h(4, 28, fi1304), ( G(h(4, 28, fi1304), M4[28,28],h(4, 28, fi1304)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304)) + Sum_{k3} ( -$(h(4, 28, fi1304), ( T( G(h(4, 28, k3), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, k3), U[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t54_29 = _t54_4;
  _t54_30 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_4, _t54_5, 3), _t54_5, 12);
  _t54_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_4, _t54_5, 0), _t54_6, 49);
  _t54_32 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_4, _t54_5, 12), _mm256_shuffle_pd(_t54_6, _t54_7, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_90 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 32);
  _t54_91 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 32);
  _t54_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 49);
  _t54_93 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t54_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 15), _t54_0)));
  _t54_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 15), _t54_0)));
  _t54_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 15), _t54_0)));
  _t54_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 0), _t54_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 15), _t54_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 0), _t54_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 15), _t54_0)));

  // 4-BLAC: 4x4 - 4x4
  _t54_25 = _mm256_sub_pd(_t54_29, _t54_17);
  _t54_26 = _mm256_sub_pd(_t54_30, _t54_18);
  _t54_27 = _mm256_sub_pd(_t54_31, _t54_19);
  _t54_28 = _mm256_sub_pd(_t54_32, _t54_20);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t54_4 = _t54_25;
  _t54_5 = _t54_26;
  _t54_6 = _t54_27;
  _t54_7 = _t54_28;

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 32);
  _t54_95 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 32);
  _t54_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 49);
  _t54_97 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t54_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 15), _t52_4)));
  _t54_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 15), _t52_4)));
  _t54_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 15), _t52_4)));
  _t54_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 0), _t52_5), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 15), _t52_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 0), _t52_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 15), _t52_4)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpperTriang
  _t54_33 = _t54_4;
  _t54_34 = _t54_5;
  _t54_35 = _t54_6;
  _t54_36 = _t54_7;

  // 4-BLAC: 4x4 - 4x4
  _t54_33 = _mm256_sub_pd(_t54_33, _t54_21);
  _t54_34 = _mm256_sub_pd(_t54_34, _t54_22);
  _t54_35 = _mm256_sub_pd(_t54_35, _t54_23);
  _t54_36 = _mm256_sub_pd(_t54_36, _t54_24);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t54_4 = _t54_33;
  _t54_5 = _t54_34;
  _t54_6 = _t54_35;
  _t54_7 = _t54_36;

  // Generating : U[28,28] = S(h(1, 28, fi1304), Sqrt( G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304)) ),h(1, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_37 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_4, 1);

  // 4-BLAC: sqrt(1x4)
  _t54_38 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_37)));

  // AVX Storer:
  _t54_8 = _t54_38;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304)) ),h(1, 28, fi1304))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t54_39 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_40 = _t54_8;

  // 4-BLAC: 1x4 / 1x4
  _t54_41 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_39), _mm256_castpd256_pd128(_t54_40)));

  // AVX Storer:
  _t54_9 = _t54_41;

  // Generating : U[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), U[28,28],h(3, 28, fi1304 + 1)) ),h(3, 28, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t54_43 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_4, 14), _mm256_permute2f128_pd(_t54_4, _t54_4, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_44 = _mm256_mul_pd(_t54_42, _t54_43);

  // AVX Storer:
  _t54_10 = _t54_44;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 28, fi1304 + 1), U[28,28],h(1, 28, fi1304 + 1)) - ( T( G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304 + 1)) ) Kro G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304 + 1)) ) ),h(1, 28, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_45 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_5, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_46 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 1);

  // 4-BLAC: (4x1)^T
  _t54_47 = _t54_46;

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_48 = _mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_49 = _mm256_mul_pd(_t54_47, _t54_48);

  // 4-BLAC: 1x4 - 1x4
  _t54_50 = _mm256_sub_pd(_t54_45, _t54_49);

  // AVX Storer:
  _t54_11 = _t54_50;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), Sqrt( G(h(1, 28, fi1304 + 1), U[28,28],h(1, 28, fi1304 + 1)) ),h(1, 28, fi1304 + 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_51 = _t54_11;

  // 4-BLAC: sqrt(1x4)
  _t54_52 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_51)));

  // AVX Storer:
  _t54_11 = _t54_52;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) - ( T( G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304 + 1)) ) Kro G(h(1, 28, fi1304), U[28,28],h(2, 28, fi1304 + 2)) ) ),h(2, 28, fi1304 + 2))

  // AVX Loader:

  // 1x2 -> 1x4
  _t54_53 = _mm256_permute2f128_pd(_t54_5, _t54_5, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_10, _t54_10, 32), _mm256_permute2f128_pd(_t54_10, _t54_10, 32), 0);

  // 4-BLAC: (4x1)^T
  _t54_55 = _t54_54;

  // AVX Loader:

  // 1x2 -> 1x4
  _t54_56 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_10, 6), _mm256_permute2f128_pd(_t54_10, _t54_10, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t54_57 = _mm256_mul_pd(_t54_55, _t54_56);

  // 4-BLAC: 1x4 - 1x4
  _t54_58 = _mm256_sub_pd(_t54_53, _t54_57);

  // AVX Storer:
  _t54_12 = _t54_58;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 1), U[28,28],h(1, 28, fi1304 + 1)) ),h(1, 28, fi1304 + 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t54_59 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_60 = _t54_11;

  // 4-BLAC: 1x4 / 1x4
  _t54_61 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_59), _mm256_castpd256_pd128(_t54_60)));

  // AVX Storer:
  _t54_13 = _t54_61;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) ),h(2, 28, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_62 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_13, _t54_13, 32), _mm256_permute2f128_pd(_t54_13, _t54_13, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t54_63 = _t54_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t54_64 = _mm256_mul_pd(_t54_62, _t54_63);

  // AVX Storer:
  _t54_12 = _t54_64;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) - ( T( G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 2)) ) * G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 2)) ) ),h(1, 28, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_65 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t54_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t54_6, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_66 = _mm256_shuffle_pd(_mm256_blend_pd(_t54_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t54_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t54_67 = _t54_66;

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_68 = _mm256_shuffle_pd(_mm256_blend_pd(_t54_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t54_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t54_69 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_mul_pd(_t54_67, _t54_68), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_mul_pd(_t54_67, _t54_68), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t54_68), _mm256_mul_pd(_t54_67, _t54_68), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t54_70 = _mm256_sub_pd(_t54_65, _t54_69);

  // AVX Storer:
  _t54_14 = _t54_70;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), Sqrt( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_71 = _t54_14;

  // 4-BLAC: sqrt(1x4)
  _t54_72 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_71)));

  // AVX Storer:
  _t54_14 = _t54_72;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) - ( T( G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 2)) ) * G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 3)) ) ),h(1, 28, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_73 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t54_6, _t54_6, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_74 = _mm256_shuffle_pd(_mm256_blend_pd(_t54_10, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t54_12, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t54_75 = _t54_74;

  // AVX Loader:

  // 2x1 -> 4x1
  _t54_76 = _mm256_blend_pd(_mm256_permute2f128_pd(_t54_10, _t54_10, 129), _t54_12, 2);

  // 4-BLAC: 1x4 * 4x1
  _t54_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_mul_pd(_t54_75, _t54_76), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_mul_pd(_t54_75, _t54_76), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t54_76), _mm256_mul_pd(_t54_75, _t54_76), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t54_78 = _mm256_sub_pd(_t54_73, _t54_77);

  // AVX Storer:
  _t54_15 = _t54_78;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) Div G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_79 = _t54_15;

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_80 = _t54_14;

  // 4-BLAC: 1x4 / 1x4
  _t54_81 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t54_79), _mm256_castpd256_pd128(_t54_80)));

  // AVX Storer:
  _t54_15 = _t54_81;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), U[28,28],h(1, 28, fi1304 + 3)) - ( T( G(h(3, 28, fi1304), U[28,28],h(1, 28, fi1304 + 3)) ) * G(h(3, 28, fi1304), U[28,28],h(1, 28, fi1304 + 3)) ) ),h(1, 28, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_82 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t54_7, _t54_7, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t54_83 = _mm256_blend_pd(_mm256_permute2f128_pd(_t54_10, _t54_15, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t54_12, 2), 10);

  // 4-BLAC: (4x1)^T
  _t54_84 = _t54_83;

  // AVX Loader:

  // 3x1 -> 4x1
  _t54_85 = _mm256_blend_pd(_mm256_permute2f128_pd(_t54_10, _t54_15, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t54_12, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t54_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_mul_pd(_t54_84, _t54_85), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_mul_pd(_t54_84, _t54_85), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t54_85), _mm256_mul_pd(_t54_84, _t54_85), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t54_87 = _mm256_sub_pd(_t54_82, _t54_86);

  // AVX Storer:
  _t54_16 = _t54_87;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), Sqrt( G(h(1, 28, fi1304 + 3), U[28,28],h(1, 28, fi1304 + 3)) ),h(1, 28, fi1304 + 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t54_88 = _t54_16;

  // 4-BLAC: sqrt(1x4)
  _t54_89 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t54_88)));

  // AVX Storer:
  _t54_16 = _t54_89;

  // Generating : U[28,28] = ( Sum_{k2} ( S(h(4, 28, fi1304), ( G(h(4, 28, fi1304), M4[28,28],h(4, 28, fi1304 + k2 + 4)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, fi1304 + k2 + 4)) ) ),h(4, 28, fi1304 + k2 + 4)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 28, fi1304), ( T( G(h(4, 28, k3), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, k3), U[28,28],h(4, 28, fi1304 + k2 + 4)) ),h(4, 28, fi1304 + k2 + 4)) ) ) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 32);
  _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 32);
  _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 49);
  _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 49);


  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t55_8 = _asm256_loadu_pd(M3 + k2 + 236);
    _t55_9 = _asm256_loadu_pd(M3 + k2 + 264);
    _t55_10 = _asm256_loadu_pd(M3 + k2 + 292);
    _t55_11 = _asm256_loadu_pd(M3 + k2 + 320);
    _t55_3 = _asm256_loadu_pd(M3 + k2 + 12);
    _t55_2 = _asm256_loadu_pd(M3 + k2 + 40);
    _t55_1 = _asm256_loadu_pd(M3 + k2 + 68);
    _t55_0 = _asm256_loadu_pd(M3 + k2 + 96);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 32);
    _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 32);
    _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t54_3, _t54_2), _mm256_unpacklo_pd(_t54_1, _t54_0), 49);
    _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t54_3, _t54_2), _mm256_unpackhi_pd(_t54_1, _t54_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t55_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 15), _t55_0)));
    _t55_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 15), _t55_0)));
    _t55_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 15), _t55_0)));
    _t55_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 0), _t55_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 15), _t55_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 0), _t55_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 15), _t55_0)));

    // 4-BLAC: 4x4 - 4x4
    _t55_8 = _mm256_sub_pd(_t55_8, _t55_4);
    _t55_9 = _mm256_sub_pd(_t55_9, _t55_5);
    _t55_10 = _mm256_sub_pd(_t55_10, _t55_6);
    _t55_11 = _mm256_sub_pd(_t55_11, _t55_7);

    // AVX Storer:
    _asm256_storeu_pd(M3 + k2 + 236, _t55_8);
    _asm256_storeu_pd(M3 + k2 + 264, _t55_9);
    _asm256_storeu_pd(M3 + k2 + 292, _t55_10);
    _asm256_storeu_pd(M3 + k2 + 320, _t55_11);
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 32);
  _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 32);
  _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 49);
  _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 49);


  for( int k2 = 0; k2 <= 15; k2+=4 ) {
    _t57_3 = _asm256_loadu_pd(M3 + k2 + 124);
    _t57_2 = _asm256_loadu_pd(M3 + k2 + 152);
    _t57_1 = _asm256_loadu_pd(M3 + k2 + 180);
    _t57_0 = _asm256_loadu_pd(M3 + k2 + 208);
    _t57_4 = _asm256_loadu_pd(M3 + k2 + 236);
    _t57_5 = _asm256_loadu_pd(M3 + k2 + 264);
    _t57_6 = _asm256_loadu_pd(M3 + k2 + 292);
    _t57_7 = _asm256_loadu_pd(M3 + k2 + 320);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 32);
    _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 32);
    _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_5, _t52_2), _mm256_unpacklo_pd(_t52_3, _t52_4), 49);
    _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_5, _t52_2), _mm256_unpackhi_pd(_t52_3, _t52_4), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t57_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 15), _t57_0)));
    _t57_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 15), _t57_0)));
    _t57_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 15), _t57_0)));
    _t57_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 0), _t57_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 15), _t57_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 0), _t57_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 15), _t57_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 - 4x4
    _t57_4 = _mm256_sub_pd(_t57_4, _t57_8);
    _t57_5 = _mm256_sub_pd(_t57_5, _t57_9);
    _t57_6 = _mm256_sub_pd(_t57_6, _t57_10);
    _t57_7 = _mm256_sub_pd(_t57_7, _t57_11);

    // AVX Storer:
    _asm256_storeu_pd(M3 + k2 + 236, _t57_4);
    _asm256_storeu_pd(M3 + k2 + 264, _t57_5);
    _asm256_storeu_pd(M3 + k2 + 292, _t57_6);
    _asm256_storeu_pd(M3 + k2 + 320, _t57_7);
  }

  _t58_5 = _asm256_loadu_pd(M3 + 236);
  _t58_2 = _asm256_loadu_pd(M3 + 264);
  _t58_3 = _asm256_loadu_pd(M3 + 292);
  _t58_4 = _asm256_loadu_pd(M3 + 320);

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_7 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_8 = _t54_14;

  // 4-BLAC: 1x4 / 1x4
  _t58_9 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t58_7), _mm256_castpd256_pd128(_t58_8)));

  // AVX Storer:
  _t58_0 = _t58_9;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 3), U[28,28],h(1, 28, fi1304 + 3)) ),h(1, 28, fi1304 + 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t58_10 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_11 = _t54_16;

  // 4-BLAC: 1x4 / 1x4
  _t58_12 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t58_10), _mm256_castpd256_pd128(_t58_11)));

  // AVX Storer:
  _t58_1 = _t58_12;

  // Generating : U[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_13 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_5 = _mm256_mul_pd(_t58_13, _t58_5);

  // AVX Storer:

  // Generating : U[28,28] = S(h(3, 28, fi1304 + 1), ( G(h(3, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304), U[28,28],h(3, 28, fi1304 + 1)) ) * G(h(1, 28, fi1304), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t58_14 = _t58_2;
  _t58_15 = _t58_3;
  _t58_16 = _t58_4;
  _t58_17 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t58_18 = _t54_10;

  // 4-BLAC: (1x4)^T
  _t58_19 = _t58_18;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 0), _t58_5);
  _t58_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 15), _t58_5);
  _t58_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 0), _t58_5);
  _t58_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 15), _t58_5);

  // 4-BLAC: 4x4 - 4x4
  _t58_24 = _mm256_sub_pd(_t58_14, _t58_20);
  _t58_25 = _mm256_sub_pd(_t58_15, _t58_21);
  _t58_26 = _mm256_sub_pd(_t58_16, _t58_22);
  _t58_27 = _mm256_sub_pd(_t58_17, _t58_23);

  // AVX Storer:
  _t58_2 = _t58_24;
  _t58_3 = _t58_25;
  _t58_4 = _t58_26;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_13, _t54_13, 32), _mm256_permute2f128_pd(_t54_13, _t54_13, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_2 = _mm256_mul_pd(_t58_28, _t58_2);

  // AVX Storer:

  // Generating : U[28,28] = S(h(2, 28, fi1304 + 2), ( G(h(2, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) ) * G(h(1, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t58_29 = _t58_3;
  _t58_30 = _t58_4;
  _t58_31 = _mm256_setzero_pd();
  _t58_32 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t58_33 = _t54_12;

  // 4-BLAC: (1x4)^T
  _t58_34 = _t58_33;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t58_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 32), _mm256_permute2f128_pd(_t58_34, _t58_34, 32), 0), _t58_2);
  _t58_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 32), _mm256_permute2f128_pd(_t58_34, _t58_34, 32), 15), _t58_2);
  _t58_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 49), _mm256_permute2f128_pd(_t58_34, _t58_34, 49), 0), _t58_2);
  _t58_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 49), _mm256_permute2f128_pd(_t58_34, _t58_34, 49), 15), _t58_2);

  // 4-BLAC: 4x4 - 4x4
  _t58_39 = _mm256_sub_pd(_t58_29, _t58_35);
  _t58_40 = _mm256_sub_pd(_t58_30, _t58_36);
  _t58_41 = _mm256_sub_pd(_t58_31, _t58_37);
  _t58_42 = _mm256_sub_pd(_t58_32, _t58_38);

  // AVX Storer:
  _t58_3 = _t58_39;
  _t58_4 = _t58_40;

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 2)) Kro G(h(1, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_0, _t58_0, 32), _mm256_permute2f128_pd(_t58_0, _t58_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_3 = _mm256_mul_pd(_t58_43, _t58_3);

  // AVX Storer:

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) ) Kro G(h(1, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_44 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_15, _t54_15, 32), _mm256_permute2f128_pd(_t54_15, _t54_15, 32), 0);

  // 4-BLAC: (4x1)^T
  _t58_45 = _t58_44;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_6 = _mm256_mul_pd(_t58_45, _t58_3);

  // 4-BLAC: 1x4 - 1x4
  _t58_4 = _mm256_sub_pd(_t58_4, _t58_6);

  // AVX Storer:

  // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 3)) Kro G(h(1, 28, fi1304 + 3), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t58_46 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_1, _t58_1, 32), _mm256_permute2f128_pd(_t58_1, _t58_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t58_4 = _mm256_mul_pd(_t58_46, _t58_4);

  // AVX Storer:

  _asm256_storeu_pd(M3 + 232, _t54_4);
  _mm256_maskstore_pd(M3 + 260, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t54_5);
  _mm256_maskstore_pd(M3 + 288, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t54_6);

  for( int fi1429 = 4; fi1429 <= 12; fi1429+=4 ) {
    _t59_3 = _asm256_loadu_pd(M3 + fi1429 + 236);
    _t59_0 = _asm256_loadu_pd(M3 + fi1429 + 264);
    _t59_1 = _asm256_loadu_pd(M3 + fi1429 + 292);
    _t59_2 = _asm256_loadu_pd(M3 + fi1429 + 320);

    // Generating : U[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_9, _t54_9, 32), _mm256_permute2f128_pd(_t54_9, _t54_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t59_3 = _mm256_mul_pd(_t59_4, _t59_3);

    // AVX Storer:

    // Generating : U[28,28] = S(h(3, 28, fi1304 + 1), ( G(h(3, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304), U[28,28],h(3, 28, fi1304 + 1)) ) * G(h(1, 28, fi1304), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // 3x4 -> 4x4
    _t59_5 = _t59_0;
    _t59_6 = _t59_1;
    _t59_7 = _t59_2;
    _t59_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t59_9 = _t54_10;

    // 4-BLAC: (1x4)^T
    _t58_19 = _t59_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t58_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 0), _t59_3);
    _t58_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 15), _t59_3);
    _t58_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 0), _t59_3);
    _t58_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 15), _t59_3);

    // 4-BLAC: 4x4 - 4x4
    _t59_10 = _mm256_sub_pd(_t59_5, _t58_20);
    _t59_11 = _mm256_sub_pd(_t59_6, _t58_21);
    _t59_12 = _mm256_sub_pd(_t59_7, _t58_22);
    _t59_13 = _mm256_sub_pd(_t59_8, _t58_23);

    // AVX Storer:
    _t59_0 = _t59_10;
    _t59_1 = _t59_11;
    _t59_2 = _t59_12;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_13, _t54_13, 32), _mm256_permute2f128_pd(_t54_13, _t54_13, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t59_0 = _mm256_mul_pd(_t59_14, _t59_0);

    // AVX Storer:

    // Generating : U[28,28] = S(h(2, 28, fi1304 + 2), ( G(h(2, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) ) * G(h(1, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // 2x4 -> 4x4
    _t59_15 = _t59_1;
    _t59_16 = _t59_2;
    _t59_17 = _mm256_setzero_pd();
    _t59_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t59_19 = _t54_12;

    // 4-BLAC: (1x4)^T
    _t58_34 = _t59_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t58_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 32), _mm256_permute2f128_pd(_t58_34, _t58_34, 32), 0), _t59_0);
    _t58_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 32), _mm256_permute2f128_pd(_t58_34, _t58_34, 32), 15), _t59_0);
    _t58_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 49), _mm256_permute2f128_pd(_t58_34, _t58_34, 49), 0), _t59_0);
    _t58_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 49), _mm256_permute2f128_pd(_t58_34, _t58_34, 49), 15), _t59_0);

    // 4-BLAC: 4x4 - 4x4
    _t59_20 = _mm256_sub_pd(_t59_15, _t58_35);
    _t59_21 = _mm256_sub_pd(_t59_16, _t58_36);
    _t59_22 = _mm256_sub_pd(_t59_17, _t58_37);
    _t59_23 = _mm256_sub_pd(_t59_18, _t58_38);

    // AVX Storer:
    _t59_1 = _t59_20;
    _t59_2 = _t59_21;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 2)) Kro G(h(1, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_0, _t58_0, 32), _mm256_permute2f128_pd(_t58_0, _t58_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t59_1 = _mm256_mul_pd(_t59_24, _t59_1);

    // AVX Storer:

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) ) Kro G(h(1, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_15, _t54_15, 32), _mm256_permute2f128_pd(_t54_15, _t54_15, 32), 0);

    // 4-BLAC: (4x1)^T
    _t58_45 = _t59_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t58_6 = _mm256_mul_pd(_t58_45, _t59_1);

    // 4-BLAC: 1x4 - 1x4
    _t59_2 = _mm256_sub_pd(_t59_2, _t58_6);

    // AVX Storer:

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 3)) Kro G(h(1, 28, fi1304 + 3), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

    // AVX Loader:

    // 1x1 -> 1x4
    _t59_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_1, _t58_1, 32), _mm256_permute2f128_pd(_t58_1, _t58_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t59_2 = _mm256_mul_pd(_t59_26, _t59_2);

    // AVX Storer:
    _asm256_storeu_pd(M3 + fi1429 + 236, _t59_3);
    _asm256_storeu_pd(M3 + fi1429 + 264, _t59_0);
    _asm256_storeu_pd(M3 + fi1429 + 292, _t59_1);
    _asm256_storeu_pd(M3 + fi1429 + 320, _t59_2);
  }

  _asm256_storeu_pd(M3 + 236, _t58_5);
  _asm256_storeu_pd(M3 + 264, _t58_2);
  _asm256_storeu_pd(M3 + 292, _t58_3);
  _asm256_storeu_pd(M3 + 320, _t58_4);

  for( int fi1304 = 12; fi1304 <= 20; fi1304+=4 ) {
    _t60_4 = _asm256_loadu_pd(M3 + 29*fi1304);
    _t60_5 = _mm256_maskload_pd(M3 + 29*fi1304 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t60_6 = _mm256_maskload_pd(M3 + 29*fi1304 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t60_7 = _mm256_maskload_pd(M3 + 29*fi1304 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t60_3 = _asm256_loadu_pd(M3 + fi1304);
    _t60_2 = _asm256_loadu_pd(M3 + fi1304 + 28);
    _t60_1 = _asm256_loadu_pd(M3 + fi1304 + 56);
    _t60_0 = _asm256_loadu_pd(M3 + fi1304 + 84);

    // Generating : U[28,28] = ( S(h(4, 28, fi1304), ( G(h(4, 28, fi1304), M4[28,28],h(4, 28, fi1304)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304)) + Sum_{k3} ( -$(h(4, 28, fi1304), ( T( G(h(4, 28, k3), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, k3), U[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304)) ) )

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t60_8 = _t60_4;
    _t60_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t60_4, _t60_5, 3), _t60_5, 12);
    _t60_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t60_4, _t60_5, 0), _t60_6, 49);
    _t60_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t60_4, _t60_5, 12), _mm256_shuffle_pd(_t60_6, _t60_7, 12), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t54_90 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 32);
    _t54_91 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 32);
    _t54_92 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 49);
    _t54_93 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t54_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 32), _mm256_permute2f128_pd(_t54_90, _t54_90, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_90, _t54_90, 49), _mm256_permute2f128_pd(_t54_90, _t54_90, 49), 15), _t60_0)));
    _t54_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 32), _mm256_permute2f128_pd(_t54_91, _t54_91, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_91, _t54_91, 49), _mm256_permute2f128_pd(_t54_91, _t54_91, 49), 15), _t60_0)));
    _t54_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 32), _mm256_permute2f128_pd(_t54_92, _t54_92, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_92, _t54_92, 49), _mm256_permute2f128_pd(_t54_92, _t54_92, 49), 15), _t60_0)));
    _t54_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 0), _t60_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 32), _mm256_permute2f128_pd(_t54_93, _t54_93, 32), 15), _t60_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 0), _t60_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_93, _t54_93, 49), _mm256_permute2f128_pd(_t54_93, _t54_93, 49), 15), _t60_0)));

    // 4-BLAC: 4x4 - 4x4
    _t54_25 = _mm256_sub_pd(_t60_8, _t54_17);
    _t54_26 = _mm256_sub_pd(_t60_9, _t54_18);
    _t54_27 = _mm256_sub_pd(_t60_10, _t54_19);
    _t54_28 = _mm256_sub_pd(_t60_11, _t54_20);

    // AVX Storer:

    // 4x4 -> 4x4 - UpTriang
    _t60_4 = _t54_25;
    _t60_5 = _t54_26;
    _t60_6 = _t54_27;
    _t60_7 = _t54_28;

    for( int k3 = 4; k3 <= fi1304 - 1; k3+=4 ) {
      _t61_3 = _asm256_loadu_pd(M3 + fi1304 + 28*k3);
      _t61_2 = _asm256_loadu_pd(M3 + fi1304 + 28*k3 + 28);
      _t61_1 = _asm256_loadu_pd(M3 + fi1304 + 28*k3 + 56);
      _t61_0 = _asm256_loadu_pd(M3 + fi1304 + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t54_94 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_3, _t61_2), _mm256_unpacklo_pd(_t61_1, _t61_0), 32);
      _t54_95 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_3, _t61_2), _mm256_unpackhi_pd(_t61_1, _t61_0), 32);
      _t54_96 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t61_3, _t61_2), _mm256_unpacklo_pd(_t61_1, _t61_0), 49);
      _t54_97 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t61_3, _t61_2), _mm256_unpackhi_pd(_t61_1, _t61_0), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t54_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 32), _mm256_permute2f128_pd(_t54_94, _t54_94, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_94, _t54_94, 49), _mm256_permute2f128_pd(_t54_94, _t54_94, 49), 15), _t61_0)));
      _t54_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 32), _mm256_permute2f128_pd(_t54_95, _t54_95, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_95, _t54_95, 49), _mm256_permute2f128_pd(_t54_95, _t54_95, 49), 15), _t61_0)));
      _t54_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 32), _mm256_permute2f128_pd(_t54_96, _t54_96, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_96, _t54_96, 49), _mm256_permute2f128_pd(_t54_96, _t54_96, 49), 15), _t61_0)));
      _t54_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 0), _t61_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 32), _mm256_permute2f128_pd(_t54_97, _t54_97, 32), 15), _t61_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 0), _t61_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_97, _t54_97, 49), _mm256_permute2f128_pd(_t54_97, _t54_97, 49), 15), _t61_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpperTriang
      _t61_4 = _t60_4;
      _t61_5 = _t60_5;
      _t61_6 = _t60_6;
      _t61_7 = _t60_7;

      // 4-BLAC: 4x4 - 4x4
      _t61_4 = _mm256_sub_pd(_t61_4, _t54_21);
      _t61_5 = _mm256_sub_pd(_t61_5, _t54_22);
      _t61_6 = _mm256_sub_pd(_t61_6, _t54_23);
      _t61_7 = _mm256_sub_pd(_t61_7, _t54_24);

      // AVX Storer:

      // 4x4 -> 4x4 - UpTriang
      _t60_4 = _t61_4;
      _t60_5 = _t61_5;
      _t60_6 = _t61_6;
      _t60_7 = _t61_7;
    }

    // Generating : U[28,28] = S(h(1, 28, fi1304), Sqrt( G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304)) ),h(1, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_9 = _mm256_blend_pd(_mm256_setzero_pd(), _t60_4, 1);

    // 4-BLAC: sqrt(1x4)
    _t62_10 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t62_9)));

    // AVX Storer:
    _t62_0 = _t62_10;

    // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304)) ),h(1, 28, fi1304))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t62_11 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_12 = _t62_0;

    // 4-BLAC: 1x4 / 1x4
    _t62_13 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t62_11), _mm256_castpd256_pd128(_t62_12)));

    // AVX Storer:
    _t62_1 = _t62_13;

    // Generating : U[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), U[28,28],h(3, 28, fi1304 + 1)) ),h(3, 28, fi1304 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_1, _t62_1, 32), _mm256_permute2f128_pd(_t62_1, _t62_1, 32), 0);

    // AVX Loader:

    // 1x3 -> 1x4
    _t62_15 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t60_4, 14), _mm256_permute2f128_pd(_t60_4, _t60_4, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t62_16 = _mm256_mul_pd(_t62_14, _t62_15);

    // AVX Storer:
    _t62_2 = _t62_16;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 28, fi1304 + 1), U[28,28],h(1, 28, fi1304 + 1)) - ( T( G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304 + 1)) ) Kro G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304 + 1)) ) ),h(1, 28, fi1304 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_17 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t60_5, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_18 = _mm256_blend_pd(_mm256_setzero_pd(), _t62_2, 1);

    // 4-BLAC: (4x1)^T
    _t54_47 = _t62_18;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_19 = _mm256_blend_pd(_mm256_setzero_pd(), _t62_2, 1);

    // 4-BLAC: 1x4 Kro 1x4
    _t54_49 = _mm256_mul_pd(_t54_47, _t62_19);

    // 4-BLAC: 1x4 - 1x4
    _t62_20 = _mm256_sub_pd(_t62_17, _t54_49);

    // AVX Storer:
    _t62_3 = _t62_20;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), Sqrt( G(h(1, 28, fi1304 + 1), U[28,28],h(1, 28, fi1304 + 1)) ),h(1, 28, fi1304 + 1))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_21 = _t62_3;

    // 4-BLAC: sqrt(1x4)
    _t62_22 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t62_21)));

    // AVX Storer:
    _t62_3 = _t62_22;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) - ( T( G(h(1, 28, fi1304), U[28,28],h(1, 28, fi1304 + 1)) ) Kro G(h(1, 28, fi1304), U[28,28],h(2, 28, fi1304 + 2)) ) ),h(2, 28, fi1304 + 2))

    // AVX Loader:

    // 1x2 -> 1x4
    _t62_23 = _mm256_permute2f128_pd(_t60_5, _t60_5, 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_2, _t62_2, 32), _mm256_permute2f128_pd(_t62_2, _t62_2, 32), 0);

    // 4-BLAC: (4x1)^T
    _t54_55 = _t62_24;

    // AVX Loader:

    // 1x2 -> 1x4
    _t62_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t62_2, 6), _mm256_permute2f128_pd(_t62_2, _t62_2, 129), 5);

    // 4-BLAC: 1x4 Kro 1x4
    _t54_57 = _mm256_mul_pd(_t54_55, _t62_25);

    // 4-BLAC: 1x4 - 1x4
    _t62_26 = _mm256_sub_pd(_t62_23, _t54_57);

    // AVX Storer:
    _t62_4 = _t62_26;

    // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 1), U[28,28],h(1, 28, fi1304 + 1)) ),h(1, 28, fi1304 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t62_27 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_28 = _t62_3;

    // 4-BLAC: 1x4 / 1x4
    _t62_29 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t62_27), _mm256_castpd256_pd128(_t62_28)));

    // AVX Storer:
    _t62_5 = _t62_29;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) ),h(2, 28, fi1304 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_30 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_5, _t62_5, 32), _mm256_permute2f128_pd(_t62_5, _t62_5, 32), 0);

    // AVX Loader:

    // 1x2 -> 1x4
    _t62_31 = _t62_4;

    // 4-BLAC: 1x4 Kro 1x4
    _t62_32 = _mm256_mul_pd(_t62_30, _t62_31);

    // AVX Storer:
    _t62_4 = _t62_32;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) - ( T( G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 2)) ) * G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 2)) ) ),h(1, 28, fi1304 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_33 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t60_6, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t60_6, 4), 129);

    // AVX Loader:

    // 2x1 -> 4x1
    _t62_34 = _mm256_shuffle_pd(_mm256_blend_pd(_t62_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t62_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (4x1)^T
    _t54_67 = _t62_34;

    // AVX Loader:

    // 2x1 -> 4x1
    _t62_35 = _mm256_shuffle_pd(_mm256_blend_pd(_t62_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t62_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: 1x4 * 4x1
    _t54_69 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t62_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t62_35), _mm256_mul_pd(_t54_67, _t62_35), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_67, _t62_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t62_35), _mm256_mul_pd(_t54_67, _t62_35), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_67, _t62_35), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_67, _t62_35), _mm256_mul_pd(_t54_67, _t62_35), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t62_36 = _mm256_sub_pd(_t62_33, _t54_69);

    // AVX Storer:
    _t62_6 = _t62_36;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), Sqrt( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 2))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_37 = _t62_6;

    // 4-BLAC: sqrt(1x4)
    _t62_38 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t62_37)));

    // AVX Storer:
    _t62_6 = _t62_38;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) - ( T( G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 2)) ) * G(h(2, 28, fi1304), U[28,28],h(1, 28, fi1304 + 3)) ) ),h(1, 28, fi1304 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_39 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t60_6, _t60_6, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 2x1 -> 4x1
    _t62_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t62_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t62_4, _mm256_setzero_pd(), 12), 1);

    // 4-BLAC: (4x1)^T
    _t54_75 = _t62_40;

    // AVX Loader:

    // 2x1 -> 4x1
    _t62_41 = _mm256_blend_pd(_mm256_permute2f128_pd(_t62_2, _t62_2, 129), _t62_4, 2);

    // 4-BLAC: 1x4 * 4x1
    _t54_77 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t62_41), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t62_41), _mm256_mul_pd(_t54_75, _t62_41), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_75, _t62_41), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t62_41), _mm256_mul_pd(_t54_75, _t62_41), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_75, _t62_41), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_75, _t62_41), _mm256_mul_pd(_t54_75, _t62_41), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t62_42 = _mm256_sub_pd(_t62_39, _t54_77);

    // AVX Storer:
    _t62_7 = _t62_42;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) Div G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_43 = _t62_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_44 = _t62_6;

    // 4-BLAC: 1x4 / 1x4
    _t62_45 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t62_43), _mm256_castpd256_pd128(_t62_44)));

    // AVX Storer:
    _t62_7 = _t62_45;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), U[28,28],h(1, 28, fi1304 + 3)) - ( T( G(h(3, 28, fi1304), U[28,28],h(1, 28, fi1304 + 3)) ) * G(h(3, 28, fi1304), U[28,28],h(1, 28, fi1304 + 3)) ) ),h(1, 28, fi1304 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_46 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t60_7, _t60_7, 129), _mm256_setzero_pd());

    // AVX Loader:

    // 3x1 -> 4x1
    _t62_47 = _mm256_blend_pd(_mm256_permute2f128_pd(_t62_2, _t62_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t62_4, 2), 10);

    // 4-BLAC: (4x1)^T
    _t54_84 = _t62_47;

    // AVX Loader:

    // 3x1 -> 4x1
    _t62_48 = _mm256_blend_pd(_mm256_permute2f128_pd(_t62_2, _t62_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t62_4, 2), 10);

    // 4-BLAC: 1x4 * 4x1
    _t54_86 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t62_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t62_48), _mm256_mul_pd(_t54_84, _t62_48), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t54_84, _t62_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t62_48), _mm256_mul_pd(_t54_84, _t62_48), 129)), _mm256_add_pd(_mm256_mul_pd(_t54_84, _t62_48), _mm256_permute2f128_pd(_mm256_mul_pd(_t54_84, _t62_48), _mm256_mul_pd(_t54_84, _t62_48), 129)), 1));

    // 4-BLAC: 1x4 - 1x4
    _t62_49 = _mm256_sub_pd(_t62_46, _t54_86);

    // AVX Storer:
    _t62_8 = _t62_49;

    // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), Sqrt( G(h(1, 28, fi1304 + 3), U[28,28],h(1, 28, fi1304 + 3)) ),h(1, 28, fi1304 + 3))

    // AVX Loader:

    // 1x1 -> 1x4
    _t62_50 = _t62_8;

    // 4-BLAC: sqrt(1x4)
    _t62_51 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t62_50)));

    // AVX Storer:
    _t62_8 = _t62_51;

    // Generating : U[28,28] = ( Sum_{k2} ( S(h(4, 28, fi1304), ( G(h(4, 28, fi1304), M4[28,28],h(4, 28, fi1304 + k2 + 4)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, fi1304 + k2 + 4)) ) ),h(4, 28, fi1304 + k2 + 4)) ) + Sum_{k3} ( Sum_{k2} ( -$(h(4, 28, fi1304), ( T( G(h(4, 28, k3), U[28,28],h(4, 28, fi1304)) ) * G(h(4, 28, k3), U[28,28],h(4, 28, fi1304 + k2 + 4)) ),h(4, 28, fi1304 + k2 + 4)) ) ) )

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 32);
    _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 32);
    _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 49);
    _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 49);

    for( int k2 = 0; k2 <= -fi1304 + 23; k2+=4 ) {
      _t63_8 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 4);
      _t63_9 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 32);
      _t63_10 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 60);
      _t63_11 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 88);
      _t63_3 = _asm256_loadu_pd(M3 + fi1304 + k2 + 4);
      _t63_2 = _asm256_loadu_pd(M3 + fi1304 + k2 + 32);
      _t63_1 = _asm256_loadu_pd(M3 + fi1304 + k2 + 60);
      _t63_0 = _asm256_loadu_pd(M3 + fi1304 + k2 + 88);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t54_98 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 32);
      _t54_99 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 32);
      _t54_100 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t60_3, _t60_2), _mm256_unpacklo_pd(_t60_1, _t60_0), 49);
      _t54_101 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t60_3, _t60_2), _mm256_unpackhi_pd(_t60_1, _t60_0), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t63_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 32), _mm256_permute2f128_pd(_t54_98, _t54_98, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_98, _t54_98, 49), _mm256_permute2f128_pd(_t54_98, _t54_98, 49), 15), _t63_0)));
      _t63_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 32), _mm256_permute2f128_pd(_t54_99, _t54_99, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_99, _t54_99, 49), _mm256_permute2f128_pd(_t54_99, _t54_99, 49), 15), _t63_0)));
      _t63_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 32), _mm256_permute2f128_pd(_t54_100, _t54_100, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_100, _t54_100, 49), _mm256_permute2f128_pd(_t54_100, _t54_100, 49), 15), _t63_0)));
      _t63_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 0), _t63_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 32), _mm256_permute2f128_pd(_t54_101, _t54_101, 32), 15), _t63_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 0), _t63_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t54_101, _t54_101, 49), _mm256_permute2f128_pd(_t54_101, _t54_101, 49), 15), _t63_0)));

      // 4-BLAC: 4x4 - 4x4
      _t63_8 = _mm256_sub_pd(_t63_8, _t63_4);
      _t63_9 = _mm256_sub_pd(_t63_9, _t63_5);
      _t63_10 = _mm256_sub_pd(_t63_10, _t63_6);
      _t63_11 = _mm256_sub_pd(_t63_11, _t63_7);

      // AVX Storer:
      _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 4, _t63_8);
      _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 32, _t63_9);
      _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 60, _t63_10);
      _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 88, _t63_11);
    }

    for( int k3 = 4; k3 <= fi1304 - 1; k3+=4 ) {

      for( int k2 = 0; k2 <= -fi1304 + 23; k2+=4 ) {
        _t64_7 = _asm256_loadu_pd(M3 + fi1304 + 28*k3);
        _t64_6 = _asm256_loadu_pd(M3 + fi1304 + 28*k3 + 28);
        _t64_5 = _asm256_loadu_pd(M3 + fi1304 + 28*k3 + 56);
        _t64_4 = _asm256_loadu_pd(M3 + fi1304 + 28*k3 + 84);
        _t64_3 = _asm256_loadu_pd(M3 + fi1304 + k2 + 28*k3 + 4);
        _t64_2 = _asm256_loadu_pd(M3 + fi1304 + k2 + 28*k3 + 32);
        _t64_1 = _asm256_loadu_pd(M3 + fi1304 + k2 + 28*k3 + 60);
        _t64_0 = _asm256_loadu_pd(M3 + fi1304 + k2 + 28*k3 + 88);
        _t64_8 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 4);
        _t64_9 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 32);
        _t64_10 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 60);
        _t64_11 = _asm256_loadu_pd(M3 + 29*fi1304 + k2 + 88);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t56_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_7, _t64_6), _mm256_unpacklo_pd(_t64_5, _t64_4), 32);
        _t56_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t64_7, _t64_6), _mm256_unpackhi_pd(_t64_5, _t64_4), 32);
        _t56_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t64_7, _t64_6), _mm256_unpacklo_pd(_t64_5, _t64_4), 49);
        _t56_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t64_7, _t64_6), _mm256_unpackhi_pd(_t64_5, _t64_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t64_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 0), _t64_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 32), _mm256_permute2f128_pd(_t56_0, _t56_0, 32), 15), _t64_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 0), _t64_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_0, _t56_0, 49), _mm256_permute2f128_pd(_t56_0, _t56_0, 49), 15), _t64_0)));
        _t64_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 0), _t64_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 32), _mm256_permute2f128_pd(_t56_1, _t56_1, 32), 15), _t64_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 0), _t64_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_1, _t56_1, 49), _mm256_permute2f128_pd(_t56_1, _t56_1, 49), 15), _t64_0)));
        _t64_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 0), _t64_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 32), _mm256_permute2f128_pd(_t56_2, _t56_2, 32), 15), _t64_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 0), _t64_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_2, _t56_2, 49), _mm256_permute2f128_pd(_t56_2, _t56_2, 49), 15), _t64_0)));
        _t64_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 0), _t64_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 32), _mm256_permute2f128_pd(_t56_3, _t56_3, 32), 15), _t64_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 0), _t64_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t56_3, _t56_3, 49), _mm256_permute2f128_pd(_t56_3, _t56_3, 49), 15), _t64_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t64_8 = _mm256_sub_pd(_t64_8, _t64_12);
        _t64_9 = _mm256_sub_pd(_t64_9, _t64_13);
        _t64_10 = _mm256_sub_pd(_t64_10, _t64_14);
        _t64_11 = _mm256_sub_pd(_t64_11, _t64_15);

        // AVX Storer:
        _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 4, _t64_8);
        _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 32, _t64_9);
        _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 60, _t64_10);
        _asm256_storeu_pd(M3 + 29*fi1304 + k2 + 88, _t64_11);
      }
    }

    // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_2 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_3 = _t62_6;

    // 4-BLAC: 1x4 / 1x4
    _t65_4 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_2), _mm256_castpd256_pd128(_t65_3)));

    // AVX Storer:
    _t65_0 = _t65_4;

    // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 3), U[28,28],h(1, 28, fi1304 + 3)) ),h(1, 28, fi1304 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t65_5 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t65_6 = _t62_8;

    // 4-BLAC: 1x4 / 1x4
    _t65_7 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t65_5), _mm256_castpd256_pd128(_t65_6)));

    // AVX Storer:
    _t65_1 = _t65_7;
    _asm256_storeu_pd(M3 + 29*fi1304, _t60_4);
    _mm256_maskstore_pd(M3 + 29*fi1304 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t60_5);
    _mm256_maskstore_pd(M3 + 29*fi1304 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t60_6);

    for( int fi1429 = 0; fi1429 <= -fi1304 + 20; fi1429+=4 ) {
      _t66_3 = _asm256_loadu_pd(M3 + 29*fi1304 + fi1429 + 4);
      _t66_0 = _asm256_loadu_pd(M3 + 29*fi1304 + fi1429 + 32);
      _t66_1 = _asm256_loadu_pd(M3 + 29*fi1304 + fi1429 + 60);
      _t66_2 = _asm256_loadu_pd(M3 + 29*fi1304 + fi1429 + 88);

      // Generating : U[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_1, _t62_1, 32), _mm256_permute2f128_pd(_t62_1, _t62_1, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_3 = _mm256_mul_pd(_t66_4, _t66_3);

      // AVX Storer:

      // Generating : U[28,28] = S(h(3, 28, fi1304 + 1), ( G(h(3, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304), U[28,28],h(3, 28, fi1304 + 1)) ) * G(h(1, 28, fi1304), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // 3x4 -> 4x4
      _t66_5 = _t66_0;
      _t66_6 = _t66_1;
      _t66_7 = _t66_2;
      _t66_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t66_9 = _t62_2;

      // 4-BLAC: (1x4)^T
      _t58_19 = _t66_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t58_20 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 0), _t66_3);
      _t58_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 32), _mm256_permute2f128_pd(_t58_19, _t58_19, 32), 15), _t66_3);
      _t58_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 0), _t66_3);
      _t58_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_19, _t58_19, 49), _mm256_permute2f128_pd(_t58_19, _t58_19, 49), 15), _t66_3);

      // 4-BLAC: 4x4 - 4x4
      _t66_10 = _mm256_sub_pd(_t66_5, _t58_20);
      _t66_11 = _mm256_sub_pd(_t66_6, _t58_21);
      _t66_12 = _mm256_sub_pd(_t66_7, _t58_22);
      _t66_13 = _mm256_sub_pd(_t66_8, _t58_23);

      // AVX Storer:
      _t66_0 = _t66_10;
      _t66_1 = _t66_11;
      _t66_2 = _t66_12;

      // Generating : U[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_5, _t62_5, 32), _mm256_permute2f128_pd(_t62_5, _t62_5, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_0 = _mm256_mul_pd(_t66_14, _t66_0);

      // AVX Storer:

      // Generating : U[28,28] = S(h(2, 28, fi1304 + 2), ( G(h(2, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304 + 1), U[28,28],h(2, 28, fi1304 + 2)) ) * G(h(1, 28, fi1304 + 1), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // 2x4 -> 4x4
      _t66_15 = _t66_1;
      _t66_16 = _t66_2;
      _t66_17 = _mm256_setzero_pd();
      _t66_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t66_19 = _t62_4;

      // 4-BLAC: (1x4)^T
      _t58_34 = _t66_19;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t58_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 32), _mm256_permute2f128_pd(_t58_34, _t58_34, 32), 0), _t66_0);
      _t58_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 32), _mm256_permute2f128_pd(_t58_34, _t58_34, 32), 15), _t66_0);
      _t58_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 49), _mm256_permute2f128_pd(_t58_34, _t58_34, 49), 0), _t66_0);
      _t58_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t58_34, _t58_34, 49), _mm256_permute2f128_pd(_t58_34, _t58_34, 49), 15), _t66_0);

      // 4-BLAC: 4x4 - 4x4
      _t66_20 = _mm256_sub_pd(_t66_15, _t58_35);
      _t66_21 = _mm256_sub_pd(_t66_16, _t58_36);
      _t66_22 = _mm256_sub_pd(_t66_17, _t58_37);
      _t66_23 = _mm256_sub_pd(_t66_18, _t58_38);

      // AVX Storer:
      _t66_1 = _t66_20;
      _t66_2 = _t66_21;

      // Generating : U[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 2)) Kro G(h(1, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_0, _t65_0, 32), _mm256_permute2f128_pd(_t65_0, _t65_0, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_1 = _mm256_mul_pd(_t66_24, _t66_1);

      // AVX Storer:

      // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) - ( T( G(h(1, 28, fi1304 + 2), U[28,28],h(1, 28, fi1304 + 3)) ) Kro G(h(1, 28, fi1304 + 2), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t62_7, _t62_7, 32), _mm256_permute2f128_pd(_t62_7, _t62_7, 32), 0);

      // 4-BLAC: (4x1)^T
      _t58_45 = _t66_25;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t58_6 = _mm256_mul_pd(_t58_45, _t66_1);

      // 4-BLAC: 1x4 - 1x4
      _t66_2 = _mm256_sub_pd(_t66_2, _t58_6);

      // AVX Storer:

      // Generating : U[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, fi1304 + 3)) Kro G(h(1, 28, fi1304 + 3), U[28,28],h(4, 28, fi1304 + fi1429 + 4)) ),h(4, 28, fi1304 + fi1429 + 4))

      // AVX Loader:

      // 1x1 -> 1x4
      _t66_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t65_1, _t65_1, 32), _mm256_permute2f128_pd(_t65_1, _t65_1, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t66_2 = _mm256_mul_pd(_t66_26, _t66_2);

      // AVX Storer:
      _asm256_storeu_pd(M3 + 29*fi1304 + fi1429 + 4, _t66_3);
      _asm256_storeu_pd(M3 + 29*fi1304 + fi1429 + 32, _t66_0);
      _asm256_storeu_pd(M3 + 29*fi1304 + fi1429 + 60, _t66_1);
      _asm256_storeu_pd(M3 + 29*fi1304 + fi1429 + 88, _t66_2);
    }
    _mm_store_sd(&(M3[29*fi1304]), _mm256_castpd256_pd128(_t62_0));
    _mm256_maskstore_pd(M3 + 29*fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t62_2);
    _mm_store_sd(&(M3[29*fi1304 + 29]), _mm256_castpd256_pd128(_t62_3));
    _mm256_maskstore_pd(M3 + 29*fi1304 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t62_4);
    _mm_store_sd(&(M3[29*fi1304 + 58]), _mm256_castpd256_pd128(_t62_6));
    _mm_store_sd(&(M3[29*fi1304 + 59]), _mm256_castpd256_pd128(_t62_7));
    _mm_store_sd(&(M3[29*fi1304 + 87]), _mm256_castpd256_pd128(_t62_8));
  }

  _t67_3 = _asm256_loadu_pd(M3 + 24);
  _t67_2 = _asm256_loadu_pd(M3 + 52);
  _t67_1 = _asm256_loadu_pd(M3 + 80);
  _t67_0 = _asm256_loadu_pd(M3 + 108);

  // Generating : U[28,28] = ( S(h(4, 28, 24), ( G(h(4, 28, 24), M4[28,28],h(4, 28, 24)) - ( T( G(h(4, 28, 0), U[28,28],h(4, 28, 24)) ) * G(h(4, 28, 0), U[28,28],h(4, 28, 24)) ) ),h(4, 28, 24)) + Sum_{k3} ( -$(h(4, 28, 24), ( T( G(h(4, 28, k3), U[28,28],h(4, 28, 24)) ) * G(h(4, 28, k3), U[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t67_12 = _t44_28;
  _t67_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 3), _t44_29, 12);
  _t67_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 0), _t44_30, 49);
  _t67_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t44_28, _t44_29, 12), _mm256_shuffle_pd(_t44_30, _t44_31, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t67_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_3, _t67_2), _mm256_unpacklo_pd(_t67_1, _t67_0), 32);
  _t67_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t67_3, _t67_2), _mm256_unpackhi_pd(_t67_1, _t67_0), 32);
  _t67_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t67_3, _t67_2), _mm256_unpacklo_pd(_t67_1, _t67_0), 49);
  _t67_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t67_3, _t67_2), _mm256_unpackhi_pd(_t67_1, _t67_0), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t67_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_16, _t67_16, 32), _mm256_permute2f128_pd(_t67_16, _t67_16, 32), 0), _t67_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_16, _t67_16, 32), _mm256_permute2f128_pd(_t67_16, _t67_16, 32), 15), _t67_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_16, _t67_16, 49), _mm256_permute2f128_pd(_t67_16, _t67_16, 49), 0), _t67_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_16, _t67_16, 49), _mm256_permute2f128_pd(_t67_16, _t67_16, 49), 15), _t67_0)));
  _t67_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_17, _t67_17, 32), _mm256_permute2f128_pd(_t67_17, _t67_17, 32), 0), _t67_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_17, _t67_17, 32), _mm256_permute2f128_pd(_t67_17, _t67_17, 32), 15), _t67_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_17, _t67_17, 49), _mm256_permute2f128_pd(_t67_17, _t67_17, 49), 0), _t67_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_17, _t67_17, 49), _mm256_permute2f128_pd(_t67_17, _t67_17, 49), 15), _t67_0)));
  _t67_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_18, _t67_18, 32), _mm256_permute2f128_pd(_t67_18, _t67_18, 32), 0), _t67_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_18, _t67_18, 32), _mm256_permute2f128_pd(_t67_18, _t67_18, 32), 15), _t67_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_18, _t67_18, 49), _mm256_permute2f128_pd(_t67_18, _t67_18, 49), 0), _t67_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_18, _t67_18, 49), _mm256_permute2f128_pd(_t67_18, _t67_18, 49), 15), _t67_0)));
  _t67_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_19, _t67_19, 32), _mm256_permute2f128_pd(_t67_19, _t67_19, 32), 0), _t67_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_19, _t67_19, 32), _mm256_permute2f128_pd(_t67_19, _t67_19, 32), 15), _t67_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_19, _t67_19, 49), _mm256_permute2f128_pd(_t67_19, _t67_19, 49), 0), _t67_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_19, _t67_19, 49), _mm256_permute2f128_pd(_t67_19, _t67_19, 49), 15), _t67_0)));

  // 4-BLAC: 4x4 - 4x4
  _t67_8 = _mm256_sub_pd(_t67_12, _t67_4);
  _t67_9 = _mm256_sub_pd(_t67_13, _t67_5);
  _t67_10 = _mm256_sub_pd(_t67_14, _t67_6);
  _t67_11 = _mm256_sub_pd(_t67_15, _t67_7);

  // AVX Storer:

  // 4x4 -> 4x4 - UpTriang
  _t44_28 = _t67_8;
  _t44_29 = _t67_9;
  _t44_30 = _t67_10;
  _t44_31 = _t67_11;


  for( int k3 = 4; k3 <= 23; k3+=4 ) {
    _t68_3 = _asm256_loadu_pd(M3 + 28*k3 + 24);
    _t68_2 = _asm256_loadu_pd(M3 + 28*k3 + 52);
    _t68_1 = _asm256_loadu_pd(M3 + 28*k3 + 80);
    _t68_0 = _asm256_loadu_pd(M3 + 28*k3 + 108);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t68_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_3, _t68_2), _mm256_unpacklo_pd(_t68_1, _t68_0), 32);
    _t68_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t68_3, _t68_2), _mm256_unpackhi_pd(_t68_1, _t68_0), 32);
    _t68_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t68_3, _t68_2), _mm256_unpacklo_pd(_t68_1, _t68_0), 49);
    _t68_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t68_3, _t68_2), _mm256_unpackhi_pd(_t68_1, _t68_0), 49);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t68_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_12, _t68_12, 32), _mm256_permute2f128_pd(_t68_12, _t68_12, 32), 0), _t68_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_12, _t68_12, 32), _mm256_permute2f128_pd(_t68_12, _t68_12, 32), 15), _t68_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_12, _t68_12, 49), _mm256_permute2f128_pd(_t68_12, _t68_12, 49), 0), _t68_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_12, _t68_12, 49), _mm256_permute2f128_pd(_t68_12, _t68_12, 49), 15), _t68_0)));
    _t68_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_13, _t68_13, 32), _mm256_permute2f128_pd(_t68_13, _t68_13, 32), 0), _t68_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_13, _t68_13, 32), _mm256_permute2f128_pd(_t68_13, _t68_13, 32), 15), _t68_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_13, _t68_13, 49), _mm256_permute2f128_pd(_t68_13, _t68_13, 49), 0), _t68_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_13, _t68_13, 49), _mm256_permute2f128_pd(_t68_13, _t68_13, 49), 15), _t68_0)));
    _t68_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_14, _t68_14, 32), _mm256_permute2f128_pd(_t68_14, _t68_14, 32), 0), _t68_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_14, _t68_14, 32), _mm256_permute2f128_pd(_t68_14, _t68_14, 32), 15), _t68_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_14, _t68_14, 49), _mm256_permute2f128_pd(_t68_14, _t68_14, 49), 0), _t68_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_14, _t68_14, 49), _mm256_permute2f128_pd(_t68_14, _t68_14, 49), 15), _t68_0)));
    _t68_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_15, _t68_15, 32), _mm256_permute2f128_pd(_t68_15, _t68_15, 32), 0), _t68_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_15, _t68_15, 32), _mm256_permute2f128_pd(_t68_15, _t68_15, 32), 15), _t68_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_15, _t68_15, 49), _mm256_permute2f128_pd(_t68_15, _t68_15, 49), 0), _t68_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t68_15, _t68_15, 49), _mm256_permute2f128_pd(_t68_15, _t68_15, 49), 15), _t68_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpperTriang
    _t68_8 = _t44_28;
    _t68_9 = _t44_29;
    _t68_10 = _t44_30;
    _t68_11 = _t44_31;

    // 4-BLAC: 4x4 - 4x4
    _t68_8 = _mm256_sub_pd(_t68_8, _t68_4);
    _t68_9 = _mm256_sub_pd(_t68_9, _t68_5);
    _t68_10 = _mm256_sub_pd(_t68_10, _t68_6);
    _t68_11 = _mm256_sub_pd(_t68_11, _t68_7);

    // AVX Storer:

    // 4x4 -> 4x4 - UpTriang
    _t44_28 = _t68_8;
    _t44_29 = _t68_9;
    _t44_30 = _t68_10;
    _t44_31 = _t68_11;
  }


  // Generating : U[28,28] = S(h(1, 28, 24), Sqrt( G(h(1, 28, 24), U[28,28],h(1, 28, 24)) ),h(1, 28, 24))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 1);

  // 4-BLAC: sqrt(1x4)
  _t69_23 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t69_22)));

  // AVX Storer:
  _t69_0 = _t69_23;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 24), U[28,28],h(1, 28, 24)) ),h(1, 28, 24))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t69_24 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_25 = _t69_0;

  // 4-BLAC: 1x4 / 1x4
  _t69_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t69_24), _mm256_castpd256_pd128(_t69_25)));

  // AVX Storer:
  _t69_1 = _t69_26;

  // Generating : U[28,28] = S(h(1, 28, 24), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 24)) Kro G(h(1, 28, 24), U[28,28],h(3, 28, 25)) ),h(3, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_27 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_1, _t69_1, 32), _mm256_permute2f128_pd(_t69_1, _t69_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t69_28 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_28, 14), _mm256_permute2f128_pd(_t44_28, _t44_28, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t69_29 = _mm256_mul_pd(_t69_27, _t69_28);

  // AVX Storer:
  _t69_2 = _t69_29;

  // Generating : U[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), U[28,28],h(1, 28, 25)) - ( T( G(h(1, 28, 24), U[28,28],h(1, 28, 25)) ) Kro G(h(1, 28, 24), U[28,28],h(1, 28, 25)) ) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_30 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_29, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t69_2, 1);

  // 4-BLAC: (4x1)^T
  _t69_32 = _t69_31;

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_33 = _mm256_blend_pd(_mm256_setzero_pd(), _t69_2, 1);

  // 4-BLAC: 1x4 Kro 1x4
  _t69_34 = _mm256_mul_pd(_t69_32, _t69_33);

  // 4-BLAC: 1x4 - 1x4
  _t69_35 = _mm256_sub_pd(_t69_30, _t69_34);

  // AVX Storer:
  _t69_3 = _t69_35;

  // Generating : U[28,28] = S(h(1, 28, 25), Sqrt( G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ),h(1, 28, 25))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_36 = _t69_3;

  // 4-BLAC: sqrt(1x4)
  _t69_37 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t69_36)));

  // AVX Storer:
  _t69_3 = _t69_37;

  // Generating : U[28,28] = S(h(1, 28, 25), ( G(h(1, 28, 25), U[28,28],h(2, 28, 26)) - ( T( G(h(1, 28, 24), U[28,28],h(1, 28, 25)) ) Kro G(h(1, 28, 24), U[28,28],h(2, 28, 26)) ) ),h(2, 28, 26))

  // AVX Loader:

  // 1x2 -> 1x4
  _t69_38 = _mm256_permute2f128_pd(_t44_29, _t44_29, 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_39 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_2, _t69_2, 32), _mm256_permute2f128_pd(_t69_2, _t69_2, 32), 0);

  // 4-BLAC: (4x1)^T
  _t69_40 = _t69_39;

  // AVX Loader:

  // 1x2 -> 1x4
  _t69_41 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t69_2, 6), _mm256_permute2f128_pd(_t69_2, _t69_2, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t69_42 = _mm256_mul_pd(_t69_40, _t69_41);

  // 4-BLAC: 1x4 - 1x4
  _t69_43 = _mm256_sub_pd(_t69_38, _t69_42);

  // AVX Storer:
  _t69_4 = _t69_43;

  // Generating : T2344[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 25), U[28,28],h(1, 28, 25)) ),h(1, 28, 25))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t69_44 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_45 = _t69_3;

  // 4-BLAC: 1x4 / 1x4
  _t69_46 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t69_44), _mm256_castpd256_pd128(_t69_45)));

  // AVX Storer:
  _t69_5 = _t69_46;

  // Generating : U[28,28] = S(h(1, 28, 25), ( G(h(1, 1, 0), T2344[1,28],h(1, 28, 25)) Kro G(h(1, 28, 25), U[28,28],h(2, 28, 26)) ),h(2, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_47 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_5, _t69_5, 32), _mm256_permute2f128_pd(_t69_5, _t69_5, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t69_48 = _t69_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t69_49 = _mm256_mul_pd(_t69_47, _t69_48);

  // AVX Storer:
  _t69_4 = _t69_49;

  // Generating : U[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), U[28,28],h(1, 28, 26)) - ( T( G(h(2, 28, 24), U[28,28],h(1, 28, 26)) ) * G(h(2, 28, 24), U[28,28],h(1, 28, 26)) ) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_50 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t44_30, 4), 129);

  // AVX Loader:

  // 2x1 -> 4x1
  _t69_51 = _mm256_shuffle_pd(_mm256_blend_pd(_t69_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t69_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t69_52 = _t69_51;

  // AVX Loader:

  // 2x1 -> 4x1
  _t69_53 = _mm256_shuffle_pd(_mm256_blend_pd(_t69_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t69_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: 1x4 * 4x1
  _t69_54 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t69_52, _t69_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_52, _t69_53), _mm256_mul_pd(_t69_52, _t69_53), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t69_52, _t69_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_52, _t69_53), _mm256_mul_pd(_t69_52, _t69_53), 129)), _mm256_add_pd(_mm256_mul_pd(_t69_52, _t69_53), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_52, _t69_53), _mm256_mul_pd(_t69_52, _t69_53), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t69_55 = _mm256_sub_pd(_t69_50, _t69_54);

  // AVX Storer:
  _t69_6 = _t69_55;

  // Generating : U[28,28] = S(h(1, 28, 26), Sqrt( G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ),h(1, 28, 26))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_56 = _t69_6;

  // 4-BLAC: sqrt(1x4)
  _t69_57 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t69_56)));

  // AVX Storer:
  _t69_6 = _t69_57;

  // Generating : U[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), U[28,28],h(1, 28, 27)) - ( T( G(h(2, 28, 24), U[28,28],h(1, 28, 26)) ) * G(h(2, 28, 24), U[28,28],h(1, 28, 27)) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_58 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_30, _t44_30, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 2x1 -> 4x1
  _t69_59 = _mm256_shuffle_pd(_mm256_blend_pd(_t69_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t69_4, _mm256_setzero_pd(), 12), 1);

  // 4-BLAC: (4x1)^T
  _t69_60 = _t69_59;

  // AVX Loader:

  // 2x1 -> 4x1
  _t69_61 = _mm256_blend_pd(_mm256_permute2f128_pd(_t69_2, _t69_2, 129), _t69_4, 2);

  // 4-BLAC: 1x4 * 4x1
  _t69_9 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t69_60, _t69_61), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_60, _t69_61), _mm256_mul_pd(_t69_60, _t69_61), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t69_60, _t69_61), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_60, _t69_61), _mm256_mul_pd(_t69_60, _t69_61), 129)), _mm256_add_pd(_mm256_mul_pd(_t69_60, _t69_61), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_60, _t69_61), _mm256_mul_pd(_t69_60, _t69_61), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t69_10 = _mm256_sub_pd(_t69_58, _t69_9);

  // AVX Storer:
  _t69_7 = _t69_10;

  // Generating : U[28,28] = S(h(1, 28, 26), ( G(h(1, 28, 26), U[28,28],h(1, 28, 27)) Div G(h(1, 28, 26), U[28,28],h(1, 28, 26)) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_11 = _t69_7;

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_12 = _t69_6;

  // 4-BLAC: 1x4 / 1x4
  _t69_13 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t69_11), _mm256_castpd256_pd128(_t69_12)));

  // AVX Storer:
  _t69_7 = _t69_13;

  // Generating : U[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), U[28,28],h(1, 28, 27)) - ( T( G(h(3, 28, 24), U[28,28],h(1, 28, 27)) ) * G(h(3, 28, 24), U[28,28],h(1, 28, 27)) ) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_14 = _mm256_unpackhi_pd(_mm256_permute2f128_pd(_t44_31, _t44_31, 129), _mm256_setzero_pd());

  // AVX Loader:

  // 3x1 -> 4x1
  _t69_15 = _mm256_blend_pd(_mm256_permute2f128_pd(_t69_2, _t69_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t69_4, 2), 10);

  // 4-BLAC: (4x1)^T
  _t69_16 = _t69_15;

  // AVX Loader:

  // 3x1 -> 4x1
  _t69_17 = _mm256_blend_pd(_mm256_permute2f128_pd(_t69_2, _t69_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t69_4, 2), 10);

  // 4-BLAC: 1x4 * 4x1
  _t69_18 = _mm256_add_pd(_mm256_blend_pd(_mm256_add_pd(_mm256_mul_pd(_t69_16, _t69_17), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_16, _t69_17), _mm256_mul_pd(_t69_16, _t69_17), 129)), _mm256_setzero_pd(), 14), _mm256_shuffle_pd(_mm256_add_pd(_mm256_mul_pd(_t69_16, _t69_17), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_16, _t69_17), _mm256_mul_pd(_t69_16, _t69_17), 129)), _mm256_add_pd(_mm256_mul_pd(_t69_16, _t69_17), _mm256_permute2f128_pd(_mm256_mul_pd(_t69_16, _t69_17), _mm256_mul_pd(_t69_16, _t69_17), 129)), 1));

  // 4-BLAC: 1x4 - 1x4
  _t69_19 = _mm256_sub_pd(_t69_14, _t69_18);

  // AVX Storer:
  _t69_8 = _t69_19;

  // Generating : U[28,28] = S(h(1, 28, 27), Sqrt( G(h(1, 28, 27), U[28,28],h(1, 28, 27)) ),h(1, 28, 27))

  // AVX Loader:

  // 1x1 -> 1x4
  _t69_20 = _t69_8;

  // 4-BLAC: sqrt(1x4)
  _t69_21 = _mm256_castpd128_pd256(_mm_sqrt_pd(_mm256_castpd256_pd128(_t69_20)));

  // AVX Storer:
  _t69_8 = _t69_21;

  _mm_store_sd(&(M3[0]), _mm256_castpd256_pd128(_t48_0));
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t48_2);
  _mm_store_sd(&(M3[29]), _mm256_castpd256_pd128(_t48_3));
  _mm256_maskstore_pd(M3 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t48_4);
  _mm_store_sd(&(M3[58]), _mm256_castpd256_pd128(_t48_6));
  _mm_store_sd(&(M3[59]), _mm256_castpd256_pd128(_t48_7));
  _mm_store_sd(&(M3[87]), _mm256_castpd256_pd128(_t48_8));
  _asm256_storeu_pd(M3 + 4, _t48_14);
  _asm256_storeu_pd(M3 + 32, _t48_11);
  _asm256_storeu_pd(M3 + 60, _t48_12);
  _asm256_storeu_pd(M3 + 88, _t48_13);
  _mm_store_sd(&(M3[116]), _mm256_castpd256_pd128(_t50_4));
  _mm256_maskstore_pd(M3 + 117, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t50_6);
  _mm_store_sd(&(M3[145]), _mm256_castpd256_pd128(_t50_7));
  _mm256_maskstore_pd(M3 + 146, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t50_8);
  _mm_store_sd(&(M3[174]), _mm256_castpd256_pd128(_t50_10));
  _mm_store_sd(&(M3[175]), _mm256_castpd256_pd128(_t50_11));
  _mm_store_sd(&(M3[203]), _mm256_castpd256_pd128(_t50_12));
  _asm256_storeu_pd(M3 + 120, _t52_5);
  _asm256_storeu_pd(M3 + 148, _t52_2);
  _asm256_storeu_pd(M3 + 176, _t52_3);
  _asm256_storeu_pd(M3 + 204, _t52_4);
  _mm_store_sd(&(M3[232]), _mm256_castpd256_pd128(_t54_8));
  _mm256_maskstore_pd(M3 + 233, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t54_10);
  _mm_store_sd(&(M3[261]), _mm256_castpd256_pd128(_t54_11));
  _mm256_maskstore_pd(M3 + 262, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t54_12);
  _mm_store_sd(&(M3[290]), _mm256_castpd256_pd128(_t54_14));
  _mm_store_sd(&(M3[291]), _mm256_castpd256_pd128(_t54_15));
  _mm_store_sd(&(M3[319]), _mm256_castpd256_pd128(_t54_16));

  for( int fi1304 = 0; fi1304 <= 23; fi1304+=4 ) {
    _t70_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304])));
    _t70_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304])));
    _t70_8 = _mm256_maskload_pd(v0 + fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t70_5 = _mm256_maskload_pd(M3 + 29*fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t70_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 29])));
    _t70_3 = _mm256_maskload_pd(M3 + 29*fi1304 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t70_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 58])));
    _t70_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 59])));
    _t70_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 87])));

    // Generating : v2[28,1] = S(h(1, 28, fi1304), ( G(h(1, 28, fi1304), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, fi1304), U0[28,28],h(1, 28, fi1304)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_13 = _t70_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_14 = _t70_6;

    // 4-BLAC: 1x4 / 1x4
    _t70_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_13), _mm256_castpd256_pd128(_t70_14)));

    // AVX Storer:
    _t70_7 = _t70_15;

    // Generating : v2[28,1] = S(h(3, 28, fi1304 + 1), ( G(h(3, 28, fi1304 + 1), v2[28,1],h(1, 1, 0)) - ( T( G(h(1, 28, fi1304), U0[28,28],h(3, 28, fi1304 + 1)) ) Kro G(h(1, 28, fi1304), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t70_16 = _t70_8;

    // AVX Loader:

    // 1x3 -> 1x4
    _t70_17 = _t70_5;

    // 4-BLAC: (1x4)^T
    _t70_18 = _t70_17;

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t70_7, _t70_7, 32), _mm256_permute2f128_pd(_t70_7, _t70_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t70_20 = _mm256_mul_pd(_t70_18, _t70_19);

    // 4-BLAC: 4x1 - 4x1
    _t70_21 = _mm256_sub_pd(_t70_16, _t70_20);

    // AVX Storer:
    _t70_8 = _t70_21;

    // Generating : v2[28,1] = S(h(1, 28, fi1304 + 1), ( G(h(1, 28, fi1304 + 1), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 1), U0[28,28],h(1, 28, fi1304 + 1)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_22 = _mm256_blend_pd(_mm256_setzero_pd(), _t70_8, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_23 = _t70_4;

    // 4-BLAC: 1x4 / 1x4
    _t70_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_22), _mm256_castpd256_pd128(_t70_23)));

    // AVX Storer:
    _t70_9 = _t70_24;

    // Generating : v2[28,1] = S(h(2, 28, fi1304 + 2), ( G(h(2, 28, fi1304 + 2), v2[28,1],h(1, 1, 0)) - ( T( G(h(1, 28, fi1304 + 1), U0[28,28],h(2, 28, fi1304 + 2)) ) Kro G(h(1, 28, fi1304 + 1), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t70_25 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t70_8, 6), _mm256_permute2f128_pd(_t70_8, _t70_8, 129), 5);

    // AVX Loader:

    // 1x2 -> 1x4
    _t70_26 = _t70_3;

    // 4-BLAC: (1x4)^T
    _t70_27 = _t70_26;

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t70_9, _t70_9, 32), _mm256_permute2f128_pd(_t70_9, _t70_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t70_29 = _mm256_mul_pd(_t70_27, _t70_28);

    // 4-BLAC: 4x1 - 4x1
    _t70_30 = _mm256_sub_pd(_t70_25, _t70_29);

    // AVX Storer:
    _t70_10 = _t70_30;

    // Generating : v2[28,1] = S(h(1, 28, fi1304 + 2), ( G(h(1, 28, fi1304 + 2), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 2), U0[28,28],h(1, 28, fi1304 + 2)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_31 = _mm256_blend_pd(_mm256_setzero_pd(), _t70_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_32 = _t70_2;

    // 4-BLAC: 1x4 / 1x4
    _t70_33 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_31), _mm256_castpd256_pd128(_t70_32)));

    // AVX Storer:
    _t70_11 = _t70_33;

    // Generating : v2[28,1] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), v2[28,1],h(1, 1, 0)) - ( T( G(h(1, 28, fi1304 + 2), U0[28,28],h(1, 28, fi1304 + 3)) ) Kro G(h(1, 28, fi1304 + 2), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_34 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t70_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_35 = _t70_1;

    // 4-BLAC: (4x1)^T
    _t70_36 = _t70_35;

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_37 = _t70_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t70_38 = _mm256_mul_pd(_t70_36, _t70_37);

    // 4-BLAC: 1x4 - 1x4
    _t70_39 = _mm256_sub_pd(_t70_34, _t70_38);

    // AVX Storer:
    _t70_12 = _t70_39;

    // Generating : v2[28,1] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 3), U0[28,28],h(1, 28, fi1304 + 3)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_40 = _t70_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t70_41 = _t70_0;

    // 4-BLAC: 1x4 / 1x4
    _t70_42 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t70_40), _mm256_castpd256_pd128(_t70_41)));

    // AVX Storer:
    _t70_12 = _t70_42;

    // Generating : v2[28,1] = Sum_{k3} ( S(h(4, 28, fi1304 + k3 + 4), ( G(h(4, 28, fi1304 + k3 + 4), v2[28,1],h(1, 1, 0)) - ( T( G(h(4, 28, fi1304), U0[28,28],h(4, 28, fi1304 + k3 + 4)) ) * G(h(4, 28, fi1304), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[fi1304]), _mm256_castpd256_pd128(_t70_7));
    _mm_store_sd(&(v0[fi1304 + 1]), _mm256_castpd256_pd128(_t70_9));
    _mm_store_sd(&(v0[fi1304 + 2]), _mm256_castpd256_pd128(_t70_11));
    _mm_store_sd(&(v0[fi1304 + 3]), _mm256_castpd256_pd128(_t70_12));

    for( int k3 = 0; k3 <= -fi1304 + 23; k3+=4 ) {
      _t71_9 = _asm256_loadu_pd(v0 + fi1304 + k3 + 4);
      _t71_7 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 4);
      _t71_6 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 32);
      _t71_5 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 60);
      _t71_4 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 88);
      _t71_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304])));
      _t71_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304 + 1])));
      _t71_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304 + 2])));
      _t71_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[fi1304 + 3])));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t71_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_7, _t71_6), _mm256_unpacklo_pd(_t71_5, _t71_4), 32);
      _t71_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t71_7, _t71_6), _mm256_unpackhi_pd(_t71_5, _t71_4), 32);
      _t71_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_7, _t71_6), _mm256_unpacklo_pd(_t71_5, _t71_4), 49);
      _t71_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t71_7, _t71_6), _mm256_unpackhi_pd(_t71_5, _t71_4), 49);

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t71_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t71_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32)), _mm256_mul_pd(_t71_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t71_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32)), _mm256_mul_pd(_t71_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t71_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32)), _mm256_mul_pd(_t71_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t71_12, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32)), _mm256_mul_pd(_t71_13, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t71_3, _t71_2), _mm256_unpacklo_pd(_t71_1, _t71_0), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t71_9 = _mm256_sub_pd(_t71_9, _t71_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + fi1304 + k3 + 4, _t71_9);
    }
  }

  _t72_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[24])));
  _t72_1 = _mm256_maskload_pd(v0 + 25, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v2[28,1] = S(h(1, 28, 24), ( G(h(1, 28, 24), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, 24), U0[28,28],h(1, 28, 24)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_6 = _t72_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_7 = _t69_0;

  // 4-BLAC: 1x4 / 1x4
  _t72_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t72_6), _mm256_castpd256_pd128(_t72_7)));

  // AVX Storer:
  _t72_0 = _t72_8;

  // Generating : v2[28,1] = S(h(3, 28, 25), ( G(h(3, 28, 25), v2[28,1],h(1, 1, 0)) - ( T( G(h(1, 28, 24), U0[28,28],h(3, 28, 25)) ) Kro G(h(1, 28, 24), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t72_9 = _t72_1;

  // AVX Loader:

  // 1x3 -> 1x4
  _t72_10 = _t69_2;

  // 4-BLAC: (1x4)^T
  _t72_11 = _t72_10;

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_12 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t72_0, _t72_0, 32), _mm256_permute2f128_pd(_t72_0, _t72_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t72_13 = _mm256_mul_pd(_t72_11, _t72_12);

  // 4-BLAC: 4x1 - 4x1
  _t72_14 = _mm256_sub_pd(_t72_9, _t72_13);

  // AVX Storer:
  _t72_1 = _t72_14;

  // Generating : v2[28,1] = S(h(1, 28, 25), ( G(h(1, 28, 25), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, 25), U0[28,28],h(1, 28, 25)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_15 = _mm256_blend_pd(_mm256_setzero_pd(), _t72_1, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_16 = _t69_3;

  // 4-BLAC: 1x4 / 1x4
  _t72_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t72_15), _mm256_castpd256_pd128(_t72_16)));

  // AVX Storer:
  _t72_2 = _t72_17;

  // Generating : v2[28,1] = S(h(2, 28, 26), ( G(h(2, 28, 26), v2[28,1],h(1, 1, 0)) - ( T( G(h(1, 28, 25), U0[28,28],h(2, 28, 26)) ) Kro G(h(1, 28, 25), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t72_18 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t72_1, 6), _mm256_permute2f128_pd(_t72_1, _t72_1, 129), 5);

  // AVX Loader:

  // 1x2 -> 1x4
  _t72_19 = _t69_4;

  // 4-BLAC: (1x4)^T
  _t72_20 = _t72_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t72_2, _t72_2, 32), _mm256_permute2f128_pd(_t72_2, _t72_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t72_22 = _mm256_mul_pd(_t72_20, _t72_21);

  // 4-BLAC: 4x1 - 4x1
  _t72_23 = _mm256_sub_pd(_t72_18, _t72_22);

  // AVX Storer:
  _t72_3 = _t72_23;

  // Generating : v2[28,1] = S(h(1, 28, 26), ( G(h(1, 28, 26), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, 26), U0[28,28],h(1, 28, 26)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t72_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_25 = _t69_6;

  // 4-BLAC: 1x4 / 1x4
  _t72_26 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t72_24), _mm256_castpd256_pd128(_t72_25)));

  // AVX Storer:
  _t72_4 = _t72_26;

  // Generating : v2[28,1] = S(h(1, 28, 27), ( G(h(1, 28, 27), v2[28,1],h(1, 1, 0)) - ( T( G(h(1, 28, 26), U0[28,28],h(1, 28, 27)) ) Kro G(h(1, 28, 26), v2[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_27 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t72_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_28 = _t69_7;

  // 4-BLAC: (4x1)^T
  _t72_29 = _t72_28;

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_30 = _t72_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t72_31 = _mm256_mul_pd(_t72_29, _t72_30);

  // 4-BLAC: 1x4 - 1x4
  _t72_32 = _mm256_sub_pd(_t72_27, _t72_31);

  // AVX Storer:
  _t72_5 = _t72_32;

  // Generating : v2[28,1] = S(h(1, 28, 27), ( G(h(1, 28, 27), v2[28,1],h(1, 1, 0)) Div G(h(1, 28, 27), U0[28,28],h(1, 28, 27)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_33 = _t72_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t72_34 = _t69_8;

  // 4-BLAC: 1x4 / 1x4
  _t72_35 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t72_33), _mm256_castpd256_pd128(_t72_34)));

  // AVX Storer:
  _t72_5 = _t72_35;

  _mm_store_sd(&(M3[696]), _mm256_castpd256_pd128(_t69_0));
  _mm256_maskstore_pd(M3 + 697, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t69_2);
  _mm_store_sd(&(M3[725]), _mm256_castpd256_pd128(_t69_3));
  _mm256_maskstore_pd(M3 + 726, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t69_4);
  _mm_store_sd(&(M3[754]), _mm256_castpd256_pd128(_t69_6));
  _mm_store_sd(&(M3[755]), _mm256_castpd256_pd128(_t69_7));
  _mm_store_sd(&(M3[783]), _mm256_castpd256_pd128(_t69_8));
  _mm_store_sd(&(v0[24]), _mm256_castpd256_pd128(_t72_0));
  _mm_store_sd(&(v0[25]), _mm256_castpd256_pd128(_t72_2));
  _mm_store_sd(&(v0[26]), _mm256_castpd256_pd128(_t72_4));
  _mm_store_sd(&(v0[27]), _mm256_castpd256_pd128(_t72_5));

  for( int fi1304 = 0; fi1304 <= 23; fi1304+=4 ) {
    _t73_7 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 27])));
    _t73_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 783])));
    _t73_8 = _mm256_maskload_pd(v0 + -fi1304 + 24, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t73_5 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 699)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 727))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 755)), 32);
    _t73_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 754])));
    _t73_3 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 698)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 726)), 0);
    _t73_2 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 725])));
    _t73_1 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 697])));
    _t73_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 696])));

    // Generating : v4[28,1] = S(h(1, 28, -fi1304 + 27), ( G(h(1, 28, -fi1304 + 27), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 27), U0[28,28],h(1, 28, -fi1304 + 27)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_13 = _t73_7;

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_14 = _t73_6;

    // 4-BLAC: 1x4 / 1x4
    _t73_15 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t73_13), _mm256_castpd256_pd128(_t73_14)));

    // AVX Storer:
    _t73_7 = _t73_15;

    // Generating : v4[28,1] = S(h(3, 28, -fi1304 + 24), ( G(h(3, 28, -fi1304 + 24), v4[28,1],h(1, 1, 0)) - ( G(h(3, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 27)) Kro G(h(1, 28, -fi1304 + 27), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 3x1 -> 4x1
    _t73_16 = _t73_8;

    // AVX Loader:

    // 3x1 -> 4x1
    _t73_17 = _t73_5;

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_18 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_7, _t73_7, 32), _mm256_permute2f128_pd(_t73_7, _t73_7, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t73_19 = _mm256_mul_pd(_t73_17, _t73_18);

    // 4-BLAC: 4x1 - 4x1
    _t73_20 = _mm256_sub_pd(_t73_16, _t73_19);

    // AVX Storer:
    _t73_8 = _t73_20;

    // Generating : v4[28,1] = S(h(1, 28, -fi1304 + 26), ( G(h(1, 28, -fi1304 + 26), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 26), U0[28,28],h(1, 28, -fi1304 + 26)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_21 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t73_8, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t73_8, 4), 129);

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_22 = _t73_4;

    // 4-BLAC: 1x4 / 1x4
    _t73_23 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t73_21), _mm256_castpd256_pd128(_t73_22)));

    // AVX Storer:
    _t73_9 = _t73_23;

    // Generating : v4[28,1] = S(h(2, 28, -fi1304 + 24), ( G(h(2, 28, -fi1304 + 24), v4[28,1],h(1, 1, 0)) - ( G(h(2, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 26)) Kro G(h(1, 28, -fi1304 + 26), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 2x1 -> 4x1
    _t73_24 = _mm256_blend_pd(_mm256_setzero_pd(), _t73_8, 3);

    // AVX Loader:

    // 2x1 -> 4x1
    _t73_25 = _t73_3;

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t73_9, _t73_9, 32), _mm256_permute2f128_pd(_t73_9, _t73_9, 32), 0);

    // 4-BLAC: 4x1 Kro 1x4
    _t73_27 = _mm256_mul_pd(_t73_25, _t73_26);

    // 4-BLAC: 4x1 - 4x1
    _t73_28 = _mm256_sub_pd(_t73_24, _t73_27);

    // AVX Storer:
    _t73_10 = _t73_28;

    // Generating : v4[28,1] = S(h(1, 28, -fi1304 + 25), ( G(h(1, 28, -fi1304 + 25), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 25), U0[28,28],h(1, 28, -fi1304 + 25)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_29 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t73_10, 2), _mm256_setzero_pd());

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_30 = _t73_2;

    // 4-BLAC: 1x4 / 1x4
    _t73_31 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t73_29), _mm256_castpd256_pd128(_t73_30)));

    // AVX Storer:
    _t73_11 = _t73_31;

    // Generating : v4[28,1] = S(h(1, 28, -fi1304 + 24), ( G(h(1, 28, -fi1304 + 24), v4[28,1],h(1, 1, 0)) - ( G(h(1, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 25)) Kro G(h(1, 28, -fi1304 + 25), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_32 = _mm256_blend_pd(_mm256_setzero_pd(), _t73_10, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_33 = _t73_1;

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_34 = _t73_11;

    // 4-BLAC: 1x4 Kro 1x4
    _t73_35 = _mm256_mul_pd(_t73_33, _t73_34);

    // 4-BLAC: 1x4 - 1x4
    _t73_36 = _mm256_sub_pd(_t73_32, _t73_35);

    // AVX Storer:
    _t73_12 = _t73_36;

    // Generating : v4[28,1] = S(h(1, 28, -fi1304 + 24), ( G(h(1, 28, -fi1304 + 24), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 24)) ),h(1, 1, 0))

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_37 = _t73_12;

    // AVX Loader:

    // 1x1 -> 1x4
    _t73_38 = _t73_0;

    // 4-BLAC: 1x4 / 1x4
    _t73_39 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t73_37), _mm256_castpd256_pd128(_t73_38)));

    // AVX Storer:
    _t73_12 = _t73_39;

    // Generating : v4[28,1] = Sum_{k3} ( S(h(4, 28, k3), ( G(h(4, 28, k3), v4[28,1],h(1, 1, 0)) - ( G(h(4, 28, k3), U0[28,28],h(4, 28, -fi1304 + 24)) * G(h(4, 28, -fi1304 + 24), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

    // AVX Loader:
    _mm_store_sd(&(v0[-fi1304 + 27]), _mm256_castpd256_pd128(_t73_7));
    _mm_store_sd(&(v0[-fi1304 + 26]), _mm256_castpd256_pd128(_t73_9));
    _mm_store_sd(&(v0[-fi1304 + 25]), _mm256_castpd256_pd128(_t73_11));
    _mm_store_sd(&(v0[-fi1304 + 24]), _mm256_castpd256_pd128(_t73_12));

    for( int k3 = 0; k3 <= -fi1304 + 23; k3+=4 ) {
      _t74_9 = _asm256_loadu_pd(v0 + k3);
      _t74_7 = _asm256_loadu_pd(M3 + -fi1304 + 28*k3 + 24);
      _t74_6 = _asm256_loadu_pd(M3 + -fi1304 + 28*k3 + 52);
      _t74_5 = _asm256_loadu_pd(M3 + -fi1304 + 28*k3 + 80);
      _t74_4 = _asm256_loadu_pd(M3 + -fi1304 + 28*k3 + 108);
      _t74_3 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 27])));
      _t74_2 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 26])));
      _t74_1 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 25])));
      _t74_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[-fi1304 + 24])));

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t74_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t74_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32)), _mm256_mul_pd(_t74_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t74_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32)), _mm256_mul_pd(_t74_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t74_7, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32)), _mm256_mul_pd(_t74_6, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t74_5, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32)), _mm256_mul_pd(_t74_4, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t74_0, _t74_1), _mm256_unpacklo_pd(_t74_2, _t74_3), 32))), 12));

      // 4-BLAC: 4x1 - 4x1
      _t74_9 = _mm256_sub_pd(_t74_9, _t74_8);

      // AVX Storer:
      _asm256_storeu_pd(v0 + k3, _t74_9);
    }
  }

  _t48_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[59])));
  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[58])));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29])));
  _t48_4 = _mm256_maskload_pd(M3 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[87])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t75_0 = _mm256_castpd128_pd256(_mm_load_sd(&(v0[3])));
  _t75_1 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

  // Generating : v4[28,1] = S(h(1, 28, 3), ( G(h(1, 28, 3), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, 3), U0[28,28],h(1, 28, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_6 = _t75_0;

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_7 = _t48_8;

  // 4-BLAC: 1x4 / 1x4
  _t75_8 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t75_6), _mm256_castpd256_pd128(_t75_7)));

  // AVX Storer:
  _t75_0 = _t75_8;

  // Generating : v4[28,1] = S(h(3, 28, 0), ( G(h(3, 28, 0), v4[28,1],h(1, 1, 0)) - ( G(h(3, 28, 0), U0[28,28],h(1, 28, 3)) Kro G(h(1, 28, 3), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t75_9 = _t75_1;

  // AVX Loader:

  // 3x1 -> 4x1
  _t75_10 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_11 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t75_0, _t75_0, 32), _mm256_permute2f128_pd(_t75_0, _t75_0, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t75_12 = _mm256_mul_pd(_t75_10, _t75_11);

  // 4-BLAC: 4x1 - 4x1
  _t75_13 = _mm256_sub_pd(_t75_9, _t75_12);

  // AVX Storer:
  _t75_1 = _t75_13;

  // Generating : v4[28,1] = S(h(1, 28, 2), ( G(h(1, 28, 2), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, 2), U0[28,28],h(1, 28, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_14 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t75_1, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t75_1, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_15 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t75_16 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t75_14), _mm256_castpd256_pd128(_t75_15)));

  // AVX Storer:
  _t75_2 = _t75_16;

  // Generating : v4[28,1] = S(h(2, 28, 0), ( G(h(2, 28, 0), v4[28,1],h(1, 1, 0)) - ( G(h(2, 28, 0), U0[28,28],h(1, 28, 2)) Kro G(h(1, 28, 2), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t75_17 = _mm256_blend_pd(_mm256_setzero_pd(), _t75_1, 3);

  // AVX Loader:

  // 2x1 -> 4x1
  _t75_18 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_19 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t75_2, _t75_2, 32), _mm256_permute2f128_pd(_t75_2, _t75_2, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t75_20 = _mm256_mul_pd(_t75_18, _t75_19);

  // 4-BLAC: 4x1 - 4x1
  _t75_21 = _mm256_sub_pd(_t75_17, _t75_20);

  // AVX Storer:
  _t75_3 = _t75_21;

  // Generating : v4[28,1] = S(h(1, 28, 1), ( G(h(1, 28, 1), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, 1), U0[28,28],h(1, 28, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_22 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t75_3, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_23 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t75_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t75_22), _mm256_castpd256_pd128(_t75_23)));

  // AVX Storer:
  _t75_4 = _t75_24;

  // Generating : v4[28,1] = S(h(1, 28, 0), ( G(h(1, 28, 0), v4[28,1],h(1, 1, 0)) - ( G(h(1, 28, 0), U0[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), v4[28,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_25 = _mm256_blend_pd(_mm256_setzero_pd(), _t75_3, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_26 = _mm256_blend_pd(_mm256_setzero_pd(), _t48_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_27 = _t75_4;

  // 4-BLAC: 1x4 Kro 1x4
  _t75_28 = _mm256_mul_pd(_t75_26, _t75_27);

  // 4-BLAC: 1x4 - 1x4
  _t75_29 = _mm256_sub_pd(_t75_25, _t75_28);

  // AVX Storer:
  _t75_5 = _t75_29;

  // Generating : v4[28,1] = S(h(1, 28, 0), ( G(h(1, 28, 0), v4[28,1],h(1, 1, 0)) Div G(h(1, 28, 0), U0[28,28],h(1, 28, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_30 = _t75_5;

  // AVX Loader:

  // 1x1 -> 1x4
  _t75_31 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t75_32 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t75_30), _mm256_castpd256_pd128(_t75_31)));

  // AVX Storer:
  _t75_5 = _t75_32;


  for( int fi1304 = 0; fi1304 <= 23; fi1304+=4 ) {
    _t76_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304])));
    _t76_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 29])));
    _t76_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 58])));
    _t76_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29*fi1304 + 87])));
    _t76_14 = _asm256_loadu_pd(M1 + 28*fi1304);
    _t76_11 = _asm256_loadu_pd(M1 + 28*fi1304 + 28);
    _t76_12 = _asm256_loadu_pd(M1 + 28*fi1304 + 56);
    _t76_13 = _asm256_loadu_pd(M1 + 28*fi1304 + 84);
    _t76_2 = _mm256_maskload_pd(M3 + 29*fi1304 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
    _t76_1 = _mm256_maskload_pd(M3 + 29*fi1304 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
    _t76_0 = _mm256_broadcast_sd(&(M3[29*fi1304 + 59]));

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304), U0[28,28],h(1, 28, fi1304)) ),h(1, 28, fi1304))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_17 = _t76_6;

    // 4-BLAC: 1x4 / 1x4
    _t76_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_16), _mm256_castpd256_pd128(_t76_17)));

    // AVX Storer:
    _t76_7 = _t76_18;

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 1), U0[28,28],h(1, 28, fi1304 + 1)) ),h(1, 28, fi1304 + 1))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_20 = _t76_5;

    // 4-BLAC: 1x4 / 1x4
    _t76_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_19), _mm256_castpd256_pd128(_t76_20)));

    // AVX Storer:
    _t76_8 = _t76_21;

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 2), U0[28,28],h(1, 28, fi1304 + 2)) ),h(1, 28, fi1304 + 2))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_23 = _t76_4;

    // 4-BLAC: 1x4 / 1x4
    _t76_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_22), _mm256_castpd256_pd128(_t76_23)));

    // AVX Storer:
    _t76_9 = _t76_24;

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, fi1304 + 3), U0[28,28],h(1, 28, fi1304 + 3)) ),h(1, 28, fi1304 + 3))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t76_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_26 = _t76_3;

    // 4-BLAC: 1x4 / 1x4
    _t76_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t76_25), _mm256_castpd256_pd128(_t76_26)));

    // AVX Storer:
    _t76_10 = _t76_27;

    // Generating : M6[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_7, _t76_7, 32), _mm256_permute2f128_pd(_t76_7, _t76_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_14 = _mm256_mul_pd(_t76_28, _t76_14);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(3, 28, fi1304 + 1), ( G(h(3, 28, fi1304 + 1), M6[28,28],h(4, 28, fi1429)) - ( T( G(h(1, 28, fi1304), U0[28,28],h(3, 28, fi1304 + 1)) ) * G(h(1, 28, fi1304), M6[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

    // AVX Loader:

    // 3x4 -> 4x4
    _t76_29 = _t76_11;
    _t76_30 = _t76_12;
    _t76_31 = _t76_13;
    _t76_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t76_33 = _t76_2;

    // 4-BLAC: (1x4)^T
    _t76_34 = _t76_33;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t76_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 32), _mm256_permute2f128_pd(_t76_34, _t76_34, 32), 0), _t76_14);
    _t76_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 32), _mm256_permute2f128_pd(_t76_34, _t76_34, 32), 15), _t76_14);
    _t76_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 49), _mm256_permute2f128_pd(_t76_34, _t76_34, 49), 0), _t76_14);
    _t76_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 49), _mm256_permute2f128_pd(_t76_34, _t76_34, 49), 15), _t76_14);

    // 4-BLAC: 4x4 - 4x4
    _t76_39 = _mm256_sub_pd(_t76_29, _t76_35);
    _t76_40 = _mm256_sub_pd(_t76_30, _t76_36);
    _t76_41 = _mm256_sub_pd(_t76_31, _t76_37);
    _t76_42 = _mm256_sub_pd(_t76_32, _t76_38);

    // AVX Storer:
    _t76_11 = _t76_39;
    _t76_12 = _t76_40;
    _t76_13 = _t76_41;

    // Generating : M6[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_43 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_8, _t76_8, 32), _mm256_permute2f128_pd(_t76_8, _t76_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_11 = _mm256_mul_pd(_t76_43, _t76_11);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(2, 28, fi1304 + 2), ( G(h(2, 28, fi1304 + 2), M6[28,28],h(4, 28, fi1429)) - ( T( G(h(1, 28, fi1304 + 1), U0[28,28],h(2, 28, fi1304 + 2)) ) * G(h(1, 28, fi1304 + 1), M6[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

    // AVX Loader:

    // 2x4 -> 4x4
    _t76_44 = _t76_12;
    _t76_45 = _t76_13;
    _t76_46 = _mm256_setzero_pd();
    _t76_47 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t76_48 = _t76_1;

    // 4-BLAC: (1x4)^T
    _t76_49 = _t76_48;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t76_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 32), _mm256_permute2f128_pd(_t76_49, _t76_49, 32), 0), _t76_11);
    _t76_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 32), _mm256_permute2f128_pd(_t76_49, _t76_49, 32), 15), _t76_11);
    _t76_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 49), _mm256_permute2f128_pd(_t76_49, _t76_49, 49), 0), _t76_11);
    _t76_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 49), _mm256_permute2f128_pd(_t76_49, _t76_49, 49), 15), _t76_11);

    // 4-BLAC: 4x4 - 4x4
    _t76_54 = _mm256_sub_pd(_t76_44, _t76_50);
    _t76_55 = _mm256_sub_pd(_t76_45, _t76_51);
    _t76_56 = _mm256_sub_pd(_t76_46, _t76_52);
    _t76_57 = _mm256_sub_pd(_t76_47, _t76_53);

    // AVX Storer:
    _t76_12 = _t76_54;
    _t76_13 = _t76_55;

    // Generating : M6[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304 + 2)) Kro G(h(1, 28, fi1304 + 2), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_9, _t76_9, 32), _mm256_permute2f128_pd(_t76_9, _t76_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_12 = _mm256_mul_pd(_t76_58, _t76_12);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), M6[28,28],h(4, 28, fi1429)) - ( T( G(h(1, 28, fi1304 + 2), U0[28,28],h(1, 28, fi1304 + 3)) ) Kro G(h(1, 28, fi1304 + 2), M6[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_59 = _t76_0;

    // 4-BLAC: (4x1)^T
    _t76_60 = _t76_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_15 = _mm256_mul_pd(_t76_60, _t76_12);

    // 4-BLAC: 1x4 - 1x4
    _t76_13 = _mm256_sub_pd(_t76_13, _t76_15);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304 + 3)) Kro G(h(1, 28, fi1304 + 3), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t76_61 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_10, _t76_10, 32), _mm256_permute2f128_pd(_t76_10, _t76_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t76_13 = _mm256_mul_pd(_t76_61, _t76_13);

    // AVX Storer:

    for( int fi1429 = 4; fi1429 <= 24; fi1429+=4 ) {
      _t77_3 = _asm256_loadu_pd(M1 + 28*fi1304 + fi1429);
      _t77_0 = _asm256_loadu_pd(M1 + 28*fi1304 + fi1429 + 28);
      _t77_1 = _asm256_loadu_pd(M1 + 28*fi1304 + fi1429 + 56);
      _t77_2 = _asm256_loadu_pd(M1 + 28*fi1304 + fi1429 + 84);

      // Generating : M6[28,28] = S(h(1, 28, fi1304), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304)) Kro G(h(1, 28, fi1304), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_7, _t76_7, 32), _mm256_permute2f128_pd(_t76_7, _t76_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_3 = _mm256_mul_pd(_t77_4, _t77_3);

      // AVX Storer:

      // Generating : M6[28,28] = S(h(3, 28, fi1304 + 1), ( G(h(3, 28, fi1304 + 1), M6[28,28],h(4, 28, fi1429)) - ( T( G(h(1, 28, fi1304), U0[28,28],h(3, 28, fi1304 + 1)) ) * G(h(1, 28, fi1304), M6[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

      // AVX Loader:

      // 3x4 -> 4x4
      _t77_5 = _t77_0;
      _t77_6 = _t77_1;
      _t77_7 = _t77_2;
      _t77_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x3 -> 1x4
      _t77_9 = _t76_2;

      // 4-BLAC: (1x4)^T
      _t76_34 = _t77_9;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t76_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 32), _mm256_permute2f128_pd(_t76_34, _t76_34, 32), 0), _t77_3);
      _t76_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 32), _mm256_permute2f128_pd(_t76_34, _t76_34, 32), 15), _t77_3);
      _t76_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 49), _mm256_permute2f128_pd(_t76_34, _t76_34, 49), 0), _t77_3);
      _t76_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_34, _t76_34, 49), _mm256_permute2f128_pd(_t76_34, _t76_34, 49), 15), _t77_3);

      // 4-BLAC: 4x4 - 4x4
      _t77_10 = _mm256_sub_pd(_t77_5, _t76_35);
      _t77_11 = _mm256_sub_pd(_t77_6, _t76_36);
      _t77_12 = _mm256_sub_pd(_t77_7, _t76_37);
      _t77_13 = _mm256_sub_pd(_t77_8, _t76_38);

      // AVX Storer:
      _t77_0 = _t77_10;
      _t77_1 = _t77_11;
      _t77_2 = _t77_12;

      // Generating : M6[28,28] = S(h(1, 28, fi1304 + 1), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304 + 1)) Kro G(h(1, 28, fi1304 + 1), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_8, _t76_8, 32), _mm256_permute2f128_pd(_t76_8, _t76_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_0 = _mm256_mul_pd(_t77_14, _t77_0);

      // AVX Storer:

      // Generating : M6[28,28] = S(h(2, 28, fi1304 + 2), ( G(h(2, 28, fi1304 + 2), M6[28,28],h(4, 28, fi1429)) - ( T( G(h(1, 28, fi1304 + 1), U0[28,28],h(2, 28, fi1304 + 2)) ) * G(h(1, 28, fi1304 + 1), M6[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

      // AVX Loader:

      // 2x4 -> 4x4
      _t77_15 = _t77_1;
      _t77_16 = _t77_2;
      _t77_17 = _mm256_setzero_pd();
      _t77_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 1x2 -> 1x4
      _t77_19 = _t76_1;

      // 4-BLAC: (1x4)^T
      _t76_49 = _t77_19;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t76_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 32), _mm256_permute2f128_pd(_t76_49, _t76_49, 32), 0), _t77_0);
      _t76_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 32), _mm256_permute2f128_pd(_t76_49, _t76_49, 32), 15), _t77_0);
      _t76_52 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 49), _mm256_permute2f128_pd(_t76_49, _t76_49, 49), 0), _t77_0);
      _t76_53 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_49, _t76_49, 49), _mm256_permute2f128_pd(_t76_49, _t76_49, 49), 15), _t77_0);

      // 4-BLAC: 4x4 - 4x4
      _t77_20 = _mm256_sub_pd(_t77_15, _t76_50);
      _t77_21 = _mm256_sub_pd(_t77_16, _t76_51);
      _t77_22 = _mm256_sub_pd(_t77_17, _t76_52);
      _t77_23 = _mm256_sub_pd(_t77_18, _t76_53);

      // AVX Storer:
      _t77_1 = _t77_20;
      _t77_2 = _t77_21;

      // Generating : M6[28,28] = S(h(1, 28, fi1304 + 2), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304 + 2)) Kro G(h(1, 28, fi1304 + 2), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_9, _t76_9, 32), _mm256_permute2f128_pd(_t76_9, _t76_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_1 = _mm256_mul_pd(_t77_24, _t77_1);

      // AVX Storer:

      // Generating : M6[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 28, fi1304 + 3), M6[28,28],h(4, 28, fi1429)) - ( T( G(h(1, 28, fi1304 + 2), U0[28,28],h(1, 28, fi1304 + 3)) ) Kro G(h(1, 28, fi1304 + 2), M6[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_25 = _t76_0;

      // 4-BLAC: (4x1)^T
      _t76_60 = _t77_25;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t76_15 = _mm256_mul_pd(_t76_60, _t77_1);

      // 4-BLAC: 1x4 - 1x4
      _t77_2 = _mm256_sub_pd(_t77_2, _t76_15);

      // AVX Storer:

      // Generating : M6[28,28] = S(h(1, 28, fi1304 + 3), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, fi1304 + 3)) Kro G(h(1, 28, fi1304 + 3), M6[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t77_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t76_10, _t76_10, 32), _mm256_permute2f128_pd(_t76_10, _t76_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t77_2 = _mm256_mul_pd(_t77_26, _t77_2);

      // AVX Storer:
      _asm256_storeu_pd(M1 + 28*fi1304 + fi1429, _t77_3);
      _asm256_storeu_pd(M1 + 28*fi1304 + fi1429 + 28, _t77_0);
      _asm256_storeu_pd(M1 + 28*fi1304 + fi1429 + 56, _t77_1);
      _asm256_storeu_pd(M1 + 28*fi1304 + fi1429 + 84, _t77_2);
    }

    // Generating : M6[28,28] = Sum_{k3} ( Sum_{k2} ( S(h(4, 28, fi1304 + k3 + 4), ( G(h(4, 28, fi1304 + k3 + 4), M6[28,28],h(4, 28, k2)) - ( T( G(h(4, 28, fi1304), U0[28,28],h(4, 28, fi1304 + k3 + 4)) ) * G(h(4, 28, fi1304), M6[28,28],h(4, 28, k2)) ) ),h(4, 28, k2)) ) )
    _asm256_storeu_pd(M1 + 28*fi1304, _t76_14);
    _asm256_storeu_pd(M1 + 28*fi1304 + 28, _t76_11);
    _asm256_storeu_pd(M1 + 28*fi1304 + 56, _t76_12);
    _asm256_storeu_pd(M1 + 28*fi1304 + 84, _t76_13);

    for( int k3 = 0; k3 <= -fi1304 + 23; k3+=4 ) {

      for( int k2 = 0; k2 <= 27; k2+=4 ) {
        _t78_12 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 112);
        _t78_13 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 140);
        _t78_14 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 168);
        _t78_15 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 196);
        _t78_7 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 4);
        _t78_6 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 32);
        _t78_5 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 60);
        _t78_4 = _asm256_loadu_pd(M3 + 29*fi1304 + k3 + 88);
        _t78_3 = _asm256_loadu_pd(M1 + 28*fi1304 + k2);
        _t78_2 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 28);
        _t78_1 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 56);
        _t78_0 = _asm256_loadu_pd(M1 + 28*fi1304 + k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t78_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t78_7, _t78_6), _mm256_unpacklo_pd(_t78_5, _t78_4), 32);
        _t78_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t78_7, _t78_6), _mm256_unpackhi_pd(_t78_5, _t78_4), 32);
        _t78_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t78_7, _t78_6), _mm256_unpacklo_pd(_t78_5, _t78_4), 49);
        _t78_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t78_7, _t78_6), _mm256_unpackhi_pd(_t78_5, _t78_4), 49);

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t78_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_16, _t78_16, 32), _mm256_permute2f128_pd(_t78_16, _t78_16, 32), 0), _t78_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_16, _t78_16, 32), _mm256_permute2f128_pd(_t78_16, _t78_16, 32), 15), _t78_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_16, _t78_16, 49), _mm256_permute2f128_pd(_t78_16, _t78_16, 49), 0), _t78_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_16, _t78_16, 49), _mm256_permute2f128_pd(_t78_16, _t78_16, 49), 15), _t78_0)));
        _t78_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_17, _t78_17, 32), _mm256_permute2f128_pd(_t78_17, _t78_17, 32), 0), _t78_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_17, _t78_17, 32), _mm256_permute2f128_pd(_t78_17, _t78_17, 32), 15), _t78_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_17, _t78_17, 49), _mm256_permute2f128_pd(_t78_17, _t78_17, 49), 0), _t78_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_17, _t78_17, 49), _mm256_permute2f128_pd(_t78_17, _t78_17, 49), 15), _t78_0)));
        _t78_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_18, _t78_18, 32), _mm256_permute2f128_pd(_t78_18, _t78_18, 32), 0), _t78_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_18, _t78_18, 32), _mm256_permute2f128_pd(_t78_18, _t78_18, 32), 15), _t78_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_18, _t78_18, 49), _mm256_permute2f128_pd(_t78_18, _t78_18, 49), 0), _t78_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_18, _t78_18, 49), _mm256_permute2f128_pd(_t78_18, _t78_18, 49), 15), _t78_0)));
        _t78_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_19, _t78_19, 32), _mm256_permute2f128_pd(_t78_19, _t78_19, 32), 0), _t78_3), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_19, _t78_19, 32), _mm256_permute2f128_pd(_t78_19, _t78_19, 32), 15), _t78_2)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_19, _t78_19, 49), _mm256_permute2f128_pd(_t78_19, _t78_19, 49), 0), _t78_1), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t78_19, _t78_19, 49), _mm256_permute2f128_pd(_t78_19, _t78_19, 49), 15), _t78_0)));

        // 4-BLAC: 4x4 - 4x4
        _t78_12 = _mm256_sub_pd(_t78_12, _t78_8);
        _t78_13 = _mm256_sub_pd(_t78_13, _t78_9);
        _t78_14 = _mm256_sub_pd(_t78_14, _t78_10);
        _t78_15 = _mm256_sub_pd(_t78_15, _t78_11);

        // AVX Storer:
        _asm256_storeu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 112, _t78_12);
        _asm256_storeu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 140, _t78_13);
        _asm256_storeu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 168, _t78_14);
        _asm256_storeu_pd(M1 + 28*fi1304 + k2 + 28*k3 + 196, _t78_15);
      }
    }
  }

  _t69_4 = _mm256_maskload_pd(M3 + 726, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t69_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[755])));
  _t69_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[696])));
  _t69_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[725])));
  _t69_2 = _mm256_maskload_pd(M3 + 697, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t69_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[783])));
  _t69_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[754])));
  _t79_7 = _asm256_loadu_pd(M1 + 672);
  _t79_4 = _asm256_loadu_pd(M1 + 700);
  _t79_5 = _asm256_loadu_pd(M1 + 728);
  _t79_6 = _asm256_loadu_pd(M1 + 756);

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 24), U0[28,28],h(1, 28, 24)) ),h(1, 28, 24))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_10 = _t69_0;

  // 4-BLAC: 1x4 / 1x4
  _t79_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_9), _mm256_castpd256_pd128(_t79_10)));

  // AVX Storer:
  _t79_0 = _t79_11;

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 25), U0[28,28],h(1, 28, 25)) ),h(1, 28, 25))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_13 = _t69_3;

  // 4-BLAC: 1x4 / 1x4
  _t79_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_12), _mm256_castpd256_pd128(_t79_13)));

  // AVX Storer:
  _t79_1 = _t79_14;

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 26), U0[28,28],h(1, 28, 26)) ),h(1, 28, 26))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_16 = _t69_6;

  // 4-BLAC: 1x4 / 1x4
  _t79_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_15), _mm256_castpd256_pd128(_t79_16)));

  // AVX Storer:
  _t79_2 = _t79_17;

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 27), U0[28,28],h(1, 28, 27)) ),h(1, 28, 27))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t79_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_19 = _t69_8;

  // 4-BLAC: 1x4 / 1x4
  _t79_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t79_18), _mm256_castpd256_pd128(_t79_19)));

  // AVX Storer:
  _t79_3 = _t79_20;

  // Generating : M6[28,28] = S(h(1, 28, 24), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 24)) Kro G(h(1, 28, 24), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_0, _t79_0, 32), _mm256_permute2f128_pd(_t79_0, _t79_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_7 = _mm256_mul_pd(_t79_21, _t79_7);

  // AVX Storer:

  // Generating : M6[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), M6[28,28],h(4, 28, fi1304)) - ( T( G(h(1, 28, 24), U0[28,28],h(3, 28, 25)) ) * G(h(1, 28, 24), M6[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

  // AVX Loader:

  // 3x4 -> 4x4
  _t79_22 = _t79_4;
  _t79_23 = _t79_5;
  _t79_24 = _t79_6;
  _t79_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t79_26 = _t69_2;

  // 4-BLAC: (1x4)^T
  _t79_27 = _t79_26;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t79_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 32), _mm256_permute2f128_pd(_t79_27, _t79_27, 32), 0), _t79_7);
  _t79_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 32), _mm256_permute2f128_pd(_t79_27, _t79_27, 32), 15), _t79_7);
  _t79_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 49), _mm256_permute2f128_pd(_t79_27, _t79_27, 49), 0), _t79_7);
  _t79_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 49), _mm256_permute2f128_pd(_t79_27, _t79_27, 49), 15), _t79_7);

  // 4-BLAC: 4x4 - 4x4
  _t79_32 = _mm256_sub_pd(_t79_22, _t79_28);
  _t79_33 = _mm256_sub_pd(_t79_23, _t79_29);
  _t79_34 = _mm256_sub_pd(_t79_24, _t79_30);
  _t79_35 = _mm256_sub_pd(_t79_25, _t79_31);

  // AVX Storer:
  _t79_4 = _t79_32;
  _t79_5 = _t79_33;
  _t79_6 = _t79_34;

  // Generating : M6[28,28] = S(h(1, 28, 25), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 25)) Kro G(h(1, 28, 25), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_36 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_1, _t79_1, 32), _mm256_permute2f128_pd(_t79_1, _t79_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_4 = _mm256_mul_pd(_t79_36, _t79_4);

  // AVX Storer:

  // Generating : M6[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), M6[28,28],h(4, 28, fi1304)) - ( T( G(h(1, 28, 25), U0[28,28],h(2, 28, 26)) ) * G(h(1, 28, 25), M6[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

  // AVX Loader:

  // 2x4 -> 4x4
  _t79_37 = _t79_5;
  _t79_38 = _t79_6;
  _t79_39 = _mm256_setzero_pd();
  _t79_40 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t79_41 = _t69_4;

  // 4-BLAC: (1x4)^T
  _t79_42 = _t79_41;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t79_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 32), _mm256_permute2f128_pd(_t79_42, _t79_42, 32), 0), _t79_4);
  _t79_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 32), _mm256_permute2f128_pd(_t79_42, _t79_42, 32), 15), _t79_4);
  _t79_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 49), _mm256_permute2f128_pd(_t79_42, _t79_42, 49), 0), _t79_4);
  _t79_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 49), _mm256_permute2f128_pd(_t79_42, _t79_42, 49), 15), _t79_4);

  // 4-BLAC: 4x4 - 4x4
  _t79_47 = _mm256_sub_pd(_t79_37, _t79_43);
  _t79_48 = _mm256_sub_pd(_t79_38, _t79_44);
  _t79_49 = _mm256_sub_pd(_t79_39, _t79_45);
  _t79_50 = _mm256_sub_pd(_t79_40, _t79_46);

  // AVX Storer:
  _t79_5 = _t79_47;
  _t79_6 = _t79_48;

  // Generating : M6[28,28] = S(h(1, 28, 26), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 26)) Kro G(h(1, 28, 26), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_2, _t79_2, 32), _mm256_permute2f128_pd(_t79_2, _t79_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_5 = _mm256_mul_pd(_t79_51, _t79_5);

  // AVX Storer:

  // Generating : M6[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), M6[28,28],h(4, 28, fi1304)) - ( T( G(h(1, 28, 26), U0[28,28],h(1, 28, 27)) ) Kro G(h(1, 28, 26), M6[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_52 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_7, _t69_7, 32), _mm256_permute2f128_pd(_t69_7, _t69_7, 32), 0);

  // 4-BLAC: (4x1)^T
  _t79_53 = _t79_52;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_8 = _mm256_mul_pd(_t79_53, _t79_5);

  // 4-BLAC: 1x4 - 1x4
  _t79_6 = _mm256_sub_pd(_t79_6, _t79_8);

  // AVX Storer:

  // Generating : M6[28,28] = S(h(1, 28, 27), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 27)) Kro G(h(1, 28, 27), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t79_54 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_3, _t79_3, 32), _mm256_permute2f128_pd(_t79_3, _t79_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t79_6 = _mm256_mul_pd(_t79_54, _t79_6);

  // AVX Storer:


  for( int fi1304 = 4; fi1304 <= 24; fi1304+=4 ) {
    _t80_3 = _asm256_loadu_pd(M1 + fi1304 + 672);
    _t80_0 = _asm256_loadu_pd(M1 + fi1304 + 700);
    _t80_1 = _asm256_loadu_pd(M1 + fi1304 + 728);
    _t80_2 = _asm256_loadu_pd(M1 + fi1304 + 756);

    // Generating : M6[28,28] = S(h(1, 28, 24), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 24)) Kro G(h(1, 28, 24), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_0, _t79_0, 32), _mm256_permute2f128_pd(_t79_0, _t79_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_3 = _mm256_mul_pd(_t80_4, _t80_3);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(3, 28, 25), ( G(h(3, 28, 25), M6[28,28],h(4, 28, fi1304)) - ( T( G(h(1, 28, 24), U0[28,28],h(3, 28, 25)) ) * G(h(1, 28, 24), M6[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

    // AVX Loader:

    // 3x4 -> 4x4
    _t80_5 = _t80_0;
    _t80_6 = _t80_1;
    _t80_7 = _t80_2;
    _t80_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t80_9 = _t69_2;

    // 4-BLAC: (1x4)^T
    _t79_27 = _t80_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t79_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 32), _mm256_permute2f128_pd(_t79_27, _t79_27, 32), 0), _t80_3);
    _t79_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 32), _mm256_permute2f128_pd(_t79_27, _t79_27, 32), 15), _t80_3);
    _t79_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 49), _mm256_permute2f128_pd(_t79_27, _t79_27, 49), 0), _t80_3);
    _t79_31 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_27, _t79_27, 49), _mm256_permute2f128_pd(_t79_27, _t79_27, 49), 15), _t80_3);

    // 4-BLAC: 4x4 - 4x4
    _t80_10 = _mm256_sub_pd(_t80_5, _t79_28);
    _t80_11 = _mm256_sub_pd(_t80_6, _t79_29);
    _t80_12 = _mm256_sub_pd(_t80_7, _t79_30);
    _t80_13 = _mm256_sub_pd(_t80_8, _t79_31);

    // AVX Storer:
    _t80_0 = _t80_10;
    _t80_1 = _t80_11;
    _t80_2 = _t80_12;

    // Generating : M6[28,28] = S(h(1, 28, 25), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 25)) Kro G(h(1, 28, 25), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_1, _t79_1, 32), _mm256_permute2f128_pd(_t79_1, _t79_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_0 = _mm256_mul_pd(_t80_14, _t80_0);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(2, 28, 26), ( G(h(2, 28, 26), M6[28,28],h(4, 28, fi1304)) - ( T( G(h(1, 28, 25), U0[28,28],h(2, 28, 26)) ) * G(h(1, 28, 25), M6[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

    // AVX Loader:

    // 2x4 -> 4x4
    _t80_15 = _t80_1;
    _t80_16 = _t80_2;
    _t80_17 = _mm256_setzero_pd();
    _t80_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t80_19 = _t69_4;

    // 4-BLAC: (1x4)^T
    _t79_42 = _t80_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t79_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 32), _mm256_permute2f128_pd(_t79_42, _t79_42, 32), 0), _t80_0);
    _t79_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 32), _mm256_permute2f128_pd(_t79_42, _t79_42, 32), 15), _t80_0);
    _t79_45 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 49), _mm256_permute2f128_pd(_t79_42, _t79_42, 49), 0), _t80_0);
    _t79_46 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_42, _t79_42, 49), _mm256_permute2f128_pd(_t79_42, _t79_42, 49), 15), _t80_0);

    // 4-BLAC: 4x4 - 4x4
    _t80_20 = _mm256_sub_pd(_t80_15, _t79_43);
    _t80_21 = _mm256_sub_pd(_t80_16, _t79_44);
    _t80_22 = _mm256_sub_pd(_t80_17, _t79_45);
    _t80_23 = _mm256_sub_pd(_t80_18, _t79_46);

    // AVX Storer:
    _t80_1 = _t80_20;
    _t80_2 = _t80_21;

    // Generating : M6[28,28] = S(h(1, 28, 26), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 26)) Kro G(h(1, 28, 26), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_2, _t79_2, 32), _mm256_permute2f128_pd(_t79_2, _t79_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_1 = _mm256_mul_pd(_t80_24, _t80_1);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(1, 28, 27), ( G(h(1, 28, 27), M6[28,28],h(4, 28, fi1304)) - ( T( G(h(1, 28, 26), U0[28,28],h(1, 28, 27)) ) Kro G(h(1, 28, 26), M6[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t69_7, _t69_7, 32), _mm256_permute2f128_pd(_t69_7, _t69_7, 32), 0);

    // 4-BLAC: (4x1)^T
    _t79_53 = _t80_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t79_8 = _mm256_mul_pd(_t79_53, _t80_1);

    // 4-BLAC: 1x4 - 1x4
    _t80_2 = _mm256_sub_pd(_t80_2, _t79_8);

    // AVX Storer:

    // Generating : M6[28,28] = S(h(1, 28, 27), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 27)) Kro G(h(1, 28, 27), M6[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t80_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t79_3, _t79_3, 32), _mm256_permute2f128_pd(_t79_3, _t79_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t80_2 = _mm256_mul_pd(_t80_26, _t80_2);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi1304 + 672, _t80_3);
    _asm256_storeu_pd(M1 + fi1304 + 700, _t80_0);
    _asm256_storeu_pd(M1 + fi1304 + 728, _t80_1);
    _asm256_storeu_pd(M1 + fi1304 + 756, _t80_2);
  }

  _asm256_storeu_pd(M1 + 672, _t79_7);
  _asm256_storeu_pd(M1 + 700, _t79_4);
  _asm256_storeu_pd(M1 + 728, _t79_5);
  _asm256_storeu_pd(M1 + 756, _t79_6);

  for( int fi1304 = 0; fi1304 <= 23; fi1304+=4 ) {
    _t81_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 783])));
    _t81_5 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 754])));
    _t81_4 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 725])));
    _t81_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[-29*fi1304 + 696])));
    _t81_14 = _asm256_loadu_pd(M1 + -28*fi1304 + 756);
    _t81_11 = _asm256_loadu_pd(M1 + -28*fi1304 + 672);
    _t81_12 = _asm256_loadu_pd(M1 + -28*fi1304 + 700);
    _t81_13 = _asm256_loadu_pd(M1 + -28*fi1304 + 728);
    _t81_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 699)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 727))), _mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 755)), 32);
    _t81_1 = _mm256_shuffle_pd(_mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 698)), _mm256_castpd128_pd256(_mm_load_sd(M3 + -29*fi1304 + 726)), 0);
    _t81_0 = _mm256_broadcast_sd(&(M3[-29*fi1304 + 697]));

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 27), U0[28,28],h(1, 28, -fi1304 + 27)) ),h(1, 28, -fi1304 + 27))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t81_16 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_17 = _t81_6;

    // 4-BLAC: 1x4 / 1x4
    _t81_18 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t81_16), _mm256_castpd256_pd128(_t81_17)));

    // AVX Storer:
    _t81_7 = _t81_18;

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 26), U0[28,28],h(1, 28, -fi1304 + 26)) ),h(1, 28, -fi1304 + 26))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t81_19 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_20 = _t81_5;

    // 4-BLAC: 1x4 / 1x4
    _t81_21 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t81_19), _mm256_castpd256_pd128(_t81_20)));

    // AVX Storer:
    _t81_8 = _t81_21;

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 25), U0[28,28],h(1, 28, -fi1304 + 25)) ),h(1, 28, -fi1304 + 25))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t81_22 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_23 = _t81_4;

    // 4-BLAC: 1x4 / 1x4
    _t81_24 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t81_22), _mm256_castpd256_pd128(_t81_23)));

    // AVX Storer:
    _t81_9 = _t81_24;

    // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 24)) ),h(1, 28, -fi1304 + 24))

    // AVX Loader:

    // Constant 1x1 -> 1x4
    _t81_25 = _mm256_set_pd(0, 0, 0, 1);

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_26 = _t81_3;

    // 4-BLAC: 1x4 / 1x4
    _t81_27 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t81_25), _mm256_castpd256_pd128(_t81_26)));

    // AVX Storer:
    _t81_10 = _t81_27;

    // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 27), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 27)) Kro G(h(1, 28, -fi1304 + 27), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_7, _t81_7, 32), _mm256_permute2f128_pd(_t81_7, _t81_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t81_14 = _mm256_mul_pd(_t81_28, _t81_14);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(3, 28, -fi1304 + 24), ( G(h(3, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) - ( G(h(3, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 27)) * G(h(1, 28, -fi1304 + 27), M8[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

    // AVX Loader:

    // 3x4 -> 4x4
    _t81_29 = _t81_11;
    _t81_30 = _t81_12;
    _t81_31 = _t81_13;
    _t81_32 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t81_33 = _t81_2;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t81_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_33, _t81_33, 32), _mm256_permute2f128_pd(_t81_33, _t81_33, 32), 0), _t81_14);
    _t81_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_33, _t81_33, 32), _mm256_permute2f128_pd(_t81_33, _t81_33, 32), 15), _t81_14);
    _t81_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_33, _t81_33, 49), _mm256_permute2f128_pd(_t81_33, _t81_33, 49), 0), _t81_14);
    _t81_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_33, _t81_33, 49), _mm256_permute2f128_pd(_t81_33, _t81_33, 49), 15), _t81_14);

    // 4-BLAC: 4x4 - 4x4
    _t81_38 = _mm256_sub_pd(_t81_29, _t81_34);
    _t81_39 = _mm256_sub_pd(_t81_30, _t81_35);
    _t81_40 = _mm256_sub_pd(_t81_31, _t81_36);
    _t81_41 = _mm256_sub_pd(_t81_32, _t81_37);

    // AVX Storer:
    _t81_11 = _t81_38;
    _t81_12 = _t81_39;
    _t81_13 = _t81_40;

    // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 26), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 26)) Kro G(h(1, 28, -fi1304 + 26), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_42 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_8, _t81_8, 32), _mm256_permute2f128_pd(_t81_8, _t81_8, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t81_13 = _mm256_mul_pd(_t81_42, _t81_13);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(2, 28, -fi1304 + 24), ( G(h(2, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) - ( G(h(2, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 26)) * G(h(1, 28, -fi1304 + 26), M8[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

    // AVX Loader:

    // 2x4 -> 4x4
    _t81_43 = _t81_11;
    _t81_44 = _t81_12;
    _t81_45 = _mm256_setzero_pd();
    _t81_46 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t81_47 = _t81_1;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t81_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_47, _t81_47, 32), _mm256_permute2f128_pd(_t81_47, _t81_47, 32), 0), _t81_13);
    _t81_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_47, _t81_47, 32), _mm256_permute2f128_pd(_t81_47, _t81_47, 32), 15), _t81_13);
    _t81_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_47, _t81_47, 49), _mm256_permute2f128_pd(_t81_47, _t81_47, 49), 0), _t81_13);
    _t81_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_47, _t81_47, 49), _mm256_permute2f128_pd(_t81_47, _t81_47, 49), 15), _t81_13);

    // 4-BLAC: 4x4 - 4x4
    _t81_52 = _mm256_sub_pd(_t81_43, _t81_48);
    _t81_53 = _mm256_sub_pd(_t81_44, _t81_49);
    _t81_54 = _mm256_sub_pd(_t81_45, _t81_50);
    _t81_55 = _mm256_sub_pd(_t81_46, _t81_51);

    // AVX Storer:
    _t81_11 = _t81_52;
    _t81_12 = _t81_53;

    // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 25), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 25)) Kro G(h(1, 28, -fi1304 + 25), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_56 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_9, _t81_9, 32), _mm256_permute2f128_pd(_t81_9, _t81_9, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t81_12 = _mm256_mul_pd(_t81_56, _t81_12);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 24), ( G(h(1, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) - ( G(h(1, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 25)) Kro G(h(1, 28, -fi1304 + 25), M8[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_57 = _t81_0;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t81_15 = _mm256_mul_pd(_t81_57, _t81_12);

    // 4-BLAC: 1x4 - 1x4
    _t81_11 = _mm256_sub_pd(_t81_11, _t81_15);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 24), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 24)) Kro G(h(1, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

    // AVX Loader:

    // 1x1 -> 1x4
    _t81_58 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_10, _t81_10, 32), _mm256_permute2f128_pd(_t81_10, _t81_10, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t81_11 = _mm256_mul_pd(_t81_58, _t81_11);

    // AVX Storer:

    for( int fi1429 = 4; fi1429 <= 24; fi1429+=4 ) {
      _t82_3 = _asm256_loadu_pd(M1 + -28*fi1304 + fi1429 + 756);
      _t82_0 = _asm256_loadu_pd(M1 + -28*fi1304 + fi1429 + 672);
      _t82_1 = _asm256_loadu_pd(M1 + -28*fi1304 + fi1429 + 700);
      _t82_2 = _asm256_loadu_pd(M1 + -28*fi1304 + fi1429 + 728);

      // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 27), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 27)) Kro G(h(1, 28, -fi1304 + 27), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t82_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_7, _t81_7, 32), _mm256_permute2f128_pd(_t81_7, _t81_7, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t82_3 = _mm256_mul_pd(_t82_4, _t82_3);

      // AVX Storer:

      // Generating : M8[28,28] = S(h(3, 28, -fi1304 + 24), ( G(h(3, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) - ( G(h(3, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 27)) * G(h(1, 28, -fi1304 + 27), M8[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

      // AVX Loader:

      // 3x4 -> 4x4
      _t82_5 = _t82_0;
      _t82_6 = _t82_1;
      _t82_7 = _t82_2;
      _t82_8 = _mm256_setzero_pd();

      // AVX Loader:

      // 3x1 -> 4x1
      _t82_9 = _t81_2;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t81_34 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_9, _t82_9, 32), _mm256_permute2f128_pd(_t82_9, _t82_9, 32), 0), _t82_3);
      _t81_35 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_9, _t82_9, 32), _mm256_permute2f128_pd(_t82_9, _t82_9, 32), 15), _t82_3);
      _t81_36 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_9, _t82_9, 49), _mm256_permute2f128_pd(_t82_9, _t82_9, 49), 0), _t82_3);
      _t81_37 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_9, _t82_9, 49), _mm256_permute2f128_pd(_t82_9, _t82_9, 49), 15), _t82_3);

      // 4-BLAC: 4x4 - 4x4
      _t82_10 = _mm256_sub_pd(_t82_5, _t81_34);
      _t82_11 = _mm256_sub_pd(_t82_6, _t81_35);
      _t82_12 = _mm256_sub_pd(_t82_7, _t81_36);
      _t82_13 = _mm256_sub_pd(_t82_8, _t81_37);

      // AVX Storer:
      _t82_0 = _t82_10;
      _t82_1 = _t82_11;
      _t82_2 = _t82_12;

      // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 26), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 26)) Kro G(h(1, 28, -fi1304 + 26), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t82_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_8, _t81_8, 32), _mm256_permute2f128_pd(_t81_8, _t81_8, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t82_2 = _mm256_mul_pd(_t82_14, _t82_2);

      // AVX Storer:

      // Generating : M8[28,28] = S(h(2, 28, -fi1304 + 24), ( G(h(2, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) - ( G(h(2, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 26)) * G(h(1, 28, -fi1304 + 26), M8[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

      // AVX Loader:

      // 2x4 -> 4x4
      _t82_15 = _t82_0;
      _t82_16 = _t82_1;
      _t82_17 = _mm256_setzero_pd();
      _t82_18 = _mm256_setzero_pd();

      // AVX Loader:

      // 2x1 -> 4x1
      _t82_19 = _t81_1;

      // AVX Loader:

      // 4-BLAC: 4x1 * 1x4
      _t81_48 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_19, _t82_19, 32), _mm256_permute2f128_pd(_t82_19, _t82_19, 32), 0), _t82_2);
      _t81_49 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_19, _t82_19, 32), _mm256_permute2f128_pd(_t82_19, _t82_19, 32), 15), _t82_2);
      _t81_50 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_19, _t82_19, 49), _mm256_permute2f128_pd(_t82_19, _t82_19, 49), 0), _t82_2);
      _t81_51 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t82_19, _t82_19, 49), _mm256_permute2f128_pd(_t82_19, _t82_19, 49), 15), _t82_2);

      // 4-BLAC: 4x4 - 4x4
      _t82_20 = _mm256_sub_pd(_t82_15, _t81_48);
      _t82_21 = _mm256_sub_pd(_t82_16, _t81_49);
      _t82_22 = _mm256_sub_pd(_t82_17, _t81_50);
      _t82_23 = _mm256_sub_pd(_t82_18, _t81_51);

      // AVX Storer:
      _t82_0 = _t82_20;
      _t82_1 = _t82_21;

      // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 25), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 25)) Kro G(h(1, 28, -fi1304 + 25), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t82_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_9, _t81_9, 32), _mm256_permute2f128_pd(_t81_9, _t81_9, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t82_1 = _mm256_mul_pd(_t82_24, _t82_1);

      // AVX Storer:

      // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 24), ( G(h(1, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) - ( G(h(1, 28, -fi1304 + 24), U0[28,28],h(1, 28, -fi1304 + 25)) Kro G(h(1, 28, -fi1304 + 25), M8[28,28],h(4, 28, fi1429)) ) ),h(4, 28, fi1429))

      // AVX Loader:

      // AVX Loader:

      // 1x1 -> 1x4
      _t82_25 = _t81_0;

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t81_15 = _mm256_mul_pd(_t82_25, _t82_1);

      // 4-BLAC: 1x4 - 1x4
      _t82_0 = _mm256_sub_pd(_t82_0, _t81_15);

      // AVX Storer:

      // Generating : M8[28,28] = S(h(1, 28, -fi1304 + 24), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, -fi1304 + 24)) Kro G(h(1, 28, -fi1304 + 24), M8[28,28],h(4, 28, fi1429)) ),h(4, 28, fi1429))

      // AVX Loader:

      // 1x1 -> 1x4
      _t82_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t81_10, _t81_10, 32), _mm256_permute2f128_pd(_t81_10, _t81_10, 32), 0);

      // AVX Loader:

      // 4-BLAC: 1x4 Kro 1x4
      _t82_0 = _mm256_mul_pd(_t82_26, _t82_0);

      // AVX Storer:
      _asm256_storeu_pd(M1 + -28*fi1304 + fi1429 + 756, _t82_3);
      _asm256_storeu_pd(M1 + -28*fi1304 + fi1429 + 728, _t82_2);
      _asm256_storeu_pd(M1 + -28*fi1304 + fi1429 + 700, _t82_1);
      _asm256_storeu_pd(M1 + -28*fi1304 + fi1429 + 672, _t82_0);
    }

    // Generating : M8[28,28] = Sum_{k3} ( Sum_{k2} ( S(h(4, 28, k3), ( G(h(4, 28, k3), M8[28,28],h(4, 28, k2)) - ( G(h(4, 28, k3), U0[28,28],h(4, 28, -fi1304 + 24)) * G(h(4, 28, -fi1304 + 24), M8[28,28],h(4, 28, k2)) ) ),h(4, 28, k2)) ) )
    _asm256_storeu_pd(M1 + -28*fi1304 + 756, _t81_14);
    _asm256_storeu_pd(M1 + -28*fi1304 + 728, _t81_13);
    _asm256_storeu_pd(M1 + -28*fi1304 + 700, _t81_12);
    _asm256_storeu_pd(M1 + -28*fi1304 + 672, _t81_11);

    for( int k3 = 0; k3 <= -fi1304 + 23; k3+=4 ) {

      for( int k2 = 0; k2 <= 27; k2+=4 ) {
        _t83_24 = _asm256_loadu_pd(M1 + k2 + 28*k3);
        _t83_25 = _asm256_loadu_pd(M1 + k2 + 28*k3 + 28);
        _t83_26 = _asm256_loadu_pd(M1 + k2 + 28*k3 + 56);
        _t83_27 = _asm256_loadu_pd(M1 + k2 + 28*k3 + 84);
        _t83_19 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 24);
        _t83_18 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 25);
        _t83_17 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 26);
        _t83_16 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 27);
        _t83_15 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 52);
        _t83_14 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 53);
        _t83_13 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 54);
        _t83_12 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 55);
        _t83_11 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 80);
        _t83_10 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 81);
        _t83_9 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 82);
        _t83_8 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 83);
        _t83_7 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 108);
        _t83_6 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 109);
        _t83_5 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 110);
        _t83_4 = _mm256_broadcast_sd(M3 + -fi1304 + 28*k3 + 111);
        _t83_3 = _asm256_loadu_pd(M1 + -28*fi1304 + k2 + 672);
        _t83_2 = _asm256_loadu_pd(M1 + -28*fi1304 + k2 + 700);
        _t83_1 = _asm256_loadu_pd(M1 + -28*fi1304 + k2 + 728);
        _t83_0 = _asm256_loadu_pd(M1 + -28*fi1304 + k2 + 756);

        // AVX Loader:

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t83_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_19, _t83_3), _mm256_mul_pd(_t83_18, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_17, _t83_1), _mm256_mul_pd(_t83_16, _t83_0)));
        _t83_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_15, _t83_3), _mm256_mul_pd(_t83_14, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_13, _t83_1), _mm256_mul_pd(_t83_12, _t83_0)));
        _t83_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_11, _t83_3), _mm256_mul_pd(_t83_10, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_9, _t83_1), _mm256_mul_pd(_t83_8, _t83_0)));
        _t83_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t83_7, _t83_3), _mm256_mul_pd(_t83_6, _t83_2)), _mm256_add_pd(_mm256_mul_pd(_t83_5, _t83_1), _mm256_mul_pd(_t83_4, _t83_0)));

        // 4-BLAC: 4x4 - 4x4
        _t83_24 = _mm256_sub_pd(_t83_24, _t83_20);
        _t83_25 = _mm256_sub_pd(_t83_25, _t83_21);
        _t83_26 = _mm256_sub_pd(_t83_26, _t83_22);
        _t83_27 = _mm256_sub_pd(_t83_27, _t83_23);

        // AVX Storer:
        _asm256_storeu_pd(M1 + k2 + 28*k3, _t83_24);
        _asm256_storeu_pd(M1 + k2 + 28*k3 + 28, _t83_25);
        _asm256_storeu_pd(M1 + k2 + 28*k3 + 56, _t83_26);
        _asm256_storeu_pd(M1 + k2 + 28*k3 + 84, _t83_27);
      }
    }
  }

  _t48_0 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[0])));
  _t48_3 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[29])));
  _t48_8 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[87])));
  _t48_2 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t48_7 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[59])));
  _t48_6 = _mm256_castpd128_pd256(_mm_load_sd(&(M3[58])));
  _t48_4 = _mm256_maskload_pd(M3 + 30, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t84_7 = _asm256_loadu_pd(M1 + 84);
  _t84_4 = _asm256_loadu_pd(M1);
  _t84_5 = _asm256_loadu_pd(M1 + 28);
  _t84_6 = _asm256_loadu_pd(M1 + 56);

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 3), U0[28,28],h(1, 28, 3)) ),h(1, 28, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t84_9 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_10 = _t48_8;

  // 4-BLAC: 1x4 / 1x4
  _t84_11 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t84_9), _mm256_castpd256_pd128(_t84_10)));

  // AVX Storer:
  _t84_0 = _t84_11;

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 2), U0[28,28],h(1, 28, 2)) ),h(1, 28, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t84_12 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_13 = _t48_6;

  // 4-BLAC: 1x4 / 1x4
  _t84_14 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t84_12), _mm256_castpd256_pd128(_t84_13)));

  // AVX Storer:
  _t84_1 = _t84_14;

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 1), U0[28,28],h(1, 28, 1)) ),h(1, 28, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t84_15 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_16 = _t48_3;

  // 4-BLAC: 1x4 / 1x4
  _t84_17 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t84_15), _mm256_castpd256_pd128(_t84_16)));

  // AVX Storer:
  _t84_2 = _t84_17;

  // Generating : T2407[1,28] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 28, 0), U0[28,28],h(1, 28, 0)) ),h(1, 28, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t84_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_19 = _t48_0;

  // 4-BLAC: 1x4 / 1x4
  _t84_20 = _mm256_castpd128_pd256(_mm_div_pd(_mm256_castpd256_pd128(_t84_18), _mm256_castpd256_pd128(_t84_19)));

  // AVX Storer:
  _t84_3 = _t84_20;

  // Generating : M8[28,28] = S(h(1, 28, 3), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 3)) Kro G(h(1, 28, 3), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_21 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_0, _t84_0, 32), _mm256_permute2f128_pd(_t84_0, _t84_0, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t84_7 = _mm256_mul_pd(_t84_21, _t84_7);

  // AVX Storer:

  // Generating : M8[28,28] = S(h(3, 28, 0), ( G(h(3, 28, 0), M8[28,28],h(4, 28, fi1304)) - ( G(h(3, 28, 0), U0[28,28],h(1, 28, 3)) * G(h(1, 28, 3), M8[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

  // AVX Loader:

  // 3x4 -> 4x4
  _t84_22 = _t84_4;
  _t84_23 = _t84_5;
  _t84_24 = _t84_6;
  _t84_25 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t84_26 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t84_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_26, _t84_26, 32), _mm256_permute2f128_pd(_t84_26, _t84_26, 32), 0), _t84_7);
  _t84_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_26, _t84_26, 32), _mm256_permute2f128_pd(_t84_26, _t84_26, 32), 15), _t84_7);
  _t84_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_26, _t84_26, 49), _mm256_permute2f128_pd(_t84_26, _t84_26, 49), 0), _t84_7);
  _t84_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_26, _t84_26, 49), _mm256_permute2f128_pd(_t84_26, _t84_26, 49), 15), _t84_7);

  // 4-BLAC: 4x4 - 4x4
  _t84_31 = _mm256_sub_pd(_t84_22, _t84_27);
  _t84_32 = _mm256_sub_pd(_t84_23, _t84_28);
  _t84_33 = _mm256_sub_pd(_t84_24, _t84_29);
  _t84_34 = _mm256_sub_pd(_t84_25, _t84_30);

  // AVX Storer:
  _t84_4 = _t84_31;
  _t84_5 = _t84_32;
  _t84_6 = _t84_33;

  // Generating : M8[28,28] = S(h(1, 28, 2), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 2)) Kro G(h(1, 28, 2), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_35 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_1, _t84_1, 32), _mm256_permute2f128_pd(_t84_1, _t84_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t84_6 = _mm256_mul_pd(_t84_35, _t84_6);

  // AVX Storer:

  // Generating : M8[28,28] = S(h(2, 28, 0), ( G(h(2, 28, 0), M8[28,28],h(4, 28, fi1304)) - ( G(h(2, 28, 0), U0[28,28],h(1, 28, 2)) * G(h(1, 28, 2), M8[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

  // AVX Loader:

  // 2x4 -> 4x4
  _t84_36 = _t84_4;
  _t84_37 = _t84_5;
  _t84_38 = _mm256_setzero_pd();
  _t84_39 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t84_40 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t84_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_40, _t84_40, 32), _mm256_permute2f128_pd(_t84_40, _t84_40, 32), 0), _t84_6);
  _t84_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_40, _t84_40, 32), _mm256_permute2f128_pd(_t84_40, _t84_40, 32), 15), _t84_6);
  _t84_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_40, _t84_40, 49), _mm256_permute2f128_pd(_t84_40, _t84_40, 49), 0), _t84_6);
  _t84_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_40, _t84_40, 49), _mm256_permute2f128_pd(_t84_40, _t84_40, 49), 15), _t84_6);

  // 4-BLAC: 4x4 - 4x4
  _t84_45 = _mm256_sub_pd(_t84_36, _t84_41);
  _t84_46 = _mm256_sub_pd(_t84_37, _t84_42);
  _t84_47 = _mm256_sub_pd(_t84_38, _t84_43);
  _t84_48 = _mm256_sub_pd(_t84_39, _t84_44);

  // AVX Storer:
  _t84_4 = _t84_45;
  _t84_5 = _t84_46;

  // Generating : M8[28,28] = S(h(1, 28, 1), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 1)) Kro G(h(1, 28, 1), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_49 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_2, _t84_2, 32), _mm256_permute2f128_pd(_t84_2, _t84_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t84_5 = _mm256_mul_pd(_t84_49, _t84_5);

  // AVX Storer:

  // Generating : M8[28,28] = S(h(1, 28, 0), ( G(h(1, 28, 0), M8[28,28],h(4, 28, fi1304)) - ( G(h(1, 28, 0), U0[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), M8[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_50 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t84_8 = _mm256_mul_pd(_t84_50, _t84_5);

  // 4-BLAC: 1x4 - 1x4
  _t84_4 = _mm256_sub_pd(_t84_4, _t84_8);

  // AVX Storer:

  // Generating : M8[28,28] = S(h(1, 28, 0), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 0)) Kro G(h(1, 28, 0), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

  // AVX Loader:

  // 1x1 -> 1x4
  _t84_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_3, _t84_3, 32), _mm256_permute2f128_pd(_t84_3, _t84_3, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t84_4 = _mm256_mul_pd(_t84_51, _t84_4);

  // AVX Storer:


  for( int fi1304 = 4; fi1304 <= 24; fi1304+=4 ) {
    _t85_3 = _asm256_loadu_pd(M1 + fi1304 + 84);
    _t85_0 = _asm256_loadu_pd(M1 + fi1304);
    _t85_1 = _asm256_loadu_pd(M1 + fi1304 + 28);
    _t85_2 = _asm256_loadu_pd(M1 + fi1304 + 56);

    // Generating : M8[28,28] = S(h(1, 28, 3), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 3)) Kro G(h(1, 28, 3), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t85_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_0, _t84_0, 32), _mm256_permute2f128_pd(_t84_0, _t84_0, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t85_3 = _mm256_mul_pd(_t85_4, _t85_3);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(3, 28, 0), ( G(h(3, 28, 0), M8[28,28],h(4, 28, fi1304)) - ( G(h(3, 28, 0), U0[28,28],h(1, 28, 3)) * G(h(1, 28, 3), M8[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

    // AVX Loader:

    // 3x4 -> 4x4
    _t85_5 = _t85_0;
    _t85_6 = _t85_1;
    _t85_7 = _t85_2;
    _t85_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t85_9 = _mm256_blend_pd(_mm256_permute2f128_pd(_t48_2, _t48_7, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t48_4, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t84_27 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_9, _t85_9, 32), _mm256_permute2f128_pd(_t85_9, _t85_9, 32), 0), _t85_3);
    _t84_28 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_9, _t85_9, 32), _mm256_permute2f128_pd(_t85_9, _t85_9, 32), 15), _t85_3);
    _t84_29 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_9, _t85_9, 49), _mm256_permute2f128_pd(_t85_9, _t85_9, 49), 0), _t85_3);
    _t84_30 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_9, _t85_9, 49), _mm256_permute2f128_pd(_t85_9, _t85_9, 49), 15), _t85_3);

    // 4-BLAC: 4x4 - 4x4
    _t85_10 = _mm256_sub_pd(_t85_5, _t84_27);
    _t85_11 = _mm256_sub_pd(_t85_6, _t84_28);
    _t85_12 = _mm256_sub_pd(_t85_7, _t84_29);
    _t85_13 = _mm256_sub_pd(_t85_8, _t84_30);

    // AVX Storer:
    _t85_0 = _t85_10;
    _t85_1 = _t85_11;
    _t85_2 = _t85_12;

    // Generating : M8[28,28] = S(h(1, 28, 2), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 2)) Kro G(h(1, 28, 2), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t85_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_1, _t84_1, 32), _mm256_permute2f128_pd(_t84_1, _t84_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t85_2 = _mm256_mul_pd(_t85_14, _t85_2);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(2, 28, 0), ( G(h(2, 28, 0), M8[28,28],h(4, 28, fi1304)) - ( G(h(2, 28, 0), U0[28,28],h(1, 28, 2)) * G(h(1, 28, 2), M8[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

    // AVX Loader:

    // 2x4 -> 4x4
    _t85_15 = _t85_0;
    _t85_16 = _t85_1;
    _t85_17 = _mm256_setzero_pd();
    _t85_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t85_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t48_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t48_4, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t84_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_19, _t85_19, 32), _mm256_permute2f128_pd(_t85_19, _t85_19, 32), 0), _t85_2);
    _t84_42 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_19, _t85_19, 32), _mm256_permute2f128_pd(_t85_19, _t85_19, 32), 15), _t85_2);
    _t84_43 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_19, _t85_19, 49), _mm256_permute2f128_pd(_t85_19, _t85_19, 49), 0), _t85_2);
    _t84_44 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t85_19, _t85_19, 49), _mm256_permute2f128_pd(_t85_19, _t85_19, 49), 15), _t85_2);

    // 4-BLAC: 4x4 - 4x4
    _t85_20 = _mm256_sub_pd(_t85_15, _t84_41);
    _t85_21 = _mm256_sub_pd(_t85_16, _t84_42);
    _t85_22 = _mm256_sub_pd(_t85_17, _t84_43);
    _t85_23 = _mm256_sub_pd(_t85_18, _t84_44);

    // AVX Storer:
    _t85_0 = _t85_20;
    _t85_1 = _t85_21;

    // Generating : M8[28,28] = S(h(1, 28, 1), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 1)) Kro G(h(1, 28, 1), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t85_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_2, _t84_2, 32), _mm256_permute2f128_pd(_t84_2, _t84_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t85_1 = _mm256_mul_pd(_t85_24, _t85_1);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(1, 28, 0), ( G(h(1, 28, 0), M8[28,28],h(4, 28, fi1304)) - ( G(h(1, 28, 0), U0[28,28],h(1, 28, 1)) Kro G(h(1, 28, 1), M8[28,28],h(4, 28, fi1304)) ) ),h(4, 28, fi1304))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t85_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_2, _t48_2, 32), _mm256_permute2f128_pd(_t48_2, _t48_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t84_8 = _mm256_mul_pd(_t85_25, _t85_1);

    // 4-BLAC: 1x4 - 1x4
    _t85_0 = _mm256_sub_pd(_t85_0, _t84_8);

    // AVX Storer:

    // Generating : M8[28,28] = S(h(1, 28, 0), ( G(h(1, 1, 0), T2407[1,28],h(1, 28, 0)) Kro G(h(1, 28, 0), M8[28,28],h(4, 28, fi1304)) ),h(4, 28, fi1304))

    // AVX Loader:

    // 1x1 -> 1x4
    _t85_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t84_3, _t84_3, 32), _mm256_permute2f128_pd(_t84_3, _t84_3, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t85_0 = _mm256_mul_pd(_t85_26, _t85_0);

    // AVX Storer:
    _asm256_storeu_pd(M1 + fi1304 + 84, _t85_3);
    _asm256_storeu_pd(M1 + fi1304 + 56, _t85_2);
    _asm256_storeu_pd(M1 + fi1304 + 28, _t85_1);
    _asm256_storeu_pd(M1 + fi1304, _t85_0);
  }


  // Generating : x[28,1] = ( Sum_{k2} ( S(h(4, 28, k2), ( G(h(4, 28, k2), y[28,1],h(1, 1, 0)) + ( G(h(4, 28, k2), M2[28,28],h(4, 28, 0)) * G(h(4, 28, 0), v0[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) ) + Sum_{k3} ( Sum_{k2} ( $(h(4, 28, k2), ( G(h(4, 28, k2), M2[28,28],h(4, 28, k3)) * G(h(4, 28, k3), v0[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t75_1);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t75_3);

  for( int k2 = 0; k2 <= 27; k2+=4 ) {
    _t86_4 = _asm256_loadu_pd(y + k2);
    _t86_3 = _asm256_loadu_pd(M2 + 28*k2);
    _t86_2 = _asm256_loadu_pd(M2 + 28*k2 + 28);
    _t86_1 = _asm256_loadu_pd(M2 + 28*k2 + 56);
    _t86_0 = _asm256_loadu_pd(M2 + 28*k2 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t86_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t86_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32)), _mm256_mul_pd(_t86_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t86_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32)), _mm256_mul_pd(_t86_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t86_3, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32)), _mm256_mul_pd(_t86_2, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t86_1, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32)), _mm256_mul_pd(_t86_0, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t75_5, _t75_4), _mm256_unpacklo_pd(_t75_2, _t75_0), 32))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t86_5 = _mm256_add_pd(_t86_4, _t86_6);

    // AVX Storer:
    _asm256_storeu_pd(x + k2, _t86_5);
  }


  for( int k3 = 4; k3 <= 27; k3+=4 ) {

    // AVX Loader:

    for( int k2 = 0; k2 <= 27; k2+=4 ) {
      _t87_4 = _asm256_loadu_pd(M2 + 28*k2 + k3);
      _t87_3 = _asm256_loadu_pd(M2 + 28*k2 + k3 + 28);
      _t87_2 = _asm256_loadu_pd(M2 + 28*k2 + k3 + 56);
      _t87_1 = _asm256_loadu_pd(M2 + 28*k2 + k3 + 84);
      _t87_0 = _asm256_loadu_pd(v0 + k3);
      _t87_5 = _asm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t87_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t87_4, _t87_0), _mm256_mul_pd(_t87_3, _t87_0)), _mm256_hadd_pd(_mm256_mul_pd(_t87_2, _t87_0), _mm256_mul_pd(_t87_1, _t87_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t87_4, _t87_0), _mm256_mul_pd(_t87_3, _t87_0)), _mm256_hadd_pd(_mm256_mul_pd(_t87_2, _t87_0), _mm256_mul_pd(_t87_1, _t87_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t87_5 = _mm256_add_pd(_t87_5, _t87_6);

      // AVX Storer:
      _asm256_storeu_pd(x + k2, _t87_5);
    }
  }


  // Generating : P[28,28] = ( ( Sum_{k2} ( ( S(h(4, 28, k2), ( G(h(4, 28, k2), Y[28,28],h(4, 28, k2)) - ( G(h(4, 28, k2), M2[28,28],h(4, 28, 0)) * G(h(4, 28, 0), M1[28,28],h(4, 28, k2)) ) ),h(4, 28, k2)) + Sum_{i0} ( S(h(4, 28, k2), ( G(h(4, 28, k2), Y[28,28],h(4, 28, i0)) - ( G(h(4, 28, k2), M2[28,28],h(4, 28, 0)) * G(h(4, 28, 0), M1[28,28],h(4, 28, i0)) ) ),h(4, 28, i0)) ) ) ) + S(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) - ( G(h(4, 28, 24), M2[28,28],h(4, 28, 0)) * G(h(4, 28, 0), M1[28,28],h(4, 28, 24)) ) ),h(4, 28, 24)) ) + Sum_{k3} ( ( Sum_{k2} ( ( -$(h(4, 28, k2), ( G(h(4, 28, k2), M2[28,28],h(4, 28, k3)) * G(h(4, 28, k3), M1[28,28],h(4, 28, k2)) ),h(4, 28, k2)) + Sum_{i0} ( -$(h(4, 28, k2), ( G(h(4, 28, k2), M2[28,28],h(4, 28, k3)) * G(h(4, 28, k3), M1[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) ) + -$(h(4, 28, 24), ( G(h(4, 28, 24), M2[28,28],h(4, 28, k3)) * G(h(4, 28, k3), M1[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) )

  _asm256_storeu_pd(M1 + 84, _t84_7);
  _asm256_storeu_pd(M1 + 56, _t84_6);
  _asm256_storeu_pd(M1 + 28, _t84_5);
  _asm256_storeu_pd(M1, _t84_4);

  for( int k2 = 0; k2 <= 23; k2+=4 ) {
    _t88_23 = _asm256_loadu_pd(Y + 29*k2);
    _t88_22 = _mm256_maskload_pd(Y + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t88_21 = _mm256_maskload_pd(Y + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t88_20 = _mm256_maskload_pd(Y + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t88_19 = _mm256_broadcast_sd(M2 + 28*k2);
    _t88_18 = _mm256_broadcast_sd(M2 + 28*k2 + 1);
    _t88_17 = _mm256_broadcast_sd(M2 + 28*k2 + 2);
    _t88_16 = _mm256_broadcast_sd(M2 + 28*k2 + 3);
    _t88_15 = _mm256_broadcast_sd(M2 + 28*k2 + 28);
    _t88_14 = _mm256_broadcast_sd(M2 + 28*k2 + 29);
    _t88_13 = _mm256_broadcast_sd(M2 + 28*k2 + 30);
    _t88_12 = _mm256_broadcast_sd(M2 + 28*k2 + 31);
    _t88_11 = _mm256_broadcast_sd(M2 + 28*k2 + 56);
    _t88_10 = _mm256_broadcast_sd(M2 + 28*k2 + 57);
    _t88_9 = _mm256_broadcast_sd(M2 + 28*k2 + 58);
    _t88_8 = _mm256_broadcast_sd(M2 + 28*k2 + 59);
    _t88_7 = _mm256_broadcast_sd(M2 + 28*k2 + 84);
    _t88_6 = _mm256_broadcast_sd(M2 + 28*k2 + 85);
    _t88_5 = _mm256_broadcast_sd(M2 + 28*k2 + 86);
    _t88_4 = _mm256_broadcast_sd(M2 + 28*k2 + 87);
    _t88_3 = _asm256_loadu_pd(M1 + k2);
    _t88_2 = _asm256_loadu_pd(M1 + k2 + 28);
    _t88_1 = _asm256_loadu_pd(M1 + k2 + 56);
    _t88_0 = _asm256_loadu_pd(M1 + k2 + 84);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t88_36 = _t88_23;
    _t88_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t88_23, _t88_22, 3), _t88_22, 12);
    _t88_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t88_23, _t88_22, 0), _t88_21, 49);
    _t88_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t88_23, _t88_22, 12), _mm256_shuffle_pd(_t88_21, _t88_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t88_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_19, _t88_3), _mm256_mul_pd(_t88_18, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_17, _t88_1), _mm256_mul_pd(_t88_16, _t88_0)));
    _t88_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_15, _t88_3), _mm256_mul_pd(_t88_14, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_13, _t88_1), _mm256_mul_pd(_t88_12, _t88_0)));
    _t88_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_11, _t88_3), _mm256_mul_pd(_t88_10, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_9, _t88_1), _mm256_mul_pd(_t88_8, _t88_0)));
    _t88_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_7, _t88_3), _mm256_mul_pd(_t88_6, _t88_2)), _mm256_add_pd(_mm256_mul_pd(_t88_5, _t88_1), _mm256_mul_pd(_t88_4, _t88_0)));

    // 4-BLAC: 4x4 - 4x4
    _t88_32 = _mm256_sub_pd(_t88_36, _t88_28);
    _t88_33 = _mm256_sub_pd(_t88_37, _t88_29);
    _t88_34 = _mm256_sub_pd(_t88_38, _t88_30);
    _t88_35 = _mm256_sub_pd(_t88_39, _t88_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t88_24 = _t88_32;
    _t88_25 = _t88_33;
    _t88_26 = _t88_34;
    _t88_27 = _t88_35;

    // AVX Loader:

    for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
      _t89_7 = _asm256_loadu_pd(Y + i0 + 28*k2);
      _t89_6 = _asm256_loadu_pd(Y + i0 + 28*k2 + 28);
      _t89_5 = _asm256_loadu_pd(Y + i0 + 28*k2 + 56);
      _t89_4 = _asm256_loadu_pd(Y + i0 + 28*k2 + 84);
      _t89_3 = _asm256_loadu_pd(M1 + i0);
      _t89_2 = _asm256_loadu_pd(M1 + i0 + 28);
      _t89_1 = _asm256_loadu_pd(M1 + i0 + 56);
      _t89_0 = _asm256_loadu_pd(M1 + i0 + 84);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t89_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_19, _t89_3), _mm256_mul_pd(_t88_18, _t89_2)), _mm256_add_pd(_mm256_mul_pd(_t88_17, _t89_1), _mm256_mul_pd(_t88_16, _t89_0)));
      _t89_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_15, _t89_3), _mm256_mul_pd(_t88_14, _t89_2)), _mm256_add_pd(_mm256_mul_pd(_t88_13, _t89_1), _mm256_mul_pd(_t88_12, _t89_0)));
      _t89_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_11, _t89_3), _mm256_mul_pd(_t88_10, _t89_2)), _mm256_add_pd(_mm256_mul_pd(_t88_9, _t89_1), _mm256_mul_pd(_t88_8, _t89_0)));
      _t89_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t88_7, _t89_3), _mm256_mul_pd(_t88_6, _t89_2)), _mm256_add_pd(_mm256_mul_pd(_t88_5, _t89_1), _mm256_mul_pd(_t88_4, _t89_0)));

      // 4-BLAC: 4x4 - 4x4
      _t89_12 = _mm256_sub_pd(_t89_7, _t89_8);
      _t89_13 = _mm256_sub_pd(_t89_6, _t89_9);
      _t89_14 = _mm256_sub_pd(_t89_5, _t89_10);
      _t89_15 = _mm256_sub_pd(_t89_4, _t89_11);

      // AVX Storer:
      _asm256_storeu_pd(P + i0 + 28*k2, _t89_12);
      _asm256_storeu_pd(P + i0 + 28*k2 + 28, _t89_13);
      _asm256_storeu_pd(P + i0 + 28*k2 + 56, _t89_14);
      _asm256_storeu_pd(P + i0 + 28*k2 + 84, _t89_15);
    }
    _asm256_storeu_pd(P + 29*k2, _t88_24);
    _mm256_maskstore_pd(P + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t88_25);
    _mm256_maskstore_pd(P + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t88_26);
    _mm256_maskstore_pd(P + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t88_27);
  }

  _t90_19 = _mm256_broadcast_sd(M2 + 672);
  _t90_18 = _mm256_broadcast_sd(M2 + 673);
  _t90_17 = _mm256_broadcast_sd(M2 + 674);
  _t90_16 = _mm256_broadcast_sd(M2 + 675);
  _t90_15 = _mm256_broadcast_sd(M2 + 700);
  _t90_14 = _mm256_broadcast_sd(M2 + 701);
  _t90_13 = _mm256_broadcast_sd(M2 + 702);
  _t90_12 = _mm256_broadcast_sd(M2 + 703);
  _t90_11 = _mm256_broadcast_sd(M2 + 728);
  _t90_10 = _mm256_broadcast_sd(M2 + 729);
  _t90_9 = _mm256_broadcast_sd(M2 + 730);
  _t90_8 = _mm256_broadcast_sd(M2 + 731);
  _t90_7 = _mm256_broadcast_sd(M2 + 756);
  _t90_6 = _mm256_broadcast_sd(M2 + 757);
  _t90_5 = _mm256_broadcast_sd(M2 + 758);
  _t90_4 = _mm256_broadcast_sd(M2 + 759);
  _t90_3 = _asm256_loadu_pd(M1 + 24);
  _t90_2 = _asm256_loadu_pd(M1 + 52);
  _t90_1 = _asm256_loadu_pd(M1 + 80);
  _t90_0 = _asm256_loadu_pd(M1 + 108);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t90_28 = _t15_28;
  _t90_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
  _t90_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
  _t90_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t90_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t90_19, _t90_3), _mm256_mul_pd(_t90_18, _t90_2)), _mm256_add_pd(_mm256_mul_pd(_t90_17, _t90_1), _mm256_mul_pd(_t90_16, _t90_0)));
  _t90_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t90_15, _t90_3), _mm256_mul_pd(_t90_14, _t90_2)), _mm256_add_pd(_mm256_mul_pd(_t90_13, _t90_1), _mm256_mul_pd(_t90_12, _t90_0)));
  _t90_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t90_11, _t90_3), _mm256_mul_pd(_t90_10, _t90_2)), _mm256_add_pd(_mm256_mul_pd(_t90_9, _t90_1), _mm256_mul_pd(_t90_8, _t90_0)));
  _t90_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t90_7, _t90_3), _mm256_mul_pd(_t90_6, _t90_2)), _mm256_add_pd(_mm256_mul_pd(_t90_5, _t90_1), _mm256_mul_pd(_t90_4, _t90_0)));

  // 4-BLAC: 4x4 - 4x4
  _t90_24 = _mm256_sub_pd(_t90_28, _t90_20);
  _t90_25 = _mm256_sub_pd(_t90_29, _t90_21);
  _t90_26 = _mm256_sub_pd(_t90_30, _t90_22);
  _t90_27 = _mm256_sub_pd(_t90_31, _t90_23);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t10_3 = _t90_24;
  _t10_2 = _t90_25;
  _t10_1 = _t90_26;
  _t10_0 = _t90_27;


  for( int k3 = 4; k3 <= 27; k3+=4 ) {

    for( int k2 = 0; k2 <= 23; k2+=4 ) {
      _t91_19 = _mm256_broadcast_sd(M2 + 28*k2 + k3);
      _t91_18 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 1);
      _t91_17 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 2);
      _t91_16 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 3);
      _t91_15 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 28);
      _t91_14 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 29);
      _t91_13 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 30);
      _t91_12 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 31);
      _t91_11 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 56);
      _t91_10 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 57);
      _t91_9 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 58);
      _t91_8 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 59);
      _t91_7 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 84);
      _t91_6 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 85);
      _t91_5 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 86);
      _t91_4 = _mm256_broadcast_sd(M2 + 28*k2 + k3 + 87);
      _t91_3 = _asm256_loadu_pd(M1 + k2 + 28*k3);
      _t91_2 = _asm256_loadu_pd(M1 + k2 + 28*k3 + 28);
      _t91_1 = _asm256_loadu_pd(M1 + k2 + 28*k3 + 56);
      _t91_0 = _asm256_loadu_pd(M1 + k2 + 28*k3 + 84);
      _t91_20 = _asm256_loadu_pd(P + 29*k2);
      _t91_21 = _mm256_maskload_pd(P + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t91_22 = _mm256_maskload_pd(P + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t91_23 = _mm256_maskload_pd(P + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t91_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_19, _t91_3), _mm256_mul_pd(_t91_18, _t91_2)), _mm256_add_pd(_mm256_mul_pd(_t91_17, _t91_1), _mm256_mul_pd(_t91_16, _t91_0)));
      _t91_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_15, _t91_3), _mm256_mul_pd(_t91_14, _t91_2)), _mm256_add_pd(_mm256_mul_pd(_t91_13, _t91_1), _mm256_mul_pd(_t91_12, _t91_0)));
      _t91_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_11, _t91_3), _mm256_mul_pd(_t91_10, _t91_2)), _mm256_add_pd(_mm256_mul_pd(_t91_9, _t91_1), _mm256_mul_pd(_t91_8, _t91_0)));
      _t91_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_7, _t91_3), _mm256_mul_pd(_t91_6, _t91_2)), _mm256_add_pd(_mm256_mul_pd(_t91_5, _t91_1), _mm256_mul_pd(_t91_4, _t91_0)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t91_28 = _t91_20;
      _t91_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t91_20, _t91_21, 3), _t91_21, 12);
      _t91_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t91_20, _t91_21, 0), _t91_22, 49);
      _t91_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t91_20, _t91_21, 12), _mm256_shuffle_pd(_t91_22, _t91_23, 12), 49);

      // 4-BLAC: 4x4 - 4x4
      _t91_28 = _mm256_sub_pd(_t91_28, _t91_24);
      _t91_29 = _mm256_sub_pd(_t91_29, _t91_25);
      _t91_30 = _mm256_sub_pd(_t91_30, _t91_26);
      _t91_31 = _mm256_sub_pd(_t91_31, _t91_27);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t91_20 = _t91_28;
      _t91_21 = _t91_29;
      _t91_22 = _t91_30;
      _t91_23 = _t91_31;

      // AVX Loader:

      for( int i0 = 4*floord(k2 - 1, 4) + 8; i0 <= 27; i0+=4 ) {
        _t92_3 = _asm256_loadu_pd(M1 + i0 + 28*k3);
        _t92_2 = _asm256_loadu_pd(M1 + i0 + 28*k3 + 28);
        _t92_1 = _asm256_loadu_pd(M1 + i0 + 28*k3 + 56);
        _t92_0 = _asm256_loadu_pd(M1 + i0 + 28*k3 + 84);
        _t92_4 = _asm256_loadu_pd(P + i0 + 28*k2);
        _t92_5 = _asm256_loadu_pd(P + i0 + 28*k2 + 28);
        _t92_6 = _asm256_loadu_pd(P + i0 + 28*k2 + 56);
        _t92_7 = _asm256_loadu_pd(P + i0 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t92_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_19, _t92_3), _mm256_mul_pd(_t91_18, _t92_2)), _mm256_add_pd(_mm256_mul_pd(_t91_17, _t92_1), _mm256_mul_pd(_t91_16, _t92_0)));
        _t92_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_15, _t92_3), _mm256_mul_pd(_t91_14, _t92_2)), _mm256_add_pd(_mm256_mul_pd(_t91_13, _t92_1), _mm256_mul_pd(_t91_12, _t92_0)));
        _t92_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_11, _t92_3), _mm256_mul_pd(_t91_10, _t92_2)), _mm256_add_pd(_mm256_mul_pd(_t91_9, _t92_1), _mm256_mul_pd(_t91_8, _t92_0)));
        _t92_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t91_7, _t92_3), _mm256_mul_pd(_t91_6, _t92_2)), _mm256_add_pd(_mm256_mul_pd(_t91_5, _t92_1), _mm256_mul_pd(_t91_4, _t92_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 - 4x4
        _t92_4 = _mm256_sub_pd(_t92_4, _t92_8);
        _t92_5 = _mm256_sub_pd(_t92_5, _t92_9);
        _t92_6 = _mm256_sub_pd(_t92_6, _t92_10);
        _t92_7 = _mm256_sub_pd(_t92_7, _t92_11);

        // AVX Storer:
        _asm256_storeu_pd(P + i0 + 28*k2, _t92_4);
        _asm256_storeu_pd(P + i0 + 28*k2 + 28, _t92_5);
        _asm256_storeu_pd(P + i0 + 28*k2 + 56, _t92_6);
        _asm256_storeu_pd(P + i0 + 28*k2 + 84, _t92_7);
      }
      _asm256_storeu_pd(P + 29*k2, _t91_20);
      _mm256_maskstore_pd(P + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t91_21);
      _mm256_maskstore_pd(P + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t91_22);
      _mm256_maskstore_pd(P + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t91_23);
    }
    _t93_19 = _mm256_broadcast_sd(M2 + k3 + 672);
    _t93_18 = _mm256_broadcast_sd(M2 + k3 + 673);
    _t93_17 = _mm256_broadcast_sd(M2 + k3 + 674);
    _t93_16 = _mm256_broadcast_sd(M2 + k3 + 675);
    _t93_15 = _mm256_broadcast_sd(M2 + k3 + 700);
    _t93_14 = _mm256_broadcast_sd(M2 + k3 + 701);
    _t93_13 = _mm256_broadcast_sd(M2 + k3 + 702);
    _t93_12 = _mm256_broadcast_sd(M2 + k3 + 703);
    _t93_11 = _mm256_broadcast_sd(M2 + k3 + 728);
    _t93_10 = _mm256_broadcast_sd(M2 + k3 + 729);
    _t93_9 = _mm256_broadcast_sd(M2 + k3 + 730);
    _t93_8 = _mm256_broadcast_sd(M2 + k3 + 731);
    _t93_7 = _mm256_broadcast_sd(M2 + k3 + 756);
    _t93_6 = _mm256_broadcast_sd(M2 + k3 + 757);
    _t93_5 = _mm256_broadcast_sd(M2 + k3 + 758);
    _t93_4 = _mm256_broadcast_sd(M2 + k3 + 759);
    _t93_3 = _asm256_loadu_pd(M1 + 28*k3 + 24);
    _t93_2 = _asm256_loadu_pd(M1 + 28*k3 + 52);
    _t93_1 = _asm256_loadu_pd(M1 + 28*k3 + 80);
    _t93_0 = _asm256_loadu_pd(M1 + 28*k3 + 108);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t93_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t93_19, _t93_3), _mm256_mul_pd(_t93_18, _t93_2)), _mm256_add_pd(_mm256_mul_pd(_t93_17, _t93_1), _mm256_mul_pd(_t93_16, _t93_0)));
    _t93_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t93_15, _t93_3), _mm256_mul_pd(_t93_14, _t93_2)), _mm256_add_pd(_mm256_mul_pd(_t93_13, _t93_1), _mm256_mul_pd(_t93_12, _t93_0)));
    _t93_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t93_11, _t93_3), _mm256_mul_pd(_t93_10, _t93_2)), _mm256_add_pd(_mm256_mul_pd(_t93_9, _t93_1), _mm256_mul_pd(_t93_8, _t93_0)));
    _t93_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t93_7, _t93_3), _mm256_mul_pd(_t93_6, _t93_2)), _mm256_add_pd(_mm256_mul_pd(_t93_5, _t93_1), _mm256_mul_pd(_t93_4, _t93_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t93_24 = _t10_3;
    _t93_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
    _t93_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
    _t93_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t93_24 = _mm256_sub_pd(_t93_24, _t93_20);
    _t93_25 = _mm256_sub_pd(_t93_25, _t93_21);
    _t93_26 = _mm256_sub_pd(_t93_26, _t93_22);
    _t93_27 = _mm256_sub_pd(_t93_27, _t93_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t10_3 = _t93_24;
    _t10_2 = _t93_25;
    _t10_1 = _t93_26;
    _t10_0 = _t93_27;
    _asm256_storeu_pd(P + 696, _t10_3);
    _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
    _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
    _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);
  }

  _asm256_storeu_pd(Y + 696, _t15_28);
  _mm256_maskstore_pd(Y + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
  _mm256_maskstore_pd(Y + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
  _mm256_maskstore_pd(Y + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  _mm_store_sd(&(v0[3]), _mm256_castpd256_pd128(_t75_0));
  _mm_store_sd(&(v0[2]), _mm256_castpd256_pd128(_t75_2));
  _mm_store_sd(&(v0[1]), _mm256_castpd256_pd128(_t75_4));
  _mm_store_sd(&(v0[0]), _mm256_castpd256_pd128(_t75_5));
  _asm256_storeu_pd(P + 696, _t10_3);
  _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t10_2);
  _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t10_1);
  _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t10_0);

}
