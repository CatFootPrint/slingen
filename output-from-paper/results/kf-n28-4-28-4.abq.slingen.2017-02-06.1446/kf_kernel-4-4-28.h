/*
 * kf_kernel.h
 *
Decl { {u'B': Matrix[B, (28, 4), GenMatAccess], u'F': SquaredMatrix[F, (28, 28), GenMatAccess], u'H': Matrix[H, (4, 28), GenMatAccess], u'Q': Symmetric[Q, (28, 28), USMatAccess], u'P': Symmetric[P, (28, 28), USMatAccess], u'R': Symmetric[R, (4, 4), USMatAccess], u'M1': Matrix[M1, (4, 28), GenMatAccess], u'M0': SquaredMatrix[M0, (28, 28), GenMatAccess], u'M3': Symmetric[M3, (4, 4), USMatAccess], u'M2': Matrix[M2, (28, 4), GenMatAccess], u'Y': Symmetric[Y, (28, 28), USMatAccess], u'v0': Matrix[v0, (4, 1), GenMatAccess], 'T250': Matrix[T250, (1, 4), GenMatAccess], u'u': Matrix[u, (4, 1), GenMatAccess], u'y': Matrix[y, (28, 1), GenMatAccess], u'x': Matrix[x, (28, 1), GenMatAccess], u'z': Matrix[z, (4, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'ldiv_ut_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'chol_u_ow_opt': {'m': 'm3.ll'}, 'ldiv_un_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'chol_u_ow_opt_m3_ldiv_un_ow_opt_m4_n1_ldiv_ut_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[28,4] ) ) * Tile( (1, 1), Tile( (4, 4), u[4,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[28,28] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[4,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[4,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[4,28] ) ) * Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[4,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[4,28] ) ) * Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[28,4] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[4,28] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[4,4] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[4,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[4,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[4,4] ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 1), M3[4,4],h(3, 4, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 1), M3[4,4],h(3, 4, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 2), M3[4,4],h(2, 4, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 2), M3[4,4],h(2, 4, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) = ( Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) Div Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 1), v0[4,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 1), v0[4,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 2), v0[4,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 2), v0[4,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 0), v0[4,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 0), v0[4,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 0), M3[4,4],h(1, 4, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 0), v0[4,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 0), v0[4,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 0), M3[4,4],h(1, 4, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(1, 4, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ) )
Eq.ann: {}
Entry 35:
For_{fi136;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 1), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 1), M1[4,28],h(4, 28, fi136)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 2), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 2), M1[4,28],h(4, 28, fi136)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ) ) )
Eq.ann: {}
 )Entry 36:
For_{fi183;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi183)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 4, 0), M3[4,4],h(1, 4, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 3), M1[4,28],h(4, 28, fi183)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M1[4,28],h(4, 28, fi183)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 4, 0), M3[4,4],h(1, 4, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 2), M1[4,28],h(4, 28, fi183)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M1[4,28],h(4, 28, fi183)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M3[4,4],h(1, 4, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 1), M1[4,28],h(4, 28, fi183)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 4, 0), M1[4,28],h(4, 28, fi183)) ) ) )
Eq.ann: {}
 )Entry 37:
Eq: Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[28,4] ) ) * Tile( (1, 1), Tile( (4, 4), v0[4,1] ) ) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[28,4] ) ) * Tile( (1, 1), Tile( (4, 4), M1[4,28] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-02-05
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


#define PARAM0 4
#define PARAM1 4
#define PARAM2 28

#define ERRTHRESH 1e-5

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5, _t2_6, _t2_7,
	_t2_8, _t2_9, _t2_10, _t2_11, _t2_12, _t2_13, _t2_14, _t2_15,
	_t2_16, _t2_17, _t2_18, _t2_19, _t2_20, _t2_21, _t2_22, _t2_23;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19, _t4_20, _t4_21, _t4_22, _t4_23,
	_t4_24, _t4_25, _t4_26, _t4_27;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7,
	_t8_8, _t8_9, _t8_10, _t8_11, _t8_12, _t8_13, _t8_14, _t8_15,
	_t8_16, _t8_17, _t8_18, _t8_19, _t8_20, _t8_21, _t8_22, _t8_23;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23,
	_t9_24, _t9_25, _t9_26, _t9_27;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7,
	_t12_8, _t12_9, _t12_10, _t12_11, _t12_12, _t12_13, _t12_14, _t12_15,
	_t12_16, _t12_17, _t12_18, _t12_19, _t12_20, _t12_21, _t12_22, _t12_23;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19, _t14_20, _t14_21, _t14_22, _t14_23,
	_t14_24, _t14_25, _t14_26, _t14_27, _t14_28, _t14_29, _t14_30, _t14_31,
	_t14_32, _t14_33, _t14_34, _t14_35, _t14_36, _t14_37, _t14_38, _t14_39,
	_t14_40, _t14_41, _t14_42, _t14_43;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19, _t17_20, _t17_21, _t17_22, _t17_23,
	_t17_24, _t17_25, _t17_26, _t17_27;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27, _t18_28, _t18_29, _t18_30, _t18_31,
	_t18_32, _t18_33, _t18_34, _t18_35, _t18_36, _t18_37, _t18_38, _t18_39,
	_t18_40, _t18_41, _t18_42, _t18_43;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5, _t22_6, _t22_7,
	_t22_8, _t22_9, _t22_10, _t22_11, _t22_12, _t22_13, _t22_14, _t22_15,
	_t22_16, _t22_17, _t22_18, _t22_19, _t22_20, _t22_21, _t22_22, _t22_23,
	_t22_24, _t22_25, _t22_26, _t22_27;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19, _t23_20, _t23_21, _t23_22, _t23_23,
	_t23_24, _t23_25, _t23_26, _t23_27;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19, _t24_20, _t24_21, _t24_22, _t24_23,
	_t24_24, _t24_25, _t24_26, _t24_27, _t24_28, _t24_29, _t24_30, _t24_31,
	_t24_32, _t24_33, _t24_34, _t24_35;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11, _t27_12, _t27_13, _t27_14, _t27_15,
	_t27_16, _t27_17, _t27_18, _t27_19, _t27_20, _t27_21, _t27_22, _t27_23;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7,
	_t28_8, _t28_9, _t28_10, _t28_11, _t28_12, _t28_13, _t28_14, _t28_15,
	_t28_16, _t28_17, _t28_18, _t28_19, _t28_20, _t28_21, _t28_22, _t28_23,
	_t28_24, _t28_25, _t28_26, _t28_27;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23,
	_t29_24, _t29_25, _t29_26, _t29_27;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7,
	_t32_8, _t32_9, _t32_10, _t32_11, _t32_12, _t32_13, _t32_14, _t32_15,
	_t32_16, _t32_17, _t32_18, _t32_19, _t32_20, _t32_21, _t32_22, _t32_23,
	_t32_24, _t32_25, _t32_26, _t32_27, _t32_28, _t32_29, _t32_30, _t32_31,
	_t32_32, _t32_33, _t32_34, _t32_35;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23,
	_t33_24, _t33_25, _t33_26, _t33_27;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11, _t34_12, _t34_13, _t34_14, _t34_15,
	_t34_16, _t34_17, _t34_18, _t34_19, _t34_20, _t34_21, _t34_22, _t34_23,
	_t34_24, _t34_25, _t34_26, _t34_27;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3, _t35_4, _t35_5, _t35_6, _t35_7,
	_t35_8, _t35_9, _t35_10, _t35_11, _t35_12, _t35_13, _t35_14, _t35_15,
	_t35_16, _t35_17, _t35_18, _t35_19, _t35_20, _t35_21, _t35_22, _t35_23,
	_t35_24, _t35_25, _t35_26, _t35_27;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11, _t38_12, _t38_13, _t38_14, _t38_15,
	_t38_16, _t38_17, _t38_18, _t38_19;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3, _t40_4, _t40_5, _t40_6, _t40_7,
	_t40_8, _t40_9, _t40_10, _t40_11, _t40_12, _t40_13, _t40_14, _t40_15,
	_t40_16, _t40_17, _t40_18, _t40_19, _t40_20, _t40_21, _t40_22, _t40_23,
	_t40_24, _t40_25, _t40_26, _t40_27;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19, _t43_20, _t43_21, _t43_22, _t43_23,
	_t43_24, _t43_25, _t43_26, _t43_27, _t43_28, _t43_29, _t43_30, _t43_31,
	_t43_32, _t43_33, _t43_34, _t43_35, _t43_36, _t43_37, _t43_38, _t43_39;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27, _t45_28, _t45_29, _t45_30, _t45_31,
	_t45_32, _t45_33, _t45_34, _t45_35, _t45_36, _t45_37, _t45_38, _t45_39,
	_t45_40, _t45_41, _t45_42, _t45_43, _t45_44, _t45_45, _t45_46, _t45_47,
	_t45_48, _t45_49, _t45_50, _t45_51, _t45_52, _t45_53, _t45_54, _t45_55,
	_t45_56, _t45_57, _t45_58, _t45_59, _t45_60, _t45_61, _t45_62, _t45_63,
	_t45_64, _t45_65, _t45_66, _t45_67, _t45_68, _t45_69, _t45_70, _t45_71,
	_t45_72, _t45_73, _t45_74, _t45_75, _t45_76, _t45_77, _t45_78, _t45_79,
	_t45_80, _t45_81, _t45_82, _t45_83, _t45_84, _t45_85, _t45_86, _t45_87,
	_t45_88, _t45_89, _t45_90, _t45_91, _t45_92, _t45_93, _t45_94, _t45_95,
	_t45_96, _t45_97, _t45_98, _t45_99, _t45_100, _t45_101, _t45_102, _t45_103,
	_t45_104, _t45_105, _t45_106, _t45_107, _t45_108, _t45_109, _t45_110, _t45_111,
	_t45_112, _t45_113, _t45_114, _t45_115, _t45_116, _t45_117, _t45_118, _t45_119,
	_t45_120, _t45_121, _t45_122, _t45_123, _t45_124, _t45_125, _t45_126, _t45_127,
	_t45_128, _t45_129, _t45_130, _t45_131, _t45_132, _t45_133, _t45_134, _t45_135,
	_t45_136, _t45_137, _t45_138, _t45_139, _t45_140, _t45_141, _t45_142, _t45_143,
	_t45_144, _t45_145, _t45_146, _t45_147, _t45_148, _t45_149, _t45_150, _t45_151,
	_t45_152, _t45_153, _t45_154, _t45_155, _t45_156, _t45_157, _t45_158, _t45_159,
	_t45_160, _t45_161, _t45_162, _t45_163, _t45_164, _t45_165, _t45_166, _t45_167,
	_t45_168, _t45_169, _t45_170, _t45_171, _t45_172, _t45_173, _t45_174, _t45_175,
	_t45_176, _t45_177, _t45_178, _t45_179, _t45_180;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11, _t46_12, _t46_13, _t46_14, _t46_15,
	_t46_16, _t46_17, _t46_18, _t46_19, _t46_20, _t46_21, _t46_22, _t46_23,
	_t46_24, _t46_25, _t46_26;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27, _t47_28, _t47_29, _t47_30, _t47_31;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23,
	_t50_24, _t50_25, _t50_26, _t50_27, _t50_28, _t50_29, _t50_30, _t50_31,
	_t50_32, _t50_33, _t50_34, _t50_35, _t50_36, _t50_37, _t50_38, _t50_39;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11;


  // Generating : y[28,1] = Sum_{i0} ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), x[28,1],h(1, 1, 0)) ) + ( G(h(4, 28, i0), B[28,4],h(4, 4, 0)) * G(h(4, 4, 0), u[4,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), x[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t0_9 = _mm256_loadu_pd(F + 28*i0);
    _t0_8 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t0_7 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t0_6 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t0_5 = _mm256_loadu_pd(x);
    _t0_4 = _mm256_loadu_pd(B + 4*i0);
    _t0_3 = _mm256_loadu_pd(B + 4*i0 + 4);
    _t0_2 = _mm256_loadu_pd(B + 4*i0 + 8);
    _t0_1 = _mm256_loadu_pd(B + 4*i0 + 12);
    _t0_0 = _mm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t1_4 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t1_3 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t1_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t1_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);
      _t1_0 = _mm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t0_10 = _mm256_add_pd(_t0_10, _t1_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(y + i0, _t0_10);
  }

  _t2_11 = _mm256_loadu_pd(P);
  _t2_10 = _mm256_maskload_pd(P + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t2_9 = _mm256_maskload_pd(P + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t2_8 = _mm256_maskload_pd(P + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t2_7 = _mm256_loadu_pd(P + 116);
  _t2_6 = _mm256_maskload_pd(P + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t2_5 = _mm256_maskload_pd(P + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t2_4 = _mm256_maskload_pd(P + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t2_3 = _mm256_loadu_pd(P + 696);
  _t2_2 = _mm256_maskload_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t2_1 = _mm256_maskload_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t2_0 = _mm256_maskload_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[28,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, i4)) * T( G(h(4, 28, 0), P[28,28],h(4, 28, i4)) ) ),h(4, 28, 0)) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 4)) * G(h(4, 28, 4), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, i4)) * T( G(h(4, 28, 4), P[28,28],h(4, 28, i4)) ) ),h(4, 28, 4)) ) ) + Sum_{k2} ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, k2)) ),h(4, 28, k2)) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, i4)) * G(h(4, 28, i4), P[28,28],h(4, 28, k2)) ),h(4, 28, k2)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, k2)) ),h(4, 28, k2)) ) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, i4)) * T( G(h(4, 28, k2), P[28,28],h(4, 28, i4)) ) ),h(4, 28, k2)) ) ) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, i4)) * G(h(4, 28, i4), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 24)) * G(h(4, 28, 24), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t2_12 = _t2_11;
  _t2_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t2_11, _t2_10, 3), _t2_10, 12);
  _t2_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_11, _t2_10, 0), _t2_9, 49);
  _t2_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_11, _t2_10, 12), _mm256_shuffle_pd(_t2_9, _t2_8, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t2_16 = _t2_7;
  _t2_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t2_7, _t2_6, 3), _t2_6, 12);
  _t2_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_7, _t2_6, 0), _t2_5, 49);
  _t2_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_7, _t2_6, 12), _mm256_shuffle_pd(_t2_5, _t2_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t2_20 = _t2_3;
  _t2_21 = _mm256_blend_pd(_mm256_shuffle_pd(_t2_3, _t2_2, 3), _t2_2, 12);
  _t2_22 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_3, _t2_2, 0), _t2_1, 49);
  _t2_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t2_3, _t2_2, 12), _mm256_shuffle_pd(_t2_1, _t2_0, 12), 49);


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t3_15 = _mm256_broadcast_sd(F + 28*i0);
    _t3_14 = _mm256_broadcast_sd(F + 28*i0 + 1);
    _t3_13 = _mm256_broadcast_sd(F + 28*i0 + 2);
    _t3_12 = _mm256_broadcast_sd(F + 28*i0 + 3);
    _t3_11 = _mm256_broadcast_sd(F + 28*i0 + 28);
    _t3_10 = _mm256_broadcast_sd(F + 28*i0 + 29);
    _t3_9 = _mm256_broadcast_sd(F + 28*i0 + 30);
    _t3_8 = _mm256_broadcast_sd(F + 28*i0 + 31);
    _t3_7 = _mm256_broadcast_sd(F + 28*i0 + 56);
    _t3_6 = _mm256_broadcast_sd(F + 28*i0 + 57);
    _t3_5 = _mm256_broadcast_sd(F + 28*i0 + 58);
    _t3_4 = _mm256_broadcast_sd(F + 28*i0 + 59);
    _t3_3 = _mm256_broadcast_sd(F + 28*i0 + 84);
    _t3_2 = _mm256_broadcast_sd(F + 28*i0 + 85);
    _t3_1 = _mm256_broadcast_sd(F + 28*i0 + 86);
    _t3_0 = _mm256_broadcast_sd(F + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t3_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t2_12), _mm256_mul_pd(_t3_14, _t2_13)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t2_14), _mm256_mul_pd(_t3_12, _t2_15)));
    _t3_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t2_12), _mm256_mul_pd(_t3_10, _t2_13)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t2_14), _mm256_mul_pd(_t3_8, _t2_15)));
    _t3_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t2_12), _mm256_mul_pd(_t3_6, _t2_13)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t2_14), _mm256_mul_pd(_t3_4, _t2_15)));
    _t3_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _t2_12), _mm256_mul_pd(_t3_2, _t2_13)), _mm256_add_pd(_mm256_mul_pd(_t3_1, _t2_14), _mm256_mul_pd(_t3_0, _t2_15)));

    // AVX Storer:

    for( int i4 = 4; i4 <= 27; i4+=4 ) {
      _t4_19 = _mm256_broadcast_sd(F + 28*i0 + i4);
      _t4_18 = _mm256_broadcast_sd(F + 28*i0 + i4 + 1);
      _t4_17 = _mm256_broadcast_sd(F + 28*i0 + i4 + 2);
      _t4_16 = _mm256_broadcast_sd(F + 28*i0 + i4 + 3);
      _t4_15 = _mm256_broadcast_sd(F + 28*i0 + i4 + 28);
      _t4_14 = _mm256_broadcast_sd(F + 28*i0 + i4 + 29);
      _t4_13 = _mm256_broadcast_sd(F + 28*i0 + i4 + 30);
      _t4_12 = _mm256_broadcast_sd(F + 28*i0 + i4 + 31);
      _t4_11 = _mm256_broadcast_sd(F + 28*i0 + i4 + 56);
      _t4_10 = _mm256_broadcast_sd(F + 28*i0 + i4 + 57);
      _t4_9 = _mm256_broadcast_sd(F + 28*i0 + i4 + 58);
      _t4_8 = _mm256_broadcast_sd(F + 28*i0 + i4 + 59);
      _t4_7 = _mm256_broadcast_sd(F + 28*i0 + i4 + 84);
      _t4_6 = _mm256_broadcast_sd(F + 28*i0 + i4 + 85);
      _t4_5 = _mm256_broadcast_sd(F + 28*i0 + i4 + 86);
      _t4_4 = _mm256_broadcast_sd(F + 28*i0 + i4 + 87);
      _t4_3 = _mm256_loadu_pd(P + i4);
      _t4_2 = _mm256_loadu_pd(P + i4 + 28);
      _t4_1 = _mm256_loadu_pd(P + i4 + 56);
      _t4_0 = _mm256_loadu_pd(P + i4 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t4_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 32);
      _t4_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 32);
      _t4_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t4_3, _t4_2), _mm256_unpacklo_pd(_t4_1, _t4_0), 49);
      _t4_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t4_3, _t4_2), _mm256_unpackhi_pd(_t4_1, _t4_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t4_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_19, _t4_24), _mm256_mul_pd(_t4_18, _t4_25)), _mm256_add_pd(_mm256_mul_pd(_t4_17, _t4_26), _mm256_mul_pd(_t4_16, _t4_27)));
      _t4_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t4_24), _mm256_mul_pd(_t4_14, _t4_25)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t4_26), _mm256_mul_pd(_t4_12, _t4_27)));
      _t4_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t4_24), _mm256_mul_pd(_t4_10, _t4_25)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t4_26), _mm256_mul_pd(_t4_8, _t4_27)));
      _t4_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t4_24), _mm256_mul_pd(_t4_6, _t4_25)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t4_26), _mm256_mul_pd(_t4_4, _t4_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t3_16 = _mm256_add_pd(_t3_16, _t4_20);
      _t3_17 = _mm256_add_pd(_t3_17, _t4_21);
      _t3_18 = _mm256_add_pd(_t3_18, _t4_22);
      _t3_19 = _mm256_add_pd(_t3_19, _t4_23);

      // AVX Storer:
    }
    _t5_19 = _mm256_loadu_pd(P + 4);
    _t5_18 = _mm256_loadu_pd(P + 32);
    _t5_17 = _mm256_loadu_pd(P + 60);
    _t5_16 = _mm256_loadu_pd(P + 88);
    _t5_15 = _mm256_broadcast_sd(F + 28*i0 + 4);
    _t5_14 = _mm256_broadcast_sd(F + 28*i0 + 5);
    _t5_13 = _mm256_broadcast_sd(F + 28*i0 + 6);
    _t5_12 = _mm256_broadcast_sd(F + 28*i0 + 7);
    _t5_11 = _mm256_broadcast_sd(F + 28*i0 + 32);
    _t5_10 = _mm256_broadcast_sd(F + 28*i0 + 33);
    _t5_9 = _mm256_broadcast_sd(F + 28*i0 + 34);
    _t5_8 = _mm256_broadcast_sd(F + 28*i0 + 35);
    _t5_7 = _mm256_broadcast_sd(F + 28*i0 + 60);
    _t5_6 = _mm256_broadcast_sd(F + 28*i0 + 61);
    _t5_5 = _mm256_broadcast_sd(F + 28*i0 + 62);
    _t5_4 = _mm256_broadcast_sd(F + 28*i0 + 63);
    _t5_3 = _mm256_broadcast_sd(F + 28*i0 + 88);
    _t5_2 = _mm256_broadcast_sd(F + 28*i0 + 89);
    _t5_1 = _mm256_broadcast_sd(F + 28*i0 + 90);
    _t5_0 = _mm256_broadcast_sd(F + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t5_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t5_19), _mm256_mul_pd(_t3_14, _t5_18)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t5_17), _mm256_mul_pd(_t3_12, _t5_16)));
    _t5_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t5_19), _mm256_mul_pd(_t3_10, _t5_18)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t5_17), _mm256_mul_pd(_t3_8, _t5_16)));
    _t5_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t5_19), _mm256_mul_pd(_t3_6, _t5_18)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t5_17), _mm256_mul_pd(_t3_4, _t5_16)));
    _t5_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _t5_19), _mm256_mul_pd(_t3_2, _t5_18)), _mm256_add_pd(_mm256_mul_pd(_t3_1, _t5_17), _mm256_mul_pd(_t3_0, _t5_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t5_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t2_16), _mm256_mul_pd(_t5_14, _t2_17)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t2_18), _mm256_mul_pd(_t5_12, _t2_19)));
    _t5_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t2_16), _mm256_mul_pd(_t5_10, _t2_17)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t2_18), _mm256_mul_pd(_t5_8, _t2_19)));
    _t5_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t2_16), _mm256_mul_pd(_t5_6, _t2_17)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t2_18), _mm256_mul_pd(_t5_4, _t2_19)));
    _t5_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_3, _t2_16), _mm256_mul_pd(_t5_2, _t2_17)), _mm256_add_pd(_mm256_mul_pd(_t5_1, _t2_18), _mm256_mul_pd(_t5_0, _t2_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t5_20 = _mm256_add_pd(_t5_20, _t5_24);
    _t5_21 = _mm256_add_pd(_t5_21, _t5_25);
    _t5_22 = _mm256_add_pd(_t5_22, _t5_26);
    _t5_23 = _mm256_add_pd(_t5_23, _t5_27);

    // AVX Storer:

    for( int i4 = 8; i4 <= 27; i4+=4 ) {
      _t6_19 = _mm256_broadcast_sd(F + 28*i0 + i4);
      _t6_18 = _mm256_broadcast_sd(F + 28*i0 + i4 + 1);
      _t6_17 = _mm256_broadcast_sd(F + 28*i0 + i4 + 2);
      _t6_16 = _mm256_broadcast_sd(F + 28*i0 + i4 + 3);
      _t6_15 = _mm256_broadcast_sd(F + 28*i0 + i4 + 28);
      _t6_14 = _mm256_broadcast_sd(F + 28*i0 + i4 + 29);
      _t6_13 = _mm256_broadcast_sd(F + 28*i0 + i4 + 30);
      _t6_12 = _mm256_broadcast_sd(F + 28*i0 + i4 + 31);
      _t6_11 = _mm256_broadcast_sd(F + 28*i0 + i4 + 56);
      _t6_10 = _mm256_broadcast_sd(F + 28*i0 + i4 + 57);
      _t6_9 = _mm256_broadcast_sd(F + 28*i0 + i4 + 58);
      _t6_8 = _mm256_broadcast_sd(F + 28*i0 + i4 + 59);
      _t6_7 = _mm256_broadcast_sd(F + 28*i0 + i4 + 84);
      _t6_6 = _mm256_broadcast_sd(F + 28*i0 + i4 + 85);
      _t6_5 = _mm256_broadcast_sd(F + 28*i0 + i4 + 86);
      _t6_4 = _mm256_broadcast_sd(F + 28*i0 + i4 + 87);
      _t6_3 = _mm256_loadu_pd(P + i4 + 112);
      _t6_2 = _mm256_loadu_pd(P + i4 + 140);
      _t6_1 = _mm256_loadu_pd(P + i4 + 168);
      _t6_0 = _mm256_loadu_pd(P + i4 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t6_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 32);
      _t6_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 32);
      _t6_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t6_3, _t6_2), _mm256_unpacklo_pd(_t6_1, _t6_0), 49);
      _t6_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t6_3, _t6_2), _mm256_unpackhi_pd(_t6_1, _t6_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_19, _t6_24), _mm256_mul_pd(_t6_18, _t6_25)), _mm256_add_pd(_mm256_mul_pd(_t6_17, _t6_26), _mm256_mul_pd(_t6_16, _t6_27)));
      _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t6_24), _mm256_mul_pd(_t6_14, _t6_25)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t6_26), _mm256_mul_pd(_t6_12, _t6_27)));
      _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t6_24), _mm256_mul_pd(_t6_10, _t6_25)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t6_26), _mm256_mul_pd(_t6_8, _t6_27)));
      _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t6_24), _mm256_mul_pd(_t6_6, _t6_25)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t6_26), _mm256_mul_pd(_t6_4, _t6_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t5_20 = _mm256_add_pd(_t5_20, _t6_20);
      _t5_21 = _mm256_add_pd(_t5_21, _t6_21);
      _t5_22 = _mm256_add_pd(_t5_22, _t6_22);
      _t5_23 = _mm256_add_pd(_t5_23, _t6_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k2 = 8; k2 <= 23; k2+=4 ) {
      _t7_3 = _mm256_loadu_pd(P + k2);
      _t7_2 = _mm256_loadu_pd(P + k2 + 28);
      _t7_1 = _mm256_loadu_pd(P + k2 + 56);
      _t7_0 = _mm256_loadu_pd(P + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t7_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t7_3), _mm256_mul_pd(_t3_14, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t7_1), _mm256_mul_pd(_t3_12, _t7_0)));
      _t7_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t7_3), _mm256_mul_pd(_t3_10, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t7_1), _mm256_mul_pd(_t3_8, _t7_0)));
      _t7_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t7_3), _mm256_mul_pd(_t3_6, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t7_1), _mm256_mul_pd(_t3_4, _t7_0)));
      _t7_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _t7_3), _mm256_mul_pd(_t3_2, _t7_2)), _mm256_add_pd(_mm256_mul_pd(_t3_1, _t7_1), _mm256_mul_pd(_t3_0, _t7_0)));

      // AVX Storer:

      for( int i4 = 4; i4 <= k2 - 1; i4+=4 ) {
        _t8_19 = _mm256_broadcast_sd(F + 28*i0 + i4);
        _t8_18 = _mm256_broadcast_sd(F + 28*i0 + i4 + 1);
        _t8_17 = _mm256_broadcast_sd(F + 28*i0 + i4 + 2);
        _t8_16 = _mm256_broadcast_sd(F + 28*i0 + i4 + 3);
        _t8_15 = _mm256_broadcast_sd(F + 28*i0 + i4 + 28);
        _t8_14 = _mm256_broadcast_sd(F + 28*i0 + i4 + 29);
        _t8_13 = _mm256_broadcast_sd(F + 28*i0 + i4 + 30);
        _t8_12 = _mm256_broadcast_sd(F + 28*i0 + i4 + 31);
        _t8_11 = _mm256_broadcast_sd(F + 28*i0 + i4 + 56);
        _t8_10 = _mm256_broadcast_sd(F + 28*i0 + i4 + 57);
        _t8_9 = _mm256_broadcast_sd(F + 28*i0 + i4 + 58);
        _t8_8 = _mm256_broadcast_sd(F + 28*i0 + i4 + 59);
        _t8_7 = _mm256_broadcast_sd(F + 28*i0 + i4 + 84);
        _t8_6 = _mm256_broadcast_sd(F + 28*i0 + i4 + 85);
        _t8_5 = _mm256_broadcast_sd(F + 28*i0 + i4 + 86);
        _t8_4 = _mm256_broadcast_sd(F + 28*i0 + i4 + 87);
        _t8_3 = _mm256_loadu_pd(P + 28*i4 + k2);
        _t8_2 = _mm256_loadu_pd(P + 28*i4 + k2 + 28);
        _t8_1 = _mm256_loadu_pd(P + 28*i4 + k2 + 56);
        _t8_0 = _mm256_loadu_pd(P + 28*i4 + k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t8_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_19, _t8_3), _mm256_mul_pd(_t8_18, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_17, _t8_1), _mm256_mul_pd(_t8_16, _t8_0)));
        _t8_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_15, _t8_3), _mm256_mul_pd(_t8_14, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_13, _t8_1), _mm256_mul_pd(_t8_12, _t8_0)));
        _t8_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_11, _t8_3), _mm256_mul_pd(_t8_10, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_9, _t8_1), _mm256_mul_pd(_t8_8, _t8_0)));
        _t8_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t8_7, _t8_3), _mm256_mul_pd(_t8_6, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t8_5, _t8_1), _mm256_mul_pd(_t8_4, _t8_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t7_4 = _mm256_add_pd(_t7_4, _t8_20);
        _t7_5 = _mm256_add_pd(_t7_5, _t8_21);
        _t7_6 = _mm256_add_pd(_t7_6, _t8_22);
        _t7_7 = _mm256_add_pd(_t7_7, _t8_23);

        // AVX Storer:
      }
      _t9_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t9_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t9_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t9_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t9_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t9_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t9_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t9_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t9_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t9_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t9_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t9_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t9_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t9_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t9_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t9_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t9_3 = _mm256_loadu_pd(P + 29*k2);
      _t9_2 = _mm256_maskload_pd(P + 29*k2 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t9_1 = _mm256_maskload_pd(P + 29*k2 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t9_0 = _mm256_maskload_pd(P + 29*k2 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t9_24 = _t9_3;
      _t9_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t9_3, _t9_2, 3), _t9_2, 12);
      _t9_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t9_3, _t9_2, 0), _t9_1, 49);
      _t9_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t9_3, _t9_2, 12), _mm256_shuffle_pd(_t9_1, _t9_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_24), _mm256_mul_pd(_t9_18, _t9_25)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_26), _mm256_mul_pd(_t9_16, _t9_27)));
      _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_24), _mm256_mul_pd(_t9_14, _t9_25)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_26), _mm256_mul_pd(_t9_12, _t9_27)));
      _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_24), _mm256_mul_pd(_t9_10, _t9_25)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_26), _mm256_mul_pd(_t9_8, _t9_27)));
      _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_24), _mm256_mul_pd(_t9_6, _t9_25)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_26), _mm256_mul_pd(_t9_4, _t9_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t7_4 = _mm256_add_pd(_t7_4, _t9_20);
      _t7_5 = _mm256_add_pd(_t7_5, _t9_21);
      _t7_6 = _mm256_add_pd(_t7_6, _t9_22);
      _t7_7 = _mm256_add_pd(_t7_7, _t9_23);

      // AVX Storer:

      for( int i4 = 4*floord(k2 - 1, 4) + 8; i4 <= 27; i4+=4 ) {
        _t10_19 = _mm256_broadcast_sd(F + 28*i0 + i4);
        _t10_18 = _mm256_broadcast_sd(F + 28*i0 + i4 + 1);
        _t10_17 = _mm256_broadcast_sd(F + 28*i0 + i4 + 2);
        _t10_16 = _mm256_broadcast_sd(F + 28*i0 + i4 + 3);
        _t10_15 = _mm256_broadcast_sd(F + 28*i0 + i4 + 28);
        _t10_14 = _mm256_broadcast_sd(F + 28*i0 + i4 + 29);
        _t10_13 = _mm256_broadcast_sd(F + 28*i0 + i4 + 30);
        _t10_12 = _mm256_broadcast_sd(F + 28*i0 + i4 + 31);
        _t10_11 = _mm256_broadcast_sd(F + 28*i0 + i4 + 56);
        _t10_10 = _mm256_broadcast_sd(F + 28*i0 + i4 + 57);
        _t10_9 = _mm256_broadcast_sd(F + 28*i0 + i4 + 58);
        _t10_8 = _mm256_broadcast_sd(F + 28*i0 + i4 + 59);
        _t10_7 = _mm256_broadcast_sd(F + 28*i0 + i4 + 84);
        _t10_6 = _mm256_broadcast_sd(F + 28*i0 + i4 + 85);
        _t10_5 = _mm256_broadcast_sd(F + 28*i0 + i4 + 86);
        _t10_4 = _mm256_broadcast_sd(F + 28*i0 + i4 + 87);
        _t10_3 = _mm256_loadu_pd(P + i4 + 28*k2);
        _t10_2 = _mm256_loadu_pd(P + i4 + 28*k2 + 28);
        _t10_1 = _mm256_loadu_pd(P + i4 + 28*k2 + 56);
        _t10_0 = _mm256_loadu_pd(P + i4 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t10_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 32);
        _t10_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 32);
        _t10_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t10_3, _t10_2), _mm256_unpacklo_pd(_t10_1, _t10_0), 49);
        _t10_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t10_3, _t10_2), _mm256_unpackhi_pd(_t10_1, _t10_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t10_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_24), _mm256_mul_pd(_t10_18, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_26), _mm256_mul_pd(_t10_16, _t10_27)));
        _t10_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_24), _mm256_mul_pd(_t10_14, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_26), _mm256_mul_pd(_t10_12, _t10_27)));
        _t10_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_24), _mm256_mul_pd(_t10_10, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_26), _mm256_mul_pd(_t10_8, _t10_27)));
        _t10_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_24), _mm256_mul_pd(_t10_6, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t10_26), _mm256_mul_pd(_t10_4, _t10_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t7_4 = _mm256_add_pd(_t7_4, _t10_20);
        _t7_5 = _mm256_add_pd(_t7_5, _t10_21);
        _t7_6 = _mm256_add_pd(_t7_6, _t10_22);
        _t7_7 = _mm256_add_pd(_t7_7, _t10_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M0 + 28*i0 + k2, _t7_4);
      _mm256_storeu_pd(M0 + 28*i0 + k2 + 28, _t7_5);
      _mm256_storeu_pd(M0 + 28*i0 + k2 + 56, _t7_6);
      _mm256_storeu_pd(M0 + 28*i0 + k2 + 84, _t7_7);
    }
    _t11_3 = _mm256_loadu_pd(P + 24);
    _t11_2 = _mm256_loadu_pd(P + 52);
    _t11_1 = _mm256_loadu_pd(P + 80);
    _t11_0 = _mm256_loadu_pd(P + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t11_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_15, _t11_3), _mm256_mul_pd(_t3_14, _t11_2)), _mm256_add_pd(_mm256_mul_pd(_t3_13, _t11_1), _mm256_mul_pd(_t3_12, _t11_0)));
    _t11_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_11, _t11_3), _mm256_mul_pd(_t3_10, _t11_2)), _mm256_add_pd(_mm256_mul_pd(_t3_9, _t11_1), _mm256_mul_pd(_t3_8, _t11_0)));
    _t11_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_7, _t11_3), _mm256_mul_pd(_t3_6, _t11_2)), _mm256_add_pd(_mm256_mul_pd(_t3_5, _t11_1), _mm256_mul_pd(_t3_4, _t11_0)));
    _t11_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t3_3, _t11_3), _mm256_mul_pd(_t3_2, _t11_2)), _mm256_add_pd(_mm256_mul_pd(_t3_1, _t11_1), _mm256_mul_pd(_t3_0, _t11_0)));

    // AVX Storer:

    for( int i4 = 4; i4 <= 23; i4+=4 ) {
      _t12_19 = _mm256_broadcast_sd(F + 28*i0 + i4);
      _t12_18 = _mm256_broadcast_sd(F + 28*i0 + i4 + 1);
      _t12_17 = _mm256_broadcast_sd(F + 28*i0 + i4 + 2);
      _t12_16 = _mm256_broadcast_sd(F + 28*i0 + i4 + 3);
      _t12_15 = _mm256_broadcast_sd(F + 28*i0 + i4 + 28);
      _t12_14 = _mm256_broadcast_sd(F + 28*i0 + i4 + 29);
      _t12_13 = _mm256_broadcast_sd(F + 28*i0 + i4 + 30);
      _t12_12 = _mm256_broadcast_sd(F + 28*i0 + i4 + 31);
      _t12_11 = _mm256_broadcast_sd(F + 28*i0 + i4 + 56);
      _t12_10 = _mm256_broadcast_sd(F + 28*i0 + i4 + 57);
      _t12_9 = _mm256_broadcast_sd(F + 28*i0 + i4 + 58);
      _t12_8 = _mm256_broadcast_sd(F + 28*i0 + i4 + 59);
      _t12_7 = _mm256_broadcast_sd(F + 28*i0 + i4 + 84);
      _t12_6 = _mm256_broadcast_sd(F + 28*i0 + i4 + 85);
      _t12_5 = _mm256_broadcast_sd(F + 28*i0 + i4 + 86);
      _t12_4 = _mm256_broadcast_sd(F + 28*i0 + i4 + 87);
      _t12_3 = _mm256_loadu_pd(P + 28*i4 + 24);
      _t12_2 = _mm256_loadu_pd(P + 28*i4 + 52);
      _t12_1 = _mm256_loadu_pd(P + 28*i4 + 80);
      _t12_0 = _mm256_loadu_pd(P + 28*i4 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t12_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_19, _t12_3), _mm256_mul_pd(_t12_18, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_17, _t12_1), _mm256_mul_pd(_t12_16, _t12_0)));
      _t12_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_15, _t12_3), _mm256_mul_pd(_t12_14, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_13, _t12_1), _mm256_mul_pd(_t12_12, _t12_0)));
      _t12_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_11, _t12_3), _mm256_mul_pd(_t12_10, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_9, _t12_1), _mm256_mul_pd(_t12_8, _t12_0)));
      _t12_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t12_7, _t12_3), _mm256_mul_pd(_t12_6, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t12_5, _t12_1), _mm256_mul_pd(_t12_4, _t12_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t11_4 = _mm256_add_pd(_t11_4, _t12_20);
      _t11_5 = _mm256_add_pd(_t11_5, _t12_21);
      _t11_6 = _mm256_add_pd(_t11_6, _t12_22);
      _t11_7 = _mm256_add_pd(_t11_7, _t12_23);

      // AVX Storer:
    }
    _t13_15 = _mm256_broadcast_sd(F + 28*i0 + 24);
    _t13_14 = _mm256_broadcast_sd(F + 28*i0 + 25);
    _t13_13 = _mm256_broadcast_sd(F + 28*i0 + 26);
    _t13_12 = _mm256_broadcast_sd(F + 28*i0 + 27);
    _t13_11 = _mm256_broadcast_sd(F + 28*i0 + 52);
    _t13_10 = _mm256_broadcast_sd(F + 28*i0 + 53);
    _t13_9 = _mm256_broadcast_sd(F + 28*i0 + 54);
    _t13_8 = _mm256_broadcast_sd(F + 28*i0 + 55);
    _t13_7 = _mm256_broadcast_sd(F + 28*i0 + 80);
    _t13_6 = _mm256_broadcast_sd(F + 28*i0 + 81);
    _t13_5 = _mm256_broadcast_sd(F + 28*i0 + 82);
    _t13_4 = _mm256_broadcast_sd(F + 28*i0 + 83);
    _t13_3 = _mm256_broadcast_sd(F + 28*i0 + 108);
    _t13_2 = _mm256_broadcast_sd(F + 28*i0 + 109);
    _t13_1 = _mm256_broadcast_sd(F + 28*i0 + 110);
    _t13_0 = _mm256_broadcast_sd(F + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t13_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t2_20), _mm256_mul_pd(_t13_14, _t2_21)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t2_22), _mm256_mul_pd(_t13_12, _t2_23)));
    _t13_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t2_20), _mm256_mul_pd(_t13_10, _t2_21)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t2_22), _mm256_mul_pd(_t13_8, _t2_23)));
    _t13_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t2_20), _mm256_mul_pd(_t13_6, _t2_21)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t2_22), _mm256_mul_pd(_t13_4, _t2_23)));
    _t13_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_3, _t2_20), _mm256_mul_pd(_t13_2, _t2_21)), _mm256_add_pd(_mm256_mul_pd(_t13_1, _t2_22), _mm256_mul_pd(_t13_0, _t2_23)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t11_4 = _mm256_add_pd(_t11_4, _t13_16);
    _t11_5 = _mm256_add_pd(_t11_5, _t13_17);
    _t11_6 = _mm256_add_pd(_t11_6, _t13_18);
    _t11_7 = _mm256_add_pd(_t11_7, _t13_19);

    // AVX Storer:
    _mm256_storeu_pd(M0 + 28*i0, _t3_16);
    _mm256_storeu_pd(M0 + 28*i0 + 28, _t3_17);
    _mm256_storeu_pd(M0 + 28*i0 + 56, _t3_18);
    _mm256_storeu_pd(M0 + 28*i0 + 84, _t3_19);
    _mm256_storeu_pd(M0 + 28*i0 + 4, _t5_20);
    _mm256_storeu_pd(M0 + 28*i0 + 32, _t5_21);
    _mm256_storeu_pd(M0 + 28*i0 + 60, _t5_22);
    _mm256_storeu_pd(M0 + 28*i0 + 88, _t5_23);
    _mm256_storeu_pd(M0 + 28*i0 + 24, _t11_4);
    _mm256_storeu_pd(M0 + 28*i0 + 52, _t11_5);
    _mm256_storeu_pd(M0 + 28*i0 + 80, _t11_6);
    _mm256_storeu_pd(M0 + 28*i0 + 108, _t11_7);
  }


  // Generating : Y[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, i0)) ),h(4, 28, i0)) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, i4)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, i4)) ) ),h(4, 28, i0)) ) ) + Sum_{k2} ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, k2), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, k2)) ),h(4, 28, k2)) + Sum_{i4} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, i4)) * T( G(h(4, 28, k2), F[28,28],h(4, 28, i4)) ) ),h(4, 28, k2)) ) ) ) ) ) + S(h(4, 28, 24), ( ( G(h(4, 28, 24), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, 24), Q[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{i4} ( $(h(4, 28, 24), ( G(h(4, 28, 24), M0[28,28],h(4, 28, i4)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, i4)) ) ),h(4, 28, 24)) ) )


  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t14_23 = _mm256_broadcast_sd(M0 + 28*i0);
    _t14_22 = _mm256_broadcast_sd(M0 + 28*i0 + 1);
    _t14_21 = _mm256_broadcast_sd(M0 + 28*i0 + 2);
    _t14_20 = _mm256_broadcast_sd(M0 + 28*i0 + 3);
    _t14_19 = _mm256_broadcast_sd(M0 + 28*i0 + 28);
    _t14_18 = _mm256_broadcast_sd(M0 + 28*i0 + 29);
    _t14_17 = _mm256_broadcast_sd(M0 + 28*i0 + 30);
    _t14_16 = _mm256_broadcast_sd(M0 + 28*i0 + 31);
    _t14_15 = _mm256_broadcast_sd(M0 + 28*i0 + 56);
    _t14_14 = _mm256_broadcast_sd(M0 + 28*i0 + 57);
    _t14_13 = _mm256_broadcast_sd(M0 + 28*i0 + 58);
    _t14_12 = _mm256_broadcast_sd(M0 + 28*i0 + 59);
    _t14_11 = _mm256_broadcast_sd(M0 + 28*i0 + 84);
    _t14_10 = _mm256_broadcast_sd(M0 + 28*i0 + 85);
    _t14_9 = _mm256_broadcast_sd(M0 + 28*i0 + 86);
    _t14_8 = _mm256_broadcast_sd(M0 + 28*i0 + 87);
    _t14_7 = _mm256_loadu_pd(F + 28*i0);
    _t14_6 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t14_5 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t14_4 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t14_3 = _mm256_loadu_pd(Q + 29*i0);
    _t14_2 = _mm256_maskload_pd(Q + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t14_1 = _mm256_maskload_pd(Q + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t14_0 = _mm256_maskload_pd(Q + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t14_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 32);
    _t14_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 32);
    _t14_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t14_7, _t14_6), _mm256_unpacklo_pd(_t14_5, _t14_4), 49);
    _t14_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t14_7, _t14_6), _mm256_unpackhi_pd(_t14_5, _t14_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t14_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_23, _t14_40), _mm256_mul_pd(_t14_22, _t14_41)), _mm256_add_pd(_mm256_mul_pd(_t14_21, _t14_42), _mm256_mul_pd(_t14_20, _t14_43)));
    _t14_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_19, _t14_40), _mm256_mul_pd(_t14_18, _t14_41)), _mm256_add_pd(_mm256_mul_pd(_t14_17, _t14_42), _mm256_mul_pd(_t14_16, _t14_43)));
    _t14_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t14_40), _mm256_mul_pd(_t14_14, _t14_41)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t14_42), _mm256_mul_pd(_t14_12, _t14_43)));
    _t14_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t14_40), _mm256_mul_pd(_t14_10, _t14_41)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t14_42), _mm256_mul_pd(_t14_8, _t14_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t14_36 = _t14_3;
    _t14_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t14_3, _t14_2, 3), _t14_2, 12);
    _t14_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_3, _t14_2, 0), _t14_1, 49);
    _t14_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_3, _t14_2, 12), _mm256_shuffle_pd(_t14_1, _t14_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t14_24 = _mm256_add_pd(_t14_32, _t14_36);
    _t14_25 = _mm256_add_pd(_t14_33, _t14_37);
    _t14_26 = _mm256_add_pd(_t14_34, _t14_38);
    _t14_27 = _mm256_add_pd(_t14_35, _t14_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t14_28 = _t14_24;
    _t14_29 = _t14_25;
    _t14_30 = _t14_26;
    _t14_31 = _t14_27;

    for( int i4 = 4; i4 <= 27; i4+=4 ) {
      _t15_19 = _mm256_broadcast_sd(M0 + 28*i0 + i4);
      _t15_18 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 1);
      _t15_17 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 2);
      _t15_16 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 3);
      _t15_15 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 28);
      _t15_14 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 29);
      _t15_13 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 30);
      _t15_12 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 31);
      _t15_11 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 56);
      _t15_10 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 57);
      _t15_9 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 58);
      _t15_8 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 59);
      _t15_7 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 84);
      _t15_6 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 85);
      _t15_5 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 86);
      _t15_4 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 87);
      _t15_3 = _mm256_loadu_pd(F + 28*i0 + i4);
      _t15_2 = _mm256_loadu_pd(F + 28*i0 + i4 + 28);
      _t15_1 = _mm256_loadu_pd(F + 28*i0 + i4 + 56);
      _t15_0 = _mm256_loadu_pd(F + 28*i0 + i4 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t15_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 32);
      _t15_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 32);
      _t15_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_3, _t15_2), _mm256_unpacklo_pd(_t15_1, _t15_0), 49);
      _t15_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_3, _t15_2), _mm256_unpackhi_pd(_t15_1, _t15_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t15_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_28), _mm256_mul_pd(_t15_18, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_30), _mm256_mul_pd(_t15_16, _t15_31)));
      _t15_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_28), _mm256_mul_pd(_t15_14, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_30), _mm256_mul_pd(_t15_12, _t15_31)));
      _t15_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_28), _mm256_mul_pd(_t15_10, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_30), _mm256_mul_pd(_t15_8, _t15_31)));
      _t15_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_7, _t15_28), _mm256_mul_pd(_t15_6, _t15_29)), _mm256_add_pd(_mm256_mul_pd(_t15_5, _t15_30), _mm256_mul_pd(_t15_4, _t15_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t15_24 = _t14_28;
      _t15_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t14_28, _t14_29, 3), _t14_29, 12);
      _t15_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_28, _t14_29, 0), _t14_30, 49);
      _t15_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t14_28, _t14_29, 12), _mm256_shuffle_pd(_t14_30, _t14_31, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t15_24 = _mm256_add_pd(_t15_24, _t15_20);
      _t15_25 = _mm256_add_pd(_t15_25, _t15_21);
      _t15_26 = _mm256_add_pd(_t15_26, _t15_22);
      _t15_27 = _mm256_add_pd(_t15_27, _t15_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t14_28 = _t15_24;
      _t14_29 = _t15_25;
      _t14_30 = _t15_26;
      _t14_31 = _t15_27;
    }

    // AVX Loader:

    for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
      _t16_7 = _mm256_loadu_pd(F + 28*k2);
      _t16_6 = _mm256_loadu_pd(F + 28*k2 + 28);
      _t16_5 = _mm256_loadu_pd(F + 28*k2 + 56);
      _t16_4 = _mm256_loadu_pd(F + 28*k2 + 84);
      _t16_3 = _mm256_loadu_pd(Q + 28*i0 + k2);
      _t16_2 = _mm256_loadu_pd(Q + 28*i0 + k2 + 28);
      _t16_1 = _mm256_loadu_pd(Q + 28*i0 + k2 + 56);
      _t16_0 = _mm256_loadu_pd(Q + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_7, _t16_6), _mm256_unpacklo_pd(_t16_5, _t16_4), 32);
      _t16_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_7, _t16_6), _mm256_unpackhi_pd(_t16_5, _t16_4), 32);
      _t16_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_7, _t16_6), _mm256_unpacklo_pd(_t16_5, _t16_4), 49);
      _t16_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_7, _t16_6), _mm256_unpackhi_pd(_t16_5, _t16_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_23, _t16_16), _mm256_mul_pd(_t14_22, _t16_17)), _mm256_add_pd(_mm256_mul_pd(_t14_21, _t16_18), _mm256_mul_pd(_t14_20, _t16_19)));
      _t16_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_19, _t16_16), _mm256_mul_pd(_t14_18, _t16_17)), _mm256_add_pd(_mm256_mul_pd(_t14_17, _t16_18), _mm256_mul_pd(_t14_16, _t16_19)));
      _t16_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t16_16), _mm256_mul_pd(_t14_14, _t16_17)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t16_18), _mm256_mul_pd(_t14_12, _t16_19)));
      _t16_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t16_16), _mm256_mul_pd(_t14_10, _t16_17)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t16_18), _mm256_mul_pd(_t14_8, _t16_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t16_8 = _mm256_add_pd(_t16_12, _t16_3);
      _t16_9 = _mm256_add_pd(_t16_13, _t16_2);
      _t16_10 = _mm256_add_pd(_t16_14, _t16_1);
      _t16_11 = _mm256_add_pd(_t16_15, _t16_0);

      // AVX Storer:

      for( int i4 = 4; i4 <= 27; i4+=4 ) {
        _t17_19 = _mm256_broadcast_sd(M0 + 28*i0 + i4);
        _t17_18 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 1);
        _t17_17 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 2);
        _t17_16 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 3);
        _t17_15 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 28);
        _t17_14 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 29);
        _t17_13 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 30);
        _t17_12 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 31);
        _t17_11 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 56);
        _t17_10 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 57);
        _t17_9 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 58);
        _t17_8 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 59);
        _t17_7 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 84);
        _t17_6 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 85);
        _t17_5 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 86);
        _t17_4 = _mm256_broadcast_sd(M0 + 28*i0 + i4 + 87);
        _t17_3 = _mm256_loadu_pd(F + i4 + 28*k2);
        _t17_2 = _mm256_loadu_pd(F + i4 + 28*k2 + 28);
        _t17_1 = _mm256_loadu_pd(F + i4 + 28*k2 + 56);
        _t17_0 = _mm256_loadu_pd(F + i4 + 28*k2 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t17_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 32);
        _t17_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 32);
        _t17_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_3, _t17_2), _mm256_unpacklo_pd(_t17_1, _t17_0), 49);
        _t17_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_3, _t17_2), _mm256_unpackhi_pd(_t17_1, _t17_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t17_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_19, _t17_24), _mm256_mul_pd(_t17_18, _t17_25)), _mm256_add_pd(_mm256_mul_pd(_t17_17, _t17_26), _mm256_mul_pd(_t17_16, _t17_27)));
        _t17_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_15, _t17_24), _mm256_mul_pd(_t17_14, _t17_25)), _mm256_add_pd(_mm256_mul_pd(_t17_13, _t17_26), _mm256_mul_pd(_t17_12, _t17_27)));
        _t17_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_11, _t17_24), _mm256_mul_pd(_t17_10, _t17_25)), _mm256_add_pd(_mm256_mul_pd(_t17_9, _t17_26), _mm256_mul_pd(_t17_8, _t17_27)));
        _t17_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t17_7, _t17_24), _mm256_mul_pd(_t17_6, _t17_25)), _mm256_add_pd(_mm256_mul_pd(_t17_5, _t17_26), _mm256_mul_pd(_t17_4, _t17_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t16_8 = _mm256_add_pd(_t16_8, _t17_20);
        _t16_9 = _mm256_add_pd(_t16_9, _t17_21);
        _t16_10 = _mm256_add_pd(_t16_10, _t17_22);
        _t16_11 = _mm256_add_pd(_t16_11, _t17_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(Y + 28*i0 + k2, _t16_8);
      _mm256_storeu_pd(Y + 28*i0 + k2 + 28, _t16_9);
      _mm256_storeu_pd(Y + 28*i0 + k2 + 56, _t16_10);
      _mm256_storeu_pd(Y + 28*i0 + k2 + 84, _t16_11);
    }
    _mm256_storeu_pd(Y + 29*i0, _t14_28);
    _mm256_maskstore_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t14_29);
    _mm256_maskstore_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t14_30);
    _mm256_maskstore_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t14_31);
  }

  _t18_23 = _mm256_broadcast_sd(M0 + 672);
  _t18_22 = _mm256_broadcast_sd(M0 + 673);
  _t18_21 = _mm256_broadcast_sd(M0 + 674);
  _t18_20 = _mm256_broadcast_sd(M0 + 675);
  _t18_19 = _mm256_broadcast_sd(M0 + 700);
  _t18_18 = _mm256_broadcast_sd(M0 + 701);
  _t18_17 = _mm256_broadcast_sd(M0 + 702);
  _t18_16 = _mm256_broadcast_sd(M0 + 703);
  _t18_15 = _mm256_broadcast_sd(M0 + 728);
  _t18_14 = _mm256_broadcast_sd(M0 + 729);
  _t18_13 = _mm256_broadcast_sd(M0 + 730);
  _t18_12 = _mm256_broadcast_sd(M0 + 731);
  _t18_11 = _mm256_broadcast_sd(M0 + 756);
  _t18_10 = _mm256_broadcast_sd(M0 + 757);
  _t18_9 = _mm256_broadcast_sd(M0 + 758);
  _t18_8 = _mm256_broadcast_sd(M0 + 759);
  _t18_7 = _mm256_loadu_pd(F + 672);
  _t18_6 = _mm256_loadu_pd(F + 700);
  _t18_5 = _mm256_loadu_pd(F + 728);
  _t18_4 = _mm256_loadu_pd(F + 756);
  _t18_3 = _mm256_loadu_pd(Q + 696);
  _t18_2 = _mm256_maskload_pd(Q + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t18_1 = _mm256_maskload_pd(Q + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t18_0 = _mm256_maskload_pd(Q + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t18_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_7, _t18_6), _mm256_unpacklo_pd(_t18_5, _t18_4), 32);
  _t18_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_7, _t18_6), _mm256_unpackhi_pd(_t18_5, _t18_4), 32);
  _t18_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_7, _t18_6), _mm256_unpacklo_pd(_t18_5, _t18_4), 49);
  _t18_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_7, _t18_6), _mm256_unpackhi_pd(_t18_5, _t18_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t18_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_23, _t18_40), _mm256_mul_pd(_t18_22, _t18_41)), _mm256_add_pd(_mm256_mul_pd(_t18_21, _t18_42), _mm256_mul_pd(_t18_20, _t18_43)));
  _t18_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_40), _mm256_mul_pd(_t18_18, _t18_41)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_42), _mm256_mul_pd(_t18_16, _t18_43)));
  _t18_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_40), _mm256_mul_pd(_t18_14, _t18_41)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_42), _mm256_mul_pd(_t18_12, _t18_43)));
  _t18_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_40), _mm256_mul_pd(_t18_10, _t18_41)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_42), _mm256_mul_pd(_t18_8, _t18_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t18_36 = _t18_3;
  _t18_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t18_3, _t18_2, 3), _t18_2, 12);
  _t18_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_3, _t18_2, 0), _t18_1, 49);
  _t18_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_3, _t18_2, 12), _mm256_shuffle_pd(_t18_1, _t18_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t18_24 = _mm256_add_pd(_t18_32, _t18_36);
  _t18_25 = _mm256_add_pd(_t18_33, _t18_37);
  _t18_26 = _mm256_add_pd(_t18_34, _t18_38);
  _t18_27 = _mm256_add_pd(_t18_35, _t18_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t18_28 = _t18_24;
  _t18_29 = _t18_25;
  _t18_30 = _t18_26;
  _t18_31 = _t18_27;


  for( int i4 = 4; i4 <= 27; i4+=4 ) {
    _t19_19 = _mm256_broadcast_sd(M0 + i4 + 672);
    _t19_18 = _mm256_broadcast_sd(M0 + i4 + 673);
    _t19_17 = _mm256_broadcast_sd(M0 + i4 + 674);
    _t19_16 = _mm256_broadcast_sd(M0 + i4 + 675);
    _t19_15 = _mm256_broadcast_sd(M0 + i4 + 700);
    _t19_14 = _mm256_broadcast_sd(M0 + i4 + 701);
    _t19_13 = _mm256_broadcast_sd(M0 + i4 + 702);
    _t19_12 = _mm256_broadcast_sd(M0 + i4 + 703);
    _t19_11 = _mm256_broadcast_sd(M0 + i4 + 728);
    _t19_10 = _mm256_broadcast_sd(M0 + i4 + 729);
    _t19_9 = _mm256_broadcast_sd(M0 + i4 + 730);
    _t19_8 = _mm256_broadcast_sd(M0 + i4 + 731);
    _t19_7 = _mm256_broadcast_sd(M0 + i4 + 756);
    _t19_6 = _mm256_broadcast_sd(M0 + i4 + 757);
    _t19_5 = _mm256_broadcast_sd(M0 + i4 + 758);
    _t19_4 = _mm256_broadcast_sd(M0 + i4 + 759);
    _t19_3 = _mm256_loadu_pd(F + i4 + 672);
    _t19_2 = _mm256_loadu_pd(F + i4 + 700);
    _t19_1 = _mm256_loadu_pd(F + i4 + 728);
    _t19_0 = _mm256_loadu_pd(F + i4 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t19_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_3, _t19_2), _mm256_unpacklo_pd(_t19_1, _t19_0), 32);
    _t19_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_3, _t19_2), _mm256_unpackhi_pd(_t19_1, _t19_0), 32);
    _t19_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_3, _t19_2), _mm256_unpacklo_pd(_t19_1, _t19_0), 49);
    _t19_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_3, _t19_2), _mm256_unpackhi_pd(_t19_1, _t19_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t19_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_28), _mm256_mul_pd(_t19_18, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_30), _mm256_mul_pd(_t19_16, _t19_31)));
    _t19_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_28), _mm256_mul_pd(_t19_14, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_30), _mm256_mul_pd(_t19_12, _t19_31)));
    _t19_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_28), _mm256_mul_pd(_t19_10, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_30), _mm256_mul_pd(_t19_8, _t19_31)));
    _t19_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_7, _t19_28), _mm256_mul_pd(_t19_6, _t19_29)), _mm256_add_pd(_mm256_mul_pd(_t19_5, _t19_30), _mm256_mul_pd(_t19_4, _t19_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t19_24 = _t18_28;
    _t19_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 3), _t18_29, 12);
    _t19_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 0), _t18_30, 49);
    _t19_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 12), _mm256_shuffle_pd(_t18_30, _t18_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t19_24 = _mm256_add_pd(_t19_24, _t19_20);
    _t19_25 = _mm256_add_pd(_t19_25, _t19_21);
    _t19_26 = _mm256_add_pd(_t19_26, _t19_22);
    _t19_27 = _mm256_add_pd(_t19_27, _t19_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t18_28 = _t19_24;
    _t18_29 = _t19_25;
    _t18_30 = _t19_26;
    _t18_31 = _t19_27;
  }

  _t20_5 = _mm256_loadu_pd(z);
  _t20_4 = _mm256_loadu_pd(H);
  _t20_3 = _mm256_loadu_pd(H + 28);
  _t20_2 = _mm256_loadu_pd(H + 56);
  _t20_1 = _mm256_loadu_pd(H + 84);
  _t20_0 = _mm256_loadu_pd(y);

  // Generating : v0[4,1] = ( S(h(4, 4, 0), ( G(h(4, 4, 0), z[4,1],h(1, 1, 0)) - ( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) * G(h(4, 28, 0), y[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{i0} ( -$(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, i0)) * G(h(4, 28, i0), y[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x1
  _t20_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t20_4, _t20_0), _mm256_mul_pd(_t20_3, _t20_0)), _mm256_hadd_pd(_mm256_mul_pd(_t20_2, _t20_0), _mm256_mul_pd(_t20_1, _t20_0)), 12));

  // 4-BLAC: 4x1 - 4x1
  _t20_7 = _mm256_sub_pd(_t20_5, _t20_6);

  // AVX Storer:


  for( int i0 = 4; i0 <= 27; i0+=4 ) {
    _t21_4 = _mm256_loadu_pd(H + i0);
    _t21_3 = _mm256_loadu_pd(H + i0 + 28);
    _t21_2 = _mm256_loadu_pd(H + i0 + 56);
    _t21_1 = _mm256_loadu_pd(H + i0 + 84);
    _t21_0 = _mm256_loadu_pd(y + i0);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t21_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 12));

    // AVX Loader:

    // 4-BLAC: 4x1 - 4x1
    _t20_7 = _mm256_sub_pd(_t20_7, _t21_5);

    // AVX Storer:
  }

  _t22_19 = _mm256_broadcast_sd(H);
  _t22_18 = _mm256_broadcast_sd(H + 1);
  _t22_17 = _mm256_broadcast_sd(H + 2);
  _t22_16 = _mm256_broadcast_sd(H + 3);
  _t22_15 = _mm256_broadcast_sd(H + 28);
  _t22_14 = _mm256_broadcast_sd(H + 29);
  _t22_13 = _mm256_broadcast_sd(H + 30);
  _t22_12 = _mm256_broadcast_sd(H + 31);
  _t22_11 = _mm256_broadcast_sd(H + 56);
  _t22_10 = _mm256_broadcast_sd(H + 57);
  _t22_9 = _mm256_broadcast_sd(H + 58);
  _t22_8 = _mm256_broadcast_sd(H + 59);
  _t22_7 = _mm256_broadcast_sd(H + 84);
  _t22_6 = _mm256_broadcast_sd(H + 85);
  _t22_5 = _mm256_broadcast_sd(H + 86);
  _t22_4 = _mm256_broadcast_sd(H + 87);
  _t22_3 = _mm256_loadu_pd(Y);
  _t22_2 = _mm256_maskload_pd(Y + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t22_1 = _mm256_maskload_pd(Y + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t22_0 = _mm256_maskload_pd(Y + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[4,28] = ( ( ( ( ( ( ( ( S(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) * T( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, 4)) * G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) * T( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{i0} ( ( ( ( S(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, i0)) ),h(4, 28, i0)) + Sum_{k2} ( $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) ) + $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, i0)) * G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) ),h(4, 28, i0)) ) + Sum_{k2} ( $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) * T( G(h(4, 28, i0), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, i0)) ) ) ) ) + S(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 4, 0), ( G(h(4, 4, 0), H[4,28],h(4, 28, 24)) * G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) )

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t22_24 = _t22_3;
  _t22_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 3), _t22_2, 12);
  _t22_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 0), _t22_1, 49);
  _t22_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 12), _mm256_shuffle_pd(_t22_1, _t22_0, 12), 49);

  // 4-BLAC: 4x4 * 4x4
  _t22_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_19, _t22_24), _mm256_mul_pd(_t22_18, _t22_25)), _mm256_add_pd(_mm256_mul_pd(_t22_17, _t22_26), _mm256_mul_pd(_t22_16, _t22_27)));
  _t22_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t22_24), _mm256_mul_pd(_t22_14, _t22_25)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t22_26), _mm256_mul_pd(_t22_12, _t22_27)));
  _t22_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t22_24), _mm256_mul_pd(_t22_10, _t22_25)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t22_26), _mm256_mul_pd(_t22_8, _t22_27)));
  _t22_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t22_24), _mm256_mul_pd(_t22_6, _t22_25)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t22_26), _mm256_mul_pd(_t22_4, _t22_27)));

  // AVX Storer:


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t23_19 = _mm256_broadcast_sd(H + k2);
    _t23_18 = _mm256_broadcast_sd(H + k2 + 1);
    _t23_17 = _mm256_broadcast_sd(H + k2 + 2);
    _t23_16 = _mm256_broadcast_sd(H + k2 + 3);
    _t23_15 = _mm256_broadcast_sd(H + k2 + 28);
    _t23_14 = _mm256_broadcast_sd(H + k2 + 29);
    _t23_13 = _mm256_broadcast_sd(H + k2 + 30);
    _t23_12 = _mm256_broadcast_sd(H + k2 + 31);
    _t23_11 = _mm256_broadcast_sd(H + k2 + 56);
    _t23_10 = _mm256_broadcast_sd(H + k2 + 57);
    _t23_9 = _mm256_broadcast_sd(H + k2 + 58);
    _t23_8 = _mm256_broadcast_sd(H + k2 + 59);
    _t23_7 = _mm256_broadcast_sd(H + k2 + 84);
    _t23_6 = _mm256_broadcast_sd(H + k2 + 85);
    _t23_5 = _mm256_broadcast_sd(H + k2 + 86);
    _t23_4 = _mm256_broadcast_sd(H + k2 + 87);
    _t23_3 = _mm256_loadu_pd(Y + k2);
    _t23_2 = _mm256_loadu_pd(Y + k2 + 28);
    _t23_1 = _mm256_loadu_pd(Y + k2 + 56);
    _t23_0 = _mm256_loadu_pd(Y + k2 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t23_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t23_3, _t23_2), _mm256_unpacklo_pd(_t23_1, _t23_0), 32);
    _t23_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t23_3, _t23_2), _mm256_unpackhi_pd(_t23_1, _t23_0), 32);
    _t23_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t23_3, _t23_2), _mm256_unpacklo_pd(_t23_1, _t23_0), 49);
    _t23_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t23_3, _t23_2), _mm256_unpackhi_pd(_t23_1, _t23_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t23_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_19, _t23_24), _mm256_mul_pd(_t23_18, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_17, _t23_26), _mm256_mul_pd(_t23_16, _t23_27)));
    _t23_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_15, _t23_24), _mm256_mul_pd(_t23_14, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_13, _t23_26), _mm256_mul_pd(_t23_12, _t23_27)));
    _t23_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_11, _t23_24), _mm256_mul_pd(_t23_10, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_9, _t23_26), _mm256_mul_pd(_t23_8, _t23_27)));
    _t23_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t23_7, _t23_24), _mm256_mul_pd(_t23_6, _t23_25)), _mm256_add_pd(_mm256_mul_pd(_t23_5, _t23_26), _mm256_mul_pd(_t23_4, _t23_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t22_20 = _mm256_add_pd(_t22_20, _t23_20);
    _t22_21 = _mm256_add_pd(_t22_21, _t23_21);
    _t22_22 = _mm256_add_pd(_t22_22, _t23_22);
    _t22_23 = _mm256_add_pd(_t22_23, _t23_23);

    // AVX Storer:
  }

  _t24_23 = _mm256_loadu_pd(Y + 4);
  _t24_22 = _mm256_loadu_pd(Y + 32);
  _t24_21 = _mm256_loadu_pd(Y + 60);
  _t24_20 = _mm256_loadu_pd(Y + 88);
  _t24_19 = _mm256_broadcast_sd(H + 4);
  _t24_18 = _mm256_broadcast_sd(H + 5);
  _t24_17 = _mm256_broadcast_sd(H + 6);
  _t24_16 = _mm256_broadcast_sd(H + 7);
  _t24_15 = _mm256_broadcast_sd(H + 32);
  _t24_14 = _mm256_broadcast_sd(H + 33);
  _t24_13 = _mm256_broadcast_sd(H + 34);
  _t24_12 = _mm256_broadcast_sd(H + 35);
  _t24_11 = _mm256_broadcast_sd(H + 60);
  _t24_10 = _mm256_broadcast_sd(H + 61);
  _t24_9 = _mm256_broadcast_sd(H + 62);
  _t24_8 = _mm256_broadcast_sd(H + 63);
  _t24_7 = _mm256_broadcast_sd(H + 88);
  _t24_6 = _mm256_broadcast_sd(H + 89);
  _t24_5 = _mm256_broadcast_sd(H + 90);
  _t24_4 = _mm256_broadcast_sd(H + 91);
  _t24_3 = _mm256_loadu_pd(Y + 116);
  _t24_2 = _mm256_maskload_pd(Y + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t24_1 = _mm256_maskload_pd(Y + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t24_0 = _mm256_maskload_pd(Y + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t24_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_19, _t24_23), _mm256_mul_pd(_t22_18, _t24_22)), _mm256_add_pd(_mm256_mul_pd(_t22_17, _t24_21), _mm256_mul_pd(_t22_16, _t24_20)));
  _t24_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t24_23), _mm256_mul_pd(_t22_14, _t24_22)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t24_21), _mm256_mul_pd(_t22_12, _t24_20)));
  _t24_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t24_23), _mm256_mul_pd(_t22_10, _t24_22)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t24_21), _mm256_mul_pd(_t22_8, _t24_20)));
  _t24_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t24_23), _mm256_mul_pd(_t22_6, _t24_22)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t24_21), _mm256_mul_pd(_t22_4, _t24_20)));

  // AVX Storer:

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t24_32 = _t24_3;
  _t24_33 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
  _t24_34 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
  _t24_35 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

  // 4-BLAC: 4x4 * 4x4
  _t24_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_19, _t24_32), _mm256_mul_pd(_t24_18, _t24_33)), _mm256_add_pd(_mm256_mul_pd(_t24_17, _t24_34), _mm256_mul_pd(_t24_16, _t24_35)));
  _t24_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t24_32), _mm256_mul_pd(_t24_14, _t24_33)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t24_34), _mm256_mul_pd(_t24_12, _t24_35)));
  _t24_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t24_32), _mm256_mul_pd(_t24_10, _t24_33)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t24_34), _mm256_mul_pd(_t24_8, _t24_35)));
  _t24_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t24_32), _mm256_mul_pd(_t24_6, _t24_33)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t24_34), _mm256_mul_pd(_t24_4, _t24_35)));

  // AVX Loader:

  // 4-BLAC: 4x4 + 4x4
  _t24_24 = _mm256_add_pd(_t24_24, _t24_28);
  _t24_25 = _mm256_add_pd(_t24_25, _t24_29);
  _t24_26 = _mm256_add_pd(_t24_26, _t24_30);
  _t24_27 = _mm256_add_pd(_t24_27, _t24_31);

  // AVX Storer:


  for( int k2 = 8; k2 <= 27; k2+=4 ) {
    _t25_19 = _mm256_broadcast_sd(H + k2);
    _t25_18 = _mm256_broadcast_sd(H + k2 + 1);
    _t25_17 = _mm256_broadcast_sd(H + k2 + 2);
    _t25_16 = _mm256_broadcast_sd(H + k2 + 3);
    _t25_15 = _mm256_broadcast_sd(H + k2 + 28);
    _t25_14 = _mm256_broadcast_sd(H + k2 + 29);
    _t25_13 = _mm256_broadcast_sd(H + k2 + 30);
    _t25_12 = _mm256_broadcast_sd(H + k2 + 31);
    _t25_11 = _mm256_broadcast_sd(H + k2 + 56);
    _t25_10 = _mm256_broadcast_sd(H + k2 + 57);
    _t25_9 = _mm256_broadcast_sd(H + k2 + 58);
    _t25_8 = _mm256_broadcast_sd(H + k2 + 59);
    _t25_7 = _mm256_broadcast_sd(H + k2 + 84);
    _t25_6 = _mm256_broadcast_sd(H + k2 + 85);
    _t25_5 = _mm256_broadcast_sd(H + k2 + 86);
    _t25_4 = _mm256_broadcast_sd(H + k2 + 87);
    _t25_3 = _mm256_loadu_pd(Y + k2 + 112);
    _t25_2 = _mm256_loadu_pd(Y + k2 + 140);
    _t25_1 = _mm256_loadu_pd(Y + k2 + 168);
    _t25_0 = _mm256_loadu_pd(Y + k2 + 196);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t25_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
    _t25_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
    _t25_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
    _t25_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t25_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_24), _mm256_mul_pd(_t25_18, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_26), _mm256_mul_pd(_t25_16, _t25_27)));
    _t25_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_24), _mm256_mul_pd(_t25_14, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_26), _mm256_mul_pd(_t25_12, _t25_27)));
    _t25_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_24), _mm256_mul_pd(_t25_10, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_26), _mm256_mul_pd(_t25_8, _t25_27)));
    _t25_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_24), _mm256_mul_pd(_t25_6, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_26), _mm256_mul_pd(_t25_4, _t25_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t24_24 = _mm256_add_pd(_t24_24, _t25_20);
    _t24_25 = _mm256_add_pd(_t24_25, _t25_21);
    _t24_26 = _mm256_add_pd(_t24_26, _t25_22);
    _t24_27 = _mm256_add_pd(_t24_27, _t25_23);

    // AVX Storer:
  }


  // AVX Loader:


  for( int i0 = 8; i0 <= 23; i0+=4 ) {
    _t26_3 = _mm256_loadu_pd(Y + i0);
    _t26_2 = _mm256_loadu_pd(Y + i0 + 28);
    _t26_1 = _mm256_loadu_pd(Y + i0 + 56);
    _t26_0 = _mm256_loadu_pd(Y + i0 + 84);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_19, _t26_3), _mm256_mul_pd(_t22_18, _t26_2)), _mm256_add_pd(_mm256_mul_pd(_t22_17, _t26_1), _mm256_mul_pd(_t22_16, _t26_0)));
    _t26_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t26_3), _mm256_mul_pd(_t22_14, _t26_2)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t26_1), _mm256_mul_pd(_t22_12, _t26_0)));
    _t26_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t26_3), _mm256_mul_pd(_t22_10, _t26_2)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t26_1), _mm256_mul_pd(_t22_8, _t26_0)));
    _t26_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t26_3), _mm256_mul_pd(_t22_6, _t26_2)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t26_1), _mm256_mul_pd(_t22_4, _t26_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= i0 - 1; k2+=4 ) {
      _t27_19 = _mm256_broadcast_sd(H + k2);
      _t27_18 = _mm256_broadcast_sd(H + k2 + 1);
      _t27_17 = _mm256_broadcast_sd(H + k2 + 2);
      _t27_16 = _mm256_broadcast_sd(H + k2 + 3);
      _t27_15 = _mm256_broadcast_sd(H + k2 + 28);
      _t27_14 = _mm256_broadcast_sd(H + k2 + 29);
      _t27_13 = _mm256_broadcast_sd(H + k2 + 30);
      _t27_12 = _mm256_broadcast_sd(H + k2 + 31);
      _t27_11 = _mm256_broadcast_sd(H + k2 + 56);
      _t27_10 = _mm256_broadcast_sd(H + k2 + 57);
      _t27_9 = _mm256_broadcast_sd(H + k2 + 58);
      _t27_8 = _mm256_broadcast_sd(H + k2 + 59);
      _t27_7 = _mm256_broadcast_sd(H + k2 + 84);
      _t27_6 = _mm256_broadcast_sd(H + k2 + 85);
      _t27_5 = _mm256_broadcast_sd(H + k2 + 86);
      _t27_4 = _mm256_broadcast_sd(H + k2 + 87);
      _t27_3 = _mm256_loadu_pd(Y + i0 + 28*k2);
      _t27_2 = _mm256_loadu_pd(Y + i0 + 28*k2 + 28);
      _t27_1 = _mm256_loadu_pd(Y + i0 + 28*k2 + 56);
      _t27_0 = _mm256_loadu_pd(Y + i0 + 28*k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t27_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_19, _t27_3), _mm256_mul_pd(_t27_18, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t27_17, _t27_1), _mm256_mul_pd(_t27_16, _t27_0)));
      _t27_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_15, _t27_3), _mm256_mul_pd(_t27_14, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t27_13, _t27_1), _mm256_mul_pd(_t27_12, _t27_0)));
      _t27_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_11, _t27_3), _mm256_mul_pd(_t27_10, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t27_9, _t27_1), _mm256_mul_pd(_t27_8, _t27_0)));
      _t27_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_7, _t27_3), _mm256_mul_pd(_t27_6, _t27_2)), _mm256_add_pd(_mm256_mul_pd(_t27_5, _t27_1), _mm256_mul_pd(_t27_4, _t27_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_4 = _mm256_add_pd(_t26_4, _t27_20);
      _t26_5 = _mm256_add_pd(_t26_5, _t27_21);
      _t26_6 = _mm256_add_pd(_t26_6, _t27_22);
      _t26_7 = _mm256_add_pd(_t26_7, _t27_23);

      // AVX Storer:
    }
    _t28_19 = _mm256_broadcast_sd(H + i0);
    _t28_18 = _mm256_broadcast_sd(H + i0 + 1);
    _t28_17 = _mm256_broadcast_sd(H + i0 + 2);
    _t28_16 = _mm256_broadcast_sd(H + i0 + 3);
    _t28_15 = _mm256_broadcast_sd(H + i0 + 28);
    _t28_14 = _mm256_broadcast_sd(H + i0 + 29);
    _t28_13 = _mm256_broadcast_sd(H + i0 + 30);
    _t28_12 = _mm256_broadcast_sd(H + i0 + 31);
    _t28_11 = _mm256_broadcast_sd(H + i0 + 56);
    _t28_10 = _mm256_broadcast_sd(H + i0 + 57);
    _t28_9 = _mm256_broadcast_sd(H + i0 + 58);
    _t28_8 = _mm256_broadcast_sd(H + i0 + 59);
    _t28_7 = _mm256_broadcast_sd(H + i0 + 84);
    _t28_6 = _mm256_broadcast_sd(H + i0 + 85);
    _t28_5 = _mm256_broadcast_sd(H + i0 + 86);
    _t28_4 = _mm256_broadcast_sd(H + i0 + 87);
    _t28_3 = _mm256_loadu_pd(Y + 29*i0);
    _t28_2 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t28_1 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t28_0 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t28_24 = _t28_3;
    _t28_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t28_3, _t28_2, 3), _t28_2, 12);
    _t28_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t28_3, _t28_2, 0), _t28_1, 49);
    _t28_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t28_3, _t28_2, 12), _mm256_shuffle_pd(_t28_1, _t28_0, 12), 49);

    // 4-BLAC: 4x4 * 4x4
    _t28_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_19, _t28_24), _mm256_mul_pd(_t28_18, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_17, _t28_26), _mm256_mul_pd(_t28_16, _t28_27)));
    _t28_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_15, _t28_24), _mm256_mul_pd(_t28_14, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_13, _t28_26), _mm256_mul_pd(_t28_12, _t28_27)));
    _t28_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_11, _t28_24), _mm256_mul_pd(_t28_10, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_9, _t28_26), _mm256_mul_pd(_t28_8, _t28_27)));
    _t28_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t28_7, _t28_24), _mm256_mul_pd(_t28_6, _t28_25)), _mm256_add_pd(_mm256_mul_pd(_t28_5, _t28_26), _mm256_mul_pd(_t28_4, _t28_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t26_4 = _mm256_add_pd(_t26_4, _t28_20);
    _t26_5 = _mm256_add_pd(_t26_5, _t28_21);
    _t26_6 = _mm256_add_pd(_t26_6, _t28_22);
    _t26_7 = _mm256_add_pd(_t26_7, _t28_23);

    // AVX Storer:

    for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
      _t29_19 = _mm256_broadcast_sd(H + k2);
      _t29_18 = _mm256_broadcast_sd(H + k2 + 1);
      _t29_17 = _mm256_broadcast_sd(H + k2 + 2);
      _t29_16 = _mm256_broadcast_sd(H + k2 + 3);
      _t29_15 = _mm256_broadcast_sd(H + k2 + 28);
      _t29_14 = _mm256_broadcast_sd(H + k2 + 29);
      _t29_13 = _mm256_broadcast_sd(H + k2 + 30);
      _t29_12 = _mm256_broadcast_sd(H + k2 + 31);
      _t29_11 = _mm256_broadcast_sd(H + k2 + 56);
      _t29_10 = _mm256_broadcast_sd(H + k2 + 57);
      _t29_9 = _mm256_broadcast_sd(H + k2 + 58);
      _t29_8 = _mm256_broadcast_sd(H + k2 + 59);
      _t29_7 = _mm256_broadcast_sd(H + k2 + 84);
      _t29_6 = _mm256_broadcast_sd(H + k2 + 85);
      _t29_5 = _mm256_broadcast_sd(H + k2 + 86);
      _t29_4 = _mm256_broadcast_sd(H + k2 + 87);
      _t29_3 = _mm256_loadu_pd(Y + 28*i0 + k2);
      _t29_2 = _mm256_loadu_pd(Y + 28*i0 + k2 + 28);
      _t29_1 = _mm256_loadu_pd(Y + 28*i0 + k2 + 56);
      _t29_0 = _mm256_loadu_pd(Y + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t29_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 32);
      _t29_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 32);
      _t29_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t29_3, _t29_2), _mm256_unpacklo_pd(_t29_1, _t29_0), 49);
      _t29_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t29_3, _t29_2), _mm256_unpackhi_pd(_t29_1, _t29_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t29_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_24), _mm256_mul_pd(_t29_18, _t29_25)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_26), _mm256_mul_pd(_t29_16, _t29_27)));
      _t29_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_24), _mm256_mul_pd(_t29_14, _t29_25)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_26), _mm256_mul_pd(_t29_12, _t29_27)));
      _t29_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_24), _mm256_mul_pd(_t29_10, _t29_25)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_26), _mm256_mul_pd(_t29_8, _t29_27)));
      _t29_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_24), _mm256_mul_pd(_t29_6, _t29_25)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_26), _mm256_mul_pd(_t29_4, _t29_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_4 = _mm256_add_pd(_t26_4, _t29_20);
      _t26_5 = _mm256_add_pd(_t26_5, _t29_21);
      _t26_6 = _mm256_add_pd(_t26_6, _t29_22);
      _t26_7 = _mm256_add_pd(_t26_7, _t29_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M1 + i0, _t26_4);
    _mm256_storeu_pd(M1 + i0 + 28, _t26_5);
    _mm256_storeu_pd(M1 + i0 + 56, _t26_6);
    _mm256_storeu_pd(M1 + i0 + 84, _t26_7);
  }

  _t30_3 = _mm256_loadu_pd(Y + 24);
  _t30_2 = _mm256_loadu_pd(Y + 52);
  _t30_1 = _mm256_loadu_pd(Y + 80);
  _t30_0 = _mm256_loadu_pd(Y + 108);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t30_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_19, _t30_3), _mm256_mul_pd(_t22_18, _t30_2)), _mm256_add_pd(_mm256_mul_pd(_t22_17, _t30_1), _mm256_mul_pd(_t22_16, _t30_0)));
  _t30_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_15, _t30_3), _mm256_mul_pd(_t22_14, _t30_2)), _mm256_add_pd(_mm256_mul_pd(_t22_13, _t30_1), _mm256_mul_pd(_t22_12, _t30_0)));
  _t30_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_11, _t30_3), _mm256_mul_pd(_t22_10, _t30_2)), _mm256_add_pd(_mm256_mul_pd(_t22_9, _t30_1), _mm256_mul_pd(_t22_8, _t30_0)));
  _t30_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t22_7, _t30_3), _mm256_mul_pd(_t22_6, _t30_2)), _mm256_add_pd(_mm256_mul_pd(_t22_5, _t30_1), _mm256_mul_pd(_t22_4, _t30_0)));

  // AVX Storer:


  for( int k2 = 4; k2 <= 23; k2+=4 ) {
    _t31_19 = _mm256_broadcast_sd(H + k2);
    _t31_18 = _mm256_broadcast_sd(H + k2 + 1);
    _t31_17 = _mm256_broadcast_sd(H + k2 + 2);
    _t31_16 = _mm256_broadcast_sd(H + k2 + 3);
    _t31_15 = _mm256_broadcast_sd(H + k2 + 28);
    _t31_14 = _mm256_broadcast_sd(H + k2 + 29);
    _t31_13 = _mm256_broadcast_sd(H + k2 + 30);
    _t31_12 = _mm256_broadcast_sd(H + k2 + 31);
    _t31_11 = _mm256_broadcast_sd(H + k2 + 56);
    _t31_10 = _mm256_broadcast_sd(H + k2 + 57);
    _t31_9 = _mm256_broadcast_sd(H + k2 + 58);
    _t31_8 = _mm256_broadcast_sd(H + k2 + 59);
    _t31_7 = _mm256_broadcast_sd(H + k2 + 84);
    _t31_6 = _mm256_broadcast_sd(H + k2 + 85);
    _t31_5 = _mm256_broadcast_sd(H + k2 + 86);
    _t31_4 = _mm256_broadcast_sd(H + k2 + 87);
    _t31_3 = _mm256_loadu_pd(Y + 28*k2 + 24);
    _t31_2 = _mm256_loadu_pd(Y + 28*k2 + 52);
    _t31_1 = _mm256_loadu_pd(Y + 28*k2 + 80);
    _t31_0 = _mm256_loadu_pd(Y + 28*k2 + 108);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t31_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t31_3), _mm256_mul_pd(_t31_18, _t31_2)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t31_1), _mm256_mul_pd(_t31_16, _t31_0)));
    _t31_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t31_3), _mm256_mul_pd(_t31_14, _t31_2)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t31_1), _mm256_mul_pd(_t31_12, _t31_0)));
    _t31_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t31_3), _mm256_mul_pd(_t31_10, _t31_2)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t31_1), _mm256_mul_pd(_t31_8, _t31_0)));
    _t31_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t31_3), _mm256_mul_pd(_t31_6, _t31_2)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t31_1), _mm256_mul_pd(_t31_4, _t31_0)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t30_4 = _mm256_add_pd(_t30_4, _t31_20);
    _t30_5 = _mm256_add_pd(_t30_5, _t31_21);
    _t30_6 = _mm256_add_pd(_t30_6, _t31_22);
    _t30_7 = _mm256_add_pd(_t30_7, _t31_23);

    // AVX Storer:
  }

  _t32_15 = _mm256_broadcast_sd(H + 24);
  _t32_14 = _mm256_broadcast_sd(H + 25);
  _t32_13 = _mm256_broadcast_sd(H + 26);
  _t32_12 = _mm256_broadcast_sd(H + 27);
  _t32_11 = _mm256_broadcast_sd(H + 52);
  _t32_10 = _mm256_broadcast_sd(H + 53);
  _t32_9 = _mm256_broadcast_sd(H + 54);
  _t32_8 = _mm256_broadcast_sd(H + 55);
  _t32_7 = _mm256_broadcast_sd(H + 80);
  _t32_6 = _mm256_broadcast_sd(H + 81);
  _t32_5 = _mm256_broadcast_sd(H + 82);
  _t32_4 = _mm256_broadcast_sd(H + 83);
  _t32_3 = _mm256_broadcast_sd(H + 108);
  _t32_2 = _mm256_broadcast_sd(H + 109);
  _t32_1 = _mm256_broadcast_sd(H + 110);
  _t32_0 = _mm256_broadcast_sd(H + 111);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t32_24 = _t18_28;
  _t32_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 3), _t18_29, 12);
  _t32_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 0), _t18_30, 49);
  _t32_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 12), _mm256_shuffle_pd(_t18_30, _t18_31, 12), 49);

  // 4-BLAC: 4x4 * 4x4
  _t32_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_15, _t32_24), _mm256_mul_pd(_t32_14, _t32_25)), _mm256_add_pd(_mm256_mul_pd(_t32_13, _t32_26), _mm256_mul_pd(_t32_12, _t32_27)));
  _t32_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_11, _t32_24), _mm256_mul_pd(_t32_10, _t32_25)), _mm256_add_pd(_mm256_mul_pd(_t32_9, _t32_26), _mm256_mul_pd(_t32_8, _t32_27)));
  _t32_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_7, _t32_24), _mm256_mul_pd(_t32_6, _t32_25)), _mm256_add_pd(_mm256_mul_pd(_t32_5, _t32_26), _mm256_mul_pd(_t32_4, _t32_27)));
  _t32_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t32_3, _t32_24), _mm256_mul_pd(_t32_2, _t32_25)), _mm256_add_pd(_mm256_mul_pd(_t32_1, _t32_26), _mm256_mul_pd(_t32_0, _t32_27)));

  // AVX Loader:

  // 4-BLAC: 4x4 + 4x4
  _t30_4 = _mm256_add_pd(_t30_4, _t32_16);
  _t30_5 = _mm256_add_pd(_t30_5, _t32_17);
  _t30_6 = _mm256_add_pd(_t30_6, _t32_18);
  _t30_7 = _mm256_add_pd(_t30_7, _t32_19);

  // AVX Storer:

  // Generating : M2[28,4] = ( ( ( ( ( ( ( ( S(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) ) ),h(4, 4, 0)) + Sum_{k2} ( $(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) ) ),h(4, 4, 0)) ) ) + S(h(4, 28, 4), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) ) ),h(4, 4, 0)) ) + $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 4)) ) ),h(4, 4, 0)) ) + Sum_{k2} ( $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) ) ),h(4, 4, 0)) ) ) + Sum_{i0} ( ( ( ( S(h(4, 28, i0), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) ) ),h(4, 4, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) ) ),h(4, 4, 0)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, i0)) ) ),h(4, 4, 0)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) ) ),h(4, 4, 0)) ) ) ) ) + S(h(4, 28, 24), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) ) ),h(4, 4, 0)) ) + Sum_{k2} ( $(h(4, 28, 24), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 4, 0), H[4,28],h(4, 28, k2)) ) ),h(4, 4, 0)) ) ) + $(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 24)) ) ),h(4, 4, 0)) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t32_28 = _t22_3;
  _t32_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 3), _t22_2, 12);
  _t32_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 0), _t22_1, 49);
  _t32_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t22_3, _t22_2, 12), _mm256_shuffle_pd(_t22_1, _t22_0, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t32_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 32);
  _t32_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 32);
  _t32_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 49);
  _t32_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 49);

  // 4-BLAC: 4x4 * 4x4
  _t32_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_28, _t32_28, 32), _mm256_permute2f128_pd(_t32_28, _t32_28, 32), 0), _t32_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_28, _t32_28, 32), _mm256_permute2f128_pd(_t32_28, _t32_28, 32), 15), _t32_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_28, _t32_28, 49), _mm256_permute2f128_pd(_t32_28, _t32_28, 49), 0), _t32_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_28, _t32_28, 49), _mm256_permute2f128_pd(_t32_28, _t32_28, 49), 15), _t32_35)));
  _t32_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_29, _t32_29, 32), _mm256_permute2f128_pd(_t32_29, _t32_29, 32), 0), _t32_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_29, _t32_29, 32), _mm256_permute2f128_pd(_t32_29, _t32_29, 32), 15), _t32_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_29, _t32_29, 49), _mm256_permute2f128_pd(_t32_29, _t32_29, 49), 0), _t32_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_29, _t32_29, 49), _mm256_permute2f128_pd(_t32_29, _t32_29, 49), 15), _t32_35)));
  _t32_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_30, _t32_30, 32), _mm256_permute2f128_pd(_t32_30, _t32_30, 32), 0), _t32_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_30, _t32_30, 32), _mm256_permute2f128_pd(_t32_30, _t32_30, 32), 15), _t32_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_30, _t32_30, 49), _mm256_permute2f128_pd(_t32_30, _t32_30, 49), 0), _t32_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_30, _t32_30, 49), _mm256_permute2f128_pd(_t32_30, _t32_30, 49), 15), _t32_35)));
  _t32_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_31, _t32_31, 32), _mm256_permute2f128_pd(_t32_31, _t32_31, 32), 0), _t32_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_31, _t32_31, 32), _mm256_permute2f128_pd(_t32_31, _t32_31, 32), 15), _t32_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_31, _t32_31, 49), _mm256_permute2f128_pd(_t32_31, _t32_31, 49), 0), _t32_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t32_31, _t32_31, 49), _mm256_permute2f128_pd(_t32_31, _t32_31, 49), 15), _t32_35)));

  // AVX Storer:


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t33_19 = _mm256_broadcast_sd(Y + k2);
    _t33_18 = _mm256_broadcast_sd(Y + k2 + 1);
    _t33_17 = _mm256_broadcast_sd(Y + k2 + 2);
    _t33_16 = _mm256_broadcast_sd(Y + k2 + 3);
    _t33_15 = _mm256_broadcast_sd(Y + k2 + 28);
    _t33_14 = _mm256_broadcast_sd(Y + k2 + 29);
    _t33_13 = _mm256_broadcast_sd(Y + k2 + 30);
    _t33_12 = _mm256_broadcast_sd(Y + k2 + 31);
    _t33_11 = _mm256_broadcast_sd(Y + k2 + 56);
    _t33_10 = _mm256_broadcast_sd(Y + k2 + 57);
    _t33_9 = _mm256_broadcast_sd(Y + k2 + 58);
    _t33_8 = _mm256_broadcast_sd(Y + k2 + 59);
    _t33_7 = _mm256_broadcast_sd(Y + k2 + 84);
    _t33_6 = _mm256_broadcast_sd(Y + k2 + 85);
    _t33_5 = _mm256_broadcast_sd(Y + k2 + 86);
    _t33_4 = _mm256_broadcast_sd(Y + k2 + 87);
    _t33_3 = _mm256_loadu_pd(H + k2);
    _t33_2 = _mm256_loadu_pd(H + k2 + 28);
    _t33_1 = _mm256_loadu_pd(H + k2 + 56);
    _t33_0 = _mm256_loadu_pd(H + k2 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t33_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 32);
    _t33_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 32);
    _t33_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t33_3, _t33_2), _mm256_unpacklo_pd(_t33_1, _t33_0), 49);
    _t33_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t33_3, _t33_2), _mm256_unpackhi_pd(_t33_1, _t33_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t33_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_19, _t33_24), _mm256_mul_pd(_t33_18, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_17, _t33_26), _mm256_mul_pd(_t33_16, _t33_27)));
    _t33_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_15, _t33_24), _mm256_mul_pd(_t33_14, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_13, _t33_26), _mm256_mul_pd(_t33_12, _t33_27)));
    _t33_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_11, _t33_24), _mm256_mul_pd(_t33_10, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_9, _t33_26), _mm256_mul_pd(_t33_8, _t33_27)));
    _t33_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_7, _t33_24), _mm256_mul_pd(_t33_6, _t33_25)), _mm256_add_pd(_mm256_mul_pd(_t33_5, _t33_26), _mm256_mul_pd(_t33_4, _t33_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t32_20 = _mm256_add_pd(_t32_20, _t33_20);
    _t32_21 = _mm256_add_pd(_t32_21, _t33_21);
    _t32_22 = _mm256_add_pd(_t32_22, _t33_22);
    _t32_23 = _mm256_add_pd(_t32_23, _t33_23);

    // AVX Storer:
  }

  _t34_3 = _mm256_loadu_pd(H + 4);
  _t34_2 = _mm256_loadu_pd(H + 32);
  _t34_1 = _mm256_loadu_pd(H + 60);
  _t34_0 = _mm256_loadu_pd(H + 88);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t34_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_23, _t24_22), _mm256_unpacklo_pd(_t24_21, _t24_20), 32);
  _t34_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t24_23, _t24_22), _mm256_unpackhi_pd(_t24_21, _t24_20), 32);
  _t34_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t24_23, _t24_22), _mm256_unpacklo_pd(_t24_21, _t24_20), 49);
  _t34_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t24_23, _t24_22), _mm256_unpackhi_pd(_t24_21, _t24_20), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t34_20 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 32);
  _t34_21 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 32);
  _t34_22 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 49);
  _t34_23 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 49);

  // 4-BLAC: 4x4 * 4x4
  _t34_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_16, _t34_16, 32), _mm256_permute2f128_pd(_t34_16, _t34_16, 32), 0), _t34_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_16, _t34_16, 32), _mm256_permute2f128_pd(_t34_16, _t34_16, 32), 15), _t34_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_16, _t34_16, 49), _mm256_permute2f128_pd(_t34_16, _t34_16, 49), 0), _t34_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_16, _t34_16, 49), _mm256_permute2f128_pd(_t34_16, _t34_16, 49), 15), _t34_23)));
  _t34_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_17, _t34_17, 32), _mm256_permute2f128_pd(_t34_17, _t34_17, 32), 0), _t34_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_17, _t34_17, 32), _mm256_permute2f128_pd(_t34_17, _t34_17, 32), 15), _t34_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_17, _t34_17, 49), _mm256_permute2f128_pd(_t34_17, _t34_17, 49), 0), _t34_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_17, _t34_17, 49), _mm256_permute2f128_pd(_t34_17, _t34_17, 49), 15), _t34_23)));
  _t34_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_18, _t34_18, 32), _mm256_permute2f128_pd(_t34_18, _t34_18, 32), 0), _t34_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_18, _t34_18, 32), _mm256_permute2f128_pd(_t34_18, _t34_18, 32), 15), _t34_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_18, _t34_18, 49), _mm256_permute2f128_pd(_t34_18, _t34_18, 49), 0), _t34_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_18, _t34_18, 49), _mm256_permute2f128_pd(_t34_18, _t34_18, 49), 15), _t34_23)));
  _t34_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_19, _t34_19, 32), _mm256_permute2f128_pd(_t34_19, _t34_19, 32), 0), _t34_20), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_19, _t34_19, 32), _mm256_permute2f128_pd(_t34_19, _t34_19, 32), 15), _t34_21)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_19, _t34_19, 49), _mm256_permute2f128_pd(_t34_19, _t34_19, 49), 0), _t34_22), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_19, _t34_19, 49), _mm256_permute2f128_pd(_t34_19, _t34_19, 49), 15), _t34_23)));

  // AVX Storer:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t34_12 = _t24_3;
  _t34_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 3), _t24_2, 12);
  _t34_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 0), _t24_1, 49);
  _t34_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t24_3, _t24_2, 12), _mm256_shuffle_pd(_t24_1, _t24_0, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t34_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 32);
  _t34_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 32);
  _t34_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t34_3, _t34_2), _mm256_unpacklo_pd(_t34_1, _t34_0), 49);
  _t34_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t34_3, _t34_2), _mm256_unpackhi_pd(_t34_1, _t34_0), 49);

  // 4-BLAC: 4x4 * 4x4
  _t34_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_12, _t34_12, 32), _mm256_permute2f128_pd(_t34_12, _t34_12, 32), 0), _t34_24), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_12, _t34_12, 32), _mm256_permute2f128_pd(_t34_12, _t34_12, 32), 15), _t34_25)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_12, _t34_12, 49), _mm256_permute2f128_pd(_t34_12, _t34_12, 49), 0), _t34_26), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_12, _t34_12, 49), _mm256_permute2f128_pd(_t34_12, _t34_12, 49), 15), _t34_27)));
  _t34_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_13, _t34_13, 32), _mm256_permute2f128_pd(_t34_13, _t34_13, 32), 0), _t34_24), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_13, _t34_13, 32), _mm256_permute2f128_pd(_t34_13, _t34_13, 32), 15), _t34_25)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_13, _t34_13, 49), _mm256_permute2f128_pd(_t34_13, _t34_13, 49), 0), _t34_26), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_13, _t34_13, 49), _mm256_permute2f128_pd(_t34_13, _t34_13, 49), 15), _t34_27)));
  _t34_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_14, _t34_14, 32), _mm256_permute2f128_pd(_t34_14, _t34_14, 32), 0), _t34_24), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_14, _t34_14, 32), _mm256_permute2f128_pd(_t34_14, _t34_14, 32), 15), _t34_25)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_14, _t34_14, 49), _mm256_permute2f128_pd(_t34_14, _t34_14, 49), 0), _t34_26), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_14, _t34_14, 49), _mm256_permute2f128_pd(_t34_14, _t34_14, 49), 15), _t34_27)));
  _t34_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_15, _t34_15, 32), _mm256_permute2f128_pd(_t34_15, _t34_15, 32), 0), _t34_24), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_15, _t34_15, 32), _mm256_permute2f128_pd(_t34_15, _t34_15, 32), 15), _t34_25)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_15, _t34_15, 49), _mm256_permute2f128_pd(_t34_15, _t34_15, 49), 0), _t34_26), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t34_15, _t34_15, 49), _mm256_permute2f128_pd(_t34_15, _t34_15, 49), 15), _t34_27)));

  // AVX Loader:

  // 4-BLAC: 4x4 + 4x4
  _t34_4 = _mm256_add_pd(_t34_4, _t34_8);
  _t34_5 = _mm256_add_pd(_t34_5, _t34_9);
  _t34_6 = _mm256_add_pd(_t34_6, _t34_10);
  _t34_7 = _mm256_add_pd(_t34_7, _t34_11);

  // AVX Storer:


  for( int k2 = 8; k2 <= 27; k2+=4 ) {
    _t35_19 = _mm256_broadcast_sd(Y + k2 + 112);
    _t35_18 = _mm256_broadcast_sd(Y + k2 + 113);
    _t35_17 = _mm256_broadcast_sd(Y + k2 + 114);
    _t35_16 = _mm256_broadcast_sd(Y + k2 + 115);
    _t35_15 = _mm256_broadcast_sd(Y + k2 + 140);
    _t35_14 = _mm256_broadcast_sd(Y + k2 + 141);
    _t35_13 = _mm256_broadcast_sd(Y + k2 + 142);
    _t35_12 = _mm256_broadcast_sd(Y + k2 + 143);
    _t35_11 = _mm256_broadcast_sd(Y + k2 + 168);
    _t35_10 = _mm256_broadcast_sd(Y + k2 + 169);
    _t35_9 = _mm256_broadcast_sd(Y + k2 + 170);
    _t35_8 = _mm256_broadcast_sd(Y + k2 + 171);
    _t35_7 = _mm256_broadcast_sd(Y + k2 + 196);
    _t35_6 = _mm256_broadcast_sd(Y + k2 + 197);
    _t35_5 = _mm256_broadcast_sd(Y + k2 + 198);
    _t35_4 = _mm256_broadcast_sd(Y + k2 + 199);
    _t35_3 = _mm256_loadu_pd(H + k2);
    _t35_2 = _mm256_loadu_pd(H + k2 + 28);
    _t35_1 = _mm256_loadu_pd(H + k2 + 56);
    _t35_0 = _mm256_loadu_pd(H + k2 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t35_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 32);
    _t35_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 32);
    _t35_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t35_3, _t35_2), _mm256_unpacklo_pd(_t35_1, _t35_0), 49);
    _t35_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t35_3, _t35_2), _mm256_unpackhi_pd(_t35_1, _t35_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t35_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_19, _t35_24), _mm256_mul_pd(_t35_18, _t35_25)), _mm256_add_pd(_mm256_mul_pd(_t35_17, _t35_26), _mm256_mul_pd(_t35_16, _t35_27)));
    _t35_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_15, _t35_24), _mm256_mul_pd(_t35_14, _t35_25)), _mm256_add_pd(_mm256_mul_pd(_t35_13, _t35_26), _mm256_mul_pd(_t35_12, _t35_27)));
    _t35_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_11, _t35_24), _mm256_mul_pd(_t35_10, _t35_25)), _mm256_add_pd(_mm256_mul_pd(_t35_9, _t35_26), _mm256_mul_pd(_t35_8, _t35_27)));
    _t35_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t35_7, _t35_24), _mm256_mul_pd(_t35_6, _t35_25)), _mm256_add_pd(_mm256_mul_pd(_t35_5, _t35_26), _mm256_mul_pd(_t35_4, _t35_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t34_4 = _mm256_add_pd(_t34_4, _t35_20);
    _t34_5 = _mm256_add_pd(_t34_5, _t35_21);
    _t34_6 = _mm256_add_pd(_t34_6, _t35_22);
    _t34_7 = _mm256_add_pd(_t34_7, _t35_23);

    // AVX Storer:
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t36_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 32);
  _t36_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 32);
  _t36_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 49);
  _t36_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 49);


  for( int i0 = 8; i0 <= 23; i0+=4 ) {
    _t37_3 = _mm256_loadu_pd(Y + i0);
    _t37_2 = _mm256_loadu_pd(Y + i0 + 28);
    _t37_1 = _mm256_loadu_pd(Y + i0 + 56);
    _t37_0 = _mm256_loadu_pd(Y + i0 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t37_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
    _t37_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
    _t37_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
    _t37_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t37_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_8, _t37_8, 32), _mm256_permute2f128_pd(_t37_8, _t37_8, 32), 0), _t36_0), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_8, _t37_8, 32), _mm256_permute2f128_pd(_t37_8, _t37_8, 32), 15), _t36_1)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_8, _t37_8, 49), _mm256_permute2f128_pd(_t37_8, _t37_8, 49), 0), _t36_2), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_8, _t37_8, 49), _mm256_permute2f128_pd(_t37_8, _t37_8, 49), 15), _t36_3)));
    _t37_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_9, _t37_9, 32), _mm256_permute2f128_pd(_t37_9, _t37_9, 32), 0), _t36_0), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_9, _t37_9, 32), _mm256_permute2f128_pd(_t37_9, _t37_9, 32), 15), _t36_1)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_9, _t37_9, 49), _mm256_permute2f128_pd(_t37_9, _t37_9, 49), 0), _t36_2), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_9, _t37_9, 49), _mm256_permute2f128_pd(_t37_9, _t37_9, 49), 15), _t36_3)));
    _t37_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_10, _t37_10, 32), _mm256_permute2f128_pd(_t37_10, _t37_10, 32), 0), _t36_0), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_10, _t37_10, 32), _mm256_permute2f128_pd(_t37_10, _t37_10, 32), 15), _t36_1)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_10, _t37_10, 49), _mm256_permute2f128_pd(_t37_10, _t37_10, 49), 0), _t36_2), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_10, _t37_10, 49), _mm256_permute2f128_pd(_t37_10, _t37_10, 49), 15), _t36_3)));
    _t37_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_11, _t37_11, 32), _mm256_permute2f128_pd(_t37_11, _t37_11, 32), 0), _t36_0), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_11, _t37_11, 32), _mm256_permute2f128_pd(_t37_11, _t37_11, 32), 15), _t36_1)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_11, _t37_11, 49), _mm256_permute2f128_pd(_t37_11, _t37_11, 49), 0), _t36_2), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t37_11, _t37_11, 49), _mm256_permute2f128_pd(_t37_11, _t37_11, 49), 15), _t36_3)));

    // AVX Storer:

    for( int k2 = 4; k2 <= i0 - 1; k2+=4 ) {
      _t38_7 = _mm256_loadu_pd(Y + i0 + 28*k2);
      _t38_6 = _mm256_loadu_pd(Y + i0 + 28*k2 + 28);
      _t38_5 = _mm256_loadu_pd(Y + i0 + 28*k2 + 56);
      _t38_4 = _mm256_loadu_pd(Y + i0 + 28*k2 + 84);
      _t38_3 = _mm256_loadu_pd(H + k2);
      _t38_2 = _mm256_loadu_pd(H + k2 + 28);
      _t38_1 = _mm256_loadu_pd(H + k2 + 56);
      _t38_0 = _mm256_loadu_pd(H + k2 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t38_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 32);
      _t38_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 32);
      _t38_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_7, _t38_6), _mm256_unpacklo_pd(_t38_5, _t38_4), 49);
      _t38_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_7, _t38_6), _mm256_unpackhi_pd(_t38_5, _t38_4), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t38_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
      _t38_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
      _t38_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
      _t38_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t38_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_12, _t38_12, 32), _mm256_permute2f128_pd(_t38_12, _t38_12, 32), 0), _t38_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_12, _t38_12, 32), _mm256_permute2f128_pd(_t38_12, _t38_12, 32), 15), _t38_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_12, _t38_12, 49), _mm256_permute2f128_pd(_t38_12, _t38_12, 49), 0), _t38_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_12, _t38_12, 49), _mm256_permute2f128_pd(_t38_12, _t38_12, 49), 15), _t38_19)));
      _t38_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_13, _t38_13, 32), _mm256_permute2f128_pd(_t38_13, _t38_13, 32), 0), _t38_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_13, _t38_13, 32), _mm256_permute2f128_pd(_t38_13, _t38_13, 32), 15), _t38_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_13, _t38_13, 49), _mm256_permute2f128_pd(_t38_13, _t38_13, 49), 0), _t38_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_13, _t38_13, 49), _mm256_permute2f128_pd(_t38_13, _t38_13, 49), 15), _t38_19)));
      _t38_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_14, _t38_14, 32), _mm256_permute2f128_pd(_t38_14, _t38_14, 32), 0), _t38_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_14, _t38_14, 32), _mm256_permute2f128_pd(_t38_14, _t38_14, 32), 15), _t38_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_14, _t38_14, 49), _mm256_permute2f128_pd(_t38_14, _t38_14, 49), 0), _t38_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_14, _t38_14, 49), _mm256_permute2f128_pd(_t38_14, _t38_14, 49), 15), _t38_19)));
      _t38_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_15, _t38_15, 32), _mm256_permute2f128_pd(_t38_15, _t38_15, 32), 0), _t38_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_15, _t38_15, 32), _mm256_permute2f128_pd(_t38_15, _t38_15, 32), 15), _t38_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_15, _t38_15, 49), _mm256_permute2f128_pd(_t38_15, _t38_15, 49), 0), _t38_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t38_15, _t38_15, 49), _mm256_permute2f128_pd(_t38_15, _t38_15, 49), 15), _t38_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t37_4 = _mm256_add_pd(_t37_4, _t38_8);
      _t37_5 = _mm256_add_pd(_t37_5, _t38_9);
      _t37_6 = _mm256_add_pd(_t37_6, _t38_10);
      _t37_7 = _mm256_add_pd(_t37_7, _t38_11);

      // AVX Storer:
    }
    _t39_7 = _mm256_loadu_pd(Y + 29*i0);
    _t39_6 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t39_5 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t39_4 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t39_3 = _mm256_loadu_pd(H + i0);
    _t39_2 = _mm256_loadu_pd(H + i0 + 28);
    _t39_1 = _mm256_loadu_pd(H + i0 + 56);
    _t39_0 = _mm256_loadu_pd(H + i0 + 84);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t39_12 = _t39_7;
    _t39_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t39_7, _t39_6, 3), _t39_6, 12);
    _t39_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t39_7, _t39_6, 0), _t39_5, 49);
    _t39_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t39_7, _t39_6, 12), _mm256_shuffle_pd(_t39_5, _t39_4, 12), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 32);
    _t39_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 32);
    _t39_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_3, _t39_2), _mm256_unpacklo_pd(_t39_1, _t39_0), 49);
    _t39_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_3, _t39_2), _mm256_unpackhi_pd(_t39_1, _t39_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_12, _t39_12, 32), _mm256_permute2f128_pd(_t39_12, _t39_12, 32), 0), _t39_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_12, _t39_12, 32), _mm256_permute2f128_pd(_t39_12, _t39_12, 32), 15), _t39_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_12, _t39_12, 49), _mm256_permute2f128_pd(_t39_12, _t39_12, 49), 0), _t39_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_12, _t39_12, 49), _mm256_permute2f128_pd(_t39_12, _t39_12, 49), 15), _t39_19)));
    _t39_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_13, _t39_13, 32), _mm256_permute2f128_pd(_t39_13, _t39_13, 32), 0), _t39_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_13, _t39_13, 32), _mm256_permute2f128_pd(_t39_13, _t39_13, 32), 15), _t39_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_13, _t39_13, 49), _mm256_permute2f128_pd(_t39_13, _t39_13, 49), 0), _t39_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_13, _t39_13, 49), _mm256_permute2f128_pd(_t39_13, _t39_13, 49), 15), _t39_19)));
    _t39_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_14, _t39_14, 32), _mm256_permute2f128_pd(_t39_14, _t39_14, 32), 0), _t39_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_14, _t39_14, 32), _mm256_permute2f128_pd(_t39_14, _t39_14, 32), 15), _t39_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_14, _t39_14, 49), _mm256_permute2f128_pd(_t39_14, _t39_14, 49), 0), _t39_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_14, _t39_14, 49), _mm256_permute2f128_pd(_t39_14, _t39_14, 49), 15), _t39_19)));
    _t39_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_15, _t39_15, 32), _mm256_permute2f128_pd(_t39_15, _t39_15, 32), 0), _t39_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_15, _t39_15, 32), _mm256_permute2f128_pd(_t39_15, _t39_15, 32), 15), _t39_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_15, _t39_15, 49), _mm256_permute2f128_pd(_t39_15, _t39_15, 49), 0), _t39_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t39_15, _t39_15, 49), _mm256_permute2f128_pd(_t39_15, _t39_15, 49), 15), _t39_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t37_4 = _mm256_add_pd(_t37_4, _t39_8);
    _t37_5 = _mm256_add_pd(_t37_5, _t39_9);
    _t37_6 = _mm256_add_pd(_t37_6, _t39_10);
    _t37_7 = _mm256_add_pd(_t37_7, _t39_11);

    // AVX Storer:

    for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
      _t40_19 = _mm256_broadcast_sd(Y + 28*i0 + k2);
      _t40_18 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 1);
      _t40_17 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 2);
      _t40_16 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 3);
      _t40_15 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 28);
      _t40_14 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 29);
      _t40_13 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 30);
      _t40_12 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 31);
      _t40_11 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 56);
      _t40_10 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 57);
      _t40_9 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 58);
      _t40_8 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 59);
      _t40_7 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 84);
      _t40_6 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 85);
      _t40_5 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 86);
      _t40_4 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 87);
      _t40_3 = _mm256_loadu_pd(H + k2);
      _t40_2 = _mm256_loadu_pd(H + k2 + 28);
      _t40_1 = _mm256_loadu_pd(H + k2 + 56);
      _t40_0 = _mm256_loadu_pd(H + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t40_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 32);
      _t40_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 32);
      _t40_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 49);
      _t40_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t40_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_19, _t40_24), _mm256_mul_pd(_t40_18, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_17, _t40_26), _mm256_mul_pd(_t40_16, _t40_27)));
      _t40_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_15, _t40_24), _mm256_mul_pd(_t40_14, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_13, _t40_26), _mm256_mul_pd(_t40_12, _t40_27)));
      _t40_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_11, _t40_24), _mm256_mul_pd(_t40_10, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_9, _t40_26), _mm256_mul_pd(_t40_8, _t40_27)));
      _t40_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_7, _t40_24), _mm256_mul_pd(_t40_6, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_5, _t40_26), _mm256_mul_pd(_t40_4, _t40_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t37_4 = _mm256_add_pd(_t37_4, _t40_20);
      _t37_5 = _mm256_add_pd(_t37_5, _t40_21);
      _t37_6 = _mm256_add_pd(_t37_6, _t40_22);
      _t37_7 = _mm256_add_pd(_t37_7, _t40_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + 4*i0, _t37_4);
    _mm256_storeu_pd(M2 + 4*i0 + 4, _t37_5);
    _mm256_storeu_pd(M2 + 4*i0 + 8, _t37_6);
    _mm256_storeu_pd(M2 + 4*i0 + 12, _t37_7);
  }


  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t41_4 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_3, _t30_2), _mm256_unpacklo_pd(_t30_1, _t30_0), 32);
  _t41_5 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t30_3, _t30_2), _mm256_unpackhi_pd(_t30_1, _t30_0), 32);
  _t41_6 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t30_3, _t30_2), _mm256_unpacklo_pd(_t30_1, _t30_0), 49);
  _t41_7 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t30_3, _t30_2), _mm256_unpackhi_pd(_t30_1, _t30_0), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t41_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 32);
  _t41_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 32);
  _t41_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 49);
  _t41_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 49);

  // 4-BLAC: 4x4 * 4x4
  _t41_0 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_4, _t41_4, 32), _mm256_permute2f128_pd(_t41_4, _t41_4, 32), 0), _t41_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_4, _t41_4, 32), _mm256_permute2f128_pd(_t41_4, _t41_4, 32), 15), _t41_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_4, _t41_4, 49), _mm256_permute2f128_pd(_t41_4, _t41_4, 49), 0), _t41_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_4, _t41_4, 49), _mm256_permute2f128_pd(_t41_4, _t41_4, 49), 15), _t41_11)));
  _t41_1 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_5, _t41_5, 32), _mm256_permute2f128_pd(_t41_5, _t41_5, 32), 0), _t41_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_5, _t41_5, 32), _mm256_permute2f128_pd(_t41_5, _t41_5, 32), 15), _t41_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_5, _t41_5, 49), _mm256_permute2f128_pd(_t41_5, _t41_5, 49), 0), _t41_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_5, _t41_5, 49), _mm256_permute2f128_pd(_t41_5, _t41_5, 49), 15), _t41_11)));
  _t41_2 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_6, _t41_6, 32), _mm256_permute2f128_pd(_t41_6, _t41_6, 32), 0), _t41_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_6, _t41_6, 32), _mm256_permute2f128_pd(_t41_6, _t41_6, 32), 15), _t41_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_6, _t41_6, 49), _mm256_permute2f128_pd(_t41_6, _t41_6, 49), 0), _t41_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_6, _t41_6, 49), _mm256_permute2f128_pd(_t41_6, _t41_6, 49), 15), _t41_11)));
  _t41_3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_7, _t41_7, 32), _mm256_permute2f128_pd(_t41_7, _t41_7, 32), 0), _t41_8), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_7, _t41_7, 32), _mm256_permute2f128_pd(_t41_7, _t41_7, 32), 15), _t41_9)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_7, _t41_7, 49), _mm256_permute2f128_pd(_t41_7, _t41_7, 49), 0), _t41_10), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_7, _t41_7, 49), _mm256_permute2f128_pd(_t41_7, _t41_7, 49), 15), _t41_11)));

  // AVX Storer:


  for( int k2 = 4; k2 <= 23; k2+=4 ) {
    _t42_7 = _mm256_loadu_pd(Y + 28*k2 + 24);
    _t42_6 = _mm256_loadu_pd(Y + 28*k2 + 52);
    _t42_5 = _mm256_loadu_pd(Y + 28*k2 + 80);
    _t42_4 = _mm256_loadu_pd(Y + 28*k2 + 108);
    _t42_3 = _mm256_loadu_pd(H + k2);
    _t42_2 = _mm256_loadu_pd(H + k2 + 28);
    _t42_1 = _mm256_loadu_pd(H + k2 + 56);
    _t42_0 = _mm256_loadu_pd(H + k2 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t42_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 32);
    _t42_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 32);
    _t42_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_7, _t42_6), _mm256_unpacklo_pd(_t42_5, _t42_4), 49);
    _t42_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_7, _t42_6), _mm256_unpackhi_pd(_t42_5, _t42_4), 49);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t42_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_3, _t42_2), _mm256_unpacklo_pd(_t42_1, _t42_0), 32);
    _t42_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_3, _t42_2), _mm256_unpackhi_pd(_t42_1, _t42_0), 32);
    _t42_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_3, _t42_2), _mm256_unpacklo_pd(_t42_1, _t42_0), 49);
    _t42_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_3, _t42_2), _mm256_unpackhi_pd(_t42_1, _t42_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t42_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_12, _t42_12, 32), _mm256_permute2f128_pd(_t42_12, _t42_12, 32), 0), _t42_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_12, _t42_12, 32), _mm256_permute2f128_pd(_t42_12, _t42_12, 32), 15), _t42_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_12, _t42_12, 49), _mm256_permute2f128_pd(_t42_12, _t42_12, 49), 0), _t42_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_12, _t42_12, 49), _mm256_permute2f128_pd(_t42_12, _t42_12, 49), 15), _t42_19)));
    _t42_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_13, _t42_13, 32), _mm256_permute2f128_pd(_t42_13, _t42_13, 32), 0), _t42_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_13, _t42_13, 32), _mm256_permute2f128_pd(_t42_13, _t42_13, 32), 15), _t42_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_13, _t42_13, 49), _mm256_permute2f128_pd(_t42_13, _t42_13, 49), 0), _t42_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_13, _t42_13, 49), _mm256_permute2f128_pd(_t42_13, _t42_13, 49), 15), _t42_19)));
    _t42_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_14, _t42_14, 32), _mm256_permute2f128_pd(_t42_14, _t42_14, 32), 0), _t42_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_14, _t42_14, 32), _mm256_permute2f128_pd(_t42_14, _t42_14, 32), 15), _t42_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_14, _t42_14, 49), _mm256_permute2f128_pd(_t42_14, _t42_14, 49), 0), _t42_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_14, _t42_14, 49), _mm256_permute2f128_pd(_t42_14, _t42_14, 49), 15), _t42_19)));
    _t42_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_15, _t42_15, 32), _mm256_permute2f128_pd(_t42_15, _t42_15, 32), 0), _t42_16), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_15, _t42_15, 32), _mm256_permute2f128_pd(_t42_15, _t42_15, 32), 15), _t42_17)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_15, _t42_15, 49), _mm256_permute2f128_pd(_t42_15, _t42_15, 49), 0), _t42_18), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t42_15, _t42_15, 49), _mm256_permute2f128_pd(_t42_15, _t42_15, 49), 15), _t42_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t41_0 = _mm256_add_pd(_t41_0, _t42_8);
    _t41_1 = _mm256_add_pd(_t41_1, _t42_9);
    _t41_2 = _mm256_add_pd(_t41_2, _t42_10);
    _t41_3 = _mm256_add_pd(_t41_3, _t42_11);

    // AVX Storer:
  }

  _t43_7 = _mm256_loadu_pd(H + 24);
  _t43_6 = _mm256_loadu_pd(H + 52);
  _t43_5 = _mm256_loadu_pd(H + 80);
  _t43_4 = _mm256_loadu_pd(H + 108);
  _t43_3 = _mm256_loadu_pd(R);
  _t43_2 = _mm256_maskload_pd(R + 4, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t43_1 = _mm256_maskload_pd(R + 8, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t43_0 = _mm256_maskload_pd(R + 12, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t43_24 = _t18_28;
  _t43_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 3), _t18_29, 12);
  _t43_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 0), _t18_30, 49);
  _t43_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 12), _mm256_shuffle_pd(_t18_30, _t18_31, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t43_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 32);
  _t43_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 32);
  _t43_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_7, _t43_6), _mm256_unpacklo_pd(_t43_5, _t43_4), 49);
  _t43_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_7, _t43_6), _mm256_unpackhi_pd(_t43_5, _t43_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t43_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_24, _t43_24, 32), _mm256_permute2f128_pd(_t43_24, _t43_24, 32), 0), _t43_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_24, _t43_24, 32), _mm256_permute2f128_pd(_t43_24, _t43_24, 32), 15), _t43_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_24, _t43_24, 49), _mm256_permute2f128_pd(_t43_24, _t43_24, 49), 0), _t43_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_24, _t43_24, 49), _mm256_permute2f128_pd(_t43_24, _t43_24, 49), 15), _t43_35)));
  _t43_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_25, _t43_25, 32), _mm256_permute2f128_pd(_t43_25, _t43_25, 32), 0), _t43_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_25, _t43_25, 32), _mm256_permute2f128_pd(_t43_25, _t43_25, 32), 15), _t43_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_25, _t43_25, 49), _mm256_permute2f128_pd(_t43_25, _t43_25, 49), 0), _t43_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_25, _t43_25, 49), _mm256_permute2f128_pd(_t43_25, _t43_25, 49), 15), _t43_35)));
  _t43_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_26, _t43_26, 32), _mm256_permute2f128_pd(_t43_26, _t43_26, 32), 0), _t43_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_26, _t43_26, 32), _mm256_permute2f128_pd(_t43_26, _t43_26, 32), 15), _t43_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_26, _t43_26, 49), _mm256_permute2f128_pd(_t43_26, _t43_26, 49), 0), _t43_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_26, _t43_26, 49), _mm256_permute2f128_pd(_t43_26, _t43_26, 49), 15), _t43_35)));
  _t43_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_27, _t43_27, 32), _mm256_permute2f128_pd(_t43_27, _t43_27, 32), 0), _t43_32), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_27, _t43_27, 32), _mm256_permute2f128_pd(_t43_27, _t43_27, 32), 15), _t43_33)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_27, _t43_27, 49), _mm256_permute2f128_pd(_t43_27, _t43_27, 49), 0), _t43_34), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t43_27, _t43_27, 49), _mm256_permute2f128_pd(_t43_27, _t43_27, 49), 15), _t43_35)));

  // AVX Loader:

  // 4-BLAC: 4x4 + 4x4
  _t41_0 = _mm256_add_pd(_t41_0, _t43_16);
  _t41_1 = _mm256_add_pd(_t41_1, _t43_17);
  _t41_2 = _mm256_add_pd(_t41_2, _t43_18);
  _t41_3 = _mm256_add_pd(_t41_3, _t43_19);

  // AVX Storer:

  // Generating : M3[4,4] = ( S(h(4, 4, 0), ( ( G(h(4, 4, 0), M1[4,28],h(4, 28, 0)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, 0)) ) ) + G(h(4, 4, 0), R[4,4],h(4, 4, 0)) ),h(4, 4, 0)) + Sum_{i0} ( $(h(4, 4, 0), ( G(h(4, 4, 0), M1[4,28],h(4, 28, i0)) * T( G(h(4, 4, 0), H[4,28],h(4, 28, i0)) ) ),h(4, 4, 0)) ) )

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t43_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 32);
  _t43_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 32);
  _t43_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_4, _t20_3), _mm256_unpacklo_pd(_t20_2, _t20_1), 49);
  _t43_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_4, _t20_3), _mm256_unpackhi_pd(_t20_2, _t20_1), 49);

  // 4-BLAC: 4x4 * 4x4
  _t43_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_20, _t22_20, 32), _mm256_permute2f128_pd(_t22_20, _t22_20, 32), 0), _t43_36), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_20, _t22_20, 32), _mm256_permute2f128_pd(_t22_20, _t22_20, 32), 15), _t43_37)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_20, _t22_20, 49), _mm256_permute2f128_pd(_t22_20, _t22_20, 49), 0), _t43_38), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_20, _t22_20, 49), _mm256_permute2f128_pd(_t22_20, _t22_20, 49), 15), _t43_39)));
  _t43_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_21, _t22_21, 32), _mm256_permute2f128_pd(_t22_21, _t22_21, 32), 0), _t43_36), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_21, _t22_21, 32), _mm256_permute2f128_pd(_t22_21, _t22_21, 32), 15), _t43_37)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_21, _t22_21, 49), _mm256_permute2f128_pd(_t22_21, _t22_21, 49), 0), _t43_38), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_21, _t22_21, 49), _mm256_permute2f128_pd(_t22_21, _t22_21, 49), 15), _t43_39)));
  _t43_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_22, _t22_22, 32), _mm256_permute2f128_pd(_t22_22, _t22_22, 32), 0), _t43_36), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_22, _t22_22, 32), _mm256_permute2f128_pd(_t22_22, _t22_22, 32), 15), _t43_37)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_22, _t22_22, 49), _mm256_permute2f128_pd(_t22_22, _t22_22, 49), 0), _t43_38), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_22, _t22_22, 49), _mm256_permute2f128_pd(_t22_22, _t22_22, 49), 15), _t43_39)));
  _t43_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_23, _t22_23, 32), _mm256_permute2f128_pd(_t22_23, _t22_23, 32), 0), _t43_36), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_23, _t22_23, 32), _mm256_permute2f128_pd(_t22_23, _t22_23, 32), 15), _t43_37)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_23, _t22_23, 49), _mm256_permute2f128_pd(_t22_23, _t22_23, 49), 0), _t43_38), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t22_23, _t22_23, 49), _mm256_permute2f128_pd(_t22_23, _t22_23, 49), 15), _t43_39)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t43_28 = _t43_3;
  _t43_29 = _mm256_blend_pd(_mm256_shuffle_pd(_t43_3, _t43_2, 3), _t43_2, 12);
  _t43_30 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t43_3, _t43_2, 0), _t43_1, 49);
  _t43_31 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t43_3, _t43_2, 12), _mm256_shuffle_pd(_t43_1, _t43_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t43_8 = _mm256_add_pd(_t43_20, _t43_28);
  _t43_9 = _mm256_add_pd(_t43_21, _t43_29);
  _t43_10 = _mm256_add_pd(_t43_22, _t43_30);
  _t43_11 = _mm256_add_pd(_t43_23, _t43_31);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t43_12 = _t43_8;
  _t43_13 = _t43_9;
  _t43_14 = _t43_10;
  _t43_15 = _t43_11;

  _mm256_storeu_pd(M1 + 4, _t24_24);
  _mm256_storeu_pd(M1 + 32, _t24_25);
  _mm256_storeu_pd(M1 + 60, _t24_26);
  _mm256_storeu_pd(M1 + 88, _t24_27);
  _mm256_storeu_pd(M1 + 24, _t30_4);
  _mm256_storeu_pd(M1 + 52, _t30_5);
  _mm256_storeu_pd(M1 + 80, _t30_6);
  _mm256_storeu_pd(M1 + 108, _t30_7);

  for( int i0 = 4; i0 <= 27; i0+=4 ) {
    _t44_19 = _mm256_broadcast_sd(M1 + i0);
    _t44_18 = _mm256_broadcast_sd(M1 + i0 + 1);
    _t44_17 = _mm256_broadcast_sd(M1 + i0 + 2);
    _t44_16 = _mm256_broadcast_sd(M1 + i0 + 3);
    _t44_15 = _mm256_broadcast_sd(M1 + i0 + 28);
    _t44_14 = _mm256_broadcast_sd(M1 + i0 + 29);
    _t44_13 = _mm256_broadcast_sd(M1 + i0 + 30);
    _t44_12 = _mm256_broadcast_sd(M1 + i0 + 31);
    _t44_11 = _mm256_broadcast_sd(M1 + i0 + 56);
    _t44_10 = _mm256_broadcast_sd(M1 + i0 + 57);
    _t44_9 = _mm256_broadcast_sd(M1 + i0 + 58);
    _t44_8 = _mm256_broadcast_sd(M1 + i0 + 59);
    _t44_7 = _mm256_broadcast_sd(M1 + i0 + 84);
    _t44_6 = _mm256_broadcast_sd(M1 + i0 + 85);
    _t44_5 = _mm256_broadcast_sd(M1 + i0 + 86);
    _t44_4 = _mm256_broadcast_sd(M1 + i0 + 87);
    _t44_3 = _mm256_loadu_pd(H + i0);
    _t44_2 = _mm256_loadu_pd(H + i0 + 28);
    _t44_1 = _mm256_loadu_pd(H + i0 + 56);
    _t44_0 = _mm256_loadu_pd(H + i0 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t44_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_3, _t44_2), _mm256_unpacklo_pd(_t44_1, _t44_0), 32);
    _t44_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_3, _t44_2), _mm256_unpackhi_pd(_t44_1, _t44_0), 32);
    _t44_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_3, _t44_2), _mm256_unpacklo_pd(_t44_1, _t44_0), 49);
    _t44_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_3, _t44_2), _mm256_unpackhi_pd(_t44_1, _t44_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t44_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_19, _t44_28), _mm256_mul_pd(_t44_18, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_17, _t44_30), _mm256_mul_pd(_t44_16, _t44_31)));
    _t44_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_28), _mm256_mul_pd(_t44_14, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_30), _mm256_mul_pd(_t44_12, _t44_31)));
    _t44_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_28), _mm256_mul_pd(_t44_10, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_30), _mm256_mul_pd(_t44_8, _t44_31)));
    _t44_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_7, _t44_28), _mm256_mul_pd(_t44_6, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_5, _t44_30), _mm256_mul_pd(_t44_4, _t44_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t44_24 = _t43_12;
    _t44_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t43_12, _t43_13, 3), _t43_13, 12);
    _t44_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t43_12, _t43_13, 0), _t43_14, 49);
    _t44_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t43_12, _t43_13, 12), _mm256_shuffle_pd(_t43_14, _t43_15, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t44_24 = _mm256_add_pd(_t44_24, _t44_20);
    _t44_25 = _mm256_add_pd(_t44_25, _t44_21);
    _t44_26 = _mm256_add_pd(_t44_26, _t44_22);
    _t44_27 = _mm256_add_pd(_t44_27, _t44_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t43_12 = _t44_24;
    _t43_13 = _t44_25;
    _t43_14 = _t44_26;
    _t43_15 = _t44_27;
  }


  // Generating : M3[4,4] = S(h(1, 4, 0), Sqrt( G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ),h(1, 4, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_179 = _mm256_blend_pd(_mm256_setzero_pd(), _t43_12, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_180 = _mm256_sqrt_pd(_t45_179);

  // AVX Storer:
  _t45_0 = _t45_180;

  // Generating : T250[1,4] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ),h(1, 4, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_25 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_26 = _t45_0;

  // 4-BLAC: 1x4 / 1x4
  _t45_27 = _mm256_div_pd(_t45_25, _t45_26);

  // AVX Storer:
  _t45_1 = _t45_27;

  // Generating : M3[4,4] = S(h(1, 4, 0), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) Kro G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ),h(3, 4, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_28 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_29 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_12, 14), _mm256_permute2f128_pd(_t43_12, _t43_12, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_30 = _mm256_mul_pd(_t45_28, _t45_29);

  // AVX Storer:
  _t45_2 = _t45_30;

  // Generating : M3[4,4] = S(h(3, 4, 1), ( G(h(3, 4, 1), M3[4,4],h(3, 4, 1)) - ( T( G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) * G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) ),h(3, 4, 1))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t45_31 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_13, 14), _mm256_permute2f128_pd(_t43_13, _t43_13, 129), 5);
  _t45_32 = _mm256_blend_pd(_mm256_shuffle_pd(_mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_13, 14), _mm256_permute2f128_pd(_t43_13, _t43_13, 129), 5), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_14, 12), _mm256_permute2f128_pd(_t43_14, _t43_14, 129), 5), 3), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_14, 12), _mm256_permute2f128_pd(_t43_14, _t43_14, 129), 5), 12);
  _t45_33 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_13, 14), _mm256_permute2f128_pd(_t43_13, _t43_13, 129), 5), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_14, 12), _mm256_permute2f128_pd(_t43_14, _t43_14, 129), 5), 0), _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t43_15, 8), _mm256_setzero_pd()), 49);
  _t45_34 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_35 = _t45_2;

  // 4-BLAC: (1x4)^T
  _t45_36 = _t45_35;

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_37 = _t45_2;

  // 4-BLAC: 4x1 * 1x4
  _t45_38 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_36, _t45_36, 32), _mm256_permute2f128_pd(_t45_36, _t45_36, 32), 0), _t45_37);
  _t45_39 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_36, _t45_36, 32), _mm256_permute2f128_pd(_t45_36, _t45_36, 32), 15), _t45_37);
  _t45_40 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_36, _t45_36, 49), _mm256_permute2f128_pd(_t45_36, _t45_36, 49), 0), _t45_37);
  _t45_41 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_36, _t45_36, 49), _mm256_permute2f128_pd(_t45_36, _t45_36, 49), 15), _t45_37);

  // 4-BLAC: 4x4 - 4x4
  _t45_42 = _mm256_sub_pd(_t45_31, _t45_38);
  _t45_43 = _mm256_sub_pd(_t45_32, _t45_39);
  _t45_44 = _mm256_sub_pd(_t45_33, _t45_40);
  _t45_45 = _mm256_sub_pd(_t45_34, _t45_41);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t45_3 = _t45_42;
  _t45_4 = _t45_43;
  _t45_5 = _t45_44;

  // Generating : M3[4,4] = S(h(1, 4, 1), Sqrt( G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ),h(1, 4, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_46 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_3, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_47 = _mm256_sqrt_pd(_t45_46);

  // AVX Storer:
  _t45_6 = _t45_47;

  // Generating : T250[1,4] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ),h(1, 4, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_48 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_49 = _t45_6;

  // 4-BLAC: 1x4 / 1x4
  _t45_50 = _mm256_div_pd(_t45_48, _t45_49);

  // AVX Storer:
  _t45_7 = _t45_50;

  // Generating : M3[4,4] = S(h(1, 4, 1), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ),h(2, 4, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_51 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_52 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_3, 6), _mm256_permute2f128_pd(_t45_3, _t45_3, 129), 5);

  // 4-BLAC: 1x4 Kro 1x4
  _t45_53 = _mm256_mul_pd(_t45_51, _t45_52);

  // AVX Storer:
  _t45_8 = _t45_53;

  // Generating : M3[4,4] = S(h(2, 4, 2), ( G(h(2, 4, 2), M3[4,4],h(2, 4, 2)) - ( T( G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) * G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) ),h(2, 4, 2))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t45_54 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_4, 6), _mm256_permute2f128_pd(_t45_4, _t45_4, 129), 5);
  _t45_55 = _mm256_shuffle_pd(_mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_4, 6), _mm256_permute2f128_pd(_t45_4, _t45_4, 129), 5), _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_5, 4), _mm256_permute2f128_pd(_t45_5, _t45_5, 129), 5), 3);
  _t45_56 = _mm256_setzero_pd();
  _t45_57 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_58 = _t45_8;

  // 4-BLAC: (1x4)^T
  _t45_59 = _t45_58;

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_60 = _t45_8;

  // 4-BLAC: 4x1 * 1x4
  _t45_61 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_59, _t45_59, 32), _mm256_permute2f128_pd(_t45_59, _t45_59, 32), 0), _t45_60);
  _t45_62 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_59, _t45_59, 32), _mm256_permute2f128_pd(_t45_59, _t45_59, 32), 15), _t45_60);
  _t45_63 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_59, _t45_59, 49), _mm256_permute2f128_pd(_t45_59, _t45_59, 49), 0), _t45_60);
  _t45_64 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_59, _t45_59, 49), _mm256_permute2f128_pd(_t45_59, _t45_59, 49), 15), _t45_60);

  // 4-BLAC: 4x4 - 4x4
  _t45_65 = _mm256_sub_pd(_t45_54, _t45_61);
  _t45_66 = _mm256_sub_pd(_t45_55, _t45_62);
  _t45_67 = _mm256_sub_pd(_t45_56, _t45_63);
  _t45_68 = _mm256_sub_pd(_t45_57, _t45_64);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t45_9 = _t45_65;
  _t45_10 = _t45_66;

  // Generating : M3[4,4] = S(h(1, 4, 2), Sqrt( G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ),h(1, 4, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_69 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_9, 1);

  // 4-BLAC: sqrt(1x4)
  _t45_70 = _mm256_sqrt_pd(_t45_69);

  // AVX Storer:
  _t45_11 = _t45_70;

  // Generating : M3[4,4] = S(h(1, 4, 2), ( G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) Div G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ),h(1, 4, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_71 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_9, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_72 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t45_73 = _mm256_div_pd(_t45_71, _t45_72);

  // AVX Storer:
  _t45_12 = _t45_73;

  // Generating : M3[4,4] = S(h(1, 4, 3), ( G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) - ( T( G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) Kro G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) ),h(1, 4, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_74 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_10, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_75 = _t45_12;

  // 4-BLAC: (4x1)^T
  _t45_76 = _t45_75;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_77 = _t45_12;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_78 = _mm256_mul_pd(_t45_76, _t45_77);

  // 4-BLAC: 1x4 - 1x4
  _t45_79 = _mm256_sub_pd(_t45_74, _t45_78);

  // AVX Storer:
  _t45_13 = _t45_79;

  // Generating : M3[4,4] = S(h(1, 4, 3), Sqrt( G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ),h(1, 4, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_80 = _t45_13;

  // 4-BLAC: sqrt(1x4)
  _t45_81 = _mm256_sqrt_pd(_t45_80);

  // AVX Storer:
  _t45_13 = _t45_81;

  // Generating : v0[4,1] = S(h(1, 4, 0), ( G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_82 = _mm256_blend_pd(_mm256_setzero_pd(), _t20_7, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_83 = _t45_0;

  // 4-BLAC: 1x4 / 1x4
  _t45_84 = _mm256_div_pd(_t45_82, _t45_83);

  // AVX Storer:
  _t45_14 = _t45_84;

  // Generating : v0[4,1] = S(h(3, 4, 1), ( G(h(3, 4, 1), v0[4,1],h(1, 1, 0)) - ( T( G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) Kro G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t45_85 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t20_7, 14), _mm256_permute2f128_pd(_t20_7, _t20_7, 129), 5);

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_86 = _t45_2;

  // 4-BLAC: (1x4)^T
  _t45_87 = _t45_86;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_88 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_14, _t45_14, 32), _mm256_permute2f128_pd(_t45_14, _t45_14, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t45_89 = _mm256_mul_pd(_t45_87, _t45_88);

  // 4-BLAC: 4x1 - 4x1
  _t45_90 = _mm256_sub_pd(_t45_85, _t45_89);

  // AVX Storer:
  _t45_15 = _t45_90;

  // Generating : v0[4,1] = S(h(1, 4, 1), ( G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_91 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_15, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_92 = _t45_6;

  // 4-BLAC: 1x4 / 1x4
  _t45_93 = _mm256_div_pd(_t45_91, _t45_92);

  // AVX Storer:
  _t45_16 = _t45_93;

  // Generating : v0[4,1] = S(h(2, 4, 2), ( G(h(2, 4, 2), v0[4,1],h(1, 1, 0)) - ( T( G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) Kro G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t45_94 = _mm256_shuffle_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_15, 6), _mm256_permute2f128_pd(_t45_15, _t45_15, 129), 5);

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_95 = _t45_8;

  // 4-BLAC: (1x4)^T
  _t45_96 = _t45_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_97 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_16, _t45_16, 32), _mm256_permute2f128_pd(_t45_16, _t45_16, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t45_98 = _mm256_mul_pd(_t45_96, _t45_97);

  // 4-BLAC: 4x1 - 4x1
  _t45_99 = _mm256_sub_pd(_t45_94, _t45_98);

  // AVX Storer:
  _t45_17 = _t45_99;

  // Generating : v0[4,1] = S(h(1, 4, 2), ( G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_100 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_17, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_101 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t45_102 = _mm256_div_pd(_t45_100, _t45_101);

  // AVX Storer:
  _t45_18 = _t45_102;

  // Generating : v0[4,1] = S(h(1, 4, 3), ( G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) - ( T( G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) Kro G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_103 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_17, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_104 = _t45_12;

  // 4-BLAC: (4x1)^T
  _t45_105 = _t45_104;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_106 = _t45_18;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_107 = _mm256_mul_pd(_t45_105, _t45_106);

  // 4-BLAC: 1x4 - 1x4
  _t45_108 = _mm256_sub_pd(_t45_103, _t45_107);

  // AVX Storer:
  _t45_19 = _t45_108;

  // Generating : v0[4,1] = S(h(1, 4, 3), ( G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_109 = _t45_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_110 = _t45_13;

  // 4-BLAC: 1x4 / 1x4
  _t45_111 = _mm256_div_pd(_t45_109, _t45_110);

  // AVX Storer:
  _t45_19 = _t45_111;

  // Generating : v0[4,1] = S(h(1, 4, 3), ( G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_112 = _t45_19;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_113 = _t45_13;

  // 4-BLAC: 1x4 / 1x4
  _t45_114 = _mm256_div_pd(_t45_112, _t45_113);

  // AVX Storer:
  _t45_19 = _t45_114;

  // Generating : v0[4,1] = S(h(3, 4, 0), ( G(h(3, 4, 0), v0[4,1],h(1, 1, 0)) - ( G(h(3, 4, 0), M3[4,4],h(1, 4, 3)) Kro G(h(1, 4, 3), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t45_115 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_14, _t45_16), _mm256_unpacklo_pd(_t45_18, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t45_116 = _mm256_blend_pd(_mm256_permute2f128_pd(_t45_2, _t45_12, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t45_8, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_117 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_19, _t45_19, 32), _mm256_permute2f128_pd(_t45_19, _t45_19, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t45_118 = _mm256_mul_pd(_t45_116, _t45_117);

  // 4-BLAC: 4x1 - 4x1
  _t45_119 = _mm256_sub_pd(_t45_115, _t45_118);

  // AVX Storer:
  _t45_20 = _t45_119;

  // Generating : v0[4,1] = S(h(1, 4, 2), ( G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_120 = _mm256_permute2f128_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_20, 4), _mm256_blend_pd(_mm256_setzero_pd(), _t45_20, 4), 129);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_121 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t45_122 = _mm256_div_pd(_t45_120, _t45_121);

  // AVX Storer:
  _t45_18 = _t45_122;

  // Generating : v0[4,1] = S(h(2, 4, 0), ( G(h(2, 4, 0), v0[4,1],h(1, 1, 0)) - ( G(h(2, 4, 0), M3[4,4],h(1, 4, 2)) Kro G(h(1, 4, 2), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t45_123 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_20, 3);

  // AVX Loader:

  // 2x1 -> 4x1
  _t45_124 = _mm256_shuffle_pd(_mm256_blend_pd(_t45_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t45_8, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_125 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_18, _t45_18, 32), _mm256_permute2f128_pd(_t45_18, _t45_18, 32), 0);

  // 4-BLAC: 4x1 Kro 1x4
  _t45_126 = _mm256_mul_pd(_t45_124, _t45_125);

  // 4-BLAC: 4x1 - 4x1
  _t45_127 = _mm256_sub_pd(_t45_123, _t45_126);

  // AVX Storer:
  _t45_21 = _t45_127;

  // Generating : v0[4,1] = S(h(1, 4, 1), ( G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 1), M3[4,4],h(1, 4, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_128 = _mm256_unpackhi_pd(_mm256_blend_pd(_mm256_setzero_pd(), _t45_21, 2), _mm256_setzero_pd());

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_129 = _t45_6;

  // 4-BLAC: 1x4 / 1x4
  _t45_130 = _mm256_div_pd(_t45_128, _t45_129);

  // AVX Storer:
  _t45_16 = _t45_130;

  // Generating : v0[4,1] = S(h(1, 4, 0), ( G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) - ( G(h(1, 4, 0), M3[4,4],h(1, 4, 1)) Kro G(h(1, 4, 1), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_131 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_21, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_132 = _mm256_blend_pd(_mm256_setzero_pd(), _t45_2, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_133 = _t45_16;

  // 4-BLAC: 1x4 Kro 1x4
  _t45_134 = _mm256_mul_pd(_t45_132, _t45_133);

  // 4-BLAC: 1x4 - 1x4
  _t45_135 = _mm256_sub_pd(_t45_131, _t45_134);

  // AVX Storer:
  _t45_14 = _t45_135;

  // Generating : v0[4,1] = S(h(1, 4, 0), ( G(h(1, 4, 0), v0[4,1],h(1, 1, 0)) Div G(h(1, 4, 0), M3[4,4],h(1, 4, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_136 = _t45_14;

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_137 = _t45_0;

  // 4-BLAC: 1x4 / 1x4
  _t45_138 = _mm256_div_pd(_t45_136, _t45_137);

  // AVX Storer:
  _t45_14 = _t45_138;

  // Generating : T250[1,4] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 4, 2), M3[4,4],h(1, 4, 2)) ),h(1, 4, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_139 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_140 = _t45_11;

  // 4-BLAC: 1x4 / 1x4
  _t45_141 = _mm256_div_pd(_t45_139, _t45_140);

  // AVX Storer:
  _t45_22 = _t45_141;

  // Generating : T250[1,4] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 4, 3), M3[4,4],h(1, 4, 3)) ),h(1, 4, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t45_142 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_143 = _t45_13;

  // 4-BLAC: 1x4 / 1x4
  _t45_144 = _mm256_div_pd(_t45_142, _t45_143);

  // AVX Storer:
  _t45_23 = _t45_144;

  // Generating : M1[4,28] = S(h(1, 4, 0), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) Kro G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_145 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_20 = _mm256_mul_pd(_t45_145, _t22_20);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(3, 4, 1), ( G(h(3, 4, 1), M1[4,28],h(4, 28, fi136)) - ( T( G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) * G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

  // AVX Loader:

  // 3x4 -> 4x4
  _t45_146 = _t22_21;
  _t45_147 = _t22_22;
  _t45_148 = _t22_23;
  _t45_149 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t45_150 = _t45_2;

  // 4-BLAC: (1x4)^T
  _t45_151 = _t45_150;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_152 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 32), _mm256_permute2f128_pd(_t45_151, _t45_151, 32), 0), _t22_20);
  _t45_153 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 32), _mm256_permute2f128_pd(_t45_151, _t45_151, 32), 15), _t22_20);
  _t45_154 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 49), _mm256_permute2f128_pd(_t45_151, _t45_151, 49), 0), _t22_20);
  _t45_155 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 49), _mm256_permute2f128_pd(_t45_151, _t45_151, 49), 15), _t22_20);

  // 4-BLAC: 4x4 - 4x4
  _t45_156 = _mm256_sub_pd(_t45_146, _t45_152);
  _t45_157 = _mm256_sub_pd(_t45_147, _t45_153);
  _t45_158 = _mm256_sub_pd(_t45_148, _t45_154);
  _t45_159 = _mm256_sub_pd(_t45_149, _t45_155);

  // AVX Storer:
  _t22_21 = _t45_156;
  _t22_22 = _t45_157;
  _t22_23 = _t45_158;

  // Generating : M1[4,28] = S(h(1, 4, 1), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_160 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_21 = _mm256_mul_pd(_t45_160, _t22_21);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(2, 4, 2), ( G(h(2, 4, 2), M1[4,28],h(4, 28, fi136)) - ( T( G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) * G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

  // AVX Loader:

  // 2x4 -> 4x4
  _t45_161 = _t22_22;
  _t45_162 = _t22_23;
  _t45_163 = _mm256_setzero_pd();
  _t45_164 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t45_165 = _t45_8;

  // 4-BLAC: (1x4)^T
  _t45_166 = _t45_165;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t45_167 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 32), _mm256_permute2f128_pd(_t45_166, _t45_166, 32), 0), _t22_21);
  _t45_168 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 32), _mm256_permute2f128_pd(_t45_166, _t45_166, 32), 15), _t22_21);
  _t45_169 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 49), _mm256_permute2f128_pd(_t45_166, _t45_166, 49), 0), _t22_21);
  _t45_170 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 49), _mm256_permute2f128_pd(_t45_166, _t45_166, 49), 15), _t22_21);

  // 4-BLAC: 4x4 - 4x4
  _t45_171 = _mm256_sub_pd(_t45_161, _t45_167);
  _t45_172 = _mm256_sub_pd(_t45_162, _t45_168);
  _t45_173 = _mm256_sub_pd(_t45_163, _t45_169);
  _t45_174 = _mm256_sub_pd(_t45_164, _t45_170);

  // AVX Storer:
  _t22_22 = _t45_171;
  _t22_23 = _t45_172;

  // Generating : M1[4,28] = S(h(1, 4, 2), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) Kro G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_175 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_22, _t45_22, 32), _mm256_permute2f128_pd(_t45_22, _t45_22, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_22 = _mm256_mul_pd(_t45_175, _t22_22);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(1, 4, 3), ( G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) - ( T( G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) Kro G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_176 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_12, _t45_12, 32), _mm256_permute2f128_pd(_t45_12, _t45_12, 32), 0);

  // 4-BLAC: (4x1)^T
  _t45_177 = _t45_176;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t45_24 = _mm256_mul_pd(_t45_177, _t22_22);

  // 4-BLAC: 1x4 - 1x4
  _t22_23 = _mm256_sub_pd(_t22_23, _t45_24);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(1, 4, 3), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) Kro G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t45_178 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_23, _t45_23, 32), _mm256_permute2f128_pd(_t45_23, _t45_23, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_23 = _mm256_mul_pd(_t45_178, _t22_23);

  // AVX Storer:

  _mm256_storeu_pd(M3, _t43_12);
  _mm256_maskstore_pd(M3 + 5, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t45_3);
  _mm256_maskstore_pd(M3 + 10, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t45_9);

  for( int fi136 = 4; fi136 <= 24; fi136+=4 ) {
    _t46_3 = _mm256_loadu_pd(M1 + fi136);
    _t46_0 = _mm256_loadu_pd(M1 + fi136 + 28);
    _t46_1 = _mm256_loadu_pd(M1 + fi136 + 56);
    _t46_2 = _mm256_loadu_pd(M1 + fi136 + 84);

    // Generating : M1[4,28] = S(h(1, 4, 0), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) Kro G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t46_3 = _mm256_mul_pd(_t46_4, _t46_3);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(3, 4, 1), ( G(h(3, 4, 1), M1[4,28],h(4, 28, fi136)) - ( T( G(h(1, 4, 0), M3[4,4],h(3, 4, 1)) ) * G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

    // AVX Loader:

    // 3x4 -> 4x4
    _t46_5 = _t46_0;
    _t46_6 = _t46_1;
    _t46_7 = _t46_2;
    _t46_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t46_9 = _t45_2;

    // 4-BLAC: (1x4)^T
    _t45_151 = _t46_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t45_152 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 32), _mm256_permute2f128_pd(_t45_151, _t45_151, 32), 0), _t46_3);
    _t45_153 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 32), _mm256_permute2f128_pd(_t45_151, _t45_151, 32), 15), _t46_3);
    _t45_154 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 49), _mm256_permute2f128_pd(_t45_151, _t45_151, 49), 0), _t46_3);
    _t45_155 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_151, _t45_151, 49), _mm256_permute2f128_pd(_t45_151, _t45_151, 49), 15), _t46_3);

    // 4-BLAC: 4x4 - 4x4
    _t46_10 = _mm256_sub_pd(_t46_5, _t45_152);
    _t46_11 = _mm256_sub_pd(_t46_6, _t45_153);
    _t46_12 = _mm256_sub_pd(_t46_7, _t45_154);
    _t46_13 = _mm256_sub_pd(_t46_8, _t45_155);

    // AVX Storer:
    _t46_0 = _t46_10;
    _t46_1 = _t46_11;
    _t46_2 = _t46_12;

    // Generating : M1[4,28] = S(h(1, 4, 1), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t46_0 = _mm256_mul_pd(_t46_14, _t46_0);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(2, 4, 2), ( G(h(2, 4, 2), M1[4,28],h(4, 28, fi136)) - ( T( G(h(1, 4, 1), M3[4,4],h(2, 4, 2)) ) * G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

    // AVX Loader:

    // 2x4 -> 4x4
    _t46_15 = _t46_1;
    _t46_16 = _t46_2;
    _t46_17 = _mm256_setzero_pd();
    _t46_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t46_19 = _t45_8;

    // 4-BLAC: (1x4)^T
    _t45_166 = _t46_19;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t45_167 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 32), _mm256_permute2f128_pd(_t45_166, _t45_166, 32), 0), _t46_0);
    _t45_168 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 32), _mm256_permute2f128_pd(_t45_166, _t45_166, 32), 15), _t46_0);
    _t45_169 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 49), _mm256_permute2f128_pd(_t45_166, _t45_166, 49), 0), _t46_0);
    _t45_170 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_166, _t45_166, 49), _mm256_permute2f128_pd(_t45_166, _t45_166, 49), 15), _t46_0);

    // 4-BLAC: 4x4 - 4x4
    _t46_20 = _mm256_sub_pd(_t46_15, _t45_167);
    _t46_21 = _mm256_sub_pd(_t46_16, _t45_168);
    _t46_22 = _mm256_sub_pd(_t46_17, _t45_169);
    _t46_23 = _mm256_sub_pd(_t46_18, _t45_170);

    // AVX Storer:
    _t46_1 = _t46_20;
    _t46_2 = _t46_21;

    // Generating : M1[4,28] = S(h(1, 4, 2), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) Kro G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_22, _t45_22, 32), _mm256_permute2f128_pd(_t45_22, _t45_22, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t46_1 = _mm256_mul_pd(_t46_24, _t46_1);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(1, 4, 3), ( G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) - ( T( G(h(1, 4, 2), M3[4,4],h(1, 4, 3)) ) Kro G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_12, _t45_12, 32), _mm256_permute2f128_pd(_t45_12, _t45_12, 32), 0);

    // 4-BLAC: (4x1)^T
    _t45_177 = _t46_25;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t45_24 = _mm256_mul_pd(_t45_177, _t46_1);

    // 4-BLAC: 1x4 - 1x4
    _t46_2 = _mm256_sub_pd(_t46_2, _t45_24);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(1, 4, 3), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) Kro G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t46_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_23, _t45_23, 32), _mm256_permute2f128_pd(_t45_23, _t45_23, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t46_2 = _mm256_mul_pd(_t46_26, _t46_2);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi136, _t46_3);
    _mm256_storeu_pd(M1 + fi136 + 28, _t46_0);
    _mm256_storeu_pd(M1 + fi136 + 56, _t46_1);
    _mm256_storeu_pd(M1 + fi136 + 84, _t46_2);
  }


  // Generating : M1[4,28] = S(h(1, 4, 3), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) Kro G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t47_1 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_23, _t45_23, 32), _mm256_permute2f128_pd(_t45_23, _t45_23, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_23 = _mm256_mul_pd(_t47_1, _t22_23);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(3, 4, 0), ( G(h(3, 4, 0), M1[4,28],h(4, 28, fi136)) - ( G(h(3, 4, 0), M3[4,4],h(1, 4, 3)) * G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

  // AVX Loader:

  // 3x4 -> 4x4
  _t47_2 = _t22_20;
  _t47_3 = _t22_21;
  _t47_4 = _t22_22;
  _t47_5 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t47_6 = _mm256_blend_pd(_mm256_permute2f128_pd(_t45_2, _t45_12, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t45_8, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t47_7 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_6, _t47_6, 32), _mm256_permute2f128_pd(_t47_6, _t47_6, 32), 0), _t22_23);
  _t47_8 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_6, _t47_6, 32), _mm256_permute2f128_pd(_t47_6, _t47_6, 32), 15), _t22_23);
  _t47_9 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_6, _t47_6, 49), _mm256_permute2f128_pd(_t47_6, _t47_6, 49), 0), _t22_23);
  _t47_10 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_6, _t47_6, 49), _mm256_permute2f128_pd(_t47_6, _t47_6, 49), 15), _t22_23);

  // 4-BLAC: 4x4 - 4x4
  _t47_11 = _mm256_sub_pd(_t47_2, _t47_7);
  _t47_12 = _mm256_sub_pd(_t47_3, _t47_8);
  _t47_13 = _mm256_sub_pd(_t47_4, _t47_9);
  _t47_14 = _mm256_sub_pd(_t47_5, _t47_10);

  // AVX Storer:
  _t22_20 = _t47_11;
  _t22_21 = _t47_12;
  _t22_22 = _t47_13;

  // Generating : M1[4,28] = S(h(1, 4, 2), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) Kro G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t47_15 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_22, _t45_22, 32), _mm256_permute2f128_pd(_t45_22, _t45_22, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_22 = _mm256_mul_pd(_t47_15, _t22_22);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(2, 4, 0), ( G(h(2, 4, 0), M1[4,28],h(4, 28, fi136)) - ( G(h(2, 4, 0), M3[4,4],h(1, 4, 2)) * G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

  // AVX Loader:

  // 2x4 -> 4x4
  _t47_16 = _t22_20;
  _t47_17 = _t22_21;
  _t47_18 = _mm256_setzero_pd();
  _t47_19 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t47_20 = _mm256_shuffle_pd(_mm256_blend_pd(_t45_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t45_8, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t47_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_20, _t47_20, 32), _mm256_permute2f128_pd(_t47_20, _t47_20, 32), 0), _t22_22);
  _t47_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_20, _t47_20, 32), _mm256_permute2f128_pd(_t47_20, _t47_20, 32), 15), _t22_22);
  _t47_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_20, _t47_20, 49), _mm256_permute2f128_pd(_t47_20, _t47_20, 49), 0), _t22_22);
  _t47_24 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t47_20, _t47_20, 49), _mm256_permute2f128_pd(_t47_20, _t47_20, 49), 15), _t22_22);

  // 4-BLAC: 4x4 - 4x4
  _t47_25 = _mm256_sub_pd(_t47_16, _t47_21);
  _t47_26 = _mm256_sub_pd(_t47_17, _t47_22);
  _t47_27 = _mm256_sub_pd(_t47_18, _t47_23);
  _t47_28 = _mm256_sub_pd(_t47_19, _t47_24);

  // AVX Storer:
  _t22_20 = _t47_25;
  _t22_21 = _t47_26;

  // Generating : M1[4,28] = S(h(1, 4, 1), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t47_29 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_21 = _mm256_mul_pd(_t47_29, _t22_21);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(1, 4, 0), ( G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) - ( G(h(1, 4, 0), M3[4,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t47_30 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_2, _t45_2, 32), _mm256_permute2f128_pd(_t45_2, _t45_2, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t47_0 = _mm256_mul_pd(_t47_30, _t22_21);

  // 4-BLAC: 1x4 - 1x4
  _t22_20 = _mm256_sub_pd(_t22_20, _t47_0);

  // AVX Storer:

  // Generating : M1[4,28] = S(h(1, 4, 0), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) Kro G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

  // AVX Loader:

  // 1x1 -> 1x4
  _t47_31 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t22_20 = _mm256_mul_pd(_t47_31, _t22_20);

  // AVX Storer:


  for( int fi136 = 4; fi136 <= 24; fi136+=4 ) {
    _t48_3 = _mm256_loadu_pd(M1 + fi136 + 84);
    _t48_0 = _mm256_loadu_pd(M1 + fi136);
    _t48_1 = _mm256_loadu_pd(M1 + fi136 + 28);
    _t48_2 = _mm256_loadu_pd(M1 + fi136 + 56);

    // Generating : M1[4,28] = S(h(1, 4, 3), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 3)) Kro G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_4 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_23, _t45_23, 32), _mm256_permute2f128_pd(_t45_23, _t45_23, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_3 = _mm256_mul_pd(_t48_4, _t48_3);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(3, 4, 0), ( G(h(3, 4, 0), M1[4,28],h(4, 28, fi136)) - ( G(h(3, 4, 0), M3[4,4],h(1, 4, 3)) * G(h(1, 4, 3), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

    // AVX Loader:

    // 3x4 -> 4x4
    _t48_5 = _t48_0;
    _t48_6 = _t48_1;
    _t48_7 = _t48_2;
    _t48_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t48_9 = _mm256_blend_pd(_mm256_permute2f128_pd(_t45_2, _t45_12, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t45_8, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t47_7 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 0), _t48_3);
    _t47_8 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 32), _mm256_permute2f128_pd(_t48_9, _t48_9, 32), 15), _t48_3);
    _t47_9 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 49), _mm256_permute2f128_pd(_t48_9, _t48_9, 49), 0), _t48_3);
    _t47_10 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_9, _t48_9, 49), _mm256_permute2f128_pd(_t48_9, _t48_9, 49), 15), _t48_3);

    // 4-BLAC: 4x4 - 4x4
    _t48_10 = _mm256_sub_pd(_t48_5, _t47_7);
    _t48_11 = _mm256_sub_pd(_t48_6, _t47_8);
    _t48_12 = _mm256_sub_pd(_t48_7, _t47_9);
    _t48_13 = _mm256_sub_pd(_t48_8, _t47_10);

    // AVX Storer:
    _t48_0 = _t48_10;
    _t48_1 = _t48_11;
    _t48_2 = _t48_12;

    // Generating : M1[4,28] = S(h(1, 4, 2), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 2)) Kro G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_14 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_22, _t45_22, 32), _mm256_permute2f128_pd(_t45_22, _t45_22, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_2 = _mm256_mul_pd(_t48_14, _t48_2);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(2, 4, 0), ( G(h(2, 4, 0), M1[4,28],h(4, 28, fi136)) - ( G(h(2, 4, 0), M3[4,4],h(1, 4, 2)) * G(h(1, 4, 2), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

    // AVX Loader:

    // 2x4 -> 4x4
    _t48_15 = _t48_0;
    _t48_16 = _t48_1;
    _t48_17 = _mm256_setzero_pd();
    _t48_18 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t48_19 = _mm256_shuffle_pd(_mm256_blend_pd(_t45_2, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t45_8, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t47_21 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_19, _t48_19, 32), _mm256_permute2f128_pd(_t48_19, _t48_19, 32), 0), _t48_2);
    _t47_22 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_19, _t48_19, 32), _mm256_permute2f128_pd(_t48_19, _t48_19, 32), 15), _t48_2);
    _t47_23 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_19, _t48_19, 49), _mm256_permute2f128_pd(_t48_19, _t48_19, 49), 0), _t48_2);
    _t47_24 = _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t48_19, _t48_19, 49), _mm256_permute2f128_pd(_t48_19, _t48_19, 49), 15), _t48_2);

    // 4-BLAC: 4x4 - 4x4
    _t48_20 = _mm256_sub_pd(_t48_15, _t47_21);
    _t48_21 = _mm256_sub_pd(_t48_16, _t47_22);
    _t48_22 = _mm256_sub_pd(_t48_17, _t47_23);
    _t48_23 = _mm256_sub_pd(_t48_18, _t47_24);

    // AVX Storer:
    _t48_0 = _t48_20;
    _t48_1 = _t48_21;

    // Generating : M1[4,28] = S(h(1, 4, 1), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_24 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_7, _t45_7, 32), _mm256_permute2f128_pd(_t45_7, _t45_7, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_1 = _mm256_mul_pd(_t48_24, _t48_1);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(1, 4, 0), ( G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) - ( G(h(1, 4, 0), M3[4,4],h(1, 4, 1)) Kro G(h(1, 4, 1), M1[4,28],h(4, 28, fi136)) ) ),h(4, 28, fi136))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_25 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_2, _t45_2, 32), _mm256_permute2f128_pd(_t45_2, _t45_2, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t47_0 = _mm256_mul_pd(_t48_25, _t48_1);

    // 4-BLAC: 1x4 - 1x4
    _t48_0 = _mm256_sub_pd(_t48_0, _t47_0);

    // AVX Storer:

    // Generating : M1[4,28] = S(h(1, 4, 0), ( G(h(1, 1, 0), T250[1,4],h(1, 4, 0)) Kro G(h(1, 4, 0), M1[4,28],h(4, 28, fi136)) ),h(4, 28, fi136))

    // AVX Loader:

    // 1x1 -> 1x4
    _t48_26 = _mm256_shuffle_pd(_mm256_permute2f128_pd(_t45_1, _t45_1, 32), _mm256_permute2f128_pd(_t45_1, _t45_1, 32), 0);

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t48_0 = _mm256_mul_pd(_t48_26, _t48_0);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi136 + 84, _t48_3);
    _mm256_storeu_pd(M1 + fi136, _t48_0);
    _mm256_storeu_pd(M1 + fi136 + 28, _t48_1);
    _mm256_storeu_pd(M1 + fi136 + 56, _t48_2);
  }


  // Generating : x[28,1] = Sum_{i0} ( S(h(4, 28, i0), ( G(h(4, 28, i0), y[28,1],h(1, 1, 0)) + ( G(h(4, 28, i0), M2[28,4],h(4, 4, 0)) * G(h(4, 4, 0), v0[4,1],h(1, 1, 0)) ) ),h(1, 1, 0)) )

  // AVX Loader:

  _mm256_storeu_pd(M2, _t32_20);
  _mm256_storeu_pd(M2 + 4, _t32_21);
  _mm256_storeu_pd(M2 + 8, _t32_22);
  _mm256_storeu_pd(M2 + 12, _t32_23);
  _mm256_storeu_pd(M2 + 16, _t34_4);
  _mm256_storeu_pd(M2 + 20, _t34_5);
  _mm256_storeu_pd(M2 + 24, _t34_6);
  _mm256_storeu_pd(M2 + 28, _t34_7);
  _mm256_storeu_pd(M2 + 96, _t41_0);
  _mm256_storeu_pd(M2 + 100, _t41_1);
  _mm256_storeu_pd(M2 + 104, _t41_2);
  _mm256_storeu_pd(M2 + 108, _t41_3);
  _mm256_maskstore_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t45_17);
  _mm256_maskstore_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_18);
  _mm256_maskstore_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_16);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_14);

  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t49_6 = _mm256_loadu_pd(y + i0);
    _t49_5 = _mm256_loadu_pd(M2 + 4*i0);
    _t49_4 = _mm256_loadu_pd(M2 + 4*i0 + 4);
    _t49_3 = _mm256_loadu_pd(M2 + 4*i0 + 8);
    _t49_2 = _mm256_loadu_pd(M2 + 4*i0 + 12);
    _t49_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t49_0 = _mm256_shuffle_pd(_mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t49_8 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t49_5, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11)), _mm256_mul_pd(_t49_4, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11))), _mm256_hadd_pd(_mm256_mul_pd(_t49_3, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11)), _mm256_mul_pd(_t49_2, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t49_5, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11)), _mm256_mul_pd(_t49_4, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11))), _mm256_hadd_pd(_mm256_mul_pd(_t49_3, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11)), _mm256_mul_pd(_t49_2, _mm256_blend_pd(_t49_1, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t49_0, _t45_19, 32), _mm256_permute2f128_pd(_t49_0, _t45_19, 32), 2), 11))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t49_7 = _mm256_add_pd(_t49_6, _t49_8);

    // AVX Storer:
    _mm256_storeu_pd(x + i0, _t49_7);
  }


  // Generating : P[28,28] = ( Sum_{i0} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) - ( G(h(4, 28, i0), M2[28,4],h(4, 4, 0)) * G(h(4, 4, 0), M1[4,28],h(4, 28, i0)) ) ),h(4, 28, i0)) + Sum_{k2} ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k2)) - ( G(h(4, 28, i0), M2[28,4],h(4, 4, 0)) * G(h(4, 4, 0), M1[4,28],h(4, 28, k2)) ) ),h(4, 28, k2)) ) ) ) + S(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) - ( G(h(4, 28, 24), M2[28,4],h(4, 4, 0)) * G(h(4, 4, 0), M1[4,28],h(4, 28, 24)) ) ),h(4, 28, 24)) )

  _mm256_storeu_pd(M1, _t22_20);
  _mm256_storeu_pd(M1 + 28, _t22_21);
  _mm256_storeu_pd(M1 + 56, _t22_22);
  _mm256_storeu_pd(M1 + 84, _t22_23);

  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t50_23 = _mm256_loadu_pd(Y + 29*i0);
    _t50_22 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t50_21 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t50_20 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t50_19 = _mm256_broadcast_sd(M2 + 4*i0);
    _t50_18 = _mm256_broadcast_sd(M2 + 4*i0 + 1);
    _t50_17 = _mm256_broadcast_sd(M2 + 4*i0 + 2);
    _t50_16 = _mm256_broadcast_sd(M2 + 4*i0 + 3);
    _t50_15 = _mm256_broadcast_sd(M2 + 4*i0 + 4);
    _t50_14 = _mm256_broadcast_sd(M2 + 4*i0 + 5);
    _t50_13 = _mm256_broadcast_sd(M2 + 4*i0 + 6);
    _t50_12 = _mm256_broadcast_sd(M2 + 4*i0 + 7);
    _t50_11 = _mm256_broadcast_sd(M2 + 4*i0 + 8);
    _t50_10 = _mm256_broadcast_sd(M2 + 4*i0 + 9);
    _t50_9 = _mm256_broadcast_sd(M2 + 4*i0 + 10);
    _t50_8 = _mm256_broadcast_sd(M2 + 4*i0 + 11);
    _t50_7 = _mm256_broadcast_sd(M2 + 4*i0 + 12);
    _t50_6 = _mm256_broadcast_sd(M2 + 4*i0 + 13);
    _t50_5 = _mm256_broadcast_sd(M2 + 4*i0 + 14);
    _t50_4 = _mm256_broadcast_sd(M2 + 4*i0 + 15);
    _t50_3 = _mm256_loadu_pd(M1 + i0);
    _t50_2 = _mm256_loadu_pd(M1 + i0 + 28);
    _t50_1 = _mm256_loadu_pd(M1 + i0 + 56);
    _t50_0 = _mm256_loadu_pd(M1 + i0 + 84);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t50_36 = _t50_23;
    _t50_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_23, _t50_22, 3), _t50_22, 12);
    _t50_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_23, _t50_22, 0), _t50_21, 49);
    _t50_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_23, _t50_22, 12), _mm256_shuffle_pd(_t50_21, _t50_20, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t50_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t50_3), _mm256_mul_pd(_t50_18, _t50_2)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t50_1), _mm256_mul_pd(_t50_16, _t50_0)));
    _t50_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t50_3), _mm256_mul_pd(_t50_14, _t50_2)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t50_1), _mm256_mul_pd(_t50_12, _t50_0)));
    _t50_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t50_3), _mm256_mul_pd(_t50_10, _t50_2)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t50_1), _mm256_mul_pd(_t50_8, _t50_0)));
    _t50_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_7, _t50_3), _mm256_mul_pd(_t50_6, _t50_2)), _mm256_add_pd(_mm256_mul_pd(_t50_5, _t50_1), _mm256_mul_pd(_t50_4, _t50_0)));

    // 4-BLAC: 4x4 - 4x4
    _t50_32 = _mm256_sub_pd(_t50_36, _t50_28);
    _t50_33 = _mm256_sub_pd(_t50_37, _t50_29);
    _t50_34 = _mm256_sub_pd(_t50_38, _t50_30);
    _t50_35 = _mm256_sub_pd(_t50_39, _t50_31);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t50_24 = _t50_32;
    _t50_25 = _t50_33;
    _t50_26 = _t50_34;
    _t50_27 = _t50_35;

    // AVX Loader:

    for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
      _t51_7 = _mm256_loadu_pd(Y + 28*i0 + k2);
      _t51_6 = _mm256_loadu_pd(Y + 28*i0 + k2 + 28);
      _t51_5 = _mm256_loadu_pd(Y + 28*i0 + k2 + 56);
      _t51_4 = _mm256_loadu_pd(Y + 28*i0 + k2 + 84);
      _t51_3 = _mm256_loadu_pd(M1 + k2);
      _t51_2 = _mm256_loadu_pd(M1 + k2 + 28);
      _t51_1 = _mm256_loadu_pd(M1 + k2 + 56);
      _t51_0 = _mm256_loadu_pd(M1 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t51_8 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t51_3), _mm256_mul_pd(_t50_18, _t51_2)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t51_1), _mm256_mul_pd(_t50_16, _t51_0)));
      _t51_9 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t51_3), _mm256_mul_pd(_t50_14, _t51_2)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t51_1), _mm256_mul_pd(_t50_12, _t51_0)));
      _t51_10 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t51_3), _mm256_mul_pd(_t50_10, _t51_2)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t51_1), _mm256_mul_pd(_t50_8, _t51_0)));
      _t51_11 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_7, _t51_3), _mm256_mul_pd(_t50_6, _t51_2)), _mm256_add_pd(_mm256_mul_pd(_t50_5, _t51_1), _mm256_mul_pd(_t50_4, _t51_0)));

      // 4-BLAC: 4x4 - 4x4
      _t51_12 = _mm256_sub_pd(_t51_7, _t51_8);
      _t51_13 = _mm256_sub_pd(_t51_6, _t51_9);
      _t51_14 = _mm256_sub_pd(_t51_5, _t51_10);
      _t51_15 = _mm256_sub_pd(_t51_4, _t51_11);

      // AVX Storer:
      _mm256_storeu_pd(P + 28*i0 + k2, _t51_12);
      _mm256_storeu_pd(P + 28*i0 + k2 + 28, _t51_13);
      _mm256_storeu_pd(P + 28*i0 + k2 + 56, _t51_14);
      _mm256_storeu_pd(P + 28*i0 + k2 + 84, _t51_15);
    }
    _mm256_storeu_pd(P + 29*i0, _t50_24);
    _mm256_maskstore_pd(P + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t50_25);
    _mm256_maskstore_pd(P + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t50_26);
    _mm256_maskstore_pd(P + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t50_27);
  }

  _t41_3 = _mm256_loadu_pd(M2 + 108);
  _t30_7 = _mm256_loadu_pd(M1 + 108);
  _t41_0 = _mm256_loadu_pd(M2 + 96);
  _t41_2 = _mm256_loadu_pd(M2 + 104);
  _t30_6 = _mm256_loadu_pd(M1 + 80);
  _t30_4 = _mm256_loadu_pd(M1 + 24);
  _t41_1 = _mm256_loadu_pd(M2 + 100);
  _t30_5 = _mm256_loadu_pd(M1 + 52);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t52_8 = _t18_28;
  _t52_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 3), _t18_29, 12);
  _t52_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 0), _t18_30, 49);
  _t52_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t18_28, _t18_29, 12), _mm256_shuffle_pd(_t18_30, _t18_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t52_0 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_0, _t41_0, 32), _mm256_permute2f128_pd(_t41_0, _t41_0, 32), 0), _t30_4), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_0, _t41_0, 32), _mm256_permute2f128_pd(_t41_0, _t41_0, 32), 15), _t30_5)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_0, _t41_0, 49), _mm256_permute2f128_pd(_t41_0, _t41_0, 49), 0), _t30_6), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_0, _t41_0, 49), _mm256_permute2f128_pd(_t41_0, _t41_0, 49), 15), _t30_7)));
  _t52_1 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_1, _t41_1, 32), _mm256_permute2f128_pd(_t41_1, _t41_1, 32), 0), _t30_4), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_1, _t41_1, 32), _mm256_permute2f128_pd(_t41_1, _t41_1, 32), 15), _t30_5)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_1, _t41_1, 49), _mm256_permute2f128_pd(_t41_1, _t41_1, 49), 0), _t30_6), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_1, _t41_1, 49), _mm256_permute2f128_pd(_t41_1, _t41_1, 49), 15), _t30_7)));
  _t52_2 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_2, _t41_2, 32), _mm256_permute2f128_pd(_t41_2, _t41_2, 32), 0), _t30_4), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_2, _t41_2, 32), _mm256_permute2f128_pd(_t41_2, _t41_2, 32), 15), _t30_5)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_2, _t41_2, 49), _mm256_permute2f128_pd(_t41_2, _t41_2, 49), 0), _t30_6), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_2, _t41_2, 49), _mm256_permute2f128_pd(_t41_2, _t41_2, 49), 15), _t30_7)));
  _t52_3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_3, _t41_3, 32), _mm256_permute2f128_pd(_t41_3, _t41_3, 32), 0), _t30_4), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_3, _t41_3, 32), _mm256_permute2f128_pd(_t41_3, _t41_3, 32), 15), _t30_5)), _mm256_add_pd(_mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_3, _t41_3, 49), _mm256_permute2f128_pd(_t41_3, _t41_3, 49), 0), _t30_6), _mm256_mul_pd(_mm256_shuffle_pd(_mm256_permute2f128_pd(_t41_3, _t41_3, 49), _mm256_permute2f128_pd(_t41_3, _t41_3, 49), 15), _t30_7)));

  // 4-BLAC: 4x4 - 4x4
  _t52_4 = _mm256_sub_pd(_t52_8, _t52_0);
  _t52_5 = _mm256_sub_pd(_t52_9, _t52_1);
  _t52_6 = _mm256_sub_pd(_t52_10, _t52_2);
  _t52_7 = _mm256_sub_pd(_t52_11, _t52_3);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t2_3 = _t52_4;
  _t2_2 = _t52_5;
  _t2_1 = _t52_6;
  _t2_0 = _t52_7;

  _mm256_storeu_pd(Y + 696, _t18_28);
  _mm256_maskstore_pd(Y + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t18_29);
  _mm256_maskstore_pd(Y + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t18_30);
  _mm256_maskstore_pd(Y + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t18_31);
  _mm256_maskstore_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_0);
  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t45_2);
  _mm256_maskstore_pd(M3 + 5, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_6);
  _mm256_maskstore_pd(M3 + 6, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t45_8);
  _mm256_maskstore_pd(M3 + 10, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_11);
  _mm256_maskstore_pd(M3 + 11, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_12);
  _mm256_maskstore_pd(M3 + 15, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_13);
  _mm256_maskstore_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t45_19);
  _mm256_storeu_pd(P + 696, _t2_3);
  _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t2_2);
  _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t2_1);
  _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t2_0);

}
