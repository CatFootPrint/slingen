/*
 * kf_kernel.h
 *
Decl { {u'B': Matrix[B, (28, 8), GenMatAccess], u'F': SquaredMatrix[F, (28, 28), GenMatAccess], u'H': Matrix[H, (8, 28), GenMatAccess], u'Q': Symmetric[Q, (28, 28), USMatAccess], u'P': Symmetric[P, (28, 28), USMatAccess], u'R': Symmetric[R, (8, 8), USMatAccess], u'M1': Matrix[M1, (8, 28), GenMatAccess], u'M0': SquaredMatrix[M0, (28, 28), GenMatAccess], u'M3': Symmetric[M3, (8, 8), USMatAccess], u'M2': Matrix[M2, (28, 8), GenMatAccess], u'Y': Symmetric[Y, (28, 28), USMatAccess], u'v0': Matrix[v0, (8, 1), GenMatAccess], u'u': Matrix[u, (8, 1), GenMatAccess], 'T608': Matrix[T608, (1, 8), GenMatAccess], u'y': Matrix[y, (28, 1), GenMatAccess], u'x': Matrix[x, (28, 1), GenMatAccess], u'z': Matrix[z, (8, 1), GenMatAccess]} }
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ann: {'part_schemes': {'ldiv_ut_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}, 'chol_u_ow_opt': {'m': 'm3.ll'}, 'ldiv_un_ow_opt': {'m': 'm4.ll', 'n': 'n1.ll'}}, 'cl1ck_v': 0, 'variant_tag': 'chol_u_ow_opt_m3_ldiv_un_ow_opt_m4_n1_ldiv_ut_ow_opt_m4_n1'}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) ) + ( Tile( (1, 1), Tile( (4, 4), B[28,8] ) ) * Tile( (1, 1), Tile( (4, 4), u[8,1] ) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) * Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M0[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), F[28,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), Q[28,28] ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), v0[8,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), z[8,1] ) ) - ( Tile( (1, 1), Tile( (4, 4), H[8,28] ) ) * Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), M1[8,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), H[8,28] ) ) * Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), M2[28,8] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[8,28] ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), M3[8,8] ) ) = ( ( Tile( (1, 1), Tile( (4, 4), M1[8,28] ) ) * T( Tile( (1, 1), Tile( (4, 4), H[8,28] ) ) ) ) + Tile( (1, 1), Tile( (4, 4), R[8,8] ) ) )
Eq.ann: {}
Entry 7:
Eq: Tile( (1, 1), G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ) )
Eq.ann: {}
Entry 8:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ) )
Eq.ann: {}
Entry 9:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) )
Eq.ann: {}
Entry 10:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), M3[8,8],h(3, 8, 1)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), M3[8,8],h(3, 8, 1)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) ) )
Eq.ann: {}
Entry 11:
Eq: Tile( (1, 1), G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ) )
Eq.ann: {}
Entry 12:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ) )
Eq.ann: {}
Entry 13:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) )
Eq.ann: {}
Entry 14:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), M3[8,8],h(2, 8, 2)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), M3[8,8],h(2, 8, 2)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) ) )
Eq.ann: {}
Entry 15:
Eq: Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ) )
Eq.ann: {}
Entry 16:
Eq: Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) = ( Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) Div Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ) )
Eq.ann: {}
Entry 17:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) ) ) )
Eq.ann: {}
Entry 18:
Eq: Tile( (1, 1), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) )
Eq.ann: {}
Entry 19:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(4, 8, 4)) ) ) )
Eq.ann: {}
Entry 20:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), M3[8,8],h(4, 8, 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(4, 8, 4)) ) ) ) )
Eq.ann: {}
Entry 21:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(4, 8, 4)) ) ) )
Eq.ann: {}
Entry 22:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), M3[8,8],h(4, 8, 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(4, 8, 4)) ) ) ) )
Eq.ann: {}
Entry 23:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ) )
Eq.ann: {}
Entry 24:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(4, 8, 4)) ) ) )
Eq.ann: {}
Entry 25:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M3[8,8],h(4, 8, 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(4, 8, 4)) ) ) ) )
Eq.ann: {}
Entry 26:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) )
Eq.ann: {}
Entry 27:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M3[8,8],h(4, 8, 4)) ) ) )
Eq.ann: {}
Entry 28:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), M3[8,8],h(4, 8, 4)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), M3[8,8],h(4, 8, 4)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ) ) )
Eq.ann: {}
Entry 29:
Eq: Tile( (1, 1), G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ) )
Eq.ann: {}
Entry 30:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ) )
Eq.ann: {}
Entry 31:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ) )
Eq.ann: {}
Entry 32:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 5), M3[8,8],h(3, 8, 5)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 5), M3[8,8],h(3, 8, 5)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ) ) )
Eq.ann: {}
Entry 33:
Eq: Tile( (1, 1), G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ) )
Eq.ann: {}
Entry 34:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ) )
Eq.ann: {}
Entry 35:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ) )
Eq.ann: {}
Entry 36:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 6), M3[8,8],h(2, 8, 6)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 6), M3[8,8],h(2, 8, 6)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ) ) )
Eq.ann: {}
Entry 37:
Eq: Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ) )
Eq.ann: {}
Entry 38:
Eq: Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) = ( Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) Div Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ) )
Eq.ann: {}
Entry 39:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) ) ) )
Eq.ann: {}
Entry 40:
Eq: Tile( (1, 1), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) = Sqrt( Tile( (1, 1), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) )
Eq.ann: {}
Entry 41:
Eq: Tile( (1, 1), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ) )
Eq.ann: {}
Entry 42:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 43:
Eq: Tile( (1, 1), G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ) )
Eq.ann: {}
Entry 44:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 45:
Eq: Tile( (1, 1), G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ) )
Eq.ann: {}
Entry 46:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 47:
Eq: Tile( (1, 1), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) )
Eq.ann: {}
Entry 48:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 49:
Eq: Tile( (1, 1), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ) )
Eq.ann: {}
Entry 50:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 5), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 5), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 51:
Eq: Tile( (1, 1), G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ) )
Eq.ann: {}
Entry 52:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 6), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 6), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 53:
Eq: Tile( (1, 1), G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ) )
Eq.ann: {}
Entry 54:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 55:
Eq: Tile( (1, 1), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) )
Eq.ann: {}
Entry 56:
Eq: Tile( (1, 1), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) )
Eq.ann: {}
Entry 57:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 4), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 4), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 4), M3[8,8],h(1, 8, 7)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 58:
Eq: Tile( (1, 1), G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ) )
Eq.ann: {}
Entry 59:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 4), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 4), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 4), M3[8,8],h(1, 8, 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 60:
Eq: Tile( (1, 1), G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ) )
Eq.ann: {}
Entry 61:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(1, 8, 5)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 62:
Eq: Tile( (1, 1), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ) )
Eq.ann: {}
Entry 63:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 64:
Eq: Tile( (1, 1), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ) )
Eq.ann: {}
Entry 65:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 0), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 0), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 0), M3[8,8],h(1, 8, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 66:
Eq: Tile( (1, 1), G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ) )
Eq.ann: {}
Entry 67:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 0), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 0), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 0), M3[8,8],h(1, 8, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 68:
Eq: Tile( (1, 1), G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ) )
Eq.ann: {}
Entry 69:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(1, 8, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) ) ) )
Eq.ann: {}
Entry 70:
Eq: Tile( (1, 1), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) = ( Tile( (1, 1), G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) Div Tile( (1, 1), G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ) )
Eq.ann: {}
Entry 71:
For_{fi353;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 1), M1[8,28],h(4, 28, fi353)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 2), M1[8,28],h(4, 28, fi353)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ) ) )
Eq.ann: {}
 )Entry 72:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), M1[8,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), M1[8,28],h(28, 28, 0)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M1[8,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 73:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ) )
Eq.ann: {}
Entry 74:
Eq: Tile( (1, 1), G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) ) = ( Tile( (1, 1), 1[1,1] ) Div Tile( (1, 1), G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ) )
Eq.ann: {}
Entry 75:
For_{fi400;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi400)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 5), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 5), M1[8,28],h(4, 28, fi400)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi400)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M1[8,28],h(4, 28, fi400)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 6), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 6), M1[8,28],h(4, 28, fi400)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M1[8,28],h(4, 28, fi400)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M1[8,28],h(4, 28, fi400)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi400)) ) ) - ( T( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M1[8,28],h(4, 28, fi400)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi400)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi400)) ) ) )
Eq.ann: {}
 )Entry 76:
For_{fi466;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi466)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 4), M3[8,8],h(1, 8, 7)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 7), M1[8,28],h(4, 28, fi466)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M1[8,28],h(4, 28, fi466)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 4), M3[8,8],h(1, 8, 6)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 6), M1[8,28],h(4, 28, fi466)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M1[8,28],h(4, 28, fi466)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M3[8,8],h(1, 8, 5)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 5), M1[8,28],h(4, 28, fi466)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 4), M1[8,28],h(4, 28, fi466)) ) ) )
Eq.ann: {}
 )Entry 77:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M1[8,28],h(28, 28, 0)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M1[8,28],h(28, 28, 0)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(4, 8, 4), M1[8,28],h(28, 28, 0)) ) ) ) )
Eq.ann: {}
Entry 78:
For_{fi513;0;24;4} ( Entry 0:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi513)) ) ) )
Eq.ann: {}
Entry 1:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(3, 8, 0), M3[8,8],h(1, 8, 3)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 3), M1[8,28],h(4, 28, fi513)) ) ) ) )
Eq.ann: {}
Entry 2:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M1[8,28],h(4, 28, fi513)) ) ) )
Eq.ann: {}
Entry 3:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(2, 8, 0), M3[8,8],h(1, 8, 2)) ) ) * Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 2), M1[8,28],h(4, 28, fi513)) ) ) ) )
Eq.ann: {}
Entry 4:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M1[8,28],h(4, 28, fi513)) ) ) )
Eq.ann: {}
Entry 5:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) - ( Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M3[8,8],h(1, 8, 1)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 1), M1[8,28],h(4, 28, fi513)) ) ) ) )
Eq.ann: {}
Entry 6:
Eq: Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) = ( Tile( (1, 1), Tile( (4, 4), G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) ) ) Kro Tile( (1, 1), Tile( (4, 4), G(h(1, 8, 0), M1[8,28],h(4, 28, fi513)) ) ) )
Eq.ann: {}
 )Entry 79:
Eq: Tile( (1, 1), Tile( (4, 4), x[28,1] ) ) = ( Tile( (1, 1), Tile( (4, 4), y[28,1] ) ) + ( Tile( (1, 1), Tile( (4, 4), M2[28,8] ) ) * Tile( (1, 1), Tile( (4, 4), v0[8,1] ) ) ) )
Eq.ann: {}
Entry 80:
Eq: Tile( (1, 1), Tile( (4, 4), P[28,28] ) ) = ( Tile( (1, 1), Tile( (4, 4), Y[28,28] ) ) - ( Tile( (1, 1), Tile( (4, 4), M2[28,8] ) ) * Tile( (1, 1), Tile( (4, 4), M1[8,28] ) ) ) )
Eq.ann: {}
 *
 * Created on: 2017-02-06
 * Author: danieles
 */

#pragma once

#include <x86intrin.h>


#define PARAM0 8
#define PARAM1 8
#define PARAM2 28

#define ERRTHRESH 1e-5

#define NUMREP 30

#define floord(n,d) (((n)<0) ? -((-(n)+(d)-1)/(d)) : (n)/(d))
#define ceild(n,d)  (((n)<0) ? -((-(n))/(d)) : ((n)+(d)-1)/(d))
#define max(x,y)    ((x) > (y) ? (x) : (y))
#define min(x,y)    ((x) < (y) ? (x) : (y))


static __attribute__((noinline)) void kernel(double const * F, double const * B, double const * u, double const * Q, double const * z, double const * H, double const * R, double * y, double * x, double * M0, double * P, double * Y, double * v0, double * M1, double * M2, double * M3)
{
  __m256d _t0_0, _t0_1, _t0_2, _t0_3, _t0_4, _t0_5, _t0_6, _t0_7,
	_t0_8, _t0_9, _t0_10, _t0_11, _t0_12;
  __m256d _t1_0, _t1_1, _t1_2, _t1_3, _t1_4, _t1_5;
  __m256d _t2_0, _t2_1, _t2_2, _t2_3, _t2_4, _t2_5;
  __m256d _t3_0, _t3_1, _t3_2, _t3_3, _t3_4, _t3_5, _t3_6, _t3_7,
	_t3_8, _t3_9, _t3_10, _t3_11, _t3_12, _t3_13, _t3_14, _t3_15,
	_t3_16, _t3_17, _t3_18, _t3_19, _t3_20, _t3_21, _t3_22, _t3_23;
  __m256d _t4_0, _t4_1, _t4_2, _t4_3, _t4_4, _t4_5, _t4_6, _t4_7,
	_t4_8, _t4_9, _t4_10, _t4_11, _t4_12, _t4_13, _t4_14, _t4_15,
	_t4_16, _t4_17, _t4_18, _t4_19;
  __m256d _t5_0, _t5_1, _t5_2, _t5_3, _t5_4, _t5_5, _t5_6, _t5_7,
	_t5_8, _t5_9, _t5_10, _t5_11, _t5_12, _t5_13, _t5_14, _t5_15,
	_t5_16, _t5_17, _t5_18, _t5_19, _t5_20, _t5_21, _t5_22, _t5_23,
	_t5_24, _t5_25, _t5_26, _t5_27;
  __m256d _t6_0, _t6_1, _t6_2, _t6_3, _t6_4, _t6_5, _t6_6, _t6_7,
	_t6_8, _t6_9, _t6_10, _t6_11, _t6_12, _t6_13, _t6_14, _t6_15,
	_t6_16, _t6_17, _t6_18, _t6_19, _t6_20, _t6_21, _t6_22, _t6_23,
	_t6_24, _t6_25, _t6_26, _t6_27;
  __m256d _t7_0, _t7_1, _t7_2, _t7_3, _t7_4, _t7_5, _t7_6, _t7_7,
	_t7_8, _t7_9, _t7_10, _t7_11, _t7_12, _t7_13, _t7_14, _t7_15,
	_t7_16, _t7_17, _t7_18, _t7_19, _t7_20, _t7_21, _t7_22, _t7_23,
	_t7_24, _t7_25, _t7_26, _t7_27;
  __m256d _t8_0, _t8_1, _t8_2, _t8_3, _t8_4, _t8_5, _t8_6, _t8_7;
  __m256d _t9_0, _t9_1, _t9_2, _t9_3, _t9_4, _t9_5, _t9_6, _t9_7,
	_t9_8, _t9_9, _t9_10, _t9_11, _t9_12, _t9_13, _t9_14, _t9_15,
	_t9_16, _t9_17, _t9_18, _t9_19, _t9_20, _t9_21, _t9_22, _t9_23;
  __m256d _t10_0, _t10_1, _t10_2, _t10_3, _t10_4, _t10_5, _t10_6, _t10_7,
	_t10_8, _t10_9, _t10_10, _t10_11, _t10_12, _t10_13, _t10_14, _t10_15,
	_t10_16, _t10_17, _t10_18, _t10_19, _t10_20, _t10_21, _t10_22, _t10_23,
	_t10_24, _t10_25, _t10_26, _t10_27;
  __m256d _t11_0, _t11_1, _t11_2, _t11_3, _t11_4, _t11_5, _t11_6, _t11_7,
	_t11_8, _t11_9, _t11_10, _t11_11, _t11_12, _t11_13, _t11_14, _t11_15,
	_t11_16, _t11_17, _t11_18, _t11_19, _t11_20, _t11_21, _t11_22, _t11_23,
	_t11_24, _t11_25, _t11_26, _t11_27;
  __m256d _t12_0, _t12_1, _t12_2, _t12_3, _t12_4, _t12_5, _t12_6, _t12_7;
  __m256d _t13_0, _t13_1, _t13_2, _t13_3, _t13_4, _t13_5, _t13_6, _t13_7,
	_t13_8, _t13_9, _t13_10, _t13_11, _t13_12, _t13_13, _t13_14, _t13_15,
	_t13_16, _t13_17, _t13_18, _t13_19, _t13_20, _t13_21, _t13_22, _t13_23;
  __m256d _t14_0, _t14_1, _t14_2, _t14_3, _t14_4, _t14_5, _t14_6, _t14_7,
	_t14_8, _t14_9, _t14_10, _t14_11, _t14_12, _t14_13, _t14_14, _t14_15,
	_t14_16, _t14_17, _t14_18, _t14_19;
  __m256d _t15_0, _t15_1, _t15_2, _t15_3, _t15_4, _t15_5, _t15_6, _t15_7,
	_t15_8, _t15_9, _t15_10, _t15_11, _t15_12, _t15_13, _t15_14, _t15_15,
	_t15_16, _t15_17, _t15_18, _t15_19, _t15_20, _t15_21, _t15_22, _t15_23,
	_t15_24, _t15_25, _t15_26, _t15_27, _t15_28, _t15_29, _t15_30, _t15_31,
	_t15_32, _t15_33, _t15_34, _t15_35, _t15_36, _t15_37, _t15_38, _t15_39,
	_t15_40, _t15_41, _t15_42, _t15_43;
  __m256d _t16_0, _t16_1, _t16_2, _t16_3, _t16_4, _t16_5, _t16_6, _t16_7,
	_t16_8, _t16_9, _t16_10, _t16_11, _t16_12, _t16_13, _t16_14, _t16_15,
	_t16_16, _t16_17, _t16_18, _t16_19, _t16_20, _t16_21, _t16_22, _t16_23,
	_t16_24, _t16_25, _t16_26, _t16_27, _t16_28, _t16_29, _t16_30, _t16_31;
  __m256d _t17_0, _t17_1, _t17_2, _t17_3, _t17_4, _t17_5, _t17_6, _t17_7,
	_t17_8, _t17_9, _t17_10, _t17_11, _t17_12, _t17_13, _t17_14, _t17_15,
	_t17_16, _t17_17, _t17_18, _t17_19;
  __m256d _t18_0, _t18_1, _t18_2, _t18_3, _t18_4, _t18_5, _t18_6, _t18_7,
	_t18_8, _t18_9, _t18_10, _t18_11, _t18_12, _t18_13, _t18_14, _t18_15,
	_t18_16, _t18_17, _t18_18, _t18_19, _t18_20, _t18_21, _t18_22, _t18_23,
	_t18_24, _t18_25, _t18_26, _t18_27;
  __m256d _t19_0, _t19_1, _t19_2, _t19_3, _t19_4, _t19_5, _t19_6, _t19_7,
	_t19_8, _t19_9, _t19_10, _t19_11, _t19_12, _t19_13, _t19_14, _t19_15,
	_t19_16, _t19_17, _t19_18, _t19_19, _t19_20, _t19_21, _t19_22, _t19_23,
	_t19_24, _t19_25, _t19_26, _t19_27, _t19_28, _t19_29, _t19_30, _t19_31,
	_t19_32, _t19_33, _t19_34, _t19_35, _t19_36, _t19_37, _t19_38, _t19_39,
	_t19_40, _t19_41, _t19_42, _t19_43;
  __m256d _t20_0, _t20_1, _t20_2, _t20_3, _t20_4, _t20_5, _t20_6, _t20_7,
	_t20_8, _t20_9, _t20_10, _t20_11, _t20_12, _t20_13, _t20_14, _t20_15,
	_t20_16, _t20_17, _t20_18, _t20_19, _t20_20, _t20_21, _t20_22, _t20_23,
	_t20_24, _t20_25, _t20_26, _t20_27, _t20_28, _t20_29, _t20_30, _t20_31;
  __m256d _t21_0, _t21_1, _t21_2, _t21_3, _t21_4, _t21_5, _t21_6, _t21_7;
  __m256d _t22_0, _t22_1, _t22_2, _t22_3, _t22_4, _t22_5;
  __m256d _t23_0, _t23_1, _t23_2, _t23_3, _t23_4, _t23_5, _t23_6, _t23_7,
	_t23_8, _t23_9, _t23_10, _t23_11, _t23_12, _t23_13, _t23_14, _t23_15,
	_t23_16, _t23_17, _t23_18, _t23_19;
  __m256d _t24_0, _t24_1, _t24_2, _t24_3, _t24_4, _t24_5, _t24_6, _t24_7,
	_t24_8, _t24_9, _t24_10, _t24_11, _t24_12, _t24_13, _t24_14, _t24_15,
	_t24_16, _t24_17, _t24_18, _t24_19;
  __m256d _t25_0, _t25_1, _t25_2, _t25_3, _t25_4, _t25_5, _t25_6, _t25_7,
	_t25_8, _t25_9, _t25_10, _t25_11, _t25_12, _t25_13, _t25_14, _t25_15,
	_t25_16, _t25_17, _t25_18, _t25_19, _t25_20, _t25_21, _t25_22, _t25_23,
	_t25_24, _t25_25, _t25_26, _t25_27;
  __m256d _t26_0, _t26_1, _t26_2, _t26_3, _t26_4, _t26_5, _t26_6, _t26_7,
	_t26_8, _t26_9, _t26_10, _t26_11, _t26_12, _t26_13, _t26_14, _t26_15,
	_t26_16, _t26_17, _t26_18, _t26_19, _t26_20, _t26_21, _t26_22, _t26_23,
	_t26_24, _t26_25, _t26_26, _t26_27;
  __m256d _t27_0, _t27_1, _t27_2, _t27_3, _t27_4, _t27_5, _t27_6, _t27_7,
	_t27_8, _t27_9, _t27_10, _t27_11, _t27_12, _t27_13, _t27_14, _t27_15,
	_t27_16, _t27_17, _t27_18, _t27_19, _t27_20, _t27_21, _t27_22, _t27_23,
	_t27_24, _t27_25, _t27_26, _t27_27;
  __m256d _t28_0, _t28_1, _t28_2, _t28_3, _t28_4, _t28_5, _t28_6, _t28_7;
  __m256d _t29_0, _t29_1, _t29_2, _t29_3, _t29_4, _t29_5, _t29_6, _t29_7,
	_t29_8, _t29_9, _t29_10, _t29_11, _t29_12, _t29_13, _t29_14, _t29_15,
	_t29_16, _t29_17, _t29_18, _t29_19, _t29_20, _t29_21, _t29_22, _t29_23;
  __m256d _t30_0, _t30_1, _t30_2, _t30_3, _t30_4, _t30_5, _t30_6, _t30_7,
	_t30_8, _t30_9, _t30_10, _t30_11, _t30_12, _t30_13, _t30_14, _t30_15,
	_t30_16, _t30_17, _t30_18, _t30_19, _t30_20, _t30_21, _t30_22, _t30_23,
	_t30_24, _t30_25, _t30_26, _t30_27;
  __m256d _t31_0, _t31_1, _t31_2, _t31_3, _t31_4, _t31_5, _t31_6, _t31_7,
	_t31_8, _t31_9, _t31_10, _t31_11, _t31_12, _t31_13, _t31_14, _t31_15,
	_t31_16, _t31_17, _t31_18, _t31_19, _t31_20, _t31_21, _t31_22, _t31_23,
	_t31_24, _t31_25, _t31_26, _t31_27;
  __m256d _t32_0, _t32_1, _t32_2, _t32_3, _t32_4, _t32_5, _t32_6, _t32_7;
  __m256d _t33_0, _t33_1, _t33_2, _t33_3, _t33_4, _t33_5, _t33_6, _t33_7,
	_t33_8, _t33_9, _t33_10, _t33_11, _t33_12, _t33_13, _t33_14, _t33_15,
	_t33_16, _t33_17, _t33_18, _t33_19, _t33_20, _t33_21, _t33_22, _t33_23;
  __m256d _t34_0, _t34_1, _t34_2, _t34_3, _t34_4, _t34_5, _t34_6, _t34_7,
	_t34_8, _t34_9, _t34_10, _t34_11, _t34_12, _t34_13, _t34_14, _t34_15,
	_t34_16, _t34_17, _t34_18, _t34_19;
  __m256d _t35_0, _t35_1, _t35_2, _t35_3;
  __m256d _t36_0, _t36_1, _t36_2, _t36_3, _t36_4, _t36_5, _t36_6, _t36_7,
	_t36_8, _t36_9, _t36_10, _t36_11, _t36_12, _t36_13, _t36_14, _t36_15,
	_t36_16, _t36_17, _t36_18, _t36_19, _t36_20, _t36_21, _t36_22, _t36_23,
	_t36_24, _t36_25, _t36_26, _t36_27;
  __m256d _t37_0, _t37_1, _t37_2, _t37_3, _t37_4, _t37_5, _t37_6, _t37_7,
	_t37_8, _t37_9, _t37_10, _t37_11, _t37_12, _t37_13, _t37_14, _t37_15,
	_t37_16, _t37_17, _t37_18, _t37_19, _t37_20, _t37_21, _t37_22, _t37_23,
	_t37_24, _t37_25, _t37_26, _t37_27;
  __m256d _t38_0, _t38_1, _t38_2, _t38_3, _t38_4, _t38_5, _t38_6, _t38_7,
	_t38_8, _t38_9, _t38_10, _t38_11;
  __m256d _t39_0, _t39_1, _t39_2, _t39_3, _t39_4, _t39_5, _t39_6, _t39_7,
	_t39_8, _t39_9, _t39_10, _t39_11, _t39_12, _t39_13, _t39_14, _t39_15,
	_t39_16, _t39_17, _t39_18, _t39_19, _t39_20, _t39_21, _t39_22, _t39_23,
	_t39_24, _t39_25, _t39_26, _t39_27, _t39_28, _t39_29, _t39_30, _t39_31,
	_t39_32, _t39_33, _t39_34, _t39_35, _t39_36, _t39_37, _t39_38, _t39_39,
	_t39_40, _t39_41, _t39_42, _t39_43, _t39_44, _t39_45, _t39_46, _t39_47,
	_t39_48, _t39_49, _t39_50, _t39_51, _t39_52, _t39_53, _t39_54, _t39_55;
  __m256d _t40_0, _t40_1, _t40_2, _t40_3, _t40_4, _t40_5, _t40_6, _t40_7,
	_t40_8, _t40_9, _t40_10, _t40_11, _t40_12, _t40_13, _t40_14, _t40_15,
	_t40_16, _t40_17, _t40_18, _t40_19, _t40_20, _t40_21, _t40_22, _t40_23,
	_t40_24, _t40_25, _t40_26, _t40_27;
  __m256d _t41_0, _t41_1, _t41_2, _t41_3, _t41_4, _t41_5, _t41_6, _t41_7,
	_t41_8, _t41_9, _t41_10, _t41_11, _t41_12, _t41_13, _t41_14, _t41_15;
  __m256d _t42_0, _t42_1, _t42_2, _t42_3, _t42_4, _t42_5, _t42_6, _t42_7,
	_t42_8, _t42_9, _t42_10, _t42_11, _t42_12, _t42_13, _t42_14, _t42_15,
	_t42_16, _t42_17, _t42_18, _t42_19, _t42_20, _t42_21, _t42_22, _t42_23,
	_t42_24, _t42_25, _t42_26, _t42_27, _t42_28, _t42_29, _t42_30, _t42_31;
  __m256d _t43_0, _t43_1, _t43_2, _t43_3, _t43_4, _t43_5, _t43_6, _t43_7,
	_t43_8, _t43_9, _t43_10, _t43_11, _t43_12, _t43_13, _t43_14, _t43_15,
	_t43_16, _t43_17, _t43_18, _t43_19, _t43_20, _t43_21, _t43_22, _t43_23,
	_t43_24, _t43_25, _t43_26, _t43_27, _t43_28, _t43_29, _t43_30, _t43_31,
	_t43_32, _t43_33, _t43_34, _t43_35;
  __m256d _t44_0, _t44_1, _t44_2, _t44_3, _t44_4, _t44_5, _t44_6, _t44_7,
	_t44_8, _t44_9, _t44_10, _t44_11, _t44_12, _t44_13, _t44_14, _t44_15,
	_t44_16, _t44_17, _t44_18, _t44_19, _t44_20, _t44_21, _t44_22, _t44_23,
	_t44_24, _t44_25, _t44_26, _t44_27, _t44_28, _t44_29, _t44_30, _t44_31;
  __m256d _t45_0, _t45_1, _t45_2, _t45_3, _t45_4, _t45_5, _t45_6, _t45_7,
	_t45_8, _t45_9, _t45_10, _t45_11, _t45_12, _t45_13, _t45_14, _t45_15,
	_t45_16, _t45_17, _t45_18, _t45_19, _t45_20, _t45_21, _t45_22, _t45_23,
	_t45_24, _t45_25, _t45_26, _t45_27;
  __m256d _t46_0, _t46_1, _t46_2, _t46_3, _t46_4, _t46_5, _t46_6, _t46_7,
	_t46_8, _t46_9, _t46_10, _t46_11;
  __m256d _t47_0, _t47_1, _t47_2, _t47_3, _t47_4, _t47_5, _t47_6, _t47_7,
	_t47_8, _t47_9, _t47_10, _t47_11, _t47_12, _t47_13, _t47_14, _t47_15,
	_t47_16, _t47_17, _t47_18, _t47_19, _t47_20, _t47_21, _t47_22, _t47_23,
	_t47_24, _t47_25, _t47_26, _t47_27;
  __m256d _t48_0, _t48_1, _t48_2, _t48_3, _t48_4, _t48_5, _t48_6, _t48_7,
	_t48_8, _t48_9, _t48_10, _t48_11, _t48_12, _t48_13, _t48_14, _t48_15,
	_t48_16, _t48_17, _t48_18, _t48_19, _t48_20, _t48_21, _t48_22, _t48_23,
	_t48_24, _t48_25, _t48_26, _t48_27, _t48_28, _t48_29, _t48_30, _t48_31,
	_t48_32, _t48_33, _t48_34, _t48_35;
  __m256d _t49_0, _t49_1, _t49_2, _t49_3, _t49_4, _t49_5, _t49_6, _t49_7,
	_t49_8, _t49_9, _t49_10, _t49_11, _t49_12, _t49_13, _t49_14, _t49_15,
	_t49_16, _t49_17, _t49_18, _t49_19, _t49_20, _t49_21, _t49_22, _t49_23,
	_t49_24, _t49_25, _t49_26, _t49_27;
  __m256d _t50_0, _t50_1, _t50_2, _t50_3, _t50_4, _t50_5, _t50_6, _t50_7,
	_t50_8, _t50_9, _t50_10, _t50_11, _t50_12, _t50_13, _t50_14, _t50_15,
	_t50_16, _t50_17, _t50_18, _t50_19, _t50_20, _t50_21, _t50_22, _t50_23,
	_t50_24, _t50_25, _t50_26, _t50_27, _t50_28, _t50_29, _t50_30, _t50_31,
	_t50_32, _t50_33, _t50_34, _t50_35, _t50_36, _t50_37, _t50_38, _t50_39,
	_t50_40, _t50_41, _t50_42, _t50_43;
  __m256d _t51_0, _t51_1, _t51_2, _t51_3, _t51_4, _t51_5, _t51_6, _t51_7,
	_t51_8, _t51_9, _t51_10, _t51_11, _t51_12, _t51_13, _t51_14, _t51_15,
	_t51_16, _t51_17, _t51_18, _t51_19, _t51_20, _t51_21, _t51_22, _t51_23,
	_t51_24, _t51_25, _t51_26, _t51_27, _t51_28, _t51_29, _t51_30, _t51_31;
  __m256d _t52_0, _t52_1, _t52_2, _t52_3, _t52_4, _t52_5, _t52_6, _t52_7,
	_t52_8, _t52_9, _t52_10, _t52_11, _t52_12, _t52_13, _t52_14, _t52_15,
	_t52_16, _t52_17, _t52_18, _t52_19;
  __m256d _t53_0, _t53_1, _t53_2, _t53_3, _t53_4, _t53_5, _t53_6, _t53_7,
	_t53_8, _t53_9, _t53_10, _t53_11, _t53_12, _t53_13, _t53_14, _t53_15,
	_t53_16, _t53_17, _t53_18, _t53_19, _t53_20, _t53_21, _t53_22, _t53_23,
	_t53_24, _t53_25, _t53_26, _t53_27;
  __m256d _t54_0, _t54_1, _t54_2, _t54_3, _t54_4, _t54_5, _t54_6, _t54_7,
	_t54_8, _t54_9, _t54_10, _t54_11, _t54_12, _t54_13, _t54_14, _t54_15,
	_t54_16, _t54_17, _t54_18, _t54_19, _t54_20, _t54_21, _t54_22, _t54_23,
	_t54_24, _t54_25, _t54_26, _t54_27, _t54_28, _t54_29, _t54_30, _t54_31,
	_t54_32, _t54_33, _t54_34, _t54_35, _t54_36, _t54_37, _t54_38, _t54_39;
  __m256d _t55_0, _t55_1, _t55_2, _t55_3, _t55_4, _t55_5, _t55_6, _t55_7,
	_t55_8, _t55_9, _t55_10, _t55_11, _t55_12, _t55_13, _t55_14, _t55_15,
	_t55_16, _t55_17, _t55_18, _t55_19, _t55_20, _t55_21, _t55_22, _t55_23,
	_t55_24, _t55_25, _t55_26, _t55_27, _t55_28, _t55_29, _t55_30, _t55_31;
  __m256d _t56_0, _t56_1, _t56_2, _t56_3, _t56_4, _t56_5, _t56_6, _t56_7,
	_t56_8, _t56_9, _t56_10, _t56_11, _t56_12, _t56_13, _t56_14, _t56_15,
	_t56_16, _t56_17, _t56_18, _t56_19, _t56_20, _t56_21, _t56_22, _t56_23,
	_t56_24, _t56_25, _t56_26, _t56_27, _t56_28, _t56_29, _t56_30, _t56_31,
	_t56_32, _t56_33, _t56_34, _t56_35, _t56_36, _t56_37, _t56_38, _t56_39,
	_t56_40, _t56_41, _t56_42, _t56_43, _t56_44, _t56_45, _t56_46, _t56_47,
	_t56_48, _t56_49, _t56_50, _t56_51, _t56_52, _t56_53, _t56_54, _t56_55,
	_t56_56, _t56_57, _t56_58, _t56_59, _t56_60, _t56_61, _t56_62, _t56_63,
	_t56_64, _t56_65, _t56_66, _t56_67, _t56_68, _t56_69, _t56_70, _t56_71,
	_t56_72, _t56_73, _t56_74, _t56_75, _t56_76, _t56_77, _t56_78, _t56_79,
	_t56_80, _t56_81, _t56_82, _t56_83, _t56_84, _t56_85, _t56_86, _t56_87,
	_t56_88, _t56_89, _t56_90, _t56_91, _t56_92, _t56_93, _t56_94, _t56_95,
	_t56_96, _t56_97, _t56_98, _t56_99, _t56_100, _t56_101, _t56_102, _t56_103,
	_t56_104, _t56_105, _t56_106, _t56_107, _t56_108, _t56_109, _t56_110, _t56_111,
	_t56_112, _t56_113, _t56_114, _t56_115, _t56_116, _t56_117, _t56_118, _t56_119,
	_t56_120, _t56_121, _t56_122, _t56_123, _t56_124, _t56_125, _t56_126, _t56_127,
	_t56_128, _t56_129, _t56_130, _t56_131, _t56_132, _t56_133, _t56_134, _t56_135,
	_t56_136, _t56_137, _t56_138, _t56_139, _t56_140, _t56_141, _t56_142, _t56_143,
	_t56_144, _t56_145, _t56_146, _t56_147, _t56_148, _t56_149, _t56_150, _t56_151,
	_t56_152, _t56_153, _t56_154, _t56_155, _t56_156, _t56_157, _t56_158, _t56_159,
	_t56_160, _t56_161, _t56_162, _t56_163, _t56_164, _t56_165, _t56_166, _t56_167,
	_t56_168, _t56_169, _t56_170, _t56_171, _t56_172, _t56_173, _t56_174, _t56_175,
	_t56_176, _t56_177, _t56_178, _t56_179, _t56_180, _t56_181, _t56_182, _t56_183,
	_t56_184, _t56_185, _t56_186, _t56_187, _t56_188, _t56_189, _t56_190, _t56_191,
	_t56_192, _t56_193, _t56_194, _t56_195, _t56_196, _t56_197, _t56_198, _t56_199,
	_t56_200, _t56_201, _t56_202, _t56_203, _t56_204, _t56_205, _t56_206, _t56_207,
	_t56_208, _t56_209, _t56_210, _t56_211, _t56_212, _t56_213, _t56_214, _t56_215,
	_t56_216, _t56_217, _t56_218, _t56_219, _t56_220, _t56_221, _t56_222, _t56_223,
	_t56_224, _t56_225, _t56_226, _t56_227, _t56_228, _t56_229, _t56_230, _t56_231,
	_t56_232, _t56_233, _t56_234, _t56_235, _t56_236, _t56_237, _t56_238, _t56_239,
	_t56_240, _t56_241, _t56_242, _t56_243, _t56_244, _t56_245, _t56_246, _t56_247,
	_t56_248, _t56_249, _t56_250, _t56_251, _t56_252, _t56_253, _t56_254, _t56_255,
	_t56_256, _t56_257, _t56_258, _t56_259, _t56_260, _t56_261, _t56_262, _t56_263,
	_t56_264, _t56_265, _t56_266, _t56_267, _t56_268, _t56_269, _t56_270, _t56_271,
	_t56_272, _t56_273, _t56_274, _t56_275, _t56_276, _t56_277, _t56_278, _t56_279,
	_t56_280, _t56_281, _t56_282, _t56_283, _t56_284, _t56_285, _t56_286, _t56_287,
	_t56_288, _t56_289, _t56_290, _t56_291, _t56_292, _t56_293, _t56_294, _t56_295,
	_t56_296, _t56_297, _t56_298, _t56_299, _t56_300, _t56_301, _t56_302, _t56_303,
	_t56_304, _t56_305, _t56_306, _t56_307, _t56_308, _t56_309, _t56_310, _t56_311,
	_t56_312, _t56_313, _t56_314, _t56_315, _t56_316, _t56_317, _t56_318, _t56_319,
	_t56_320, _t56_321, _t56_322, _t56_323, _t56_324, _t56_325, _t56_326, _t56_327,
	_t56_328, _t56_329, _t56_330, _t56_331, _t56_332, _t56_333, _t56_334, _t56_335,
	_t56_336, _t56_337, _t56_338, _t56_339, _t56_340, _t56_341, _t56_342, _t56_343,
	_t56_344, _t56_345, _t56_346, _t56_347, _t56_348, _t56_349, _t56_350, _t56_351,
	_t56_352, _t56_353, _t56_354, _t56_355, _t56_356, _t56_357, _t56_358, _t56_359,
	_t56_360, _t56_361, _t56_362, _t56_363, _t56_364, _t56_365, _t56_366, _t56_367,
	_t56_368, _t56_369, _t56_370, _t56_371, _t56_372, _t56_373, _t56_374, _t56_375,
	_t56_376, _t56_377, _t56_378, _t56_379, _t56_380, _t56_381, _t56_382, _t56_383,
	_t56_384, _t56_385, _t56_386, _t56_387, _t56_388, _t56_389, _t56_390, _t56_391,
	_t56_392, _t56_393, _t56_394, _t56_395, _t56_396, _t56_397, _t56_398, _t56_399,
	_t56_400, _t56_401, _t56_402, _t56_403, _t56_404, _t56_405, _t56_406, _t56_407,
	_t56_408, _t56_409, _t56_410, _t56_411, _t56_412, _t56_413, _t56_414, _t56_415,
	_t56_416, _t56_417, _t56_418, _t56_419, _t56_420, _t56_421, _t56_422, _t56_423,
	_t56_424, _t56_425, _t56_426, _t56_427, _t56_428, _t56_429, _t56_430, _t56_431,
	_t56_432, _t56_433, _t56_434, _t56_435, _t56_436, _t56_437, _t56_438, _t56_439,
	_t56_440, _t56_441, _t56_442, _t56_443, _t56_444, _t56_445, _t56_446;
  __m256d _t57_0, _t57_1, _t57_2, _t57_3, _t57_4, _t57_5, _t57_6, _t57_7,
	_t57_8, _t57_9, _t57_10, _t57_11, _t57_12, _t57_13, _t57_14, _t57_15,
	_t57_16, _t57_17, _t57_18, _t57_19, _t57_20, _t57_21, _t57_22, _t57_23,
	_t57_24, _t57_25, _t57_26, _t57_27, _t57_28;
  __m256d _t58_0, _t58_1, _t58_2, _t58_3;
  __m256d _t59_0, _t59_1, _t59_2, _t59_3, _t59_4, _t59_5, _t59_6, _t59_7,
	_t59_8, _t59_9, _t59_10, _t59_11, _t59_12, _t59_13, _t59_14, _t59_15,
	_t59_16, _t59_17, _t59_18, _t59_19, _t59_20, _t59_21, _t59_22, _t59_23,
	_t59_24, _t59_25, _t59_26, _t59_27;
  __m256d _t60_0, _t60_1, _t60_2, _t60_3, _t60_4, _t60_5, _t60_6, _t60_7,
	_t60_8, _t60_9, _t60_10, _t60_11, _t60_12, _t60_13, _t60_14, _t60_15,
	_t60_16, _t60_17, _t60_18, _t60_19, _t60_20, _t60_21, _t60_22, _t60_23,
	_t60_24, _t60_25, _t60_26, _t60_27, _t60_28, _t60_29, _t60_30, _t60_31,
	_t60_32, _t60_33, _t60_34, _t60_35, _t60_36, _t60_37, _t60_38, _t60_39,
	_t60_40, _t60_41, _t60_42, _t60_43, _t60_44, _t60_45, _t60_46, _t60_47,
	_t60_48, _t60_49, _t60_50, _t60_51, _t60_52, _t60_53, _t60_54, _t60_55,
	_t60_56, _t60_57;
  __m256d _t61_0, _t61_1, _t61_2, _t61_3, _t61_4, _t61_5, _t61_6, _t61_7,
	_t61_8, _t61_9, _t61_10, _t61_11, _t61_12, _t61_13, _t61_14, _t61_15,
	_t61_16, _t61_17, _t61_18, _t61_19, _t61_20, _t61_21, _t61_22, _t61_23,
	_t61_24, _t61_25, _t61_26, _t61_27, _t61_28;
  __m256d _t62_0, _t62_1, _t62_2, _t62_3, _t62_4, _t62_5, _t62_6, _t62_7,
	_t62_8, _t62_9, _t62_10, _t62_11, _t62_12, _t62_13, _t62_14, _t62_15,
	_t62_16, _t62_17, _t62_18, _t62_19, _t62_20, _t62_21, _t62_22, _t62_23,
	_t62_24, _t62_25, _t62_26, _t62_27, _t62_28, _t62_29, _t62_30, _t62_31,
	_t62_32, _t62_33, _t62_34, _t62_35, _t62_36, _t62_37, _t62_38, _t62_39,
	_t62_40;
  __m256d _t63_0, _t63_1, _t63_2, _t63_3, _t63_4, _t63_5, _t63_6, _t63_7,
	_t63_8, _t63_9, _t63_10, _t63_11, _t63_12, _t63_13, _t63_14, _t63_15,
	_t63_16, _t63_17, _t63_18, _t63_19, _t63_20, _t63_21, _t63_22, _t63_23,
	_t63_24, _t63_25, _t63_26, _t63_27, _t63_28, _t63_29, _t63_30, _t63_31,
	_t63_32, _t63_33, _t63_34, _t63_35;
  __m256d _t64_0, _t64_1, _t64_2, _t64_3, _t64_4, _t64_5, _t64_6, _t64_7,
	_t64_8, _t64_9, _t64_10, _t64_11, _t64_12, _t64_13, _t64_14, _t64_15,
	_t64_16, _t64_17, _t64_18, _t64_19, _t64_20, _t64_21, _t64_22, _t64_23,
	_t64_24, _t64_25, _t64_26, _t64_27;
  __m256d _t65_0, _t65_1, _t65_2, _t65_3, _t65_4, _t65_5, _t65_6, _t65_7,
	_t65_8, _t65_9, _t65_10, _t65_11, _t65_12, _t65_13, _t65_14, _t65_15,
	_t65_16, _t65_17, _t65_18, _t65_19, _t65_20, _t65_21, _t65_22, _t65_23,
	_t65_24, _t65_25, _t65_26, _t65_27, _t65_28, _t65_29, _t65_30, _t65_31,
	_t65_32, _t65_33, _t65_34, _t65_35, _t65_36, _t65_37, _t65_38, _t65_39,
	_t65_40;
  __m256d _t66_0, _t66_1, _t66_2, _t66_3, _t66_4, _t66_5, _t66_6, _t66_7,
	_t66_8, _t66_9, _t66_10, _t66_11, _t66_12, _t66_13, _t66_14, _t66_15,
	_t66_16, _t66_17, _t66_18, _t66_19, _t66_20, _t66_21, _t66_22, _t66_23,
	_t66_24, _t66_25, _t66_26, _t66_27, _t66_28, _t66_29, _t66_30, _t66_31,
	_t66_32, _t66_33, _t66_34, _t66_35;
  __m256d _t67_0, _t67_1, _t67_2, _t67_3, _t67_4, _t67_5, _t67_6, _t67_7,
	_t67_8, _t67_9, _t67_10, _t67_11, _t67_12, _t67_13, _t67_14, _t67_15;
  __m256d _t68_0, _t68_1, _t68_2, _t68_3, _t68_4, _t68_5, _t68_6, _t68_7,
	_t68_8, _t68_9, _t68_10, _t68_11, _t68_12, _t68_13, _t68_14, _t68_15,
	_t68_16, _t68_17, _t68_18, _t68_19, _t68_20, _t68_21, _t68_22, _t68_23,
	_t68_24, _t68_25, _t68_26, _t68_27, _t68_28, _t68_29, _t68_30, _t68_31,
	_t68_32, _t68_33, _t68_34, _t68_35, _t68_36, _t68_37, _t68_38, _t68_39,
	_t68_40, _t68_41, _t68_42, _t68_43, _t68_44, _t68_45, _t68_46, _t68_47,
	_t68_48, _t68_49, _t68_50, _t68_51, _t68_52, _t68_53, _t68_54, _t68_55,
	_t68_56, _t68_57, _t68_58, _t68_59, _t68_60, _t68_61, _t68_62, _t68_63,
	_t68_64, _t68_65, _t68_66, _t68_67;
  __m256d _t69_0, _t69_1, _t69_2, _t69_3, _t69_4, _t69_5, _t69_6, _t69_7,
	_t69_8, _t69_9, _t69_10, _t69_11, _t69_12, _t69_13, _t69_14, _t69_15,
	_t69_16, _t69_17, _t69_18, _t69_19, _t69_20, _t69_21, _t69_22, _t69_23;
  __m256d _t70_0, _t70_1, _t70_2, _t70_3, _t70_4, _t70_5, _t70_6, _t70_7,
	_t70_8, _t70_9, _t70_10, _t70_11, _t70_12, _t70_13, _t70_14, _t70_15,
	_t70_16, _t70_17, _t70_18, _t70_19, _t70_20, _t70_21, _t70_22, _t70_23,
	_t70_24, _t70_25, _t70_26, _t70_27, _t70_28, _t70_29, _t70_30, _t70_31,
	_t70_32, _t70_33, _t70_34, _t70_35, _t70_36, _t70_37, _t70_38, _t70_39,
	_t70_40, _t70_41, _t70_42, _t70_43, _t70_44, _t70_45, _t70_46, _t70_47,
	_t70_48, _t70_49, _t70_50, _t70_51, _t70_52, _t70_53, _t70_54, _t70_55,
	_t70_56, _t70_57, _t70_58, _t70_59;


  // Generating : y[28,1] = Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), x[28,1],h(1, 1, 0)) ) + ( G(h(4, 28, i0), B[28,8],h(4, 8, 0)) * G(h(4, 8, 0), u[8,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), x[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), B[28,8],h(4, 8, 4)) * G(h(4, 8, 4), u[8,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t0_9 = _mm256_loadu_pd(F + 28*i0);
    _t0_8 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t0_7 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t0_6 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t0_5 = _mm256_loadu_pd(x);
    _t0_4 = _mm256_loadu_pd(B + 8*i0);
    _t0_3 = _mm256_loadu_pd(B + 8*i0 + 8);
    _t0_2 = _mm256_loadu_pd(B + 8*i0 + 16);
    _t0_1 = _mm256_loadu_pd(B + 8*i0 + 24);
    _t0_0 = _mm256_loadu_pd(u);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_11 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_9, _t0_5), _mm256_mul_pd(_t0_8, _t0_5)), _mm256_hadd_pd(_mm256_mul_pd(_t0_7, _t0_5), _mm256_mul_pd(_t0_6, _t0_5)), 12));

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t0_12 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t0_4, _t0_0), _mm256_mul_pd(_t0_3, _t0_0)), _mm256_hadd_pd(_mm256_mul_pd(_t0_2, _t0_0), _mm256_mul_pd(_t0_1, _t0_0)), 12));

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_11, _t0_12);

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t1_4 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t1_3 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t1_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t1_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);
      _t1_0 = _mm256_loadu_pd(x + k2);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t1_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t1_4, _t1_0), _mm256_mul_pd(_t1_3, _t1_0)), _mm256_hadd_pd(_mm256_mul_pd(_t1_2, _t1_0), _mm256_mul_pd(_t1_1, _t1_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 + 4x1
      _t0_10 = _mm256_add_pd(_t0_10, _t1_5);

      // AVX Storer:
    }
    _t2_4 = _mm256_loadu_pd(B + 8*i0 + 4);
    _t2_3 = _mm256_loadu_pd(B + 8*i0 + 12);
    _t2_2 = _mm256_loadu_pd(B + 8*i0 + 20);
    _t2_1 = _mm256_loadu_pd(B + 8*i0 + 28);
    _t2_0 = _mm256_loadu_pd(u + 4);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t2_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t2_4, _t2_0), _mm256_mul_pd(_t2_3, _t2_0)), _mm256_hadd_pd(_mm256_mul_pd(_t2_2, _t2_0), _mm256_mul_pd(_t2_1, _t2_0)), 12));

    // AVX Loader:

    // 4-BLAC: 4x1 + 4x1
    _t0_10 = _mm256_add_pd(_t0_10, _t2_5);

    // AVX Storer:
    _mm256_storeu_pd(y + i0, _t0_10);
  }

  _t3_11 = _mm256_loadu_pd(P);
  _t3_10 = _mm256_maskload_pd(P + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_9 = _mm256_maskload_pd(P + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_8 = _mm256_maskload_pd(P + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t3_7 = _mm256_loadu_pd(P + 116);
  _t3_6 = _mm256_maskload_pd(P + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_5 = _mm256_maskload_pd(P + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_4 = _mm256_maskload_pd(P + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t3_3 = _mm256_loadu_pd(P + 696);
  _t3_2 = _mm256_maskload_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t3_1 = _mm256_maskload_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t3_0 = _mm256_maskload_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M0[28,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, 0), P[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 4)) * G(h(4, 28, 4), P[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, 4), P[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{k3} ( ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k3)) * G(h(4, 28, k3), P[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * T( G(h(4, 28, k3), P[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) * G(h(4, 28, 0), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) * G(h(4, 28, k2), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), F[28,28],h(4, 28, 24)) * G(h(4, 28, 24), P[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_12 = _t3_11;
  _t3_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 3), _t3_10, 12);
  _t3_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 0), _t3_9, 49);
  _t3_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_11, _t3_10, 12), _mm256_shuffle_pd(_t3_9, _t3_8, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_16 = _t3_7;
  _t3_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 3), _t3_6, 12);
  _t3_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 0), _t3_5, 49);
  _t3_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_7, _t3_6, 12), _mm256_shuffle_pd(_t3_5, _t3_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t3_20 = _t3_3;
  _t3_21 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t3_22 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t3_23 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);


  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t4_15 = _mm256_broadcast_sd(F + 28*i0);
    _t4_14 = _mm256_broadcast_sd(F + 28*i0 + 1);
    _t4_13 = _mm256_broadcast_sd(F + 28*i0 + 2);
    _t4_12 = _mm256_broadcast_sd(F + 28*i0 + 3);
    _t4_11 = _mm256_broadcast_sd(F + 28*i0 + 28);
    _t4_10 = _mm256_broadcast_sd(F + 28*i0 + 29);
    _t4_9 = _mm256_broadcast_sd(F + 28*i0 + 30);
    _t4_8 = _mm256_broadcast_sd(F + 28*i0 + 31);
    _t4_7 = _mm256_broadcast_sd(F + 28*i0 + 56);
    _t4_6 = _mm256_broadcast_sd(F + 28*i0 + 57);
    _t4_5 = _mm256_broadcast_sd(F + 28*i0 + 58);
    _t4_4 = _mm256_broadcast_sd(F + 28*i0 + 59);
    _t4_3 = _mm256_broadcast_sd(F + 28*i0 + 84);
    _t4_2 = _mm256_broadcast_sd(F + 28*i0 + 85);
    _t4_1 = _mm256_broadcast_sd(F + 28*i0 + 86);
    _t4_0 = _mm256_broadcast_sd(F + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t4_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t3_12), _mm256_mul_pd(_t4_14, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t3_14), _mm256_mul_pd(_t4_12, _t3_15)));
    _t4_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t3_12), _mm256_mul_pd(_t4_10, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t3_14), _mm256_mul_pd(_t4_8, _t3_15)));
    _t4_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t3_12), _mm256_mul_pd(_t4_6, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t3_14), _mm256_mul_pd(_t4_4, _t3_15)));
    _t4_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t3_12), _mm256_mul_pd(_t4_2, _t3_13)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t3_14), _mm256_mul_pd(_t4_0, _t3_15)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t5_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t5_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t5_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t5_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t5_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t5_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t5_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t5_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t5_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t5_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t5_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t5_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t5_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t5_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t5_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t5_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t5_3 = _mm256_loadu_pd(P + k2);
      _t5_2 = _mm256_loadu_pd(P + k2 + 28);
      _t5_1 = _mm256_loadu_pd(P + k2 + 56);
      _t5_0 = _mm256_loadu_pd(P + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t5_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 32);
      _t5_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 32);
      _t5_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t5_3, _t5_2), _mm256_unpacklo_pd(_t5_1, _t5_0), 49);
      _t5_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t5_3, _t5_2), _mm256_unpackhi_pd(_t5_1, _t5_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t5_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_19, _t5_24), _mm256_mul_pd(_t5_18, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_17, _t5_26), _mm256_mul_pd(_t5_16, _t5_27)));
      _t5_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_15, _t5_24), _mm256_mul_pd(_t5_14, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_13, _t5_26), _mm256_mul_pd(_t5_12, _t5_27)));
      _t5_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_11, _t5_24), _mm256_mul_pd(_t5_10, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_9, _t5_26), _mm256_mul_pd(_t5_8, _t5_27)));
      _t5_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t5_7, _t5_24), _mm256_mul_pd(_t5_6, _t5_25)), _mm256_add_pd(_mm256_mul_pd(_t5_5, _t5_26), _mm256_mul_pd(_t5_4, _t5_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t4_16 = _mm256_add_pd(_t4_16, _t5_20);
      _t4_17 = _mm256_add_pd(_t4_17, _t5_21);
      _t4_18 = _mm256_add_pd(_t4_18, _t5_22);
      _t4_19 = _mm256_add_pd(_t4_19, _t5_23);

      // AVX Storer:
    }
    _t6_19 = _mm256_loadu_pd(P + 4);
    _t6_18 = _mm256_loadu_pd(P + 32);
    _t6_17 = _mm256_loadu_pd(P + 60);
    _t6_16 = _mm256_loadu_pd(P + 88);
    _t6_15 = _mm256_broadcast_sd(F + 28*i0 + 4);
    _t6_14 = _mm256_broadcast_sd(F + 28*i0 + 5);
    _t6_13 = _mm256_broadcast_sd(F + 28*i0 + 6);
    _t6_12 = _mm256_broadcast_sd(F + 28*i0 + 7);
    _t6_11 = _mm256_broadcast_sd(F + 28*i0 + 32);
    _t6_10 = _mm256_broadcast_sd(F + 28*i0 + 33);
    _t6_9 = _mm256_broadcast_sd(F + 28*i0 + 34);
    _t6_8 = _mm256_broadcast_sd(F + 28*i0 + 35);
    _t6_7 = _mm256_broadcast_sd(F + 28*i0 + 60);
    _t6_6 = _mm256_broadcast_sd(F + 28*i0 + 61);
    _t6_5 = _mm256_broadcast_sd(F + 28*i0 + 62);
    _t6_4 = _mm256_broadcast_sd(F + 28*i0 + 63);
    _t6_3 = _mm256_broadcast_sd(F + 28*i0 + 88);
    _t6_2 = _mm256_broadcast_sd(F + 28*i0 + 89);
    _t6_1 = _mm256_broadcast_sd(F + 28*i0 + 90);
    _t6_0 = _mm256_broadcast_sd(F + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t6_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t6_19), _mm256_mul_pd(_t4_14, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t6_17), _mm256_mul_pd(_t4_12, _t6_16)));
    _t6_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t6_19), _mm256_mul_pd(_t4_10, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t6_17), _mm256_mul_pd(_t4_8, _t6_16)));
    _t6_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t6_19), _mm256_mul_pd(_t4_6, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t6_17), _mm256_mul_pd(_t4_4, _t6_16)));
    _t6_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t6_19), _mm256_mul_pd(_t4_2, _t6_18)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t6_17), _mm256_mul_pd(_t4_0, _t6_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t6_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_15, _t3_16), _mm256_mul_pd(_t6_14, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_13, _t3_18), _mm256_mul_pd(_t6_12, _t3_19)));
    _t6_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_11, _t3_16), _mm256_mul_pd(_t6_10, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_9, _t3_18), _mm256_mul_pd(_t6_8, _t3_19)));
    _t6_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_7, _t3_16), _mm256_mul_pd(_t6_6, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_5, _t3_18), _mm256_mul_pd(_t6_4, _t3_19)));
    _t6_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t6_3, _t3_16), _mm256_mul_pd(_t6_2, _t3_17)), _mm256_add_pd(_mm256_mul_pd(_t6_1, _t3_18), _mm256_mul_pd(_t6_0, _t3_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t6_20 = _mm256_add_pd(_t6_20, _t6_24);
    _t6_21 = _mm256_add_pd(_t6_21, _t6_25);
    _t6_22 = _mm256_add_pd(_t6_22, _t6_26);
    _t6_23 = _mm256_add_pd(_t6_23, _t6_27);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t7_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t7_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t7_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t7_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t7_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t7_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t7_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t7_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t7_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t7_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t7_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t7_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t7_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t7_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t7_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t7_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t7_3 = _mm256_loadu_pd(P + k2 + 112);
      _t7_2 = _mm256_loadu_pd(P + k2 + 140);
      _t7_1 = _mm256_loadu_pd(P + k2 + 168);
      _t7_0 = _mm256_loadu_pd(P + k2 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t7_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 32);
      _t7_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 32);
      _t7_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t7_3, _t7_2), _mm256_unpacklo_pd(_t7_1, _t7_0), 49);
      _t7_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t7_3, _t7_2), _mm256_unpackhi_pd(_t7_1, _t7_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t7_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_19, _t7_24), _mm256_mul_pd(_t7_18, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_17, _t7_26), _mm256_mul_pd(_t7_16, _t7_27)));
      _t7_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_15, _t7_24), _mm256_mul_pd(_t7_14, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_13, _t7_26), _mm256_mul_pd(_t7_12, _t7_27)));
      _t7_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_11, _t7_24), _mm256_mul_pd(_t7_10, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_9, _t7_26), _mm256_mul_pd(_t7_8, _t7_27)));
      _t7_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t7_7, _t7_24), _mm256_mul_pd(_t7_6, _t7_25)), _mm256_add_pd(_mm256_mul_pd(_t7_5, _t7_26), _mm256_mul_pd(_t7_4, _t7_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t6_20 = _mm256_add_pd(_t6_20, _t7_20);
      _t6_21 = _mm256_add_pd(_t6_21, _t7_21);
      _t6_22 = _mm256_add_pd(_t6_22, _t7_22);
      _t6_23 = _mm256_add_pd(_t6_23, _t7_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k3 = 8; k3 <= 23; k3+=4 ) {
      _t8_3 = _mm256_loadu_pd(P + k3);
      _t8_2 = _mm256_loadu_pd(P + k3 + 28);
      _t8_1 = _mm256_loadu_pd(P + k3 + 56);
      _t8_0 = _mm256_loadu_pd(P + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t8_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t8_3), _mm256_mul_pd(_t4_14, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t8_1), _mm256_mul_pd(_t4_12, _t8_0)));
      _t8_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t8_3), _mm256_mul_pd(_t4_10, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t8_1), _mm256_mul_pd(_t4_8, _t8_0)));
      _t8_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t8_3), _mm256_mul_pd(_t4_6, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t8_1), _mm256_mul_pd(_t4_4, _t8_0)));
      _t8_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t8_3), _mm256_mul_pd(_t4_2, _t8_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t8_1), _mm256_mul_pd(_t4_0, _t8_0)));

      // AVX Storer:

      for( int k2 = 4; k2 <= k3 - 1; k2+=4 ) {
        _t9_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
        _t9_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
        _t9_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
        _t9_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
        _t9_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
        _t9_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
        _t9_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
        _t9_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
        _t9_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
        _t9_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
        _t9_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
        _t9_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
        _t9_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
        _t9_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
        _t9_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
        _t9_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
        _t9_3 = _mm256_loadu_pd(P + 28*k2 + k3);
        _t9_2 = _mm256_loadu_pd(P + 28*k2 + k3 + 28);
        _t9_1 = _mm256_loadu_pd(P + 28*k2 + k3 + 56);
        _t9_0 = _mm256_loadu_pd(P + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t9_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_19, _t9_3), _mm256_mul_pd(_t9_18, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_17, _t9_1), _mm256_mul_pd(_t9_16, _t9_0)));
        _t9_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_15, _t9_3), _mm256_mul_pd(_t9_14, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_13, _t9_1), _mm256_mul_pd(_t9_12, _t9_0)));
        _t9_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_11, _t9_3), _mm256_mul_pd(_t9_10, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_9, _t9_1), _mm256_mul_pd(_t9_8, _t9_0)));
        _t9_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t9_7, _t9_3), _mm256_mul_pd(_t9_6, _t9_2)), _mm256_add_pd(_mm256_mul_pd(_t9_5, _t9_1), _mm256_mul_pd(_t9_4, _t9_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t8_4 = _mm256_add_pd(_t8_4, _t9_20);
        _t8_5 = _mm256_add_pd(_t8_5, _t9_21);
        _t8_6 = _mm256_add_pd(_t8_6, _t9_22);
        _t8_7 = _mm256_add_pd(_t8_7, _t9_23);

        // AVX Storer:
      }
      _t10_19 = _mm256_broadcast_sd(F + 28*i0 + k3);
      _t10_18 = _mm256_broadcast_sd(F + 28*i0 + k3 + 1);
      _t10_17 = _mm256_broadcast_sd(F + 28*i0 + k3 + 2);
      _t10_16 = _mm256_broadcast_sd(F + 28*i0 + k3 + 3);
      _t10_15 = _mm256_broadcast_sd(F + 28*i0 + k3 + 28);
      _t10_14 = _mm256_broadcast_sd(F + 28*i0 + k3 + 29);
      _t10_13 = _mm256_broadcast_sd(F + 28*i0 + k3 + 30);
      _t10_12 = _mm256_broadcast_sd(F + 28*i0 + k3 + 31);
      _t10_11 = _mm256_broadcast_sd(F + 28*i0 + k3 + 56);
      _t10_10 = _mm256_broadcast_sd(F + 28*i0 + k3 + 57);
      _t10_9 = _mm256_broadcast_sd(F + 28*i0 + k3 + 58);
      _t10_8 = _mm256_broadcast_sd(F + 28*i0 + k3 + 59);
      _t10_7 = _mm256_broadcast_sd(F + 28*i0 + k3 + 84);
      _t10_6 = _mm256_broadcast_sd(F + 28*i0 + k3 + 85);
      _t10_5 = _mm256_broadcast_sd(F + 28*i0 + k3 + 86);
      _t10_4 = _mm256_broadcast_sd(F + 28*i0 + k3 + 87);
      _t10_3 = _mm256_loadu_pd(P + 29*k3);
      _t10_2 = _mm256_maskload_pd(P + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t10_1 = _mm256_maskload_pd(P + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t10_0 = _mm256_maskload_pd(P + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t10_24 = _t10_3;
      _t10_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 3), _t10_2, 12);
      _t10_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 0), _t10_1, 49);
      _t10_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t10_3, _t10_2, 12), _mm256_shuffle_pd(_t10_1, _t10_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t10_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_19, _t10_24), _mm256_mul_pd(_t10_18, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_17, _t10_26), _mm256_mul_pd(_t10_16, _t10_27)));
      _t10_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_15, _t10_24), _mm256_mul_pd(_t10_14, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_13, _t10_26), _mm256_mul_pd(_t10_12, _t10_27)));
      _t10_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_11, _t10_24), _mm256_mul_pd(_t10_10, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_9, _t10_26), _mm256_mul_pd(_t10_8, _t10_27)));
      _t10_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t10_7, _t10_24), _mm256_mul_pd(_t10_6, _t10_25)), _mm256_add_pd(_mm256_mul_pd(_t10_5, _t10_26), _mm256_mul_pd(_t10_4, _t10_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t8_4 = _mm256_add_pd(_t8_4, _t10_20);
      _t8_5 = _mm256_add_pd(_t8_5, _t10_21);
      _t8_6 = _mm256_add_pd(_t8_6, _t10_22);
      _t8_7 = _mm256_add_pd(_t8_7, _t10_23);

      // AVX Storer:

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t11_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
        _t11_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
        _t11_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
        _t11_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
        _t11_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
        _t11_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
        _t11_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
        _t11_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
        _t11_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
        _t11_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
        _t11_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
        _t11_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
        _t11_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
        _t11_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
        _t11_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
        _t11_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
        _t11_3 = _mm256_loadu_pd(P + k2 + 28*k3);
        _t11_2 = _mm256_loadu_pd(P + k2 + 28*k3 + 28);
        _t11_1 = _mm256_loadu_pd(P + k2 + 28*k3 + 56);
        _t11_0 = _mm256_loadu_pd(P + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t11_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 32);
        _t11_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 32);
        _t11_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t11_3, _t11_2), _mm256_unpacklo_pd(_t11_1, _t11_0), 49);
        _t11_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t11_3, _t11_2), _mm256_unpackhi_pd(_t11_1, _t11_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t11_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_19, _t11_24), _mm256_mul_pd(_t11_18, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_17, _t11_26), _mm256_mul_pd(_t11_16, _t11_27)));
        _t11_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_15, _t11_24), _mm256_mul_pd(_t11_14, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_13, _t11_26), _mm256_mul_pd(_t11_12, _t11_27)));
        _t11_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_11, _t11_24), _mm256_mul_pd(_t11_10, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_9, _t11_26), _mm256_mul_pd(_t11_8, _t11_27)));
        _t11_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t11_7, _t11_24), _mm256_mul_pd(_t11_6, _t11_25)), _mm256_add_pd(_mm256_mul_pd(_t11_5, _t11_26), _mm256_mul_pd(_t11_4, _t11_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t8_4 = _mm256_add_pd(_t8_4, _t11_20);
        _t8_5 = _mm256_add_pd(_t8_5, _t11_21);
        _t8_6 = _mm256_add_pd(_t8_6, _t11_22);
        _t8_7 = _mm256_add_pd(_t8_7, _t11_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M0 + 28*i0 + k3, _t8_4);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 28, _t8_5);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 56, _t8_6);
      _mm256_storeu_pd(M0 + 28*i0 + k3 + 84, _t8_7);
    }
    _t12_3 = _mm256_loadu_pd(P + 24);
    _t12_2 = _mm256_loadu_pd(P + 52);
    _t12_1 = _mm256_loadu_pd(P + 80);
    _t12_0 = _mm256_loadu_pd(P + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t12_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_15, _t12_3), _mm256_mul_pd(_t4_14, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_13, _t12_1), _mm256_mul_pd(_t4_12, _t12_0)));
    _t12_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_11, _t12_3), _mm256_mul_pd(_t4_10, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_9, _t12_1), _mm256_mul_pd(_t4_8, _t12_0)));
    _t12_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_7, _t12_3), _mm256_mul_pd(_t4_6, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_5, _t12_1), _mm256_mul_pd(_t4_4, _t12_0)));
    _t12_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t4_3, _t12_3), _mm256_mul_pd(_t4_2, _t12_2)), _mm256_add_pd(_mm256_mul_pd(_t4_1, _t12_1), _mm256_mul_pd(_t4_0, _t12_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t13_19 = _mm256_broadcast_sd(F + 28*i0 + k2);
      _t13_18 = _mm256_broadcast_sd(F + 28*i0 + k2 + 1);
      _t13_17 = _mm256_broadcast_sd(F + 28*i0 + k2 + 2);
      _t13_16 = _mm256_broadcast_sd(F + 28*i0 + k2 + 3);
      _t13_15 = _mm256_broadcast_sd(F + 28*i0 + k2 + 28);
      _t13_14 = _mm256_broadcast_sd(F + 28*i0 + k2 + 29);
      _t13_13 = _mm256_broadcast_sd(F + 28*i0 + k2 + 30);
      _t13_12 = _mm256_broadcast_sd(F + 28*i0 + k2 + 31);
      _t13_11 = _mm256_broadcast_sd(F + 28*i0 + k2 + 56);
      _t13_10 = _mm256_broadcast_sd(F + 28*i0 + k2 + 57);
      _t13_9 = _mm256_broadcast_sd(F + 28*i0 + k2 + 58);
      _t13_8 = _mm256_broadcast_sd(F + 28*i0 + k2 + 59);
      _t13_7 = _mm256_broadcast_sd(F + 28*i0 + k2 + 84);
      _t13_6 = _mm256_broadcast_sd(F + 28*i0 + k2 + 85);
      _t13_5 = _mm256_broadcast_sd(F + 28*i0 + k2 + 86);
      _t13_4 = _mm256_broadcast_sd(F + 28*i0 + k2 + 87);
      _t13_3 = _mm256_loadu_pd(P + 28*k2 + 24);
      _t13_2 = _mm256_loadu_pd(P + 28*k2 + 52);
      _t13_1 = _mm256_loadu_pd(P + 28*k2 + 80);
      _t13_0 = _mm256_loadu_pd(P + 28*k2 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t13_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_19, _t13_3), _mm256_mul_pd(_t13_18, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_17, _t13_1), _mm256_mul_pd(_t13_16, _t13_0)));
      _t13_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_15, _t13_3), _mm256_mul_pd(_t13_14, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_13, _t13_1), _mm256_mul_pd(_t13_12, _t13_0)));
      _t13_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_11, _t13_3), _mm256_mul_pd(_t13_10, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_9, _t13_1), _mm256_mul_pd(_t13_8, _t13_0)));
      _t13_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t13_7, _t13_3), _mm256_mul_pd(_t13_6, _t13_2)), _mm256_add_pd(_mm256_mul_pd(_t13_5, _t13_1), _mm256_mul_pd(_t13_4, _t13_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t12_4 = _mm256_add_pd(_t12_4, _t13_20);
      _t12_5 = _mm256_add_pd(_t12_5, _t13_21);
      _t12_6 = _mm256_add_pd(_t12_6, _t13_22);
      _t12_7 = _mm256_add_pd(_t12_7, _t13_23);

      // AVX Storer:
    }
    _t14_15 = _mm256_broadcast_sd(F + 28*i0 + 24);
    _t14_14 = _mm256_broadcast_sd(F + 28*i0 + 25);
    _t14_13 = _mm256_broadcast_sd(F + 28*i0 + 26);
    _t14_12 = _mm256_broadcast_sd(F + 28*i0 + 27);
    _t14_11 = _mm256_broadcast_sd(F + 28*i0 + 52);
    _t14_10 = _mm256_broadcast_sd(F + 28*i0 + 53);
    _t14_9 = _mm256_broadcast_sd(F + 28*i0 + 54);
    _t14_8 = _mm256_broadcast_sd(F + 28*i0 + 55);
    _t14_7 = _mm256_broadcast_sd(F + 28*i0 + 80);
    _t14_6 = _mm256_broadcast_sd(F + 28*i0 + 81);
    _t14_5 = _mm256_broadcast_sd(F + 28*i0 + 82);
    _t14_4 = _mm256_broadcast_sd(F + 28*i0 + 83);
    _t14_3 = _mm256_broadcast_sd(F + 28*i0 + 108);
    _t14_2 = _mm256_broadcast_sd(F + 28*i0 + 109);
    _t14_1 = _mm256_broadcast_sd(F + 28*i0 + 110);
    _t14_0 = _mm256_broadcast_sd(F + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t14_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_15, _t3_20), _mm256_mul_pd(_t14_14, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_13, _t3_22), _mm256_mul_pd(_t14_12, _t3_23)));
    _t14_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_11, _t3_20), _mm256_mul_pd(_t14_10, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_9, _t3_22), _mm256_mul_pd(_t14_8, _t3_23)));
    _t14_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_7, _t3_20), _mm256_mul_pd(_t14_6, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_5, _t3_22), _mm256_mul_pd(_t14_4, _t3_23)));
    _t14_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t14_3, _t3_20), _mm256_mul_pd(_t14_2, _t3_21)), _mm256_add_pd(_mm256_mul_pd(_t14_1, _t3_22), _mm256_mul_pd(_t14_0, _t3_23)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t12_4 = _mm256_add_pd(_t12_4, _t14_16);
    _t12_5 = _mm256_add_pd(_t12_5, _t14_17);
    _t12_6 = _mm256_add_pd(_t12_6, _t14_18);
    _t12_7 = _mm256_add_pd(_t12_7, _t14_19);

    // AVX Storer:
    _mm256_storeu_pd(M0 + 28*i0, _t4_16);
    _mm256_storeu_pd(M0 + 28*i0 + 28, _t4_17);
    _mm256_storeu_pd(M0 + 28*i0 + 56, _t4_18);
    _mm256_storeu_pd(M0 + 28*i0 + 84, _t4_19);
    _mm256_storeu_pd(M0 + 28*i0 + 4, _t6_20);
    _mm256_storeu_pd(M0 + 28*i0 + 32, _t6_21);
    _mm256_storeu_pd(M0 + 28*i0 + 60, _t6_22);
    _mm256_storeu_pd(M0 + 28*i0 + 88, _t6_23);
    _mm256_storeu_pd(M0 + 28*i0 + 24, _t12_4);
    _mm256_storeu_pd(M0 + 28*i0 + 52, _t12_5);
    _mm256_storeu_pd(M0 + 28*i0 + 80, _t12_6);
    _mm256_storeu_pd(M0 + 28*i0 + 108, _t12_7);
  }


  // Generating : Y[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, i0)) ),h(4, 28, i0)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, i0), F[28,28],h(4, 28, k2)) ) ),h(4, 28, i0)) ) ) + Sum_{k3} ( ( S(h(4, 28, i0), ( ( G(h(4, 28, i0), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, k3), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, i0), Q[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, k3), F[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) ) + S(h(4, 28, 24), ( ( G(h(4, 28, 24), M0[28,28],h(4, 28, 0)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, 0)) ) ) + G(h(4, 28, 24), Q[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 28, 24), ( G(h(4, 28, 24), M0[28,28],h(4, 28, k2)) * T( G(h(4, 28, 24), F[28,28],h(4, 28, k2)) ) ),h(4, 28, 24)) ) )


  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t15_23 = _mm256_broadcast_sd(M0 + 28*i0);
    _t15_22 = _mm256_broadcast_sd(M0 + 28*i0 + 1);
    _t15_21 = _mm256_broadcast_sd(M0 + 28*i0 + 2);
    _t15_20 = _mm256_broadcast_sd(M0 + 28*i0 + 3);
    _t15_19 = _mm256_broadcast_sd(M0 + 28*i0 + 28);
    _t15_18 = _mm256_broadcast_sd(M0 + 28*i0 + 29);
    _t15_17 = _mm256_broadcast_sd(M0 + 28*i0 + 30);
    _t15_16 = _mm256_broadcast_sd(M0 + 28*i0 + 31);
    _t15_15 = _mm256_broadcast_sd(M0 + 28*i0 + 56);
    _t15_14 = _mm256_broadcast_sd(M0 + 28*i0 + 57);
    _t15_13 = _mm256_broadcast_sd(M0 + 28*i0 + 58);
    _t15_12 = _mm256_broadcast_sd(M0 + 28*i0 + 59);
    _t15_11 = _mm256_broadcast_sd(M0 + 28*i0 + 84);
    _t15_10 = _mm256_broadcast_sd(M0 + 28*i0 + 85);
    _t15_9 = _mm256_broadcast_sd(M0 + 28*i0 + 86);
    _t15_8 = _mm256_broadcast_sd(M0 + 28*i0 + 87);
    _t15_7 = _mm256_loadu_pd(F + 28*i0);
    _t15_6 = _mm256_loadu_pd(F + 28*i0 + 28);
    _t15_5 = _mm256_loadu_pd(F + 28*i0 + 56);
    _t15_4 = _mm256_loadu_pd(F + 28*i0 + 84);
    _t15_3 = _mm256_loadu_pd(Q + 29*i0);
    _t15_2 = _mm256_maskload_pd(Q + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t15_1 = _mm256_maskload_pd(Q + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t15_0 = _mm256_maskload_pd(Q + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t15_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 32);
    _t15_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 32);
    _t15_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t15_7, _t15_6), _mm256_unpacklo_pd(_t15_5, _t15_4), 49);
    _t15_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t15_7, _t15_6), _mm256_unpackhi_pd(_t15_5, _t15_4), 49);

    // 4-BLAC: 4x4 * 4x4
    _t15_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t15_40), _mm256_mul_pd(_t15_22, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t15_42), _mm256_mul_pd(_t15_20, _t15_43)));
    _t15_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t15_40), _mm256_mul_pd(_t15_18, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t15_42), _mm256_mul_pd(_t15_16, _t15_43)));
    _t15_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t15_40), _mm256_mul_pd(_t15_14, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t15_42), _mm256_mul_pd(_t15_12, _t15_43)));
    _t15_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t15_40), _mm256_mul_pd(_t15_10, _t15_41)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t15_42), _mm256_mul_pd(_t15_8, _t15_43)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t15_36 = _t15_3;
    _t15_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 3), _t15_2, 12);
    _t15_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 0), _t15_1, 49);
    _t15_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_3, _t15_2, 12), _mm256_shuffle_pd(_t15_1, _t15_0, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t15_24 = _mm256_add_pd(_t15_32, _t15_36);
    _t15_25 = _mm256_add_pd(_t15_33, _t15_37);
    _t15_26 = _mm256_add_pd(_t15_34, _t15_38);
    _t15_27 = _mm256_add_pd(_t15_35, _t15_39);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t15_28 = _t15_24;
    _t15_29 = _t15_25;
    _t15_30 = _t15_26;
    _t15_31 = _t15_27;

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t16_19 = _mm256_broadcast_sd(M0 + 28*i0 + k2);
      _t16_18 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 1);
      _t16_17 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 2);
      _t16_16 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 3);
      _t16_15 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 28);
      _t16_14 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 29);
      _t16_13 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 30);
      _t16_12 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 31);
      _t16_11 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 56);
      _t16_10 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 57);
      _t16_9 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 58);
      _t16_8 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 59);
      _t16_7 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 84);
      _t16_6 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 85);
      _t16_5 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 86);
      _t16_4 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 87);
      _t16_3 = _mm256_loadu_pd(F + 28*i0 + k2);
      _t16_2 = _mm256_loadu_pd(F + 28*i0 + k2 + 28);
      _t16_1 = _mm256_loadu_pd(F + 28*i0 + k2 + 56);
      _t16_0 = _mm256_loadu_pd(F + 28*i0 + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t16_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 32);
      _t16_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 32);
      _t16_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t16_3, _t16_2), _mm256_unpacklo_pd(_t16_1, _t16_0), 49);
      _t16_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t16_3, _t16_2), _mm256_unpackhi_pd(_t16_1, _t16_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t16_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_19, _t16_28), _mm256_mul_pd(_t16_18, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_17, _t16_30), _mm256_mul_pd(_t16_16, _t16_31)));
      _t16_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_15, _t16_28), _mm256_mul_pd(_t16_14, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_13, _t16_30), _mm256_mul_pd(_t16_12, _t16_31)));
      _t16_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_11, _t16_28), _mm256_mul_pd(_t16_10, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_9, _t16_30), _mm256_mul_pd(_t16_8, _t16_31)));
      _t16_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t16_7, _t16_28), _mm256_mul_pd(_t16_6, _t16_29)), _mm256_add_pd(_mm256_mul_pd(_t16_5, _t16_30), _mm256_mul_pd(_t16_4, _t16_31)));

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t16_24 = _t15_28;
      _t16_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 3), _t15_29, 12);
      _t16_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 0), _t15_30, 49);
      _t16_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t15_28, _t15_29, 12), _mm256_shuffle_pd(_t15_30, _t15_31, 12), 49);

      // 4-BLAC: 4x4 + 4x4
      _t16_24 = _mm256_add_pd(_t16_24, _t16_20);
      _t16_25 = _mm256_add_pd(_t16_25, _t16_21);
      _t16_26 = _mm256_add_pd(_t16_26, _t16_22);
      _t16_27 = _mm256_add_pd(_t16_27, _t16_23);

      // AVX Storer:

      // 4x4 -> 4x4 - UpSymm
      _t15_28 = _t16_24;
      _t15_29 = _t16_25;
      _t15_30 = _t16_26;
      _t15_31 = _t16_27;
    }

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 27; k3+=4 ) {
      _t17_7 = _mm256_loadu_pd(F + 28*k3);
      _t17_6 = _mm256_loadu_pd(F + 28*k3 + 28);
      _t17_5 = _mm256_loadu_pd(F + 28*k3 + 56);
      _t17_4 = _mm256_loadu_pd(F + 28*k3 + 84);
      _t17_3 = _mm256_loadu_pd(Q + 28*i0 + k3);
      _t17_2 = _mm256_loadu_pd(Q + 28*i0 + k3 + 28);
      _t17_1 = _mm256_loadu_pd(Q + 28*i0 + k3 + 56);
      _t17_0 = _mm256_loadu_pd(Q + 28*i0 + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t17_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_6), _mm256_unpacklo_pd(_t17_5, _t17_4), 32);
      _t17_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_7, _t17_6), _mm256_unpackhi_pd(_t17_5, _t17_4), 32);
      _t17_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t17_7, _t17_6), _mm256_unpacklo_pd(_t17_5, _t17_4), 49);
      _t17_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t17_7, _t17_6), _mm256_unpackhi_pd(_t17_5, _t17_4), 49);

      // 4-BLAC: 4x4 * 4x4
      _t17_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_23, _t17_16), _mm256_mul_pd(_t15_22, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_21, _t17_18), _mm256_mul_pd(_t15_20, _t17_19)));
      _t17_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_19, _t17_16), _mm256_mul_pd(_t15_18, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_17, _t17_18), _mm256_mul_pd(_t15_16, _t17_19)));
      _t17_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_15, _t17_16), _mm256_mul_pd(_t15_14, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_13, _t17_18), _mm256_mul_pd(_t15_12, _t17_19)));
      _t17_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t15_11, _t17_16), _mm256_mul_pd(_t15_10, _t17_17)), _mm256_add_pd(_mm256_mul_pd(_t15_9, _t17_18), _mm256_mul_pd(_t15_8, _t17_19)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t17_8 = _mm256_add_pd(_t17_12, _t17_3);
      _t17_9 = _mm256_add_pd(_t17_13, _t17_2);
      _t17_10 = _mm256_add_pd(_t17_14, _t17_1);
      _t17_11 = _mm256_add_pd(_t17_15, _t17_0);

      // AVX Storer:

      for( int k2 = 4; k2 <= 27; k2+=4 ) {
        _t18_19 = _mm256_broadcast_sd(M0 + 28*i0 + k2);
        _t18_18 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 1);
        _t18_17 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 2);
        _t18_16 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 3);
        _t18_15 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 28);
        _t18_14 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 29);
        _t18_13 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 30);
        _t18_12 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 31);
        _t18_11 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 56);
        _t18_10 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 57);
        _t18_9 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 58);
        _t18_8 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 59);
        _t18_7 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 84);
        _t18_6 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 85);
        _t18_5 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 86);
        _t18_4 = _mm256_broadcast_sd(M0 + 28*i0 + k2 + 87);
        _t18_3 = _mm256_loadu_pd(F + k2 + 28*k3);
        _t18_2 = _mm256_loadu_pd(F + k2 + 28*k3 + 28);
        _t18_1 = _mm256_loadu_pd(F + k2 + 28*k3 + 56);
        _t18_0 = _mm256_loadu_pd(F + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t18_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 32);
        _t18_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 32);
        _t18_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t18_3, _t18_2), _mm256_unpacklo_pd(_t18_1, _t18_0), 49);
        _t18_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t18_3, _t18_2), _mm256_unpackhi_pd(_t18_1, _t18_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t18_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_19, _t18_24), _mm256_mul_pd(_t18_18, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_17, _t18_26), _mm256_mul_pd(_t18_16, _t18_27)));
        _t18_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_15, _t18_24), _mm256_mul_pd(_t18_14, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_13, _t18_26), _mm256_mul_pd(_t18_12, _t18_27)));
        _t18_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_11, _t18_24), _mm256_mul_pd(_t18_10, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_9, _t18_26), _mm256_mul_pd(_t18_8, _t18_27)));
        _t18_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t18_7, _t18_24), _mm256_mul_pd(_t18_6, _t18_25)), _mm256_add_pd(_mm256_mul_pd(_t18_5, _t18_26), _mm256_mul_pd(_t18_4, _t18_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t17_8 = _mm256_add_pd(_t17_8, _t18_20);
        _t17_9 = _mm256_add_pd(_t17_9, _t18_21);
        _t17_10 = _mm256_add_pd(_t17_10, _t18_22);
        _t17_11 = _mm256_add_pd(_t17_11, _t18_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(Y + 28*i0 + k3, _t17_8);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 28, _t17_9);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 56, _t17_10);
      _mm256_storeu_pd(Y + 28*i0 + k3 + 84, _t17_11);
    }
    _mm256_storeu_pd(Y + 29*i0, _t15_28);
    _mm256_maskstore_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t15_29);
    _mm256_maskstore_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t15_30);
    _mm256_maskstore_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t15_31);
  }

  _t19_23 = _mm256_broadcast_sd(M0 + 672);
  _t19_22 = _mm256_broadcast_sd(M0 + 673);
  _t19_21 = _mm256_broadcast_sd(M0 + 674);
  _t19_20 = _mm256_broadcast_sd(M0 + 675);
  _t19_19 = _mm256_broadcast_sd(M0 + 700);
  _t19_18 = _mm256_broadcast_sd(M0 + 701);
  _t19_17 = _mm256_broadcast_sd(M0 + 702);
  _t19_16 = _mm256_broadcast_sd(M0 + 703);
  _t19_15 = _mm256_broadcast_sd(M0 + 728);
  _t19_14 = _mm256_broadcast_sd(M0 + 729);
  _t19_13 = _mm256_broadcast_sd(M0 + 730);
  _t19_12 = _mm256_broadcast_sd(M0 + 731);
  _t19_11 = _mm256_broadcast_sd(M0 + 756);
  _t19_10 = _mm256_broadcast_sd(M0 + 757);
  _t19_9 = _mm256_broadcast_sd(M0 + 758);
  _t19_8 = _mm256_broadcast_sd(M0 + 759);
  _t19_7 = _mm256_loadu_pd(F + 672);
  _t19_6 = _mm256_loadu_pd(F + 700);
  _t19_5 = _mm256_loadu_pd(F + 728);
  _t19_4 = _mm256_loadu_pd(F + 756);
  _t19_3 = _mm256_loadu_pd(Q + 696);
  _t19_2 = _mm256_maskload_pd(Q + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t19_1 = _mm256_maskload_pd(Q + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t19_0 = _mm256_maskload_pd(Q + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t19_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_6), _mm256_unpacklo_pd(_t19_5, _t19_4), 32);
  _t19_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_7, _t19_6), _mm256_unpackhi_pd(_t19_5, _t19_4), 32);
  _t19_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t19_7, _t19_6), _mm256_unpacklo_pd(_t19_5, _t19_4), 49);
  _t19_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t19_7, _t19_6), _mm256_unpackhi_pd(_t19_5, _t19_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t19_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_23, _t19_40), _mm256_mul_pd(_t19_22, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_21, _t19_42), _mm256_mul_pd(_t19_20, _t19_43)));
  _t19_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_19, _t19_40), _mm256_mul_pd(_t19_18, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_17, _t19_42), _mm256_mul_pd(_t19_16, _t19_43)));
  _t19_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_15, _t19_40), _mm256_mul_pd(_t19_14, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_13, _t19_42), _mm256_mul_pd(_t19_12, _t19_43)));
  _t19_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t19_11, _t19_40), _mm256_mul_pd(_t19_10, _t19_41)), _mm256_add_pd(_mm256_mul_pd(_t19_9, _t19_42), _mm256_mul_pd(_t19_8, _t19_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t19_36 = _t19_3;
  _t19_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 3), _t19_2, 12);
  _t19_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 0), _t19_1, 49);
  _t19_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_3, _t19_2, 12), _mm256_shuffle_pd(_t19_1, _t19_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t19_24 = _mm256_add_pd(_t19_32, _t19_36);
  _t19_25 = _mm256_add_pd(_t19_33, _t19_37);
  _t19_26 = _mm256_add_pd(_t19_34, _t19_38);
  _t19_27 = _mm256_add_pd(_t19_35, _t19_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t19_28 = _t19_24;
  _t19_29 = _t19_25;
  _t19_30 = _t19_26;
  _t19_31 = _t19_27;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t20_19 = _mm256_broadcast_sd(M0 + k2 + 672);
    _t20_18 = _mm256_broadcast_sd(M0 + k2 + 673);
    _t20_17 = _mm256_broadcast_sd(M0 + k2 + 674);
    _t20_16 = _mm256_broadcast_sd(M0 + k2 + 675);
    _t20_15 = _mm256_broadcast_sd(M0 + k2 + 700);
    _t20_14 = _mm256_broadcast_sd(M0 + k2 + 701);
    _t20_13 = _mm256_broadcast_sd(M0 + k2 + 702);
    _t20_12 = _mm256_broadcast_sd(M0 + k2 + 703);
    _t20_11 = _mm256_broadcast_sd(M0 + k2 + 728);
    _t20_10 = _mm256_broadcast_sd(M0 + k2 + 729);
    _t20_9 = _mm256_broadcast_sd(M0 + k2 + 730);
    _t20_8 = _mm256_broadcast_sd(M0 + k2 + 731);
    _t20_7 = _mm256_broadcast_sd(M0 + k2 + 756);
    _t20_6 = _mm256_broadcast_sd(M0 + k2 + 757);
    _t20_5 = _mm256_broadcast_sd(M0 + k2 + 758);
    _t20_4 = _mm256_broadcast_sd(M0 + k2 + 759);
    _t20_3 = _mm256_loadu_pd(F + k2 + 672);
    _t20_2 = _mm256_loadu_pd(F + k2 + 700);
    _t20_1 = _mm256_loadu_pd(F + k2 + 728);
    _t20_0 = _mm256_loadu_pd(F + k2 + 756);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t20_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 32);
    _t20_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 32);
    _t20_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t20_3, _t20_2), _mm256_unpacklo_pd(_t20_1, _t20_0), 49);
    _t20_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t20_3, _t20_2), _mm256_unpackhi_pd(_t20_1, _t20_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t20_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_19, _t20_28), _mm256_mul_pd(_t20_18, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_17, _t20_30), _mm256_mul_pd(_t20_16, _t20_31)));
    _t20_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_15, _t20_28), _mm256_mul_pd(_t20_14, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_13, _t20_30), _mm256_mul_pd(_t20_12, _t20_31)));
    _t20_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_11, _t20_28), _mm256_mul_pd(_t20_10, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_9, _t20_30), _mm256_mul_pd(_t20_8, _t20_31)));
    _t20_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t20_7, _t20_28), _mm256_mul_pd(_t20_6, _t20_29)), _mm256_add_pd(_mm256_mul_pd(_t20_5, _t20_30), _mm256_mul_pd(_t20_4, _t20_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t20_24 = _t19_28;
    _t20_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
    _t20_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
    _t20_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t20_24 = _mm256_add_pd(_t20_24, _t20_20);
    _t20_25 = _mm256_add_pd(_t20_25, _t20_21);
    _t20_26 = _mm256_add_pd(_t20_26, _t20_22);
    _t20_27 = _mm256_add_pd(_t20_27, _t20_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t19_28 = _t20_24;
    _t19_29 = _t20_25;
    _t19_30 = _t20_26;
    _t19_31 = _t20_27;
  }


  // Generating : v0[8,1] = Sum_{i0} ( ( S(h(4, 8, i0), ( G(h(4, 8, i0), z[8,1],h(1, 1, 0)) - ( G(h(4, 8, i0), H[8,28],h(4, 28, 0)) * G(h(4, 28, 0), y[28,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + Sum_{k3} ( -$(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k3)) * G(h(4, 28, k3), y[28,1],h(1, 1, 0)) ),h(1, 1, 0)) ) ) )

  // AVX Loader:


  for( int i0 = 0; i0 <= 7; i0+=4 ) {
    _t21_5 = _mm256_loadu_pd(z + i0);
    _t21_4 = _mm256_loadu_pd(H + 28*i0);
    _t21_3 = _mm256_loadu_pd(H + 28*i0 + 28);
    _t21_2 = _mm256_loadu_pd(H + 28*i0 + 56);
    _t21_1 = _mm256_loadu_pd(H + 28*i0 + 84);
    _t21_0 = _mm256_loadu_pd(y);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t21_6 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t21_4, _t21_0), _mm256_mul_pd(_t21_3, _t21_0)), _mm256_hadd_pd(_mm256_mul_pd(_t21_2, _t21_0), _mm256_mul_pd(_t21_1, _t21_0)), 12));

    // 4-BLAC: 4x1 - 4x1
    _t21_7 = _mm256_sub_pd(_t21_5, _t21_6);

    // AVX Storer:

    for( int k3 = 4; k3 <= 27; k3+=4 ) {
      _t22_4 = _mm256_loadu_pd(H + 28*i0 + k3);
      _t22_3 = _mm256_loadu_pd(H + 28*i0 + k3 + 28);
      _t22_2 = _mm256_loadu_pd(H + 28*i0 + k3 + 56);
      _t22_1 = _mm256_loadu_pd(H + 28*i0 + k3 + 84);
      _t22_0 = _mm256_loadu_pd(y + k3);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x1
      _t22_5 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_4, _t22_0), _mm256_mul_pd(_t22_3, _t22_0)), _mm256_hadd_pd(_mm256_mul_pd(_t22_2, _t22_0), _mm256_mul_pd(_t22_1, _t22_0)), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t22_4, _t22_0), _mm256_mul_pd(_t22_3, _t22_0)), _mm256_hadd_pd(_mm256_mul_pd(_t22_2, _t22_0), _mm256_mul_pd(_t22_1, _t22_0)), 12));

      // AVX Loader:

      // 4-BLAC: 4x1 - 4x1
      _t21_7 = _mm256_sub_pd(_t21_7, _t22_5);

      // AVX Storer:
    }
    _mm256_storeu_pd(v0 + i0, _t21_7);
  }

  _t23_7 = _mm256_loadu_pd(Y);
  _t23_6 = _mm256_maskload_pd(Y + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t23_5 = _mm256_maskload_pd(Y + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t23_4 = _mm256_maskload_pd(Y + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
  _t23_3 = _mm256_loadu_pd(Y + 116);
  _t23_2 = _mm256_maskload_pd(Y + 144, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t23_1 = _mm256_maskload_pd(Y + 172, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t23_0 = _mm256_maskload_pd(Y + 200, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M1[8,28] = Sum_{i0} ( ( ( ( ( ( ( ( ( S(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) ),h(4, 28, 0)) + Sum_{k2} ( $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k2)) * T( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 0)) ) ) + S(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, 4)) * G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) ),h(4, 28, 4)) ) + Sum_{k2} ( $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k2)) * T( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, 4)) ) ) + Sum_{k3} ( ( ( ( S(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) + Sum_{k2} ( $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) + $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k3)) * G(h(4, 28, k3), Y[28,28],h(4, 28, k3)) ),h(4, 28, k3)) ) + Sum_{k2} ( $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k2)) * T( G(h(4, 28, k3), Y[28,28],h(4, 28, k2)) ) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, 0)) * G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) + Sum_{k2} ( $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, k2)) * G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) ) + $(h(4, 8, i0), ( G(h(4, 8, i0), H[8,28],h(4, 28, 24)) * G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) ),h(4, 28, 24)) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_8 = _t23_7;
  _t23_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 3), _t23_6, 12);
  _t23_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 0), _t23_5, 49);
  _t23_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 12), _mm256_shuffle_pd(_t23_5, _t23_4, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_12 = _t23_3;
  _t23_13 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 3), _t23_2, 12);
  _t23_14 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 0), _t23_1, 49);
  _t23_15 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 12), _mm256_shuffle_pd(_t23_1, _t23_0, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t23_16 = _t19_28;
  _t23_17 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t23_18 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t23_19 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);


  for( int i0 = 0; i0 <= 7; i0+=4 ) {
    _t24_15 = _mm256_broadcast_sd(H + 28*i0);
    _t24_14 = _mm256_broadcast_sd(H + 28*i0 + 1);
    _t24_13 = _mm256_broadcast_sd(H + 28*i0 + 2);
    _t24_12 = _mm256_broadcast_sd(H + 28*i0 + 3);
    _t24_11 = _mm256_broadcast_sd(H + 28*i0 + 28);
    _t24_10 = _mm256_broadcast_sd(H + 28*i0 + 29);
    _t24_9 = _mm256_broadcast_sd(H + 28*i0 + 30);
    _t24_8 = _mm256_broadcast_sd(H + 28*i0 + 31);
    _t24_7 = _mm256_broadcast_sd(H + 28*i0 + 56);
    _t24_6 = _mm256_broadcast_sd(H + 28*i0 + 57);
    _t24_5 = _mm256_broadcast_sd(H + 28*i0 + 58);
    _t24_4 = _mm256_broadcast_sd(H + 28*i0 + 59);
    _t24_3 = _mm256_broadcast_sd(H + 28*i0 + 84);
    _t24_2 = _mm256_broadcast_sd(H + 28*i0 + 85);
    _t24_1 = _mm256_broadcast_sd(H + 28*i0 + 86);
    _t24_0 = _mm256_broadcast_sd(H + 28*i0 + 87);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t24_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t23_8), _mm256_mul_pd(_t24_14, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t23_10), _mm256_mul_pd(_t24_12, _t23_11)));
    _t24_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t23_8), _mm256_mul_pd(_t24_10, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t23_10), _mm256_mul_pd(_t24_8, _t23_11)));
    _t24_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t23_8), _mm256_mul_pd(_t24_6, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t23_10), _mm256_mul_pd(_t24_4, _t23_11)));
    _t24_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t23_8), _mm256_mul_pd(_t24_2, _t23_9)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t23_10), _mm256_mul_pd(_t24_0, _t23_11)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t25_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t25_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t25_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t25_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t25_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t25_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t25_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t25_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t25_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t25_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t25_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t25_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t25_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t25_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t25_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t25_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t25_3 = _mm256_loadu_pd(Y + k2);
      _t25_2 = _mm256_loadu_pd(Y + k2 + 28);
      _t25_1 = _mm256_loadu_pd(Y + k2 + 56);
      _t25_0 = _mm256_loadu_pd(Y + k2 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t25_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 32);
      _t25_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 32);
      _t25_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t25_3, _t25_2), _mm256_unpacklo_pd(_t25_1, _t25_0), 49);
      _t25_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t25_3, _t25_2), _mm256_unpackhi_pd(_t25_1, _t25_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t25_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_19, _t25_24), _mm256_mul_pd(_t25_18, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_17, _t25_26), _mm256_mul_pd(_t25_16, _t25_27)));
      _t25_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_15, _t25_24), _mm256_mul_pd(_t25_14, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_13, _t25_26), _mm256_mul_pd(_t25_12, _t25_27)));
      _t25_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_11, _t25_24), _mm256_mul_pd(_t25_10, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_9, _t25_26), _mm256_mul_pd(_t25_8, _t25_27)));
      _t25_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t25_7, _t25_24), _mm256_mul_pd(_t25_6, _t25_25)), _mm256_add_pd(_mm256_mul_pd(_t25_5, _t25_26), _mm256_mul_pd(_t25_4, _t25_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t24_16 = _mm256_add_pd(_t24_16, _t25_20);
      _t24_17 = _mm256_add_pd(_t24_17, _t25_21);
      _t24_18 = _mm256_add_pd(_t24_18, _t25_22);
      _t24_19 = _mm256_add_pd(_t24_19, _t25_23);

      // AVX Storer:
    }
    _t26_19 = _mm256_loadu_pd(Y + 4);
    _t26_18 = _mm256_loadu_pd(Y + 32);
    _t26_17 = _mm256_loadu_pd(Y + 60);
    _t26_16 = _mm256_loadu_pd(Y + 88);
    _t26_15 = _mm256_broadcast_sd(H + 28*i0 + 4);
    _t26_14 = _mm256_broadcast_sd(H + 28*i0 + 5);
    _t26_13 = _mm256_broadcast_sd(H + 28*i0 + 6);
    _t26_12 = _mm256_broadcast_sd(H + 28*i0 + 7);
    _t26_11 = _mm256_broadcast_sd(H + 28*i0 + 32);
    _t26_10 = _mm256_broadcast_sd(H + 28*i0 + 33);
    _t26_9 = _mm256_broadcast_sd(H + 28*i0 + 34);
    _t26_8 = _mm256_broadcast_sd(H + 28*i0 + 35);
    _t26_7 = _mm256_broadcast_sd(H + 28*i0 + 60);
    _t26_6 = _mm256_broadcast_sd(H + 28*i0 + 61);
    _t26_5 = _mm256_broadcast_sd(H + 28*i0 + 62);
    _t26_4 = _mm256_broadcast_sd(H + 28*i0 + 63);
    _t26_3 = _mm256_broadcast_sd(H + 28*i0 + 88);
    _t26_2 = _mm256_broadcast_sd(H + 28*i0 + 89);
    _t26_1 = _mm256_broadcast_sd(H + 28*i0 + 90);
    _t26_0 = _mm256_broadcast_sd(H + 28*i0 + 91);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t26_19), _mm256_mul_pd(_t24_14, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t26_17), _mm256_mul_pd(_t24_12, _t26_16)));
    _t26_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t26_19), _mm256_mul_pd(_t24_10, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t26_17), _mm256_mul_pd(_t24_8, _t26_16)));
    _t26_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t26_19), _mm256_mul_pd(_t24_6, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t26_17), _mm256_mul_pd(_t24_4, _t26_16)));
    _t26_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t26_19), _mm256_mul_pd(_t24_2, _t26_18)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t26_17), _mm256_mul_pd(_t24_0, _t26_16)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t26_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_15, _t23_12), _mm256_mul_pd(_t26_14, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_13, _t23_14), _mm256_mul_pd(_t26_12, _t23_15)));
    _t26_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_11, _t23_12), _mm256_mul_pd(_t26_10, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_9, _t23_14), _mm256_mul_pd(_t26_8, _t23_15)));
    _t26_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_7, _t23_12), _mm256_mul_pd(_t26_6, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_5, _t23_14), _mm256_mul_pd(_t26_4, _t23_15)));
    _t26_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t26_3, _t23_12), _mm256_mul_pd(_t26_2, _t23_13)), _mm256_add_pd(_mm256_mul_pd(_t26_1, _t23_14), _mm256_mul_pd(_t26_0, _t23_15)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t26_20 = _mm256_add_pd(_t26_20, _t26_24);
    _t26_21 = _mm256_add_pd(_t26_21, _t26_25);
    _t26_22 = _mm256_add_pd(_t26_22, _t26_26);
    _t26_23 = _mm256_add_pd(_t26_23, _t26_27);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t27_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t27_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t27_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t27_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t27_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t27_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t27_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t27_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t27_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t27_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t27_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t27_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t27_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t27_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t27_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t27_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t27_3 = _mm256_loadu_pd(Y + k2 + 112);
      _t27_2 = _mm256_loadu_pd(Y + k2 + 140);
      _t27_1 = _mm256_loadu_pd(Y + k2 + 168);
      _t27_0 = _mm256_loadu_pd(Y + k2 + 196);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t27_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 32);
      _t27_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 32);
      _t27_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t27_3, _t27_2), _mm256_unpacklo_pd(_t27_1, _t27_0), 49);
      _t27_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t27_3, _t27_2), _mm256_unpackhi_pd(_t27_1, _t27_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t27_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_19, _t27_24), _mm256_mul_pd(_t27_18, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_17, _t27_26), _mm256_mul_pd(_t27_16, _t27_27)));
      _t27_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_15, _t27_24), _mm256_mul_pd(_t27_14, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_13, _t27_26), _mm256_mul_pd(_t27_12, _t27_27)));
      _t27_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_11, _t27_24), _mm256_mul_pd(_t27_10, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_9, _t27_26), _mm256_mul_pd(_t27_8, _t27_27)));
      _t27_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t27_7, _t27_24), _mm256_mul_pd(_t27_6, _t27_25)), _mm256_add_pd(_mm256_mul_pd(_t27_5, _t27_26), _mm256_mul_pd(_t27_4, _t27_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t26_20 = _mm256_add_pd(_t26_20, _t27_20);
      _t26_21 = _mm256_add_pd(_t26_21, _t27_21);
      _t26_22 = _mm256_add_pd(_t26_22, _t27_22);
      _t26_23 = _mm256_add_pd(_t26_23, _t27_23);

      // AVX Storer:
    }

    // AVX Loader:

    for( int k3 = 8; k3 <= 23; k3+=4 ) {
      _t28_3 = _mm256_loadu_pd(Y + k3);
      _t28_2 = _mm256_loadu_pd(Y + k3 + 28);
      _t28_1 = _mm256_loadu_pd(Y + k3 + 56);
      _t28_0 = _mm256_loadu_pd(Y + k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t28_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t28_3), _mm256_mul_pd(_t24_14, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t28_1), _mm256_mul_pd(_t24_12, _t28_0)));
      _t28_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t28_3), _mm256_mul_pd(_t24_10, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t28_1), _mm256_mul_pd(_t24_8, _t28_0)));
      _t28_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t28_3), _mm256_mul_pd(_t24_6, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t28_1), _mm256_mul_pd(_t24_4, _t28_0)));
      _t28_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t28_3), _mm256_mul_pd(_t24_2, _t28_2)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t28_1), _mm256_mul_pd(_t24_0, _t28_0)));

      // AVX Storer:

      for( int k2 = 4; k2 <= k3 - 1; k2+=4 ) {
        _t29_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
        _t29_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
        _t29_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
        _t29_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
        _t29_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
        _t29_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
        _t29_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
        _t29_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
        _t29_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
        _t29_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
        _t29_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
        _t29_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
        _t29_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
        _t29_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
        _t29_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
        _t29_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
        _t29_3 = _mm256_loadu_pd(Y + 28*k2 + k3);
        _t29_2 = _mm256_loadu_pd(Y + 28*k2 + k3 + 28);
        _t29_1 = _mm256_loadu_pd(Y + 28*k2 + k3 + 56);
        _t29_0 = _mm256_loadu_pd(Y + 28*k2 + k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: 4x4 * 4x4
        _t29_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_19, _t29_3), _mm256_mul_pd(_t29_18, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_17, _t29_1), _mm256_mul_pd(_t29_16, _t29_0)));
        _t29_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_15, _t29_3), _mm256_mul_pd(_t29_14, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_13, _t29_1), _mm256_mul_pd(_t29_12, _t29_0)));
        _t29_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_11, _t29_3), _mm256_mul_pd(_t29_10, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_9, _t29_1), _mm256_mul_pd(_t29_8, _t29_0)));
        _t29_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t29_7, _t29_3), _mm256_mul_pd(_t29_6, _t29_2)), _mm256_add_pd(_mm256_mul_pd(_t29_5, _t29_1), _mm256_mul_pd(_t29_4, _t29_0)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t28_4 = _mm256_add_pd(_t28_4, _t29_20);
        _t28_5 = _mm256_add_pd(_t28_5, _t29_21);
        _t28_6 = _mm256_add_pd(_t28_6, _t29_22);
        _t28_7 = _mm256_add_pd(_t28_7, _t29_23);

        // AVX Storer:
      }
      _t30_19 = _mm256_broadcast_sd(H + 28*i0 + k3);
      _t30_18 = _mm256_broadcast_sd(H + 28*i0 + k3 + 1);
      _t30_17 = _mm256_broadcast_sd(H + 28*i0 + k3 + 2);
      _t30_16 = _mm256_broadcast_sd(H + 28*i0 + k3 + 3);
      _t30_15 = _mm256_broadcast_sd(H + 28*i0 + k3 + 28);
      _t30_14 = _mm256_broadcast_sd(H + 28*i0 + k3 + 29);
      _t30_13 = _mm256_broadcast_sd(H + 28*i0 + k3 + 30);
      _t30_12 = _mm256_broadcast_sd(H + 28*i0 + k3 + 31);
      _t30_11 = _mm256_broadcast_sd(H + 28*i0 + k3 + 56);
      _t30_10 = _mm256_broadcast_sd(H + 28*i0 + k3 + 57);
      _t30_9 = _mm256_broadcast_sd(H + 28*i0 + k3 + 58);
      _t30_8 = _mm256_broadcast_sd(H + 28*i0 + k3 + 59);
      _t30_7 = _mm256_broadcast_sd(H + 28*i0 + k3 + 84);
      _t30_6 = _mm256_broadcast_sd(H + 28*i0 + k3 + 85);
      _t30_5 = _mm256_broadcast_sd(H + 28*i0 + k3 + 86);
      _t30_4 = _mm256_broadcast_sd(H + 28*i0 + k3 + 87);
      _t30_3 = _mm256_loadu_pd(Y + 29*k3);
      _t30_2 = _mm256_maskload_pd(Y + 29*k3 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
      _t30_1 = _mm256_maskload_pd(Y + 29*k3 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
      _t30_0 = _mm256_maskload_pd(Y + 29*k3 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

      // AVX Loader:

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t30_24 = _t30_3;
      _t30_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 3), _t30_2, 12);
      _t30_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 0), _t30_1, 49);
      _t30_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t30_3, _t30_2, 12), _mm256_shuffle_pd(_t30_1, _t30_0, 12), 49);

      // 4-BLAC: 4x4 * 4x4
      _t30_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_19, _t30_24), _mm256_mul_pd(_t30_18, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_17, _t30_26), _mm256_mul_pd(_t30_16, _t30_27)));
      _t30_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_15, _t30_24), _mm256_mul_pd(_t30_14, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_13, _t30_26), _mm256_mul_pd(_t30_12, _t30_27)));
      _t30_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_11, _t30_24), _mm256_mul_pd(_t30_10, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_9, _t30_26), _mm256_mul_pd(_t30_8, _t30_27)));
      _t30_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t30_7, _t30_24), _mm256_mul_pd(_t30_6, _t30_25)), _mm256_add_pd(_mm256_mul_pd(_t30_5, _t30_26), _mm256_mul_pd(_t30_4, _t30_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t28_4 = _mm256_add_pd(_t28_4, _t30_20);
      _t28_5 = _mm256_add_pd(_t28_5, _t30_21);
      _t28_6 = _mm256_add_pd(_t28_6, _t30_22);
      _t28_7 = _mm256_add_pd(_t28_7, _t30_23);

      // AVX Storer:

      for( int k2 = 4*floord(k3 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t31_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
        _t31_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
        _t31_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
        _t31_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
        _t31_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
        _t31_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
        _t31_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
        _t31_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
        _t31_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
        _t31_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
        _t31_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
        _t31_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
        _t31_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
        _t31_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
        _t31_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
        _t31_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
        _t31_3 = _mm256_loadu_pd(Y + k2 + 28*k3);
        _t31_2 = _mm256_loadu_pd(Y + k2 + 28*k3 + 28);
        _t31_1 = _mm256_loadu_pd(Y + k2 + 28*k3 + 56);
        _t31_0 = _mm256_loadu_pd(Y + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t31_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 32);
        _t31_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 32);
        _t31_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t31_3, _t31_2), _mm256_unpacklo_pd(_t31_1, _t31_0), 49);
        _t31_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t31_3, _t31_2), _mm256_unpackhi_pd(_t31_1, _t31_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t31_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_19, _t31_24), _mm256_mul_pd(_t31_18, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_17, _t31_26), _mm256_mul_pd(_t31_16, _t31_27)));
        _t31_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_15, _t31_24), _mm256_mul_pd(_t31_14, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_13, _t31_26), _mm256_mul_pd(_t31_12, _t31_27)));
        _t31_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_11, _t31_24), _mm256_mul_pd(_t31_10, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_9, _t31_26), _mm256_mul_pd(_t31_8, _t31_27)));
        _t31_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t31_7, _t31_24), _mm256_mul_pd(_t31_6, _t31_25)), _mm256_add_pd(_mm256_mul_pd(_t31_5, _t31_26), _mm256_mul_pd(_t31_4, _t31_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t28_4 = _mm256_add_pd(_t28_4, _t31_20);
        _t28_5 = _mm256_add_pd(_t28_5, _t31_21);
        _t28_6 = _mm256_add_pd(_t28_6, _t31_22);
        _t28_7 = _mm256_add_pd(_t28_7, _t31_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M1 + 28*i0 + k3, _t28_4);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 28, _t28_5);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 56, _t28_6);
      _mm256_storeu_pd(M1 + 28*i0 + k3 + 84, _t28_7);
    }
    _t32_3 = _mm256_loadu_pd(Y + 24);
    _t32_2 = _mm256_loadu_pd(Y + 52);
    _t32_1 = _mm256_loadu_pd(Y + 80);
    _t32_0 = _mm256_loadu_pd(Y + 108);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t32_4 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_15, _t32_3), _mm256_mul_pd(_t24_14, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_13, _t32_1), _mm256_mul_pd(_t24_12, _t32_0)));
    _t32_5 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_11, _t32_3), _mm256_mul_pd(_t24_10, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_9, _t32_1), _mm256_mul_pd(_t24_8, _t32_0)));
    _t32_6 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_7, _t32_3), _mm256_mul_pd(_t24_6, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_5, _t32_1), _mm256_mul_pd(_t24_4, _t32_0)));
    _t32_7 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t24_3, _t32_3), _mm256_mul_pd(_t24_2, _t32_2)), _mm256_add_pd(_mm256_mul_pd(_t24_1, _t32_1), _mm256_mul_pd(_t24_0, _t32_0)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t33_19 = _mm256_broadcast_sd(H + 28*i0 + k2);
      _t33_18 = _mm256_broadcast_sd(H + 28*i0 + k2 + 1);
      _t33_17 = _mm256_broadcast_sd(H + 28*i0 + k2 + 2);
      _t33_16 = _mm256_broadcast_sd(H + 28*i0 + k2 + 3);
      _t33_15 = _mm256_broadcast_sd(H + 28*i0 + k2 + 28);
      _t33_14 = _mm256_broadcast_sd(H + 28*i0 + k2 + 29);
      _t33_13 = _mm256_broadcast_sd(H + 28*i0 + k2 + 30);
      _t33_12 = _mm256_broadcast_sd(H + 28*i0 + k2 + 31);
      _t33_11 = _mm256_broadcast_sd(H + 28*i0 + k2 + 56);
      _t33_10 = _mm256_broadcast_sd(H + 28*i0 + k2 + 57);
      _t33_9 = _mm256_broadcast_sd(H + 28*i0 + k2 + 58);
      _t33_8 = _mm256_broadcast_sd(H + 28*i0 + k2 + 59);
      _t33_7 = _mm256_broadcast_sd(H + 28*i0 + k2 + 84);
      _t33_6 = _mm256_broadcast_sd(H + 28*i0 + k2 + 85);
      _t33_5 = _mm256_broadcast_sd(H + 28*i0 + k2 + 86);
      _t33_4 = _mm256_broadcast_sd(H + 28*i0 + k2 + 87);
      _t33_3 = _mm256_loadu_pd(Y + 28*k2 + 24);
      _t33_2 = _mm256_loadu_pd(Y + 28*k2 + 52);
      _t33_1 = _mm256_loadu_pd(Y + 28*k2 + 80);
      _t33_0 = _mm256_loadu_pd(Y + 28*k2 + 108);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t33_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_19, _t33_3), _mm256_mul_pd(_t33_18, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_17, _t33_1), _mm256_mul_pd(_t33_16, _t33_0)));
      _t33_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_15, _t33_3), _mm256_mul_pd(_t33_14, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_13, _t33_1), _mm256_mul_pd(_t33_12, _t33_0)));
      _t33_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_11, _t33_3), _mm256_mul_pd(_t33_10, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_9, _t33_1), _mm256_mul_pd(_t33_8, _t33_0)));
      _t33_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t33_7, _t33_3), _mm256_mul_pd(_t33_6, _t33_2)), _mm256_add_pd(_mm256_mul_pd(_t33_5, _t33_1), _mm256_mul_pd(_t33_4, _t33_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t32_4 = _mm256_add_pd(_t32_4, _t33_20);
      _t32_5 = _mm256_add_pd(_t32_5, _t33_21);
      _t32_6 = _mm256_add_pd(_t32_6, _t33_22);
      _t32_7 = _mm256_add_pd(_t32_7, _t33_23);

      // AVX Storer:
    }
    _t34_15 = _mm256_broadcast_sd(H + 28*i0 + 24);
    _t34_14 = _mm256_broadcast_sd(H + 28*i0 + 25);
    _t34_13 = _mm256_broadcast_sd(H + 28*i0 + 26);
    _t34_12 = _mm256_broadcast_sd(H + 28*i0 + 27);
    _t34_11 = _mm256_broadcast_sd(H + 28*i0 + 52);
    _t34_10 = _mm256_broadcast_sd(H + 28*i0 + 53);
    _t34_9 = _mm256_broadcast_sd(H + 28*i0 + 54);
    _t34_8 = _mm256_broadcast_sd(H + 28*i0 + 55);
    _t34_7 = _mm256_broadcast_sd(H + 28*i0 + 80);
    _t34_6 = _mm256_broadcast_sd(H + 28*i0 + 81);
    _t34_5 = _mm256_broadcast_sd(H + 28*i0 + 82);
    _t34_4 = _mm256_broadcast_sd(H + 28*i0 + 83);
    _t34_3 = _mm256_broadcast_sd(H + 28*i0 + 108);
    _t34_2 = _mm256_broadcast_sd(H + 28*i0 + 109);
    _t34_1 = _mm256_broadcast_sd(H + 28*i0 + 110);
    _t34_0 = _mm256_broadcast_sd(H + 28*i0 + 111);

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t34_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_15, _t23_16), _mm256_mul_pd(_t34_14, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_13, _t23_18), _mm256_mul_pd(_t34_12, _t23_19)));
    _t34_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_11, _t23_16), _mm256_mul_pd(_t34_10, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_9, _t23_18), _mm256_mul_pd(_t34_8, _t23_19)));
    _t34_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_7, _t23_16), _mm256_mul_pd(_t34_6, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_5, _t23_18), _mm256_mul_pd(_t34_4, _t23_19)));
    _t34_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t34_3, _t23_16), _mm256_mul_pd(_t34_2, _t23_17)), _mm256_add_pd(_mm256_mul_pd(_t34_1, _t23_18), _mm256_mul_pd(_t34_0, _t23_19)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t32_4 = _mm256_add_pd(_t32_4, _t34_16);
    _t32_5 = _mm256_add_pd(_t32_5, _t34_17);
    _t32_6 = _mm256_add_pd(_t32_6, _t34_18);
    _t32_7 = _mm256_add_pd(_t32_7, _t34_19);

    // AVX Storer:
    _mm256_storeu_pd(M1 + 28*i0, _t24_16);
    _mm256_storeu_pd(M1 + 28*i0 + 28, _t24_17);
    _mm256_storeu_pd(M1 + 28*i0 + 56, _t24_18);
    _mm256_storeu_pd(M1 + 28*i0 + 84, _t24_19);
    _mm256_storeu_pd(M1 + 28*i0 + 4, _t26_20);
    _mm256_storeu_pd(M1 + 28*i0 + 32, _t26_21);
    _mm256_storeu_pd(M1 + 28*i0 + 60, _t26_22);
    _mm256_storeu_pd(M1 + 28*i0 + 88, _t26_23);
    _mm256_storeu_pd(M1 + 28*i0 + 24, _t32_4);
    _mm256_storeu_pd(M1 + 28*i0 + 52, _t32_5);
    _mm256_storeu_pd(M1 + 28*i0 + 80, _t32_6);
    _mm256_storeu_pd(M1 + 28*i0 + 108, _t32_7);
  }


  // Generating : M2[28,8] = ( ( ( Sum_{k3} ( ( S(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, 0)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, 0)) ) ),h(4, 8, k3)) + Sum_{k2} ( $(h(4, 28, 0), ( G(h(4, 28, 0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, k2)) ) ),h(4, 8, k3)) ) ) ) + Sum_{k3} ( ( ( S(h(4, 28, 4), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 4)) ) * T( G(h(4, 8, k3), H[8,28],h(4, 28, 0)) ) ),h(4, 8, k3)) + $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, 4)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, 4)) ) ),h(4, 8, k3)) ) + Sum_{k2} ( $(h(4, 28, 4), ( G(h(4, 28, 4), Y[28,28],h(4, 28, k2)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, k2)) ) ),h(4, 8, k3)) ) ) ) ) + Sum_{i0} ( Sum_{k3} ( ( ( ( S(h(4, 28, i0), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 8, k3), H[8,28],h(4, 28, 0)) ) ),h(4, 8, k3)) + Sum_{k2} ( $(h(4, 28, i0), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, i0)) ) * T( G(h(4, 8, k3), H[8,28],h(4, 28, k2)) ) ),h(4, 8, k3)) ) ) + $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, i0)) ) ),h(4, 8, k3)) ) + Sum_{k2} ( $(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k2)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, k2)) ) ),h(4, 8, k3)) ) ) ) ) ) + Sum_{k3} ( ( ( S(h(4, 28, 24), ( T( G(h(4, 28, 0), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 8, k3), H[8,28],h(4, 28, 0)) ) ),h(4, 8, k3)) + Sum_{k2} ( $(h(4, 28, 24), ( T( G(h(4, 28, k2), Y[28,28],h(4, 28, 24)) ) * T( G(h(4, 8, k3), H[8,28],h(4, 28, k2)) ) ),h(4, 8, k3)) ) ) + $(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) * T( G(h(4, 8, k3), H[8,28],h(4, 28, 24)) ) ),h(4, 8, k3)) ) ) )

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t35_0 = _t23_7;
  _t35_1 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 3), _t23_6, 12);
  _t35_2 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 0), _t23_5, 49);
  _t35_3 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_7, _t23_6, 12), _mm256_shuffle_pd(_t23_5, _t23_4, 12), 49);


  for( int k3 = 0; k3 <= 7; k3+=4 ) {
    _t36_19 = _mm256_loadu_pd(H + 28*k3);
    _t36_18 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t36_17 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t36_16 = _mm256_loadu_pd(H + 28*k3 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t36_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_19, _t36_18), _mm256_unpacklo_pd(_t36_17, _t36_16), 32);
    _t36_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_19, _t36_18), _mm256_unpackhi_pd(_t36_17, _t36_16), 32);
    _t36_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t36_19, _t36_18), _mm256_unpacklo_pd(_t36_17, _t36_16), 49);
    _t36_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t36_19, _t36_18), _mm256_unpackhi_pd(_t36_17, _t36_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t36_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_15, _t36_24), _mm256_mul_pd(_t36_14, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_13, _t36_26), _mm256_mul_pd(_t36_12, _t36_27)));
    _t36_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_11, _t36_24), _mm256_mul_pd(_t36_10, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_9, _t36_26), _mm256_mul_pd(_t36_8, _t36_27)));
    _t36_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_7, _t36_24), _mm256_mul_pd(_t36_6, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_5, _t36_26), _mm256_mul_pd(_t36_4, _t36_27)));
    _t36_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t36_3, _t36_24), _mm256_mul_pd(_t36_2, _t36_25)), _mm256_add_pd(_mm256_mul_pd(_t36_1, _t36_26), _mm256_mul_pd(_t36_0, _t36_27)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 27; k2+=4 ) {
      _t37_19 = _mm256_broadcast_sd(Y + k2);
      _t37_18 = _mm256_broadcast_sd(Y + k2 + 1);
      _t37_17 = _mm256_broadcast_sd(Y + k2 + 2);
      _t37_16 = _mm256_broadcast_sd(Y + k2 + 3);
      _t37_15 = _mm256_broadcast_sd(Y + k2 + 28);
      _t37_14 = _mm256_broadcast_sd(Y + k2 + 29);
      _t37_13 = _mm256_broadcast_sd(Y + k2 + 30);
      _t37_12 = _mm256_broadcast_sd(Y + k2 + 31);
      _t37_11 = _mm256_broadcast_sd(Y + k2 + 56);
      _t37_10 = _mm256_broadcast_sd(Y + k2 + 57);
      _t37_9 = _mm256_broadcast_sd(Y + k2 + 58);
      _t37_8 = _mm256_broadcast_sd(Y + k2 + 59);
      _t37_7 = _mm256_broadcast_sd(Y + k2 + 84);
      _t37_6 = _mm256_broadcast_sd(Y + k2 + 85);
      _t37_5 = _mm256_broadcast_sd(Y + k2 + 86);
      _t37_4 = _mm256_broadcast_sd(Y + k2 + 87);
      _t37_3 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t37_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t37_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t37_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t37_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 32);
      _t37_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 32);
      _t37_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t37_3, _t37_2), _mm256_unpacklo_pd(_t37_1, _t37_0), 49);
      _t37_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t37_3, _t37_2), _mm256_unpackhi_pd(_t37_1, _t37_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t37_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_19, _t37_24), _mm256_mul_pd(_t37_18, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_17, _t37_26), _mm256_mul_pd(_t37_16, _t37_27)));
      _t37_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_15, _t37_24), _mm256_mul_pd(_t37_14, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_13, _t37_26), _mm256_mul_pd(_t37_12, _t37_27)));
      _t37_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_11, _t37_24), _mm256_mul_pd(_t37_10, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_9, _t37_26), _mm256_mul_pd(_t37_8, _t37_27)));
      _t37_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t37_7, _t37_24), _mm256_mul_pd(_t37_6, _t37_25)), _mm256_add_pd(_mm256_mul_pd(_t37_5, _t37_26), _mm256_mul_pd(_t37_4, _t37_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t36_20 = _mm256_add_pd(_t36_20, _t37_20);
      _t36_21 = _mm256_add_pd(_t36_21, _t37_21);
      _t36_22 = _mm256_add_pd(_t36_22, _t37_22);
      _t36_23 = _mm256_add_pd(_t36_23, _t37_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + k3, _t36_20);
    _mm256_storeu_pd(M2 + k3 + 8, _t36_21);
    _mm256_storeu_pd(M2 + k3 + 16, _t36_22);
    _mm256_storeu_pd(M2 + k3 + 24, _t36_23);
  }

  _t38_3 = _mm256_loadu_pd(Y + 4);
  _t38_2 = _mm256_loadu_pd(Y + 32);
  _t38_1 = _mm256_loadu_pd(Y + 60);
  _t38_0 = _mm256_loadu_pd(Y + 88);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t38_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 32);
  _t38_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 32);
  _t38_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t38_3, _t38_2), _mm256_unpacklo_pd(_t38_1, _t38_0), 49);
  _t38_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t38_3, _t38_2), _mm256_unpackhi_pd(_t38_1, _t38_0), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t38_4 = _t23_3;
  _t38_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 3), _t23_2, 12);
  _t38_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 0), _t23_1, 49);
  _t38_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t23_3, _t23_2, 12), _mm256_shuffle_pd(_t23_1, _t23_0, 12), 49);


  for( int k3 = 0; k3 <= 7; k3+=4 ) {
    _t39_39 = _mm256_loadu_pd(H + 28*k3);
    _t39_38 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t39_37 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t39_36 = _mm256_loadu_pd(H + 28*k3 + 84);
    _t39_35 = _mm256_loadu_pd(H + 28*k3 + 4);
    _t39_34 = _mm256_loadu_pd(H + 28*k3 + 32);
    _t39_33 = _mm256_loadu_pd(H + 28*k3 + 60);
    _t39_32 = _mm256_loadu_pd(H + 28*k3 + 88);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_48 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_39, _t39_38), _mm256_unpacklo_pd(_t39_37, _t39_36), 32);
    _t39_49 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_39, _t39_38), _mm256_unpackhi_pd(_t39_37, _t39_36), 32);
    _t39_50 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_39, _t39_38), _mm256_unpacklo_pd(_t39_37, _t39_36), 49);
    _t39_51 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_39, _t39_38), _mm256_unpackhi_pd(_t39_37, _t39_36), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_15, _t39_48), _mm256_mul_pd(_t39_14, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_13, _t39_50), _mm256_mul_pd(_t39_12, _t39_51)));
    _t39_41 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_11, _t39_48), _mm256_mul_pd(_t39_10, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_9, _t39_50), _mm256_mul_pd(_t39_8, _t39_51)));
    _t39_42 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_7, _t39_48), _mm256_mul_pd(_t39_6, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_5, _t39_50), _mm256_mul_pd(_t39_4, _t39_51)));
    _t39_43 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_3, _t39_48), _mm256_mul_pd(_t39_2, _t39_49)), _mm256_add_pd(_mm256_mul_pd(_t39_1, _t39_50), _mm256_mul_pd(_t39_0, _t39_51)));

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t39_52 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_35, _t39_34), _mm256_unpacklo_pd(_t39_33, _t39_32), 32);
    _t39_53 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_35, _t39_34), _mm256_unpackhi_pd(_t39_33, _t39_32), 32);
    _t39_54 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t39_35, _t39_34), _mm256_unpacklo_pd(_t39_33, _t39_32), 49);
    _t39_55 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t39_35, _t39_34), _mm256_unpackhi_pd(_t39_33, _t39_32), 49);

    // 4-BLAC: 4x4 * 4x4
    _t39_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_31, _t39_52), _mm256_mul_pd(_t39_30, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_29, _t39_54), _mm256_mul_pd(_t39_28, _t39_55)));
    _t39_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_27, _t39_52), _mm256_mul_pd(_t39_26, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_25, _t39_54), _mm256_mul_pd(_t39_24, _t39_55)));
    _t39_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_23, _t39_52), _mm256_mul_pd(_t39_22, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_21, _t39_54), _mm256_mul_pd(_t39_20, _t39_55)));
    _t39_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t39_19, _t39_52), _mm256_mul_pd(_t39_18, _t39_53)), _mm256_add_pd(_mm256_mul_pd(_t39_17, _t39_54), _mm256_mul_pd(_t39_16, _t39_55)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t39_40 = _mm256_add_pd(_t39_40, _t39_44);
    _t39_41 = _mm256_add_pd(_t39_41, _t39_45);
    _t39_42 = _mm256_add_pd(_t39_42, _t39_46);
    _t39_43 = _mm256_add_pd(_t39_43, _t39_47);

    // AVX Storer:

    for( int k2 = 8; k2 <= 27; k2+=4 ) {
      _t40_19 = _mm256_broadcast_sd(Y + k2 + 112);
      _t40_18 = _mm256_broadcast_sd(Y + k2 + 113);
      _t40_17 = _mm256_broadcast_sd(Y + k2 + 114);
      _t40_16 = _mm256_broadcast_sd(Y + k2 + 115);
      _t40_15 = _mm256_broadcast_sd(Y + k2 + 140);
      _t40_14 = _mm256_broadcast_sd(Y + k2 + 141);
      _t40_13 = _mm256_broadcast_sd(Y + k2 + 142);
      _t40_12 = _mm256_broadcast_sd(Y + k2 + 143);
      _t40_11 = _mm256_broadcast_sd(Y + k2 + 168);
      _t40_10 = _mm256_broadcast_sd(Y + k2 + 169);
      _t40_9 = _mm256_broadcast_sd(Y + k2 + 170);
      _t40_8 = _mm256_broadcast_sd(Y + k2 + 171);
      _t40_7 = _mm256_broadcast_sd(Y + k2 + 196);
      _t40_6 = _mm256_broadcast_sd(Y + k2 + 197);
      _t40_5 = _mm256_broadcast_sd(Y + k2 + 198);
      _t40_4 = _mm256_broadcast_sd(Y + k2 + 199);
      _t40_3 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t40_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t40_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t40_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t40_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 32);
      _t40_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 32);
      _t40_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t40_3, _t40_2), _mm256_unpacklo_pd(_t40_1, _t40_0), 49);
      _t40_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t40_3, _t40_2), _mm256_unpackhi_pd(_t40_1, _t40_0), 49);

      // 4-BLAC: 4x4 * 4x4
      _t40_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_19, _t40_24), _mm256_mul_pd(_t40_18, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_17, _t40_26), _mm256_mul_pd(_t40_16, _t40_27)));
      _t40_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_15, _t40_24), _mm256_mul_pd(_t40_14, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_13, _t40_26), _mm256_mul_pd(_t40_12, _t40_27)));
      _t40_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_11, _t40_24), _mm256_mul_pd(_t40_10, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_9, _t40_26), _mm256_mul_pd(_t40_8, _t40_27)));
      _t40_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t40_7, _t40_24), _mm256_mul_pd(_t40_6, _t40_25)), _mm256_add_pd(_mm256_mul_pd(_t40_5, _t40_26), _mm256_mul_pd(_t40_4, _t40_27)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t39_40 = _mm256_add_pd(_t39_40, _t40_20);
      _t39_41 = _mm256_add_pd(_t39_41, _t40_21);
      _t39_42 = _mm256_add_pd(_t39_42, _t40_22);
      _t39_43 = _mm256_add_pd(_t39_43, _t40_23);

      // AVX Storer:
    }
    _mm256_storeu_pd(M2 + k3 + 32, _t39_40);
    _mm256_storeu_pd(M2 + k3 + 40, _t39_41);
    _mm256_storeu_pd(M2 + k3 + 48, _t39_42);
    _mm256_storeu_pd(M2 + k3 + 56, _t39_43);
  }


  for( int i0 = 8; i0 <= 23; i0+=4 ) {
    _t41_7 = _mm256_loadu_pd(Y + i0);
    _t41_6 = _mm256_loadu_pd(Y + i0 + 28);
    _t41_5 = _mm256_loadu_pd(Y + i0 + 56);
    _t41_4 = _mm256_loadu_pd(Y + i0 + 84);
    _t41_3 = _mm256_loadu_pd(Y + 29*i0);
    _t41_2 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t41_1 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t41_0 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t41_12 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
    _t41_13 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
    _t41_14 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
    _t41_15 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t41_8 = _t41_3;
    _t41_9 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
    _t41_10 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
    _t41_11 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

    for( int k3 = 0; k3 <= 7; k3+=4 ) {
      _t42_19 = _mm256_loadu_pd(H + 28*k3);
      _t42_18 = _mm256_loadu_pd(H + 28*k3 + 28);
      _t42_17 = _mm256_loadu_pd(H + 28*k3 + 56);
      _t42_16 = _mm256_loadu_pd(H + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 32);
      _t42_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 32);
      _t42_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t41_7, _t41_6), _mm256_unpacklo_pd(_t41_5, _t41_4), 49);
      _t42_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t41_7, _t41_6), _mm256_unpackhi_pd(_t41_5, _t41_4), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t42_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_19, _t42_18), _mm256_unpacklo_pd(_t42_17, _t42_16), 32);
      _t42_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_19, _t42_18), _mm256_unpackhi_pd(_t42_17, _t42_16), 32);
      _t42_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t42_19, _t42_18), _mm256_unpacklo_pd(_t42_17, _t42_16), 49);
      _t42_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t42_19, _t42_18), _mm256_unpackhi_pd(_t42_17, _t42_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t42_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_15, _t42_28), _mm256_mul_pd(_t42_14, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_13, _t42_30), _mm256_mul_pd(_t42_12, _t42_31)));
      _t42_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_11, _t42_28), _mm256_mul_pd(_t42_10, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_9, _t42_30), _mm256_mul_pd(_t42_8, _t42_31)));
      _t42_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_7, _t42_28), _mm256_mul_pd(_t42_6, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_5, _t42_30), _mm256_mul_pd(_t42_4, _t42_31)));
      _t42_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t42_3, _t42_28), _mm256_mul_pd(_t42_2, _t42_29)), _mm256_add_pd(_mm256_mul_pd(_t42_1, _t42_30), _mm256_mul_pd(_t42_0, _t42_31)));

      // AVX Storer:

      for( int k2 = 4; k2 <= i0 - 1; k2+=4 ) {
        _t43_23 = _mm256_loadu_pd(Y + i0 + 28*k2);
        _t43_22 = _mm256_loadu_pd(Y + i0 + 28*k2 + 28);
        _t43_21 = _mm256_loadu_pd(Y + i0 + 28*k2 + 56);
        _t43_20 = _mm256_loadu_pd(Y + i0 + 28*k2 + 84);
        _t43_19 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t43_18 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t43_17 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t43_16 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_23, _t43_22), _mm256_unpacklo_pd(_t43_21, _t43_20), 32);
        _t43_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_23, _t43_22), _mm256_unpackhi_pd(_t43_21, _t43_20), 32);
        _t43_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_23, _t43_22), _mm256_unpacklo_pd(_t43_21, _t43_20), 49);
        _t43_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_23, _t43_22), _mm256_unpackhi_pd(_t43_21, _t43_20), 49);

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t43_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_19, _t43_18), _mm256_unpacklo_pd(_t43_17, _t43_16), 32);
        _t43_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_19, _t43_18), _mm256_unpackhi_pd(_t43_17, _t43_16), 32);
        _t43_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t43_19, _t43_18), _mm256_unpacklo_pd(_t43_17, _t43_16), 49);
        _t43_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t43_19, _t43_18), _mm256_unpackhi_pd(_t43_17, _t43_16), 49);

        // 4-BLAC: 4x4 * 4x4
        _t43_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_15, _t43_32), _mm256_mul_pd(_t43_14, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_13, _t43_34), _mm256_mul_pd(_t43_12, _t43_35)));
        _t43_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_11, _t43_32), _mm256_mul_pd(_t43_10, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_9, _t43_34), _mm256_mul_pd(_t43_8, _t43_35)));
        _t43_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_7, _t43_32), _mm256_mul_pd(_t43_6, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_5, _t43_34), _mm256_mul_pd(_t43_4, _t43_35)));
        _t43_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t43_3, _t43_32), _mm256_mul_pd(_t43_2, _t43_33)), _mm256_add_pd(_mm256_mul_pd(_t43_1, _t43_34), _mm256_mul_pd(_t43_0, _t43_35)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t42_20 = _mm256_add_pd(_t42_20, _t43_24);
        _t42_21 = _mm256_add_pd(_t42_21, _t43_25);
        _t42_22 = _mm256_add_pd(_t42_22, _t43_26);
        _t42_23 = _mm256_add_pd(_t42_23, _t43_27);

        // AVX Storer:
      }
      _t44_19 = _mm256_loadu_pd(H + i0 + 28*k3);
      _t44_18 = _mm256_loadu_pd(H + i0 + 28*k3 + 28);
      _t44_17 = _mm256_loadu_pd(H + i0 + 28*k3 + 56);
      _t44_16 = _mm256_loadu_pd(H + i0 + 28*k3 + 84);

      // AVX Loader:

      // 4x4 -> 4x4 - UpSymm
      _t44_24 = _t41_3;
      _t44_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 3), _t41_2, 12);
      _t44_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 0), _t41_1, 49);
      _t44_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t41_3, _t41_2, 12), _mm256_shuffle_pd(_t41_1, _t41_0, 12), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t44_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_19, _t44_18), _mm256_unpacklo_pd(_t44_17, _t44_16), 32);
      _t44_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_19, _t44_18), _mm256_unpackhi_pd(_t44_17, _t44_16), 32);
      _t44_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t44_19, _t44_18), _mm256_unpacklo_pd(_t44_17, _t44_16), 49);
      _t44_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t44_19, _t44_18), _mm256_unpackhi_pd(_t44_17, _t44_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t44_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_15, _t44_28), _mm256_mul_pd(_t44_14, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_13, _t44_30), _mm256_mul_pd(_t44_12, _t44_31)));
      _t44_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_11, _t44_28), _mm256_mul_pd(_t44_10, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_9, _t44_30), _mm256_mul_pd(_t44_8, _t44_31)));
      _t44_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_7, _t44_28), _mm256_mul_pd(_t44_6, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_5, _t44_30), _mm256_mul_pd(_t44_4, _t44_31)));
      _t44_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t44_3, _t44_28), _mm256_mul_pd(_t44_2, _t44_29)), _mm256_add_pd(_mm256_mul_pd(_t44_1, _t44_30), _mm256_mul_pd(_t44_0, _t44_31)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t42_20 = _mm256_add_pd(_t42_20, _t44_20);
      _t42_21 = _mm256_add_pd(_t42_21, _t44_21);
      _t42_22 = _mm256_add_pd(_t42_22, _t44_22);
      _t42_23 = _mm256_add_pd(_t42_23, _t44_23);

      // AVX Storer:

      for( int k2 = 4*floord(i0 - 1, 4) + 8; k2 <= 27; k2+=4 ) {
        _t45_19 = _mm256_broadcast_sd(Y + 28*i0 + k2);
        _t45_18 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 1);
        _t45_17 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 2);
        _t45_16 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 3);
        _t45_15 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 28);
        _t45_14 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 29);
        _t45_13 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 30);
        _t45_12 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 31);
        _t45_11 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 56);
        _t45_10 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 57);
        _t45_9 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 58);
        _t45_8 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 59);
        _t45_7 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 84);
        _t45_6 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 85);
        _t45_5 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 86);
        _t45_4 = _mm256_broadcast_sd(Y + 28*i0 + k2 + 87);
        _t45_3 = _mm256_loadu_pd(H + k2 + 28*k3);
        _t45_2 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
        _t45_1 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
        _t45_0 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

        // AVX Loader:

        // AVX Loader:

        // 4-BLAC: (4x4)^T
        _t45_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 32);
        _t45_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 32);
        _t45_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t45_3, _t45_2), _mm256_unpacklo_pd(_t45_1, _t45_0), 49);
        _t45_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t45_3, _t45_2), _mm256_unpackhi_pd(_t45_1, _t45_0), 49);

        // 4-BLAC: 4x4 * 4x4
        _t45_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_19, _t45_24), _mm256_mul_pd(_t45_18, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_17, _t45_26), _mm256_mul_pd(_t45_16, _t45_27)));
        _t45_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_15, _t45_24), _mm256_mul_pd(_t45_14, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_13, _t45_26), _mm256_mul_pd(_t45_12, _t45_27)));
        _t45_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_11, _t45_24), _mm256_mul_pd(_t45_10, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_9, _t45_26), _mm256_mul_pd(_t45_8, _t45_27)));
        _t45_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t45_7, _t45_24), _mm256_mul_pd(_t45_6, _t45_25)), _mm256_add_pd(_mm256_mul_pd(_t45_5, _t45_26), _mm256_mul_pd(_t45_4, _t45_27)));

        // AVX Loader:

        // 4-BLAC: 4x4 + 4x4
        _t42_20 = _mm256_add_pd(_t42_20, _t45_20);
        _t42_21 = _mm256_add_pd(_t42_21, _t45_21);
        _t42_22 = _mm256_add_pd(_t42_22, _t45_22);
        _t42_23 = _mm256_add_pd(_t42_23, _t45_23);

        // AVX Storer:
      }
      _mm256_storeu_pd(M2 + 8*i0 + k3, _t42_20);
      _mm256_storeu_pd(M2 + 8*i0 + k3 + 8, _t42_21);
      _mm256_storeu_pd(M2 + 8*i0 + k3 + 16, _t42_22);
      _mm256_storeu_pd(M2 + 8*i0 + k3 + 24, _t42_23);
    }
  }

  _t46_3 = _mm256_loadu_pd(Y + 24);
  _t46_2 = _mm256_loadu_pd(Y + 52);
  _t46_1 = _mm256_loadu_pd(Y + 80);
  _t46_0 = _mm256_loadu_pd(Y + 108);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t46_8 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 32);
  _t46_9 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 32);
  _t46_10 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t46_3, _t46_2), _mm256_unpacklo_pd(_t46_1, _t46_0), 49);
  _t46_11 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t46_3, _t46_2), _mm256_unpackhi_pd(_t46_1, _t46_0), 49);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t46_4 = _t19_28;
  _t46_5 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t46_6 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t46_7 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);


  for( int k3 = 0; k3 <= 7; k3+=4 ) {
    _t47_19 = _mm256_loadu_pd(H + 28*k3);
    _t47_18 = _mm256_loadu_pd(H + 28*k3 + 28);
    _t47_17 = _mm256_loadu_pd(H + 28*k3 + 56);
    _t47_16 = _mm256_loadu_pd(H + 28*k3 + 84);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t47_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_19, _t47_18), _mm256_unpacklo_pd(_t47_17, _t47_16), 32);
    _t47_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_19, _t47_18), _mm256_unpackhi_pd(_t47_17, _t47_16), 32);
    _t47_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t47_19, _t47_18), _mm256_unpacklo_pd(_t47_17, _t47_16), 49);
    _t47_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t47_19, _t47_18), _mm256_unpackhi_pd(_t47_17, _t47_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t47_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_15, _t47_24), _mm256_mul_pd(_t47_14, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_13, _t47_26), _mm256_mul_pd(_t47_12, _t47_27)));
    _t47_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_11, _t47_24), _mm256_mul_pd(_t47_10, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_9, _t47_26), _mm256_mul_pd(_t47_8, _t47_27)));
    _t47_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_7, _t47_24), _mm256_mul_pd(_t47_6, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_5, _t47_26), _mm256_mul_pd(_t47_4, _t47_27)));
    _t47_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t47_3, _t47_24), _mm256_mul_pd(_t47_2, _t47_25)), _mm256_add_pd(_mm256_mul_pd(_t47_1, _t47_26), _mm256_mul_pd(_t47_0, _t47_27)));

    // AVX Storer:

    for( int k2 = 4; k2 <= 23; k2+=4 ) {
      _t48_23 = _mm256_loadu_pd(Y + 28*k2 + 24);
      _t48_22 = _mm256_loadu_pd(Y + 28*k2 + 52);
      _t48_21 = _mm256_loadu_pd(Y + 28*k2 + 80);
      _t48_20 = _mm256_loadu_pd(Y + 28*k2 + 108);
      _t48_19 = _mm256_loadu_pd(H + k2 + 28*k3);
      _t48_18 = _mm256_loadu_pd(H + k2 + 28*k3 + 28);
      _t48_17 = _mm256_loadu_pd(H + k2 + 28*k3 + 56);
      _t48_16 = _mm256_loadu_pd(H + k2 + 28*k3 + 84);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_23, _t48_22), _mm256_unpacklo_pd(_t48_21, _t48_20), 32);
      _t48_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_23, _t48_22), _mm256_unpackhi_pd(_t48_21, _t48_20), 32);
      _t48_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_23, _t48_22), _mm256_unpacklo_pd(_t48_21, _t48_20), 49);
      _t48_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_23, _t48_22), _mm256_unpackhi_pd(_t48_21, _t48_20), 49);

      // AVX Loader:

      // 4-BLAC: (4x4)^T
      _t48_32 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_19, _t48_18), _mm256_unpacklo_pd(_t48_17, _t48_16), 32);
      _t48_33 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_19, _t48_18), _mm256_unpackhi_pd(_t48_17, _t48_16), 32);
      _t48_34 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t48_19, _t48_18), _mm256_unpacklo_pd(_t48_17, _t48_16), 49);
      _t48_35 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t48_19, _t48_18), _mm256_unpackhi_pd(_t48_17, _t48_16), 49);

      // 4-BLAC: 4x4 * 4x4
      _t48_24 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_15, _t48_32), _mm256_mul_pd(_t48_14, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_13, _t48_34), _mm256_mul_pd(_t48_12, _t48_35)));
      _t48_25 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_11, _t48_32), _mm256_mul_pd(_t48_10, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_9, _t48_34), _mm256_mul_pd(_t48_8, _t48_35)));
      _t48_26 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_7, _t48_32), _mm256_mul_pd(_t48_6, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_5, _t48_34), _mm256_mul_pd(_t48_4, _t48_35)));
      _t48_27 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t48_3, _t48_32), _mm256_mul_pd(_t48_2, _t48_33)), _mm256_add_pd(_mm256_mul_pd(_t48_1, _t48_34), _mm256_mul_pd(_t48_0, _t48_35)));

      // AVX Loader:

      // 4-BLAC: 4x4 + 4x4
      _t47_20 = _mm256_add_pd(_t47_20, _t48_24);
      _t47_21 = _mm256_add_pd(_t47_21, _t48_25);
      _t47_22 = _mm256_add_pd(_t47_22, _t48_26);
      _t47_23 = _mm256_add_pd(_t47_23, _t48_27);

      // AVX Storer:
    }
    _t49_19 = _mm256_loadu_pd(H + 28*k3 + 24);
    _t49_18 = _mm256_loadu_pd(H + 28*k3 + 52);
    _t49_17 = _mm256_loadu_pd(H + 28*k3 + 80);
    _t49_16 = _mm256_loadu_pd(H + 28*k3 + 108);

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t49_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t49_19, _t49_18), _mm256_unpacklo_pd(_t49_17, _t49_16), 32);
    _t49_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t49_19, _t49_18), _mm256_unpackhi_pd(_t49_17, _t49_16), 32);
    _t49_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t49_19, _t49_18), _mm256_unpacklo_pd(_t49_17, _t49_16), 49);
    _t49_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t49_19, _t49_18), _mm256_unpackhi_pd(_t49_17, _t49_16), 49);

    // 4-BLAC: 4x4 * 4x4
    _t49_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_15, _t49_24), _mm256_mul_pd(_t49_14, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_13, _t49_26), _mm256_mul_pd(_t49_12, _t49_27)));
    _t49_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_11, _t49_24), _mm256_mul_pd(_t49_10, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_9, _t49_26), _mm256_mul_pd(_t49_8, _t49_27)));
    _t49_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_7, _t49_24), _mm256_mul_pd(_t49_6, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_5, _t49_26), _mm256_mul_pd(_t49_4, _t49_27)));
    _t49_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t49_3, _t49_24), _mm256_mul_pd(_t49_2, _t49_25)), _mm256_add_pd(_mm256_mul_pd(_t49_1, _t49_26), _mm256_mul_pd(_t49_0, _t49_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t47_20 = _mm256_add_pd(_t47_20, _t49_20);
    _t47_21 = _mm256_add_pd(_t47_21, _t49_21);
    _t47_22 = _mm256_add_pd(_t47_22, _t49_22);
    _t47_23 = _mm256_add_pd(_t47_23, _t49_23);

    // AVX Storer:
    _mm256_storeu_pd(M2 + k3 + 192, _t47_20);
    _mm256_storeu_pd(M2 + k3 + 200, _t47_21);
    _mm256_storeu_pd(M2 + k3 + 208, _t47_22);
    _mm256_storeu_pd(M2 + k3 + 216, _t47_23);
  }

  _t50_23 = _mm256_broadcast_sd(M1);
  _t50_22 = _mm256_broadcast_sd(M1 + 1);
  _t50_21 = _mm256_broadcast_sd(M1 + 2);
  _t50_20 = _mm256_broadcast_sd(M1 + 3);
  _t50_19 = _mm256_broadcast_sd(M1 + 28);
  _t50_18 = _mm256_broadcast_sd(M1 + 29);
  _t50_17 = _mm256_broadcast_sd(M1 + 30);
  _t50_16 = _mm256_broadcast_sd(M1 + 31);
  _t50_15 = _mm256_broadcast_sd(M1 + 56);
  _t50_14 = _mm256_broadcast_sd(M1 + 57);
  _t50_13 = _mm256_broadcast_sd(M1 + 58);
  _t50_12 = _mm256_broadcast_sd(M1 + 59);
  _t50_11 = _mm256_broadcast_sd(M1 + 84);
  _t50_10 = _mm256_broadcast_sd(M1 + 85);
  _t50_9 = _mm256_broadcast_sd(M1 + 86);
  _t50_8 = _mm256_broadcast_sd(M1 + 87);
  _t50_7 = _mm256_loadu_pd(H);
  _t50_6 = _mm256_loadu_pd(H + 28);
  _t50_5 = _mm256_loadu_pd(H + 56);
  _t50_4 = _mm256_loadu_pd(H + 84);
  _t50_3 = _mm256_loadu_pd(R);
  _t50_2 = _mm256_maskload_pd(R + 8, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t50_1 = _mm256_maskload_pd(R + 16, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t50_0 = _mm256_maskload_pd(R + 24, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // Generating : M3[8,8] = ( ( ( ( ( S(h(4, 8, 0), ( ( G(h(4, 8, 0), M1[8,28],h(4, 28, 0)) * T( G(h(4, 8, 0), H[8,28],h(4, 28, 0)) ) ) + G(h(4, 8, 0), R[8,8],h(4, 8, 0)) ),h(4, 8, 0)) + Sum_{k2} ( $(h(4, 8, 0), ( G(h(4, 8, 0), M1[8,28],h(4, 28, k2)) * T( G(h(4, 8, 0), H[8,28],h(4, 28, k2)) ) ),h(4, 8, 0)) ) ) + S(h(4, 8, 0), ( ( G(h(4, 8, 0), M1[8,28],h(4, 28, 0)) * T( G(h(4, 8, 4), H[8,28],h(4, 28, 0)) ) ) + G(h(4, 8, 0), R[8,8],h(4, 8, 4)) ),h(4, 8, 4)) ) + Sum_{k2} ( $(h(4, 8, 0), ( G(h(4, 8, 0), M1[8,28],h(4, 28, k2)) * T( G(h(4, 8, 4), H[8,28],h(4, 28, k2)) ) ),h(4, 8, 4)) ) ) + S(h(4, 8, 4), ( ( G(h(4, 8, 4), M1[8,28],h(4, 28, 0)) * T( G(h(4, 8, 4), H[8,28],h(4, 28, 0)) ) ) + G(h(4, 8, 4), R[8,8],h(4, 8, 4)) ),h(4, 8, 4)) ) + Sum_{k2} ( $(h(4, 8, 4), ( G(h(4, 8, 4), M1[8,28],h(4, 28, k2)) * T( G(h(4, 8, 4), H[8,28],h(4, 28, k2)) ) ),h(4, 8, 4)) ) )

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t50_40 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 32);
  _t50_41 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 32);
  _t50_42 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t50_7, _t50_6), _mm256_unpacklo_pd(_t50_5, _t50_4), 49);
  _t50_43 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t50_7, _t50_6), _mm256_unpackhi_pd(_t50_5, _t50_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t50_32 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_23, _t50_40), _mm256_mul_pd(_t50_22, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_21, _t50_42), _mm256_mul_pd(_t50_20, _t50_43)));
  _t50_33 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t50_40), _mm256_mul_pd(_t50_18, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t50_42), _mm256_mul_pd(_t50_16, _t50_43)));
  _t50_34 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t50_40), _mm256_mul_pd(_t50_14, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t50_42), _mm256_mul_pd(_t50_12, _t50_43)));
  _t50_35 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t50_40), _mm256_mul_pd(_t50_10, _t50_41)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t50_42), _mm256_mul_pd(_t50_8, _t50_43)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t50_36 = _t50_3;
  _t50_37 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 3), _t50_2, 12);
  _t50_38 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 0), _t50_1, 49);
  _t50_39 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_3, _t50_2, 12), _mm256_shuffle_pd(_t50_1, _t50_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t50_24 = _mm256_add_pd(_t50_32, _t50_36);
  _t50_25 = _mm256_add_pd(_t50_33, _t50_37);
  _t50_26 = _mm256_add_pd(_t50_34, _t50_38);
  _t50_27 = _mm256_add_pd(_t50_35, _t50_39);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t50_28 = _t50_24;
  _t50_29 = _t50_25;
  _t50_30 = _t50_26;
  _t50_31 = _t50_27;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t51_19 = _mm256_broadcast_sd(M1 + k2);
    _t51_18 = _mm256_broadcast_sd(M1 + k2 + 1);
    _t51_17 = _mm256_broadcast_sd(M1 + k2 + 2);
    _t51_16 = _mm256_broadcast_sd(M1 + k2 + 3);
    _t51_15 = _mm256_broadcast_sd(M1 + k2 + 28);
    _t51_14 = _mm256_broadcast_sd(M1 + k2 + 29);
    _t51_13 = _mm256_broadcast_sd(M1 + k2 + 30);
    _t51_12 = _mm256_broadcast_sd(M1 + k2 + 31);
    _t51_11 = _mm256_broadcast_sd(M1 + k2 + 56);
    _t51_10 = _mm256_broadcast_sd(M1 + k2 + 57);
    _t51_9 = _mm256_broadcast_sd(M1 + k2 + 58);
    _t51_8 = _mm256_broadcast_sd(M1 + k2 + 59);
    _t51_7 = _mm256_broadcast_sd(M1 + k2 + 84);
    _t51_6 = _mm256_broadcast_sd(M1 + k2 + 85);
    _t51_5 = _mm256_broadcast_sd(M1 + k2 + 86);
    _t51_4 = _mm256_broadcast_sd(M1 + k2 + 87);
    _t51_3 = _mm256_loadu_pd(H + k2);
    _t51_2 = _mm256_loadu_pd(H + k2 + 28);
    _t51_1 = _mm256_loadu_pd(H + k2 + 56);
    _t51_0 = _mm256_loadu_pd(H + k2 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t51_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 32);
    _t51_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 32);
    _t51_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t51_3, _t51_2), _mm256_unpacklo_pd(_t51_1, _t51_0), 49);
    _t51_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t51_3, _t51_2), _mm256_unpackhi_pd(_t51_1, _t51_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t51_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_19, _t51_28), _mm256_mul_pd(_t51_18, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_17, _t51_30), _mm256_mul_pd(_t51_16, _t51_31)));
    _t51_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_15, _t51_28), _mm256_mul_pd(_t51_14, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_13, _t51_30), _mm256_mul_pd(_t51_12, _t51_31)));
    _t51_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_11, _t51_28), _mm256_mul_pd(_t51_10, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_9, _t51_30), _mm256_mul_pd(_t51_8, _t51_31)));
    _t51_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t51_7, _t51_28), _mm256_mul_pd(_t51_6, _t51_29)), _mm256_add_pd(_mm256_mul_pd(_t51_5, _t51_30), _mm256_mul_pd(_t51_4, _t51_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t51_24 = _t50_28;
    _t51_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 3), _t50_29, 12);
    _t51_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 0), _t50_30, 49);
    _t51_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t50_28, _t50_29, 12), _mm256_shuffle_pd(_t50_30, _t50_31, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t51_24 = _mm256_add_pd(_t51_24, _t51_20);
    _t51_25 = _mm256_add_pd(_t51_25, _t51_21);
    _t51_26 = _mm256_add_pd(_t51_26, _t51_22);
    _t51_27 = _mm256_add_pd(_t51_27, _t51_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t50_28 = _t51_24;
    _t50_29 = _t51_25;
    _t50_30 = _t51_26;
    _t50_31 = _t51_27;
  }

  _t52_7 = _mm256_loadu_pd(H + 112);
  _t52_6 = _mm256_loadu_pd(H + 140);
  _t52_5 = _mm256_loadu_pd(H + 168);
  _t52_4 = _mm256_loadu_pd(H + 196);
  _t52_3 = _mm256_loadu_pd(R + 4);
  _t52_2 = _mm256_loadu_pd(R + 12);
  _t52_1 = _mm256_loadu_pd(R + 20);
  _t52_0 = _mm256_loadu_pd(R + 28);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t52_16 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 32);
  _t52_17 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 32);
  _t52_18 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 49);
  _t52_19 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t52_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_23, _t52_16), _mm256_mul_pd(_t50_22, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_21, _t52_18), _mm256_mul_pd(_t50_20, _t52_19)));
  _t52_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_19, _t52_16), _mm256_mul_pd(_t50_18, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_17, _t52_18), _mm256_mul_pd(_t50_16, _t52_19)));
  _t52_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_15, _t52_16), _mm256_mul_pd(_t50_14, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_13, _t52_18), _mm256_mul_pd(_t50_12, _t52_19)));
  _t52_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t50_11, _t52_16), _mm256_mul_pd(_t50_10, _t52_17)), _mm256_add_pd(_mm256_mul_pd(_t50_9, _t52_18), _mm256_mul_pd(_t50_8, _t52_19)));

  // AVX Loader:

  // 4-BLAC: 4x4 + 4x4
  _t52_8 = _mm256_add_pd(_t52_12, _t52_3);
  _t52_9 = _mm256_add_pd(_t52_13, _t52_2);
  _t52_10 = _mm256_add_pd(_t52_14, _t52_1);
  _t52_11 = _mm256_add_pd(_t52_15, _t52_0);

  // AVX Storer:


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t53_19 = _mm256_broadcast_sd(M1 + k2);
    _t53_18 = _mm256_broadcast_sd(M1 + k2 + 1);
    _t53_17 = _mm256_broadcast_sd(M1 + k2 + 2);
    _t53_16 = _mm256_broadcast_sd(M1 + k2 + 3);
    _t53_15 = _mm256_broadcast_sd(M1 + k2 + 28);
    _t53_14 = _mm256_broadcast_sd(M1 + k2 + 29);
    _t53_13 = _mm256_broadcast_sd(M1 + k2 + 30);
    _t53_12 = _mm256_broadcast_sd(M1 + k2 + 31);
    _t53_11 = _mm256_broadcast_sd(M1 + k2 + 56);
    _t53_10 = _mm256_broadcast_sd(M1 + k2 + 57);
    _t53_9 = _mm256_broadcast_sd(M1 + k2 + 58);
    _t53_8 = _mm256_broadcast_sd(M1 + k2 + 59);
    _t53_7 = _mm256_broadcast_sd(M1 + k2 + 84);
    _t53_6 = _mm256_broadcast_sd(M1 + k2 + 85);
    _t53_5 = _mm256_broadcast_sd(M1 + k2 + 86);
    _t53_4 = _mm256_broadcast_sd(M1 + k2 + 87);
    _t53_3 = _mm256_loadu_pd(H + k2 + 112);
    _t53_2 = _mm256_loadu_pd(H + k2 + 140);
    _t53_1 = _mm256_loadu_pd(H + k2 + 168);
    _t53_0 = _mm256_loadu_pd(H + k2 + 196);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t53_24 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_3, _t53_2), _mm256_unpacklo_pd(_t53_1, _t53_0), 32);
    _t53_25 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_3, _t53_2), _mm256_unpackhi_pd(_t53_1, _t53_0), 32);
    _t53_26 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t53_3, _t53_2), _mm256_unpacklo_pd(_t53_1, _t53_0), 49);
    _t53_27 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t53_3, _t53_2), _mm256_unpackhi_pd(_t53_1, _t53_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t53_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_19, _t53_24), _mm256_mul_pd(_t53_18, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_17, _t53_26), _mm256_mul_pd(_t53_16, _t53_27)));
    _t53_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_15, _t53_24), _mm256_mul_pd(_t53_14, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_13, _t53_26), _mm256_mul_pd(_t53_12, _t53_27)));
    _t53_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_11, _t53_24), _mm256_mul_pd(_t53_10, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_9, _t53_26), _mm256_mul_pd(_t53_8, _t53_27)));
    _t53_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t53_7, _t53_24), _mm256_mul_pd(_t53_6, _t53_25)), _mm256_add_pd(_mm256_mul_pd(_t53_5, _t53_26), _mm256_mul_pd(_t53_4, _t53_27)));

    // AVX Loader:

    // 4-BLAC: 4x4 + 4x4
    _t52_8 = _mm256_add_pd(_t52_8, _t53_20);
    _t52_9 = _mm256_add_pd(_t52_9, _t53_21);
    _t52_10 = _mm256_add_pd(_t52_10, _t53_22);
    _t52_11 = _mm256_add_pd(_t52_11, _t53_23);

    // AVX Storer:
  }

  _t54_19 = _mm256_broadcast_sd(M1 + 112);
  _t54_18 = _mm256_broadcast_sd(M1 + 113);
  _t54_17 = _mm256_broadcast_sd(M1 + 114);
  _t54_16 = _mm256_broadcast_sd(M1 + 115);
  _t54_15 = _mm256_broadcast_sd(M1 + 140);
  _t54_14 = _mm256_broadcast_sd(M1 + 141);
  _t54_13 = _mm256_broadcast_sd(M1 + 142);
  _t54_12 = _mm256_broadcast_sd(M1 + 143);
  _t54_11 = _mm256_broadcast_sd(M1 + 168);
  _t54_10 = _mm256_broadcast_sd(M1 + 169);
  _t54_9 = _mm256_broadcast_sd(M1 + 170);
  _t54_8 = _mm256_broadcast_sd(M1 + 171);
  _t54_7 = _mm256_broadcast_sd(M1 + 196);
  _t54_6 = _mm256_broadcast_sd(M1 + 197);
  _t54_5 = _mm256_broadcast_sd(M1 + 198);
  _t54_4 = _mm256_broadcast_sd(M1 + 199);
  _t54_3 = _mm256_loadu_pd(R + 36);
  _t54_2 = _mm256_maskload_pd(R + 44, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
  _t54_1 = _mm256_maskload_pd(R + 52, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
  _t54_0 = _mm256_maskload_pd(R + 60, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t54_36 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 32);
  _t54_37 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 32);
  _t54_38 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_7, _t52_6), _mm256_unpacklo_pd(_t52_5, _t52_4), 49);
  _t54_39 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_7, _t52_6), _mm256_unpackhi_pd(_t52_5, _t52_4), 49);

  // 4-BLAC: 4x4 * 4x4
  _t54_28 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_19, _t54_36), _mm256_mul_pd(_t54_18, _t54_37)), _mm256_add_pd(_mm256_mul_pd(_t54_17, _t54_38), _mm256_mul_pd(_t54_16, _t54_39)));
  _t54_29 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_15, _t54_36), _mm256_mul_pd(_t54_14, _t54_37)), _mm256_add_pd(_mm256_mul_pd(_t54_13, _t54_38), _mm256_mul_pd(_t54_12, _t54_39)));
  _t54_30 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_11, _t54_36), _mm256_mul_pd(_t54_10, _t54_37)), _mm256_add_pd(_mm256_mul_pd(_t54_9, _t54_38), _mm256_mul_pd(_t54_8, _t54_39)));
  _t54_31 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t54_7, _t54_36), _mm256_mul_pd(_t54_6, _t54_37)), _mm256_add_pd(_mm256_mul_pd(_t54_5, _t54_38), _mm256_mul_pd(_t54_4, _t54_39)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t54_32 = _t54_3;
  _t54_33 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 3), _t54_2, 12);
  _t54_34 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 0), _t54_1, 49);
  _t54_35 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_3, _t54_2, 12), _mm256_shuffle_pd(_t54_1, _t54_0, 12), 49);

  // 4-BLAC: 4x4 + 4x4
  _t54_20 = _mm256_add_pd(_t54_28, _t54_32);
  _t54_21 = _mm256_add_pd(_t54_29, _t54_33);
  _t54_22 = _mm256_add_pd(_t54_30, _t54_34);
  _t54_23 = _mm256_add_pd(_t54_31, _t54_35);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t54_24 = _t54_20;
  _t54_25 = _t54_21;
  _t54_26 = _t54_22;
  _t54_27 = _t54_23;


  for( int k2 = 4; k2 <= 27; k2+=4 ) {
    _t55_19 = _mm256_broadcast_sd(M1 + k2 + 112);
    _t55_18 = _mm256_broadcast_sd(M1 + k2 + 113);
    _t55_17 = _mm256_broadcast_sd(M1 + k2 + 114);
    _t55_16 = _mm256_broadcast_sd(M1 + k2 + 115);
    _t55_15 = _mm256_broadcast_sd(M1 + k2 + 140);
    _t55_14 = _mm256_broadcast_sd(M1 + k2 + 141);
    _t55_13 = _mm256_broadcast_sd(M1 + k2 + 142);
    _t55_12 = _mm256_broadcast_sd(M1 + k2 + 143);
    _t55_11 = _mm256_broadcast_sd(M1 + k2 + 168);
    _t55_10 = _mm256_broadcast_sd(M1 + k2 + 169);
    _t55_9 = _mm256_broadcast_sd(M1 + k2 + 170);
    _t55_8 = _mm256_broadcast_sd(M1 + k2 + 171);
    _t55_7 = _mm256_broadcast_sd(M1 + k2 + 196);
    _t55_6 = _mm256_broadcast_sd(M1 + k2 + 197);
    _t55_5 = _mm256_broadcast_sd(M1 + k2 + 198);
    _t55_4 = _mm256_broadcast_sd(M1 + k2 + 199);
    _t55_3 = _mm256_loadu_pd(H + k2 + 112);
    _t55_2 = _mm256_loadu_pd(H + k2 + 140);
    _t55_1 = _mm256_loadu_pd(H + k2 + 168);
    _t55_0 = _mm256_loadu_pd(H + k2 + 196);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: (4x4)^T
    _t55_28 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 32);
    _t55_29 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_3, _t55_2), _mm256_unpackhi_pd(_t55_1, _t55_0), 32);
    _t55_30 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t55_3, _t55_2), _mm256_unpacklo_pd(_t55_1, _t55_0), 49);
    _t55_31 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t55_3, _t55_2), _mm256_unpackhi_pd(_t55_1, _t55_0), 49);

    // 4-BLAC: 4x4 * 4x4
    _t55_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_19, _t55_28), _mm256_mul_pd(_t55_18, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_17, _t55_30), _mm256_mul_pd(_t55_16, _t55_31)));
    _t55_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_15, _t55_28), _mm256_mul_pd(_t55_14, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_13, _t55_30), _mm256_mul_pd(_t55_12, _t55_31)));
    _t55_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_11, _t55_28), _mm256_mul_pd(_t55_10, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_9, _t55_30), _mm256_mul_pd(_t55_8, _t55_31)));
    _t55_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t55_7, _t55_28), _mm256_mul_pd(_t55_6, _t55_29)), _mm256_add_pd(_mm256_mul_pd(_t55_5, _t55_30), _mm256_mul_pd(_t55_4, _t55_31)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t55_24 = _t54_24;
    _t55_25 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_24, _t54_25, 3), _t54_25, 12);
    _t55_26 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_24, _t54_25, 0), _t54_26, 49);
    _t55_27 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_24, _t54_25, 12), _mm256_shuffle_pd(_t54_26, _t54_27, 12), 49);

    // 4-BLAC: 4x4 + 4x4
    _t55_24 = _mm256_add_pd(_t55_24, _t55_20);
    _t55_25 = _mm256_add_pd(_t55_25, _t55_21);
    _t55_26 = _mm256_add_pd(_t55_26, _t55_22);
    _t55_27 = _mm256_add_pd(_t55_27, _t55_23);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t54_24 = _t55_24;
    _t54_25 = _t55_25;
    _t54_26 = _t55_26;
    _t54_27 = _t55_27;
  }

  _t56_65 = _mm256_maskload_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_67 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_68 = _mm256_maskload_pd(M3 + 9, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_69 = _mm256_maskload_pd(M3 + 17, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_70 = _mm256_maskload_pd(M3 + 25, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t56_71 = _mm256_maskload_pd(M3 + 9, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_73 = _mm256_maskload_pd(M3 + 10, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_74 = _mm256_maskload_pd(M3 + 18, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_75 = _mm256_maskload_pd(M3 + 26, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t56_76 = _mm256_maskload_pd(M3 + 18, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_77 = _mm256_maskload_pd(M3 + 19, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_78 = _mm256_maskload_pd(M3 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_61 = _mm256_broadcast_sd(&(M3[19]));
  _t56_81 = _mm256_maskload_pd(M3 + 36, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_83 = _mm256_maskload_pd(M3 + 37, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_84 = _mm256_maskload_pd(M3 + 45, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_85 = _mm256_maskload_pd(M3 + 53, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_86 = _mm256_maskload_pd(M3 + 61, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, 0));
  _t56_87 = _mm256_maskload_pd(M3 + 45, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_89 = _mm256_maskload_pd(M3 + 46, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_90 = _mm256_maskload_pd(M3 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_91 = _mm256_maskload_pd(M3 + 62, _mm256_setr_epi64x(0, (__int64)1 << 63, 0, 0));
  _t56_92 = _mm256_maskload_pd(M3 + 54, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_93 = _mm256_maskload_pd(M3 + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_94 = _mm256_maskload_pd(M3 + 63, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_95 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_96 = _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_57 = _mm256_broadcast_sd(&(v0[0]));
  _t56_97 = _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_98 = _mm256_maskload_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_56 = _mm256_broadcast_sd(&(v0[1]));
  _t56_99 = _mm256_maskload_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_100 = _mm256_maskload_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_127 = _mm256_loadu_pd(v0 + 4);
  _t56_101 = _mm256_maskload_pd(v0 + 4, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_102 = _mm256_maskload_pd(v0 + 5, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_55 = _mm256_broadcast_sd(&(v0[4]));
  _t56_103 = _mm256_maskload_pd(v0 + 5, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_104 = _mm256_maskload_pd(v0 + 6, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_54 = _mm256_broadcast_sd(&(v0[5]));
  _t56_105 = _mm256_maskload_pd(v0 + 6, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_106 = _mm256_maskload_pd(v0 + 7, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_53 = _mm256_broadcast_sd(&(v0[7]));
  _t56_108 = _mm256_maskload_pd(v0 + 4, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_52 = _mm256_broadcast_sd(&(v0[6]));
  _t56_51 = _mm256_maskload_pd(M3 + 37, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_109 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));
  _t56_50 = _mm256_broadcast_sd(&(v0[3]));
  _t56_110 = _mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0));
  _t56_49 = _mm256_broadcast_sd(&(v0[2]));
  _t56_48 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_115 = _mm256_loadu_pd(M1);
  _t56_111 = _mm256_loadu_pd(M1 + 28);
  _t56_112 = _mm256_loadu_pd(M1 + 56);
  _t56_113 = _mm256_loadu_pd(M1 + 84);

  // Generating : M3[8,8] = S(h(1, 8, 0), Sqrt( G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ),h(1, 8, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_129 = _t56_65;

  // 4-BLAC: sqrt(1x4)
  _t56_130 = _mm256_sqrt_pd(_t56_129);

  // AVX Storer:
  _t56_65 = _t56_130;

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ),h(1, 8, 0))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_131 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_132 = _t56_65;

  // 4-BLAC: 1x4 / 1x4
  _t56_133 = _mm256_div_pd(_t56_131, _t56_132);

  // AVX Storer:
  _t56_66 = _t56_133;

  // Generating : M3[8,8] = S(h(1, 8, 0), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) Kro G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ),h(3, 8, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_134 = _t56_64;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_135 = _t56_67;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_136 = _mm256_mul_pd(_t56_134, _t56_135);

  // AVX Storer:
  _t56_67 = _t56_136;

  // Generating : M3[8,8] = S(h(3, 8, 1), ( G(h(3, 8, 1), M3[8,8],h(3, 8, 1)) - ( T( G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) * G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) ),h(3, 8, 1))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t56_137 = _t56_68;
  _t56_138 = _mm256_blend_pd(_mm256_shuffle_pd(_t56_68, _t56_69, 3), _t56_69, 12);
  _t56_139 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t56_68, _t56_69, 0), _t56_70, 49);
  _t56_140 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_141 = _t56_67;

  // 4-BLAC: (1x4)^T
  _t56_142 = _t56_141;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_143 = _t56_67;

  // 4-BLAC: 4x1 * 1x4
  _t56_144 = _mm256_mul_pd(_t56_47, _t56_143);
  _t56_145 = _mm256_mul_pd(_t56_46, _t56_143);
  _t56_146 = _mm256_mul_pd(_t56_45, _t56_143);
  _t56_147 = _mm256_mul_pd(_t56_44, _t56_143);

  // 4-BLAC: 4x4 - 4x4
  _t56_148 = _mm256_sub_pd(_t56_137, _t56_144);
  _t56_149 = _mm256_sub_pd(_t56_138, _t56_145);
  _t56_150 = _mm256_sub_pd(_t56_139, _t56_146);
  _t56_151 = _mm256_sub_pd(_t56_140, _t56_147);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t56_68 = _t56_148;
  _t56_69 = _t56_149;
  _t56_70 = _t56_150;

  // Generating : M3[8,8] = S(h(1, 8, 1), Sqrt( G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ),h(1, 8, 1))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_152 = _t56_71;

  // 4-BLAC: sqrt(1x4)
  _t56_153 = _mm256_sqrt_pd(_t56_152);

  // AVX Storer:
  _t56_71 = _t56_153;

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ),h(1, 8, 1))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_154 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_155 = _t56_71;

  // 4-BLAC: 1x4 / 1x4
  _t56_156 = _mm256_div_pd(_t56_154, _t56_155);

  // AVX Storer:
  _t56_72 = _t56_156;

  // Generating : M3[8,8] = S(h(1, 8, 1), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ),h(2, 8, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_157 = _t56_63;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_158 = _t56_73;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_159 = _mm256_mul_pd(_t56_157, _t56_158);

  // AVX Storer:
  _t56_73 = _t56_159;

  // Generating : M3[8,8] = S(h(2, 8, 2), ( G(h(2, 8, 2), M3[8,8],h(2, 8, 2)) - ( T( G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) * G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) ),h(2, 8, 2))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t56_160 = _t56_74;
  _t56_161 = _mm256_shuffle_pd(_t56_74, _t56_75, 3);
  _t56_162 = _mm256_setzero_pd();
  _t56_163 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_164 = _t56_73;

  // 4-BLAC: (1x4)^T
  _t56_165 = _t56_164;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_166 = _t56_73;

  // 4-BLAC: 4x1 * 1x4
  _t56_167 = _mm256_mul_pd(_t56_43, _t56_166);
  _t56_168 = _mm256_mul_pd(_t56_42, _t56_166);
  _t56_169 = _mm256_mul_pd(_t56_41, _t56_166);
  _t56_170 = _mm256_mul_pd(_t56_40, _t56_166);

  // 4-BLAC: 4x4 - 4x4
  _t56_171 = _mm256_sub_pd(_t56_160, _t56_167);
  _t56_172 = _mm256_sub_pd(_t56_161, _t56_168);
  _t56_173 = _mm256_sub_pd(_t56_162, _t56_169);
  _t56_174 = _mm256_sub_pd(_t56_163, _t56_170);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t56_74 = _t56_171;
  _t56_75 = _t56_172;

  // Generating : M3[8,8] = S(h(1, 8, 2), Sqrt( G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ),h(1, 8, 2))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_175 = _t56_76;

  // 4-BLAC: sqrt(1x4)
  _t56_176 = _mm256_sqrt_pd(_t56_175);

  // AVX Storer:
  _t56_76 = _t56_176;

  // Generating : M3[8,8] = S(h(1, 8, 2), ( G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) Div G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ),h(1, 8, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_177 = _t56_77;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_178 = _t56_76;

  // 4-BLAC: 1x4 / 1x4
  _t56_179 = _mm256_div_pd(_t56_177, _t56_178);

  // AVX Storer:
  _t56_77 = _t56_179;

  // Generating : M3[8,8] = S(h(1, 8, 3), ( G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) - ( T( G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) Kro G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) ),h(1, 8, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_180 = _t56_78;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_181 = _t56_77;

  // 4-BLAC: (4x1)^T
  _t56_182 = _t56_181;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_183 = _t56_77;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_184 = _mm256_mul_pd(_t56_182, _t56_183);

  // 4-BLAC: 1x4 - 1x4
  _t56_185 = _mm256_sub_pd(_t56_180, _t56_184);

  // AVX Storer:
  _t56_78 = _t56_185;

  // Generating : M3[8,8] = S(h(1, 8, 3), Sqrt( G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ),h(1, 8, 3))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_186 = _t56_78;

  // 4-BLAC: sqrt(1x4)
  _t56_187 = _mm256_sqrt_pd(_t56_186);

  // AVX Storer:
  _t56_78 = _t56_187;

  // Generating : M3[8,8] = S(h(1, 8, 0), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) Kro G(h(1, 8, 0), M3[8,8],h(4, 8, 4)) ),h(4, 8, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_188 = _t56_64;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_8 = _mm256_mul_pd(_t56_188, _t52_8);

  // AVX Storer:

  // Generating : M3[8,8] = S(h(3, 8, 1), ( G(h(3, 8, 1), M3[8,8],h(4, 8, 4)) - ( T( G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) * G(h(1, 8, 0), M3[8,8],h(4, 8, 4)) ) ),h(4, 8, 4))

  // AVX Loader:

  // 3x4 -> 4x4
  _t56_189 = _t52_9;
  _t56_190 = _t52_10;
  _t56_191 = _t52_11;
  _t56_192 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_193 = _t56_67;

  // 4-BLAC: (1x4)^T
  _t56_194 = _t56_193;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_195 = _mm256_mul_pd(_t56_39, _t52_8);
  _t56_196 = _mm256_mul_pd(_t56_38, _t52_8);
  _t56_197 = _mm256_mul_pd(_t56_37, _t52_8);
  _t56_198 = _mm256_mul_pd(_t56_36, _t52_8);

  // 4-BLAC: 4x4 - 4x4
  _t56_199 = _mm256_sub_pd(_t56_189, _t56_195);
  _t56_200 = _mm256_sub_pd(_t56_190, _t56_196);
  _t56_201 = _mm256_sub_pd(_t56_191, _t56_197);
  _t56_202 = _mm256_sub_pd(_t56_192, _t56_198);

  // AVX Storer:
  _t52_9 = _t56_199;
  _t52_10 = _t56_200;
  _t52_11 = _t56_201;

  // Generating : M3[8,8] = S(h(1, 8, 1), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M3[8,8],h(4, 8, 4)) ),h(4, 8, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_203 = _t56_63;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_9 = _mm256_mul_pd(_t56_203, _t52_9);

  // AVX Storer:

  // Generating : M3[8,8] = S(h(2, 8, 2), ( G(h(2, 8, 2), M3[8,8],h(4, 8, 4)) - ( T( G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) * G(h(1, 8, 1), M3[8,8],h(4, 8, 4)) ) ),h(4, 8, 4))

  // AVX Loader:

  // 2x4 -> 4x4
  _t56_204 = _t52_10;
  _t56_205 = _t52_11;
  _t56_206 = _mm256_setzero_pd();
  _t56_207 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_208 = _t56_73;

  // 4-BLAC: (1x4)^T
  _t56_209 = _t56_208;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_210 = _mm256_mul_pd(_t56_35, _t52_9);
  _t56_211 = _mm256_mul_pd(_t56_34, _t52_9);
  _t56_212 = _mm256_mul_pd(_t56_33, _t52_9);
  _t56_213 = _mm256_mul_pd(_t56_32, _t52_9);

  // 4-BLAC: 4x4 - 4x4
  _t56_214 = _mm256_sub_pd(_t56_204, _t56_210);
  _t56_215 = _mm256_sub_pd(_t56_205, _t56_211);
  _t56_216 = _mm256_sub_pd(_t56_206, _t56_212);
  _t56_217 = _mm256_sub_pd(_t56_207, _t56_213);

  // AVX Storer:
  _t52_10 = _t56_214;
  _t52_11 = _t56_215;

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ),h(1, 8, 2))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_218 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_219 = _t56_76;

  // 4-BLAC: 1x4 / 1x4
  _t56_220 = _mm256_div_pd(_t56_218, _t56_219);

  // AVX Storer:
  _t56_79 = _t56_220;

  // Generating : M3[8,8] = S(h(1, 8, 2), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) Kro G(h(1, 8, 2), M3[8,8],h(4, 8, 4)) ),h(4, 8, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_221 = _t56_62;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_10 = _mm256_mul_pd(_t56_221, _t52_10);

  // AVX Storer:

  // Generating : M3[8,8] = S(h(1, 8, 3), ( G(h(1, 8, 3), M3[8,8],h(4, 8, 4)) - ( T( G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) Kro G(h(1, 8, 2), M3[8,8],h(4, 8, 4)) ) ),h(4, 8, 4))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_222 = _t56_61;

  // 4-BLAC: (4x1)^T
  _t56_223 = _t56_222;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_114 = _mm256_mul_pd(_t56_223, _t52_10);

  // 4-BLAC: 1x4 - 1x4
  _t52_11 = _mm256_sub_pd(_t52_11, _t56_114);

  // AVX Storer:

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ),h(1, 8, 3))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_224 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_225 = _t56_78;

  // 4-BLAC: 1x4 / 1x4
  _t56_226 = _mm256_div_pd(_t56_224, _t56_225);

  // AVX Storer:
  _t56_80 = _t56_226;

  // Generating : M3[8,8] = S(h(1, 8, 3), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) Kro G(h(1, 8, 3), M3[8,8],h(4, 8, 4)) ),h(4, 8, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_227 = _t56_60;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t52_11 = _mm256_mul_pd(_t56_227, _t52_11);

  // AVX Storer:

  // Generating : M3[8,8] = S(h(4, 8, 4), ( G(h(4, 8, 4), M3[8,8],h(4, 8, 4)) - ( T( G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) * G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) ),h(4, 8, 4))

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t56_228 = _t54_24;
  _t56_229 = _mm256_blend_pd(_mm256_shuffle_pd(_t54_24, _t54_25, 3), _t54_25, 12);
  _t56_230 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_24, _t54_25, 0), _t54_26, 49);
  _t56_231 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t54_24, _t54_25, 12), _mm256_shuffle_pd(_t54_26, _t54_27, 12), 49);

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t56_439 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_8, _t52_9), _mm256_unpacklo_pd(_t52_10, _t52_11), 32);
  _t56_440 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_8, _t52_9), _mm256_unpackhi_pd(_t52_10, _t52_11), 32);
  _t56_441 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_8, _t52_9), _mm256_unpacklo_pd(_t52_10, _t52_11), 49);
  _t56_442 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_8, _t52_9), _mm256_unpackhi_pd(_t52_10, _t52_11), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t56_117 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t56_15, _t52_8), _mm256_mul_pd(_t56_14, _t52_9)), _mm256_add_pd(_mm256_mul_pd(_t56_13, _t52_10), _mm256_mul_pd(_t56_12, _t52_11)));
  _t56_118 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t56_11, _t52_8), _mm256_mul_pd(_t56_10, _t52_9)), _mm256_add_pd(_mm256_mul_pd(_t56_9, _t52_10), _mm256_mul_pd(_t56_8, _t52_11)));
  _t56_119 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t56_7, _t52_8), _mm256_mul_pd(_t56_6, _t52_9)), _mm256_add_pd(_mm256_mul_pd(_t56_5, _t52_10), _mm256_mul_pd(_t56_4, _t52_11)));
  _t56_120 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t56_3, _t52_8), _mm256_mul_pd(_t56_2, _t52_9)), _mm256_add_pd(_mm256_mul_pd(_t56_1, _t52_10), _mm256_mul_pd(_t56_0, _t52_11)));

  // 4-BLAC: 4x4 - 4x4
  _t56_123 = _mm256_sub_pd(_t56_228, _t56_117);
  _t56_124 = _mm256_sub_pd(_t56_229, _t56_118);
  _t56_125 = _mm256_sub_pd(_t56_230, _t56_119);
  _t56_126 = _mm256_sub_pd(_t56_231, _t56_120);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t54_24 = _t56_123;
  _t54_25 = _t56_124;
  _t54_26 = _t56_125;
  _t54_27 = _t56_126;

  // Generating : M3[8,8] = S(h(1, 8, 4), Sqrt( G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ),h(1, 8, 4))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_232 = _t56_81;

  // 4-BLAC: sqrt(1x4)
  _t56_233 = _mm256_sqrt_pd(_t56_232);

  // AVX Storer:
  _t56_81 = _t56_233;

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ),h(1, 8, 4))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_234 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_235 = _t56_81;

  // 4-BLAC: 1x4 / 1x4
  _t56_236 = _mm256_div_pd(_t56_234, _t56_235);

  // AVX Storer:
  _t56_82 = _t56_236;

  // Generating : M3[8,8] = S(h(1, 8, 4), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) Kro G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ),h(3, 8, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_237 = _t56_59;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_238 = _t56_83;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_239 = _mm256_mul_pd(_t56_237, _t56_238);

  // AVX Storer:
  _t56_83 = _t56_239;

  // Generating : M3[8,8] = S(h(3, 8, 5), ( G(h(3, 8, 5), M3[8,8],h(3, 8, 5)) - ( T( G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) * G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) ),h(3, 8, 5))

  // AVX Loader:

  // 3x3 -> 4x4 - UpSymm
  _t56_240 = _t56_84;
  _t56_241 = _mm256_blend_pd(_mm256_shuffle_pd(_t56_84, _t56_85, 3), _t56_85, 12);
  _t56_242 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t56_84, _t56_85, 0), _t56_86, 49);
  _t56_243 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_244 = _t56_83;

  // 4-BLAC: (1x4)^T
  _t56_245 = _t56_244;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_246 = _t56_83;

  // 4-BLAC: 4x1 * 1x4
  _t56_247 = _mm256_mul_pd(_t56_31, _t56_246);
  _t56_248 = _mm256_mul_pd(_t56_30, _t56_246);
  _t56_249 = _mm256_mul_pd(_t56_29, _t56_246);
  _t56_250 = _mm256_mul_pd(_t56_28, _t56_246);

  // 4-BLAC: 4x4 - 4x4
  _t56_251 = _mm256_sub_pd(_t56_240, _t56_247);
  _t56_252 = _mm256_sub_pd(_t56_241, _t56_248);
  _t56_253 = _mm256_sub_pd(_t56_242, _t56_249);
  _t56_254 = _mm256_sub_pd(_t56_243, _t56_250);

  // AVX Storer:

  // 4x4 -> 3x3 - UpSymm
  _t56_84 = _t56_251;
  _t56_85 = _t56_252;
  _t56_86 = _t56_253;

  // Generating : M3[8,8] = S(h(1, 8, 5), Sqrt( G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ),h(1, 8, 5))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_255 = _t56_87;

  // 4-BLAC: sqrt(1x4)
  _t56_256 = _mm256_sqrt_pd(_t56_255);

  // AVX Storer:
  _t56_87 = _t56_256;

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ),h(1, 8, 5))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t56_257 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_258 = _t56_87;

  // 4-BLAC: 1x4 / 1x4
  _t56_259 = _mm256_div_pd(_t56_257, _t56_258);

  // AVX Storer:
  _t56_88 = _t56_259;

  // Generating : M3[8,8] = S(h(1, 8, 5), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ),h(2, 8, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_260 = _t56_58;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_261 = _t56_89;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_262 = _mm256_mul_pd(_t56_260, _t56_261);

  // AVX Storer:
  _t56_89 = _t56_262;

  // Generating : M3[8,8] = S(h(2, 8, 6), ( G(h(2, 8, 6), M3[8,8],h(2, 8, 6)) - ( T( G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) * G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) ),h(2, 8, 6))

  // AVX Loader:

  // 2x2 -> 4x4 - UpSymm
  _t56_263 = _t56_90;
  _t56_264 = _mm256_shuffle_pd(_t56_90, _t56_91, 3);
  _t56_265 = _mm256_setzero_pd();
  _t56_266 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_267 = _t56_89;

  // 4-BLAC: (1x4)^T
  _t56_268 = _t56_267;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_269 = _t56_89;

  // 4-BLAC: 4x1 * 1x4
  _t56_270 = _mm256_mul_pd(_t56_27, _t56_269);
  _t56_271 = _mm256_mul_pd(_t56_26, _t56_269);
  _t56_272 = _mm256_mul_pd(_t56_25, _t56_269);
  _t56_273 = _mm256_mul_pd(_t56_24, _t56_269);

  // 4-BLAC: 4x4 - 4x4
  _t56_274 = _mm256_sub_pd(_t56_263, _t56_270);
  _t56_275 = _mm256_sub_pd(_t56_264, _t56_271);
  _t56_276 = _mm256_sub_pd(_t56_265, _t56_272);
  _t56_277 = _mm256_sub_pd(_t56_266, _t56_273);

  // AVX Storer:

  // 4x4 -> 2x2 - UpSymm
  _t56_90 = _t56_274;
  _t56_91 = _t56_275;

  // Generating : M3[8,8] = S(h(1, 8, 6), Sqrt( G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ),h(1, 8, 6))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_278 = _t56_92;

  // 4-BLAC: sqrt(1x4)
  _t56_279 = _mm256_sqrt_pd(_t56_278);

  // AVX Storer:
  _t56_92 = _t56_279;

  // Generating : M3[8,8] = S(h(1, 8, 6), ( G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) Div G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ),h(1, 8, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_280 = _t56_93;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_281 = _t56_92;

  // 4-BLAC: 1x4 / 1x4
  _t56_282 = _mm256_div_pd(_t56_280, _t56_281);

  // AVX Storer:
  _t56_93 = _t56_282;

  // Generating : M3[8,8] = S(h(1, 8, 7), ( G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) - ( T( G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) Kro G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) ),h(1, 8, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_283 = _t56_94;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_284 = _t56_93;

  // 4-BLAC: (4x1)^T
  _t56_285 = _t56_284;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_286 = _t56_93;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_287 = _mm256_mul_pd(_t56_285, _t56_286);

  // 4-BLAC: 1x4 - 1x4
  _t56_288 = _mm256_sub_pd(_t56_283, _t56_287);

  // AVX Storer:
  _t56_94 = _t56_288;

  // Generating : M3[8,8] = S(h(1, 8, 7), Sqrt( G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ),h(1, 8, 7))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_289 = _t56_94;

  // 4-BLAC: sqrt(1x4)
  _t56_290 = _mm256_sqrt_pd(_t56_289);

  // AVX Storer:
  _t56_94 = _t56_290;

  // Generating : v0[8,1] = S(h(1, 8, 0), ( G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_291 = _t56_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_292 = _t56_65;

  // 4-BLAC: 1x4 / 1x4
  _t56_293 = _mm256_div_pd(_t56_291, _t56_292);

  // AVX Storer:
  _t56_95 = _t56_293;

  // Generating : v0[8,1] = S(h(3, 8, 1), ( G(h(3, 8, 1), v0[8,1],h(1, 1, 0)) - ( T( G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) Kro G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_294 = _t56_96;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_295 = _t56_67;

  // 4-BLAC: (1x4)^T
  _t56_296 = _t56_295;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_297 = _t56_57;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_298 = _mm256_mul_pd(_t56_296, _t56_297);

  // 4-BLAC: 4x1 - 4x1
  _t56_299 = _mm256_sub_pd(_t56_294, _t56_298);

  // AVX Storer:
  _t56_96 = _t56_299;

  // Generating : v0[8,1] = S(h(1, 8, 1), ( G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_300 = _t56_97;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_301 = _t56_71;

  // 4-BLAC: 1x4 / 1x4
  _t56_302 = _mm256_div_pd(_t56_300, _t56_301);

  // AVX Storer:
  _t56_97 = _t56_302;

  // Generating : v0[8,1] = S(h(2, 8, 2), ( G(h(2, 8, 2), v0[8,1],h(1, 1, 0)) - ( T( G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) Kro G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_303 = _t56_98;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_304 = _t56_73;

  // 4-BLAC: (1x4)^T
  _t56_305 = _t56_304;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_306 = _t56_56;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_307 = _mm256_mul_pd(_t56_305, _t56_306);

  // 4-BLAC: 4x1 - 4x1
  _t56_308 = _mm256_sub_pd(_t56_303, _t56_307);

  // AVX Storer:
  _t56_98 = _t56_308;

  // Generating : v0[8,1] = S(h(1, 8, 2), ( G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_309 = _t56_99;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_310 = _t56_76;

  // 4-BLAC: 1x4 / 1x4
  _t56_311 = _mm256_div_pd(_t56_309, _t56_310);

  // AVX Storer:
  _t56_99 = _t56_311;

  // Generating : v0[8,1] = S(h(1, 8, 3), ( G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) - ( T( G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) Kro G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_312 = _t56_100;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_313 = _t56_77;

  // 4-BLAC: (4x1)^T
  _t56_314 = _t56_313;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_315 = _t56_99;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_316 = _mm256_mul_pd(_t56_314, _t56_315);

  // 4-BLAC: 1x4 - 1x4
  _t56_317 = _mm256_sub_pd(_t56_312, _t56_316);

  // AVX Storer:
  _t56_100 = _t56_317;

  // Generating : v0[8,1] = S(h(1, 8, 3), ( G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_318 = _t56_100;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_319 = _t56_78;

  // 4-BLAC: 1x4 / 1x4
  _t56_320 = _mm256_div_pd(_t56_318, _t56_319);

  // AVX Storer:
  _t56_100 = _t56_320;

  // Generating : v0[8,1] = S(h(4, 8, 4), ( G(h(4, 8, 4), v0[8,1],h(1, 1, 0)) - ( T( G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) * G(h(4, 8, 0), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t56_443 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_8, _t52_9), _mm256_unpacklo_pd(_t52_10, _t52_11), 32);
  _t56_444 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_8, _t52_9), _mm256_unpackhi_pd(_t52_10, _t52_11), 32);
  _t56_445 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_8, _t52_9), _mm256_unpacklo_pd(_t52_10, _t52_11), 49);
  _t56_446 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_8, _t52_9), _mm256_unpackhi_pd(_t52_10, _t52_11), 49);

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x1
  _t56_121 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t56_443, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32)), _mm256_mul_pd(_t56_444, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t56_445, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32)), _mm256_mul_pd(_t56_446, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t56_443, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32)), _mm256_mul_pd(_t56_444, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t56_445, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32)), _mm256_mul_pd(_t56_446, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32))), 12));

  // 4-BLAC: 4x1 - 4x1
  _t56_127 = _mm256_sub_pd(_t56_127, _t56_121);

  // AVX Storer:

  // Generating : v0[8,1] = S(h(1, 8, 4), ( G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_321 = _t56_101;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_322 = _t56_81;

  // 4-BLAC: 1x4 / 1x4
  _t56_323 = _mm256_div_pd(_t56_321, _t56_322);

  // AVX Storer:
  _t56_101 = _t56_323;

  // Generating : v0[8,1] = S(h(3, 8, 5), ( G(h(3, 8, 5), v0[8,1],h(1, 1, 0)) - ( T( G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) Kro G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_324 = _t56_102;

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_325 = _t56_83;

  // 4-BLAC: (1x4)^T
  _t56_326 = _t56_325;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_327 = _t56_55;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_328 = _mm256_mul_pd(_t56_326, _t56_327);

  // 4-BLAC: 4x1 - 4x1
  _t56_329 = _mm256_sub_pd(_t56_324, _t56_328);

  // AVX Storer:
  _t56_102 = _t56_329;

  // Generating : v0[8,1] = S(h(1, 8, 5), ( G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_330 = _t56_103;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_331 = _t56_87;

  // 4-BLAC: 1x4 / 1x4
  _t56_332 = _mm256_div_pd(_t56_330, _t56_331);

  // AVX Storer:
  _t56_103 = _t56_332;

  // Generating : v0[8,1] = S(h(2, 8, 6), ( G(h(2, 8, 6), v0[8,1],h(1, 1, 0)) - ( T( G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) Kro G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_333 = _t56_104;

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_334 = _t56_89;

  // 4-BLAC: (1x4)^T
  _t56_335 = _t56_334;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_336 = _t56_54;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_337 = _mm256_mul_pd(_t56_335, _t56_336);

  // 4-BLAC: 4x1 - 4x1
  _t56_338 = _mm256_sub_pd(_t56_333, _t56_337);

  // AVX Storer:
  _t56_104 = _t56_338;

  // Generating : v0[8,1] = S(h(1, 8, 6), ( G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_339 = _t56_105;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_340 = _t56_92;

  // 4-BLAC: 1x4 / 1x4
  _t56_341 = _mm256_div_pd(_t56_339, _t56_340);

  // AVX Storer:
  _t56_105 = _t56_341;

  // Generating : v0[8,1] = S(h(1, 8, 7), ( G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) - ( T( G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) Kro G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_342 = _t56_106;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_343 = _t56_93;

  // 4-BLAC: (4x1)^T
  _t56_344 = _t56_343;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_345 = _t56_105;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_346 = _mm256_mul_pd(_t56_344, _t56_345);

  // 4-BLAC: 1x4 - 1x4
  _t56_347 = _mm256_sub_pd(_t56_342, _t56_346);

  // AVX Storer:
  _t56_106 = _t56_347;

  // Generating : v0[8,1] = S(h(1, 8, 7), ( G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_348 = _t56_106;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_349 = _t56_94;

  // 4-BLAC: 1x4 / 1x4
  _t56_350 = _mm256_div_pd(_t56_348, _t56_349);

  // AVX Storer:
  _t56_106 = _t56_350;

  // Generating : v0[8,1] = S(h(1, 8, 7), ( G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_351 = _t56_106;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_352 = _t56_94;

  // 4-BLAC: 1x4 / 1x4
  _t56_353 = _mm256_div_pd(_t56_351, _t56_352);

  // AVX Storer:
  _t56_106 = _t56_353;

  // Generating : v0[8,1] = S(h(3, 8, 4), ( G(h(3, 8, 4), v0[8,1],h(1, 1, 0)) - ( G(h(3, 8, 4), M3[8,8],h(1, 8, 7)) Kro G(h(1, 8, 7), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_354 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _mm256_setzero_pd()), 32);

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_355 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_83, _t56_93, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_89, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_356 = _t56_53;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_357 = _mm256_mul_pd(_t56_355, _t56_356);

  // 4-BLAC: 4x1 - 4x1
  _t56_358 = _mm256_sub_pd(_t56_354, _t56_357);

  // AVX Storer:
  _t56_107 = _t56_358;

  // Generating : v0[8,1] = S(h(1, 8, 6), ( G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_359 = _t56_105;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_360 = _t56_92;

  // 4-BLAC: 1x4 / 1x4
  _t56_361 = _mm256_div_pd(_t56_359, _t56_360);

  // AVX Storer:
  _t56_105 = _t56_361;

  // Generating : v0[8,1] = S(h(2, 8, 4), ( G(h(2, 8, 4), v0[8,1],h(1, 1, 0)) - ( G(h(2, 8, 4), M3[8,8],h(1, 8, 6)) Kro G(h(1, 8, 6), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_362 = _t56_108;

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_363 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_83, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_89, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_364 = _t56_52;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_365 = _mm256_mul_pd(_t56_363, _t56_364);

  // 4-BLAC: 4x1 - 4x1
  _t56_366 = _mm256_sub_pd(_t56_362, _t56_365);

  // AVX Storer:
  _t56_108 = _t56_366;

  // Generating : v0[8,1] = S(h(1, 8, 5), ( G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 5), M3[8,8],h(1, 8, 5)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_367 = _t56_103;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_368 = _t56_87;

  // 4-BLAC: 1x4 / 1x4
  _t56_369 = _mm256_div_pd(_t56_367, _t56_368);

  // AVX Storer:
  _t56_103 = _t56_369;

  // Generating : v0[8,1] = S(h(1, 8, 4), ( G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) - ( G(h(1, 8, 4), M3[8,8],h(1, 8, 5)) Kro G(h(1, 8, 5), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_370 = _t56_101;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_371 = _t56_51;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_372 = _t56_103;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_373 = _mm256_mul_pd(_t56_371, _t56_372);

  // 4-BLAC: 1x4 - 1x4
  _t56_374 = _mm256_sub_pd(_t56_370, _t56_373);

  // AVX Storer:
  _t56_101 = _t56_374;

  // Generating : v0[8,1] = S(h(1, 8, 4), ( G(h(1, 8, 4), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 4), M3[8,8],h(1, 8, 4)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_375 = _t56_101;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_376 = _t56_81;

  // 4-BLAC: 1x4 / 1x4
  _t56_377 = _mm256_div_pd(_t56_375, _t56_376);

  // AVX Storer:
  _t56_101 = _t56_377;

  // Generating : v0[8,1] = S(h(4, 8, 0), ( G(h(4, 8, 0), v0[8,1],h(1, 1, 0)) - ( G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) * G(h(4, 8, 4), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x1
  _t56_122 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t52_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32)), _mm256_mul_pd(_t52_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t52_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32)), _mm256_mul_pd(_t52_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t52_8, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32)), _mm256_mul_pd(_t52_9, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32))), _mm256_hadd_pd(_mm256_mul_pd(_t52_10, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32)), _mm256_mul_pd(_t52_11, _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_101, _t56_103), _mm256_unpacklo_pd(_t56_105, _t56_106), 32))), 12));

  // 4-BLAC: 4x1 - 4x1
  _t56_128 = _mm256_sub_pd(_mm256_permute2f128_pd(_mm256_unpacklo_pd(_t56_95, _t56_97), _mm256_unpacklo_pd(_t56_99, _t56_100), 32), _t56_122);

  // AVX Storer:

  // Generating : v0[8,1] = S(h(1, 8, 3), ( G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 3), M3[8,8],h(1, 8, 3)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_378 = _t56_100;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_379 = _t56_78;

  // 4-BLAC: 1x4 / 1x4
  _t56_380 = _mm256_div_pd(_t56_378, _t56_379);

  // AVX Storer:
  _t56_100 = _t56_380;

  // Generating : v0[8,1] = S(h(3, 8, 0), ( G(h(3, 8, 0), v0[8,1],h(1, 1, 0)) - ( G(h(3, 8, 0), M3[8,8],h(1, 8, 3)) Kro G(h(1, 8, 3), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_381 = _t56_109;

  // AVX Loader:

  // 3x1 -> 4x1
  _t56_382 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_67, _t56_77, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_73, 2), 10);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_383 = _t56_50;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_384 = _mm256_mul_pd(_t56_382, _t56_383);

  // 4-BLAC: 4x1 - 4x1
  _t56_385 = _mm256_sub_pd(_t56_381, _t56_384);

  // AVX Storer:
  _t56_109 = _t56_385;

  // Generating : v0[8,1] = S(h(1, 8, 2), ( G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 2), M3[8,8],h(1, 8, 2)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_386 = _t56_99;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_387 = _t56_76;

  // 4-BLAC: 1x4 / 1x4
  _t56_388 = _mm256_div_pd(_t56_386, _t56_387);

  // AVX Storer:
  _t56_99 = _t56_388;

  // Generating : v0[8,1] = S(h(2, 8, 0), ( G(h(2, 8, 0), v0[8,1],h(1, 1, 0)) - ( G(h(2, 8, 0), M3[8,8],h(1, 8, 2)) Kro G(h(1, 8, 2), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_389 = _t56_110;

  // AVX Loader:

  // 2x1 -> 4x1
  _t56_390 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_67, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_73, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_391 = _t56_49;

  // 4-BLAC: 4x1 Kro 1x4
  _t56_392 = _mm256_mul_pd(_t56_390, _t56_391);

  // 4-BLAC: 4x1 - 4x1
  _t56_393 = _mm256_sub_pd(_t56_389, _t56_392);

  // AVX Storer:
  _t56_110 = _t56_393;

  // Generating : v0[8,1] = S(h(1, 8, 1), ( G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 1), M3[8,8],h(1, 8, 1)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_394 = _t56_97;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_395 = _t56_71;

  // 4-BLAC: 1x4 / 1x4
  _t56_396 = _mm256_div_pd(_t56_394, _t56_395);

  // AVX Storer:
  _t56_97 = _t56_396;

  // Generating : v0[8,1] = S(h(1, 8, 0), ( G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) - ( G(h(1, 8, 0), M3[8,8],h(1, 8, 1)) Kro G(h(1, 8, 1), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_397 = _t56_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_398 = _t56_48;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_399 = _t56_97;

  // 4-BLAC: 1x4 Kro 1x4
  _t56_400 = _mm256_mul_pd(_t56_398, _t56_399);

  // 4-BLAC: 1x4 - 1x4
  _t56_401 = _mm256_sub_pd(_t56_397, _t56_400);

  // AVX Storer:
  _t56_95 = _t56_401;

  // Generating : v0[8,1] = S(h(1, 8, 0), ( G(h(1, 8, 0), v0[8,1],h(1, 1, 0)) Div G(h(1, 8, 0), M3[8,8],h(1, 8, 0)) ),h(1, 1, 0))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_402 = _t56_95;

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_403 = _t56_65;

  // 4-BLAC: 1x4 / 1x4
  _t56_404 = _mm256_div_pd(_t56_402, _t56_403);

  // AVX Storer:
  _t56_95 = _t56_404;

  // Generating : M1[8,28] = S(h(1, 8, 0), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) Kro G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_405 = _t56_64;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_115 = _mm256_mul_pd(_t56_405, _t56_115);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(3, 8, 1), ( G(h(3, 8, 1), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) * G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 3x4 -> 4x4
  _t56_406 = _t56_111;
  _t56_407 = _t56_112;
  _t56_408 = _t56_113;
  _t56_409 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t56_410 = _t56_67;

  // 4-BLAC: (1x4)^T
  _t56_411 = _t56_410;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_412 = _mm256_mul_pd(_t56_23, _t56_115);
  _t56_413 = _mm256_mul_pd(_t56_22, _t56_115);
  _t56_414 = _mm256_mul_pd(_t56_21, _t56_115);
  _t56_415 = _mm256_mul_pd(_t56_20, _t56_115);

  // 4-BLAC: 4x4 - 4x4
  _t56_416 = _mm256_sub_pd(_t56_406, _t56_412);
  _t56_417 = _mm256_sub_pd(_t56_407, _t56_413);
  _t56_418 = _mm256_sub_pd(_t56_408, _t56_414);
  _t56_419 = _mm256_sub_pd(_t56_409, _t56_415);

  // AVX Storer:
  _t56_111 = _t56_416;
  _t56_112 = _t56_417;
  _t56_113 = _t56_418;

  // Generating : M1[8,28] = S(h(1, 8, 1), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_420 = _t56_63;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_111 = _mm256_mul_pd(_t56_420, _t56_111);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(2, 8, 2), ( G(h(2, 8, 2), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) * G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 2x4 -> 4x4
  _t56_421 = _t56_112;
  _t56_422 = _t56_113;
  _t56_423 = _mm256_setzero_pd();
  _t56_424 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t56_425 = _t56_73;

  // 4-BLAC: (1x4)^T
  _t56_426 = _t56_425;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t56_427 = _mm256_mul_pd(_t56_19, _t56_111);
  _t56_428 = _mm256_mul_pd(_t56_18, _t56_111);
  _t56_429 = _mm256_mul_pd(_t56_17, _t56_111);
  _t56_430 = _mm256_mul_pd(_t56_16, _t56_111);

  // 4-BLAC: 4x4 - 4x4
  _t56_431 = _mm256_sub_pd(_t56_421, _t56_427);
  _t56_432 = _mm256_sub_pd(_t56_422, _t56_428);
  _t56_433 = _mm256_sub_pd(_t56_423, _t56_429);
  _t56_434 = _mm256_sub_pd(_t56_424, _t56_430);

  // AVX Storer:
  _t56_112 = _t56_431;
  _t56_113 = _t56_432;

  // Generating : M1[8,28] = S(h(1, 8, 2), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) Kro G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_435 = _t56_62;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_112 = _mm256_mul_pd(_t56_435, _t56_112);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 3), ( G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) Kro G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_436 = _t56_61;

  // 4-BLAC: (4x1)^T
  _t56_437 = _t56_436;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_116 = _mm256_mul_pd(_t56_437, _t56_112);

  // 4-BLAC: 1x4 - 1x4
  _t56_113 = _mm256_sub_pd(_t56_113, _t56_116);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 3), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) Kro G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t56_438 = _t56_60;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_113 = _mm256_mul_pd(_t56_438, _t56_113);

  // AVX Storer:

  _mm256_storeu_pd(M3, _t50_28);
  _mm256_maskstore_pd(M3 + 9, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_68);
  _mm256_maskstore_pd(M3 + 18, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_74);
  _mm256_maskstore_pd(M3 + 19, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_77);

  for( int fi353 = 4; fi353 <= 24; fi353+=4 ) {
    _t57_3 = _mm256_loadu_pd(M1 + fi353);
    _t57_0 = _mm256_loadu_pd(M1 + fi353 + 28);
    _t57_1 = _mm256_loadu_pd(M1 + fi353 + 56);
    _t57_2 = _mm256_loadu_pd(M1 + fi353 + 84);

    // Generating : M1[8,28] = S(h(1, 8, 0), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) Kro G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_4 = _t56_64;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_3 = _mm256_mul_pd(_t57_4, _t57_3);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(3, 8, 1), ( G(h(3, 8, 1), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 0), M3[8,8],h(3, 8, 1)) ) * G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 3x4 -> 4x4
    _t57_5 = _t57_0;
    _t57_6 = _t57_1;
    _t57_7 = _t57_2;
    _t57_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t57_9 = _t56_67;

    // 4-BLAC: (1x4)^T
    _t57_10 = _t57_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t56_412 = _mm256_mul_pd(_t56_23, _t57_3);
    _t56_413 = _mm256_mul_pd(_t56_22, _t57_3);
    _t56_414 = _mm256_mul_pd(_t56_21, _t57_3);
    _t56_415 = _mm256_mul_pd(_t56_20, _t57_3);

    // 4-BLAC: 4x4 - 4x4
    _t57_11 = _mm256_sub_pd(_t57_5, _t56_412);
    _t57_12 = _mm256_sub_pd(_t57_6, _t56_413);
    _t57_13 = _mm256_sub_pd(_t57_7, _t56_414);
    _t57_14 = _mm256_sub_pd(_t57_8, _t56_415);

    // AVX Storer:
    _t57_0 = _t57_11;
    _t57_1 = _t57_12;
    _t57_2 = _t57_13;

    // Generating : M1[8,28] = S(h(1, 8, 1), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_15 = _t56_63;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_0 = _mm256_mul_pd(_t57_15, _t57_0);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(2, 8, 2), ( G(h(2, 8, 2), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 1), M3[8,8],h(2, 8, 2)) ) * G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 2x4 -> 4x4
    _t57_16 = _t57_1;
    _t57_17 = _t57_2;
    _t57_18 = _mm256_setzero_pd();
    _t57_19 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t57_20 = _t56_73;

    // 4-BLAC: (1x4)^T
    _t57_21 = _t57_20;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t56_427 = _mm256_mul_pd(_t56_19, _t57_0);
    _t56_428 = _mm256_mul_pd(_t56_18, _t57_0);
    _t56_429 = _mm256_mul_pd(_t56_17, _t57_0);
    _t56_430 = _mm256_mul_pd(_t56_16, _t57_0);

    // 4-BLAC: 4x4 - 4x4
    _t57_22 = _mm256_sub_pd(_t57_16, _t56_427);
    _t57_23 = _mm256_sub_pd(_t57_17, _t56_428);
    _t57_24 = _mm256_sub_pd(_t57_18, _t56_429);
    _t57_25 = _mm256_sub_pd(_t57_19, _t56_430);

    // AVX Storer:
    _t57_1 = _t57_22;
    _t57_2 = _t57_23;

    // Generating : M1[8,28] = S(h(1, 8, 2), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) Kro G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_26 = _t56_62;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_1 = _mm256_mul_pd(_t57_26, _t57_1);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 3), ( G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 2), M3[8,8],h(1, 8, 3)) ) Kro G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_27 = _t56_61;

    // 4-BLAC: (4x1)^T
    _t56_437 = _t57_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t56_116 = _mm256_mul_pd(_t56_437, _t57_1);

    // 4-BLAC: 1x4 - 1x4
    _t57_2 = _mm256_sub_pd(_t57_2, _t56_116);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 3), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) Kro G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t57_28 = _t56_60;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t57_2 = _mm256_mul_pd(_t57_28, _t57_2);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi353, _t57_3);
    _mm256_storeu_pd(M1 + fi353 + 28, _t57_0);
    _mm256_storeu_pd(M1 + fi353 + 56, _t57_1);
    _mm256_storeu_pd(M1 + fi353 + 84, _t57_2);
  }


  // Generating : M1[8,28] = Sum_{i0} ( S(h(4, 8, 4), ( G(h(4, 8, 4), M1[8,28],h(4, 28, i0)) - ( T( G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) ) * G(h(4, 8, 0), M1[8,28],h(4, 28, i0)) ) ),h(4, 28, i0)) )

  // AVX Loader:

  // 4-BLAC: (4x4)^T
  _t58_0 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_8, _t52_9), _mm256_unpacklo_pd(_t52_10, _t52_11), 32);
  _t58_1 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_8, _t52_9), _mm256_unpackhi_pd(_t52_10, _t52_11), 32);
  _t58_2 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_t52_8, _t52_9), _mm256_unpacklo_pd(_t52_10, _t52_11), 49);
  _t58_3 = _mm256_permute2f128_pd(_mm256_unpackhi_pd(_t52_8, _t52_9), _mm256_unpackhi_pd(_t52_10, _t52_11), 49);

  _mm256_storeu_pd(M1, _t56_115);
  _mm256_storeu_pd(M1 + 28, _t56_111);
  _mm256_storeu_pd(M1 + 56, _t56_112);
  _mm256_storeu_pd(M1 + 84, _t56_113);

  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t59_24 = _mm256_loadu_pd(M1 + i0 + 112);
    _t59_25 = _mm256_loadu_pd(M1 + i0 + 140);
    _t59_26 = _mm256_loadu_pd(M1 + i0 + 168);
    _t59_27 = _mm256_loadu_pd(M1 + i0 + 196);
    _t59_19 = _mm256_loadu_pd(M1 + i0);
    _t59_18 = _mm256_loadu_pd(M1 + i0 + 28);
    _t59_17 = _mm256_loadu_pd(M1 + i0 + 56);
    _t59_16 = _mm256_loadu_pd(M1 + i0 + 84);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t59_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_15, _t59_19), _mm256_mul_pd(_t59_14, _t59_18)), _mm256_add_pd(_mm256_mul_pd(_t59_13, _t59_17), _mm256_mul_pd(_t59_12, _t59_16)));
    _t59_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_11, _t59_19), _mm256_mul_pd(_t59_10, _t59_18)), _mm256_add_pd(_mm256_mul_pd(_t59_9, _t59_17), _mm256_mul_pd(_t59_8, _t59_16)));
    _t59_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_7, _t59_19), _mm256_mul_pd(_t59_6, _t59_18)), _mm256_add_pd(_mm256_mul_pd(_t59_5, _t59_17), _mm256_mul_pd(_t59_4, _t59_16)));
    _t59_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t59_3, _t59_19), _mm256_mul_pd(_t59_2, _t59_18)), _mm256_add_pd(_mm256_mul_pd(_t59_1, _t59_17), _mm256_mul_pd(_t59_0, _t59_16)));

    // 4-BLAC: 4x4 - 4x4
    _t59_24 = _mm256_sub_pd(_t59_24, _t59_20);
    _t59_25 = _mm256_sub_pd(_t59_25, _t59_21);
    _t59_26 = _mm256_sub_pd(_t59_26, _t59_22);
    _t59_27 = _mm256_sub_pd(_t59_27, _t59_23);

    // AVX Storer:
    _mm256_storeu_pd(M1 + i0 + 112, _t59_24);
    _mm256_storeu_pd(M1 + i0 + 140, _t59_25);
    _mm256_storeu_pd(M1 + i0 + 168, _t59_26);
    _mm256_storeu_pd(M1 + i0 + 196, _t59_27);
  }

  _t60_16 = _mm256_loadu_pd(M1 + 112);
  _t60_13 = _mm256_loadu_pd(M1 + 140);
  _t60_14 = _mm256_loadu_pd(M1 + 168);
  _t60_15 = _mm256_loadu_pd(M1 + 196);
  _t60_9 = _mm256_broadcast_sd(&(M3[55]));

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 6), M3[8,8],h(1, 8, 6)) ),h(1, 8, 6))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_18 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_19 = _t56_92;

  // 4-BLAC: 1x4 / 1x4
  _t60_20 = _mm256_div_pd(_t60_18, _t60_19);

  // AVX Storer:
  _t60_11 = _t60_20;

  // Generating : T608[1,8] = S(h(1, 1, 0), ( G(h(1, 1, 0), 1[1,1],h(1, 1, 0)) Div G(h(1, 8, 7), M3[8,8],h(1, 8, 7)) ),h(1, 8, 7))

  // AVX Loader:

  // Constant 1x1 -> 1x4
  _t60_21 = _mm256_set_pd(0, 0, 0, 1);

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_22 = _t56_94;

  // 4-BLAC: 1x4 / 1x4
  _t60_23 = _mm256_div_pd(_t60_21, _t60_22);

  // AVX Storer:
  _t60_12 = _t60_23;

  // Generating : M1[8,28] = S(h(1, 8, 4), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) Kro G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_24 = _t56_59;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_16 = _mm256_mul_pd(_t60_24, _t60_16);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(3, 8, 5), ( G(h(3, 8, 5), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) * G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 3x4 -> 4x4
  _t60_25 = _t60_13;
  _t60_26 = _t60_14;
  _t60_27 = _t60_15;
  _t60_28 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x3 -> 1x4
  _t60_29 = _t56_83;

  // 4-BLAC: (1x4)^T
  _t60_30 = _t60_29;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t60_31 = _mm256_mul_pd(_t60_7, _t60_16);
  _t60_32 = _mm256_mul_pd(_t60_6, _t60_16);
  _t60_33 = _mm256_mul_pd(_t60_5, _t60_16);
  _t60_34 = _mm256_mul_pd(_t60_4, _t60_16);

  // 4-BLAC: 4x4 - 4x4
  _t60_35 = _mm256_sub_pd(_t60_25, _t60_31);
  _t60_36 = _mm256_sub_pd(_t60_26, _t60_32);
  _t60_37 = _mm256_sub_pd(_t60_27, _t60_33);
  _t60_38 = _mm256_sub_pd(_t60_28, _t60_34);

  // AVX Storer:
  _t60_13 = _t60_35;
  _t60_14 = _t60_36;
  _t60_15 = _t60_37;

  // Generating : M1[8,28] = S(h(1, 8, 5), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_39 = _t56_58;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_13 = _mm256_mul_pd(_t60_39, _t60_13);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(2, 8, 6), ( G(h(2, 8, 6), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) * G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 2x4 -> 4x4
  _t60_40 = _t60_14;
  _t60_41 = _t60_15;
  _t60_42 = _mm256_setzero_pd();
  _t60_43 = _mm256_setzero_pd();

  // AVX Loader:

  // 1x2 -> 1x4
  _t60_44 = _t56_89;

  // 4-BLAC: (1x4)^T
  _t60_45 = _t60_44;

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t60_46 = _mm256_mul_pd(_t60_3, _t60_13);
  _t60_47 = _mm256_mul_pd(_t60_2, _t60_13);
  _t60_48 = _mm256_mul_pd(_t60_1, _t60_13);
  _t60_49 = _mm256_mul_pd(_t60_0, _t60_13);

  // 4-BLAC: 4x4 - 4x4
  _t60_50 = _mm256_sub_pd(_t60_40, _t60_46);
  _t60_51 = _mm256_sub_pd(_t60_41, _t60_47);
  _t60_52 = _mm256_sub_pd(_t60_42, _t60_48);
  _t60_53 = _mm256_sub_pd(_t60_43, _t60_49);

  // AVX Storer:
  _t60_14 = _t60_50;
  _t60_15 = _t60_51;

  // Generating : M1[8,28] = S(h(1, 8, 6), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) Kro G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_54 = _t60_10;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_14 = _mm256_mul_pd(_t60_54, _t60_14);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 7), ( G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) Kro G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_55 = _t60_9;

  // 4-BLAC: (4x1)^T
  _t60_56 = _t60_55;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_17 = _mm256_mul_pd(_t60_56, _t60_14);

  // 4-BLAC: 1x4 - 1x4
  _t60_15 = _mm256_sub_pd(_t60_15, _t60_17);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 7), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) Kro G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t60_57 = _t60_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_15 = _mm256_mul_pd(_t60_57, _t60_15);

  // AVX Storer:

  _mm256_storeu_pd(M3 + 36, _t54_24);
  _mm256_maskstore_pd(M3 + 45, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_84);
  _mm256_maskstore_pd(M3 + 54, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_90);
  _mm256_maskstore_pd(M3 + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_93);

  for( int fi353 = 4; fi353 <= 24; fi353+=4 ) {
    _t61_3 = _mm256_loadu_pd(M1 + fi353 + 112);
    _t61_0 = _mm256_loadu_pd(M1 + fi353 + 140);
    _t61_1 = _mm256_loadu_pd(M1 + fi353 + 168);
    _t61_2 = _mm256_loadu_pd(M1 + fi353 + 196);

    // Generating : M1[8,28] = S(h(1, 8, 4), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) Kro G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_4 = _t56_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_3 = _mm256_mul_pd(_t61_4, _t61_3);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(3, 8, 5), ( G(h(3, 8, 5), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 4), M3[8,8],h(3, 8, 5)) ) * G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 3x4 -> 4x4
    _t61_5 = _t61_0;
    _t61_6 = _t61_1;
    _t61_7 = _t61_2;
    _t61_8 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x3 -> 1x4
    _t61_9 = _t56_83;

    // 4-BLAC: (1x4)^T
    _t61_10 = _t61_9;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_31 = _mm256_mul_pd(_t60_7, _t61_3);
    _t60_32 = _mm256_mul_pd(_t60_6, _t61_3);
    _t60_33 = _mm256_mul_pd(_t60_5, _t61_3);
    _t60_34 = _mm256_mul_pd(_t60_4, _t61_3);

    // 4-BLAC: 4x4 - 4x4
    _t61_11 = _mm256_sub_pd(_t61_5, _t60_31);
    _t61_12 = _mm256_sub_pd(_t61_6, _t60_32);
    _t61_13 = _mm256_sub_pd(_t61_7, _t60_33);
    _t61_14 = _mm256_sub_pd(_t61_8, _t60_34);

    // AVX Storer:
    _t61_0 = _t61_11;
    _t61_1 = _t61_12;
    _t61_2 = _t61_13;

    // Generating : M1[8,28] = S(h(1, 8, 5), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_15 = _t56_58;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_0 = _mm256_mul_pd(_t61_15, _t61_0);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(2, 8, 6), ( G(h(2, 8, 6), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 5), M3[8,8],h(2, 8, 6)) ) * G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 2x4 -> 4x4
    _t61_16 = _t61_1;
    _t61_17 = _t61_2;
    _t61_18 = _mm256_setzero_pd();
    _t61_19 = _mm256_setzero_pd();

    // AVX Loader:

    // 1x2 -> 1x4
    _t61_20 = _t56_89;

    // 4-BLAC: (1x4)^T
    _t61_21 = _t61_20;

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t60_46 = _mm256_mul_pd(_t60_3, _t61_0);
    _t60_47 = _mm256_mul_pd(_t60_2, _t61_0);
    _t60_48 = _mm256_mul_pd(_t60_1, _t61_0);
    _t60_49 = _mm256_mul_pd(_t60_0, _t61_0);

    // 4-BLAC: 4x4 - 4x4
    _t61_22 = _mm256_sub_pd(_t61_16, _t60_46);
    _t61_23 = _mm256_sub_pd(_t61_17, _t60_47);
    _t61_24 = _mm256_sub_pd(_t61_18, _t60_48);
    _t61_25 = _mm256_sub_pd(_t61_19, _t60_49);

    // AVX Storer:
    _t61_1 = _t61_22;
    _t61_2 = _t61_23;

    // Generating : M1[8,28] = S(h(1, 8, 6), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) Kro G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_26 = _t60_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_1 = _mm256_mul_pd(_t61_26, _t61_1);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 7), ( G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) - ( T( G(h(1, 8, 6), M3[8,8],h(1, 8, 7)) ) Kro G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_27 = _t60_9;

    // 4-BLAC: (4x1)^T
    _t60_56 = _t61_27;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t60_17 = _mm256_mul_pd(_t60_56, _t61_1);

    // 4-BLAC: 1x4 - 1x4
    _t61_2 = _mm256_sub_pd(_t61_2, _t60_17);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 7), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) Kro G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t61_28 = _t60_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t61_2 = _mm256_mul_pd(_t61_28, _t61_2);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi353 + 112, _t61_3);
    _mm256_storeu_pd(M1 + fi353 + 140, _t61_0);
    _mm256_storeu_pd(M1 + fi353 + 168, _t61_1);
    _mm256_storeu_pd(M1 + fi353 + 196, _t61_2);
  }

  _t56_93 = _mm256_maskload_pd(M3 + 55, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t62_8 = _mm256_broadcast_sd(&(M3[37]));

  // Generating : M1[8,28] = S(h(1, 8, 7), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) Kro G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_10 = _t60_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_15 = _mm256_mul_pd(_t62_10, _t60_15);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(3, 8, 4), ( G(h(3, 8, 4), M1[8,28],h(4, 28, fi353)) - ( G(h(3, 8, 4), M3[8,8],h(1, 8, 7)) * G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 3x4 -> 4x4
  _t62_11 = _t60_16;
  _t62_12 = _t60_13;
  _t62_13 = _t60_14;
  _t62_14 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t62_15 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_83, _t56_93, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_89, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t62_16 = _mm256_mul_pd(_t62_7, _t60_15);
  _t62_17 = _mm256_mul_pd(_t62_6, _t60_15);
  _t62_18 = _mm256_mul_pd(_t62_5, _t60_15);
  _t62_19 = _mm256_mul_pd(_t62_4, _t60_15);

  // 4-BLAC: 4x4 - 4x4
  _t62_20 = _mm256_sub_pd(_t62_11, _t62_16);
  _t62_21 = _mm256_sub_pd(_t62_12, _t62_17);
  _t62_22 = _mm256_sub_pd(_t62_13, _t62_18);
  _t62_23 = _mm256_sub_pd(_t62_14, _t62_19);

  // AVX Storer:
  _t60_16 = _t62_20;
  _t60_13 = _t62_21;
  _t60_14 = _t62_22;

  // Generating : M1[8,28] = S(h(1, 8, 6), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) Kro G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_24 = _t60_10;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_14 = _mm256_mul_pd(_t62_24, _t60_14);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(2, 8, 4), ( G(h(2, 8, 4), M1[8,28],h(4, 28, fi353)) - ( G(h(2, 8, 4), M3[8,8],h(1, 8, 6)) * G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 2x4 -> 4x4
  _t62_25 = _t60_16;
  _t62_26 = _t60_13;
  _t62_27 = _mm256_setzero_pd();
  _t62_28 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t62_29 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_83, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_89, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t62_30 = _mm256_mul_pd(_t62_3, _t60_14);
  _t62_31 = _mm256_mul_pd(_t62_2, _t60_14);
  _t62_32 = _mm256_mul_pd(_t62_1, _t60_14);
  _t62_33 = _mm256_mul_pd(_t62_0, _t60_14);

  // 4-BLAC: 4x4 - 4x4
  _t62_34 = _mm256_sub_pd(_t62_25, _t62_30);
  _t62_35 = _mm256_sub_pd(_t62_26, _t62_31);
  _t62_36 = _mm256_sub_pd(_t62_27, _t62_32);
  _t62_37 = _mm256_sub_pd(_t62_28, _t62_33);

  // AVX Storer:
  _t60_16 = _t62_34;
  _t60_13 = _t62_35;

  // Generating : M1[8,28] = S(h(1, 8, 5), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_38 = _t56_58;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_13 = _mm256_mul_pd(_t62_38, _t60_13);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 4), ( G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) - ( G(h(1, 8, 4), M3[8,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_39 = _t62_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t62_9 = _mm256_mul_pd(_t62_39, _t60_13);

  // 4-BLAC: 1x4 - 1x4
  _t60_16 = _mm256_sub_pd(_t60_16, _t62_9);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 4), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) Kro G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t62_40 = _t56_59;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t60_16 = _mm256_mul_pd(_t62_40, _t60_16);

  // AVX Storer:

  _mm256_maskstore_pd(M3 + 37, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_83);

  for( int fi353 = 4; fi353 <= 24; fi353+=4 ) {
    _t63_12 = _mm256_loadu_pd(M1 + fi353 + 196);
    _t63_9 = _mm256_loadu_pd(M1 + fi353 + 112);
    _t63_10 = _mm256_loadu_pd(M1 + fi353 + 140);
    _t63_11 = _mm256_loadu_pd(M1 + fi353 + 168);
    _t63_8 = _mm256_maskload_pd(M3 + 37, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : M1[8,28] = S(h(1, 8, 7), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 7)) Kro G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_13 = _t60_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t63_12 = _mm256_mul_pd(_t63_13, _t63_12);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(3, 8, 4), ( G(h(3, 8, 4), M1[8,28],h(4, 28, fi353)) - ( G(h(3, 8, 4), M3[8,8],h(1, 8, 7)) * G(h(1, 8, 7), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 3x4 -> 4x4
    _t63_14 = _t63_9;
    _t63_15 = _t63_10;
    _t63_16 = _t63_11;
    _t63_17 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t63_18 = _mm256_blend_pd(_mm256_permute2f128_pd(_t63_8, _t56_93, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_89, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t62_16 = _mm256_mul_pd(_t63_7, _t63_12);
    _t62_17 = _mm256_mul_pd(_t63_6, _t63_12);
    _t62_18 = _mm256_mul_pd(_t63_5, _t63_12);
    _t62_19 = _mm256_mul_pd(_t63_4, _t63_12);

    // 4-BLAC: 4x4 - 4x4
    _t63_19 = _mm256_sub_pd(_t63_14, _t62_16);
    _t63_20 = _mm256_sub_pd(_t63_15, _t62_17);
    _t63_21 = _mm256_sub_pd(_t63_16, _t62_18);
    _t63_22 = _mm256_sub_pd(_t63_17, _t62_19);

    // AVX Storer:
    _t63_9 = _t63_19;
    _t63_10 = _t63_20;
    _t63_11 = _t63_21;

    // Generating : M1[8,28] = S(h(1, 8, 6), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 6)) Kro G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_23 = _t60_10;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t63_11 = _mm256_mul_pd(_t63_23, _t63_11);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(2, 8, 4), ( G(h(2, 8, 4), M1[8,28],h(4, 28, fi353)) - ( G(h(2, 8, 4), M3[8,8],h(1, 8, 6)) * G(h(1, 8, 6), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 2x4 -> 4x4
    _t63_24 = _t63_9;
    _t63_25 = _t63_10;
    _t63_26 = _mm256_setzero_pd();
    _t63_27 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t63_28 = _mm256_shuffle_pd(_mm256_blend_pd(_t63_8, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_89, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t62_30 = _mm256_mul_pd(_t63_3, _t63_11);
    _t62_31 = _mm256_mul_pd(_t63_2, _t63_11);
    _t62_32 = _mm256_mul_pd(_t63_1, _t63_11);
    _t62_33 = _mm256_mul_pd(_t63_0, _t63_11);

    // 4-BLAC: 4x4 - 4x4
    _t63_29 = _mm256_sub_pd(_t63_24, _t62_30);
    _t63_30 = _mm256_sub_pd(_t63_25, _t62_31);
    _t63_31 = _mm256_sub_pd(_t63_26, _t62_32);
    _t63_32 = _mm256_sub_pd(_t63_27, _t62_33);

    // AVX Storer:
    _t63_9 = _t63_29;
    _t63_10 = _t63_30;

    // Generating : M1[8,28] = S(h(1, 8, 5), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_33 = _t56_58;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t63_10 = _mm256_mul_pd(_t63_33, _t63_10);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 4), ( G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) - ( G(h(1, 8, 4), M3[8,8],h(1, 8, 5)) Kro G(h(1, 8, 5), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_34 = _t62_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t62_9 = _mm256_mul_pd(_t63_34, _t63_10);

    // 4-BLAC: 1x4 - 1x4
    _t63_9 = _mm256_sub_pd(_t63_9, _t62_9);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 4), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 4)) Kro G(h(1, 8, 4), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t63_35 = _t56_59;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t63_9 = _mm256_mul_pd(_t63_35, _t63_9);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi353 + 196, _t63_12);
    _mm256_storeu_pd(M1 + fi353 + 112, _t63_9);
    _mm256_storeu_pd(M1 + fi353 + 140, _t63_10);
    _mm256_storeu_pd(M1 + fi353 + 168, _t63_11);
  }


  // Generating : M1[8,28] = Sum_{i0} ( S(h(4, 8, 0), ( G(h(4, 8, 0), M1[8,28],h(4, 28, i0)) - ( G(h(4, 8, 0), M3[8,8],h(4, 8, 4)) * G(h(4, 8, 4), M1[8,28],h(4, 28, i0)) ) ),h(4, 28, i0)) )

  // AVX Loader:

  _mm256_storeu_pd(M3 + 4, _t52_8);
  _mm256_storeu_pd(M3 + 12, _t52_9);
  _mm256_storeu_pd(M3 + 20, _t52_10);
  _mm256_storeu_pd(M3 + 28, _t52_11);
  _mm256_storeu_pd(M1 + 112, _t60_16);
  _mm256_storeu_pd(M1 + 140, _t60_13);
  _mm256_storeu_pd(M1 + 168, _t60_14);
  _mm256_storeu_pd(M1 + 196, _t60_15);

  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t64_24 = _mm256_loadu_pd(M1 + i0);
    _t64_25 = _mm256_loadu_pd(M1 + i0 + 28);
    _t64_26 = _mm256_loadu_pd(M1 + i0 + 56);
    _t64_27 = _mm256_loadu_pd(M1 + i0 + 84);
    _t64_19 = _mm256_broadcast_sd(M3 + 4);
    _t64_18 = _mm256_broadcast_sd(M3 + 5);
    _t64_17 = _mm256_broadcast_sd(M3 + 6);
    _t64_16 = _mm256_broadcast_sd(M3 + 7);
    _t64_15 = _mm256_broadcast_sd(M3 + 12);
    _t64_14 = _mm256_broadcast_sd(M3 + 13);
    _t64_13 = _mm256_broadcast_sd(M3 + 14);
    _t64_12 = _mm256_broadcast_sd(M3 + 15);
    _t64_11 = _mm256_broadcast_sd(M3 + 20);
    _t64_10 = _mm256_broadcast_sd(M3 + 21);
    _t64_9 = _mm256_broadcast_sd(M3 + 22);
    _t64_8 = _mm256_broadcast_sd(M3 + 23);
    _t64_7 = _mm256_broadcast_sd(M3 + 28);
    _t64_6 = _mm256_broadcast_sd(M3 + 29);
    _t64_5 = _mm256_broadcast_sd(M3 + 30);
    _t64_4 = _mm256_broadcast_sd(M3 + 31);
    _t64_3 = _mm256_loadu_pd(M1 + i0 + 112);
    _t64_2 = _mm256_loadu_pd(M1 + i0 + 140);
    _t64_1 = _mm256_loadu_pd(M1 + i0 + 168);
    _t64_0 = _mm256_loadu_pd(M1 + i0 + 196);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t64_20 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_19, _t64_3), _mm256_mul_pd(_t64_18, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_17, _t64_1), _mm256_mul_pd(_t64_16, _t64_0)));
    _t64_21 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_15, _t64_3), _mm256_mul_pd(_t64_14, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_13, _t64_1), _mm256_mul_pd(_t64_12, _t64_0)));
    _t64_22 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_11, _t64_3), _mm256_mul_pd(_t64_10, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_9, _t64_1), _mm256_mul_pd(_t64_8, _t64_0)));
    _t64_23 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t64_7, _t64_3), _mm256_mul_pd(_t64_6, _t64_2)), _mm256_add_pd(_mm256_mul_pd(_t64_5, _t64_1), _mm256_mul_pd(_t64_4, _t64_0)));

    // 4-BLAC: 4x4 - 4x4
    _t64_24 = _mm256_sub_pd(_t64_24, _t64_20);
    _t64_25 = _mm256_sub_pd(_t64_25, _t64_21);
    _t64_26 = _mm256_sub_pd(_t64_26, _t64_22);
    _t64_27 = _mm256_sub_pd(_t64_27, _t64_23);

    // AVX Storer:
    _mm256_storeu_pd(M1 + i0, _t64_24);
    _mm256_storeu_pd(M1 + i0 + 28, _t64_25);
    _mm256_storeu_pd(M1 + i0 + 56, _t64_26);
    _mm256_storeu_pd(M1 + i0 + 84, _t64_27);
  }

  _t56_77 = _mm256_maskload_pd(M3 + 19, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0));
  _t56_113 = _mm256_loadu_pd(M1 + 84);
  _t56_115 = _mm256_loadu_pd(M1);
  _t56_112 = _mm256_loadu_pd(M1 + 56);
  _t56_111 = _mm256_loadu_pd(M1 + 28);
  _t65_8 = _mm256_broadcast_sd(&(M3[1]));

  // Generating : M1[8,28] = S(h(1, 8, 3), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) Kro G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_10 = _t56_60;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_113 = _mm256_mul_pd(_t65_10, _t56_113);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(3, 8, 0), ( G(h(3, 8, 0), M1[8,28],h(4, 28, fi353)) - ( G(h(3, 8, 0), M3[8,8],h(1, 8, 3)) * G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 3x4 -> 4x4
  _t65_11 = _t56_115;
  _t65_12 = _t56_111;
  _t65_13 = _t56_112;
  _t65_14 = _mm256_setzero_pd();

  // AVX Loader:

  // 3x1 -> 4x1
  _t65_15 = _mm256_blend_pd(_mm256_permute2f128_pd(_t56_67, _t56_77, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_73, 2), 10);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t65_16 = _mm256_mul_pd(_t65_7, _t56_113);
  _t65_17 = _mm256_mul_pd(_t65_6, _t56_113);
  _t65_18 = _mm256_mul_pd(_t65_5, _t56_113);
  _t65_19 = _mm256_mul_pd(_t65_4, _t56_113);

  // 4-BLAC: 4x4 - 4x4
  _t65_20 = _mm256_sub_pd(_t65_11, _t65_16);
  _t65_21 = _mm256_sub_pd(_t65_12, _t65_17);
  _t65_22 = _mm256_sub_pd(_t65_13, _t65_18);
  _t65_23 = _mm256_sub_pd(_t65_14, _t65_19);

  // AVX Storer:
  _t56_115 = _t65_20;
  _t56_111 = _t65_21;
  _t56_112 = _t65_22;

  // Generating : M1[8,28] = S(h(1, 8, 2), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) Kro G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_24 = _t56_62;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_112 = _mm256_mul_pd(_t65_24, _t56_112);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(2, 8, 0), ( G(h(2, 8, 0), M1[8,28],h(4, 28, fi353)) - ( G(h(2, 8, 0), M3[8,8],h(1, 8, 2)) * G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // 2x4 -> 4x4
  _t65_25 = _t56_115;
  _t65_26 = _t56_111;
  _t65_27 = _mm256_setzero_pd();
  _t65_28 = _mm256_setzero_pd();

  // AVX Loader:

  // 2x1 -> 4x1
  _t65_29 = _mm256_shuffle_pd(_mm256_blend_pd(_t56_67, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_73, _mm256_setzero_pd(), 12), 1);

  // AVX Loader:

  // 4-BLAC: 4x1 * 1x4
  _t65_30 = _mm256_mul_pd(_t65_3, _t56_112);
  _t65_31 = _mm256_mul_pd(_t65_2, _t56_112);
  _t65_32 = _mm256_mul_pd(_t65_1, _t56_112);
  _t65_33 = _mm256_mul_pd(_t65_0, _t56_112);

  // 4-BLAC: 4x4 - 4x4
  _t65_34 = _mm256_sub_pd(_t65_25, _t65_30);
  _t65_35 = _mm256_sub_pd(_t65_26, _t65_31);
  _t65_36 = _mm256_sub_pd(_t65_27, _t65_32);
  _t65_37 = _mm256_sub_pd(_t65_28, _t65_33);

  // AVX Storer:
  _t56_115 = _t65_34;
  _t56_111 = _t65_35;

  // Generating : M1[8,28] = S(h(1, 8, 1), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_38 = _t56_63;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_111 = _mm256_mul_pd(_t65_38, _t56_111);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 0), ( G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) - ( G(h(1, 8, 0), M3[8,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

  // AVX Loader:

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_39 = _t65_8;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t65_9 = _mm256_mul_pd(_t65_39, _t56_111);

  // 4-BLAC: 1x4 - 1x4
  _t56_115 = _mm256_sub_pd(_t56_115, _t65_9);

  // AVX Storer:

  // Generating : M1[8,28] = S(h(1, 8, 0), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) Kro G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

  // AVX Loader:

  // 1x1 -> 1x4
  _t65_40 = _t56_64;

  // AVX Loader:

  // 4-BLAC: 1x4 Kro 1x4
  _t56_115 = _mm256_mul_pd(_t65_40, _t56_115);

  // AVX Storer:

  _mm256_maskstore_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0), _t56_67);

  for( int fi353 = 4; fi353 <= 24; fi353+=4 ) {
    _t66_12 = _mm256_loadu_pd(M1 + fi353 + 84);
    _t66_9 = _mm256_loadu_pd(M1 + fi353);
    _t66_10 = _mm256_loadu_pd(M1 + fi353 + 28);
    _t66_11 = _mm256_loadu_pd(M1 + fi353 + 56);
    _t66_8 = _mm256_maskload_pd(M3 + 1, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63, 0));

    // Generating : M1[8,28] = S(h(1, 8, 3), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 3)) Kro G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_13 = _t56_60;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t66_12 = _mm256_mul_pd(_t66_13, _t66_12);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(3, 8, 0), ( G(h(3, 8, 0), M1[8,28],h(4, 28, fi353)) - ( G(h(3, 8, 0), M3[8,8],h(1, 8, 3)) * G(h(1, 8, 3), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 3x4 -> 4x4
    _t66_14 = _t66_9;
    _t66_15 = _t66_10;
    _t66_16 = _t66_11;
    _t66_17 = _mm256_setzero_pd();

    // AVX Loader:

    // 3x1 -> 4x1
    _t66_18 = _mm256_blend_pd(_mm256_permute2f128_pd(_t66_8, _t56_77, 33), _mm256_blend_pd(_mm256_setzero_pd(), _t56_73, 2), 10);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_16 = _mm256_mul_pd(_t66_7, _t66_12);
    _t65_17 = _mm256_mul_pd(_t66_6, _t66_12);
    _t65_18 = _mm256_mul_pd(_t66_5, _t66_12);
    _t65_19 = _mm256_mul_pd(_t66_4, _t66_12);

    // 4-BLAC: 4x4 - 4x4
    _t66_19 = _mm256_sub_pd(_t66_14, _t65_16);
    _t66_20 = _mm256_sub_pd(_t66_15, _t65_17);
    _t66_21 = _mm256_sub_pd(_t66_16, _t65_18);
    _t66_22 = _mm256_sub_pd(_t66_17, _t65_19);

    // AVX Storer:
    _t66_9 = _t66_19;
    _t66_10 = _t66_20;
    _t66_11 = _t66_21;

    // Generating : M1[8,28] = S(h(1, 8, 2), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 2)) Kro G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_23 = _t56_62;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t66_11 = _mm256_mul_pd(_t66_23, _t66_11);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(2, 8, 0), ( G(h(2, 8, 0), M1[8,28],h(4, 28, fi353)) - ( G(h(2, 8, 0), M3[8,8],h(1, 8, 2)) * G(h(1, 8, 2), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // 2x4 -> 4x4
    _t66_24 = _t66_9;
    _t66_25 = _t66_10;
    _t66_26 = _mm256_setzero_pd();
    _t66_27 = _mm256_setzero_pd();

    // AVX Loader:

    // 2x1 -> 4x1
    _t66_28 = _mm256_shuffle_pd(_mm256_blend_pd(_t66_8, _mm256_setzero_pd(), 12), _mm256_blend_pd(_t56_73, _mm256_setzero_pd(), 12), 1);

    // AVX Loader:

    // 4-BLAC: 4x1 * 1x4
    _t65_30 = _mm256_mul_pd(_t66_3, _t66_11);
    _t65_31 = _mm256_mul_pd(_t66_2, _t66_11);
    _t65_32 = _mm256_mul_pd(_t66_1, _t66_11);
    _t65_33 = _mm256_mul_pd(_t66_0, _t66_11);

    // 4-BLAC: 4x4 - 4x4
    _t66_29 = _mm256_sub_pd(_t66_24, _t65_30);
    _t66_30 = _mm256_sub_pd(_t66_25, _t65_31);
    _t66_31 = _mm256_sub_pd(_t66_26, _t65_32);
    _t66_32 = _mm256_sub_pd(_t66_27, _t65_33);

    // AVX Storer:
    _t66_9 = _t66_29;
    _t66_10 = _t66_30;

    // Generating : M1[8,28] = S(h(1, 8, 1), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_33 = _t56_63;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t66_10 = _mm256_mul_pd(_t66_33, _t66_10);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 0), ( G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) - ( G(h(1, 8, 0), M3[8,8],h(1, 8, 1)) Kro G(h(1, 8, 1), M1[8,28],h(4, 28, fi353)) ) ),h(4, 28, fi353))

    // AVX Loader:

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_34 = _t65_8;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t65_9 = _mm256_mul_pd(_t66_34, _t66_10);

    // 4-BLAC: 1x4 - 1x4
    _t66_9 = _mm256_sub_pd(_t66_9, _t65_9);

    // AVX Storer:

    // Generating : M1[8,28] = S(h(1, 8, 0), ( G(h(1, 1, 0), T608[1,8],h(1, 8, 0)) Kro G(h(1, 8, 0), M1[8,28],h(4, 28, fi353)) ),h(4, 28, fi353))

    // AVX Loader:

    // 1x1 -> 1x4
    _t66_35 = _t56_64;

    // AVX Loader:

    // 4-BLAC: 1x4 Kro 1x4
    _t66_9 = _mm256_mul_pd(_t66_35, _t66_9);

    // AVX Storer:
    _mm256_storeu_pd(M1 + fi353 + 84, _t66_12);
    _mm256_storeu_pd(M1 + fi353, _t66_9);
    _mm256_storeu_pd(M1 + fi353 + 28, _t66_10);
    _mm256_storeu_pd(M1 + fi353 + 56, _t66_11);
  }


  // Generating : x[28,1] = Sum_{i0} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), y[28,1],h(1, 1, 0)) + ( G(h(4, 28, i0), M2[28,8],h(4, 8, 0)) * G(h(4, 8, 0), v0[8,1],h(1, 1, 0)) ) ),h(1, 1, 0)) + $(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,8],h(4, 8, 4)) * G(h(4, 8, 4), v0[8,1],h(1, 1, 0)) ),h(1, 1, 0)) ) )

  // AVX Loader:

  // AVX Loader:

  _mm256_maskstore_pd(v0 + 6, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_104);
  _mm256_maskstore_pd(v0 + 6, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_105);
  _mm256_maskstore_pd(v0 + 5, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_103);
  _mm256_maskstore_pd(v0 + 4, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_101);
  _mm256_storeu_pd(v0, _t56_128);
  _mm256_maskstore_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_99);
  _mm256_maskstore_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_97);
  _mm256_maskstore_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_95);

  for( int i0 = 0; i0 <= 27; i0+=4 ) {
    _t67_12 = _mm256_loadu_pd(y + i0);
    _t67_11 = _mm256_loadu_pd(M2 + 8*i0);
    _t67_10 = _mm256_loadu_pd(M2 + 8*i0 + 8);
    _t67_9 = _mm256_loadu_pd(M2 + 8*i0 + 16);
    _t67_8 = _mm256_loadu_pd(M2 + 8*i0 + 24);
    _t67_7 = _mm256_loadu_pd(M2 + 8*i0 + 4);
    _t67_6 = _mm256_loadu_pd(M2 + 8*i0 + 12);
    _t67_5 = _mm256_loadu_pd(M2 + 8*i0 + 20);
    _t67_4 = _mm256_loadu_pd(M2 + 8*i0 + 28);
    _t67_3 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(v0 + 4, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(v0 + 5, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(v0 + 6, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t67_2 = _mm256_shuffle_pd(_mm256_maskload_pd(v0 + 4, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(v0 + 5, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);
    _t67_1 = _mm256_permute2f128_pd(_mm256_unpacklo_pd(_mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0))), _mm256_maskload_pd(v0 + 2, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 32);
    _t67_0 = _mm256_shuffle_pd(_mm256_maskload_pd(v0, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), _mm256_maskload_pd(v0 + 1, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0)), 0);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t67_14 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t67_11, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8)), _mm256_mul_pd(_t67_10, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8))), _mm256_hadd_pd(_mm256_mul_pd(_t67_9, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8)), _mm256_mul_pd(_t67_8, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t67_11, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8)), _mm256_mul_pd(_t67_10, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8))), _mm256_hadd_pd(_mm256_mul_pd(_t67_9, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8)), _mm256_mul_pd(_t67_8, _mm256_blend_pd(_mm256_blend_pd(_t67_0, _t67_1, 4), _mm256_unpacklo_pd(_mm256_setzero_pd(), _mm256_permute2f128_pd(_t56_100, _t56_100, 8)), 8))), 12));

    // 4-BLAC: 4x1 + 4x1
    _t67_13 = _mm256_add_pd(_t67_12, _t67_14);

    // AVX Storer:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x1
    _t67_15 = _mm256_add_pd(_mm256_permute2f128_pd(_mm256_hadd_pd(_mm256_mul_pd(_t67_7, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11)), _mm256_mul_pd(_t67_6, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11))), _mm256_hadd_pd(_mm256_mul_pd(_t67_5, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11)), _mm256_mul_pd(_t67_4, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11))), 33), _mm256_blend_pd(_mm256_hadd_pd(_mm256_mul_pd(_t67_7, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11)), _mm256_mul_pd(_t67_6, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11))), _mm256_hadd_pd(_mm256_mul_pd(_t67_5, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11)), _mm256_mul_pd(_t67_4, _mm256_blend_pd(_t67_3, _mm256_shuffle_pd(_mm256_permute2f128_pd(_t67_2, _t56_106, 32), _mm256_permute2f128_pd(_t67_2, _t56_106, 32), 2), 11))), 12));

    // AVX Loader:

    // 4-BLAC: 4x1 + 4x1
    _t67_13 = _mm256_add_pd(_t67_13, _t67_15);

    // AVX Storer:
    _mm256_storeu_pd(x + i0, _t67_13);
  }


  // Generating : P[28,28] = ( ( Sum_{i0} ( ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, i0)) - ( G(h(4, 28, i0), M2[28,8],h(4, 8, 0)) * G(h(4, 8, 0), M1[8,28],h(4, 28, i0)) ) ),h(4, 28, i0)) + -$(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,8],h(4, 8, 4)) * G(h(4, 8, 4), M1[8,28],h(4, 28, i0)) ),h(4, 28, i0)) ) + Sum_{k3} ( ( S(h(4, 28, i0), ( G(h(4, 28, i0), Y[28,28],h(4, 28, k3)) - ( G(h(4, 28, i0), M2[28,8],h(4, 8, 0)) * G(h(4, 8, 0), M1[8,28],h(4, 28, k3)) ) ),h(4, 28, k3)) + -$(h(4, 28, i0), ( G(h(4, 28, i0), M2[28,8],h(4, 8, 4)) * G(h(4, 8, 4), M1[8,28],h(4, 28, k3)) ),h(4, 28, k3)) ) ) ) ) + S(h(4, 28, 24), ( G(h(4, 28, 24), Y[28,28],h(4, 28, 24)) - ( G(h(4, 28, 24), M2[28,8],h(4, 8, 0)) * G(h(4, 8, 0), M1[8,28],h(4, 28, 24)) ) ),h(4, 28, 24)) ) + -$(h(4, 28, 24), ( G(h(4, 28, 24), M2[28,8],h(4, 8, 4)) * G(h(4, 8, 4), M1[8,28],h(4, 28, 24)) ),h(4, 28, 24)) )

  _mm256_storeu_pd(M1, _t56_115);
  _mm256_storeu_pd(M1 + 28, _t56_111);
  _mm256_storeu_pd(M1 + 56, _t56_112);
  _mm256_storeu_pd(M1 + 84, _t56_113);

  for( int i0 = 0; i0 <= 23; i0+=4 ) {
    _t68_43 = _mm256_loadu_pd(Y + 29*i0);
    _t68_42 = _mm256_maskload_pd(Y + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63));
    _t68_41 = _mm256_maskload_pd(Y + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63));
    _t68_40 = _mm256_maskload_pd(Y + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63));
    _t68_39 = _mm256_broadcast_sd(M2 + 8*i0);
    _t68_38 = _mm256_broadcast_sd(M2 + 8*i0 + 1);
    _t68_37 = _mm256_broadcast_sd(M2 + 8*i0 + 2);
    _t68_36 = _mm256_broadcast_sd(M2 + 8*i0 + 3);
    _t68_35 = _mm256_broadcast_sd(M2 + 8*i0 + 8);
    _t68_34 = _mm256_broadcast_sd(M2 + 8*i0 + 9);
    _t68_33 = _mm256_broadcast_sd(M2 + 8*i0 + 10);
    _t68_32 = _mm256_broadcast_sd(M2 + 8*i0 + 11);
    _t68_31 = _mm256_broadcast_sd(M2 + 8*i0 + 16);
    _t68_30 = _mm256_broadcast_sd(M2 + 8*i0 + 17);
    _t68_29 = _mm256_broadcast_sd(M2 + 8*i0 + 18);
    _t68_28 = _mm256_broadcast_sd(M2 + 8*i0 + 19);
    _t68_27 = _mm256_broadcast_sd(M2 + 8*i0 + 24);
    _t68_26 = _mm256_broadcast_sd(M2 + 8*i0 + 25);
    _t68_25 = _mm256_broadcast_sd(M2 + 8*i0 + 26);
    _t68_24 = _mm256_broadcast_sd(M2 + 8*i0 + 27);
    _t68_23 = _mm256_loadu_pd(M1 + i0);
    _t68_22 = _mm256_loadu_pd(M1 + i0 + 28);
    _t68_21 = _mm256_loadu_pd(M1 + i0 + 56);
    _t68_20 = _mm256_loadu_pd(M1 + i0 + 84);
    _t68_19 = _mm256_broadcast_sd(M2 + 8*i0 + 4);
    _t68_18 = _mm256_broadcast_sd(M2 + 8*i0 + 5);
    _t68_17 = _mm256_broadcast_sd(M2 + 8*i0 + 6);
    _t68_16 = _mm256_broadcast_sd(M2 + 8*i0 + 7);
    _t68_15 = _mm256_broadcast_sd(M2 + 8*i0 + 12);
    _t68_14 = _mm256_broadcast_sd(M2 + 8*i0 + 13);
    _t68_13 = _mm256_broadcast_sd(M2 + 8*i0 + 14);
    _t68_12 = _mm256_broadcast_sd(M2 + 8*i0 + 15);
    _t68_11 = _mm256_broadcast_sd(M2 + 8*i0 + 20);
    _t68_10 = _mm256_broadcast_sd(M2 + 8*i0 + 21);
    _t68_9 = _mm256_broadcast_sd(M2 + 8*i0 + 22);
    _t68_8 = _mm256_broadcast_sd(M2 + 8*i0 + 23);
    _t68_7 = _mm256_broadcast_sd(M2 + 8*i0 + 28);
    _t68_6 = _mm256_broadcast_sd(M2 + 8*i0 + 29);
    _t68_5 = _mm256_broadcast_sd(M2 + 8*i0 + 30);
    _t68_4 = _mm256_broadcast_sd(M2 + 8*i0 + 31);
    _t68_3 = _mm256_loadu_pd(M1 + i0 + 112);
    _t68_2 = _mm256_loadu_pd(M1 + i0 + 140);
    _t68_1 = _mm256_loadu_pd(M1 + i0 + 168);
    _t68_0 = _mm256_loadu_pd(M1 + i0 + 196);

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t68_60 = _t68_43;
    _t68_61 = _mm256_blend_pd(_mm256_shuffle_pd(_t68_43, _t68_42, 3), _t68_42, 12);
    _t68_62 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t68_43, _t68_42, 0), _t68_41, 49);
    _t68_63 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t68_43, _t68_42, 12), _mm256_shuffle_pd(_t68_41, _t68_40, 12), 49);

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t68_48 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_39, _t68_23), _mm256_mul_pd(_t68_38, _t68_22)), _mm256_add_pd(_mm256_mul_pd(_t68_37, _t68_21), _mm256_mul_pd(_t68_36, _t68_20)));
    _t68_49 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_35, _t68_23), _mm256_mul_pd(_t68_34, _t68_22)), _mm256_add_pd(_mm256_mul_pd(_t68_33, _t68_21), _mm256_mul_pd(_t68_32, _t68_20)));
    _t68_50 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_31, _t68_23), _mm256_mul_pd(_t68_30, _t68_22)), _mm256_add_pd(_mm256_mul_pd(_t68_29, _t68_21), _mm256_mul_pd(_t68_28, _t68_20)));
    _t68_51 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_27, _t68_23), _mm256_mul_pd(_t68_26, _t68_22)), _mm256_add_pd(_mm256_mul_pd(_t68_25, _t68_21), _mm256_mul_pd(_t68_24, _t68_20)));

    // 4-BLAC: 4x4 - 4x4
    _t68_56 = _mm256_sub_pd(_t68_60, _t68_48);
    _t68_57 = _mm256_sub_pd(_t68_61, _t68_49);
    _t68_58 = _mm256_sub_pd(_t68_62, _t68_50);
    _t68_59 = _mm256_sub_pd(_t68_63, _t68_51);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t68_44 = _t68_56;
    _t68_45 = _t68_57;
    _t68_46 = _t68_58;
    _t68_47 = _t68_59;

    // AVX Loader:

    // AVX Loader:

    // 4-BLAC: 4x4 * 4x4
    _t68_52 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_19, _t68_3), _mm256_mul_pd(_t68_18, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t68_17, _t68_1), _mm256_mul_pd(_t68_16, _t68_0)));
    _t68_53 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_15, _t68_3), _mm256_mul_pd(_t68_14, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t68_13, _t68_1), _mm256_mul_pd(_t68_12, _t68_0)));
    _t68_54 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_11, _t68_3), _mm256_mul_pd(_t68_10, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t68_9, _t68_1), _mm256_mul_pd(_t68_8, _t68_0)));
    _t68_55 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_7, _t68_3), _mm256_mul_pd(_t68_6, _t68_2)), _mm256_add_pd(_mm256_mul_pd(_t68_5, _t68_1), _mm256_mul_pd(_t68_4, _t68_0)));

    // AVX Loader:

    // 4x4 -> 4x4 - UpSymm
    _t68_64 = _t68_44;
    _t68_65 = _mm256_blend_pd(_mm256_shuffle_pd(_t68_44, _t68_45, 3), _t68_45, 12);
    _t68_66 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t68_44, _t68_45, 0), _t68_46, 49);
    _t68_67 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t68_44, _t68_45, 12), _mm256_shuffle_pd(_t68_46, _t68_47, 12), 49);

    // 4-BLAC: 4x4 - 4x4
    _t68_64 = _mm256_sub_pd(_t68_64, _t68_52);
    _t68_65 = _mm256_sub_pd(_t68_65, _t68_53);
    _t68_66 = _mm256_sub_pd(_t68_66, _t68_54);
    _t68_67 = _mm256_sub_pd(_t68_67, _t68_55);

    // AVX Storer:

    // 4x4 -> 4x4 - UpSymm
    _t68_44 = _t68_64;
    _t68_45 = _t68_65;
    _t68_46 = _t68_66;
    _t68_47 = _t68_67;

    // AVX Loader:

    // AVX Loader:

    for( int k3 = 4*floord(i0 - 1, 4) + 8; k3 <= 27; k3+=4 ) {
      _t69_11 = _mm256_loadu_pd(Y + 28*i0 + k3);
      _t69_10 = _mm256_loadu_pd(Y + 28*i0 + k3 + 28);
      _t69_9 = _mm256_loadu_pd(Y + 28*i0 + k3 + 56);
      _t69_8 = _mm256_loadu_pd(Y + 28*i0 + k3 + 84);
      _t69_7 = _mm256_loadu_pd(M1 + k3);
      _t69_6 = _mm256_loadu_pd(M1 + k3 + 28);
      _t69_5 = _mm256_loadu_pd(M1 + k3 + 56);
      _t69_4 = _mm256_loadu_pd(M1 + k3 + 84);
      _t69_3 = _mm256_loadu_pd(M1 + k3 + 112);
      _t69_2 = _mm256_loadu_pd(M1 + k3 + 140);
      _t69_1 = _mm256_loadu_pd(M1 + k3 + 168);
      _t69_0 = _mm256_loadu_pd(M1 + k3 + 196);

      // AVX Loader:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t69_12 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_39, _t69_7), _mm256_mul_pd(_t68_38, _t69_6)), _mm256_add_pd(_mm256_mul_pd(_t68_37, _t69_5), _mm256_mul_pd(_t68_36, _t69_4)));
      _t69_13 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_35, _t69_7), _mm256_mul_pd(_t68_34, _t69_6)), _mm256_add_pd(_mm256_mul_pd(_t68_33, _t69_5), _mm256_mul_pd(_t68_32, _t69_4)));
      _t69_14 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_31, _t69_7), _mm256_mul_pd(_t68_30, _t69_6)), _mm256_add_pd(_mm256_mul_pd(_t68_29, _t69_5), _mm256_mul_pd(_t68_28, _t69_4)));
      _t69_15 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_27, _t69_7), _mm256_mul_pd(_t68_26, _t69_6)), _mm256_add_pd(_mm256_mul_pd(_t68_25, _t69_5), _mm256_mul_pd(_t68_24, _t69_4)));

      // 4-BLAC: 4x4 - 4x4
      _t69_20 = _mm256_sub_pd(_t69_11, _t69_12);
      _t69_21 = _mm256_sub_pd(_t69_10, _t69_13);
      _t69_22 = _mm256_sub_pd(_t69_9, _t69_14);
      _t69_23 = _mm256_sub_pd(_t69_8, _t69_15);

      // AVX Storer:

      // AVX Loader:

      // AVX Loader:

      // 4-BLAC: 4x4 * 4x4
      _t69_16 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_19, _t69_3), _mm256_mul_pd(_t68_18, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t68_17, _t69_1), _mm256_mul_pd(_t68_16, _t69_0)));
      _t69_17 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_15, _t69_3), _mm256_mul_pd(_t68_14, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t68_13, _t69_1), _mm256_mul_pd(_t68_12, _t69_0)));
      _t69_18 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_11, _t69_3), _mm256_mul_pd(_t68_10, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t68_9, _t69_1), _mm256_mul_pd(_t68_8, _t69_0)));
      _t69_19 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t68_7, _t69_3), _mm256_mul_pd(_t68_6, _t69_2)), _mm256_add_pd(_mm256_mul_pd(_t68_5, _t69_1), _mm256_mul_pd(_t68_4, _t69_0)));

      // AVX Loader:

      // 4-BLAC: 4x4 - 4x4
      _t69_20 = _mm256_sub_pd(_t69_20, _t69_16);
      _t69_21 = _mm256_sub_pd(_t69_21, _t69_17);
      _t69_22 = _mm256_sub_pd(_t69_22, _t69_18);
      _t69_23 = _mm256_sub_pd(_t69_23, _t69_19);

      // AVX Storer:
      _mm256_storeu_pd(P + 28*i0 + k3, _t69_20);
      _mm256_storeu_pd(P + 28*i0 + k3 + 28, _t69_21);
      _mm256_storeu_pd(P + 28*i0 + k3 + 56, _t69_22);
      _mm256_storeu_pd(P + 28*i0 + k3 + 84, _t69_23);
    }
    _mm256_storeu_pd(P + 29*i0, _t68_44);
    _mm256_maskstore_pd(P + 29*i0 + 28, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t68_45);
    _mm256_maskstore_pd(P + 29*i0 + 56, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t68_46);
    _mm256_maskstore_pd(P + 29*i0 + 84, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t68_47);
  }

  _t70_39 = _mm256_broadcast_sd(M2 + 192);
  _t70_38 = _mm256_broadcast_sd(M2 + 193);
  _t70_37 = _mm256_broadcast_sd(M2 + 194);
  _t70_36 = _mm256_broadcast_sd(M2 + 195);
  _t70_35 = _mm256_broadcast_sd(M2 + 200);
  _t70_34 = _mm256_broadcast_sd(M2 + 201);
  _t70_33 = _mm256_broadcast_sd(M2 + 202);
  _t70_32 = _mm256_broadcast_sd(M2 + 203);
  _t70_31 = _mm256_broadcast_sd(M2 + 208);
  _t70_30 = _mm256_broadcast_sd(M2 + 209);
  _t70_29 = _mm256_broadcast_sd(M2 + 210);
  _t70_28 = _mm256_broadcast_sd(M2 + 211);
  _t70_27 = _mm256_broadcast_sd(M2 + 216);
  _t70_26 = _mm256_broadcast_sd(M2 + 217);
  _t70_25 = _mm256_broadcast_sd(M2 + 218);
  _t70_24 = _mm256_broadcast_sd(M2 + 219);
  _t70_23 = _mm256_loadu_pd(M1 + 24);
  _t70_22 = _mm256_loadu_pd(M1 + 52);
  _t70_21 = _mm256_loadu_pd(M1 + 80);
  _t70_20 = _mm256_loadu_pd(M1 + 108);
  _t70_19 = _mm256_broadcast_sd(M2 + 196);
  _t70_18 = _mm256_broadcast_sd(M2 + 197);
  _t70_17 = _mm256_broadcast_sd(M2 + 198);
  _t70_16 = _mm256_broadcast_sd(M2 + 199);
  _t70_15 = _mm256_broadcast_sd(M2 + 204);
  _t70_14 = _mm256_broadcast_sd(M2 + 205);
  _t70_13 = _mm256_broadcast_sd(M2 + 206);
  _t70_12 = _mm256_broadcast_sd(M2 + 207);
  _t70_11 = _mm256_broadcast_sd(M2 + 212);
  _t70_10 = _mm256_broadcast_sd(M2 + 213);
  _t70_9 = _mm256_broadcast_sd(M2 + 214);
  _t70_8 = _mm256_broadcast_sd(M2 + 215);
  _t70_7 = _mm256_broadcast_sd(M2 + 220);
  _t70_6 = _mm256_broadcast_sd(M2 + 221);
  _t70_5 = _mm256_broadcast_sd(M2 + 222);
  _t70_4 = _mm256_broadcast_sd(M2 + 223);
  _t70_3 = _mm256_loadu_pd(M1 + 136);
  _t70_2 = _mm256_loadu_pd(M1 + 164);
  _t70_1 = _mm256_loadu_pd(M1 + 192);
  _t70_0 = _mm256_loadu_pd(M1 + 220);

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t70_52 = _t19_28;
  _t70_53 = _mm256_blend_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 3), _t19_29, 12);
  _t70_54 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 0), _t19_30, 49);
  _t70_55 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t19_28, _t19_29, 12), _mm256_shuffle_pd(_t19_30, _t19_31, 12), 49);

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t70_40 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_39, _t70_23), _mm256_mul_pd(_t70_38, _t70_22)), _mm256_add_pd(_mm256_mul_pd(_t70_37, _t70_21), _mm256_mul_pd(_t70_36, _t70_20)));
  _t70_41 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_35, _t70_23), _mm256_mul_pd(_t70_34, _t70_22)), _mm256_add_pd(_mm256_mul_pd(_t70_33, _t70_21), _mm256_mul_pd(_t70_32, _t70_20)));
  _t70_42 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_31, _t70_23), _mm256_mul_pd(_t70_30, _t70_22)), _mm256_add_pd(_mm256_mul_pd(_t70_29, _t70_21), _mm256_mul_pd(_t70_28, _t70_20)));
  _t70_43 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_27, _t70_23), _mm256_mul_pd(_t70_26, _t70_22)), _mm256_add_pd(_mm256_mul_pd(_t70_25, _t70_21), _mm256_mul_pd(_t70_24, _t70_20)));

  // 4-BLAC: 4x4 - 4x4
  _t70_48 = _mm256_sub_pd(_t70_52, _t70_40);
  _t70_49 = _mm256_sub_pd(_t70_53, _t70_41);
  _t70_50 = _mm256_sub_pd(_t70_54, _t70_42);
  _t70_51 = _mm256_sub_pd(_t70_55, _t70_43);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t3_3 = _t70_48;
  _t3_2 = _t70_49;
  _t3_1 = _t70_50;
  _t3_0 = _t70_51;

  // AVX Loader:

  // AVX Loader:

  // 4-BLAC: 4x4 * 4x4
  _t70_44 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_19, _t70_3), _mm256_mul_pd(_t70_18, _t70_2)), _mm256_add_pd(_mm256_mul_pd(_t70_17, _t70_1), _mm256_mul_pd(_t70_16, _t70_0)));
  _t70_45 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_15, _t70_3), _mm256_mul_pd(_t70_14, _t70_2)), _mm256_add_pd(_mm256_mul_pd(_t70_13, _t70_1), _mm256_mul_pd(_t70_12, _t70_0)));
  _t70_46 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_11, _t70_3), _mm256_mul_pd(_t70_10, _t70_2)), _mm256_add_pd(_mm256_mul_pd(_t70_9, _t70_1), _mm256_mul_pd(_t70_8, _t70_0)));
  _t70_47 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(_t70_7, _t70_3), _mm256_mul_pd(_t70_6, _t70_2)), _mm256_add_pd(_mm256_mul_pd(_t70_5, _t70_1), _mm256_mul_pd(_t70_4, _t70_0)));

  // AVX Loader:

  // 4x4 -> 4x4 - UpSymm
  _t70_56 = _t3_3;
  _t70_57 = _mm256_blend_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 3), _t3_2, 12);
  _t70_58 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 0), _t3_1, 49);
  _t70_59 = _mm256_permute2f128_pd(_mm256_shuffle_pd(_t3_3, _t3_2, 12), _mm256_shuffle_pd(_t3_1, _t3_0, 12), 49);

  // 4-BLAC: 4x4 - 4x4
  _t70_56 = _mm256_sub_pd(_t70_56, _t70_44);
  _t70_57 = _mm256_sub_pd(_t70_57, _t70_45);
  _t70_58 = _mm256_sub_pd(_t70_58, _t70_46);
  _t70_59 = _mm256_sub_pd(_t70_59, _t70_47);

  // AVX Storer:

  // 4x4 -> 4x4 - UpSymm
  _t3_3 = _t70_56;
  _t3_2 = _t70_57;
  _t3_1 = _t70_58;
  _t3_0 = _t70_59;

  _mm256_storeu_pd(Y + 696, _t19_28);
  _mm256_maskstore_pd(Y + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t19_29);
  _mm256_maskstore_pd(Y + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t19_30);
  _mm256_maskstore_pd(Y + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t19_31);
  _mm256_maskstore_pd(M3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_65);
  _mm256_maskstore_pd(M3 + 9, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_71);
  _mm256_maskstore_pd(M3 + 10, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_73);
  _mm256_maskstore_pd(M3 + 18, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_76);
  _mm256_maskstore_pd(M3 + 27, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_78);
  _mm256_maskstore_pd(M3 + 36, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_81);
  _mm256_maskstore_pd(M3 + 45, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_87);
  _mm256_maskstore_pd(M3 + 46, _mm256_setr_epi64x((__int64)1 << 63, (__int64)1 << 63, 0, 0), _t56_89);
  _mm256_maskstore_pd(M3 + 54, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_92);
  _mm256_maskstore_pd(M3 + 63, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_94);
  _mm256_maskstore_pd(v0 + 3, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_100);
  _mm256_maskstore_pd(v0 + 7, _mm256_setr_epi64x((__int64)1 << 63, 0, 0, 0), _t56_106);
  _mm256_storeu_pd(P + 696, _t3_3);
  _mm256_maskstore_pd(P + 724, _mm256_setr_epi64x(0, (__int64)1 << 63, (__int64)1 << 63, (__int64)1 << 63), _t3_2);
  _mm256_maskstore_pd(P + 752, _mm256_setr_epi64x(0, 0, (__int64)1 << 63, (__int64)1 << 63), _t3_1);
  _mm256_maskstore_pd(P + 780, _mm256_setr_epi64x(0, 0, 0, (__int64)1 << 63), _t3_0);

}
